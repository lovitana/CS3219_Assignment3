<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9993985">
A Random Walk–Based Model for
Identifying Semantic Orientation
</title>
<author confidence="0.990673">
Ahmed Hassan*
</author>
<affiliation confidence="0.867767">
Microsoft Research
</affiliation>
<author confidence="0.96407">
Amjad Abu-Jbara**
</author>
<affiliation confidence="0.988678">
University of Michigan
</affiliation>
<author confidence="0.990998">
Wanchen Lut
</author>
<affiliation confidence="0.997356">
University of Michigan
</affiliation>
<author confidence="0.96417">
Dragomir Radevt
</author>
<affiliation confidence="0.997986">
University of Michigan
</affiliation>
<construct confidence="0.745191714285714">
Automatically identifying the sentiment polarity of words is a very important task that has
been used as the essential building block of many natural language processing systems such as
text classification, text filtering, product review analysis, survey response analysis, and on-line
discussion mining. We propose a method for identifying the sentiment polarity of words that
applies a Markov random walk model to a large word relatedness graph, and produces a polarity
estimate for any given word. The model can accurately and quickly assign a polarity sign and
magnitude to any word. It can be used both in a semi-supervised setting where a training set of
</construct>
<bodyText confidence="0.867906636363637">
labeled words is used, and in a weakly supervised setting where only a handful of seed words is
used to define the two polarity classes. The method is experimentally tested using a gold standard
set of positive and negative words from the General Inquirer lexicon. We also show how our
method can be used for three-way classification which identifies neutral words in addition to
positive and negative words. Our experiments show that the proposed method outperforms the
state-of-the-art methods in the semi-supervised setting and is comparable to the best reported
values in the weakly supervised setting. In addition, the proposed method is faster and does not
need a large corpus. We also present extensions of our methods for identifying the polarity of
foreign words and out-of-vocabulary words.
* Microsoft Research, Redmond, WA, USA. E-mail: hassanam®microsoft.com. This research was
performed while at the University of Michigan.
</bodyText>
<affiliation confidence="0.685548666666667">
** Department of Electrical Engineering &amp; Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: amjbara®umich.edu.
t Department of Electrical Engineering &amp; Computer Science, University of Michigan, Ann Arbor, MI, USA.
</affiliation>
<email confidence="0.569849">
E-mail: wanchlu®umich.edu.
</email>
<affiliation confidence="0.8993325">
t Department of Electrical Engineering &amp; Computer Science and School of Information, University of
Michigan, Ann Arbor, MI, USA. E-mail: radev®umich.edu.
</affiliation>
<note confidence="0.9716044">
Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication:
14 July 2013.
doi:10.1162/COLI a 00192
© 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
</note>
<sectionHeader confidence="0.987101" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999891878048781">
Identifying emotions and attitudes from unstructured text has a variety of possible
applications. For example, there has been a large body of work for mining product
reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002)
have shown how product reputation mining helps with marketing and customer re-
lation management. The Google products catalog and many on-line shopping sites
like Amazon.com provide customers not only with comprehensive information and
reviews about a product, but also with faceted sentiment summaries. Such systems are
all supported by a sentiment lexicon, some even in multiple languages.
Another interesting application is mining on-line discussions. An enormous num-
ber of discussion groups exist on the Web. Millions of users post content to these groups
covering pretty much every possible topic. Tracking a participant attitude toward differ-
ent topics and toward other participants is a very important task that makes use of sen-
timent lexicons. For example, Tong (2001) presented the concept of sentiment timelines.
His system classifies discussion posts about movies as either positive or negative. This
is used to produce a plot of the number of positive and negative sentiment messages
over time. All these applications would benefit from an automatic way of identifying
semantic orientation of words.
In this article, we study the task of automatically identifying the semantic orienta-
tion of any word by analyzing its relations to other words, Automatically classifying
words as positive, negative, or neutral enables us to automatically identify the polarity
of larger pieces of text. This could be a very useful building block for systems that
mine surveys, product reviews, and on-line discussions. We apply a Markov random
walk model to a large semantic relatedness graph, producing a polarity estimate for
any given word. Previous work on identifying the semantic orientation of words has
addressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005)
and a weakly supervised (Turney and Littman 2003) learning problem. In the semi-
supervised setting, a training set of labeled words is used to train the model. In the
weakly supervised setting, only a handful of seeds are used to define the two polarity
classes.
Our proposed method can be used both in a semi-supervised and in a weakly
supervised setting. Empirical experiments on a labeled set of positive and negative
words show that the proposed method outperforms the state-of-the-art methods in the
semi-supervised setting. The results in the weakly supervised setting are comparable to
the best reported values. The proposed method has the advantages that it is faster and
does not need a large training corpus.
The rest of the article is structured as follows. In Section 2, we review related work
on word polarity and subjectivity classification and note applications of the random
walk and hitting times framework. Section 3 presents our method for identifying word
polarity. We describe how the proposed method can be extended to cover foreign
languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes
our experimental set-up. We present our conclusions in Section 7.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2. Related Work
</sectionHeader>
<subsectionHeader confidence="0.755189">
2.1 Identifying Word Polarity
</subsectionHeader>
<bodyText confidence="0.968199">
Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word
polarity of adjectives. They extract all conjunctions of adjectives from a given corpus
</bodyText>
<page confidence="0.99097">
540
</page>
<note confidence="0.945827">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.9999327">
and then they classify each conjunctive expression as either the same orientation such
as “simple and well-received” or different orientation such as “simplistic but well-
received.” The result is a graph that they cluster into two subsets of adjectives. They
classify the cluster with the higher average frequency as positive. They created and
labeled their own data set for experiments. Their approach works only with adjectives
because there is nothing wrong with conjunctions of nouns or verbs with opposite
polarities (“war and peace”, “rise and fall”, etc.).
Turney and Littman (2003) identify word polarity by looking at its statistical asso-
ciation with a set of positive/negative seed words. They use two statistical measures
for estimating association: Pointwise Mutual Information (PMI) and Latent Semantic
Analysis (LSA). To get co-occurrence statistics, they submit several queries to a search
engine. Each query consists of the given word and one of the seed words. They use the
search engine NEAR operator to look for instances where the given word is physically
close to the seed word in the returned document. They present their method as an un-
supervised method where a very small number of seed words are used to define
semantic orientation rather than train the model. One of the limitations of their method
is that it requires a large corpus of text to achieve good performance. They use sev-
eral corpora; the size of the best performing data set is roughly one hundred billion
words (Turney and Littman 2003).
Takamura et al. (2005) propose using spin models for extracting semantic orienta-
tion of words. They construct a network of words using gloss definitions, thesaurus, and
co-occurrence statistics. They regard each word as an electron. Each electron has a spin
and each spin has a direction taking one of two values: up or down. Two neighboring
spins tend to have the same orientation from an energy point of view. Their hypothesis
is that as neighboring electrons tend to have the same spin direction, neighboring words
tend to have similar polarity. They pose the problem as an optimization problem and
use the mean field method to find the best solution. The analogy with electrons leads
them to assume that each word should be either positive or negative. This assumption
is not accurate because most of the words in the language do not have any semantic ori-
entation. They report that their method could get misled by noise in the gloss definition
and their computations sometimes get trapped in a local optimum because of its greedy
optimization flavor.
Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms
and then use the shortest paths between any given word and the words “good” and
“bad” to determine word polarity. They report that using shortest paths could be very
noisy. For example, “good” and “bad” themselves are closely related in WordNet with
a 5-long sequence “good, sound, heavy, big, bad.” A given word w may be more
connected to one set of words (e.g., positive words); yet have a shorter path connecting
it to one word in the other set. Restricting seed words to only two words affects their
accuracy. Adding more seed words could help but it will make their method extremely
costly from the computation point of view. They evaluate their method using only
adjectives.
Hu and Liu (2004) propose another method that uses WordNet. They use WordNet
synonyms and antonyms to predict the polarity of words. For any word whose polarity
is unknown, they search WordNet and a list of seed labeled words to predict its polarity.
They check if any of the synonyms of the given word has known polarity. If so, they
label it with the label of its synonym. Otherwise, they check if any of the antonyms
of the given word has known polarity. If so, they label it with the opposite label of
the antonym. They continue in a bootstrapping manner until they label all possible
words.
</bodyText>
<page confidence="0.99338">
541
</page>
<note confidence="0.809749">
Computational Linguistics Volume 40, Number 3
</note>
<subsectionHeader confidence="0.99256">
2.2 Building Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.999941461538462">
A number of other methods try to build lexicons of polarized words. Esuli and
Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses
of the word as found in some dictionary. Then, a binary text classifier is trained using
the textual representation and applied to new words.
Kim and Hovy (2004) start with two lists of positive and negative seed words. Word-
Net is used to expand these lists. Synonyms of positive words and antonyms of negative
words are considered positive, and synonyms of negative words and antonyms of posi-
tive words are considered negative. A similar method is presented in Andreevskaia and
Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively
expand a list of seeds. The sentiment classes are treated as fuzzy categories where some
words are very central to one category, whereas others may be interpreted differently.
Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that
overtly marked words such as dishonest, unhappy, and impure tend to have negative
semantic orientations whereas their unmarked counterparts (honest, happy, and pure)
tend to have positive semantic orientation. They use a set of 11 antonym-generating affix
patterns to generate overtly marked words and their counterparts from the Macquarie
Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the
sentiment lexicon using a Roget-like thesaurus. Their method does not require seed
sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of
the marking theory is language-dependent and cannot be applied from one language to
another.
Contrasting the dictionary based approaches that rely on resources such as Word-
Net, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons
semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic fea-
tures and context coherency (i.e., the tendency for same polarities to appear succes-
sively) to detect polar clauses.
</bodyText>
<subsectionHeader confidence="0.999596">
2.3 Random Walk–Based Methods
</subsectionHeader>
<bodyText confidence="0.997690277777778">
Closest to our work in its methodology is probably the line of research on semi-
supervised graphical methods for sentiment classification. Rao and Ravichandran
(2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled
and labeled nodes, each node representing a word that can be either positive or neg-
ative, and each edge representing some semantic relatedness that can be constructed
using resources like WordNet or other thesaurus. They evaluate two semi-supervised
methods: Mincut (including its variant, Randomized Mincut) and label propagation.
The general idea of label propagation is defining a probability distribution over the
positive and negative classes for each node in the graph. A Markov random walk is
performed on the graph to recover this distribution for the unlabeled nodes.
Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a
similar label propagation method on a lexical graph built from WordNet, where a small
set of words with known polarities are used as seeds. Brody and Elhadad (2010) use
label propagation over a graph constructed of adjectives only.
Velikovich et al. (2010) compare label propagation with a Web-based method and
conclude that label propagation is not suitable when the whole Web is used as a
background corpus, because the constructed graph is very noisy and contains many
dense subgraphs, unlike the lexical graph constructed from WordNet.
</bodyText>
<page confidence="0.985219">
542
</page>
<note confidence="0.674965">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.99755575">
Random walk–based methods have been studied in the context of many other NLP
tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel
corpora, where each node represents a phrase and two nodes are connected by an edge
if they are aligned in a phrase table. Then they compute hitting time of random walks
to learn paraphrases.
Our work is different from previous random walk methods in that it uses the mean
hitting time as the criterion for assigning polarity labels. Our experiments showed that
this achieves better results than methods that use label propagation.
</bodyText>
<subsectionHeader confidence="0.999432">
2.4 Subjectivity Analysis
</subsectionHeader>
<bodyText confidence="0.999908764705882">
Subjectivity analysis is another research line that is closely related to our work. The
main task in subjectivity analysis is to identify text that presents opinion as opposed to
objective text that present factual information (Wiebe 2000). Text could be either words,
phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of
subjectivity analysis such as classifying e-mails and mining reviews. For example, to
analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from
individual sentences as nodes to determine whether a sentence is subjective or objective.
Each node (sentence) has an individual subjectivity score obtained from a first-pass
classifier using sentence features and linguistic knowledge. Edges are weighted by a
similarity metric of how likely it is that the two sentences will be in the same subjectivity
class. All sentences to be classified are represented as unlabeled nodes and the only two
labeled nodes represent the subjective and objective classes. A Mincut algorithm is then
performed on the constructed graph to obtain the subjectivity classes for individual
sentences. The authors also integrate the subjectivity classification of isolated sentences
to document level sentiment analysis.
There are two main categories of work on subjectivity analysis. In the first cate-
gory, subjective words and phrases are identified without considering their context
(Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In
the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff
and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and
Mihalcea (2006a) studied the association of word subjectivity and word sense. They
showed that different subjectivity labels can be assigned to different senses of the same
word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles
from a wide variety of news sources manually annotated for opinions and other private
states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions
and emotions in language.
In addition, there has been a large body of work on labeling subjectivity of WordNet
words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or ob-
jective, utilizing the MPQA corpus. They show that subjectivity information for Word-
Net senses can improve word sense disambiguation tasks for subjectivity ambiguous
words.
Su and Markert (2009) propose a semi-supervised minimum cut framework to label
word sense entries in WordNet with subjectivity information. Their method requires
less training data other than the sense definitions and relational structure of WordNet.
</bodyText>
<subsectionHeader confidence="0.992647">
2.5 Word Polarity Classification for Foreign Languages
</subsectionHeader>
<bodyText confidence="0.988159">
Word sentiment and subjectivity has also been studied for languages other than English.
Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity
</bodyText>
<page confidence="0.994475">
543
</page>
<note confidence="0.338475">
Computational Linguistics Volume 40, Number 3
</note>
<bodyText confidence="0.999591">
lexicon based on an English lexicon, an on-line translation service, and Wordnet.
Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a
parallel corpus to generate subjectivity analysis resources for foreign languages. Rao
and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi
WordNet and French using a French thesaurus.
</bodyText>
<sectionHeader confidence="0.977309" genericHeader="method">
3. Approach
</sectionHeader>
<bodyText confidence="0.9999436">
We use a Markov random walk model to identify the polarity of words. Assume that
we have a network of words, some of which are labeled as either positive or nega-
tive. In this network, two words are connected if they are related. Different sources
of information are used to decide whether two words are related. For example, the
synonyms of a word are all semantically related to it. The intuition behind connect-
ing semantically related words is that those words tend to have similar polarities.
Now imagine a random surfer walking along the network starting from an unlabeled
word w.
The random walk continues until the surfer hits a labeled word. If the word w is
positive then the probability that the random walk hits a positive word is higher, and if
w is negative then the probability that the random walk hits a negative word is higher.
Thus, if the word w is positive then the average time it takes a random walk starting at
w to hit a positive node should be much less than the average time it takes a random
walk starting at w to hit a negative node. If w doesn’t have a clear polarity and we would
like to say that it is neutral, we expect that the positive hitting time and negative hitting
time to not have a significant difference.
We describe how we construct a word relatedness graph in Section 3.1. The random
walk model is described in Section 3.2. Hitting time is defined in Section 3.3. Finally,
an algorithm for computing a sign and magnitude for the polarity of any given word
is described in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.989496">
3.1 Network Construction
</subsectionHeader>
<bodyText confidence="0.980644166666667">
We construct a network where two nodes are linked if they are semantically related.
Several sources of information are used as indicators of the relatedness of words. One
such source is WordNet (Miller 1995). WordNet is a large lexical database of English.
Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms
(synsets), each expressing a distinct concept (Miller 1995). Synsets are interlinked by
means of conceptual-semantic and lexical relations.
The simplest approach is to connect words that occur in the same WordNet synset.
We can collect all words in WordNet, and add links between any two words that
occur in the same synset. The resulting graph is a graph G(W,E) where W is a set
of word/part-of-speech (POS) pairs for all the words in WordNet. E is the set of
edges connecting each pair of synonymous words. Nodes represent word/POS pairs
rather than words because the part of speech tags are helpful in disambiguating
the different senses for a given word. For example, the word “fine” has two dif-
ferent meanings, with two opposite polarities when used as an adjective and as a
noun.
Several other methods can be used to link words. For example, we can use other
WordNet relations: hypernyms, similar to, and so forth. Another source of links be-
tween words is co-occurrence statistics from a corpus. Following the method presented
</bodyText>
<page confidence="0.989917">
544
</page>
<note confidence="0.673329">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.999785714285714">
in Hatzivassiloglou and McKeown (1997), we can connect words if they appear together
in a conjunction in the corpus. This method is only applicable to adjectives. If two
adjectives are connected by “and,” it is highly likely that they have the same semantic
orientation. In all our experiments, we restricted the network to only WordNet relations.
We study the effect of using co-occurrence statistics to connect words later at the end of
our experiments. If more than one relation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
</bodyText>
<subsectionHeader confidence="0.997242">
3.2 Random Walk Model
</subsectionHeader>
<bodyText confidence="0.999915133333333">
Imagine a random surfer walking along the word relatedness graph G. Starting from a
word with unknown polarity i, it moves to a node j with probability Pij after the first
step. The walk continues until the surfer hits a word with known polarity. Seed words
with known polarity act as an absorbing boundary for the random walk. If we repeat
the number of random walks N times, the percentage of times in which the walk ends at
a positive/negative word could be used as an indicator of its positive/negative polarity.
The average time a random walk starting at w takes to hit the set of positive/negative
nodes is also an indicator of its polarity. This view is closely related to the partially la-
beled classification with random walks approach in Szummer and Jaakkola (2002) and
the semi-supervised learning using harmonic functions approach in Zhu, Ghahramani,
and Lafferty (2003).
Let W be the set of words in our lexicon. We construct a graph whose nodes V are
all words in W. Edges E correspond to the relatedness between words. We define the
transition probability Pt+1|t(j|i) from i to j by normalizing the weights of the edges out
of node i, so:
</bodyText>
<equation confidence="0.9938895">
Pt+1|t(j|i) = Wij/ � Wik (1)
k
</equation>
<bodyText confidence="0.99989025">
where k represents all nodes in the neighborhood of i. Pt+1|t(j|i) denotes the transition
probability from node i at step t to node j at time step t + 1. We note that the matrix of
weights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t(j|i) is not
necessarily symmetric because of the node outdegree normalization.
</bodyText>
<subsectionHeader confidence="0.998995">
3.3 First-Passage Time
</subsectionHeader>
<bodyText confidence="0.999926714285714">
The mean first-passage (hitting) time h(i|k) is defined as the average number of steps a
random walker, starting in state i =� k, will take to enter state k for the first time (Norris
1997). Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Consider
a subset of vertices S C V. Consider a random walk on G starting at node i E� S. Let Nt
denote the position of the random surfer at time t. Let h(i|S) be the average number of
steps a random walker, starting in state i E� S, will take to enter a state k E S for the first
time. Let TS be the first-passage for any vertex in S.
</bodyText>
<equation confidence="0.987597333333333">
P(TS = t|N0 = i) =
E pij x P(TS = t − 1|N0 = j) (2)
jEV
</equation>
<page confidence="0.949949">
545
</page>
<note confidence="0.292585">
Computational Linguistics Volume 40, Number 3
</note>
<bodyText confidence="0.720839">
h(i|S) is the expectation of TS. Hence:
</bodyText>
<equation confidence="0.9945168">
h(i|S) = E(TS|N0 = i)
t x P(TS = t|N0 = i)
pijP(TS = t − 1|N0 = j)
(t − 1)pijP(TS = t − 1|N0 = j)
pijP(TS = t − 1|N0 = j)
tP(TS = t|N0 = j) + 1
pij x h(j|S) + 1 (3)
Hence the first-passage (hitting) time can be formally defined as:
h(i|S) = 0 i E S (4)
�EjEV pij x h(j|S) + 1 otherwise
</equation>
<subsectionHeader confidence="0.990307">
3.4 Word Polarity Calculation
</subsectionHeader>
<bodyText confidence="0.972725777777778">
Based on the description of the random walk model and the first-passage (hitting)
time above, we now propose our word polarity identification algorithm. We begin by
constructing a word relatedness graph and defining a random walk on that graph as
described above. Let S+ and S− be two sets of vertices representing seed words that are
already labeled as either positive or negative, respectively.
For any given word w, we compute the hitting time h(w|S+) and h(w|S−) for the
two sets iteratively as described earlier. The ratio between the two hitting times is then
used as an indication of how positive/negative the given word is. This is useful in
case we need to provide a confidence measure for the prediction. This could be used
to allow the model to abstain from classifying words when the confidence level is low.
It also means that our method can be easily extended from two-way classification (i.e.,
positive or negative) to three-way classification (positive, negative, or neutral). This can
be done by setting a threshold γ on the ratio of positive and negative hitting time,
and classifying a word to positive or negative only when the two hitting times have
a significant difference; otherwise we classify it to neutral.
When the relatedness graph is very large, computing hitting time as described
earlier may be very time consuming. The graph constructed from the English WordNet
=
</bodyText>
<figure confidence="0.996548178571428">
00
E
t=1
=
00
E
t=1
E=
jEV
+E
jEV
E=
jEV
E=
jEV
00
E
t=1
00
E
t=1
pij
00
E
t=1
E
t
jEV
</figure>
<page confidence="0.917283">
546
</page>
<note confidence="0.592088">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.573113">
Algorithm 13-class word polarity using random walks (parameter γ : 0 &lt; γ &lt; 1)
Require: A word relatedness graph G
</bodyText>
<listItem confidence="0.989986071428572">
1: Given a word w in V
2: Define a random walk on the graph. The transition probability between any two
nodes i, and j is defined as: Pt+1|t(j|i) = Wij/ Ek Wik
3: Start k independent random walks from w with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h*(w|S+) be the estimated value for h(w|S+)
6: Repeat for negative words computing h*(w|S−)
7: if h*(w|S+) ≤ γh*(w|S−) then
8: Classify w as positive
9: else if h*(w|S−) ≤ γh*(w|S+) then
10: Classify w as negative
11: else
12: Classify w as neutral
13: end if
</listItem>
<bodyText confidence="0.9992406">
and synsets contains 155,000 nodes and 117,000 edges. To overcome this problem, we
propose a Monte Carlo–based algorithm (Algorithm 1) for estimating it.
In the case of binary classification, where each word must be either positive or
negative, if h(w|S+) is greater than h(w|S−), the word is classified as negative and
positive otherwise. This can be achieved by setting parameter γ = 1 in Algorithm 1.
</bodyText>
<sectionHeader confidence="0.969128" genericHeader="method">
4. Foreign Word Polarity
</sectionHeader>
<bodyText confidence="0.99976735">
As we mentioned earlier, a large body of research has focused on identifying the
semantic orientation of words. This work has almost exclusively dealt with English and
uses several language-dependent resources. When we try to apply these methods to
other languages, we run into the problem of the lack of resources in other languages
when compared with English. For example, the General Inquirer lexicon (Stone et al.
1966) has thousands of English words labeled with semantic orientation. Most of the
literature has used it as a source of labeled seeds or for evaluation. Such lexicons are
not readily available in other languages.
As we showed earlier, WordNet (Miller 1995) has been used for this task. How-
ever, even though W have been built for other languages, their coverage is relatively
limited when compared to the English WordNet. The current release of English Word-
Net (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the
resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al.
2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan
et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in
Spanish, 15K in German, and 22K in French, among other European languages. In some
cases, accuracy was traded for coverage. For example, the current release of the Japanese
WordNet has 57K synsets but contains errors in as many as 5% of the entries.1
In this section, we show how we can extend the methods presented earlier to predict
the semantic orientation of foreign words. The proposed method is based on creating
</bodyText>
<footnote confidence="0.94185">
1 http://nlpwww.nict.go.jp/wn-ja/index.en.html.
</footnote>
<page confidence="0.97831">
547
</page>
<note confidence="0.323144">
Computational Linguistics Volume 40, Number 3
</note>
<bodyText confidence="0.998997384615385">
a multilingual network of words that represents both English and foreign words. The
network has English–English connections, as well as Foreign–Foreign connections and
English–Foreign connections. This allows us to benefit from the richness of the resources
built for the English language and at the same time utilize resources specific to foreign
languages. We define a random walk model over the multilingual network and pre-
dict the semantic orientation of any given word by comparing the mean hitting time
of a random walk starting from it to a positive and a negative set of seed English
words.
We use Arabic and Hindi in our experiments. We compare the performance of sev-
eral methods using the foreign language resources only, and the multilingual network
that has both English and foreign words. We show that bootstrapping from languages
with dense resources such as English is useful for improving the performance on other
languages with limited resources.
</bodyText>
<subsectionHeader confidence="0.982511">
4.1 Multilingual Word Network
</subsectionHeader>
<bodyText confidence="0.999871">
We build a network G(V, E) where V = Ven U Vfr is the union of the sets of English and
Foreign words. E is a set of edges connecting nodes in V. There are three types of connec-
tions: English–English connections, Foreign–Foreign connections, and English–Foreign
connections. For the English–English connections, we use the same methodology as in
Section 3.
Foreign–Foreign connections are created in a similar way to the English con-
nections. Some foreign languages have lexical resources based on the design of the
Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic Word-
Net (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al.
2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of
Hatzivassiloglou and McKeown (1997).
Finally, to connect foreign words to English words, we use a Foreign to English dic-
tionary. For every word in a list of foreign words, we look up its meaning in a dictionary
and add an edge between the foreign word and every other English word that appeared
as a possible meaning for it. If there is no comprehensive enough dictionary available,
constructing a multilingual word network like a translation graph (Etzioni et al. 2007)
may be a resolution.
</bodyText>
<subsectionHeader confidence="0.980886">
4.2 Foreign Word Semantic Orientation Prediction
</subsectionHeader>
<bodyText confidence="0.99939175">
We use the multilingual network described previously to predict the semantic orien-
tation of words based on the mean hitting time to two sets of positive and negative
seeds. Given two lists of seed English words with known polarity, we define two sets
of nodes S+ and S− representing those seeds. For any given word w, we calculate the
mean hitting time between w and the two seed sets h(w|S+) and h(w|S−). If h(w|S+)
is greater than h(w|S−), the word is classified as negative; otherwise it is classified as
positive. We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997)
and Stone et al. (1966).
</bodyText>
<sectionHeader confidence="0.996391" genericHeader="method">
5. Out-of-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.993155">
We observed that a significant portion of the text used on-line in discussions, comments,
product reviews, and so on, contains words that are not defined in WordNet or in
</bodyText>
<page confidence="0.977836">
548
</page>
<note confidence="0.806337">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.999789222222222">
standard dictionaries. We call these words Out-of-Vocabulary (OOV) words. Table 6
later in this article shows some OOV word examples. To show the importance of OOV
word polarity identification, we calculated the proportion of OOV words in three
corpora used for sentiment studies: a set of movie reviews, a set of on-line discussions
from a political forum, and a set of randomly sampled tweets. For each word in the
data, we look it up in two standard English dictionaries, together containing 160K
unique words. Table 1 shows the statistics.
OOV words have a high chance of being polarized because people tend to use
informal language or special acronyms to emphasize their attitudes or impress the
audience. Therefore, being able to automatically identify the polarity of OOV words
will essentially benefit real-world applications.
Consider the graph G(W, E) described in Section 3.1. So far, the only resource we use
to construct the graph is WordNet synsets. The first step in our approach to OOV word
polarity identification is to find the words in WordNet that are related to an OOV
word. Next, we add the OOV words to our graph by creating a new node for each OOV
word and adding an edge between each OOV word and each of its related words. Once
we have constructed the extended network, we use the random walk model described
in Section 3.2 to predict the polarity of each OOV word.
</bodyText>
<subsectionHeader confidence="0.990539">
5.1 Mining OOV Word Relatedness from the Web
</subsectionHeader>
<bodyText confidence="0.999805230769231">
There are several alternative methods of linking words in the graph. Agirre et al. (2009)
studied the strengths and weaknesses of different approaches to term similarity and
relatedness. They noticed that lexicographical methods such as the WordNet suffer from
the limitation of lexicon coverage, which is the case here with OOV words. To overcome
this limitation, we use a Web-based distributional approach to find the set of related
words to each OOV word. We perform a Web search using the OOV word as a search
query and retrieve the top S search results. We extract the textual content of the retrieved
results and tokenize it. After removing all the stop words, we compute the number of
times each word co-occurs with the OOV word in the same document. We rank the
words based on their co-occurrence frequency and return the top R words as the set of
related words to the given OOV word.
We experimented with three different variants of this approach. In the first variant,
the frequency values of the co-occurring words are normalized by the lengths of the
</bodyText>
<tableCaption confidence="0.998306">
Table 1
</tableCaption>
<table confidence="0.989770923076923">
Proportion of OOV words in some corpora used for real world applications. (Numbers in
parentheses exclude words whose first letters are capitalized because they are likely to refer to
named entities.)
corpus source # of words Percentage
of OOV
Movie reviews 3,411 customer reviews from IMDb for the 10.7 M (9.5 M) 5.3 (2.7)
movie The Dark Knight (2008)
Political forum 23K sentences from www.politicalforum.com 381 K (348 K) 8 (6)
on various topics
tweets 0.6M random English tweets from twitter.com. 7.1 M (5.9 M) 30 (27)
(We count a tweet as in English if at least half
of the words are English dictionary words.
Tags and symbols were removed.)
</table>
<page confidence="0.72353">
549
</page>
<note confidence="0.395947">
Computational Linguistics Volume 40, Number 3
</note>
<bodyText confidence="0.9997484">
documents that contributed to the count of each word. The intuition here is that longer
documents contain more words and hence the probability that a word in the that
document is related to the search query (i.e., the OOV word) is lower than when the
document is shorter.
In the second variant, we only consider the words that appear in the proximity of
the OOV word (i.e., within d words around the OOV word) when we compute the co-
occurrence frequency. The intuition here is that words that appear near the OOV word
are more likely to be semantically related than the words that appear far away.
In the third variant, instead of searching the entire Web, we limit the search to
social text. In the experiments described subsequently, we search for the OOV words
in tweets posted on Twitter.2 The intuition here is that searching the entire Web is likely
to return results that do not necessarily contain opinionated text—particularly because
many words have different senses. In contrast, the text written in a social context is more
likely to carry sentiment and express emotions. This helps us find better related words
that suit our task.
</bodyText>
<subsectionHeader confidence="0.984661">
5.2 Word Network Extension with OOV Words
</subsectionHeader>
<bodyText confidence="0.99796">
To extend the graph to include OOV words, we start with the graph G(W, E) constructed
from WordNet synsets. For each OOV word that does not exist in G, we create a new
node w. We set the part of speech of w to unspecified. Then we use the Web-based method
described in the previous section to find a set of words that are most related to w. Finally,
we create a link between each OOV word and each of its related words. To predict the
polarity of an OOV word, we use the same random walk model described earlier.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="evaluation">
6. Experiments
</sectionHeader>
<bodyText confidence="0.999824333333333">
We performed experiments on the gold-standard data set for positive/negative words
from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4,206 words,
1, 915 of which are positive and 2,291 of which are negative. Some of the ambiguous
words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005).
Some examples of positive/negative words are listed in Table 2.
We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the
word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to
generate co-occurrence statistics in the experiments that used them. We used 10-fold
cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical
significance was tested using a two-tailed paired t-test. All reported results are statisti-
cally significant at the 0.05 level. We perform experiments varying the parameters and
the network. We also look at the performance of the proposed method for different
parts of speech, and for different confidence levels. We compare our method to the
Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin
model described in Takamura, Inui, and Okumura (2005), the shortest path method
described in Kamps et al. (2004), a re-implementation of the label propagation and
Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping
method described in Hu and Liu (2004).
</bodyText>
<footnote confidence="0.897207">
2 http://www.twitter.com.
</footnote>
<page confidence="0.987124">
550
</page>
<note confidence="0.970855">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<tableCaption confidence="0.99733">
Table 2
</tableCaption>
<table confidence="0.987369090909091">
Examples of positive and negative words.
Positive Negative
able adjective abandon verb
acceptable adjective abuse verb
admire verb burglar noun
amazing adjective chaos noun
careful adjective contagious adjective
ease noun corruption noun
guide verb lie verb
inspire verb reluctant adjective
truthful adjective wrong adjective
</table>
<subsectionHeader confidence="0.996046">
6.1 Comparison with Other Methods
</subsectionHeader>
<bodyText confidence="0.99969225">
This method could be used in a semi-supervised setting where a set of labeled words are
used and the system learns from these labeled nodes and from other unlabeled nodes.
Under this setting, we compare our method to the spin model described in Takamura,
Inui, and Okumura (2005). Table 3 compares the performance using 10-fold cross val-
idation. The table shows that the proposed method outperforms the spin model. The
spin model approach uses word glosses, WordNet synonym, hypernym, and antonym
relations, in addition to co-occurrence statistics extracted from corpus. The proposed
method achieves better performance by only using WordNet synonym, hypernym, and
similar to relations. Adding co-occurrence statistics slightly improved performance, and
using glosses did not help at all.
We also compare our method to a re-implementation of the label propagation (LP)
method. Our method outperforms the LP method in both the 10-fold cross-validation
set-up and when only 14 seeds are used.
We also compare our method to the SO-PMI method. Turney and Littman (2002)
propose two methods for predicting the semantic orientation of words. They use
Latent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) for
measuring the statistical association between any given word and a set of 14 seed
words. They describe this method as unsupervised because they only use 14 seeds
as paradigm words that define the semantic orientation rather than train the model
(Turney 2002).
</bodyText>
<tableCaption confidence="0.74198">
Table 3
Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,
and the random walks model for 10-fold cross-validation and 14 seeds.
</tableCaption>
<table confidence="0.999775">
– CV 14 seeds
SO-PMI (1 × 107) – 61.3
SO-PMI (2 × 109) – 76.1
SO-PMI (1 × 1011) – 82.8
Spin Model 91.5 81.9
Label Propagation 88.40 74.83
Random Walks 93.1 82.1
</table>
<page confidence="0.95242">
551
</page>
<figure confidence="0.9162242">
Computational Linguistics Volume 40, Number 3
The SO-PMI value can be calculated as follows:
SO-PMI(w) = log hitsw,pos x hitsneg
(5)
hitsw,neg x hitspos
</figure>
<bodyText confidence="0.996428805555556">
where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a
commercial search engine when the search query is the given word and the disjunction
of all positive seed words. hitspos is the number of hits when we search for the disjunction
of all positive seed words. hitsw,neg, and hitsneg are defined similarly.
After Turney (2002), we use our method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al. 1966) using only 14 seed words. The network
we used contains only WordNet relations. No glosses or co-occurrence statistics are
used. The results comparing the SO-PMI method with different data set sizes, the spin
model, and the proposed method using only 14 seeds is shown in Table 3. We observe
that the random walk method outperforms SO-PMI when SO-PMI uses data sets of
sizes 1 x 107 and 2 x 109 words. The performance of SO-PMI and the random walk
methods are comparable when SO-PMI uses a very large data set (1 x 1011 words). The
performance of the spin model approach is also comparable to the other two methods.
The advantages of the random walk method over SO-PMI is that it is faster and it does
not need a very large corpus. Another advantage is that the random walk method can
be used along with the labeled data from the General Inquirer lexicon (Stone et al. 1966)
to get much better performance. This is costly for the SO-PMI method because that will
require the submission of almost 4,000 queries to a commercial search engine.
We also compare our method with the bootstrapping method described in Hu and
Liu (2004), and the shortest path method described in Kamps et al. (2004). We build a
network using only WordNet synonyms and hypernyms. We restrict the test set to the
set of adjectives in the General Inquirer lexicon because our method is mainly interested
in classifying adjectives.
The performance of the spin model, the bootstrapping method, the shortest path
method, the LP method, the Mincut method, and the random walk method for only
adjectives is shown in Table 4. We notice from the table that the random walk method
outperforms the spin model, the bootstrapping method, the shortest path method,
the LP method, and the Mincut method for adjectives. The reported accuracy for the
shortest path method only considers the words it could assign a non-zero orientation
value. If we consider all words, its accuracy will drop to around 61%.
6.1.1 Varying Parameters. As we mentioned in Section 3.4, we use a parameter m to put
an upper bound on the length of random walks. In this section, we explore the impact
of this parameter on our method’s performance.
Figure 1 shows the accuracy of the random walk method as a function of the
maximum number of steps m as it varies from 5 to 50. We use a network built from
WordNet synonyms and hypernyms only. The number of samples k was set to 1, 000.
</bodyText>
<tableCaption confidence="0.877351666666667">
Table 4
Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk
model.
</tableCaption>
<table confidence="0.9737985">
Method Spin Model Bootstrap Shortest Path LP Mincut Random Walks
Accuracy 83.6 72.8 68.8 84.8 73.8 88.8
</table>
<page confidence="0.995885">
552
</page>
<note confidence="0.966413">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<figureCaption confidence="0.992113">
Figure 1
</figureCaption>
<bodyText confidence="0.9874335">
The effect of varying the maximum number of steps (m) on accuracy (k = 1,000).
We perform 10-fold cross-validation using the General Inquirer lexicon. We observe
that the maximum number of steps m has very little impact on performance until it
rises above 30. At that point, the performance drops by no more than 1%, and then it no
longer changes as m increases. An interesting observation is that the proposed method
performs quite well with a very small number of steps (around 10). We looked at the
data set to understand why increasing the number of steps beyond 30 negatively affects
performance. We found out that when the number of steps is very large compared with
the diameter of the graph, the random walk that starts at ambiguous words (which are
hard to classify) have the chance of moving until it hits a node in the opposite class.
That does not happen when the limit on the number of steps is smaller because those
walks are then terminated without hitting any labeled nodes and are hence ignored.
Next, we study the effect of the number of samples k on our method’s performance.
As explained in Section 3.4, k is the number of samples used by the Monte Carlo
algorithm to find an estimate for the hitting time. Figure 2 shows the accuracy of the
random walks method as a function of the number of samples k. We use the same
</bodyText>
<figureCaption confidence="0.841492">
Figure 2
</figureCaption>
<bodyText confidence="0.76394">
The effect of varying the number of samples (k) on accuracy.
</bodyText>
<page confidence="0.994909">
553
</page>
<note confidence="0.543693">
Computational Linguistics Volume 40, Number 3
</note>
<bodyText confidence="0.999635888888889">
settings as in the previous experiment. The only difference is that we fix m at 15 and
vary k from 10 to 20, 000 (note the logarithmic scale). We notice that the performance
is badly affected when the value of k is very small (less than 100). We also notice that
after 1, 000, varying k has very little, if any, effect on performance. This shows that the
Monte Carlo algorithm for computing the random walks hitting time performs quite
well with values of the number of samples as small as 1, 000.
The preceding experiments suggest that the parameter m has very little impact
on the performance. This suggests that the approach is fairly robust (i.e., it is quite
insensitive to different parameter settings).
</bodyText>
<subsubsectionHeader confidence="0.59215">
6.1.2 Other Experiments. We now measure the performance of the random walk method
</subsubsectionHeader>
<bodyText confidence="0.969835217391304">
when the system is allowed to abstain from classifying the words for which it has low
confidence. We regard the ratio between the hitting time to positive words and hitting
time to negative words as a confidence measure and evaluate the top words with the
highest confidence level at different values of threshold. Figure 3 shows the accuracy for
10-fold cross validation and for using only 14 seeds at different thresholds. We notice
that the accuracy improves by abstaining from classifying the difficult words. The figure
shows that the top 60% words are classified with accuracy greater than 99% for 10-fold
cross validation and 92% with 14 seed words. This may be compared with the work
described in Takamura, Inui, and Okumura (2005), where they achieve the 92% level
when they only consider the top 1,000 words (28%).
Figure 4 shows a learning curve displaying how the performance of both the pro-
posed method and the LP method is affected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy exceeds 90% when the training set size
rises above 20%. The accuracy steadily increases as the size of labeled data increases.
We also looked at the classification accuracy for different parts of speech in Figure 5.
We notice that, in the case of 10-fold cross-validation, the performance is consistent
across parts of speech. However, when we only use 14 seeds—all of which are ad-
jectives, similar to Turney and Littman (2003)—we notice that the performance on
adjectives is much better than other parts of speech. When we use 14 seeds but replace
some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the per-
formance for nouns and verbs improves considerably at the cost of a small drop in the
Figure 3
Accuracy for words with high confidence measure.
</bodyText>
<page confidence="0.998298">
554
</page>
<note confidence="0.963012">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<figureCaption confidence="0.976422">
Figure 4
</figureCaption>
<figure confidence="0.971095928571429">
The effect of varying the number of seeds on accuracy.
CV 14 Adj Seeds 14 Seeds
100
95
90
85
80
75
70
65
60
55
50
Adj Adv Noun Verb
</figure>
<figureCaption confidence="0.992858">
Figure 5
</figureCaption>
<bodyText confidence="0.910908533333333">
Accuracy for different parts of speech.
performance on adjectives. Finally, we tried adding edges to the network from glosses
and co-occurrence statistics but we did not get any statistically significant improvement.
Some of the words that were very weakly linked benefited from adding new types
of links and they were correctly predicted. Others were misled by the noise and were
incorrectly classified. We had a closer look at the results to find out what are the reasons
behind incorrect predictions. We found two main reasons. First, some words have more
than one sense, possibly with different semantic orientations. Disambiguating the sense
of words given their context before trying to predict their polarity should solve this
problem. The second reason is that some words have very few connections in the
thesaurus. A possible solution to this might be to identify those words and add more
links to them from glosses of co-occurrence statistics in the corpus.
6.1.3 General Purpose Three-Way Classification. The experiments described so far all use
the General Inquirer lexicon, which contains a well-established gold standard data set
of positive and negative words. However, in realistic applications, a general purpose
</bodyText>
<page confidence="0.994948">
555
</page>
<note confidence="0.544667">
Computational Linguistics Volume 40, Number 3
</note>
<tableCaption confidence="0.991159">
Table 5
</tableCaption>
<table confidence="0.9585685">
Accuracy for three classes on a general purpose list of 2,000 words.
Class Positive Negative Neutral Overall
Accuracy
68.0 82.1 80.6 77.9
</table>
<bodyText confidence="0.998234222222222">
list of words will frequently have neutral words that don’t express sentiment polarity.
To evaluate the effectiveness of the random walk method in distinguishing polarized
words from neutral words, we constructed a data set of 2, 000 words randomly picked
from a standard English dictionary3 and hand labeled them with three classes: posi-
tive, negative, and neutral. Among the 2, 000 words, 494 were labeled positive,
491 negative, and 1,015 neutral. The distribution among different parts of speech is
532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.
We used the semi-supervised setting with the General Inquirer lexicon polarized
word list as the training set. Because the 2, 000 test set has some portion of polarized
words overlapping with the training set, we excluded the words that appear in the test
set from the training set. We performed Algorithm 2 in Section 3.4 with parameters
γ = 0.8, m = 15, k = 1, 000. The overall accuracy as well as the precision for each class is
shown in Table 5. We can see that the accuracy of the positive class is much lower than
the negative class, due to the many positive words classified as neutral. This means
that the average confidence of negative words is higher than positive words. One factor
that could have caused this is the bias originating from the training set. Because there
are more negative seeds than positive ones, the constructed graph has an overall bias
towards the negative class.
</bodyText>
<subsectionHeader confidence="0.9976">
6.2 Foreign Words
</subsectionHeader>
<bodyText confidence="0.999837555555556">
In addition to the English data we described earlier, we constructed a labeled set of 300
Arabic and 300 Hindi words for evaluation. For every language, we asked two native
speakers to examine a large amount of text and identify a set of positive and negative
words. We also used an Arabic–English and a Hindi–English dictionary to generate
Foreign–English links.
We compare our results with two baselines. The first is the SO-PMI method de-
scribed in Turney and Littman (2003). We used the same seven positive and seven
negative seeds as Turney and Littman (2003).
The second baseline constructs a network of only foreign words as described earlier.
It uses mean hitting time to find the semantic association of any given word. We used
10-fold cross-validation for this experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use the hitting time as before to predict
semantic orientation. We used the English words from Stone et al. (1966) as seeds and
the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN.
Figure 6 compares the accuracy of the three methods for Arabic and Hindi. We
notice that the SO-PMI and the hitting time–based methods perform poorly on both
Arabic and Hindi. This is clearly evident when we consider that the accuracy of the two
systems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan
</bodyText>
<footnote confidence="0.9912885">
3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of the
words in a corpus.
</footnote>
<page confidence="0.992338">
556
</page>
<note confidence="0.870761">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<bodyText confidence="0.996038714285714">
and Radev 2010). This supports our hypothesis that state-of-the-art methods, designed
for English, perform poorly on foreign languages due to the limited amount of resources
in them. The figure also shows that the proposed method, which combines resources
from both English and foreign languages, performs significantly better. Finally, we
studied how much improvement is achieved by including links between foreign words
from global WordNets. We found out that it improves the performance by 2.5% and 4%
for Arabic and Hindi, respectively.
</bodyText>
<subsectionHeader confidence="0.999468">
6.3 OOV Words
</subsectionHeader>
<bodyText confidence="0.999917777777778">
We created a labeled set of 300 positive and negative OOV words. We asked a native
English speaker to examine a large number of threads posted on several on-line forums
and identify OOV words and label them with their polarities. Some examples of posi-
tive/negative OOV words are listed in Table 6.
The baseline we use for OOV words is the SO-PMI method with the same 14 seeds
as in Turney and Littman (2003). The calculation of SO-PMI is given in Equation (5).
We used the approach described in Section 5 to automatically label the words. We
used the words of the General Inquirer lexicon as labeled seeds. We set the maximum
number of steps m to 15 and the number of samples k to 1, 000. We experimented with
</bodyText>
<tableCaption confidence="0.912443">
Table 6
</tableCaption>
<table confidence="0.896149625">
Examples of positive and negative OOV words.
Positive Negative
Word Meaning Word Meaning
beautimous beautiful and fabulous disastrophy a catastrophy and a disaster
gr8 great banjaxed ruined
buffting attractive ijit idiot
100
SO-PMI HT - FR HT - FR+EN
</table>
<figure confidence="0.9316112">
Arabic Hindi
Figure 6
Accuracy of foreign word polarity identification.
90
80
70
60
50
40
30
20
10
0
557
Computational Linguistics Volume 40, Number 3
</figure>
<figureCaption confidence="0.999546">
Figure 7
</figureCaption>
<bodyText confidence="0.977951333333333">
Accuracy of different methods in predicting OOV words polarity.
the three variants we proposed for extracting the related words as described in Section 5.
We give the experimental set-up for each variant here:
</bodyText>
<listItem confidence="0.984160777777778">
1. Search the entire Web (WS): We used Yahoo search4 to execute the search
queries. For each OOV word, we retrieve the top 500 results and use them
to extract the related words.
2. Search the entire Web and limit the extraction of related words to the
proximity of the OOV word (WSP): We fix the proximity of a given
OOV word to 15 words before and 15 words after the OOV word (we
experimented with different ranges but no significant changes were
observed).
3. Limit the search to social content (SOC): We limit the search for OOV
</listItem>
<bodyText confidence="0.9232875625">
words to tweets posted on Twitter. We use the Twitter search API
to submit the search queries. For each OOV word, we retrieve
10,000 tweets. Each tweet is maximum of 140 characters long.
Figure 7 shows the results of the three methods compared with the baseline SO-PMI.
The results show that extracting related words from tweets gives the best accuracy. This
corroborated our intuition that using social content is more likely to provide sentiment-
related words. The baseline SO-PMI and WS obtain very similar accuracy. This agrees
with the comparable performance of the two methods in the earlier experiment on the
General Inquirer lexicon.
The three variant methods for obtaining related words have a tunable parameter
R, the number of related words extracted for each OOV word. We observe that R
has a non-negligible effect on the prediction accuracy. The results shown in Figure 8
correspond to R = 90. To better understand the impact of varying this parameter, we ran
the experiment that uses Twitter to extract related words several times using different
values for R. Figure 8 shows how the accuracy of polarity prediction changes as R
changes.
</bodyText>
<footnote confidence="0.862735">
4 http://www.yahoo.com.
</footnote>
<page confidence="0.98674">
558
</page>
<note confidence="0.51004">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<figure confidence="0.9985139">
Accuracy
70%
65%
60%
45%
40%
55%
50%
0 20 40 60 80 100 120 140 160 180
Number of related words
</figure>
<figureCaption confidence="0.989407">
Figure 8
</figureCaption>
<bodyText confidence="0.875844">
The effect of varying the number of extracted related words on accuracy.
</bodyText>
<sectionHeader confidence="0.925985" genericHeader="conclusions">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.999869869565217">
Predicting the semantic orientation of words is a very interesting task in natural lan-
guage processing and it has a wide variety of applications. We proposed a method for
automatically predicting the semantic orientation of words using random walks and
hitting time. The proposed method is based on the observation that a random walk
starting at a given word is more likely to hit another word with the same semantic
orientation before hitting a word with a different semantic orientation. The proposed
method can be used in a semi-supervised setting, where a training set of labeled words
is used, and in a weakly supervised setting, where only a handful of seeds is used to
define the two polarity classes. We predict semantic orientation with high accuracy.
The proposed method is fast, simple to implement, and does not need any corpus. We
also extended the proposed method to cover the problem of predicting the semantic
orientation of foreign words. All previous work on this task has almost exclusively
focused on English. Applying off-the-shelf methods developed for English to other
languages does not work well because of the limited amount of resources available
in foreign languages compared with English. We show that the proposed method can
predict the semantic orientation of foreign words with high accuracy and outperforms
state-of-the-art methods limited to using language specific resources. Finally, we further
extended the method to cover out-of-vocabulary words. These words do not exist in
WordNet and are not defined in the standard dictionaries of the language. We proposed
using a Web-based approach to add the OOV words to our words network based on
co-occurrence statistics, then use the same random walk model to predict the polar-
ity. We showed that this method can predict the polarity of OOV words with good
accuracy.
</bodyText>
<sectionHeader confidence="0.995993" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.974475666666667">
This research was funded by the Office of the
Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects
Activity (IARPA), through the U.S. Army
Research Lab. All statements of fact, opinion,
or conclusions contained herein are those of
the authors and should not be construed as
representing the official views or policies of
IARPA, the ODNI, or the U.S. Government.
</footnote>
<page confidence="0.988949">
559
</page>
<note confidence="0.69423">
Computational Linguistics Volume 40, Number 3
</note>
<sectionHeader confidence="0.954361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.980812512820513">
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas¸ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
wordnet-based approaches. In Proceedings
of Human Language Technologies: The 2009
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ’09, pages 19–27,
Stroudsburg, PA.
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction
from WordNet glosses. In EACL’06,
pages 209–216.
Banea, Carmen, Rada Mihalcea, and
Janyce Wiebe. 2008. A bootstrapping
method for building subjectivity lexicons
for languages with scarce resources.
In LREC’08, pages 2,764–2,767.
Black, W., S. Elkateb, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006. Introducing the
Arabic WordNet project. In Third
International WordNet Conference,
pages 295–299.
Blair-Goldensohn, Sasha, Tyler Neylon,
Kerry Hannan, George A. Reis, Ryan
McDonald, and Jeff Reynar. 2008. Building
a sentiment summarizer for local service
reviews. In NLP in the Information
Explosion Era.
Brody, Samuel and Noemie Elhadad. 2010.
An unsupervised aspect-sentiment model
for online reviews. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 804–812, Los Angeles, CA.
Elkateb, S., W. Black, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006a. Building a WordNet
for Arabic. In Fifth International Conference
on Language Resources and Evaluation,
pages 29–34.
Elkateb, S., W. Black, P. Vossen, D. Farwell,
H. Rodriguez, A. Pease, and M. Alkhalifa.
2006b. Arabic WordNet and the challenges
of Arabic. In Arabic NLP/MT Conference,
pages 15–24.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation
of terms through gloss classification.
In CIKM’05, pages 617–624.
Esuli, Andrea and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical
resource for opinion mining. In LREC’06,
pages 417–422.
Etzioni, Oren, Kobi Reiter, Stephen Soderl,
and Marcus Sammer. 2007. Lexical
translation with application to image
search on the Web. In Proceedings of
Machine Translation Summit XI.
Hassan, Ahmed and Dragomir R. Radev.
2010. Identifying text polarity using
random walks. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 395–403,
Uppsala.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In EACL’97,
pages 174–181.
Hatzivassiloglou, Vasileios and Janyce
Wiebe. 2000. Effects of adjective
orientation and gradability on sentence
subjectivity. In COLING, pages 299–305.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews.
In KDD’04, pages 168–177.
Jha, S., D. Narayan, P. Pande, and
P. Bhattacharyya. 2001. A WordNet for
Hindi. In International Workshop on Lexical
Resources in Natural Language Processing.
Jijkoun, Valentin and Katja Hofmann. 2009.
Generating a non-English subjectivity
lexicon: Relations that matter. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 398–405, Athens.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten De Rijke. 2004.
Using WordNet to measure semantic
orientations of adjectives. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation
(LREC 2004), pages 1115–1118.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In EMNLP’06, pages 355–363.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In COLING, pages 1,367–1,373.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good
time. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145–153,
Los Angeles, CA.
Lewis, D. D., Y. Yang, T. Rose, and F. Li.
2004. Rcv1: A new benchmark collection
for text categorization research. Journal of
Machine Learning Research, 5:361–397.
Mihalcea, Rada and Carmen Banea. 2007.
Learning multilingual subjective language
</reference>
<page confidence="0.977753">
560
</page>
<note confidence="0.852298">
Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation
</note>
<reference confidence="0.999846974576271">
via cross-lingual projections. In Proceedings
of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 976–983.
Miller, George A. 1995. Wordnet: A lexical
database for English. Communications of
ACM, 38(11):39–41.
Mohammad, Saif, Cody Dunne, and Bonnie
Dorr. 2009. Generating high-coverage
semantic orientation lexicons from overtly
marked words and a thesaurus. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing: Volume 2, EMNLP ’09,
pages 599–608, Stroudsburg, PA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In KDD’02, pages 341–349.
Narayan, Dipak, Debasri Chakrabarti,
Prabhakar Pande, and P. Bhattacharyya.
2002. An experience in building the Indo
WordNet—a WordNet for Hindi. In First
International Conference on Global WordNet.
Nasukawa, Tetsuya and Jeonghee Yi. 2003.
Sentiment analysis: Capturing favorability
using natural language processing.
In K-CAP ’03: Proceedings of the 2nd
International Conference on Knowledge
Capture, pages 70–77.
Norris, J. 1997. Markov Chains. Cambridge
University Press.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics, ACL ’04,
Stroudsburg, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In HLT-EMNLP’05,
pages 339–346.
Rao, Delip and Deepak Ravichandran.
2009. Semi-supervised polarity lexicon
induction. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 675–682, Athens.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for
subjective expressions. In EMNLP’03,
pages 105–112.
Stone, Philip, Dexter Dunphy, Marchall
Smith, and Daniel Ogilvie. 1966. The
General Inquirer: A Computer Approach
to Content Analysis. The MIT Press.
Su, Fangzhong and Katja Markert. 2009.
Subjectivity recognition on word
senses via semi-supervised mincuts.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ’09,
pages 1–9, Stroudsburg, PA.
Szummer, Martin and Tommi Jaakkola.
2002. Partially labeled classification with
Markov random walks. In NIPS’02,
pages 945–952.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using
spin model. In ACL’05, pages 133–140.
Tong, Richard M. 2001. An operational
system for detecting and tracking opinions
in on-line discussion. Workshop note,
SIGIR 2001 Workshop on Operational Text
Classification.
Turney, Peter and Michael Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems,
21:315–346.
Turney, Peter D. 2002. Thumbs up or thumbs
down?: Semantic orientation applied to
unsupervised classification of reviews.
In ACL’02, pages 417–424.
Velikovich, Leonid, Sasha Blair-Goldensohn,
Kerry Hannan, and Ryan McDonald. 2010.
The viability of Web-derived polarity
lexicons. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 777–785,
Los Angeles, CA.
Vossen, P. 1997. Eurowordnet: A multilingual
database for information retrieval. In
DELOS Workshop on Cross-Language
Information Retrieval, pages 5–7.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the Seventeenth National Conference on
Artificial Intelligence and the Twelfth
Conference on Innovative Applications of
Artificial Intelligence, pages 735–740.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second SIGdial Workshop on Discourse and
Dialogue, pages 1–10.
Wiebe, Janyce and Rada Mihalcea.
2006a. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,065–1,072, Sydney.
Wiebe, Janyce and Rada Mihalcea. 2006b.
Word sense and subjectivity. In Proceedings
</reference>
<page confidence="0.935516">
561
</page>
<reference confidence="0.971846047619048">
Computational Linguistics Volume 40, Number 3
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, ACL-44,
pages 1,065–1,072, Stroudsburg, PA.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165–210.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In EMNLP’03, pages 129–136.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In ICML’03,
pages 912–919.
</reference>
<page confidence="0.997294">
562
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622193">
<title confidence="0.983117">A Random Walk–Based Model for Identifying Semantic Orientation</title>
<affiliation confidence="0.986589">Microsoft Research University of Michigan University of Michigan University of Michigan</affiliation>
<abstract confidence="0.990688666666667">Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification, text filtering, product review analysis, survey response analysis, and on-line discussion mining. We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph, and produces a polarity estimate for any given word. The model can accurately and quickly assign a polarity sign and magnitude to any word. It can be used both in a semi-supervised setting where a training set of labeled words is used, and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes. The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon. We also show how our method can be used for three-way classification which identifies neutral words in addition to positive and negative words. Our experiments show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting and is comparable to the best reported values in the weakly supervised setting. In addition, the proposed method is faster and does not need a large corpus. We also present extensions of our methods for identifying the polarity of foreign words and out-of-vocabulary words. Research, Redmond, WA, USA. E-mail: This research was performed while at the University of Michigan.</abstract>
<address confidence="0.923023">of Electrical Engineering &amp; Computer Science, University of Michigan, Ann Arbor, MI, USA. of Electrical Engineering &amp; Computer Science, University of Michigan, Ann Arbor, MI, USA.</address>
<note confidence="0.972916428571429">of Electrical Engineering &amp; Computer Science and School of Information, University of Ann Arbor, MI, USA. E-mail: Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication: 14 July 2013. doi:10.1162/COLI a 00192 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<location>Stroudsburg, PA.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet for fuzzy sentiment: Sentiment tag extraction from WordNet glosses.</title>
<date>2006</date>
<booktitle>In EACL’06,</booktitle>
<pages>209--216</pages>
<contexts>
<context position="10701" citStr="Andreevskaia and Bergler (2006)" startWordPosition="1693" endWordPosition="1696">o build lexicons of polarized words. Esuli and Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, whereas others may be interpreted differently. Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that overtly marked words such as dishonest, unhappy, and impure tend to have negative semantic orientations whereas their unmarked counterparts (honest, happy, and pure) tend to have positive semantic orientation. They use a set of 11 antonym-generating affix patterns to gener</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Andreevskaia, Alina and Sabine Bergler. 2006. Mining WordNet for fuzzy sentiment: Sentiment tag extraction from WordNet glosses. In EACL’06, pages 209–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>A bootstrapping method for building subjectivity lexicons for languages with scarce resources.</title>
<date>2008</date>
<booktitle>In LREC’08,</booktitle>
<pages>2--764</pages>
<marker>Banea, Mihalcea, Wiebe, 2008</marker>
<rawString>Banea, Carmen, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexicons for languages with scarce resources. In LREC’08, pages 2,764–2,767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Black</author>
<author>S Elkateb</author>
<author>H Rodriguez</author>
<author>M Alkhalifa</author>
<author>P Vossen</author>
<author>A Pease</author>
<author>C Fellbaum</author>
</authors>
<title>Introducing the Arabic WordNet project.</title>
<date>2006</date>
<booktitle>In Third International WordNet Conference,</booktitle>
<pages>295--299</pages>
<contexts>
<context position="27560" citStr="Black et al. 2006" startWordPosition="4506" endWordPosition="4509">ne et al. 1966) has thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on cr</context>
<context position="29874" citStr="Black et al. 2006" startWordPosition="4875" endWordPosition="4878">k We build a network G(V, E) where V = Ven U Vfr is the union of the sets of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation gra</context>
</contexts>
<marker>Black, Elkateb, Rodriguez, Alkhalifa, Vossen, Pease, Fellbaum, 2006</marker>
<rawString>Black, W., S. Elkateb, H. Rodriguez, M. Alkhalifa, P. Vossen, A. Pease, and C. Fellbaum. 2006. Introducing the Arabic WordNet project. In Third International WordNet Conference, pages 295–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Tyler Neylon</author>
<author>Kerry Hannan</author>
<author>George A Reis</author>
<author>Ryan McDonald</author>
<author>Jeff Reynar</author>
</authors>
<title>Building a sentiment summarizer for local service reviews.</title>
<date>2008</date>
<booktitle>In NLP in the Information Explosion Era.</booktitle>
<contexts>
<context position="13007" citStr="Blair-Goldensohn et al. (2008)" startWordPosition="2040" endWordPosition="2043">h node representing a word that can be either positive or negative, and each edge representing some semantic relatedness that can be constructed using resources like WordNet or other thesaurus. They evaluate two semi-supervised methods: Mincut (including its variant, Randomized Mincut) and label propagation. The general idea of label propagation is defining a probability distribution over the positive and negative classes for each node in the graph. A Markov random walk is performed on the graph to recover this distribution for the unlabeled nodes. Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a similar label propagation method on a lexical graph built from WordNet, where a small set of words with known polarities are used as seeds. Brody and Elhadad (2010) use label propagation over a graph constructed of adjectives only. Velikovich et al. (2010) compare label propagation with a Web-based method and conclude that label propagation is not suitable when the whole Web is used as a background corpus, because the constructed graph is very noisy and contains many dense subgraphs, unlike the lexical graph constructed from WordNet. 542 Hassan et al. A Random Walk–Based Model for Ident</context>
</contexts>
<marker>Blair-Goldensohn, Neylon, Hannan, Reis, McDonald, Reynar, 2008</marker>
<rawString>Blair-Goldensohn, Sasha, Tyler Neylon, Kerry Hannan, George A. Reis, Ryan McDonald, and Jeff Reynar. 2008. Building a sentiment summarizer for local service reviews. In NLP in the Information Explosion Era.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>804--812</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="13178" citStr="Brody and Elhadad (2010)" startWordPosition="2070" endWordPosition="2073">other thesaurus. They evaluate two semi-supervised methods: Mincut (including its variant, Randomized Mincut) and label propagation. The general idea of label propagation is defining a probability distribution over the positive and negative classes for each node in the graph. A Markov random walk is performed on the graph to recover this distribution for the unlabeled nodes. Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a similar label propagation method on a lexical graph built from WordNet, where a small set of words with known polarities are used as seeds. Brody and Elhadad (2010) use label propagation over a graph constructed of adjectives only. Velikovich et al. (2010) compare label propagation with a Web-based method and conclude that label propagation is not suitable when the whole Web is used as a background corpus, because the constructed graph is very noisy and contains many dense subgraphs, unlike the lexical graph constructed from WordNet. 542 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Random walk–based methods have been studied in the context of many other NLP tasks. For example, Kok and Brockett (2010) construct a graph from</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Brody, Samuel and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 804–812, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elkateb</author>
<author>W Black</author>
<author>H Rodriguez</author>
<author>M Alkhalifa</author>
<author>P Vossen</author>
<author>A Pease</author>
<author>C Fellbaum</author>
</authors>
<title>Building a WordNet for Arabic. In</title>
<date>2006</date>
<booktitle>Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>29--34</pages>
<contexts>
<context position="27581" citStr="Elkateb et al. 2006" startWordPosition="4510" endWordPosition="4513"> thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on creating 1 http://nlpww</context>
<context position="29895" citStr="Elkateb et al. 2006" startWordPosition="4879" endWordPosition="4882">k G(V, E) where V = Ven U Vfr is the union of the sets of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 20</context>
</contexts>
<marker>Elkateb, Black, Rodriguez, Alkhalifa, Vossen, Pease, Fellbaum, 2006</marker>
<rawString>Elkateb, S., W. Black, H. Rodriguez, M. Alkhalifa, P. Vossen, A. Pease, and C. Fellbaum. 2006a. Building a WordNet for Arabic. In Fifth International Conference on Language Resources and Evaluation, pages 29–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elkateb</author>
<author>W Black</author>
<author>P Vossen</author>
<author>D Farwell</author>
<author>H Rodriguez</author>
<author>A Pease</author>
<author>M Alkhalifa</author>
</authors>
<date>2006</date>
<booktitle>Arabic WordNet and the challenges of Arabic. In Arabic NLP/MT Conference,</booktitle>
<pages>15--24</pages>
<contexts>
<context position="27581" citStr="Elkateb et al. 2006" startWordPosition="4510" endWordPosition="4513"> thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on creating 1 http://nlpww</context>
<context position="29895" citStr="Elkateb et al. 2006" startWordPosition="4879" endWordPosition="4882">k G(V, E) where V = Ven U Vfr is the union of the sets of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 20</context>
</contexts>
<marker>Elkateb, Black, Vossen, Farwell, Rodriguez, Pease, Alkhalifa, 2006</marker>
<rawString>Elkateb, S., W. Black, P. Vossen, D. Farwell, H. Rodriguez, A. Pease, and M. Alkhalifa. 2006b. Arabic WordNet and the challenges of Arabic. In Arabic NLP/MT Conference, pages 15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss classification.</title>
<date>2005</date>
<booktitle>In CIKM’05,</booktitle>
<pages>617--624</pages>
<contexts>
<context position="10133" citStr="Esuli and Sebastiani (2005" startWordPosition="1599" endWordPosition="1602">whose polarity is unknown, they search WordNet and a list of seed labeled words to predict its polarity. They check if any of the synonyms of the given word has known polarity. If so, they label it with the label of its synonym. Otherwise, they check if any of the antonyms of the given word has known polarity. If so, they label it with the opposite label of the antonym. They continue in a bootstrapping manner until they label all possible words. 541 Computational Linguistics Volume 40, Number 3 2.2 Building Sentiment Lexicons A number of other methods try to build lexicons of polarized words. Esuli and Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antony</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Esuli, Andrea and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classification. In CIKM’05, pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>LREC’06,</booktitle>
<pages>417--422</pages>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, Andrea and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In LREC’06, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Kobi Reiter</author>
<author>Stephen Soderl</author>
<author>Marcus Sammer</author>
</authors>
<title>Lexical translation with application to image search on the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of Machine Translation</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="30498" citStr="Etzioni et al. 2007" startWordPosition="4981" endWordPosition="4984">kateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 2007) may be a resolution. 4.2 Foreign Word Semantic Orientation Prediction We use the multilingual network described previously to predict the semantic orientation of words based on the mean hitting time to two sets of positive and negative seeds. Given two lists of seed English words with known polarity, we define two sets of nodes S+ and S− representing those seeds. For any given word w, we calculate the mean hitting time between w and the two seed sets h(w|S+) and h(w|S−). If h(w|S+) is greater than h(w|S−), the word is classified as negative; otherwise it is classified as positive. We used the</context>
</contexts>
<marker>Etzioni, Reiter, Soderl, Sammer, 2007</marker>
<rawString>Etzioni, Oren, Kobi Reiter, Stephen Soderl, and Marcus Sammer. 2007. Lexical translation with application to image search on the Web. In Proceedings of Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying text polarity using random walks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>395--403</pages>
<location>Uppsala.</location>
<marker>Hassan, Radev, 2010</marker>
<rawString>Hassan, Ahmed and Dragomir R. Radev. 2010. Identifying text polarity using random walks. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395–403, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In EACL’97,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="5804" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="880" endWordPosition="883">ages that it is faster and does not need a large training corpus. The rest of the article is structured as follows. In Section 2, we review related work on word polarity and subjectivity classification and note applications of the random walk and hitting times framework. Section 3 presents our method for identifying word polarity. We describe how the proposed method can be extended to cover foreign languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes our experimental set-up. We present our conclusions in Section 7. 2. Related Work 2.1 Identifying Word Polarity Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word polarity of adjectives. They extract all conjunctions of adjectives from a given corpus 540 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation and then they classify each conjunctive expression as either the same orientation such as “simple and well-received” or different orientation such as “simplistic but wellreceived.” The result is a graph that they cluster into two subsets of adjectives. They classify the cluster with the higher average frequency as positive. They created and labeled their own data set for experiments. </context>
<context position="20716" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="3276" endWordPosition="3279">present word/POS pairs rather than words because the part of speech tags are helpful in disambiguating the different senses for a given word. For example, the word “fine” has two different meanings, with two opposite polarities when used as an adjective and as a noun. Several other methods can be used to link words. For example, we can use other WordNet relations: hypernyms, similar to, and so forth. Another source of links between words is co-occurrence statistics from a corpus. Following the method presented 544 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation in Hatzivassiloglou and McKeown (1997), we can connect words if they appear together in a conjunction in the corpus. This method is only applicable to adjectives. If two adjectives are connected by “and,” it is highly likely that they have the same semantic orientation. In all our experiments, we restricted the network to only WordNet relations. We study the effect of using co-occurrence statistics to connect words later at the end of our experiments. If more than one relation exists between any two words, the strength of the corresponding edge is adjusted accordingly. 3.2 Random Walk Model Imagine a random surfer walking along th</context>
<context position="30063" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="4906" endWordPosition="4909"> connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 2007) may be a resolution. 4.2 Foreign Word Semantic Orientation Prediction We use the multilingual network described previously to predict the semantic orientation of wo</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Hatzivassiloglou, Vasileios and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In EACL’97, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>299--305</pages>
<contexts>
<context position="15730" citStr="Hatzivassiloglou and Wiebe 2000" startWordPosition="2463" endWordPosition="2466">tences will be in the same subjectivity class. All sentences to be classified are represented as unlabeled nodes and the only two labeled nodes represent the subjective and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sen</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Hatzivassiloglou, Vasileios and Janyce Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In COLING, pages 299–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In KDD’04,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="9378" citStr="Hu and Liu (2004)" startWordPosition="1469" endWordPosition="1472">bad” to determine word polarity. They report that using shortest paths could be very noisy. For example, “good” and “bad” themselves are closely related in WordNet with a 5-long sequence “good, sound, heavy, big, bad.” A given word w may be more connected to one set of words (e.g., positive words); yet have a shorter path connecting it to one word in the other set. Restricting seed words to only two words affects their accuracy. Adding more seed words could help but it will make their method extremely costly from the computation point of view. They evaluate their method using only adjectives. Hu and Liu (2004) propose another method that uses WordNet. They use WordNet synonyms and antonyms to predict the polarity of words. For any word whose polarity is unknown, they search WordNet and a list of seed labeled words to predict its polarity. They check if any of the synonyms of the given word has known polarity. If so, they label it with the label of its synonym. Otherwise, they check if any of the antonyms of the given word has known polarity. If so, they label it with the opposite label of the antonym. They continue in a bootstrapping manner until they label all possible words. 541 Computational Lin</context>
<context position="37824" citStr="Hu and Liu (2004)" startWordPosition="6220" endWordPosition="6223">stically significant at the 0.05 level. We perform experiments varying the parameters and the network. We also look at the performance of the proposed method for different parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method described in Kamps et al. (2004), a re-implementation of the label propagation and Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping method described in Hu and Liu (2004). 2 http://www.twitter.com. 550 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Table 2 Examples of positive and negative words. Positive Negative able adjective abandon verb acceptable adjective abuse verb admire verb burglar noun amazing adjective chaos noun careful adjective contagious adjective ease noun corruption noun guide verb lie verb inspire verb reluctant adjective truthful adjective wrong adjective 6.1 Comparison with Other Methods This method could be used in a semi-supervised setting where a set of labeled words are used and the system learns from the</context>
<context position="41874" citStr="Hu and Liu (2004)" startWordPosition="6882" endWordPosition="6885">(1 x 1011 words). The performance of the spin model approach is also comparable to the other two methods. The advantages of the random walk method over SO-PMI is that it is faster and it does not need a very large corpus. Another advantage is that the random walk method can be used along with the labeled data from the General Inquirer lexicon (Stone et al. 1966) to get much better performance. This is costly for the SO-PMI method because that will require the submission of almost 4,000 queries to a commercial search engine. We also compare our method with the bootstrapping method described in Hu and Liu (2004), and the shortest path method described in Kamps et al. (2004). We build a network using only WordNet synonyms and hypernyms. We restrict the test set to the set of adjectives in the General Inquirer lexicon because our method is mainly interested in classifying adjectives. The performance of the spin model, the bootstrapping method, the shortest path method, the LP method, the Mincut method, and the random walk method for only adjectives is shown in Table 4. We notice from the table that the random walk method outperforms the spin model, the bootstrapping method, the shortest path method, th</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, Minqing and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD’04, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jha</author>
<author>D Narayan</author>
<author>P Pande</author>
<author>P Bhattacharyya</author>
</authors>
<title>A WordNet for Hindi.</title>
<date>2001</date>
<booktitle>In International Workshop on Lexical Resources in Natural Language Processing.</booktitle>
<contexts>
<context position="27652" citStr="Jha et al. 2001" startWordPosition="4522" endWordPosition="4525">literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on creating 1 http://nlpwww.nict.go.jp/wn-ja/index.en.html. 547 Computational Linguistics Volume </context>
<context position="29944" citStr="Jha et al. 2001" startWordPosition="4888" endWordPosition="4891">s of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 2007) may be a resolution. 4.2 Foreign Word Semanti</context>
</contexts>
<marker>Jha, Narayan, Pande, Bhattacharyya, 2001</marker>
<rawString>Jha, S., D. Narayan, P. Pande, and P. Bhattacharyya. 2001. A WordNet for Hindi. In International Workshop on Lexical Resources in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Katja Hofmann</author>
</authors>
<title>Generating a non-English subjectivity lexicon: Relations that matter.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>398--405</pages>
<location>Athens.</location>
<contexts>
<context position="17170" citStr="Jijkoun and Hofmann (2009)" startWordPosition="2679" endWordPosition="2682">es in WordNet as subjective or objective, utilizing the MPQA corpus. They show that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivity ambiguous words. Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information. Their method requires less training data other than the sense definitions and relational structure of WordNet. 2.5 Word Polarity Classification for Foreign Languages Word sentiment and subjectivity has also been studied for languages other than English. Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity 543 Computational Linguistics Volume 40, Number 3 lexicon based on an English lexicon, an on-line translation service, and Wordnet. Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a parallel corpus to generate subjectivity analysis resources for foreign languages. Rao and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi WordNet and French using a French thesaurus. 3. Approach We use a Markov random walk model to identify the polarity of words. Assume that we have a network</context>
</contexts>
<marker>Jijkoun, Hofmann, 2009</marker>
<rawString>Jijkoun, Valentin and Katja Hofmann. 2009. Generating a non-English subjectivity lexicon: Relations that matter. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 398–405, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten De Rijke</author>
</authors>
<title>Using WordNet to measure semantic orientations of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1115--1118</pages>
<marker>Kamps, Marx, Mokken, De Rijke, 2004</marker>
<rawString>Kamps, Jaap, Maarten Marx, Robert J. Mokken, and Maarten De Rijke. 2004. Using WordNet to measure semantic orientations of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In EMNLP’06,</booktitle>
<pages>355--363</pages>
<contexts>
<context position="11942" citStr="Kanayama and Nasukawa (2006)" startWordPosition="1878" endWordPosition="1881">arked words and their counterparts from the Macquarie Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the sentiment lexicon using a Roget-like thesaurus. Their method does not require seed sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of the marking theory is language-dependent and cannot be applied from one language to another. Contrasting the dictionary based approaches that rely on resources such as WordNet, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic features and context coherency (i.e., the tendency for same polarities to appear successively) to detect polar clauses. 2.3 Random Walk–Based Methods Closest to our work in its methodology is probably the line of research on semisupervised graphical methods for sentiment classification. Rao and Ravichandran (2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled and labeled nodes, each node representing a word that can be either positive or negative, and each edge representing some semantic relatedness that can be constructed using resources like</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Kanayama, Hiroshi and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In EMNLP’06, pages 355–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<pages>1--367</pages>
<contexts>
<context position="10369" citStr="Kim and Hovy (2004)" startWordPosition="1639" endWordPosition="1642">y check if any of the antonyms of the given word has known polarity. If so, they label it with the opposite label of the antonym. They continue in a bootstrapping manner until they label all possible words. 541 Computational Linguistics Volume 40, Number 3 2.2 Building Sentiment Lexicons A number of other methods try to build lexicons of polarized words. Esuli and Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, whereas others may be interpreted differently. Mohammad, Dunne, and</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim, Soo-Min and Eduard Hovy. 2004. Determining the sentiment of opinions. In COLING, pages 1,367–1,373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Chris Brockett</author>
</authors>
<title>Hitting the right paraphrases in good time.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>145--153</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="13755" citStr="Kok and Brockett (2010)" startWordPosition="2160" endWordPosition="2163">es are used as seeds. Brody and Elhadad (2010) use label propagation over a graph constructed of adjectives only. Velikovich et al. (2010) compare label propagation with a Web-based method and conclude that label propagation is not suitable when the whole Web is used as a background corpus, because the constructed graph is very noisy and contains many dense subgraphs, unlike the lexical graph constructed from WordNet. 542 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Random walk–based methods have been studied in the context of many other NLP tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel corpora, where each node represents a phrase and two nodes are connected by an edge if they are aligned in a phrase table. Then they compute hitting time of random walks to learn paraphrases. Our work is different from previous random walk methods in that it uses the mean hitting time as the criterion for assigning polarity labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task i</context>
</contexts>
<marker>Kok, Brockett, 2010</marker>
<rawString>Kok, Stanley and Chris Brockett. 2010. Hitting the right paraphrases in good time. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 145–153, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="36940" citStr="Lewis et al. 2004" startWordPosition="6085" endWordPosition="6088"> the same random walk model described earlier. 6. Experiments We performed experiments on the gold-standard data set for positive/negative words from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4,206 words, 1, 915 of which are positive and 2,291 of which are negative. Some of the ambiguous words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005). Some examples of positive/negative words are listed in Table 2. We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to generate co-occurrence statistics in the experiments that used them. We used 10-fold cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical significance was tested using a two-tailed paired t-test. All reported results are statistically significant at the 0.05 level. We perform experiments varying the parameters and the network. We also look at the performance of the proposed method for different parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the </context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, D. D., Y. Yang, T. Rose, and F. Li. 2004. Rcv1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>976--983</pages>
<contexts>
<context position="17386" citStr="Mihalcea and Banea (2007)" startWordPosition="2710" endWordPosition="2713">ert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information. Their method requires less training data other than the sense definitions and relational structure of WordNet. 2.5 Word Polarity Classification for Foreign Languages Word sentiment and subjectivity has also been studied for languages other than English. Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity 543 Computational Linguistics Volume 40, Number 3 lexicon based on an English lexicon, an on-line translation service, and Wordnet. Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a parallel corpus to generate subjectivity analysis resources for foreign languages. Rao and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi WordNet and French using a French thesaurus. 3. Approach We use a Markov random walk model to identify the polarity of words. Assume that we have a network of words, some of which are labeled as either positive or negative. In this network, two words are connected if they are related. Different sources of information are used to decide whether two words are related. Fo</context>
</contexts>
<marker>Mihalcea, Banea, 2007</marker>
<rawString>Mihalcea, Rada and Carmen Banea. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="8670" citStr="Miller 1995" startWordPosition="1349" endWordPosition="1350">rds tend to have similar polarity. They pose the problem as an optimization problem and use the mean field method to find the best solution. The analogy with electrons leads them to assume that each word should be either positive or negative. This assumption is not accurate because most of the words in the language do not have any semantic orientation. They report that their method could get misled by noise in the gloss definition and their computations sometimes get trapped in a local optimum because of its greedy optimization flavor. Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms and then use the shortest paths between any given word and the words “good” and “bad” to determine word polarity. They report that using shortest paths could be very noisy. For example, “good” and “bad” themselves are closely related in WordNet with a 5-long sequence “good, sound, heavy, big, bad.” A given word w may be more connected to one set of words (e.g., positive words); yet have a shorter path connecting it to one word in the other set. Restricting seed words to only two words affects their accuracy. Adding more seed words could help but it will make their method extremely co</context>
<context position="19432" citStr="Miller 1995" startWordPosition="3068" endWordPosition="3069">ct that the positive hitting time and negative hitting time to not have a significant difference. We describe how we construct a word relatedness graph in Section 3.1. The random walk model is described in Section 3.2. Hitting time is defined in Section 3.3. Finally, an algorithm for computing a sign and magnitude for the polarity of any given word is described in Section 3.4. 3.1 Network Construction We construct a network where two nodes are linked if they are semantically related. Several sources of information are used as indicators of the relatedness of words. One such source is WordNet (Miller 1995). WordNet is a large lexical database of English. Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (Miller 1995). Synsets are interlinked by means of conceptual-semantic and lexical relations. The simplest approach is to connect words that occur in the same WordNet synset. We can collect all words in WordNet, and add links between any two words that occur in the same synset. The resulting graph is a graph G(W,E) where W is a set of word/part-of-speech (POS) pairs for all the words in WordNet. E is the set of edges c</context>
<context position="27211" citStr="Miller 1995" startWordPosition="4449" endWordPosition="4450"> identifying the semantic orientation of words. This work has almost exclusively dealt with English and uses several language-dependent resources. When we try to apply these methods to other languages, we run into the problem of the lack of resources in other languages when compared with English. For example, the General Inquirer lexicon (Stone et al. 1966) has thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European</context>
<context position="36813" citStr="Miller 1995" startWordPosition="6064" endWordPosition="6065">lly, we create a link between each OOV word and each of its related words. To predict the polarity of an OOV word, we use the same random walk model described earlier. 6. Experiments We performed experiments on the gold-standard data set for positive/negative words from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4,206 words, 1, 915 of which are positive and 2,291 of which are negative. Some of the ambiguous words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005). Some examples of positive/negative words are listed in Table 2. We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to generate co-occurrence statistics in the experiments that used them. We used 10-fold cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical significance was tested using a two-tailed paired t-test. All reported results are statistically significant at the 0.05 level. We perform experiments varying the parameters and the network. We also look at the performance of the proposed method for different parts of speech, and for different</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, George A. 1995. Wordnet: A lexical database for English. Communications of ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Cody Dunne</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09,</booktitle>
<pages>599--608</pages>
<location>Stroudsburg, PA.</location>
<marker>Mohammad, Dunne, Dorr, 2009</marker>
<rawString>Mohammad, Saif, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09, pages 599–608, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Morinaga</author>
<author>Kenji Yamanishi</author>
<author>Kenji Tateishi</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Mining product reputations on the Web. In</title>
<date>2002</date>
<booktitle>KDD’02,</booktitle>
<pages>341--349</pages>
<contexts>
<context position="2691" citStr="Morinaga et al. 2002" startWordPosition="396" endWordPosition="399">t Department of Electrical Engineering &amp; Computer Science and School of Information, University of Michigan, Ann Arbor, MI, USA. E-mail: radev®umich.edu. Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication: 14 July 2013. doi:10.1162/COLI a 00192 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 1. Introduction Identifying emotions and attitudes from unstructured text has a variety of possible applications. For example, there has been a large body of work for mining product reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002) have shown how product reputation mining helps with marketing and customer relation management. The Google products catalog and many on-line shopping sites like Amazon.com provide customers not only with comprehensive information and reviews about a product, but also with faceted sentiment summaries. Such systems are all supported by a sentiment lexicon, some even in multiple languages. Another interesting application is mining on-line discussions. An enormous number of discussion groups exist on the Web. Millions of users post content to these groups cov</context>
</contexts>
<marker>Morinaga, Yamanishi, Tateishi, Fukushima, 2002</marker>
<rawString>Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the Web. In KDD’02, pages 341–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipak Narayan</author>
<author>Debasri Chakrabarti</author>
<author>Prabhakar Pande</author>
<author>P Bhattacharyya</author>
</authors>
<title>An experience in building the Indo WordNet—a WordNet for Hindi.</title>
<date>2002</date>
<booktitle>In First International Conference on Global WordNet.</booktitle>
<contexts>
<context position="27674" citStr="Narayan et al. 2002" startWordPosition="4526" endWordPosition="4529">ed it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on creating 1 http://nlpwww.nict.go.jp/wn-ja/index.en.html. 547 Computational Linguistics Volume 40, Number 3 a multili</context>
<context position="29966" citStr="Narayan et al. 2002" startWordPosition="4892" endWordPosition="4895">Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 2007) may be a resolution. 4.2 Foreign Word Semantic Orientation Predicti</context>
</contexts>
<marker>Narayan, Chakrabarti, Pande, Bhattacharyya, 2002</marker>
<rawString>Narayan, Dipak, Debasri Chakrabarti, Prabhakar Pande, and P. Bhattacharyya. 2002. An experience in building the Indo WordNet—a WordNet for Hindi. In First International Conference on Global WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: Capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In K-CAP ’03: Proceedings of the 2nd International Conference on Knowledge Capture,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="15862" citStr="Nasukawa and Yi 2003" startWordPosition="2485" endWordPosition="2488"> represent the subjective and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on </context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Nasukawa, Tetsuya and Jeonghee Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In K-CAP ’03: Proceedings of the 2nd International Conference on Knowledge Capture, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Norris</author>
</authors>
<title>Markov Chains.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="23011" citStr="Norris 1997" startWordPosition="3673" endWordPosition="3674"> the edges out of node i, so: Pt+1|t(j|i) = Wij/ � Wik (1) k where k represents all nodes in the neighborhood of i. Pt+1|t(j|i) denotes the transition probability from node i at step t to node j at time step t + 1. We note that the matrix of weights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t(j|i) is not necessarily symmetric because of the node outdegree normalization. 3.3 First-Passage Time The mean first-passage (hitting) time h(i|k) is defined as the average number of steps a random walker, starting in state i =� k, will take to enter state k for the first time (Norris 1997). Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Consider a subset of vertices S C V. Consider a random walk on G starting at node i E� S. Let Nt denote the position of the random surfer at time t. Let h(i|S) be the average number of steps a random walker, starting in state i E� S, will take to enter a state k E S for the first time. Let TS be the first-passage for any vertex in S. P(TS = t|N0 = i) = E pij x P(TS = t − 1|N0 = j) (2) jEV 545 Computational Linguistics Volume 40, Number 3 h(i|S) is the expectation of TS. Hence: h(i|S) = E(TS|N0 = i) t x P(TS = t|N0 = i) </context>
</contexts>
<marker>Norris, 1997</marker>
<rawString>Norris, J. 1997. Markov Chains. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04,</booktitle>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="14743" citStr="Pang and Lee (2004)" startWordPosition="2317" endWordPosition="2320">labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task in subjectivity analysis is to identify text that presents opinion as opposed to objective text that present factual information (Wiebe 2000). Text could be either words, phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of subjectivity analysis such as classifying e-mails and mining reviews. For example, to analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from individual sentences as nodes to determine whether a sentence is subjective or objective. Each node (sentence) has an individual subjectivity score obtained from a first-pass classifier using sentence features and linguistic knowledge. Edges are weighted by a similarity metric of how likely it is that the two sentences will be in the same subjectivity class. All sentences to be classified are represented as unlabeled nodes and the only two labeled nodes represent the subjective and objective classes. A Mincut algorithm is then performed on the construc</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP’05,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="15942" citStr="Popescu and Etzioni 2005" startWordPosition="2497" endWordPosition="2500"> performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word se</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Popescu, Ana-Maria and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In HLT-EMNLP’05, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semi-supervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>675--682</pages>
<location>Athens.</location>
<contexts>
<context position="12272" citStr="Rao and Ravichandran (2009)" startWordPosition="1928" endWordPosition="1931">nguage-dependent and cannot be applied from one language to another. Contrasting the dictionary based approaches that rely on resources such as WordNet, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic features and context coherency (i.e., the tendency for same polarities to appear successively) to detect polar clauses. 2.3 Random Walk–Based Methods Closest to our work in its methodology is probably the line of research on semisupervised graphical methods for sentiment classification. Rao and Ravichandran (2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled and labeled nodes, each node representing a word that can be either positive or negative, and each edge representing some semantic relatedness that can be constructed using resources like WordNet or other thesaurus. They evaluate two semi-supervised methods: Mincut (including its variant, Randomized Mincut) and label propagation. The general idea of label propagation is defining a probability distribution over the positive and negative classes for each node in the graph. A Markov random walk is performed on the </context>
<context position="17557" citStr="Rao and Ravichandran (2009)" startWordPosition="2734" endWordPosition="2737"> other than the sense definitions and relational structure of WordNet. 2.5 Word Polarity Classification for Foreign Languages Word sentiment and subjectivity has also been studied for languages other than English. Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity 543 Computational Linguistics Volume 40, Number 3 lexicon based on an English lexicon, an on-line translation service, and Wordnet. Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a parallel corpus to generate subjectivity analysis resources for foreign languages. Rao and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi WordNet and French using a French thesaurus. 3. Approach We use a Markov random walk model to identify the polarity of words. Assume that we have a network of words, some of which are labeled as either positive or negative. In this network, two words are connected if they are related. Different sources of information are used to decide whether two words are related. For example, the synonyms of a word are all semantically related to it. The intuition behind connecting semantically related words is that those words tend to have similar p</context>
<context position="37763" citStr="Rao and Ravichandran (2009)" startWordPosition="6210" endWordPosition="6213">tested using a two-tailed paired t-test. All reported results are statistically significant at the 0.05 level. We perform experiments varying the parameters and the network. We also look at the performance of the proposed method for different parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method described in Kamps et al. (2004), a re-implementation of the label propagation and Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping method described in Hu and Liu (2004). 2 http://www.twitter.com. 550 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Table 2 Examples of positive and negative words. Positive Negative able adjective abandon verb acceptable adjective abuse verb admire verb burglar noun amazing adjective chaos noun careful adjective contagious adjective ease noun corruption noun guide verb lie verb inspire verb reluctant adjective truthful adjective wrong adjective 6.1 Comparison with Other Methods This method could be used in a semi-supervised setting where a</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Rao, Delip and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 675–682, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions. In</title>
<date>2003</date>
<booktitle>EMNLP’03,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="15885" citStr="Riloff and Wiebe 2003" startWordPosition="2489" endWordPosition="2492">ive and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity o</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Riloff, Ellen and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP’03, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stone</author>
<author>Dexter Dunphy</author>
<author>Marchall Smith</author>
<author>Daniel Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="26958" citStr="Stone et al. 1966" startWordPosition="4406" endWordPosition="4409"> negative, if h(w|S+) is greater than h(w|S−), the word is classified as negative and positive otherwise. This can be achieved by setting parameter γ = 1 in Algorithm 1. 4. Foreign Word Polarity As we mentioned earlier, a large body of research has focused on identifying the semantic orientation of words. This work has almost exclusively dealt with English and uses several language-dependent resources. When we try to apply these methods to other languages, we run into the problem of the lack of resources in other languages when compared with English. For example, the General Inquirer lexicon (Stone et al. 1966) has thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 20</context>
<context position="31185" citStr="Stone et al. (1966)" startWordPosition="5098" endWordPosition="5101">on We use the multilingual network described previously to predict the semantic orientation of words based on the mean hitting time to two sets of positive and negative seeds. Given two lists of seed English words with known polarity, we define two sets of nodes S+ and S− representing those seeds. For any given word w, we calculate the mean hitting time between w and the two seed sets h(w|S+) and h(w|S−). If h(w|S+) is greater than h(w|S−), the word is classified as negative; otherwise it is classified as positive. We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997) and Stone et al. (1966). 5. Out-of-Vocabulary Words We observed that a significant portion of the text used on-line in discussions, comments, product reviews, and so on, contains words that are not defined in WordNet or in 548 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation standard dictionaries. We call these words Out-of-Vocabulary (OOV) words. Table 6 later in this article shows some OOV word examples. To show the importance of OOV word polarity identification, we calculated the proportion of OOV words in three corpora used for sentiment studies: a set of movie reviews, a set of on-li</context>
<context position="36520" citStr="Stone et al. 1966" startWordPosition="6012" endWordPosition="6015">tart with the graph G(W, E) constructed from WordNet synsets. For each OOV word that does not exist in G, we create a new node w. We set the part of speech of w to unspecified. Then we use the Web-based method described in the previous section to find a set of words that are most related to w. Finally, we create a link between each OOV word and each of its related words. To predict the polarity of an OOV word, we use the same random walk model described earlier. 6. Experiments We performed experiments on the gold-standard data set for positive/negative words from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4,206 words, 1, 915 of which are positive and 2,291 of which are negative. Some of the ambiguous words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005). Some examples of positive/negative words are listed in Table 2. We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to generate co-occurrence statistics in the experiments that used them. We used 10-fold cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical s</context>
<context position="40747" citStr="Stone et al. 1966" startWordPosition="6683" endWordPosition="6686">tational Linguistics Volume 40, Number 3 The SO-PMI value can be calculated as follows: SO-PMI(w) = log hitsw,pos x hitsneg (5) hitsw,neg x hitspos where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a commercial search engine when the search query is the given word and the disjunction of all positive seed words. hitspos is the number of hits when we search for the disjunction of all positive seed words. hitsw,neg, and hitsneg are defined similarly. After Turney (2002), we use our method to predict semantic orientation of words in the General Inquirer lexicon (Stone et al. 1966) using only 14 seed words. The network we used contains only WordNet relations. No glosses or co-occurrence statistics are used. The results comparing the SO-PMI method with different data set sizes, the spin model, and the proposed method using only 14 seeds is shown in Table 3. We observe that the random walk method outperforms SO-PMI when SO-PMI uses data sets of sizes 1 x 107 and 2 x 109 words. The performance of SO-PMI and the random walk methods are comparable when SO-PMI uses a very large data set (1 x 1011 words). The performance of the spin model approach is also comparable to the oth</context>
<context position="51643" citStr="Stone et al. (1966)" startWordPosition="8547" endWordPosition="8550">s. We compare our results with two baselines. The first is the SO-PMI method described in Turney and Littman (2003). We used the same seven positive and seven negative seeds as Turney and Littman (2003). The second baseline constructs a network of only foreign words as described earlier. It uses mean hitting time to find the semantic association of any given word. We used 10-fold cross-validation for this experiment. We will refer to this system as HT-FR. Finally, we build a multilingual network and use the hitting time as before to predict semantic orientation. We used the English words from Stone et al. (1966) as seeds and the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN. Figure 6 compares the accuracy of the three methods for Arabic and Hindi. We notice that the SO-PMI and the hitting time–based methods perform poorly on both Arabic and Hindi. This is clearly evident when we consider that the accuracy of the two systems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan 3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of the words in a corpus. 556 Hassan et al. A Random Walk–Based Model for </context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Stone, Philip, Dexter Dunphy, Marchall Smith, and Daniel Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>Subjectivity recognition on word senses via semi-supervised mincuts.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>1--9</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="16771" citStr="Su and Markert (2009)" startWordPosition="2623" endWordPosition="2626">d Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or objective, utilizing the MPQA corpus. They show that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivity ambiguous words. Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information. Their method requires less training data other than the sense definitions and relational structure of WordNet. 2.5 Word Polarity Classification for Foreign Languages Word sentiment and subjectivity has also been studied for languages other than English. Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity 543 Computational Linguistics Volume 40, Number 3 lexicon based on an English lexicon, an on-line translation service, and Wordnet. Mihalcea a</context>
</contexts>
<marker>Su, Markert, 2009</marker>
<rawString>Su, Fangzhong and Katja Markert. 2009. Subjectivity recognition on word senses via semi-supervised mincuts. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 1–9, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Szummer</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Partially labeled classification with Markov random walks.</title>
<date>2002</date>
<booktitle>In NIPS’02,</booktitle>
<pages>945--952</pages>
<contexts>
<context position="22050" citStr="Szummer and Jaakkola (2002)" startWordPosition="3503" endWordPosition="3506">lity Pij after the first step. The walk continues until the surfer hits a word with known polarity. Seed words with known polarity act as an absorbing boundary for the random walk. If we repeat the number of random walks N times, the percentage of times in which the walk ends at a positive/negative word could be used as an indicator of its positive/negative polarity. The average time a random walk starting at w takes to hit the set of positive/negative nodes is also an indicator of its polarity. This view is closely related to the partially labeled classification with random walks approach in Szummer and Jaakkola (2002) and the semi-supervised learning using harmonic functions approach in Zhu, Ghahramani, and Lafferty (2003). Let W be the set of words in our lexicon. We construct a graph whose nodes V are all words in W. Edges E correspond to the relatedness between words. We define the transition probability Pt+1|t(j|i) from i to j by normalizing the weights of the edges out of node i, so: Pt+1|t(j|i) = Wij/ � Wik (1) k where k represents all nodes in the neighborhood of i. Pt+1|t(j|i) denotes the transition probability from node i at step t to node j at time step t + 1. We note that the matrix of weights W</context>
</contexts>
<marker>Szummer, Jaakkola, 2002</marker>
<rawString>Szummer, Martin and Tommi Jaakkola. 2002. Partially labeled classification with Markov random walks. In NIPS’02, pages 945–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In ACL’05,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="7566" citStr="Takamura et al. (2005)" startWordPosition="1163" endWordPosition="1166">ists of the given word and one of the seed words. They use the search engine NEAR operator to look for instances where the given word is physically close to the seed word in the returned document. They present their method as an unsupervised method where a very small number of seed words are used to define semantic orientation rather than train the model. One of the limitations of their method is that it requires a large corpus of text to achieve good performance. They use several corpora; the size of the best performing data set is roughly one hundred billion words (Turney and Littman 2003). Takamura et al. (2005) propose using spin models for extracting semantic orientation of words. They construct a network of words using gloss definitions, thesaurus, and co-occurrence statistics. They regard each word as an electron. Each electron has a spin and each spin has a direction taking one of two values: up or down. Two neighboring spins tend to have the same orientation from an energy point of view. Their hypothesis is that as neighboring electrons tend to have the same spin direction, neighboring words tend to have similar polarity. They pose the problem as an optimization problem and use the mean field m</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Takamura, Hiroya, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In ACL’05, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Tong</author>
</authors>
<title>An operational system for detecting and tracking opinions in on-line discussion.</title>
<date>2001</date>
<booktitle>Workshop note, SIGIR 2001 Workshop on Operational Text Classification.</booktitle>
<contexts>
<context position="3504" citStr="Tong (2001)" startWordPosition="521" endWordPosition="522">azon.com provide customers not only with comprehensive information and reviews about a product, but also with faceted sentiment summaries. Such systems are all supported by a sentiment lexicon, some even in multiple languages. Another interesting application is mining on-line discussions. An enormous number of discussion groups exist on the Web. Millions of users post content to these groups covering pretty much every possible topic. Tracking a participant attitude toward different topics and toward other participants is a very important task that makes use of sentiment lexicons. For example, Tong (2001) presented the concept of sentiment timelines. His system classifies discussion posts about movies as either positive or negative. This is used to produce a plot of the number of positive and negative sentiment messages over time. All these applications would benefit from an automatic way of identifying semantic orientation of words. In this article, we study the task of automatically identifying the semantic orientation of any word by analyzing its relations to other words, Automatically classifying words as positive, negative, or neutral enables us to automatically identify the polarity of l</context>
</contexts>
<marker>Tong, 2001</marker>
<rawString>Tong, Richard M. 2001. An operational system for detecting and tracking opinions in on-line discussion. Workshop note, SIGIR 2001 Workshop on Operational Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>21--315</pages>
<contexts>
<context position="4566" citStr="Turney and Littman 2003" startWordPosition="683" endWordPosition="686">y analyzing its relations to other words, Automatically classifying words as positive, negative, or neutral enables us to automatically identify the polarity of larger pieces of text. This could be a very useful building block for systems that mine surveys, product reviews, and on-line discussions. We apply a Markov random walk model to a large semantic relatedness graph, producing a polarity estimate for any given word. Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005) and a weakly supervised (Turney and Littman 2003) learning problem. In the semisupervised setting, a training set of labeled words is used to train the model. In the weakly supervised setting, only a handful of seeds are used to define the two polarity classes. Our proposed method can be used both in a semi-supervised and in a weakly supervised setting. Empirical experiments on a labeled set of positive and negative words show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting. The results in the weakly supervised setting are comparable to the best reported values. The proposed method has the adv</context>
<context position="6605" citStr="Turney and Littman (2003)" startWordPosition="1003" endWordPosition="1006">for Identifying Semantic Orientation and then they classify each conjunctive expression as either the same orientation such as “simple and well-received” or different orientation such as “simplistic but wellreceived.” The result is a graph that they cluster into two subsets of adjectives. They classify the cluster with the higher average frequency as positive. They created and labeled their own data set for experiments. Their approach works only with adjectives because there is nothing wrong with conjunctions of nouns or verbs with opposite polarities (“war and peace”, “rise and fall”, etc.). Turney and Littman (2003) identify word polarity by looking at its statistical association with a set of positive/negative seed words. They use two statistical measures for estimating association: Pointwise Mutual Information (PMI) and Latent Semantic Analysis (LSA). To get co-occurrence statistics, they submit several queries to a search engine. Each query consists of the given word and one of the seed words. They use the search engine NEAR operator to look for instances where the given word is physically close to the seed word in the returned document. They present their method as an unsupervised method where a very</context>
<context position="47181" citStr="Turney and Littman (2003)" startWordPosition="7802" endWordPosition="7805"> a learning curve displaying how the performance of both the proposed method and the LP method is affected with varying the labeled set size (i.e., the number of seeds). We notice that the accuracy exceeds 90% when the training set size rises above 20%. The accuracy steadily increases as the size of labeled data increases. We also looked at the classification accuracy for different parts of speech in Figure 5. We notice that, in the case of 10-fold cross-validation, the performance is consistent across parts of speech. However, when we only use 14 seeds—all of which are adjectives, similar to Turney and Littman (2003)—we notice that the performance on adjectives is much better than other parts of speech. When we use 14 seeds but replace some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the performance for nouns and verbs improves considerably at the cost of a small drop in the Figure 3 Accuracy for words with high confidence measure. 554 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Figure 4 The effect of varying the number of seeds on accuracy. CV 14 Adj Seeds 14 Seeds 100 95 90 85 80 75 70 65 60 55 50 Adj Adv Noun Verb Figure 5 Accuracy for diff</context>
<context position="51139" citStr="Turney and Littman (2003)" startWordPosition="8463" endWordPosition="8466">t. Because there are more negative seeds than positive ones, the constructed graph has an overall bias towards the negative class. 6.2 Foreign Words In addition to the English data we described earlier, we constructed a labeled set of 300 Arabic and 300 Hindi words for evaluation. For every language, we asked two native speakers to examine a large amount of text and identify a set of positive and negative words. We also used an Arabic–English and a Hindi–English dictionary to generate Foreign–English links. We compare our results with two baselines. The first is the SO-PMI method described in Turney and Littman (2003). We used the same seven positive and seven negative seeds as Turney and Littman (2003). The second baseline constructs a network of only foreign words as described earlier. It uses mean hitting time to find the semantic association of any given word. We used 10-fold cross-validation for this experiment. We will refer to this system as HT-FR. Finally, we build a multilingual network and use the hitting time as before to predict semantic orientation. We used the English words from Stone et al. (1966) as seeds and the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN</context>
<context position="53238" citStr="Turney and Littman (2003)" startWordPosition="8812" endWordPosition="8815">we studied how much improvement is achieved by including links between foreign words from global WordNets. We found out that it improves the performance by 2.5% and 4% for Arabic and Hindi, respectively. 6.3 OOV Words We created a labeled set of 300 positive and negative OOV words. We asked a native English speaker to examine a large number of threads posted on several on-line forums and identify OOV words and label them with their polarities. Some examples of positive/negative OOV words are listed in Table 6. The baseline we use for OOV words is the SO-PMI method with the same 14 seeds as in Turney and Littman (2003). The calculation of SO-PMI is given in Equation (5). We used the approach described in Section 5 to automatically label the words. We used the words of the General Inquirer lexicon as labeled seeds. We set the maximum number of steps m to 15 and the number of samples k to 1, 000. We experimented with Table 6 Examples of positive and negative OOV words. Positive Negative Word Meaning Word Meaning beautimous beautiful and fabulous disastrophy a catastrophy and a disaster gr8 great banjaxed ruined buffting attractive ijit idiot 100 SO-PMI HT - FR HT - FR+EN Arabic Hindi Figure 6 Accuracy of fore</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Turney, Peter and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL’02,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="2705" citStr="Turney 2002" startWordPosition="400" endWordPosition="401">ical Engineering &amp; Computer Science and School of Information, University of Michigan, Ann Arbor, MI, USA. E-mail: radev®umich.edu. Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication: 14 July 2013. doi:10.1162/COLI a 00192 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 1. Introduction Identifying emotions and attitudes from unstructured text has a variety of possible applications. For example, there has been a large body of work for mining product reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002) have shown how product reputation mining helps with marketing and customer relation management. The Google products catalog and many on-line shopping sites like Amazon.com provide customers not only with comprehensive information and reviews about a product, but also with faceted sentiment summaries. Such systems are all supported by a sentiment lexicon, some even in multiple languages. Another interesting application is mining on-line discussions. An enormous number of discussion groups exist on the Web. Millions of users post content to these groups covering pretty m</context>
<context position="36680" citStr="Turney (2002)" startWordPosition="6043" endWordPosition="6044">nspecified. Then we use the Web-based method described in the previous section to find a set of words that are most related to w. Finally, we create a link between each OOV word and each of its related words. To predict the polarity of an OOV word, we use the same random walk model described earlier. 6. Experiments We performed experiments on the gold-standard data set for positive/negative words from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4,206 words, 1, 915 of which are positive and 2,291 of which are negative. Some of the ambiguous words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005). Some examples of positive/negative words are listed in Table 2. We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to generate co-occurrence statistics in the experiments that used them. We used 10-fold cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical significance was tested using a two-tailed paired t-test. All reported results are statistically significant at the 0.05 level. We perform experiments varying th</context>
<context position="39783" citStr="Turney 2002" startWordPosition="6517" endWordPosition="6518">thod. Our method outperforms the LP method in both the 10-fold cross-validation set-up and when only 14 seeds are used. We also compare our method to the SO-PMI method. Turney and Littman (2002) propose two methods for predicting the semantic orientation of words. They use Latent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) for measuring the statistical association between any given word and a set of 14 seed words. They describe this method as unsupervised because they only use 14 seeds as paradigm words that define the semantic orientation rather than train the model (Turney 2002). Table 3 Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model, and the random walks model for 10-fold cross-validation and 14 seeds. – CV 14 seeds SO-PMI (1 × 107) – 61.3 SO-PMI (2 × 109) – 76.1 SO-PMI (1 × 1011) – 82.8 Spin Model 91.5 81.9 Label Propagation 88.40 74.83 Random Walks 93.1 82.1 551 Computational Linguistics Volume 40, Number 3 The SO-PMI value can be calculated as follows: SO-PMI(w) = log hitsw,pos x hitsneg (5) hitsw,neg x hitspos where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a commercial search e</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter D. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews. In ACL’02, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of Web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>777--785</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="11822" citStr="Velikovich et al. (2010)" startWordPosition="1863" endWordPosition="1866"> to have positive semantic orientation. They use a set of 11 antonym-generating affix patterns to generate overtly marked words and their counterparts from the Macquarie Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the sentiment lexicon using a Roget-like thesaurus. Their method does not require seed sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of the marking theory is language-dependent and cannot be applied from one language to another. Contrasting the dictionary based approaches that rely on resources such as WordNet, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic features and context coherency (i.e., the tendency for same polarities to appear successively) to detect polar clauses. 2.3 Random Walk–Based Methods Closest to our work in its methodology is probably the line of research on semisupervised graphical methods for sentiment classification. Rao and Ravichandran (2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled and labeled nodes, each node representing a word that can be either</context>
<context position="13270" citStr="Velikovich et al. (2010)" startWordPosition="2084" endWordPosition="2087">andomized Mincut) and label propagation. The general idea of label propagation is defining a probability distribution over the positive and negative classes for each node in the graph. A Markov random walk is performed on the graph to recover this distribution for the unlabeled nodes. Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a similar label propagation method on a lexical graph built from WordNet, where a small set of words with known polarities are used as seeds. Brody and Elhadad (2010) use label propagation over a graph constructed of adjectives only. Velikovich et al. (2010) compare label propagation with a Web-based method and conclude that label propagation is not suitable when the whole Web is used as a background corpus, because the constructed graph is very noisy and contains many dense subgraphs, unlike the lexical graph constructed from WordNet. 542 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation Random walk–based methods have been studied in the context of many other NLP tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel corpora, where each node represents a phrase and two nodes are connected</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Velikovich, Leonid, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of Web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 777–785, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<title>Eurowordnet: A multilingual database for information retrieval.</title>
<date>1997</date>
<booktitle>In DELOS Workshop on Cross-Language Information Retrieval,</booktitle>
<pages>5--7</pages>
<contexts>
<context position="27723" citStr="Vossen 1997" startWordPosition="4535" endWordPosition="4536">h lexicons are not readily available in other languages. As we showed earlier, WordNet (Miller 1995) has been used for this task. However, even though W have been built for other languages, their coverage is relatively limited when compared to the English WordNet. The current release of English WordNet (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in Spanish, 15K in German, and 22K in French, among other European languages. In some cases, accuracy was traded for coverage. For example, the current release of the Japanese WordNet has 57K synsets but contains errors in as many as 5% of the entries.1 In this section, we show how we can extend the methods presented earlier to predict the semantic orientation of foreign words. The proposed method is based on creating 1 http://nlpwww.nict.go.jp/wn-ja/index.en.html. 547 Computational Linguistics Volume 40, Number 3 a multilingual network of words that represents both Engli</context>
<context position="29839" citStr="Vossen 1997" startWordPosition="4870" endWordPosition="4871">. 4.1 Multilingual Word Network We build a network G(V, E) where V = Ven U Vfr is the union of the sets of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual </context>
</contexts>
<marker>Vossen, 1997</marker>
<rawString>Vossen, P. 1997. Eurowordnet: A multilingual database for information retrieval. In DELOS Workshop on Cross-Language Information Retrieval, pages 5–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and the Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="14495" citStr="Wiebe 2000" startWordPosition="2280" endWordPosition="2281"> they are aligned in a phrase table. Then they compute hitting time of random walks to learn paraphrases. Our work is different from previous random walk methods in that it uses the mean hitting time as the criterion for assigning polarity labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task in subjectivity analysis is to identify text that presents opinion as opposed to objective text that present factual information (Wiebe 2000). Text could be either words, phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of subjectivity analysis such as classifying e-mails and mining reviews. For example, to analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from individual sentences as nodes to determine whether a sentence is subjective or objective. Each node (sentence) has an individual subjectivity score obtained from a first-pass classifier using sentence features and linguistic knowledge. Edges are weighted by a similarity metric of how likely it is that the two</context>
<context position="15730" citStr="Wiebe 2000" startWordPosition="2465" endWordPosition="2466"> same subjectivity class. All sentences to be classified are represented as unlabeled nodes and the only two labeled nodes represent the subjective and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sen</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Wiebe, Janyce. 2000. Learning subjective adjectives from corpora. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and the Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 735–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
<author>Theresa Wilson</author>
</authors>
<title>A corpus study of evaluative and speculative language.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="14581" citStr="Wiebe et al. (2001)" startWordPosition="2292" endWordPosition="2295">walks to learn paraphrases. Our work is different from previous random walk methods in that it uses the mean hitting time as the criterion for assigning polarity labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task in subjectivity analysis is to identify text that presents opinion as opposed to objective text that present factual information (Wiebe 2000). Text could be either words, phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of subjectivity analysis such as classifying e-mails and mining reviews. For example, to analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from individual sentences as nodes to determine whether a sentence is subjective or objective. Each node (sentence) has an individual subjectivity score obtained from a first-pass classifier using sentence features and linguistic knowledge. Edges are weighted by a similarity metric of how likely it is that the two sentences will be in the same subjectivity class. All sentences to be classified are </context>
</contexts>
<marker>Wiebe, Bruce, Bell, Martin, Wilson, 2001</marker>
<rawString>Wiebe, Janyce, Rebecca Bruce, Matthew Bell, Melanie Martin, and Theresa Wilson. 2001. A corpus study of evaluative and speculative language. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--065</pages>
<location>Sydney.</location>
<contexts>
<context position="15968" citStr="Wiebe and Mihalcea (2006" startWordPosition="2501" endWordPosition="2504">ed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subject</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Wiebe, Janyce and Rada Mihalcea. 2006a. Word sense and subjectivity. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 1,065–1,072, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings Computational Linguistics Volume 40, Number 3 of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>1--065</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="15968" citStr="Wiebe and Mihalcea (2006" startWordPosition="2501" endWordPosition="2504">ed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subject</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Wiebe, Janyce and Rada Mihalcea. 2006b. Word sense and subjectivity. In Proceedings Computational Linguistics Volume 40, Number 3 of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 1,065–1,072, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP’03,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="15915" citStr="Yu and Hatzivassiloglou 2003" startWordPosition="2493" endWordPosition="2496">es. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mih</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP’03, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In ICML’03,</booktitle>
<pages>912--919</pages>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In ICML’03, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
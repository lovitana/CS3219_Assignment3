<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999489666666667">
A Large-Scale Pseudoword-Based
Evaluation Framework for State-of-the-Art
Word Sense Disambiguation
</title>
<author confidence="0.993007">
Mohammad Taher Pilehvar*
</author>
<affiliation confidence="0.991336">
Sapienza University of Rome
</affiliation>
<author confidence="0.996035">
Roberto Navigli*
</author>
<affiliation confidence="0.990515">
Sapienza University of Rome
</affiliation>
<bodyText confidence="0.989628714285714">
The evaluation of several tasks in lexical semantics is often limited by the lack of large numbers
of manual annotations, not only for training purposes, but also for testing purposes. Word Sense
Disambiguation (WSD) is a case in point, as hand-labeled data sets are particularly hard and
time-consuming to create. Consequently, evaluations tend to be performed on a small scale, which
does not allow for in-depth analysis of the factors that determine a system’s performance.
In this article we address this issue by means of a realistic simulation of large-scale evalua-
tion for the WSD task. We do this by providing two main contributions: First, we put forward
two novel approaches to the wide-coverage generation of semantically aware pseudowords (i.e.,
artificial words capable of modeling real polysemous words); second, we leverage the most
suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a large-
scale experimental framework for the comparison of state-of-the-art supervised and knowledge-
based algorithms. Using this framework, we study the impact of supervision and knowledge on
the two major disambiguation paradigms and perform an in-depth analysis of the factors which
affect their performance.
</bodyText>
<sectionHeader confidence="0.994278" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.995233333333333">
Word Sense Disambiguation (WSD) is a core research field in computational linguis-
tics dealing with the automatic assignment of senses to words occurring in a given
context (Navigli 2009, 2012). There are two major paradigms in WSD: supervised and
knowledge-based. Supervised WSD starts from a training set and learns a computa-
tional model of the word of interest, which is later used at test time to classify new
instances of the same word. Knowledge-based WSD, instead, performs the disambigua-
tion task by using an existing lexical knowledge base—that is, a semantic network to
which graph algorithms, for example, can be applied. However, both disambiguation
paradigms have to face the so-called knowledge acquisition bottleneck, namely, the
</bodyText>
<note confidence="0.633698428571429">
* Department of Computer Science, Sapienza University of Rome, Viale Regina Elena 295, Roma 00161,
Italy. E-mail: {pilehvar,navigli}@di.uniroma1.it.
Submission received: 31 August 2013; revised version received: 31 October 2013; accepted for publication:
6 March 2014.
doi:10.1162/COLI a 00202
© 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.996817113636364">
difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins
1993).
Unfortunately, providing knowledge on a large scale is a time-consuming process,
which has to be carried out separately for each word sense and repeated for each
new language of interest. Importantly, the largest manual efforts for providing a wide-
coverage semantic network and training corpus for WSD date back to the early 1990s for
the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor
corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained
by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative
editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually
is still an arduous and understandably infrequent endeavor. Despite recent efforts in
this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010),
most work is now aimed either at the automatic acquisition of training data (Zhong
and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros
and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of an-
notations via games (Venhuizen et al. 2013) or even video games with a purpose, as
recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance
can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based
(Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different
settings and conditions. Moreover, existing studies hypothesize that this performance
can be further improved when larger amounts of manually crafted sense-tagged data
or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008;
Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, how-
ever, are obtained on small-scale data sets with different characteristics, thus making it
difficult to draw conclusions on the factors that impact the system’s performance.
In this article we address this issue by providing two main contributions:
• We first focus on novel, flexible techniques for creating new types of
artificial words that model real words by preserving their semantics as
much as possible. Our semantically aware pseudowords can be used to
model any word in the lexicon,1 therefore aiming for wide coverage.
We perform different experiments to show that our semantically aware
pseudowords are good at modeling existing ambiguous words in terms
of disambiguation difficulty, representativeness, and distinguishability of
the artificial senses.
• We leverage our semantically aware pseudowords to create, for the first
time, a large-scale evaluation framework for WSD. Using this framework,
we are able to perform an experimental comparison of state-of-the-art
systems for supervised and knowledge-based WSD on a very large
data set made up of millions of sense-tagged sentences. Our large-scale
framework enables us to carry out an in-depth analysis of the factors and
conditions that determine the systems’ performance.
In our recent work (Pilehvar and Navigli 2013), we presented an approach for the
generation of semantically aware pseudowords, called similarity-based pseudowords.
At the core of this approach was the Personalized PageRank algorithm (Haveliwala
</bodyText>
<footnote confidence="0.9780395">
1 Although in our experiments we focus on nouns only, the same approach can potentially be used for any
other open-class part of speech.
</footnote>
<page confidence="0.988447">
838
</page>
<note confidence="0.860402">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<bodyText confidence="0.998730791666667">
2002) on the WordNet graph, which was utilized to find the most semantically similar
monosemous representative for a given sense of a real ambiguous word. The main
strength of the similarity-based approach lies in its flexibility, allowing high minimum
frequency constraints to be set on its selection of pseudosenses, while maintaining its
overall sense modeling quality.
In this article we extend our previous work as follows: 1) we propose a new
approach for generating semantically aware pseudowords which leverages topic signa-
tures; 2) we utilize the best type of pseudoword to create a novel framework for large-
scale evaluation and comparison of WSD systems; 3) based on this framework, we carry
out a large-scale comparison of state-of-the-art supervised and knowledge-based WSD
algorithms; and 4) we study the impact of the amount of supervision and knowledge
on the two major disambiguation paradigms and perform an in-depth analysis of the
factors and conditions that determine their performance.
The remainder of this article is organized as follows: In Section 2 we survey related
work concerning the impact of the knowledge acquisition bottleneck on WSD and pro-
vide an explanation of our pseudoword-based approach. In Section 3 we describe pseu-
dowords and overview the existing approaches to their generation. We then present two
new approaches that address the issues associated with existing pseudowords, hence
enabling the wide-coverage generation of semantically aware pseudowords. In Sec-
tion 4, we perform various experiments to assess the degree of realism of our proposed
pseudowords. We then illustrate how we leverage our pseudowords to generate large
sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based
WSD is described in Section 6. Experimental results as well as the findings are presented
and discussed in Section 7. Finally, we provide concluding remarks in Section 8.
</bodyText>
<sectionHeader confidence="0.999886" genericHeader="keywords">
2. Related Work
</sectionHeader>
<subsectionHeader confidence="0.987137">
2.1 Supervised WSD and the Knowledge Acquisition Bottleneck
</subsectionHeader>
<bodyText confidence="0.9999329">
Over the last few decades, WSD systems have been suffering from disappointingly
low performance, especially in an all-words setting in which one has to cover the
entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a).
In fact, one of the major obstacles to high-performance WSD is the so-called knowledge
acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate
word experts, supervised systems need training data for each word of interest, a very
demanding task as far as wide coverage is concerned (i.e., one which would require the
manual annotation of millions of word instances in context).
In an effort to address this issue, several approaches to the automatic acquisition of
sense-tagged corpora have been proposed. Some of these approaches are based on boot-
strapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely,
algorithms which start from a large unlabeled corpus, and a small labeled one, and iter-
atively populate the latter with an increasing number of sense-annotated sentences from
the former data set. Other approaches search the Web or large corpora to retrieve, for
each sense, a large number of sentences containing either a set of sense-specific mono-
semous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre
2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge re-
sources, such as Wikipedia, have also been exploited for generating sense-tagged data
(Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such
as the encyclopedic nature of the sense inventory and the lack of training of annotators.
</bodyText>
<page confidence="0.995721">
839
</page>
<note confidence="0.830325">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.999939052631579">
An alternative approach to acquiring sense-tagged data is to leverage multilingual
resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan,
Ng, and Zhong 2007). Most of these techniques, however, require human intervention
for mapping the translation of a word in the target language to the correct sense of the
corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this
problem by using a bilingual dictionary. However, the dictionary has to be aligned to the
sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available
that covers the full range of meanings in a lexicon. The approach, implemented in a
system based on Support Vector Machines and called It Makes Sense (Zhong and Ng
2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD
tasks. However, according to our calculation on the available models,2 this approach can
only provide training examples for about one third of ambiguous nouns in WordNet,
more than half of which have only one of their senses covered.
Middle-ground approaches have also been proposed that either mix arbitrary sense-
tagged corpora with a small amount of tagged data for the domain of interest (Khapra
et al. 2010), or estimate the sense distribution of the new domain data set with the help
of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisi-
tion bottleneck. However, domain adaptation approaches typically suffer from lower
disambiguation performance and still require annotated data for the domain of interest.
</bodyText>
<subsectionHeader confidence="0.998416">
2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck
</subsectionHeader>
<bodyText confidence="0.9987412">
Knowledge-based WSD systems are equally affected by the knowledge acquisition
bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in
carrying out the disambiguation task. Therefore, in order to obtain high performance,
knowledge-based systems are applied to large, wide-coverage lexical knowledge bases.
However, the largest hand-crafted resource of this kind (i.e., WordNet) dates back to
1990 with subsequent updates, which attests to the high cost of knowledge engineering
on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while ne-
glecting much syntagmatic relational information between concepts. As a consequence,
over the past few years several automatic techniques have been proposed that enrich
WordNet with new relation edges, such as those obtained from disambiguated glosses
(Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signa-
tures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured
resources (Hovy, Navigli, and Ponzetto 2013).
Enriched knowledge bases have been shown to greatly benefit graph-based ap-
proaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009;
Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and
Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs
WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these
methods outperform supervised WSD systems when applied within a domain, but,
when the knowledge base is enriched with tens of thousands of semantic relations
automatically extracted from Wikipedia, performance comparable to that of state-of-
the-art supervised systems can be obtained in a general all-words setting, too (Ponzetto
and Navigli 2010; Moro, Raganato, and Navigli 2014).
Recently, a multilingual graph-based WSD approach has been developed that lever-
ages a large multilingual semantic network, called BabelNet (Navigli and Ponzetto
</bodyText>
<footnote confidence="0.823996">
2 http://nlp.comp.nus.edu.sg/sw/models.tar.gz.
</footnote>
<page confidence="0.990329">
840
</page>
<note confidence="0.861271">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<bodyText confidence="0.98985525">
2012a), to achieve state-of-the-art results on both general all-words and domain-
oriented WSD (Navigli and Ponzetto 2012b). Experimental results show that the joint
use of multilingual knowledge enables further improvements over monolingual WSD.
However, the power of this disambiguation system lies mainly in its usage of the
BabelNet multilingual semantic network. In fact, Agirre, Lopez de Lacalle, and Soroa
(2014) showed that under similar conditions (i.e., when the same lexical knowledge
base was used), the PPR algorithm can outperform the graph-based WSD algorithms
used by Navigli and Ponzetto (2012b).
</bodyText>
<subsectionHeader confidence="0.984488">
2.3 The Supervision vs. Knowledge Dilemma
</subsectionHeader>
<bodyText confidence="0.999977257142857">
Unfortunately, as of today we do not have unequivocal insights into which disambigua-
tion paradigm is more suitable under which conditions. As a matter of fact, not only
does each implemented system come with its own amount and kind of supervision or
knowledge, making it hard to determine the contribution of the supervision or knowl-
edge vs. that of the WSD algorithm, but test data sets are small, typically comprising
one or two thousand sense-tagged word items, which prevents us from drawing solid
conclusions. Even the largest annotation effort ever—namely, the SemCor sense-tagged
data set (Miller et al. 1993), comprising around 235,000 semantic annotations—covers
only about 15% of word types in WordNet with an average of 10 instances per word,
thus precluding large-scale experimental studies.
A possible solution to this current limit in the evaluation of WSD systems is
to generate sense-annotated data with the help of artificial ambiguous words, called
pseudowords. Pseudowords are created by conflating a set of unambiguous words
called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale,
Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large
amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords
have also been used in other work aimed at studying the effects of data size on machine
learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selec-
tional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky
2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011).
However, constructing a pseudoword by merely combining a random set of unam-
biguous words picked out to be in the same range of occurrence frequency (Sch¨utze
1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not
provide a suitable model of a real polysemous word (Gaustad 2001), since in the real
world different senses, unless homonymous, share some semantic or pragmatic relation.
For this reason, random pseudowords, when used for WSD evaluation, were found to
be easier to disambiguate compared with the human-generated pseudowords (Gaustad
2001), thus leading to an optimistic upper-bound estimate on the performance of WSD
classifiers (Nakov and Hearst 2003).
Several researchers addressed the issue of producing pseudowords that can model
semantic relationships between senses. To this end Nakov and Hearst (2003) used lexical
category membership from a medical term hierarchy (extracted from MeSH3 [Medical
Subject Headings]) to create “more plausible” pseudowords. By considering the distri-
butions from lexical category co-occurrence, they produced a set of pseudowords that
were closer to real ambiguous words in terms of disambiguation difficulty than random
</bodyText>
<footnote confidence="0.676831">
3 http://www.nlm.nih.gov/mesh.
</footnote>
<page confidence="0.99063">
841
</page>
<note confidence="0.829855">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.99978605">
pseudowords. However, this approach requires a specific hierarchical lexicon and falls
short of creating many pseudowords with high polysemy.
More recent work has focused on the identification of monosemous representatives
in the surroundings of a sense, that is, selected among concepts directly related to the
given sense. Senses of a real ambiguous word have been modeled by picking out the
most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al.
2006). Pseudowords are then constructed by conflating these morphemes accordingly.
However, this method leverages a specific Chinese hierarchical lexicon, in which differ-
ent levels of the hierarchy correspond to different levels of sense granularity. A more
flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous
words in WordNet. For each particular sense, they search its surroundings in the Word-
Net graph in order to find an unambiguous representative for that sense.
Unfortunately, as we discuss in detail in the next section, none of these proposals
can enable a large-scale evaluation framework for WSD, mainly because they suf-
fer from coverage issues that prevent the creation of wide-coverage sense-annotated
data sets. In this article we propose new pseudoword generation techniques that allow
for the creation of thousands of artificial words having sufficient occurrence coverage
within a large corpus. We then leverage our semantically aware pseudowords to create
an evaluation framework which enables a large-scale comparison of state-of-the-art
supervised and knowledge-based WSD.
</bodyText>
<sectionHeader confidence="0.991299" genericHeader="introduction">
3. Pseudowords
</sectionHeader>
<bodyText confidence="0.9667745">
A pseudoword is an artificially created ambiguous word created by concatenating two
or more distinct words. Formally, p = w1*w2*... *wn is a pseudoword with polysemy
degree n where each wi is called a pseudosense. Each pseudosense is usually identified
by an unambiguous word drawn from the set of monosemous words in a given lexicon
(e.g., WordNet). For instance, press release*ship*camel is a pseudoword with three distinct
meanings explicitly identified by its pseudosenses (i.e., press release, ship, and camel).
Pseudowords are particularly useful for creating artificially annotated data sets.
To this end, an untagged corpus C is automatically annotated with a pseudoword
p = w1*w2*... *wn by substituting all occurrences of wi in C with p for each pseudosense
i ∈ {1, ... , n}. As an example, consider the following three sentences:
a1. The goal of a press release is to attract favorable media attention.
a2. For a ship to float, its weight must be less than that of the water displaced by its hull.
a3. During the winter, the camel can go fifty days without being watered.
In order to generate annotated data, it is enough to replace the individual occur-
rences of press release, ship and camel with the pseudoword press release*ship*camel, while
noting the replaced term as the corresponding sense:
b1. The goal of a press release*ship*camelpress release is to attract favorable media attention.
b2. For a press release*ship*camelship to float, its weight must be less than that of the water
displaced by the hull.
b3. During the winter, the press release*ship*camelcamel can go fifty days without being watered.
where b1, b2, and b3 are three annotated sentences for our pseudoword press
release*ship*camel with three different intended senses. This way, pseudowords can be
leveraged to automatically annotate an arbitrarily large number of sentences. As men-
tioned earlier, the first restriction on the choice of pseudosenses is that they need to
</bodyText>
<page confidence="0.986271">
842
</page>
<note confidence="0.854187">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<bodyText confidence="0.999858595238095">
be unambiguous, so as to avoid the introduction of uncontrolled ambiguity. Another
constraint is that the pseudosense wi must appear in a sufficient number of sentences in
the corpus C. This constraint on the occurrence frequency guarantees that there exist as
many sentences in the corpus as the number of annotated sentences that are requested
for the task of interest which will exploit the resulting annotated corpus.
The pseudoword in our example was generated by randomly selecting three
monosemous words from WordNet. This can be considered as the most immediate
approach for generating a pseudoword where constituents are randomly picked from
the set of all monosemous words given by a lexicon. This results in a set of pseudowords
(hereafter called random pseudowords) that are highly likely to have semantically
unrelated pseudosenses. However, we know that the different senses of a real word
are often in a semantic or etymological relationship. Therefore, random pseudowords
can only model homonymous distinctions (such as the centimeter vs. curium senses of
the noun cm), and fall short of modeling systematic polysemy (such as the lack vs.
insufficiency senses of the noun deficiency).
A pseudoword generation approach ought to be able to address this weakness of
random pseudowords. A possible solution is to create pseudowords that model existing
ambiguous words by providing, for each pseudoword, a one-to-one correspondence be-
tween each pseudosense and a corresponding sense of the modeled word. For instance,
lack*shortfall is a good pseudoword modeling the real word deficiency as its pseudosenses
preserve the meanings of their corresponding real word’s senses. We call artificial words
of this kind semantically aware pseudowords, in that they aim at listing senses that
are in specific relations to each other, thus mirroring the relations existing between
the senses of real words in the lexicon. For example, the lack-insufficiency relation is
encoded in the pseudoword for deficiency, which would not be possible if we generated
a random pseudoword.
Semantically aware pseudowords enable the generation of artificially annotated
data sets that have similar properties to their real counterparts and this makes them
particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006;
Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated
data set different senses of a word appear in distinct contexts. The extent of this dis-
tinction, however, depends on the semantic relatedness of the corresponding senses.
The intuition behind semantically aware pseudowords is that they model each sense
of an ambiguous word through a semantically similar monosemous representative that
should appear naturally in contexts that are similar to those of its corresponding real
sense. For this reason, these pseudowords should be expected to result in data sets
wherein the distinctions between different sense contexts are similar to those in real
sense-annotated data sets.
In the next three sections we describe three techniques, two of which are presented
in full detail for the first time in this article, for the generation of semantically aware
pseudowords that use WordNet as the reference lexicon. In what follows we focus on
nominal pseudowords, and leave the extension to other parts of speech to future work.
</bodyText>
<subsectionHeader confidence="0.978185">
3.1 Vicinity-Based Pseudowords
</subsectionHeader>
<bodyText confidence="0.9999504">
As discussed in Section 2, earlier techniques for the generation of semantically aware
pseudowords were either inherently restricted to specific hierarchical lexicons utilized
in the generation process, or to the number of pseudowords they could generate. An
idea put forward by Otrusina and Smrz (2010) was to create pseudowords by combining
representatives for each individual sense of a real ambiguous word in WordNet. The
</bodyText>
<page confidence="0.994819">
843
</page>
<note confidence="0.338468">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.999784153846154">
representatives were selected among monosemous relatives (i.e., unambiguous words
that are structurally related to a given sense). This method for finding monosemous
representatives for senses has been in use since 1998, when it was first proposed for
the unsupervised acquisition of sense-tagged corpora (Leacock, Chodorow, and Miller
1998).
Specifically, Otrusina and Smrz (2010) exploit WordNet, whose conceptual units are
synonym sets, called synsets, which encode the different meanings of words. In order to
find a monosemous representative for a given synset, the approach (hereafter referred
to as the vicinity-based approach) performs a search on the set of words in the same
synset and the surrounding ones (i.e., the synsets connected to that synset by means of
WordNet’s lexico-semantic relations). These related synsets include siblings and direct
hyponyms. In the case where no monosemous candidate could be found among these
synsets, the search space is further extended to hypernyms and meronyms.
As an example, consider the ambiguous noun coke, which has three senses (i.e., fuel,
drink, and drug) in WordNet 3.0. We show in Table 1, for each of the three senses
of coke, the set of nouns in the corresponding synset as well as in the surrounding
synsets. Monosemous words are shown in bold in the table. As can be seen, there
exist multiple monosemous candidates for each sense (coca cola, pepsi, and pepsi cola
for the second sense; nose candy, cocaine, and cocain for the third sense; and dozens of
candidates in the direct siblings’ vicinity of the first sense). Among these candidates
Otrusina and Smrz (2010) select those whose occurrence frequency ratio in a given text
corpus is most similar to that of the senses of the corresponding real word as given by
a sense-annotated corpus. However, calculating the occurrence frequency of individual
senses of a word requires a large-enough sense-tagged corpus. This dependency on
sense-annotated data is a disadvantage of the vicinity-based approach that limits its
ability in modeling arbitrary words.
</bodyText>
<tableCaption confidence="0.906053">
Table 1
</tableCaption>
<bodyText confidence="0.7985838">
Synset neighbors of the three senses of coke (We use the sense notation of Navigli [2009] where
the irh sense of word w is denoted as wi. We also highlight monosemous words in bold.)
Synset literals sense 1 {coke1}
sense 2 {coca cola1, coke2}
sense 3 {coke3, blow6, nose candy1, snow4, C12}
</bodyText>
<construct confidence="0.663570857142857">
Direct siblings sense 1 {biomass1}, {butane1}, {charcoal1, wood coal2}, {coal gas1},
{coke1}, {diesel oil1}, diesel fuel1}, {fire7}, {fossil fuel1},
{fuel oil1, heating oil1}, {gasohol1}, {gasoline1, gasolene1,
gas3, petrol1}, {illuminant1}, {kerosene1, kerosine1, lamp oil1,
coal oil1}, {methyl alcohol1, wood alcohol1, wood spirit1},
{nuclear fuel1}, {propane1}, {red fire1}, {combustible1,
combustible material1}, {water gas1}, {firewood1}, {igniter1,
</construct>
<equation confidence="0.819241555555556">
ignitor1, lighter1}
sense 2 {Pepsi1, pepsi cola1}
sense 3 {basuco1}, {crack8, crack cocaine1, tornado2}
Hypernyms sense 1 {fuel1}
sense 2 {cola2, dope3}
sense 3 {cocaine1, cocain1}
Hyponyms sense 1 -
sense 2 -
sense 3 -
</equation>
<page confidence="0.981463">
844
</page>
<note confidence="0.797618">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.997634">
Table 2
</tableCaption>
<table confidence="0.970304714285714">
Noun coverage percentage of vicinity-based pseudowords by degree of polysemy for different
values of minimum frequency.
2 3 4 5 6 7 8 9 10 11 12 &gt;12 overall
87 82 74 71 67 70 60 64 45 46 44 28 83
64 56 47 44 38 41 31 33 17 13 20 10 59
52 43 33 27 22 25 19 17 8 10 8 10 46
31 20 16 7 4 6 4 3 0 0 0 0 25
</table>
<bodyText confidence="0.99995548">
In addition to this limitation, the vicinity-based approach suffers from lack of
flexibility in generating pseudowords that can be leveraged for creating a large-scale
pseudosense-tagged corpus, where we need each pseudosense to occur with a relatively
large minimum frequency. Due to its small search space, the approach falls short of
identifying suitable monosemous representatives for many given senses, which under-
mines its ability to cover most of the ambiguous nouns in WordNet. We show in Table 2
the percentage of nouns in WordNet that could be modeled using the vicinity-based
approach when Gigaword (Graff and Cieri 2003) was used as our corpus. Coverage
statistics are presented for four different values of minimum frequency: 0 (no minimum
frequency constraint), 50, 200, and 1,000. Besides the overall coverage (rightmost col-
umn), in the table we also present the coverage percentage by degree of polysemy.
Here, an ambiguous noun in WordNet is considered as covered by its corresponding
vicinity-based pseudoword if, for each of its senses, a suitable monosemous candidate
can be found in its surrounding that also satisfies the specified minimum frequency in
the corpus. As can be seen from the table, the approach can only model about 60% of
ambiguous nouns in WordNet 3.0 when a small minimum frequency of 50 sentences
in the large Gigaword corpus is assumed. The coverage continues to drop with the
increase of minimum frequency up to only 25% of the ambiguous nouns covered when
a minimum frequency of 1,000 noun occurrences is required (last row of Table 2), with
most of the covered words having low polysemy. This shows that the approach is not
flexible enough for generating pseudowords that can be leveraged for creating large,
wide-coverage pseudosense-annotated data sets.
In order to address the aforementioned coverage issue of the vicinity-based ap-
proach, in the next two sections we propose two new approaches for the generation
of semantically aware pseudowords.
</bodyText>
<subsectionHeader confidence="0.995285">
3.2 Similarity-Based Pseudowords
</subsectionHeader>
<bodyText confidence="0.9971322">
We propose a new approach to the generation of pseudowords that enables the creation
of semantically aware pseudowords while tackling the coverage and flexibility issues
of the vicinity-based approach. In contrast to the vicinity-based method, which takes
as its search space the surroundings of a sense, our technique considers the WordNet
semantic network in its entirety, hence enabling us to determine a graded degree of
similarity between a given sense and all other synsets in WordNet. The similarity-based
approach identifies, for each sense of a given ambiguous word, the most semantically
similar monosemous word satisfying the minimum occurrence frequency constraint.
Our method can be considered an extension of the vicinity-based approach as it
replaces its pseudosense selection technique with a graph-based similarity measure.
</bodyText>
<figure confidence="0.991624714285714">
Polysemy
Minimum
Frequency
0
50
200
1,000
</figure>
<page confidence="0.84902">
845
</page>
<note confidence="0.289199">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.99963572">
This expands the search space for finding pseudosenses from a small set of surrounding
synsets to virtually all synsets in WordNet.
In order to measure semantic similarity we used the PPR (Haveliwala 2002) algo-
rithm, a graph-based technique that has been used previously as a core component for
semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and
WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be
used to estimate a probability distribution denoting the structural importance of all
the nodes in a graph for a given target node. When applied on a semantic network,
such as the WordNet graph whose nodes are synsets and edges the lexico-semantic
relations, the notion of importance can be interpreted as semantic similarity. The reason
behind our selecting a graph-based similarity measure was that the alternative context-
based methods, such as Lin’s (1998) measure, have been shown to require a wide-
coverage sense-tagged data set in order to calculate similarities on a sense-by-sense
basis for all words in the lexicon (Otrusina and Smrz 2010). Also, among WordNet-based
approaches, PPR reports state-of-the-art results on semantic similarity (Agirre et al.
2009) and WSD data sets (Agirre, Lopez de Lacalle, and Soroa 2014), thus representing
a suitable graph-based measure for finding the most appropriate pseudosenses.
In Algorithm 1 we present the procedure for the generation of our similarity-based
pseudowords. The algorithm takes as input an ambiguous word w, and generates its
corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith
sense of w. Additionally, the algorithm provides, for each generated pseudoword, a
confidence degree denoting the average ranking of the selected pseudosenses.
The algorithm models a given ambiguous word w by iterating over the synsets
corresponding to its individual senses (lines 5–18) and identifying the most suitable
monosemous representative for each. For each sense of w, we run the PPR algorithm
</bodyText>
<construct confidence="0.497618">
Algorithm 1: Generate a similarity-based pseudoword
</construct>
<bodyText confidence="0.510576">
Input: an ambiguous word w in WordNet
Output: a similarity-based pseudoword Pw and a confidence score averageRank
</bodyText>
<listItem confidence="0.938001857142857">
1 begin
2 Pw +- 0
3 totalRank +- 0
4 i +- 1
5 foreach s E Synsets(w) do
6 similarSynsets +- PersonalizedPageRank(s)
7 sort similarSynsets in descending order;
8 foreach s&apos; E similarSynsets do
9 totalRank +- totalRank + 1
10 foreach w&apos; E SynsetLiterals(s&apos;) do
11 if |Synsets(w&apos;) |= 1 and Freq(w&apos;) &gt; minFreq and �j : (j, w&apos;) E Pw then
12 Pw +- Pw U {(i,w&apos;)}
13 go to line 17
14 end
15 end
16 end
17 i +- i + 1
18 end
19 averageRank +- totalRank / |Synsets(w)|
20 return (Pw, averageRank)
21 end
</listItem>
<page confidence="0.97216">
846
</page>
<note confidence="0.556714">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.976747">
Table 3
</tableCaption>
<bodyText confidence="0.42014375">
Top five entries of the similarSynsets list for different senses of word coke (we show both
WordNet 3.0 offsets and synsets). The highest-ranking monosemous noun in each list is
shown in bold.
Sense no. Offset PPR score Terms in synset (literals)
</bodyText>
<equation confidence="0.749253866666667">
14685768-n 0.225 coke1
14875077-n 0.148 fuel1
1 00498836-v 0.096 coke4
00146138-v 0.038 change state1, turn14
15100644-n 0.011 firewood1
07927931-n 0.237 cola2, dope3
07928696-n 0.217 coca cola1, coke2
2 07927197-n 0.083 soft drink1
12197601-n 0.045 cola nut1, kola nut1
07928790-n 0.040 pepsi1, pepsi cola1
03060294-n 0.278 cocaine1, cocain1
03066743-n 0.205 blow6, c12, coke3, nose candy1, snow4
3 03492717-n 0.046 hard drug1
00021679-v 0.041 cocainise1, cocainize1
03060074-n 0.041 coca3
</equation>
<bodyText confidence="0.999793652173913">
by initializing it from the corresponding synset s (line 6). As a result, PPR outputs a
probability distribution over all synsets in WordNet denoting the semantic similarity of
each synset to s.4 The synset distribution is then sorted according to its values (line 7).
We then go through all its nominal synsets (s&apos;) in the search for a suitable monosemous
noun (line 11). This search continues until a suitable candidate is found that satisfies
the minimum occurrence frequency minFreq. Upon finding this candidate, the selected
monosemous word w&apos; is added as the corresponding pseudosense for the ith sense of Pw
(line 12). These steps are repeated for every sense of w.
The higher the position of a selected pseudosense in the sorted list of similarSynsets,
the more confidence we have in the preservation of meaning. Therefore, we calculate a
confidence score (averageRank in the algorithm) as the average of the synset’s positions
(in the various similarSynsets lists) from which the pseudosenses of Pw are picked out
(line 19). We will later use this confidence score for evaluating our pseudowords. The
algorithm returns as its output, for a given word w, the corresponding pseudoword Pw
along with its averageRank score (line 20).
Consider the generation process of the similarity-based pseudoword for our word
coke. Table 3 shows the list of top-five most similar synsets for each of the three senses
of this term, as given by the PPR algorithm. Our algorithm selects the highest ranking
monosemous candidates that satisfy the minimum frequency (=1,000 in the example)
for each sense (shown in bold in the table). Hence, fuel*coca cola*cocaine is returned as
the pseudoword corresponding to the word coke. Note that the top-ranking synsets are
those also found by the vicinity-based approach. However, thanks to PPR working
on the entire network, our similarity-based approach can back off to more distant,
</bodyText>
<footnote confidence="0.997848666666667">
4 In the PPR probabililty distribution, the top-ranking synsets contain words that are most likely similar
to the target sense, whereas we move to a graded notion of relatedness as far as lower-ranking ones are
concerned (Agirre et al. 2009).
</footnote>
<page confidence="0.990673">
847
</page>
<note confidence="0.333243">
Computational Linguistics Volume 40, Number 4
</note>
<tableCaption confidence="0.9434615">
Table 4
Sample similarity-based pseudowords generated (with minimum frequency of 1,000 occurrences
in Gigaword) for four different nouns in WordNet 3.0. Words shown in bold are those that
could not be modeled using the vicinity-based approach for the given minimum frequency.
Pseudosenses which are not picked out from the surrounding of the corresponding sense
(hence, could not be modeled using the vicinity-based approach) are shown in bold in the
second column of the table.
word similarity-based pseudoword
bernoulli physicist*mathematician*astronomer
coach football coach*tutor*passenger car*clarence*public transport
green greenery*central park*labor leader*green party*river*golf course*greens*max
sunray sunbeam*vine*sunlight
</tableCaption>
<bodyText confidence="0.9978954">
though similar, synsets. We show in Table 4 some examples of ambiguous words along
with their generated similarity-based pseudowords (minimum frequency is again set to
1,000).
Having the large search space of virtually all synsets in WordNet, the similarity-
based approach is able to select a monosemous candidate for each sense from a
relatively-large ranked list of similar synsets. This solves both the coverage and flex-
ibility issues of the vicinity-based approach for higher values of minimum frequency.
However, as mentioned earlier, the higher the position of a selected pseudosense in the
sorted list of similarSynsets, the more confidence we have in the preservation of meaning.
For this reason, we analyzed the averageRank values output by Algorithm 1 in order to
see how often our algorithm needs to resort to lower-ranking items in the similarSynsets
list. We present in Table 5, for each polysemy degree and for six different values of
minFreq, the mean and mode statistics of the averageRank scores of all the generated
pseudowords for all the nouns (up to polysemy degree 12) in WordNet. As can be seen
in the table, the higher the value of minFreq, the further the algorithm descends through
</bodyText>
<tableCaption confidence="0.845332">
Table 5
Statistics of averageRank scores of similarity-based pseudowords: we show mean and mode
positions for six different values of minimum occurrence frequency (0, 200, 500, 1,000, 2,000, and
5,000) and for each polysemy degree (we show the average value in the case of multiple modes).
</tableCaption>
<bodyText confidence="0.784358666666667">
minFreq 0 200 500 1,000 2,000 5,000
poly. mean mode mean mode mean mode mean mode mean mode mean mode
2 2.0 1.0 10.7 2.0 16.9 2.0 28.3 4.0 52.2 3.0 66.8 3.5
</bodyText>
<figure confidence="0.962326">
3 2.2 2.0 9.7 2.0 15.4 4.7 23.4 4.7 39.6 6.3 51.6 11.7
4 2.3 2.0 8.6 3.0 14.2 7.8 22.5 10.9 33.4 12.3 45.9 18.3
5 2.2 2.0 8.5 5.0 14.6 5.6 21.9 16.0 33.7 14.4 48.3 18.2
6 2.3 2.0 9.0 4.0 15.6 3.8 21.3 12.2 26.5 17.2 41.2 26.0
7 2.2 2.0 8.0 6.0 13.0 8.4 17.8 7.7 26.0 18.9 40.7 27.3
8 2.2 2.0 8.5 4.0 12.7 9.8 19.4 16.1 29.8 28.7 44.1 42.6
9 2.2 2.0 7.5 4.0 12.3 12.4 17.5 17.6 26.4 30.2 40.9 37.9
10 2.2 2.0 7.1 5.0 11.5 7.9 16.2 15.0 25.6 19.2 38.1 38.1
11 2.4 2.0 7.7 8.0 12.0 15.3 16.5 11.5 23.9 7.5 37.9 35.7
12 2.4 2.0 7.7 4.0 12.9 12.9 17.5 23.3 25.7 25.7 44.2 32.5
&gt;12 2.5 1.0 7.2 2.0 10.8 2.0 16.0 4.0 22.4 4.0 39.1 4.0
overall 2.1 1.0 10.1 2.0 16.1 2.0 26.1 4.0 46.3 4.0 60.2 4.0
</figure>
<page confidence="0.929417">
848
</page>
<note confidence="0.746317">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.956954">
Table 6
</tableCaption>
<bodyText confidence="0.965534416666667">
Top five words in the topic signatures for different senses of noun coke. The first monosemous
noun for each sense is shown in bold.
Sense no. Sense 1 Sense 2 Sense 3
weight word weight word weight word
0.190 oil 0.477 pepsi 0.275 heroin
0.129 gas 0.069 colon 0.151 drug
Topic Signatures 0.108 fuel 0.031 coca 0.102 hard
0.092 wood 0.008 coke 0.034 cocaine
0.079 fire 0.007 star 0.025 user
the list similarSynsets to select a pseudosense. However, the mode statistics in the table
suggests that even when minFreq is set to a large value, most of the pseudosenses are
picked out from the highest-ranking positions in the similarSynsets list.
</bodyText>
<subsectionHeader confidence="0.999421">
3.3 Topic Signature-Based Pseudowords
</subsectionHeader>
<bodyText confidence="0.999925928571429">
As an alternative means of finding suitable monosemous representatives for word
senses with the PPR algorithm, we propose using automatically generated topic signa-
tures. Topic signatures (TS) are weighted topical vectors that are associated with senses
or concepts (Lin and Hovy 2000). The dimensions of these vectors are the words in the
vocabulary and their weights determine the relatedness of each of these words to the
target word sense. These vectors can be obtained automatically from large corpora or
the Web with the help of monosemous relatives.
In order to generate a TS-based pseudoword for a word w, we first sort the weighted
vectors associated with the senses of w. Then, from each of these vectors, we select
the monosemous word with highest relatedness (i.e., largest weight) which satisfies the
minimum frequency constraint. The generation process of the TS-based pseudowords
is very similar to that of similarity-based pseudowords: Whereas the latter performs a
search in the sorted PPR vector of a particular sense to obtain a suitable monosemous
representative, the former considers the sorted TS vector as its search space. Also note
that the PPR vectors are indexed with synsets, whereas topic signatures have lemmas
as their indices.
In our experiments we used the topic signatures provided by Agirre and de Lacalle
(2004) for nominal senses of polysemous nouns in WordNet 1.6.5 The monosemous
relatives for each sense were obtained by taking into account WordNet relations such as
synonyms, hypernyms, hyponyms, and siblings that were later used to query the Web
and create a large corpus. This corpus was then used to build topic signatures.
Table 6 shows the top five words in the topic signatures for different senses of the
word coke. The first monosemous candidate for each sense is shown in bold (again,
the minimum frequency is assumed to be 1,000 here). The corresponding pseudoword
generated using this approach is fuel*pepsi*heroin.
Even though the TS-based approach shares the monosemous relatives idea with
the vicinity-based approach, the additional step of gathering related instances for these
representatives guarantees wider coverage. We calculated the coverage of TS-based
</bodyText>
<footnote confidence="0.734601">
5 Available from: http://ixa.si.ehu.es/Ixa/resources/sensecorpus.
</footnote>
<page confidence="0.99052">
849
</page>
<note confidence="0.337745">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.999856428571428">
pseudowords to be 84% (over ambiguous nouns of WordNet 1.6 and with no minimum
frequency constraint), which is comparable to that of vicinity-based pseudowords (i.e.,
83%, see Table 2). We observed in Table 2 that the coverage of the vicinity-based
pseudowords drops rapidly with the increase in minimum frequency such that for a
minimum frequency of 1,000 only 25% of the polysemous nouns could be modeled. The
TS-based approach, instead, provides a better flexibility for higher values of minimum
frequency, hence enabling the generation of large-scale annotated data sets. Thanks to
its larger search space, the TS-based approach is able to retain the same 84% coverage
for a minimum frequency of 1,000.
Compared with the similarity-based pseudoword generation (described in Sec-
tion 3.2), this approach provides a different way of overcoming the coverage issue of
vicinity-based pseudowords. However, the former guarantees 100% coverage, whereas
the latter suffers from the lack of monosemous relatives for a portion of WordNet senses,
leading to non-optimal coverage.
</bodyText>
<sectionHeader confidence="0.945236" genericHeader="method">
4. Pseudoword Evaluation
</sectionHeader>
<bodyText confidence="0.9997548">
In Sections 3.2 and 3.3 we presented two techniques for the generation of semantically
aware pseudowords that were able to address the coverage and flexibility issues of the
vicinity-based approach. In order to verify the ability of these pseudowords to model
various properties of real ambiguous words, we performed three separate evaluations
so as to assess them from different perspectives:
</bodyText>
<listItem confidence="0.981045">
• Disambiguation difficulty in comparison to real words, where we
extrinsically study the impact of the pseudoword quality on the
disambiguation performance (Section 4.1).
• Representative power of pseudosenses, where we assess the semantic
closeness of pseudosenses to their corresponding real senses (Section 4.2).
• Distinguishability of pseudosenses, where we determine to what extent
pseudosenses are specific to a fine-grained real sense rather than covering
multiple senses (Section 4.3).
</listItem>
<bodyText confidence="0.9999328">
Given that our aim was to leverage these pseudowords for creating large-scale
pseudosense-annotated data sets, we performed evaluations on pseudowords gener-
ated with minFreq per pseudosense set to a high value of 1,000 (i.e., we can generate
1,000 annotated sentences for each pseudosense) using the English Gigaword corpus
(Graff and Cieri 2003).
</bodyText>
<subsectionHeader confidence="0.995931">
4.1 Disambiguation Difficulty of Pseudowords
</subsectionHeader>
<bodyText confidence="0.999966888888889">
Our first experiment is an extrinsic evaluation to assess the correlation between the
difficulty of the disambiguation task when using pseudowords and real words. The
basic idea behind this experiment is to verify, through a disambiguation task, if
the semantic similarity among the senses of an ambiguous word is preserved in its cor-
responding pseudoword. Semantically similar senses of an ambiguous word will tend
to appear in similar contexts, making it relatively difficult to discriminate between them.
Conversely, ambiguous words that have semantically distinct senses (e.g., homonyms)
will be relatively easier to disambiguate. Given that our pseudowords directly model
real ambiguous words, we ideally expect a pseudoword to preserve the same level
</bodyText>
<page confidence="0.88821">
850
</page>
<bodyText confidence="0.97940936">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
of semantic similarity between pseudosenses as that of its corresponding real word,
and therefore to exhibit a comparable degree of disambiguation difficulty to that of its
corresponding real word.
We performed this evaluation in the style of earlier work (Otrusina and Smrz 2010;
Lu et al. 2006). In order to test a pseudoword generation approach using this style,
first, all the sense-tagged words in a manually annotated lexical sample data set are
modeled using the approach. Next, a corresponding pseudosense-annotated data set
is automatically constructed by sampling sentences from a corpus while maintaining
the same number of training and test sentences for each word as that of the original
manually tagged data set. A correlation analysis is then carried out to compare the
disambiguation performance of a supervised WSD system on a given ambiguous word
against its corresponding pseudoword. In this experiment we evaluate our similarity-
based, TS-based, and, as baseline, random pseudowords. Owing to the fact that for the
given minimum frequency of 1,000 we could generate only 5 of the 20 nouns using the
vicinity-based approach, we had to exclude the approach from this experiment.
We selected the Senseval-3 English lexical sample data set (Mihalcea, Chklovski,
and Kilgarriff 2004) as our manually sense-tagged corpus. The data set provides for
20 nouns of polysemy 3 to 10 an average number of 180 and 90 sense-tagged sentences in
its training and test sets, respectively. We generated the similarity-based and TS-based
pseudowords corresponding to these 20 nouns, as well as a set of 20 random pseudo-
words. For each set of these pseudowords we generated corresponding pseudosense-
annotated training and test data sets by randomly sampling distinct sentences from
the English Gigaword corpus (Graff and Cieri 2003). Therefore, we ended up with
four data sets, namely: the Senseval-3 data set of real words, and the three artificially
sense-tagged data sets for the similarity-based, TS-based, and random pseudowords.
Each of the artificially annotated data sets consisted of training and test portions
comprising the same number of instances per sense (i.e., the same sense distribution)
as that of the original Senseval-3 training and test data sets. Next, for each of our four
data sets, we trained a supervised WSD system on the training set and applied it to the
corresponding test set. In order to ensure more reliable results we follow Otrusina and
Smrz (2010) and report, for all experiments in this evaluation, the average results for
five runs. To this end, we randomly sampled the training and test data sets from the
combination of all items while preserving the original proportions. Also, in the random
setting, we provide the results averaged on a set of 25 different pseudowords modeling
a given ambiguous noun.
As our WSD system for this experiment, we used IMS (Zhong and Ng 2010), a
state-of-the-art supervised WSD system that is based on support vector machines (we
will describe IMS in more detail in Section 6.4). Note that we measure disambiguation
difficulty in terms of the system’s recall performance (cf. Section 6.6 for evaluation
measures).
We present in Figure 1 the scatterplot of the recall performance (hence the disam-
biguation difficulty) for real words vs. those for the similarity-based, TS-based, and
random pseudowords. For each set of pseudowords, we also show the line fitted to
the corresponding set of points by means of linear regression. Ideally, this line should
coincide with the dashed diagonal line in the figure, denoting perfect similarity. In
the next three subsections we provide an analysis of the scatter plot in Figure 1 and
a discussion.
4.1.1 Overall Disambiguation Difficulty. The closer a line of best fit is to the center of the
plot, the closer are its corresponding pseudowords to real words in terms of overall
</bodyText>
<page confidence="0.978823">
851
</page>
<figure confidence="0.998406333333333">
Computational Linguistics Volume 40, Number 4
Recall with pseudowords 100 Similarity Based
80
60
Random
TS Based
40
40 60 80 100
Recall with real words
</figure>
<figureCaption confidence="0.996749">
Figure 1
</figureCaption>
<bodyText confidence="0.998898444444445">
Scatterplot of the recall performance (hence the disambiguation difficulty) of real words versus
those for similarity-based, TS-based, and random pseudowords. The star shows the center of the
untruncated plot.
disambiguation difficulty (note that the plot’s axes are truncated to the range [40,100]
and the center point is shown by the star). As can be seen in Figure 1, the line corre-
sponding to our similarity-based pseudowords is the closest to the center, showing that
these pseudowords provide a better modeling of real words in terms of disambiguation
difficulty. We also show the corresponding values of recall performance in Table 7.
We can see from the table that the overall system performance of similarity-based
</bodyText>
<tableCaption confidence="0.876258">
Table 7
</tableCaption>
<bodyText confidence="0.964601431818182">
Recall performance of IMS on the 20 nouns of the Senseval-3 lexical-sample test set (Real
column) compared with the corresponding similarity-based (SB), TS-based (TS), and random
(Rnd) pseudowords. The last three columns show absolute differences between the real setting
and the three pseudoword settings.
Word Real SB TS Rnd |Real - SB ||Real - TS ||Real - Rnd|
argument 50.44 68.79 76.26 77.15 18.35 25.82 26.71
arm 92.30 85.69 85.69 88.11 6.61 6.61 4.19
atmosphere 70.52 69.15 84.75 80.44 1.37 14.23 10.32
audience 81.28 73.74 80.61 83.76 7.54 0.67 4.22
bank 85.76 83.07 80.00 82.46 2.69 5.76 3.99
degree 78.42 81.58 82.63 80.59 3.16 4.21 4.35
difference 62.46 61.43 76.86 75.17 1.03 14.40 12.90
difficulty 52.72 51.82 73.64 67.23 0.90 20.92 14.97
disc 78.62 76.48 69.45 78.07 2.14 9.17 6.18
image 71.78 75.76 73.03 81.50 3.98 1.25 10.02
interest 77.34 73.19 60.88 71.70 4.15 16.46 6.85
judgment 55.64 66.87 55.00 59.64 11.23 0.64 9.01
organization 80.36 72.86 73.57 78.65 7.50 6.79 3.65
paper 60.84 66.29 65.33 73.14 5.45 4.49 12.59
party 82.94 80.00 79.41 81.04 2.94 3.53 3.74
performance 58.56 64.76 67.14 73.86 6.20 8.58 15.52
plan 88.42 85.41 90.27 87.39 3.01 1.85 3.12
shelter 58.48 74.75 87.00 80.21 16.27 28.52 21.73
sort 67.64 88.15 80.74 77.37 20.51 13.10 9.73
source 63.46 67.74 70.97 66.26 4.28 7.51 7.03
overall 73.26 75.43 76.42 78.80 129.31 194.51 196.35
852
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
pseudowords (75.43) is closest to that of real words (73.26). This value is 76.42 and 78.80
for TS-based and random pseudowords, respectively.
In addition, for 12 of the 20 nouns, the similarity-based approach provides the
pseudowords that are closest to real words in terms of WSD recall performance
(|Real - SBI column in the table) as shown in bold in the table. This number drops
to 5 and 3 for the TS-based and random pseudowords, respectively. Accordingly, the
overall sum of the differences (distance) between the recall values is smallest (129.31)
for similarity-based pseudowords among the three kinds of pseudoword (194.51 for
TS-based pseudowords and an average of 196.35 for random pseudowords, ranging
from 158.32 to 262.04).
Even though TS-based pseudowords are only 1% away from similarity-based pseu-
dowords in terms of overall performance, their distance from real words is much higher
than that of similarity-based pseudowords (194.51 vs. 129.31). This suggests that the
former tend to have a lower correlation with real words than the latter. In the following,
we investigate the correlation between the disambiguation difficulties of real words and
our three types of pseudowords.
</bodyText>
<subsubsectionHeader confidence="0.641404">
4.1.2 Correlation Between Disambiguation Difficulties. The smaller the angular deviation of
</subsubsectionHeader>
<bodyText confidence="0.99833035483871">
the line of best fit for a set of pseudowords is from the diagonal line, the higher is the cor-
relation between the disambiguation difficulties of those pseudowords and real words.
As can be seen in the figure, the line corresponding to the similarity-based pseudowords
has the smallest deviation from the diagonal line, showing its higher correlation with
real words. The Pearson correlation coefficient between the disambiguation difficulties
of similarity-based pseudowords and real words is 0.74. This value drops to 0.43 and
0.54 for TS-based and random pseudowords, respectively. Even worse, the value of 0.54
is the average of 25 highly variable correlation values (in the range of [0.18, 0.67]) over
our 25 sets of random pseudowords. The reason why TS-based pseudowords show a
lower correlation than random pseudowords can be found in the fact that the reported
values for the latter are averaged over 25 runs. More precisely, the correlation value of
0.43 of topic signatures has to be compared with the range [0.18, 0.67] of correlations
obtained by different sets of random pseudowords.
4.1.3 Discussion. We leveraged PPR and topic signatures as our sense modeling compo-
nents for the generation of semantically aware pseudowords. Both approaches could
solve the low coverage problem, although the results presented in this section suggest
that the topic signature-based approach is not good at providing suitable monosemous
substitutes for senses of real ambiguous words. A closer look at the similarity-based
and TS-based pseudowords generated for some of the nouns in the Senseval-3 data set,
shown in Table 8, provides a clear explanation for this shortcoming of topic signature-
based pseudowords. In fact, topic signatures are based on co-occurrence information
from the Web snippets retrieved for each sense of an ambiguous noun. As a result, many
of the top-ranking words in each topic signature are syntagmatically related to the given
sense. For example, consider the paralysis pseudosense of arm, european pseudosense
of plan, and moral or weekly pseudosenses of paper. Despite being semantically related,
these pseudosenses cannot be considered as good substitutes for their correspond-
ing senses. Our similarity-based approach, instead, tends to favor paradigmatic (i.e.,
taxonomic) relations, which is, in fact, the reason behind its better ability at finding
suitable substitutes for senses of real words.
Given their significantly lower ability at modeling real words, the TS-based pseu-
doword generation approach cannot be considered as a candidate for the generation
</bodyText>
<page confidence="0.997308">
853
</page>
<note confidence="0.346334">
Computational Linguistics Volume 40, Number 4
</note>
<tableCaption confidence="0.990751">
Table 8
</tableCaption>
<bodyText confidence="0.6113625">
The similarity-based (SB) and TS-based (TS) pseudowords obtained for 6 of the 20 nouns used in
our disambiguation experiment.
</bodyText>
<table confidence="0.881076307692308">
word type: equivalent pseudoword
difficulty SB: workout*deterrent*predicament*complexity
TS: get*autism*ski*credibility
arm SB: forearm*baseball cap*sword*armchair*executive branch*garment
TS: paralysis*glue*weaponry*wheelchair*pension*clothing
plan SB: retirement plan*architect*diagram
TS: employee*european*pale
performance SB: concert*encore*achievement*feat*processing
TS: theatrical*musical*ballroom*recruitment*steady
party SB: political party*dinner party*clique*fiesta*someone
TS: socialist*prom*transaction*coronation*boomer
paper SB: piece of paper*papers*news story*telecom*editorial*publishing house*movie
TS: towel*moral*vitamin*brochure*weekly*firm*forecast
</table>
<bodyText confidence="0.998333">
of large-scale data sets for our experiments. Hence, we do not consider this type of
pseudoword in our further evaluations and focus on similarity-based pseudowords
only.
</bodyText>
<subsectionHeader confidence="0.992606">
4.2 Representative Power of Pseudosenses
</subsectionHeader>
<bodyText confidence="0.999753923076923">
In order to maximize the possibility of preserving the meaning of the original synset,
a pseudosense should be selected from the set of words in the same synset, or in
the directly related synsets (e.g., hypernym synsets). However, many of the WordNet
synsets do not contain monosemous terms and the similarity-based approach often
needs to look further into the other indirectly related synsets so as to find a suitable
pseudosense. In order to assess how often this happens, we carried out an experiment
to get a clear idea of the exact statistics on the distances of the synsets from which
pseudosenses are selected from the synsets containing the original senses. To this end,
we went through all our similarity-based pseudowords and, for each pseudosense
wi, checked the relationship in WordNet between the synset containing wi and the
corresponding real sense.
We show in Table 9 how the pseudosenses are distributed across different types of
WordNet relations, including indirect ones. As can be seen in the table, when minFreq
</bodyText>
<tableCaption confidence="0.974722">
Table 9
</tableCaption>
<table confidence="0.843970090909091">
Percentage of similarity-based pseudosenses obtained from different types of WordNet relations.
0 200 500 1,000
Synonyms 24.0 8.3 5.7 4.0
Hypernyms 30.6 17.9 14.2 11.4
Hyponyms 9.6 7.2 5.7 4.6
Meronyms 0.4 0.4 0.3 0.3
Siblings 9.7 17.9 17.2 16.2
Other indirect relations 25.7 48.3 56.9 63.5
minFreq
Relation
type
</table>
<page confidence="0.943354">
854
</page>
<note confidence="0.724447">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.935895">
Table 10
</tableCaption>
<figure confidence="0.936156766666667">
Examples for representativeness scores assigned by the two annotators to pseudosenses of the
term mosaic.
sense sense definition (in short) and synset
art consisting of a design made of small pieces
1
{mosaic}
viral disease in solanaceous plants
2
{mosaic}
a freeware browser
3
{mosaic}
a pattern resembling a mosaic
4
{mosaic}
transducer on a television camera tube
5
{mosaic}
arrangement of aerial photographs
6
{mosaic, arial mosaic, photomosaic}
average score
pseudosense score (1) score (2)
fine art 3 3
disease 4 3
web browser 4 4
knowledge 2 1
electronic equipment 3 3
photograph 3 4
3.17 3.00
</figure>
<bodyText confidence="0.999828464285714">
is set to 1, 000, only about 20% of the pseudosenses are picked out from synonyms or
generalization/specialization relations (hypernym and hyponyms). This shows that a
considerable portion of our pseudosenses are selected from synsets that are indirectly
related to the target synset that is being modeled. These indirectly related synsets can
potentially result in pseudosenses that do not have very similar meanings to the original
synsets, and hence are not good representatives of them.
Having observed this, we carried out an experiment to evaluate the representative
power of similarity-based pseudosenses to assess how well each pseudosense models
its corresponding real sense. For this purpose, we randomly sampled 10 pseudowords
for each degree of polysemy from 2 to 12 from the entire set of pseudowords6 generated
with minimum frequency of 1,000, totaling 110 pseudowords with 770 pseudosenses.
We then asked two annotators, neither of whom was an author of this paper, to judge
the representative power of each pseudosense according to the following scale: 1 (com-
pletely unrelated), 2 (somewhat related), 3 (good substitute), 4 (perfect substitute). The
annotators were provided with the WordNet definitions of the corresponding synsets.
As an example, consider the pseudowords corresponding to the noun mosaic shown
in Table 10. We present in the table the representativeness scores given by each of our
annotators to the individual pseudosenses of this word. The overall representativeness
score is calculated as the average of the scores given by the two annotators. In the case
of our example, the overall score is 3.085. We also calculated the Spearman correlation
between the scores given by the two annotators for all the 770 cases to be 0.66. We show
in Table 11 (top) the overall representativeness scores averaged for the full set of 770
pseudosenses, classified by polysemy degree. As can be seen from the table, the overall
representative score remains around 3.0 for all polysemy degrees from 2 to 12, with the
overall score being 3.1. This shows that even though about 64% of the pseudosenses
are picked out from indirect relations (when minimum frequency is 1,000, cf. Table 9),
they can still be considered as good representatives for their corresponding real senses.
We also present in Table 11 (bottom) the average representativeness scores only for
</bodyText>
<footnote confidence="0.8134625">
6 The set contained 15,935 pseudowords corresponding to all the polysemous nouns in WordNet 3.0
(see also Section 6.2).
</footnote>
<page confidence="0.993597">
855
</page>
<note confidence="0.346011">
Computational Linguistics Volume 40, Number 4
</note>
<tableCaption confidence="0.987074">
Table 11
</tableCaption>
<bodyText confidence="0.913189294117647">
Average representativeness scores for pseudosenses of different polysemy classes (scores
range from 1 to 4) and from different WordNet relations. We also show, in the last two rows, the
average scores for only those pseudosenses that are picked from synonyms or directly related
and sibling synsets.
Polysemy 2 3 4 5 6 7 8 9 10 11 12 overall
Overall score 3.3 3.4 3.1 3.1 2.9 3.1 2.9 2.8 3.3 3.1 3.3 3.1
Direct relations and siblings only 3.4 3.6 3.4 3.3 3.4 3.3 2.8 3.0 3.4 3.2 3.8 3.3
Synonyms only 4.0 – – 4.0 – 4.0 4.0 4.0 4.0 4.0 4.0 4.0
those pseudosenses that are picked from words in the same synset (synonyms) or in
the directly related and sibling synsets. As can be seen, the synonymous pseudosenses
are always rated with the highest possible score of 4, whereas those obtained from
direct relations maintain a relatively higher score compared with the overall repre-
sentative score, which includes many pseudosenses picked from indirect relations. In
fact, the similarity-based pseudoword generation approach improves the vicinity-based
method to full coverage and provides a significantly better level of flexibility for higher
values of minimum frequency, while maintaining a good degree of sense modeling
ability.
</bodyText>
<subsectionHeader confidence="0.999698">
4.3 Distinguishability Between Pseudosenses
</subsectionHeader>
<bodyText confidence="0.9998603">
A fundamental property of an ambiguous word is that its different senses have distinct
meanings. We expect a semantically aware pseudoword to inherit this property of its
real counterpart (i.e., to have pseudosenses that are semantically distinguishable from
each other while being semantically similar to their corresponding senses). As an exam-
ple, consider the similarity-based pseudoword philanthropist*benefactor7 corresponding
to the noun donor.8 The two pseudosenses of the pseudoword can be considered as
good representatives for their corresponding senses. However, the distinguishability
of the two real senses is not preserved in the corresponding pseudoword: whereas
philanthropist only applies to the first sense, benefactor can be equally good for both
senses of donor.
Hence, we performed another evaluation in order to determine the degree of the
distinguishability of pseudosenses of our pseudowords. For this evaluation, we used
the same set of 110 pseudowords as in the previous experiment (Section 4.2). For
each of these pseudowords, we presented its pseudosenses in random order to two
annotators. In addition, we provided these annotators with the WordNet definitions of
the senses of the corresponding noun and asked them to associate each pseudosense
with the most appropriate WordNet sense. The annotators were instructed to leave a
pseudosense unmapped if they found it to be equally mappable to multiple senses. We
then calculated the distinguishability score for each polysemy degree as the ratio of the
number of correct mappings to the total number of senses.
</bodyText>
<listItem confidence="0.58318">
7 From WordNet: “Philanthropist: someone who makes charitable donations intended to increase human
well-being”; “Benefactor: a person who helps people or institutions (especially with financial help).”
8 The term donor has two senses according to WordNet 3.0: (1) “person who makes a gift of property”;
(2) “(medicine) someone who gives blood or tissue or an organ to be used in another person (the host).”
</listItem>
<page confidence="0.980418">
856
</page>
<figure confidence="0.974332263157895">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
web browser
fine art
electronic equipment
photograph
disease
knowledge
art consisting of a design made of small pieces
{mosaic}
viral disease in solanaceous plants
{mosaic}
a freeware browser
{mosaic}
a pattern resembling a mosaic
{mosaic}
transducer on a television camera tube
{mosaic}
arrangement of aerial photographs
{mosaic, arial_mosaic, photomosaic}
</figure>
<figureCaption confidence="0.996133">
Figure 2
</figureCaption>
<bodyText confidence="0.999879714285714">
Mappings provided by an annotator from pseudosenses of the similarity-based pseudoword
for the noun mosaic to its real senses (as defined in WordNet 3.0). In this case all mappings are
correct and hence the distinguishability score for this pseudoword will be 6/6=1.
For instance, consider the noun mosaic that has six senses in WordNet 3.0. As we
also saw in the previous experiment, the corresponding similarity-based pseudoword
for this noun is fine art*disease*web browser*knowledge*electronic equipment*photograph. As
illustrated in Figure 2, we provided the shuffled list of pseudosenses of this pseu-
doword (left column) and the WordNet definitions of the senses of its corresponding
noun, namely, mosaic (right column), to each annotator and asked them to map each
pseudosense to its most suitable real sense. In this case, both annotators mapped all
pseudosenses to their correct senses; hence, the distinguishability score given by each
annotator for this pseudoword was 6/6 = 1.
We show in Table 12 the average distinguishability scores for each degree of poly-
semy 2 to 12 as well as the overall score that is calculated as the average of per-
polysemy scores. As can be seen in the table, the distinguishability score is inversely
proportional to the polysemy degree (there is a high negative Pearson correlation of 0.9
between the two). However, the score remains above 0.70 even for the pseudowords
with higher polysemous degrees. The overall score of 0.79 shows that a large portion
of pseudosenses can be associated with their corresponding real senses only. Therefore
we can conclude that the similarity-based pseudowords effectively preserve the distin-
guishability of senses of their real counterparts.
</bodyText>
<subsectionHeader confidence="0.932071">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999943">
We performed three experiments to evaluate the reliability of our pseudowords. We
showed that the similarity-based pseudowords are fairly close to their real counterparts
in terms of disambiguation difficulty. Even though our similarity-based pseudowords
were slightly easier to disambiguate in comparison to real words, the high correlation
observed in the first evaluation (Section 4.1) serves as a guarantee that our pseudowords
</bodyText>
<tableCaption confidence="0.937394">
Table 12
</tableCaption>
<bodyText confidence="0.4494455">
Average distinguishability scores for pseudosenses of different polysemy classes (scores range
from 0 to 1).
</bodyText>
<table confidence="0.9952915">
Polysemy 2 3 4 5 6 7 8 9 10 11 12 overall
Distinguishability score 0.90 0.83 0.83 0.82 0.81 0.77 0.75 0.73 0.80 0.71 0.70 0.79
</table>
<page confidence="0.557288">
857
</page>
<note confidence="0.294577">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.972625153846154">
can be reliable substitutes for real words in experiments concerning the analysis and
comparison of WSD systems.
Our further experiments provided manual evaluations of the representativeness of
individual pseudosenses of our similarity-based pseudowords as well as the distin-
guishability of their pseudosenses from one another. In the representativeness exper-
iment, we assessed, for each individual sense of each pseudoword in our sample set,
if the meaning of the corresponding real sense is preserved and if each pseudosense
can be considered as a good representative of its corresponding real sense. Finally, in
the distinguishability experiment our aim was to investigate the ability of similarity-
based pseudowords at preserving the distinguishability among senses of real words.
Experimental results proved that the similarity-based approach is able to provide a good
modeling of individual senses of real words while preserving the distinguishability of
their senses.
</bodyText>
<sectionHeader confidence="0.930001" genericHeader="method">
5. Sampling Pseudosense-Tagged Corpora
</sectionHeader>
<bodyText confidence="0.99998275">
As a result of our evaluations we know that the similarity-based pseudowords are
reliable substitutes for real ambiguous words in the disambiguation task. As described
in Section 3, a pseudosense-tagged corpus can be generated for each pseudoword
p = w1 * w2 * ... * wn by substituting individual occurrences of its pseudosenses wi
with the pseudoword p itself, while marking the pseudosense wi as its annotation. An
obvious question that arises here is how to sample and distribute the sentences for a
pseudoword across its pseudosenses. In the following two sections we illustrate two
corpus sampling strategies used in our experiments.
</bodyText>
<subsectionHeader confidence="0.989479">
5.1 Uniform Sense Distribution
</subsectionHeader>
<bodyText confidence="0.99997875">
A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense
distribution. In this setting, all senses of a pseudoword are assumed to be observed with
equal probability in the tagged corpus (i.e., we extract the same number of sentences
from the corpus for each pseudosense of a given pseudoword).
</bodyText>
<subsectionHeader confidence="0.996469">
5.2 Natural Sense Distribution
</subsectionHeader>
<bodyText confidence="0.999979933333333">
Although the uniform distribution can be useful in specific applications such as dictio-
nary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource
mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural
text we know that most of the occurrences of an ambiguous word correspond to a
usually small subset of predominant senses of that word (Zipf 1949; Sanderson and
Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real
text are usually distributed across its senses according to a highly skewed distribu-
tion. In order to model this natural distribution, we adopt a distribution sampling
strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993),
the largest sense-tagged corpus of English. However, as we show in Table 13, Sem-
Cor provides reliable distribution estimates for only some hundred words. The table
shows for each degree of polysemy the number of distinct nouns in SemCor that are
sense-annotated at least once, 10 times, or 20 times, compared with the correspond-
ing total number of ambiguous nouns in WordNet (last column in the table). Be-
cause we could obtain from SemCor the sense distribution of only some hundred
</bodyText>
<page confidence="0.994618">
858
</page>
<note confidence="0.865879">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.995549">
Table 13
</tableCaption>
<table confidence="0.993938142857143">
Number of distinct nouns annotated in SemCor at least 1, 10, or 20 times. We also show the total
number of WordNet ambiguous nouns (last row) for different polysemy degrees.
Polysemy 2 3 4 5 6 7 8 9 10 11 12 &gt;12 total
Frequency &gt; 1 2,349 1,315 703 453 241 183 81 91 57 45 23 49 5,590
Frequency &gt; 10 301 242 196 180 122 97 61 60 43 35 20 43 1,400
Frequency &gt; 20 122 97 112 111 70 63 47 42 29 24 15 35 767
WN amb. nouns 10,257 2,989 1,178 620 306 212 96 94 60 48 25 50 15,935
</table>
<tableCaption confidence="0.987254">
Table 14
</tableCaption>
<table confidence="0.997776785714286">
Average sense distribution for nouns in SemCor. We select only those nouns for which there exist
at least 10 sense-tagged occurrences in SemCor.
Poly. p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12
2 87.8 12.2
3 78.1 17.9 4.0
4 72.9 19.6 6.4 1.1
5 71.1 18.8 7.4 2.3 0.4
6 65.4 21.5 8.5 3.1 1.2 0.3
7 61.1 23.0 9.5 4.1 1.8 0.4 0.1
8 62.7 20.9 9.3 4.4 1.8 0.6 0.2 0.1
9 56.0 22.4 10.4 5.6 3.4 1.6 0.6 0.0 0.0
10 51.1 24.4 11.7 7.0 3.6 1.4 0.6 0.2 0.0 0.0
11 50.7 23.5 12.0 6.8 3.6 1.7 1.0 0.4 0.2 0.1 0.0
12 54.3 18.2 9.1 7.2 4.4 2.7 2.1 1.4 0.5 0.1 0.0 0.0
</table>
<bodyText confidence="0.999506333333333">
low-polysemy nouns and a few dozen high-polysemy nouns, we decided to drop the
requirement of estimating sense distributions directly (i.e., to model the semantically
aware pseudoword pw on the sense distribution of w). Instead, we first collected all the
sense distributions of nouns with at least 10 occurrences in SemCor. Our choice of 10 as
the minimum occurrence frequency was to guarantee some hundreds of distributions
for lower polysemy degrees and dozens for the higher ones (see Table 13). In addition,
given the highly skewed nature of sense distributions in SemCor, 10 samples should
usually be enough for a reliable estimation of the corresponding sense distributions
to be made, even for higher polysemy degrees. Having at hand a large set of distri-
butions estimated for each polysemy degree, every time we needed a new pseudo-
word with m senses, we randomly picked out a sense distribution of size m from our
collection.
We show the macro-averaged9 sense distribution for each polysemy degree from
2 to 12 in Table 14. As can be seen from the table, all average distributions, especially
those of low polysemy nouns, are skewed towards predominant senses.
</bodyText>
<listItem confidence="0.453169">
9 There are some outliers in SemCor that would have negatively affected the average distributions if
</listItem>
<bodyText confidence="0.750221">
micro-averaging was used. For instance, the three-sense word person has over 6,000 instances, all of
which are tagged with the first sense. This would bias the micro-averaged sense distribution given that
there exist approximately an overall 17,000 instances for three-sense nouns. For our sampling strategy,
though, we do not need distribution averaging.
</bodyText>
<page confidence="0.991976">
859
</page>
<note confidence="0.640161">
Computational Linguistics Volume 40, Number 4
</note>
<sectionHeader confidence="0.994742" genericHeader="method">
6. Experimental Set-up
</sectionHeader>
<bodyText confidence="0.999990142857143">
In this article up to this point we have provided the basis for creating large-scale
pseudosense-annotated data sets by proposing a flexible approach for generating se-
mantically aware pseudowords that model arbitrary real words. We have also ex-
plained different sampling strategies for distributing pseudosense-annotated sentences
according to two different distributions. We are now ready to set up our experimental
framework for large-scale WSD.
We first describe the text corpus used in our experiments (Section 6.1), then explain
how we selected a reliable subset of pseudowords for the experiments (Section 6.2);
this is followed by a description of the process of generating training and test data sets
(Section 6.3). In Section 6.4 we introduce the two WSD systems used as representatives
of the two main WSD paradigms (i.e., supervised and knowledge-based) in our experi-
ments. We then provide, in Section 6.5, the details of the method through application of
which our knowledge-based system is able to benefit from the training data. Finally, in
Section 6.6 we describe the evaluation measures used in our experiments.
</bodyText>
<subsectionHeader confidence="0.977658">
6.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999926">
We sampled all the sentences for pseudosense tagging from the English Gigaword cor-
pus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The cor-
pus comprises about 4.1 million documents, each containing an average of 430 words,
totaling approximately 1.76 billion words. In a preprocessing phase, we removed sen-
tences whose length was either longer than 50 words or shorter than 10 words. The
corpus was then annotated with part-of-speech tags using the C&amp;C tagger (Curran and
Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus
contained around 50 million sentences.
</bodyText>
<subsectionHeader confidence="0.997958">
6.2 Pseudoword Selection
</subsectionHeader>
<bodyText confidence="0.999221631578947">
As a result of our similarity-based approach, we could generate as many pseudowords
as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two
reasons that will be explained shortly, we only considered a reliable subset of these
pseudowords for generating the data sets for our experiments.
Firstly, we did not consider nouns with polysemy degree higher than 12 in our
experiments, as it is not possible to perform a reliable analysis on such degrees given
that very few pseudowords can be generated for them (about 0.3% of ambiguous
nouns in WordNet have polysemy degree 13 or higher). Secondly, we observed that in
practice a large enough portion of pseudowords for each polysemy degree can provide
a reliable performance estimation on that polysemy degree. Therefore, we selected, for
each polysemy degree, the top 300 pseudowords according to the calculated averageRank
score (cf. Section 3.2). Given that the score denoted our confidence in the preservation
of meaning while modeling pseudosenses, this top-ranking subset of pseudowords
is the most reliable one. Table 15 (first row) shows the distribution of this subset of
pseudowords across different degrees of polysemy. Note that for polysemy degrees 6 to
12, where there exist less than 300 nouns in WordNet, we consider all the corresponding
pseudowords. In Appendix A, we show that this subset is large enough for an accurate
estimation of the performance of a WSD system. We also sampled a separate set of
199 pseudowords for tuning purposes (cf. Section 6.5.1 for tuning). Table 15 (second
</bodyText>
<page confidence="0.990596">
860
</page>
<note confidence="0.862969">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.995252">
Table 15
</tableCaption>
<table confidence="0.9846185">
Number of pseudowords per degree of polysemy (2 to 12) in our test and tuning sets.
Polysemy 2 3 4 5 6 7 8 9 10 11 12 Total
Test set 300 300 300 300 278 192 84 87 54 43 22 1,960
Tuning set 30 30 30 30 28 19 9 9 6 5 3 199
</table>
<bodyText confidence="0.853478">
row) shows the distribution of this tuning set of pseudowords across different polysemy
degrees.
</bodyText>
<subsectionHeader confidence="0.996894">
6.3 Generating Data Sets
</subsectionHeader>
<bodyText confidence="0.999893625">
Data set size. The first question that comes to mind before generating data sets is that
of the number of sentences to be pseudosense-tagged for each pseudoword. As we
showed in Section 3.2 (Table 5), the minimum occurrence frequency (minFreq, which
corresponds to the number of sentences to be tagged with a particular pseudosense)
directly affects the averageRank score, a measure that we interpreted as our confidence in
the preservation of meaning of a real sense through its corresponding pseudosense. This
suggests a trade-off between the scale of our experiments and their overall accuracy. We
show in Table 16 the statistics of the averageRank score for the subset of pseudowords
selected for our experiment when six different values of minFreq were assumed while
generating pseudowords. Currently, the MASC corpus (Ide et al. 2010), even though
covering a small set of 20 nouns, provides the highest number of manually annotated
instances per word, namely, 1,000 sentences. In our experiments we followed MASC
and generated 1,000 annotated instances for each of our 1,960 pseudowords. As can be
seen from Table 16, when minFreq = 1,000, a pseudosense is on average selected from the
8.4th position in the similarSynsets list (given by mean), with most of them being picked
out from the first position (given by mode).
</bodyText>
<tableCaption confidence="0.847804">
Table 16
</tableCaption>
<bodyText confidence="0.817500833333333">
Statistics of the averageRank score of the subset of pseudowords selected for our experiments:
we show mean and mode statistics for six different values of minimum occurrence frequency
(minFreq) and for each polysemy degree (average value is presented in the case of multiple
modes).
minFreq 0 200 500 1,000 2,000 5,000
poly. mean mode mean mode mean mode mean mode mean mode mean mode
</bodyText>
<table confidence="0.950357083333333">
2 1.0 1.0 1.0 1.0 1.1 1.0 1.2 1.0 1.5 1.0 1.9 2.0
3 1.0 1.0 1.6 2.0 2.3 2.7 2.8 3.2 3.5 4.0 5.4 7.7
4 1.0 1.0 2.4 3.0 3.8 5.3 5.2 7.1 6.8 7.4 11.6 14.8
5 1.4 1.0 3.8 5.0 6.2 5.6 8.8 13.6 12.5 14.4 20.8 18.2
6 2.1 2.0 6.4 4.0 10.2 3.8 14.8 12.2 20.8 17.2 35.8 26.0
7 2.0 2.0 6.4 6.0 10.1 8.4 14.0 7.7 20.3 18.9 33.7 27.3
8 1.9 2.0 6.5 4.0 10.1 9.8 15.9 13.5 23.9 21.9 37.9 42.6
9 2.0 2.0 6.3 4.0 10.7 12.4 15.4 17.6 22.6 30.2 36.0 37.9
10 2.0 2.0 5.7 5.0 9.2 7.9 13.3 15.0 20.9 19.2 31.9 31.9
11 2.1 2.0 6.8 8.0 10.8 13.6 13.9 11.5 20.0 7.5 32.0 35.7
12 2.2 2.0 5.6 4.0 10.5 10.5 14.3 23.3 21.7 21.7 37.0 32.5
overall 1.5 1.0 3.8 1.0 6.0 1.0 8.4 1.0 11.9 2.0 19.7 2.0
</table>
<page confidence="0.943043">
861
</page>
<figure confidence="0.9970464">
Computational Linguistics Volume 40, Number 4
Distribution
80
70
60
50
40
30
20
10
0
natural
average-cross-domain
1 2 3 4 5 6 7 8 9 10 11 12
Sense ranking by predominance
</figure>
<figureCaption confidence="0.942713">
Figure 3
</figureCaption>
<bodyText confidence="0.996820828571429">
Sense distribution in a naturally distributed text as well as the average of sense distributions
across different domains.
Data set configurations. In addition to being large-scale and accurate, we also wanted
our experiments to cover a wide range of possible real-world scenarios. In Section 5,
we identified two different sense distributions according to which we could produce
pseudosense-tagged corpora, namely, the uniform distribution and the natural one.
In our experiments, we considered all four possible ways of combining the sense
distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform
(Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the
following rationale for each of them:
Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig
2000; McCarthy et al. 2004), in which senses are naturally distributed according to the
same distribution both in the training and the test data sets.
Nat-Uni. This configuration trains a WSD system with a natural distribution but applies
it to texts for which the distribution is unknown (e.g., in different domains). Given
that any choice of a different sense distribution for the test data set would have been
arbitrary, we selected the uniform one as the approximate average of sense distribu-
tions across different domains. In other words, our assumption was that the uniform
distribution could be thought of as the fairest different distribution. To verify this, we
studied the variability of sense distribution across texts belonging to different domains.
We started from a data set of sense-annotated documents from 30 different domains
provided by Faralli and Navigli (2012). We then estimated the average sense distribu-
tion of all nouns across documents, shown in Figure 3 (light columns) for polysemy
degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average
sense distribution across domains is not skewed, in contrast to the natural sense dis-
tribution (dark columns in the figure). In addition, this configuration models a setting
in which the system is not effectively provided with knowledge of all senses in the
test set.
Uni-Uni. This configuration assumes a system with the same amount of knowledge for
all senses, tested on a task in which all senses are equally important. As is also the case
for the Nat-Uni configuration, the uniformly distributed test set in Uni-Uni also models
tasks such as dictionary disambiguation, in which sense-wise precision matters (Flati
and Navigli 2012).
Uni-Nat. Similarly to Uni-Uni, this configuration takes no stand on the training sense
distribution, but tests it on naturally distributed data.
</bodyText>
<page confidence="0.960835">
862
</page>
<note confidence="0.583529">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<bodyText confidence="0.9992959">
Data set split. We created our training and test sets by sampling 1,000 sentences per
pseudoword from the Gigaword corpus for each of the two sense distributions (i.e.,
natural and uniform). Out of these sentences, we kept 200 (i.e., 20%) as a test set
and used the remaining 800 (i.e., 80%) for training. In order to be able to analyze
the impact of the amount of knowledge on the disambiguation performance, the 800
sentences in the training data set were split into 10 different subsets of varying size
(from 80 sentences [i.e., 10% of training instances] to the full set of 800 sentences in
10 steps) while at the same time preserving the original sense distribution for all these
sets. Overall, the data sets comprised about 2 million10 pseudosense-tagged sentences
for each of the four configurations.
</bodyText>
<subsectionHeader confidence="0.920209">
6.4 Systems
</subsectionHeader>
<bodyText confidence="0.977753827586207">
We chose state-of-the-art off-the-shelf representatives for the two mainstream WSD
paradigms, that is, supervised and knowledge-based WSD.
6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and
Ng 2010) as the representative supervised WSD system. IMS is a publicly available
English all-words WSD system achieving state-of-the-art results on several Senseval
and SemEval tasks.11 The system classifies words in context using linear support vector
machines. The context (a sentence in our case) is represented as a standard vector of
features including parts of speech, surrounding words, and local collocations (Lee and
Ng 2002).
For each of the four configurations (see Section 6.3) and for each pseudoword, IMS
was trained with the corresponding training set and the learned word expert model
was then applied to the test set. In our experiments, we used the default configuration
of IMS where the system adopts a linear SVM classifier with L2-loss function.
6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we
used UKB.12 UKB is a publicly available graph-based WSD system that exploits a pre-
existing lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB pro-
vides an implementation of the PPR algorithm (Haveliwala 2002), adapted to the task of
WSD, as proposed by Agirre and Soroa (2009). PPR is applied to a graph representation
of a Lexical Knowledge Base (LKB), which is typically WordNet or an extension of it
with additional semantic edges. We used the w2w variant, which has been shown to
perform best (Agirre and Soroa 2009), where PPR is initialized by concentrating the
probability mass on the context words other than the target word to be disambiguated.
The most suitable sense of the latter is then chosen by selecting the highest-ranking
vertex (i.e., sense) of the word.
Similarly to IMS, we used for UKB the corresponding training set in each training–
test configuration. However, a typical knowledge-based WSD system (such as UKB)
cannot directly learn from the training data (which, instead, is naturally suited to
supervised WSD systems). In the following section we describe the method used in
our experiments to transfer these data into readily available knowledge for UKB.
</bodyText>
<footnote confidence="0.924725333333333">
10 1,000 sentences × (1,960+199) pseudowords.
11 http://nlp.comp.nus.edu.sg/software/ims/.
12 http://ixa2.si.ehu.es/ukb/.
</footnote>
<page confidence="0.995623">
863
</page>
<note confidence="0.544733">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.999294333333333">
Hereafter, we will use IMS and UKB to mean supervised and knowledge-based
systems, respectively, since we consider these two systems as state-of-the-art represen-
tatives of their corresponding paradigms.
</bodyText>
<subsectionHeader confidence="0.994832">
6.5 Enriching the LKB Using Training Data
</subsectionHeader>
<bodyText confidence="0.999816666666667">
Whereas supervised WSD exploits a training set to perform sense classification,
knowledge-based approaches use lexical knowledge bases instead. Therefore, a simi-
lar operation to that of providing an increasingly large training set is to enrich basic
knowledge bases such as WordNet with additional semantic edges, as has previously
been done, among others, by Navigli (2005), Cuadros and Rigau (2008), and Navigli and
Lapata (2010). The automatic knowledge injection step, however, is less immediate and
natural than the supervised one. In fact, pseudowords cannot be directly used to obtain
a ready-to-use set of relation edges. To cope with this issue, in each configuration we
used the corresponding training set (on which IMS was trained) to extract knowledge
that could be used to enrich the WordNet LKB.13 To this end, given a pseudoword p and
for each pseudosense wi ∈ p, we identified the most semantically related words w&apos; to wi
using the Dice coefficient:
</bodyText>
<equation confidence="0.9987">
2c(wi, w&apos;) (1)
c(wi) + c(w&apos;)
</equation>
<bodyText confidence="0.991001541666667">
where c(wi,w&apos;) is the number of sentences in which wi and w&apos; co-occur, and c(wi) and
c(w&apos;) are the total number of sentences containing individual occurrences of wi and w&apos;,
respectively. We then connect, in the WordNet graph, wi to all the senses of each of the
top-K related words. Ideally, the corpus used for calculating these statistics should be
fully sense-tagged, namely, each usage of an ambiguous co-occurring word tagged with
the intended sense. However, because our training data (as is customary for WSD lexical
sample data sets) do not provide sense annotations for context words, these edges are
semi-noisy in that we connect an unambiguous endpoint wi to all senses of w&apos;.
As an example, consider the pseudosense wi = airplane, which is directly linked to
40 other nodes (synsets) in WordNet: 15 hyponyms, 10 meronyms, 14 domain-related
synsets, and a hypernym. We show 10 of these connections in Figure 4 (dashed lines). By
exploiting the sentences tagged with pseudosense airplane in the training set, we obtain
the list of K top-ranking semantically related words using the above-mentioned pro-
cedure. We then connect in the WordNet graph airplane1n to every sense of these words
(we show 10 such new linkings in the figure). The highlighted nodes in the figure are the
new direct neighbors of airplane1n in the enriched graph. As can be seen in the example,
our enrichment approach provides many additional syntagmatic relations to the initial
mostly paradigmatic relations in WordNet. As a result of this enrichment procedure,
we are able to generate a LKB consisting of WordNet plus semi-noisy semantic edges
obtained from the co-occurrence statistics of each pseudosense of our pseudowords.
6.5.1 Tuning. Although we described the method used in our experiments to obtain
new semantic edges with the help of co-occurrence statistics, we did not show how
we set the value of K—that is, the number of top-ranking related words we obtain
from a given set of n pseudosense-tagged sentences to be used for LKB enrichment.
</bodyText>
<page confidence="0.9111545">
13 We used the WordNet 3.0 LKB provided in UKB.
864
</page>
<note confidence="0.758981">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<figureCaption confidence="0.985264">
Figure 4
</figureCaption>
<bodyText confidence="0.980443666666667">
Enriching the WordNet LKB by adding edges between airplane and its top-K most similar words
(as obtained from the corresponding training set). For brevity, we show only a small part of
nodes here (airplane is connected to 40 synsets in WordNet). The dashed lines correspond to
existing edges in WordNet and solid lines represent the new additional edges. The highlighted
nodes are the new direct neighbors of airplane in the enriched LKB graph.
To calculate the optimal value of K, we carried out a tuning experiment on a data set
built for our subset of 199 pseudowords dedicated for tuning (cf. Section 6.2). In order
to consider the data set size factor in our tuning, we experimented on three different
sizes of training data: 80, 400, and 800 sentences (first, middle, and last size steps). For
each of these training data set sizes, we generated LKBs for different values of K and
carried out disambiguation on the tuning test set.
Figure 5 shows how the UKB recall performance (for more details on our evalua-
tions measure see Section 6.6) varies when the value of K is varied from 25 to 800 (in
increasing steps of 25). We show in the figure the average performance value for the
three training sizes (i.e., 80, 400, and 800). As can be observed from the figure, recall
</bodyText>
<figure confidence="0.855458">
100 200 300 400 500 600 700 800
Number of top-ranking related words used to obtain additional edges (IQ
</figure>
<figureCaption confidence="0.4901275">
Figure 5
UKB recall performance when varying the number of top-ranking related words used for the
LKB enrichment (K) from 25 to 800 (in 32 steps). We show the average performance over three
sizes of training data: 80, 400, and 800 sentences.
</figureCaption>
<figure confidence="0.999288214285714">
Recall
0.68
0.66
0.64
0.62
0.58
0.56
0.54
0.52
0.6
Nat-Uni
Uni-Nat
Nat-Nat
Uni-Uni
</figure>
<page confidence="0.737086">
865
</page>
<note confidence="0.402606">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.9996141875">
is not always directly proportional to the number of additional edges. In fact, after
a certain point recall starts to decay as the number of additional edges increases. We
present in Table 17 the corresponding values for a part of Figure 5 (i.e., K in the range [25,
300]), where the optimal recall value seems to occur for all four data set configurations.
As can be seen in the table, the best performance occurs at K = 125 for Uni-Uni and
Nat-Nat configurations. However, the best performance is seen at K = 75 and K = 150
for Uni-Nat and Nat-Uni configurations, respectively.
Given that for the supervised system we did not perform any tuning based on the
sense distribution of the test data set, in order to enable a fair comparison we chose the
same optimal K value irrespective of the test data. The last two rows in the table show
the average performance for the two distributions of training data (for instance, “Uni-*”
stands for the average performance over Uni-Uni and Uni-Nat configurations). It can be
seen that the maximum average performance occurs at K = 125 for the Uni training data
and at K = 150 for the Nat training data. Therefore, depending on the sense distribution
of training data, we used two different cutting thresholds (K) on the number of related
words considered for enriching the corresponding LKB.
</bodyText>
<subsectionHeader confidence="0.950246">
6.6 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999996363636364">
It is customary in the WSD literature to evaluate the performance of a disambiguation
system based on precision, recall, and F1 measure (Navigli 2009). Precision calculates
the portion of items that are correctly disambiguated from among the total output by the
system, and recall measures the portion of the total items in the data set that are correctly
disambiguated by the system. F1 is the harmonic mean of precision and recall. Because
in our setting all the pseudowords to be disambiguated in the test set are covered in the
training data and also included as a node in the LKB, IMS and UKB always provide an
answer for each item in the test set. For such a full-coverage case, the values of precision,
recall, and F1 will be equal. Hence, in our experiments, we report the recall performance
of the systems only. In addition, throughout this article, we present the results in terms
of recall percentage (i.e., the value of recall multiplied by 100).
</bodyText>
<sectionHeader confidence="0.857963" genericHeader="method">
7. Experiments and Results
</sectionHeader>
<bodyText confidence="0.999805333333333">
As discussed in the experimental set-up, WSD experiments were carried out with IMS
and UKB while injecting an increasingly higher amount of supervision and knowledge,
respectively, that is, from 0 to 800 training sentences (cf. Section 6.3). We show the overall
</bodyText>
<tableCaption confidence="0.83841625">
Table 17
Recall performance of UKB when varying K for all the four data set configurations. The last two
rows show the average performance for each training data distribution. The maximum values
for each configuration are shown in bold.
</tableCaption>
<table confidence="0.997783285714286">
edges (K) 25 50 75 100 125 150 175 200 225 250 275 300
Uni-Uni 57.65 58.54 59.27 59.53 59.71 59.54 59.35 59.06 58.90 58.63 58.50 58.48
Nat-Nat 64.32 65.42 65.86 65.93 66.09 66.07 66.07 66.07 65.87 65.78 65.58 65.42
Uni-Nat 59.28 59.87 60.36 60.33 60.26 60.28 60.04 60.02 59.90 59.86 59.68 59.65
Nat-Uni 55.01 56.08 56.52 56.84 57.05 57.14 57.11 57.11 57.00 56.93 56.67 56.46
Uni-* 58.47 59.20 59.82 59.93 59.99 59.91 59.69 59.54 59.40 59.24 59.09 59.06
Nat-* 59.66 60.75 61.19 61.38 61.57 61.61 61.59 61.59 61.43 61.35 61.13 60.94
</table>
<page confidence="0.936495">
866
</page>
<note confidence="0.838524">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.996499">
Table 18
</tableCaption>
<table confidence="0.983758777777778">
Performance of IMS and UKB on the naturally distributed test set when varying the size of the
training set per pseudoword (MFS for Nat-Nat = 70.5%). Note that we do not report an MFS
baseline for Uni-Nat configuration as there is no most frequent sense in the training data.
Config. Train System Size of training data
0 80 160 240 320 400 480 560 640 720 800
Nat-Nat Nat IMS – 81.9∗ 84.4 86.4 87.4 88.2 88.7 89.3 89.7 90.0 90.3*
UKB 38.8 56.50 59.2 61.1 62.1 62.8 63.4 63.8 64.2 64.4 64.60
Uni-Nat Uni IMS – 59.8 66.3 69.8 72.2 74.0 75.2 76.3 77.2 77.9 78.6
UKB 38.8 53.6 55.2 56.4 57.5 58.4 59.1 59.7 60.1 60.5 60.8
</table>
<tableCaption confidence="0.998938">
Table 19
</tableCaption>
<bodyText confidence="0.936092">
Performance of IMS and UKB on the uniformly distributed test set when varying the size of
the training set per pseudoword (MFS = 25.0%). Note that for Uni-Uni the MFS baseline is
not affected by the choice of the most frequent sense in the training data.
</bodyText>
<table confidence="0.975077">
Config. Train System Size of training data
0 80 160 240 320 400 480 560 640 720 800
Nat-Uni Nat IMS – 35.7∗ 39.0 41.1 42.5 43.8 44.6 45.4 46.0 46.6 47.1*
UKB 38.8 52.30 54.4 55.7 56.5 57.1 57.4 57.8 58.1 58.4 58.60
Uni-Uni Uni IMS – 59.8 66.4 70.0 72.5 74.3 75.5 76.6 77.5 78.2 78.9
UKB 38.8 53.9 55.4 56.5 57.6 58.6 59.3 59.9 60.3 60.7 61.0
</table>
<bodyText confidence="0.999381307692308">
recall performance of both systems on natural and uniform test sets in Tables 18 and
19, respectively.14 Note that in each table the training set can also be either uniformly
or naturally distributed, resulting in an overall four training–test configurations for
each system in the two tables. For each configuration, we show the recall performance
values as we vary the size of the corresponding training set from 0 to 800 sentences
per pseudoword (whereas the size of the test set, which comprises 200 sentences per
pseudoword, is the same across different training sizes, cf. Section 6.3). For 0 training
size, we only show the results of UKB, which is merely based on the vanilla WordNet
LKB.
The Most Frequent Sense (MFS) baseline values for the naturally and uniformly
distributed test sets are 70.5% and 25.0%, respectively. The best recall performance
(among the two systems) in each training–test configuration and for each size of the
training data set is shown in bold.
</bodyText>
<subsectionHeader confidence="0.998703">
7.1 General Overview of the Results
</subsectionHeader>
<bodyText confidence="0.9998985">
We observe that IMS has a considerably larger performance variation across different
configurations (ranging from 35.7%∗ to 81.9%∗ with 80 training sentences, and from
47.1%* to 90.3%* with 800), whereas UKB is less sensitive to training and test distri-
butions (52.3%°–56.5%° with 80 training sentences, and 58.6%0–64.6%0 with 800). The
</bodyText>
<page confidence="0.895463">
14 Symbols in the tables are for easier referencing.
867
</page>
<note confidence="0.533992">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.9987138">
performance of IMS (Nat-Nat) with 160 training sentences is in line with competitive
results on the Senseval-3 lexical sample data set (Mihalcea, Chklovski, and Kilgarriff
2004) in which there exist around 180 training sentences for each noun on average. In
fact the latter are in the 73% ballpark against an MFS recall of 55.2% (Zhong and Ng
2010), whereas IMS obtains 84.4% against 70.5% MFS in our setting. The 15% shift in
MFS is due to the sense distribution of our Nat data set, which is more skewed towards
frequent senses compared to that of the Senseval-3 lexical sample data set. In addition,
the average polysemy degree of our pseudowords is slightly lower than that of the
Senseval-3 nouns (average polysemy 5.1 of pseudowords vs. 5.8 of Senseval-3 nouns),
which also contributes to higher recall in our experiments.
</bodyText>
<subsectionHeader confidence="0.999607">
7.2 Corroboration of Previous Findings on a Large Scale
</subsectionHeader>
<bodyText confidence="0.99990975">
Before moving to a detailed analysis and discussion of our results, we briefly report
here on the results of the experiments that were conducted in order to confirm some of
the previous findings in the literature on a large scale. We provide the details of these
experiments in Appendix B. In summary, we were interested in verifying:
</bodyText>
<listItem confidence="0.671943375">
• The relation between system’s performance and polysemy degree:
our results confirm previous findings by Palmer, Dang, and Fellbaum
(2007) that the two are inversely proportional. We also found IMS
to be particularly robust on highly polysemous words in the Nat-Nat
configuration. The inverse proportionality was approximately logarithmic
in all system configurations except for IMS in the Nat-Nat configuration
in which the proportionality was linear (Appendix B.1).
• The relation between connectivity of a node in the LKB and
</listItem>
<bodyText confidence="0.997662142857143">
disambiguation accuracy: we found that the higher the connectivity of a
node, the more accurate will be its disambiguation. This corroborates
the preliminary findings of Navigli and Lapata (2010) on a much larger
scale (Appendix B.2).
Previous work has made claims only on significantly smaller amounts of annotated
data (e.g., in Navigli and Lapata 2010), whereas we show for the first time that these
hold in large-scale experiments with several orders of magnitudes more annotated data.
</bodyText>
<subsectionHeader confidence="0.997064">
7.3 UKB Largely Benefits from Semi-Noisy Edges
</subsectionHeader>
<bodyText confidence="0.9999633">
Thanks to our framework, we can go into considerably greater detail on the second
point that we verified in Section 7.2. As can be seen in Tables 18 and 19, the enrichment
of the WordNet LKB proves to be highly beneficial. The performance of UKB increases
significantly, even when the edges are harvested from a low number of training sen-
tences, which is particularly impressive because the added edges are semi-noisy. In fact,
recall grows, with Nat test, from 38.8% to 56.5% (Nat-Nat) and 53.6% (Uni-Nat), and,
with Uni test, from 38.8% to 52.3% (Nat-Uni) and 53.9% (Uni-Uni), when using just
80 training sentences per pseudoword. The impact of semi-noisy edges is even higher
with more training data, ranging between +19.8% and +25.8% improvement when the
full set of 800 training sentences is used.
</bodyText>
<page confidence="0.991309">
868
</page>
<note confidence="0.723049">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<subsectionHeader confidence="0.922667">
7.4 Performance of the Systems in Different Configurations: IMS Leads
</subsectionHeader>
<bodyText confidence="0.999761428571429">
Except in Nat-Uni, IMS outperforms UKB in all other configurations. The gap is particu-
larly evident in the Nat-Nat configuration, which is the typical setting for lexical sample
WSD tasks. The performance of UKB is closer to that of IMS for smaller sizes of training
data irrespective of the configuration. The gap, however, expands with the growth in
the number of sentences in the training set. This shows that the learning rate of IMS is
faster than that for UKB.
Nat-Uni is the only configuration in which UKB surpasses IMS. The low per-
formance of IMS shows that providing enough training instances for all senses is
always beneficial for IMS, and this happens in the Nat-Nat, Uni-Uni, and Uni-Nat
configurations.
IMS benefits from two advantages in the Nat-Nat configuration: (1) It is aware of
the sense distribution in the test set and (2) Due to the skewed sense distribution in this
configuration, often some of the senses of a word are not covered in the training data
(and in the test set as well). This reduces the number of classes in the classification
task, making it a potentially easier task to carry out. We will talk more about the
first point in Section 7.6. The second point, namely, the partial coverage of senses,
even though making the disambiguation an easier task in the Nat-Nat configuration,
is responsible for the very low performance of IMS on the uniformly distributed test set
(i.e., Nat-Uni configuration). UKB, on the other hand, is not as sensitive as its supervised
counterpart to the sense distribution, making it robust across different configurations—
with generally lower performance, however.
</bodyText>
<subsectionHeader confidence="0.885201">
7.5 UKB is More Robust with Respect to Sense Skewness
</subsectionHeader>
<bodyText confidence="0.99691625">
To investigate if our WSD systems are biased in favor of more frequent senses, we
calculated the recall performance by sense predominance in the Nat test setting
(in the Uni test setting we assume no sense predominance). In other words, we
separately calculated the recall performance of IMS and UKB on items tagged in the
test set with the first (i.e., most frequent) sense of each pseudoword, the second (i.e.,
second most frequent) sense, and so on. We show in Figure 6 the overall recall by
sense predominance, averaged over the 10 training sizes for each of the two possible
configurations with the Nat test set. As can be seen UKB tends to be more robust
</bodyText>
<figure confidence="0.997769153846154">
Recall
0.8
0.6
0.4
0.2
0
1 2 3 4 5 6 7 8 9 10
Sense ranking by predominance
1
IMS (Nat-Nat)
IMS (Uni-Nat)
UKB (Nat-Nat)
UKB (Uni-Nat)
</figure>
<figureCaption confidence="0.765563">
Figure 6
</figureCaption>
<footnote confidence="0.57765">
Recall by sense predominance in the naturally distributed test set (we show up to 10th order,
since higher orders do not have many instances to provide reliable results).
</footnote>
<page confidence="0.9928">
869
</page>
<note confidence="0.542714">
Computational Linguistics Volume 40, Number 4
</note>
<bodyText confidence="0.999657888888889">
across sense ranking, irrespective of the distribution of the training data. In contrast,
IMS is not equally robust across configurations: Although its recall is relatively stable
in the Uni-Nat configuration, it is not when the training set is naturally distributed
(Nat-Nat). In fact, this shows that IMS, when trained on naturally distributed data, is
biased towards classifying most of the instances as more frequent senses. This lack of
robustness is shown in the figure from the rapid performance drop of IMS in the Nat-
Nat configuration (from 97.1% recall on the most predominant sense to about 11.3% on
the tenth pseudosense of a pseudoword), indicating that the system tends to perform
considerably better on more frequent senses when a natural distribution is assumed.
</bodyText>
<subsectionHeader confidence="0.987147">
7.6 Impact of the Knowledge of Sense Distribution
</subsectionHeader>
<bodyText confidence="0.999899571428571">
Previous work (Escudero, M`arquez, and Rigau 2000; Agirre and Mart´ınez 2004; Chan
and Ng 2005b) has highlighted the impact of the underlying sense distribution for a
supervised disambiguation task. However, we do not know much, especially on a large
scale, about the effect of integrating sense distribution information into knowledge-
based systems. In order to gain more insight into this, we carried out a pilot study to see
how much improvement UKB can gain if explicitly provided with domain knowledge
in the form of sense distribution, so as to give the same advantage to both knowledge-
based and supervised systems. UKB provides, for each disambiguation instance, a
probability distribution over the senses of the target word where each probability value
can be regarded as the chance of the corresponding sense to be selected in the given
context. A possible way to inject the sense distribution knowledge into UKB is to scale
the scores assigned to each sense by the corresponding probability values in the sense
distribution. According to this procedure, the scaled score Pi for the ith sense of a target
word w (with |S |senses) is obtained by:
</bodyText>
<equation confidence="0.981589">
Pi = pidi (2)
Us∈S psds
</equation>
<bodyText confidence="0.999396">
where di is the probability of the ith sense according to the corresponding sense dis-
tribution and pi is the probability score assigned to the ith sense of w by UKB in
the given context. In this way we provide UKB with additional information that can
increase the selection chance of more frequent senses in situations wherein the system
is not confident in its choice of the winning sense (i.e., instances where UKB considers
comparable chances for multiple senses to be selected). We used two different ways of
calculating sense probability values di:
</bodyText>
<listItem confidence="0.9840784">
• Pseudoword-specific sense distribution, where the probability values are
directly calculated from the corresponding pseudoword that is being
disambiguated.
• Average distribution, where values of di are the average per polysemy
probabilities estimated using SemCor (cf. Table 14).
</listItem>
<bodyText confidence="0.999325">
We show in Table 20 the overall UKB performance in the Nat-Nat configuration
when injected into the pseudoword-specific and average sense probabilities. We can
see from the table that, when using only the WordNet LKB (training size = 0), the
provided pseudoword-specific information can boost the performance of UKB by over
26 percentage points (from 38.9 percentage points to 65.6 percentage points). For other
</bodyText>
<page confidence="0.995761">
870
</page>
<note confidence="0.921648">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<tableCaption confidence="0.985298">
Table 20
</tableCaption>
<bodyText confidence="0.5838346">
Performance of UKB on Nat-Nat configuration when injected with the pseudoword-specific
(specific) and average sense distribution (average) information as well as the original
performance values for UKB and IMS (as reported earlier in Table 18). All the improvements of
the UKB (specific) and UKB (average) systems over the UKB (original) system are statistically
significant at the p &lt; 0.001 level.
</bodyText>
<table confidence="0.998432">
Size of training data 0 80 160 240 320 400 480 560 640 720 800
IMS - 81.9* 84.4 86.4 87.4 88.2 88.7 89.3 89.7 90.0 90.3
UKB (original) 38.8 56.5 59.2 61.1 62.1 62.8 63.4 63.8 64.2 64.4 64.6
UKB (specific) 65.6 78.0* 78.8 79.2 79.5 79.7 79.9 80.1 80.3 80.4 80.4
UKB (average) 58.7 75.2 75.8 76.3 76.7 76.8 76.9 77.0 77.1 77.2 77.2
</table>
<bodyText confidence="0.999698444444444">
sizes of the training data an improvement of 15 percentage points to 21 percentage
points is achieved. When provided with this additional sense distribution information,
UKB yields performance comparable to that of IMS (especially for smaller sizes of
the training data, e.g., 78.0* of UKB vs. 81.9* of IMS for training size 80). Note that,
being a supervised system, IMS already benefits from this pseudoword-specific sense
distribution information. Similarly, when provided with the average sense distribution
information, UKB exhibits a considerable improvement ranging from 20 percentage
points for WordNet LKB only (training size = 0) to 12 percentage points for the training
size of 800 sentences.
</bodyText>
<subsectionHeader confidence="0.995205">
7.7 Performance Upperbound of the Systems
</subsectionHeader>
<bodyText confidence="0.99997675">
A possible way to examine how well a system fits the training data is to carry out train-
ing and test on the same data set. This is precisely the setting we explore in this experi-
ment. Our aim was to have an estimate of the performance upperbound of each of our
two WSD systems. We observed that IMS attains an optimal recall value of 100.0 for all
data set sizes and for both sense distributions (i.e., uniform and natural distributions),
showing that its models perfectly fit the training data. However, as mentioned earlier
in Section 6.5, in our setting the automatic enrichment of the LKB is less immediate and
natural than the training of the supervised system. In fact, the annotated data cannot
be directly utilized by UKB. Instead, co-occurrence statistics obtained from this data
were used to enrich the LKB of UKB. We estimated the performance upperbound of
UKB (and therefore the capability of our LKB enrichment approach) by performing an
experiment where additional edges were obtained by exploiting the sentences in the test
data set. The enrichment procedure was, however, the same as the one used in our main
experiments (see Section 6.5). The experiment was carried out on both our test data sets,
namely, naturally and uniformly distributed. In addition, we did not use the values of K
(i.e., maximum number of related words per pseudosense used for enriching the LKB)
tuned for our main experiments (cf. Section 6.5.1) as we expected the test sentences to be
able to provide more beneficial additional edges; instead we used five different values
of K, from 200 to 1,000.
Table 21 shows the UKB performance when additional edges are obtained from test
data sets. We present the overall as well as polysemy-specific performance values for
five different values of K. As expected, a higher performance was shown by UKB in this
setting in comparison with the normal setting where sentences in the training data were
exploited for enriching the LKB. An interesting finding here is that even when the test
</bodyText>
<page confidence="0.992801">
871
</page>
<table confidence="0.5063065">
Computational Linguistics Volume 40, Number 4
By polysemy
</table>
<tableCaption confidence="0.981716">
Table 21
</tableCaption>
<table confidence="0.997071823529412">
Performance of UKB when additional edges are obtained by exploiting the sentences in the test
data set. We show results for both uniformly and naturally distributed test data and for different
values of K (maximum number of related words per pseudosense used for enriching LKB).
Uniformly distributed data set Naturally distributed data set
K 200 400 600 800 1,000 200 400 600 800 1,000
overall 78.6 83.8 86.4 87.9 88.4 78.0 82.4 85.5 86.6 88.4
2 87.9 91.7 93.2 94.2 94.9 89.4 92.1 93.9 93.7 95.1
3 83.6 87.5 89.6 91.3 92.0 83.0 87.1 89.5 90.4 92.2
4 79.8 84.5 87.3 88.9 89.5 79.1 83.6 86.8 87.7 89.5
5 78.1 82.9 85.7 87.3 87.7 76.9 81.8 84.9 85.6 87.9
6 75.1 81.1 84.2 85.8 86.9 74.3 79.3 83.0 84.7 86.4
7 73.4 80.3 83.4 84.7 85.0 72.2 77.1 80.8 81.6 84.2
8 71.0 78.1 80.5 81.8 82.1 69.6 74.2 78.0 82.4 82.5
9 70.5 77.3 80.8 82.6 82.5 68.0 73.0 77.3 78.5 80.9
10 68.0 75.1 77.8 79.2 79.7 67.0 72.5 76.8 78.2 80.2
11 70.3 77.8 80.5 81.4 81.5 69.2 75.3 79.7 83.1 82.4
12 64.6 73.3 77.0 78.8 79.4 68.2 73.5 77.5 80.3 81.9
</table>
<bodyText confidence="0.9985816">
data set is used for obtaining the additional edges, UKB can hardly cross into 90%. In
other words, there is a gap of about 10% resulting from the semi-noisy enrichment of
UKB. This shows that our LKB enrichment approach is not optimal. We defer the task
of improving the current enrichment technique to future work. In fact, our framework
enables other knowledge enrichment approaches to be effectively tested and compared.
</bodyText>
<subsectionHeader confidence="0.996553">
7.8 Summary of the Results
</subsectionHeader>
<bodyText confidence="0.999759666666667">
Here, we summarize our experiments aimed at analyzing the behavior of state-of-the-
art supervised and knowledge-based systems in different settings, and also to verify
their dependence on various factors:
</bodyText>
<listItem confidence="0.97703075">
• IMS has a considerably larger performance variation across different
data set configurations whereas UKB is less sensitive to the underlying
sense distribution (Section 7.1).
• The semi-noisy enrichment of LKB results in a huge improvement in the
performance of UKB even when the edges are harvested from a low
number of training sentences (Section 7.3).
• IMS outperforms UKB in all configurations but Nat-Uni, which models a
WSD setting across different domains where some senses in the test set
might not be covered in the training data set. The gap between IMS and
UKB is especially noticeable in the Nat-Nat configuration (Section 7.4).
• UKB is more robust with respect to sense skewness whereas IMS is highly
biased towards classifying most of the instances as more frequent senses
(Section 7.5).
• Injecting sense distribution information to UKB highly boosts its
performance, providing an interesting mixture of knowledge and
supervision (Section 7.6).
</listItem>
<page confidence="0.986587">
872
</page>
<note confidence="0.675075">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<listItem confidence="0.9351305">
• The upperbound performance of IMS when trained on the test data set
is 1.0 whereas that of UKB lags 0.1 behind (Section 7.7).
</listItem>
<bodyText confidence="0.999447">
In addition, we performed a set of experiments in order to verify some existing
findings in the literature at a large scale. We briefly reported these results in Section 7.2.
See Appendix B for the details.
</bodyText>
<sectionHeader confidence="0.676589" genericHeader="method">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999980322580645">
In this article we proposed a novel framework for the experimental comparison of
state-of-the-art supervised and knowledge-based WSD systems on a large scale. At
the core of our approach lies the usage of a new type of realistic pseudowords, which
makes it possible to model virtually all ambiguous nouns in a lexicon. As a result, we
could generate pseudowords modeling each polysemous noun in WordNet, whose high
quality we assessed from different perspectives.
We selected a reliable subset of pseudowords for each of which we sampled 1,000
tagged instances from a large corpus, resulting in a 2 million pseudosense-tagged
corpus. This corpus was then used for training a state-of-the-art supervised WSD
system (i.e., IMS) as well as for automatically injecting large quantities of (semi-noisy)
semantic relations into the WordNet graph for use by an off-the-shelf knowledge-based
WSD system (i.e., UKB). Our pseudoword-based framework enabled the analysis of
the conditions and factors that impact the performance of these state-of-the-art WSD
systems on a large scale, a study which has never heretofore been possible.
We hope our work will pave the way for new research on the generation and
exploitation of large-scale sense-annotated corpora. Furthermore, our new type of pseu-
doword might also be used for a realistic, wide-coverage evaluation of other difficult
tasks such as Word Sense Induction (Bordag 2006; Di Marco and Navigli 2013; Navigli
and Vannella 2013), Entity Linking (Moro, Raganato, and Navigli 2014) and selectional
preference acquisition (Chambers and Jurafsky 2010; Erk, Pad´o, and Pad´o 2010), among
others.
We are releasing to the research community the entire set of 15,935 pseudowords
of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments
(http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated
corpus, this will allow for future experimental comparisons and studies with other WSD
systems, also in other languages. In fact, our pseudowords and our WSD framework are
not language-dependent and can readily be applied to other languages with the help of
multilingual semantic networks such as BabelNet (Navigli and Ponzetto 2012a) and the
use of multilingual WSD algorithms (Moro, Raganato, and Navigli 2014). Finally, along
the lines of Cuadros and Rigau (2007), our framework could be used in the future to test
and compare various LKB enrichment techniques.
</bodyText>
<sectionHeader confidence="0.775386" genericHeader="method">
Appendix A: Reliability of Our Findings
</sectionHeader>
<bodyText confidence="0.999959857142857">
In order to verify if the selected subset of pseudowords (cf. Section 6.3) was large
enough to provide a reliable estimation of per-polysemy performance, we calculated,
for both our systems, confidence intervals of performance values. Table A.1 shows, for
the training set consisting of 400 sentences, the 95% confidence interval values for the
obtained per-polysemy performance (Table B.1) as well as for the overall performance
(Tables 18 and 19). As could have been expected, the confidence interval is smaller
for lower polysemy degrees where there exist more pseudowords. We can see from
</bodyText>
<page confidence="0.99801">
873
</page>
<note confidence="0.549035">
Computational Linguistics Volume 40, Number 4
</note>
<tableCaption confidence="0.922658333333333">
Table A.1
Two-sided 95% confidence interval for the performance values when the training set contains
400 sentences for both the systems and for all our four configurations.
</tableCaption>
<table confidence="0.9998873">
Polysemy 2 3 4 5 6 7 8 9 10 11 12 overall
# of pseudowords 300 300 300 300 278 192 84 87 54 43 22 1,960
Nat-Nat IMS 0.82 0.89 0.94 0.94 0.97 1.21 1.87 1.92 2.45 2.20 5.48 0.41
UKB 2.00 2.17 2.21 2.18 2.25 2.60 3.74 3.74 4.87 4.11 7.67 0.96
Uni-Uni IMS 0.74 0.92 0.85 0.85 0.91 1.06 1.42 1.42 1.80 2.06 2.81 0.51
UKB 1.30 1.29 1.40 1.35 1.37 1.77 2.55 2.65 2.86 3.04 4.68 0.69
Uni-Nat IMS 0.80 1.15 1.08 1.21 1.19 1.51 2.21 2.51 2.73 2.90 6.20 0.59
UKB 2.26 2.47 2.41 2.33 2.34 2.76 4.01 4.00 4.76 4.13 7.95 1.00
Nat-Uni IMS 1.87 1.86 1.62 1.53 1.38 1.58 2.28 2.06 2.44 2.34 3.97 0.79
UKB 1.47 1.41 1.32 1.31 1.35 1.73 2.30 2.49 2.60 2.81 4.42 0.68
</table>
<bodyText confidence="0.999617333333333">
the table that the confidence interval always remains below 2.0 and 3.0, respectively,
for IMS and UKB for polysemy degrees up to five for which we set an upperbound
of 300 pseudowords. This shows that the subset of 300 pseudowords we picked for
those polysemy degrees is large enough to provide an accurate polysemy-specific
performance.
In addition, the overall confidence interval is always ≤ 1.0 in all system con-
figurations. This shows that the overall recall performance values we reported in
Tables 18 and 19 are quite accurate. These results also hold for other sizes of the training
data.
</bodyText>
<sectionHeader confidence="0.880598" genericHeader="conclusions">
Appendix B: Corroboration of Previous Findings
</sectionHeader>
<bodyText confidence="0.999578">
In the next two sections, we provide the details of the experiments carried out in order
to verify some existing findings in the literature on a large scale.
</bodyText>
<tableCaption confidence="0.542417666666667">
Table B.1
Average per polysemy performance of IMS and UKB in all the four configurations (averaged
over all the 10 size steps which is also very close to the results with 400 sentences).
</tableCaption>
<table confidence="0.999840615384615">
Nat-Nat Uni-Uni Uni-Nat Nat-Uni MFS baseline
polysemy IMS UKB IMS UKB IMS UKB IMS UKB Nat-Nat * – Uni
2 94.7 81.7 87.5 76.5 87.5 78.8 64.4 74.0 87.7 50.0
3 91.0 70.8 80.0 66.6 79.5 66.1 51.8 64.5 78.2 33.3
4 88.9 63.5 75.6 59.5 75.9 59.0 44.7 57.6 72.0 25.0
5 87.6 60.7 71.8 55.4 71.7 56.1 38.5 53.6 71.3 20.0
6 85.9 55.3 68.1 52.6 68.7 51.3 35.6 51.7 64.2 16.7
7 83.4 52.5 65.0 49.1 65.2 48.0 33.9 47.7 58.8 14.3
8 82.2 49.3 62.2 46.2 60.7 46.0 30.6 45.2 59.0 12.5
9 81.2 47.1 60.3 44.9 58.9 42.9 28.8 44.1 58.7 11.1
10 78.5 42.6 58.1 42.0 55.1 39.2 27.9 41.4 52.4 10.0
11 76.1 46.3 53.5 43.8 53.7 43.2 27.0 42.2 49.6 9.1
12 75.2 44.6 52.5 39.7 51.9 42.4 26.2 38.8 54.9 8.3
</table>
<page confidence="0.982638">
874
</page>
<note confidence="0.677309">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
B.1 Performance by Polysemy
</note>
<bodyText confidence="0.987907413043478">
Previous work (Palmer, Dang, and Fellbaum 2007) has shown that both manual and
automatic disambiguation can be affected by polysemy. In this section, we verify with
our large-scale framework the relation between disambiguation performance and pol-
ysemy degree. Table B.1 presents the performance values (averaged over all 10 sizes
of training data) as classified by polysemy degree for each system and for all the
four configurations. As a general trend, irrespective of the configuration and system,
the performance is inversely proportional to polysemy degree. The type of inverse
proportionality is approximately logarithmic in all system configurations except for IMS
in the Nat-Nat configuration, where it is approximately linear.
Another interesting observation is in the variation of the polysemy-wise perfor-
mance difference between the two systems across different configurations. On average,
the absolute polysemy-wise difference between the two systems is 14.1 in the Uni-Uni,
Uni-Nat, and Nat-Uni configurations with the minimum difference being 8.7 (Uni-Nat,
polysemy 2) and the maximum being 17.4 (Uni-Nat again, polysemy 6). However, in
the Nat-Nat configuration the difference between the two systems increases rapidly
with polysemy. Starting with a value of 13 at polysemy 2, the difference value rapidly
increases with polysemy to a maximum of 35.9 at polysemy 10 (the absolute difference
is on average 28.2 in this configuration). This divergence in the polysemy-wise perfor-
mance of our two systems in the Nat-Nat configuration shows that IMS, in addition to
being particularly good at this configuration, is able to further extend its lead over UKB
at higher polysemy degrees.
B.2 Performance by Pseudosense Node Degree
As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector
centrality, whose behavior highly depends on the structure of the graph it is applied
to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010)
has shown that a denser graph with a large number of semantic relations benefits the
eigenvector centrality-based approaches, enabling them to provide more accurate dis-
ambiguation judgments. These evaluations, however, were carried out on the WordNet
graph leveraged for the disambiguation of instances from the SemCor data set. In this
section, we perform a similar analysis but on a much larger scale, that is, in a setting
with hundreds of thousands of disambiguation instances and using a much denser
graph. Essentially, the graphs used in our experiments consist of the same nodes (i.e.,
synsets) as the WordNet graph but enriched with thousands of additional semantic
edges obtained from co-occurrence statistics (cf. Section 6.5).
We follow Navigli and Lapata (2010) and take as our measure of graph connectivity
the degree centrality which is calculated based on the number of edges incident to a
particular node in a graph. We show in Figure B.1 how the nodes are distributed in the
graph according to their degree. We present the distributions for the two LKBs enriched
with full naturally and uniformly distributed training data (i.e., 800 sentences) as well
as for the original WordNet graph. The slightly higher degree of the nodes in the LKB
enriched using the naturally distributed data set is due to the availability of a higher
number of additional edges per pseudosense in this setting (obtained from 150 related
words per pseudosense for the naturally distributed data set vs. 125 for the uniformly
distributed data set, cf. Section 6.5.1).
In Figure B.2, we show the average node degree for different ranges of UKB recall
performance (20 intervals from 0.0 to 1.0). Each point in the graph shows the average
</bodyText>
<page confidence="0.995684">
875
</page>
<figure confidence="0.474402">
Node degree
</figure>
<figureCaption confidence="0.621379">
Figure B.1
</figureCaption>
<bodyText confidence="0.997247625">
Distribution of nodes by incident edges in three different LKBs: WordNet 3.0 (WN30), the
enriched LKB with 800 sentences of uniformly- (Uni) and naturally- (Nat) distributed data sets.
degree of the set of nodes (i.e., pseudosenses) on which UKB obtains a recall that
falls within the corresponding range. As can be seen in the figure, irrespective of the
configuration and training size, the higher the connectivity of a node, the more accurate
will be its disambiguation. This is in line with earlier research (Navigli and Lapata 2010),
in which it is hypothesized that WSD performance increases when the target sense in
the graph tends to have a higher number of incident edges. On the other hand, this
</bodyText>
<figure confidence="0.999017303030303">
Computational Linguistics Volume 40, Number 4
1e+06
100000
10000
1000
100
10
1
Number of nodes
WN30
Nat
Uni
25-50
0-25
925-950
250-275
225-250
175-200
775-800
650-675
950-975
475-500
350-375
50-75
150-175
125-150
100-125
750-775
725-750
700-725
675-700
625-650
600-625
75-100
900-925
875-900
850-875
825-850
800-825
575-600
550-575
525-550
500-525
450-475
425-450
400-425
375-400
325-350
300-325
275-300
200-225
975-1000
Nat-Nat
800
80
Uni-Uni
800
80
5-10
0-5
0-5
5-10
90-95
45-50
85-90
15-20
70-75
40-45
10-15
15-20
10-15
75-80
65-70
60-65
75-80
70-75
65-70
60-65
90-95
85-90
80-85
55-60
50-55
35-40
30-35
25-30
20-25
80-85
55-60
50-55
45-50
40-45
35-40
30-35
25-30
20-25
95-100
95-100
Nat-Uni
800
80
Uni-Nat
80
800
1000
800
600
400
200
0
1000
800
600
400
200
0
Average node degree
Average node degree
1000
800
600
400
200
0
1000
800
600
400
200
0
0-5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45-50 50-55 55-60 60-65 65-70 70-75 75-80 80-85 85-90 90-95 95-100 0-5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45-50 50-55 55-60 60-65 65-70 70-75 75-80 80-85 85-90 90-95 95-100
Performance range Performance range
</figure>
<figureCaption confidence="0.973173">
Figure B.2
</figureCaption>
<bodyText confidence="0.771471">
Average number of incident edges on a pseudosense node in LKB vs. recall performance of
UKB on the instances tagged with that specific pseudosense. We present the results for all
configurations and for two sizes of the training data: 80 and 800 sentences.
</bodyText>
<page confidence="0.986698">
876
</page>
<note confidence="0.645792">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<bodyText confidence="0.9998215">
trend is almost identical for the two training data sizes, namely, 80 and 800. This shows
that the direct proportionality of node degree and disambiguation performance holds
for different sizes of training data. Recall that the value of K—the maximum number
of top-ranking related words used for LKB enrichment—was fixed (cf. Section 6.5.1).
This explains why the average node degree values belonging to the two highly different
sizes of the training data (i.e., 80 and 800 sentences) are comparable in Figure B.2. In
fact, as the number of training sentences increases, more reliable sets of related words
get selected that are likely to provide semantic edges that are more beneficial. However,
the value of K, and hence the number of additional edges, remains almost constant
across different sizes of the training data.
</bodyText>
<sectionHeader confidence="0.998371" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995970333333333">
The authors gratefully acknowledge the
support of the ERC Starting Grant
MultiJEDI no. 259234.
</bodyText>
<sectionHeader confidence="0.998715" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99959285882353">
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas¸ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
WordNet-based approaches. In Proceedings
of Human Language Technologies 2009: The
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL-09), pages 19–27, Boulder, CO.
Agirre, Eneko, Olatz Ansa, David Martinez,
and Eduard Hovy. 2001. Enriching
WordNet concepts with topic signatures.
In Proceedings of the NAACL Workshop on
WordNet and Other Lexical Resources,
pages 23–28, Pittsburg, PA.
Agirre, Eneko and Oier Lopez de Lacalle.
2004. Publicly available topic signatures
for all WordNet nominal senses. In
Proceedings of the LREC, pages 1,123–1,126,
Lisbon.
Agirre, Eneko, Oier Lopez de Lacalle, and
Aitor Soroa. 2009. Knowledge-based
WSD on specific domains: Performing
better than generic supervised WSD.
In Proceedings of the 21st International Joint
Conference on Artificial Intelligence (IJCAI),
pages 1,501–1,506, Pasadena, CA.
Agirre, Eneko, Oier Lopez de Lacalle,
and Aitor Soroa. 2014. Random walks
for knowledge-based Word Sense
Disambiguation. Computational
Linguistics, 40(1):57–84.
Agirre, Eneko and David Martinez.
2004. Unsupervised WSD based on
automatically retrieved examples:
The importance of bias. In Proceedings
of EMNLP, pages 25–32, Barcelona.
Agirre, Eneko and Aitor Soroa. 2009.
Personalizing PageRank for Word Sense
Disambiguation. In Proceedings of the
12th Conference of the European Chapter of
the Association for Computational Linguistics
(EACL), pages 33–41, Athens.
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, pages 26–33,
Toulouse.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing, pages 59–68, Honolulu, HI.
Bordag, Stefan. 2006. Word Sense Induction:
Triplet-based clustering and automatic
evaluation. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL), pages 137–144, Trento.
Buchanan, Bruce G. and David C. Wilkins,
editors. 1993. Readings in Knowledge
Acquisition and Learning: Automating the
Construction and Improvement of Expert
Systems. Morgan Kaufmann Publishers
Inc., San Francisco, CA.
Chambers, Nathanael and Dan Jurafsky.
2010. Improving the use of pseudo-words
for evaluating Selectional Preferences.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
ACL ’10, pages 445–453, Uppsala.
Chan, Yee Seng and Hwee Tou Ng. 2005a.
Scaling up word sense disambiguation
via parallel texts. In Proceedings of the
20th National Conference on Artificial
Intelligence (AAAI), pages 1,037–1,042,
Pittsburgh, PA.
Chan, Yee Seng and Hwee Tou Ng. 2005b.
Word Sense Disambiguation with
distribution estimation. In Proceedings
of the 19th International Joint Conference
on Artificial Intelligence, IJCAI’05,
pages 1,010–1,015, Edinburgh.
</reference>
<page confidence="0.989319">
877
</page>
<note confidence="0.5912">
Computational Linguistics Volume 40, Number 4
</note>
<reference confidence="0.993552516949152">
Chan, Yee Seng and Hwee Tou Ng. 2007.
Domain adaptation with active learning
for Word Sense Disambiguation.
In Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics, pages 49–56, Prague.
Chan, Yee Seng, Hwee Tou Ng, and Zhi
Zhong. 2007. NUS-PT: Exploiting parallel
texts for Word Sense Disambiguation in
the English all-words tasks. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
pages 253–256, Prague.
Cuadros, Montse and German Rigau.
2006. Quality assessment of large scale
knowledge resources. In Proceedings
of the 2006 Conference on Empirical
Methods in Natural Language Processing,
pages 534–541, Sydney.
Cuadros, Montse and German Rigau. 2007.
SemEval-2007 task 16: Evaluation of
wide coverage knowledge resources.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 81–86, Prague.
Cuadros, Montse and German Rigau.
2008. KnowNet: Building a large net of
knowledge from the Web. In Proceedings
of the 22nd International Conference on
Computational Linguistics, pages 161–168,
Manchester.
Curran, James R. and Stephen Clark. 2003.
Investigating GIS and smoothing for
maximum entropy taggers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics - Volume 1, pages 91–98,
Budapest.
Di Marco, Antonio and Roberto Navigli.
2013. Clustering and diversifying Web
search results with graph-based Word
Sense Induction. Computational Linguistics,
39(3):709–754.
Erk, Katrin. 2007. A simple, similarity-based
model for Selectional Preferences. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics,
pages 216–223, Prague.
Erk, Katrin, Sebastian Pad´o, and Ulrike
Pad´o. 2010. A flexible, corpus-driven
model of regular and inverse Selectional
Preferences. Computational Linguistics,
36(4):723–763.
Escudero, Gerard, Lluis M`arquez, and
German Rigau. 2000. An empirical study
of the domain dependence of supervised
Word Sense Disambiguation systems.
In Proceedings of the 2000 Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora:
Held in Conjunction with the 38th Annual
Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ’00,
pages 172–180, Hong Kong.
Faralli, Stefano and Roberto Navigli. 2012.
A new minimally-supervised framework
for Domain Word Sense Disambiguation.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 1,411–1,422, Jeju.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Flati, Tiziano and Roberto Navigli. 2012.
The CQC algorithm: Cycling in graphs
to semantically enrich and enhance a
bilingual dictionary. Journal of Artificial
Intelligence Research (JAIR), 43:135–171.
Gale, William, Kenneth Church, and
David Yarowsky. 1992a. Work on statistical
methods for Word Sense Disambiguation.
In Proceedings of the AAAI Fall Symposium
on Probabilistic Approaches to Natural
Language, pages 54–60, Cambridge, MA.
Gale, William A., Kenneth Church, and
David Yarowsky. 1992b. A method for
disambiguating word senses in a corpus.
Computers and the Humanities, 26:415–439.
Gaustad, Tanja. 2001. Statistical corpus-based
word sense disambiguation: Pseudowords
vs. real ambiguous words. In Proceedings
of the Student Research Workshop of the
39th Annual Meeting of the Association for
Computational Linguistics (ACL/EACL
2001), pages 61–66, Toulouse.
Graff, David and Christopher Cieri.
2003. English gigaword, LDC2003T05.
In Linguistic Data Consortium.
Philadelphia, PA.
Haveliwala, Taher H. 2002. Topic-sensitive
PageRank. In Proceedings of the 11th
International Conference on World Wide
Web (WWW 2002), pages 517–526,
Honolulu, HI.
Hovy, Eduard H., Roberto Navigli, and
Simone Paolo Ponzetto. 2013.
Collaboratively built semi-structured
content and artificial intelligence: The
story so far. Artificial Intelligence, 194:2–27.
Hughes, Thad and Daniel Ramage. 2007.
Lexical semantic relatedness with
random graph walks. In Proceedings of
the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning,
EMNLP-CoNLL ’07, pages 581–589,
Prague.
</reference>
<page confidence="0.996092">
878
</page>
<note confidence="0.863826">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<reference confidence="0.998533237288135">
Ide, Nancy, Collin F. Baker, Christiane
Fellbaum, and Rebecca J. Passonneau.
2010. The manually annotated sub-corpus:
A community resource for and by the
people. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (Short Papers), pages 68–73,
Uppsala.
Jurgens, David and Keith Stevens. 2011.
Measuring the impact of sense similarity
on Word Sense Induction. In Proceedings of
the First Workshop on Unsupervised Learning
in NLP, EMNLP ’11, pages 113–123,
Edinburgh.
Khapra, Mitesh, Anup Kulkarni, Saurabh
Sohoney, and Pushpak Bhattacharyya.
2010. All words domain adapted WSD:
Finding a middle ground between
supervision and unsupervision. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 1,532–1,541, Uppsala.
Kilgarriff, Adam and Joseph Rosenzweig.
2000. English Senseval: Report and results.
In Proceedings of the 2nd Conference on
Language Resources and Evaluation (LREC),
pages 1,239–1,244, Athens.
Leacock, Claudia, Martin Chodorow, and
George Miller. 1998. Using corpus
statistics and WordNet relations for sense
identification. Computational Linguistics,
24(1):147–166.
Lee, Yoong Keok and Hwee Tou Ng. 2002.
An empirical evaluation of knowledge
sources and learning algorithms for
Word Sense Disambiguation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
EMNLP ’02, pages 41–48, Philadelphia, PA.
Lin, Chin-Yew and Eduard Hovy. 2000.
The automated acquisition of topic
signatures for text summarization.
In Proceedings of the 18th Conference on
Computational Linguistics - Volume 1,
COLING ’00, pages 495–501, Saarbr¨ucken.
Lin, Dekang.1998. An information-theoretic
definition of similarity. In Proceedings of the
15th International Conference on Machine
Learning, pages 296–304, Madison, WI.
Litkowski, Ken. 2004. Senseval-3 task: Word
Sense Disambiguation of WordNet glosses.
In Senseval-3: Third International Workshop
on the Evaluation of Systems for the Semantic
Analysis of Text, pages 13–16, Barcelona.
Lu, Zhimao, Haifeng Wang, Jianmin Yao,
Ting Liu, and Sheng Li. 2006. An
equivalent pseudoword solution to
Chinese Word Sense Disambiguation.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th Annual Meeting of the
Association for Computational Linguistics,
pages 457–464, Sydney.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In ARPA Human Language
Technology Workshop, pages 114–119,
Plainsboro, NJ.
Martinez, David. 2004. Supervised Word Sense
Disambiguation: Facing Current Challenges.
Ph.D. Thesis. University of the Basque
Country, Spain.
Martinez, David, Oier Lopez de Lacalle,
and Eneko Agirre. 2008. On the use of
automatically acquired examples for
all-nouns Word Sense Disambiguation.
Journal of Artificial Intelligence Research,
33(1):79–107.
Matuschek, Michael and Iryna Gurevych.
2013. Dijkstra-WSA: A graph-based
approach to word sense alignment.
Transactions of the Association for
Computational Linguistics (TACL),
(1):151–164.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics
(ACL’04), pages 279–286, Barcelona.
Mihalcea, Rada. 2002. Bootstrapping large
sense tagged corpora. In Proceedings of the
3rd International Conference on Language
Resources and Evaluations (LREC),
pages 1,407–1,411, Las Palmas.
Mihalcea, Rada. 2007. Using Wikipedia for
automatic Word Sense Disambiguation.
In Proceedings of NAACL-HLT-07,
pages 196–203, Rochester, NY.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The Senseval-3
English lexical sample task. In Proceedings
of Senseval-3: The Third International
Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 25–28,
Barcelona.
Mihalcea, Rada and Dan Moldovan. 1999.
An automatic method for generating sense
tagged corpora. In Proceedings AAAI ’99,
pages 461–466, Orlando, FL.
Mihalcea, Rada and Dan Moldovan. 2001.
eXtended WordNet: Progress report.
In Proceedings of the NAACL Workshop
on WordNet and Other Lexical Resources,
pages 95–100, Pittsburgh, PA.
</reference>
<page confidence="0.970329">
879
</page>
<reference confidence="0.994997991596638">
Computational Linguistics Volume 40, Number 4
Miller, George A., R. T. Beckwith,
Christiane D. Fellbaum, D. Gross, and
K. Miller. 1990. WordNet: An online
lexical database. International Journal of
Lexicography, 3(4):235–244.
Miller, George A., Claudia Leacock, Randee
Tengi, and Ross Bunker. 1993. A semantic
concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language
Technology, pages 303–308, Plainsboro, NJ.
Moro, Andrea, Roberto Navigli,
Francesco Maria Tucci, and Rebecca J.
Passonneau. 2014. Annotating the MASC
Corpus with BabelNet. In Proceedings
of LREC, pages 4,214–4,219, Reykjavic.
Moro, Andrea, Alessandro Raganato, and
Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation:
A unified approach. Transactions of the
Association for Computational Linguistics,
2:231–244.
Nakov, Preslav I. and Marti A. Hearst.
2003. Category-based pseudowords.
In HLT-NAACL 2003–Short Papers,
pages 67–69, Edmonton.
Navigli, Roberto. 2005. Semi-automatic
extension of large-scale linguistic
knowledge bases. In Proceedings of
FLAIRS-05, pages 548–553, Clearwater
Beach, FL.
Navigli, Roberto. 2008. A structural approach
to the automatic adjudication of word
sense disagreements. Journal of Natural
Language Engineering, 14(4):293–310.
Navigli, Roberto. 2009. Word Sense
Disambiguation: A survey. ACM
Computing Surveys, 41(2):1–69.
Navigli, Roberto. 2012. A quick tour of word
sense disambiguation, induction and
related approaches. In Proceedings of the
38th Conference on Current Trends in Theory
and Practice of Computer Science (SOFSEM),
pages 115–129, Spindleruv Mlyn.
Navigli, Roberto and Mirella Lapata.
2010. An experimental study on graph
connectivity for unsupervised Word
Sense Disambiguation. IEEE Transactions
on Pattern Anaylsis and Machine Intelligence,
32(4):678–692.
Navigli, Roberto and Simone Paolo Ponzetto.
2012a. BabelNet: The automatic
construction, evaluation and application of
a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.
Navigli, Roberto and Simone Paolo Ponzetto.
2012b. Joining forces pays off: Multilingual
joint Word Sense Disambiguation.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 1,399–1,410, Jeju.
Navigli, Roberto and Daniele Vannella. 2013.
SemEval-2013 task 11: Evaluating Word
Sense Induction and Disambiguation
within an end-user application. In
Proceedings of the 7th International Workshop
on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference
on Lexical and Computational Semantics
(*SEM 2013), pages 193–201, Atlanta, GA.
Otrusina, Lubomir and Pavel Smrz. 2010. A
new approach to pseudoword generation.
In Proceedings of the International Conference
on Language Resources and Evaluation
(LREC), pages 1,195–1,199, Valletta.
Palmer, Martha, Hoa Dang, and Christiane
Fellbaum. 2007. Making fine-grained and
coarse-grained sense distinctions, both
manually and automatically. Natural
Language Engineering, 13(2):137–163.
Pham, Thanh Phong, Hwee Tou Ng,
and Wee Sun Lee. 2005. Word Sense
Disambiguation with semi-supervised
learning. In Proceedings of the 20th National
Conference on Artificial Intelligence, AAAI’05,
pages 1,093–1,098, Pittsburgh, PA.
Pilehvar, Mohammad Taher, David Jurgens,
and Roberto Navigli. 2013. Align,
disambiguate and walk: A unified
approach for measuring semantic
similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics, pages 1,341–1,351, Sofia.
Pilehvar, Mohammad Taher and Roberto
Navigli. 2013. Paving the way to a
large-scale pseudosense-annotated
dataset. In Proceedings of the 2013 Conference
of the North American Chapter of the
Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT
2013), pages 1,100–1,109, Atlanta, GA.
Ponzetto, Simone Paolo and Roberto Navigli.
2010. Knowledge-rich Word Sense
Disambiguation rivaling supervised
system. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 1,522–1,531,
Uppsala.
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007a.
SemEval-2007 task-17: English lexical
sample, SRL and all words. In Proceedings
of the 4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 87–92,
Prague.
Pradhan, Sameer S., Eduard Hovy,
Mitch Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2007b.
</reference>
<page confidence="0.96769">
880
</page>
<note confidence="0.587867">
Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD
</note>
<reference confidence="0.999448678571428">
OntoNotes: A unified relational semantic
representation. In Proceedings of the
1st International Conference on Semantic
Computing (ICSC), pages 517–526,
Irvine, CA.
Sanderson, Mark and C. J. Van Rijsbergen.
1999. The impact on retrieval effectiveness
of skewed frequency distributions. ACM
Transactions on Information Systems,
17:440–465.
Sch¨utze, Hinrich. 1992. Dimensions of
meaning. In Supercomputing ’92:
Proceedings of the 1992 ACM/IEEE
Conference on Supercomputing,
pages 787–796, Los Alamitos, CA.
Shen, Hui, Razvan Bunescu, and Rada
Mihalcea. 2013. Coarse to fine grained
sense disambiguation in Wikipedia.
In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1:
Proceedings of the Main Conference and the
Shared Task: Semantic Textual Similarity,
pages 22–31, Atlanta, GA.
Snow, Rion, Brendan O’Connor, Daniel
Jurafsky, and Andrew Ng. 2008. Cheap
and fast—but is it good? Evaluating
non-expert annotations for natural
language tasks. In Proceedings of
the Conference on Empirical Methods
in Natural Language Processing,
pages 254–263, Honolulu, HI.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Proceedings
of the 3rd International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text (SENSEVAL-3),
pages 41–43, Barcelona.
Vannella, Daniele, David Jurgens, Daniele
Scarfini, Domenico Toscani, and Roberto
Navigli. 2014. Validating and extending
semantic knowledge bases using video
games with a purpose. In Proceedings
of the 52nd Annual Meeting of the
Association for Computational Linguistics,
pages 1,294–1,304, Baltimore, MD.
Venhuizen, Noortje J., Valerio Basile,
Kilian Evang, and Johan Bos. 2013.
Gamification for word sense labeling.
In Proceedings of the International Conference
on Computational Semantics (IWCS),
pages 397–403, Potsdam.
Wang, Xinglong and John Carroll. 2005.
Word Sense Disambiguation using sense
examples automatically acquired from
a second language. In Proceedings of the
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing, HLT ’05, pages 547–554,
Vancouver.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the 3rd DARPA
Workshop on Human Language Technology,
pages 266–271, Princeton, NJ.
Yarowsky, David. 1995. Unsupervised
Word Sense Disambiguation rivaling
supervised methods. In Proceedings of the
33rd Annual Meeting of the Association for
Computational Linguistics, pages 189–196,
Cambridge, MA.
Zhong, Zhi and Hwee Tou Ng. 2009.
Word Sense Disambiguation for all
words without hard labor. In Proceedings
of the 21st International Joint Conference
on Artificial Intelligence (IJCAI),
pages 1,616–1,622, Pasadena, CA.
Zhong, Zhi and Hwee Tou Ng. 2010. It
makes sense: A wide-coverage Word
Sense Disambiguation system for free
text. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 78–83, Uppsala.
Zipf, George K. 1949. Human Behavior and the
Principle of Least-Effort. Addison-Wesley,
Cambridge, MA.
</reference>
<page confidence="0.998435">
881
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648316">
<title confidence="0.998650333333333">A Large-Scale Pseudoword-Based Evaluation Framework for State-of-the-Art Word Sense Disambiguation</title>
<author confidence="0.760102">Taher</author>
<affiliation confidence="0.955429">Sapienza University of Rome Sapienza University of Rome</affiliation>
<abstract confidence="0.985851428571428">The evaluation of several tasks in lexical semantics is often limited by the lack of large numbers of manual annotations, not only for training purposes, but also for testing purposes. Word Sense Disambiguation (WSD) is a case in point, as hand-labeled data sets are particularly hard and time-consuming to create. Consequently, evaluations tend to be performed on a small scale, which does not allow for in-depth analysis of the factors that determine a system’s performance. In this article we address this issue by means of a realistic simulation of large-scale evaluation for the WSD task. We do this by providing two main contributions: First, we put forward two novel approaches to the wide-coverage generation of semantically aware pseudowords (i.e., artificial words capable of modeling real polysemous words); second, we leverage the most suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a largescale experimental framework for the comparison of state-of-the-art supervised and knowledgebased algorithms. Using this framework, we study the impact of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors which affect their performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies 2009: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-09),</booktitle>
<pages>pages</pages>
<location>Boulder, CO.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of Human Language Technologies 2009: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-09), pages 19–27, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
<author>Eduard Hovy</author>
</authors>
<title>Enriching WordNet concepts with topic signatures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<pages>23--28</pages>
<location>Pittsburg, PA.</location>
<contexts>
<context position="12519" citStr="Agirre et al. 2001" startWordPosition="1885" endWordPosition="1888">ases. However, the largest hand-crafted resource of this kind (i.e., WordNet) dates back to 1990 with subsequent updates, which attests to the high cost of knowledge engineering on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while neglecting much syntagmatic relational information between concepts. As a consequence, over the past few years several automatic techniques have been proposed that enrich WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signatures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013). Enriched knowledge bases have been shown to greatly benefit graph-based approaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied within a domain, but, when the knowled</context>
</contexts>
<marker>Agirre, Ansa, Martinez, Hovy, 2001</marker>
<rawString>Agirre, Eneko, Olatz Ansa, David Martinez, and Eduard Hovy. 2001. Enriching WordNet concepts with topic signatures. In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources, pages 23–28, Pittsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Publicly available topic signatures for all WordNet nominal senses.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC,</booktitle>
<pages>1--123</pages>
<location>Lisbon.</location>
<marker>Agirre, de Lacalle, 2004</marker>
<rawString>Agirre, Eneko and Oier Lopez de Lacalle. 2004. Publicly available topic signatures for all WordNet nominal senses. In Proceedings of the LREC, pages 1,123–1,126, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Knowledge-based WSD on specific domains: Performing better than generic supervised WSD.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1--501</pages>
<location>Pasadena, CA.</location>
<marker>Agirre, de Lacalle, Soroa, 2009</marker>
<rawString>Agirre, Eneko, Oier Lopez de Lacalle, and Aitor Soroa. 2009. Knowledge-based WSD on specific domains: Performing better than generic supervised WSD. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), pages 1,501–1,506, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Agirre, Eneko, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based Word Sense Disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Unsupervised WSD based on automatically retrieved examples: The importance of bias.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>25--32</pages>
<location>Barcelona.</location>
<marker>Agirre, Martinez, 2004</marker>
<rawString>Agirre, Eneko and David Martinez. 2004. Unsupervised WSD based on automatically retrieved examples: The importance of bias. In Proceedings of EMNLP, pages 25–32, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>33--41</pages>
<location>Athens.</location>
<contexts>
<context position="31618" citStr="Agirre and Soroa 2009" startWordPosition="4816" endWordPosition="4819"> of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure. Polysemy Minimum Frequency 0 50 200 1,000 845 Computational Linguistics Volume 40, Number 4 This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet. In order to measure semantic similarity we used the PPR (Haveliwala 2002) algorithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have been shown to require a widecoverage sense-tagged data set in order to</context>
<context position="85793" citStr="Agirre and Soroa (2009)" startWordPosition="13419" endWordPosition="13422"> trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB.12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB provides an implementation of the PPR algorithm (Haveliwala 2002), adapted to the task of WSD, as proposed by Agirre and Soroa (2009). PPR is applied to a graph representation of a Lexical Knowledge Base (LKB), which is typically WordNet or an extension of it with additional semantic edges. We used the w2w variant, which has been shown to perform best (Agirre and Soroa 2009), where PPR is initialized by concentrating the probability mass on the context words other than the target word to be disambiguated. The most suitable sense of the latter is then chosen by selecting the highest-ranking vertex (i.e., sense) of the word. Similarly to IMS, we used for UKB the corresponding training set in each training– test configuration.</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Agirre, Eneko and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 33–41, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse.</location>
<contexts>
<context position="15801" citStr="Banko and Brill 2001" startWordPosition="2364" endWordPosition="2367">o this current limit in the evaluation of WSD systems is to generate sense-annotated data with the help of artificial ambiguous words, called pseudowords. Pseudowords are created by conflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, Michele and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>59--68</pages>
<location>Honolulu, HI.</location>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Bergsma, Shane, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 59–68, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bordag</author>
</authors>
<title>Word Sense Induction: Triplet-based clustering and automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>137--144</pages>
<location>Trento.</location>
<contexts>
<context position="23244" citStr="Bordag 2006" startWordPosition="3496" endWordPosition="3497"> kind semantically aware pseudowords, in that they aim at listing senses that are in specific relations to each other, thus mirroring the relations existing between the senses of real words in the lexicon. For example, the lack-insufficiency relation is encoded in the pseudoword for deficiency, which would not be possible if we generated a random pseudoword. Semantically aware pseudowords enable the generation of artificially annotated data sets that have similar properties to their real counterparts and this makes them particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006; Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated data set different senses of a word appear in distinct contexts. The extent of this distinction, however, depends on the semantic relatedness of the corresponding senses. The intuition behind semantically aware pseudowords is that they model each sense of an ambiguous word through a semantically similar monosemous representative that should appear naturally in contexts that are similar to those of its corresponding real sense. For this reason, these pseudowords should be expected to result in data sets w</context>
<context position="115562" citStr="Bordag 2006" startWordPosition="18349" endWordPosition="18350">relations into the WordNet graph for use by an off-the-shelf knowledge-based WSD system (i.e., UKB). Our pseudoword-based framework enabled the analysis of the conditions and factors that impact the performance of these state-of-the-art WSD systems on a large scale, a study which has never heretofore been possible. We hope our work will pave the way for new research on the generation and exploitation of large-scale sense-annotated corpora. Furthermore, our new type of pseudoword might also be used for a realistic, wide-coverage evaluation of other difficult tasks such as Word Sense Induction (Bordag 2006; Di Marco and Navigli 2013; Navigli and Vannella 2013), Entity Linking (Moro, Raganato, and Navigli 2014) and selectional preference acquisition (Chambers and Jurafsky 2010; Erk, Pad´o, and Pad´o 2010), among others. We are releasing to the research community the entire set of 15,935 pseudowords of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudow</context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>Bordag, Stefan. 2006. Word Sense Induction: Triplet-based clustering and automatic evaluation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 137–144, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce G Buchanan</author>
<author>David C Wilkins</author>
<author>editors</author>
</authors>
<date>1993</date>
<booktitle>Readings in Knowledge Acquisition and Learning: Automating the Construction and Improvement of Expert Systems.</booktitle>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA.</location>
<marker>Buchanan, Wilkins, editors, 1993</marker>
<rawString>Buchanan, Bruce G. and David C. Wilkins, editors. 1993. Readings in Knowledge Acquisition and Learning: Automating the Construction and Improvement of Expert Systems. Morgan Kaufmann Publishers Inc., San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Improving the use of pseudo-words for evaluating Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>445--453</pages>
<location>Uppsala.</location>
<contexts>
<context position="15910" citStr="Chambers and Jurafsky 2010" startWordPosition="2380" endWordPosition="2383"> of artificial ambiguous words, called pseudowords. Pseudowords are created by conflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compa</context>
<context position="115735" citStr="Chambers and Jurafsky 2010" startWordPosition="18371" endWordPosition="18374">e conditions and factors that impact the performance of these state-of-the-art WSD systems on a large scale, a study which has never heretofore been possible. We hope our work will pave the way for new research on the generation and exploitation of large-scale sense-annotated corpora. Furthermore, our new type of pseudoword might also be used for a realistic, wide-coverage evaluation of other difficult tasks such as Word Sense Induction (Bordag 2006; Di Marco and Navigli 2013; Navigli and Vannella 2013), Entity Linking (Moro, Raganato, and Navigli 2014) and selectional preference acquisition (Chambers and Jurafsky 2010; Erk, Pad´o, and Pad´o 2010), among others. We are releasing to the research community the entire set of 15,935 pseudowords of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet (Navigli</context>
</contexts>
<marker>Chambers, Jurafsky, 2010</marker>
<rawString>Chambers, Nathanael and Dan Jurafsky. 2010. Improving the use of pseudo-words for evaluating Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 445–453, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1--037</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="10045" citStr="Chan and Ng 2005" startWordPosition="1512" endWordPosition="1515">semous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and ca</context>
<context position="11290" citStr="Chan and Ng 2005" startWordPosition="1716" endWordPosition="1719">d Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisition bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. Therefore, in order to obtain high performance, knowledge-based systems are applied to large, wide-coverage lexical </context>
<context position="105077" citStr="Chan and Ng 2005" startWordPosition="16625" endWordPosition="16628">, when trained on naturally distributed data, is biased towards classifying most of the instances as more frequent senses. This lack of robustness is shown in the figure from the rapid performance drop of IMS in the NatNat configuration (from 97.1% recall on the most predominant sense to about 11.3% on the tenth pseudosense of a pseudoword), indicating that the system tends to perform considerably better on more frequent senses when a natural distribution is assumed. 7.6 Impact of the Knowledge of Sense Distribution Previous work (Escudero, M`arquez, and Rigau 2000; Agirre and Mart´ınez 2004; Chan and Ng 2005b) has highlighted the impact of the underlying sense distribution for a supervised disambiguation task. However, we do not know much, especially on a large scale, about the effect of integrating sense distribution information into knowledgebased systems. In order to gain more insight into this, we carried out a pilot study to see how much improvement UKB can gain if explicitly provided with domain knowledge in the form of sense distribution, so as to give the same advantage to both knowledgebased and supervised systems. UKB provides, for each disambiguation instance, a probability distributio</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Chan, Yee Seng and Hwee Tou Ng. 2005a. Scaling up word sense disambiguation via parallel texts. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI), pages 1,037–1,042, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word Sense Disambiguation with distribution estimation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05,</booktitle>
<pages>1--010</pages>
<location>Edinburgh.</location>
<contexts>
<context position="10045" citStr="Chan and Ng 2005" startWordPosition="1512" endWordPosition="1515">semous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and ca</context>
<context position="11290" citStr="Chan and Ng 2005" startWordPosition="1716" endWordPosition="1719">d Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisition bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. Therefore, in order to obtain high performance, knowledge-based systems are applied to large, wide-coverage lexical </context>
<context position="105077" citStr="Chan and Ng 2005" startWordPosition="16625" endWordPosition="16628">, when trained on naturally distributed data, is biased towards classifying most of the instances as more frequent senses. This lack of robustness is shown in the figure from the rapid performance drop of IMS in the NatNat configuration (from 97.1% recall on the most predominant sense to about 11.3% on the tenth pseudosense of a pseudoword), indicating that the system tends to perform considerably better on more frequent senses when a natural distribution is assumed. 7.6 Impact of the Knowledge of Sense Distribution Previous work (Escudero, M`arquez, and Rigau 2000; Agirre and Mart´ınez 2004; Chan and Ng 2005b) has highlighted the impact of the underlying sense distribution for a supervised disambiguation task. However, we do not know much, especially on a large scale, about the effect of integrating sense distribution information into knowledgebased systems. In order to gain more insight into this, we carried out a pilot study to see how much improvement UKB can gain if explicitly provided with domain knowledge in the form of sense distribution, so as to give the same advantage to both knowledgebased and supervised systems. UKB provides, for each disambiguation instance, a probability distributio</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Chan, Yee Seng and Hwee Tou Ng. 2005b. Word Sense Disambiguation with distribution estimation. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI’05, pages 1,010–1,015, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>49--56</pages>
<location>Prague.</location>
<marker>Chan, Ng, 2007</marker>
<rawString>Chan, Yee Seng and Hwee Tou Ng. 2007. Domain adaptation with active learning for Word Sense Disambiguation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 49–56, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>NUS-PT: Exploiting parallel texts for Word Sense Disambiguation in the English all-words tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>253--256</pages>
<location>Prague.</location>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Chan, Yee Seng, Hwee Tou Ng, and Zhi Zhong. 2007. NUS-PT: Exploiting parallel texts for Word Sense Disambiguation in the English all-words tasks. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 253–256, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>Quality assessment of large scale knowledge resources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>534--541</pages>
<location>Sydney.</location>
<contexts>
<context position="121822" citStr="Cuadros and Rigau 2006" startWordPosition="19380" endWordPosition="19383">ith polysemy to a maximum of 35.9 at polysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise performance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees. B.2 Performance by Pseudosense Node Degree As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate disambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments</context>
</contexts>
<marker>Cuadros, Rigau, 2006</marker>
<rawString>Cuadros, Montse and German Rigau. 2006. Quality assessment of large scale knowledge resources. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 534–541, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>SemEval-2007 task 16: Evaluation of wide coverage knowledge resources.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>81--86</pages>
<location>Prague.</location>
<contexts>
<context position="116487" citStr="Cuadros and Rigau (2007)" startWordPosition="18480" endWordPosition="18483">WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet (Navigli and Ponzetto 2012a) and the use of multilingual WSD algorithms (Moro, Raganato, and Navigli 2014). Finally, along the lines of Cuadros and Rigau (2007), our framework could be used in the future to test and compare various LKB enrichment techniques. Appendix A: Reliability of Our Findings In order to verify if the selected subset of pseudowords (cf. Section 6.3) was large enough to provide a reliable estimation of per-polysemy performance, we calculated, for both our systems, confidence intervals of performance values. Table A.1 shows, for the training set consisting of 400 sentences, the 95% confidence interval values for the obtained per-polysemy performance (Table B.1) as well as for the overall performance (Tables 18 and 19). As could ha</context>
</contexts>
<marker>Cuadros, Rigau, 2007</marker>
<rawString>Cuadros, Montse and German Rigau. 2007. SemEval-2007 task 16: Evaluation of wide coverage knowledge resources. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81–86, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Cuadros</author>
<author>German Rigau</author>
</authors>
<title>KnowNet: Building a large net of knowledge from the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>161--168</pages>
<location>Manchester.</location>
<contexts>
<context position="3720" citStr="Cuadros and Rigau 2008" startWordPosition="552" endWordPosition="555">Cor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are ma</context>
<context position="12544" citStr="Cuadros and Rigau 2008" startWordPosition="1889" endWordPosition="1892">argest hand-crafted resource of this kind (i.e., WordNet) dates back to 1990 with subsequent updates, which attests to the high cost of knowledge engineering on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while neglecting much syntagmatic relational information between concepts. As a consequence, over the past few years several automatic techniques have been proposed that enrich WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signatures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013). Enriched knowledge bases have been shown to greatly benefit graph-based approaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied within a domain, but, when the knowledge base is enriched with </context>
<context position="87518" citStr="Cuadros and Rigau (2008)" startWordPosition="13676" endWordPosition="13679">e IMS and UKB to mean supervised and knowledge-based systems, respectively, since we consider these two systems as state-of-the-art representatives of their corresponding paradigms. 6.5 Enriching the LKB Using Training Data Whereas supervised WSD exploits a training set to perform sense classification, knowledge-based approaches use lexical knowledge bases instead. Therefore, a similar operation to that of providing an increasingly large training set is to enrich basic knowledge bases such as WordNet with additional semantic edges, as has previously been done, among others, by Navigli (2005), Cuadros and Rigau (2008), and Navigli and Lapata (2010). The automatic knowledge injection step, however, is less immediate and natural than the supervised one. In fact, pseudowords cannot be directly used to obtain a ready-to-use set of relation edges. To cope with this issue, in each configuration we used the corresponding training set (on which IMS was trained) to extract knowledge that could be used to enrich the WordNet LKB.13 To this end, given a pseudoword p and for each pseudosense wi ∈ p, we identified the most semantically related words w&apos; to wi using the Dice coefficient: 2c(wi, w&apos;) (1) c(wi) + c(w&apos;) where</context>
</contexts>
<marker>Cuadros, Rigau, 2008</marker>
<rawString>Cuadros, Montse and German Rigau. 2008. KnowNet: Building a large net of knowledge from the Web. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 161–168, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>91--98</pages>
<location>Budapest.</location>
<contexts>
<context position="76082" citStr="Curran and Clark 2003" startWordPosition="11813" endWordPosition="11816"> training data. Finally, in Section 6.6 we describe the evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&amp;C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>Curran, James R. and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics - Volume 1, pages 91–98, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and diversifying Web search results with graph-based Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Di Marco, Antonio and Roberto Navigli. 2013. Clustering and diversifying Web search results with graph-based Word Sense Induction. Computational Linguistics, 39(3):709–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for Selectional Preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--223</pages>
<location>Prague.</location>
<contexts>
<context position="15850" citStr="Erk 2007" startWordPosition="2373" endWordPosition="2374">nerate sense-annotated data with the help of artificial ambiguous words, called pseudowords. Pseudowords are created by conflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for W</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Erk, Katrin. 2007. A simple, similarity-based model for Selectional Preferences. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 216–223, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A flexible, corpus-driven model of regular and inverse Selectional Preferences.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Erk, Katrin, Sebastian Pad´o, and Ulrike Pad´o. 2010. A flexible, corpus-driven model of regular and inverse Selectional Preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gerard Escudero</author>
<author>Lluis M`arquez</author>
<author>German Rigau</author>
</authors>
<title>An empirical study of the domain dependence of supervised Word Sense Disambiguation systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00,</booktitle>
<pages>172--180</pages>
<location>Hong Kong.</location>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Escudero, Gerard, Lluis M`arquez, and German Rigau. 2000. An empirical study of the domain dependence of supervised Word Sense Disambiguation systems. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00, pages 172–180, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>A new minimally-supervised framework for Domain Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--411</pages>
<contexts>
<context position="82522" citStr="Faralli and Navigli (2012)" startWordPosition="12899" endWordPosition="12902">hich the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest different distribution. To verify this, we studied the variability of sense distribution across texts belonging to different domains. We started from a data set of sense-annotated documents from 30 different domains provided by Faralli and Navigli (2012). We then estimated the average sense distribution of all nouns across documents, shown in Figure 3 (light columns) for polysemy degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average sense distribution across domains is not skewed, in contrast to the natural sense distribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set. Uni-Uni. This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in </context>
</contexts>
<marker>Faralli, Navigli, 2012</marker>
<rawString>Faralli, Stefano and Roberto Navigli. 2012. A new minimally-supervised framework for Domain Word Sense Disambiguation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1,411–1,422, Jeju.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="32133" citStr="(1998)" startWordPosition="4901" endWordPosition="4901">Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have been shown to require a widecoverage sense-tagged data set in order to calculate similarities on a sense-by-sense basis for all words in the lexicon (Otrusina and Smrz 2010). Also, among WordNet-based approaches, PPR reports state-of-the-art results on semantic similarity (Agirre et al. 2009) and WSD data sets (Agirre, Lopez de Lacalle, and Soroa 2014), thus representing a suitable graph-based measure for finding the most appropriate pseudosenses. In Algorithm 1 we present the procedure for the generation of our similarity-based pseudowords. The algorithm takes as input an ambig</context>
</contexts>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>The CQC algorithm: Cycling in graphs to semantically enrich and enhance a bilingual dictionary.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>43--135</pages>
<contexts>
<context position="70525" citStr="Flati and Navigli 2012" startWordPosition="10863" endWordPosition="10866">llowing two sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), </context>
<context position="83374" citStr="Flati and Navigli 2012" startWordPosition="13039" endWordPosition="13042">bution across domains is not skewed, in contrast to the natural sense distribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set. Uni-Uni. This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in which all senses are equally important. As is also the case for the Nat-Uni configuration, the uniformly distributed test set in Uni-Uni also models tasks such as dictionary disambiguation, in which sense-wise precision matters (Flati and Navigli 2012). Uni-Nat. Similarly to Uni-Uni, this configuration takes no stand on the training sense distribution, but tests it on naturally distributed data. 862 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD Data set split. We created our training and test sets by sampling 1,000 sentences per pseudoword from the Gigaword corpus for each of the two sense distributions (i.e., natural and uniform). Out of these sentences, we kept 200 (i.e., 20%) as a test set and used the remaining 800 (i.e., 80%) for training. In order to be able to analyze the impact of the amount of kno</context>
</contexts>
<marker>Flati, Navigli, 2012</marker>
<rawString>Flati, Tiziano and Roberto Navigli. 2012. The CQC algorithm: Cycling in graphs to semantically enrich and enhance a bilingual dictionary. Journal of Artificial Intelligence Research (JAIR), 43:135–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>Work on statistical methods for Word Sense Disambiguation.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>54--60</pages>
<location>Cambridge, MA.</location>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky. 1992a. Work on statistical methods for Word Sense Disambiguation. In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54–60, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William A., Kenneth Church, and David Yarowsky. 1992b. A method for disambiguating word senses in a corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Gaustad</author>
</authors>
<title>Statistical corpus-based word sense disambiguation: Pseudowords vs. real ambiguous words.</title>
<date>2001</date>
<booktitle>In Proceedings of the Student Research Workshop of the 39th Annual Meeting of the Association for Computational Linguistics (ACL/EACL</booktitle>
<pages>61--66</pages>
<location>Toulouse.</location>
<contexts>
<context position="16292" citStr="Gaustad 2001" startWordPosition="2443" endWordPosition="2444">ork aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an optimistic upper-bound estimate on the performance of WSD classifiers (Nakov and Hearst 2003). Several researchers addressed the issue of producing pseudowords that can model semantic relationships between senses. To this end Nakov and Hearst (2003) used lexical category membership from a medical term hie</context>
</contexts>
<marker>Gaustad, 2001</marker>
<rawString>Gaustad, Tanja. 2001. Statistical corpus-based word sense disambiguation: Pseudowords vs. real ambiguous words. In Proceedings of the Student Research Workshop of the 39th Annual Meeting of the Association for Computational Linguistics (ACL/EACL 2001), pages 61–66, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<date>2003</date>
<note>English gigaword, LDC2003T05.</note>
<contexts>
<context position="28882" citStr="Graff and Cieri 2003" startWordPosition="4393" endWordPosition="4396"> to this limitation, the vicinity-based approach suffers from lack of flexibility in generating pseudowords that can be leveraged for creating a large-scale pseudosense-tagged corpus, where we need each pseudosense to occur with a relatively large minimum frequency. Due to its small search space, the approach falls short of identifying suitable monosemous representatives for many given senses, which undermines its ability to cover most of the ambiguous nouns in WordNet. We show in Table 2 the percentage of nouns in WordNet that could be modeled using the vicinity-based approach when Gigaword (Graff and Cieri 2003) was used as our corpus. Coverage statistics are presented for four different values of minimum frequency: 0 (no minimum frequency constraint), 50, 200, and 1,000. Besides the overall coverage (rightmost column), in the table we also present the coverage percentage by degree of polysemy. Here, an ambiguous noun in WordNet is considered as covered by its corresponding vicinity-based pseudoword if, for each of its senses, a suitable monosemous candidate can be found in its surrounding that also satisfies the specified minimum frequency in the corpus. As can be seen from the table, the approach c</context>
<context position="45464" citStr="Graff and Cieri 2003" startWordPosition="7019" endWordPosition="7022">assess the semantic closeness of pseudosenses to their corresponding real senses (Section 4.2). • Distinguishability of pseudosenses, where we determine to what extent pseudosenses are specific to a fine-grained real sense rather than covering multiple senses (Section 4.3). Given that our aim was to leverage these pseudowords for creating large-scale pseudosense-annotated data sets, we performed evaluations on pseudowords generated with minFreq per pseudosense set to a high value of 1,000 (i.e., we can generate 1,000 annotated sentences for each pseudosense) using the English Gigaword corpus (Graff and Cieri 2003). 4.1 Disambiguation Difficulty of Pseudowords Our first experiment is an extrinsic evaluation to assess the correlation between the difficulty of the disambiguation task when using pseudowords and real words. The basic idea behind this experiment is to verify, through a disambiguation task, if the semantic similarity among the senses of an ambiguous word is preserved in its corresponding pseudoword. Semantically similar senses of an ambiguous word will tend to appear in similar contexts, making it relatively difficult to discriminate between them. Conversely, ambiguous words that have semanti</context>
<context position="48183" citStr="Graff and Cieri 2003" startWordPosition="7433" endWordPosition="7436">he Senseval-3 English lexical sample data set (Mihalcea, Chklovski, and Kilgarriff 2004) as our manually sense-tagged corpus. The data set provides for 20 nouns of polysemy 3 to 10 an average number of 180 and 90 sense-tagged sentences in its training and test sets, respectively. We generated the similarity-based and TS-based pseudowords corresponding to these 20 nouns, as well as a set of 20 random pseudowords. For each set of these pseudowords we generated corresponding pseudosenseannotated training and test data sets by randomly sampling distinct sentences from the English Gigaword corpus (Graff and Cieri 2003). Therefore, we ended up with four data sets, namely: the Senseval-3 data set of real words, and the three artificially sense-tagged data sets for the similarity-based, TS-based, and random pseudowords. Each of the artificially annotated data sets consisted of training and test portions comprising the same number of instances per sense (i.e., the same sense distribution) as that of the original Senseval-3 training and test data sets. Next, for each of our four data sets, we trained a supervised WSD system on the training set and applied it to the corresponding test set. In order to ensure more</context>
<context position="75680" citStr="Graff and Cieri 2003" startWordPosition="11749" endWordPosition="11752">followed by a description of the process of generating training and test data sets (Section 6.3). In Section 6.4 we introduce the two WSD systems used as representatives of the two main WSD paradigms (i.e., supervised and knowledge-based) in our experiments. We then provide, in Section 6.5, the details of the method through application of which our knowledge-based system is able to benefit from the training data. Finally, in Section 6.6 we describe the evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&amp;C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could genera</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>Graff, David and Christopher Cieri. 2003. English gigaword, LDC2003T05.</rawString>
</citation>
<citation valid="false">
<booktitle>In Linguistic Data Consortium.</booktitle>
<location>Philadelphia, PA.</location>
<marker></marker>
<rawString>In Linguistic Data Consortium. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive PageRank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th International Conference on World Wide Web (WWW</booktitle>
<pages>517--526</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="31416" citStr="Haveliwala 2002" startWordPosition="4786" endWordPosition="4787">identifies, for each sense of a given ambiguous word, the most semantically similar monosemous word satisfying the minimum occurrence frequency constraint. Our method can be considered an extension of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure. Polysemy Minimum Frequency 0 50 200 1,000 845 Computational Linguistics Volume 40, Number 4 This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet. In order to measure semantic similarity we used the PPR (Haveliwala 2002) algorithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behi</context>
<context position="85725" citStr="Haveliwala 2002" startWordPosition="13408" endWordPosition="13409">igurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB.12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB provides an implementation of the PPR algorithm (Haveliwala 2002), adapted to the task of WSD, as proposed by Agirre and Soroa (2009). PPR is applied to a graph representation of a Lexical Knowledge Base (LKB), which is typically WordNet or an extension of it with additional semantic edges. We used the w2w variant, which has been shown to perform best (Agirre and Soroa 2009), where PPR is initialized by concentrating the probability mass on the context words other than the target word to be disambiguated. The most suitable sense of the latter is then chosen by selecting the highest-ranking vertex (i.e., sense) of the word. Similarly to IMS, we used for UKB </context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Haveliwala, Taher H. 2002. Topic-sensitive PageRank. In Proceedings of the 11th International Conference on World Wide Web (WWW 2002), pages 517–526, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semi-structured content and artificial intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Hovy, Eduard H., Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semi-structured content and artificial intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<pages>581--589</pages>
<location>Prague.</location>
<contexts>
<context position="31549" citStr="Hughes and Ramage 2007" startWordPosition="4805" endWordPosition="4808">rrence frequency constraint. Our method can be considered an extension of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure. Polysemy Minimum Frequency 0 50 200 1,000 845 Computational Linguistics Volume 40, Number 4 This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet. In order to measure semantic similarity we used the PPR (Haveliwala 2002) algorithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have b</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Hughes, Thad and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 581–589, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Collin F Baker</author>
<author>Christiane Fellbaum</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>The manually annotated sub-corpus: A community resource for and by the people.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (Short Papers),</booktitle>
<pages>68--73</pages>
<location>Uppsala.</location>
<contexts>
<context position="3535" citStr="Ide et al. 2010" startWordPosition="521" endWordPosition="524">ing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and </context>
<context position="79072" citStr="Ide et al. 2010" startWordPosition="12306" endWordPosition="12309">frequency (minFreq, which corresponds to the number of sentences to be tagged with a particular pseudosense) directly affects the averageRank score, a measure that we interpreted as our confidence in the preservation of meaning of a real sense through its corresponding pseudosense. This suggests a trade-off between the scale of our experiments and their overall accuracy. We show in Table 16 the statistics of the averageRank score for the subset of pseudowords selected for our experiment when six different values of minFreq were assumed while generating pseudowords. Currently, the MASC corpus (Ide et al. 2010), even though covering a small set of 20 nouns, provides the highest number of manually annotated instances per word, namely, 1,000 sentences. In our experiments we followed MASC and generated 1,000 annotated instances for each of our 1,960 pseudowords. As can be seen from Table 16, when minFreq = 1,000, a pseudosense is on average selected from the 8.4th position in the similarSynsets list (given by mean), with most of them being picked out from the first position (given by mode). Table 16 Statistics of the averageRank score of the subset of pseudowords selected for our experiments: we show m</context>
</contexts>
<marker>Ide, Baker, Fellbaum, Passonneau, 2010</marker>
<rawString>Ide, Nancy, Collin F. Baker, Christiane Fellbaum, and Rebecca J. Passonneau. 2010. The manually annotated sub-corpus: A community resource for and by the people. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 68–73, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>Measuring the impact of sense similarity on Word Sense Induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11,</booktitle>
<pages>113--123</pages>
<location>Edinburgh.</location>
<contexts>
<context position="15989" citStr="Jurgens and Stevens 2011" startWordPosition="2393" endWordPosition="2396">nflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an opt</context>
<context position="23270" citStr="Jurgens and Stevens 2011" startWordPosition="3498" endWordPosition="3501">cally aware pseudowords, in that they aim at listing senses that are in specific relations to each other, thus mirroring the relations existing between the senses of real words in the lexicon. For example, the lack-insufficiency relation is encoded in the pseudoword for deficiency, which would not be possible if we generated a random pseudoword. Semantically aware pseudowords enable the generation of artificially annotated data sets that have similar properties to their real counterparts and this makes them particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006; Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated data set different senses of a word appear in distinct contexts. The extent of this distinction, however, depends on the semantic relatedness of the corresponding senses. The intuition behind semantically aware pseudowords is that they model each sense of an ambiguous word through a semantically similar monosemous representative that should appear naturally in contexts that are similar to those of its corresponding real sense. For this reason, these pseudowords should be expected to result in data sets wherein the distinctions be</context>
</contexts>
<marker>Jurgens, Stevens, 2011</marker>
<rawString>Jurgens, David and Keith Stevens. 2011. Measuring the impact of sense similarity on Word Sense Induction. In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11, pages 113–123, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Anup Kulkarni</author>
<author>Saurabh Sohoney</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>All words domain adapted WSD: Finding a middle ground between supervision and unsupervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--532</pages>
<location>Uppsala.</location>
<contexts>
<context position="11175" citStr="Khapra et al. 2010" startWordPosition="1695" endWordPosition="1698"> a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisition bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. T</context>
</contexts>
<marker>Khapra, Kulkarni, Sohoney, Bhattacharyya, 2010</marker>
<rawString>Khapra, Mitesh, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1,532–1,541, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>English Senseval: Report and results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1--239</pages>
<location>Athens.</location>
<contexts>
<context position="81644" citStr="Kilgarriff and Rosenzweig 2000" startWordPosition="12761" endWordPosition="12764"> our experiments to cover a wide range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be th</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Kilgarriff, Adam and Joseph Rosenzweig. 2000. English Senseval: Report and results. In Proceedings of the 2nd Conference on Language Resources and Evaluation (LREC), pages 1,239–1,244, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>George Miller</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Leacock, Claudia, Martin Chodorow, and George Miller. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02,</booktitle>
<pages>41--48</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="85082" citStr="Lee and Ng 2002" startWordPosition="13305" endWordPosition="13308">representatives for the two mainstream WSD paradigms, that is, supervised and knowledge-based WSD. 6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and Ng 2010) as the representative supervised WSD system. IMS is a publicly available English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks.11 The system classifies words in context using linear support vector machines. The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (Lee and Ng 2002). For each of the four configurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB.12 UKB is a publicly available graph-based WSD system that exploits a preexisting lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB provides an implementa</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Lee, Yoong Keok and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for Word Sense Disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02, pages 41–48, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics - Volume 1, COLING ’00,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="41083" citStr="Lin and Hovy 2000" startWordPosition="6357" endWordPosition="6360">034 cocaine 0.079 fire 0.007 star 0.025 user the list similarSynsets to select a pseudosense. However, the mode statistics in the table suggests that even when minFreq is set to a large value, most of the pseudosenses are picked out from the highest-ranking positions in the similarSynsets list. 3.3 Topic Signature-Based Pseudowords As an alternative means of finding suitable monosemous representatives for word senses with the PPR algorithm, we propose using automatically generated topic signatures. Topic signatures (TS) are weighted topical vectors that are associated with senses or concepts (Lin and Hovy 2000). The dimensions of these vectors are the words in the vocabulary and their weights determine the relatedness of each of these words to the target word sense. These vectors can be obtained automatically from large corpora or the Web with the help of monosemous relatives. In order to generate a TS-based pseudoword for a word w, we first sort the weighted vectors associated with the senses of w. Then, from each of these vectors, we select the monosemous word with highest relatedness (i.e., largest weight) which satisfies the minimum frequency constraint. The generation process of the TS-based ps</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Lin, Chin-Yew and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th Conference on Computational Linguistics - Volume 1, COLING ’00, pages 495–501, Saarbr¨ucken.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekang 1998 Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, WI.</location>
<marker>Lin, </marker>
<rawString>Lin, Dekang.1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296–304, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>Senseval-3 task: Word Sense Disambiguation of WordNet glosses.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>13--16</pages>
<location>Barcelona.</location>
<contexts>
<context position="70500" citStr="Litkowski 2004" startWordPosition="10861" endWordPosition="10862">enses. In the following two sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemC</context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Litkowski, Ken. 2004. Senseval-3 task: Word Sense Disambiguation of WordNet glosses. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 13–16, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhimao Lu</author>
<author>Haifeng Wang</author>
<author>Jianmin Yao</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An equivalent pseudoword solution to Chinese Word Sense Disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>457--464</pages>
<location>Sydney.</location>
<contexts>
<context position="17745" citStr="Lu et al. 2006" startWordPosition="2650" endWordPosition="2653">ords in terms of disambiguation difficulty than random 3 http://www.nlm.nih.gov/mesh. 841 Computational Linguistics Volume 40, Number 4 pseudowords. However, this approach requires a specific hierarchical lexicon and falls short of creating many pseudowords with high polysemy. More recent work has focused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale e</context>
<context position="46649" citStr="Lu et al. 2006" startWordPosition="7194" endWordPosition="7197">iguous words that have semantically distinct senses (e.g., homonyms) will be relatively easier to disambiguate. Given that our pseudowords directly model real ambiguous words, we ideally expect a pseudoword to preserve the same level 850 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD of semantic similarity between pseudosenses as that of its corresponding real word, and therefore to exhibit a comparable degree of disambiguation difficulty to that of its corresponding real word. We performed this evaluation in the style of earlier work (Otrusina and Smrz 2010; Lu et al. 2006). In order to test a pseudoword generation approach using this style, first, all the sense-tagged words in a manually annotated lexical sample data set are modeled using the approach. Next, a corresponding pseudosense-annotated data set is automatically constructed by sampling sentences from a corpus while maintaining the same number of training and test sentences for each word as that of the original manually tagged data set. A correlation analysis is then carried out to compare the disambiguation performance of a supervised WSD system on a given ambiguous word against its corresponding pseud</context>
</contexts>
<marker>Lu, Wang, Yao, Liu, Li, 2006</marker>
<rawString>Lu, Zhimao, Haifeng Wang, Jianmin Yao, Ting Liu, and Sheng Li. 2006. An equivalent pseudoword solution to Chinese Word Sense Disambiguation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 457–464, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert Macintyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop,</booktitle>
<pages>114--119</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="76132" citStr="Marcus et al. 1994" startWordPosition="11822" endWordPosition="11825">he evaluation measures used in our experiments. 6.1 Corpus We sampled all the sentences for pseudosense tagging from the English Gigaword corpus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The corpus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sentences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&amp;C tagger (Curran and Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two reasons that will be explained shortly, we only considered a reliable subset of these pseudowords for generating the data sets for our experiments. Firstly, we did not consider nouns with polysemy degree higher than 12 in our experiments, as it is not possible to perform a reliable analysis on such degrees given that very few pseudowords can be ge</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, Macintyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop, pages 114–119, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
</authors>
<title>Supervised Word Sense Disambiguation: Facing Current Challenges.</title>
<date>2004</date>
<tech>Ph.D. Thesis.</tech>
<institution>University of the Basque Country,</institution>
<contexts>
<context position="4347" citStr="Martinez 2004" startWordPosition="646" endWordPosition="647"> Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address this issue by providing two main contributions: • We first focus on novel, flexible techniques for creating new types of artificial words that model real words by preserving their semantics as much as possible. Our semantically aware pseudowords can be used to model any word in the lex</context>
</contexts>
<marker>Martinez, 2004</marker>
<rawString>Martinez, David. 2004. Supervised Word Sense Disambiguation: Facing Current Challenges. Ph.D. Thesis. University of the Basque Country, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
</authors>
<title>Oier Lopez de Lacalle, and Eneko Agirre.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Martinez, 2008</marker>
<rawString>Martinez, David, Oier Lopez de Lacalle, and Eneko Agirre. 2008. On the use of automatically acquired examples for all-nouns Word Sense Disambiguation. Journal of Artificial Intelligence Research, 33(1):79–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dijkstra-WSA: A graph-based approach to word sense alignment.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--151</pages>
<contexts>
<context position="70614" citStr="Matuschek and Gurevych 2013" startWordPosition="10875" endWordPosition="10878">ments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, SemCor provi</context>
</contexts>
<marker>Matuschek, Gurevych, 2013</marker>
<rawString>Matuschek, Michael and Iryna Gurevych. 2013. Dijkstra-WSA: A graph-based approach to word sense alignment. Transactions of the Association for Computational Linguistics (TACL), (1):151–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>279--286</pages>
<location>Barcelona.</location>
<contexts>
<context position="81667" citStr="McCarthy et al. 2004" startWordPosition="12765" endWordPosition="12768"> range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one. In our experiments, we considered all four possible ways of combining the sense distributions of training–test data—that is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them: Nat-Nat. This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets. Nat-Uni. This configuration trains a WSD system with a natural distribution but applies it to texts for which the distribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>McCarthy, Diana, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 279–286, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluations (LREC),</booktitle>
<pages>1--407</pages>
<location>Las Palmas.</location>
<contexts>
<context position="9046" citStr="Mihalcea 2002" startWordPosition="1362" endWordPosition="1363">f the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploit</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Mihalcea, Rada. 2002. Bootstrapping large sense tagged corpora. In Proceedings of the 3rd International Conference on Language Resources and Evaluations (LREC), pages 1,407–1,411, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT-07,</booktitle>
<pages>196--203</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="3323" citStr="Mihalcea 2007" startWordPosition="493" endWordPosition="494">wledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a r</context>
<context position="9696" citStr="Mihalcea 2007" startWordPosition="1461" endWordPosition="1462">gorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Rece</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Mihalcea, Rada. 2007. Using Wikipedia for automatic Word Sense Disambiguation. In Proceedings of NAACL-HLT-07, pages 196–203, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<location>Barcelona.</location>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Mihalcea, Rada, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Proceedings of Senseval-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings AAAI ’99,</booktitle>
<pages>461--466</pages>
<location>Orlando, FL.</location>
<contexts>
<context position="9568" citStr="Mihalcea and Moldovan 1999" startWordPosition="1442" endWordPosition="1445">en proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapp</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Mihalcea, Rada and Dan Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings AAAI ’99, pages 461–466, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>eXtended WordNet: Progress report.</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<booktitle>In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<volume>40</volume>
<pages>95--100</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="12440" citStr="Mihalcea and Moldovan 2001" startWordPosition="1874" endWordPosition="1877">ormance, knowledge-based systems are applied to large, wide-coverage lexical knowledge bases. However, the largest hand-crafted resource of this kind (i.e., WordNet) dates back to 1990 with subsequent updates, which attests to the high cost of knowledge engineering on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while neglecting much syntagmatic relational information between concepts. As a consequence, over the past few years several automatic techniques have been proposed that enrich WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signatures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013). Enriched knowledge bases have been shown to greatly benefit graph-based approaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outper</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Mihalcea, Rada and Dan Moldovan. 2001. eXtended WordNet: Progress report. In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources, pages 95–100, Pittsburgh, PA. Computational Linguistics Volume 40, Number 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>R T Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3058" citStr="Miller et al. 1990" startWordPosition="447" endWordPosition="450"> for publication: 6 March 2014. doi:10.1162/COLI a 00202 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 4 difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins 1993). Unfortunately, providing knowledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lex</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George A., R. T. Beckwith, Christiane D. Fellbaum, D. Gross, and K. Miller. 1990. WordNet: An online lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="3129" citStr="Miller et al. 1993" startWordPosition="460" endWordPosition="463">ation for Computational Linguistics Computational Linguistics Volume 40, Number 4 difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins 1993). Unfortunately, providing knowledge on a large scale is a time-consuming process, which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzett</context>
<context position="14971" citStr="Miller et al. 1993" startWordPosition="2241" endWordPosition="2244">ma Unfortunately, as of today we do not have unequivocal insights into which disambiguation paradigm is more suitable under which conditions. As a matter of fact, not only does each implemented system come with its own amount and kind of supervision or knowledge, making it hard to determine the contribution of the supervision or knowledge vs. that of the WSD algorithm, but test data sets are small, typically comprising one or two thousand sense-tagged word items, which prevents us from drawing solid conclusions. Even the largest annotation effort ever—namely, the SemCor sense-tagged data set (Miller et al. 1993), comprising around 235,000 semantic annotations—covers only about 15% of word types in WordNet with an average of 10 instances per word, thus precluding large-scale experimental studies. A possible solution to this current limit in the evaluation of WSD systems is to generate sense-annotated data with the help of artificial ambiguous words, called pseudowords. Pseudowords are created by conflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale, Church, and Yarowsky (1992a) and Sch¨utze (1992) as a means of generating large amounts</context>
<context position="71123" citStr="Miller et al. 1993" startWordPosition="10959" endWordPosition="10962">ati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, SemCor provides reliable distribution estimates for only some hundred words. The table shows for each degree of polysemy the number of distinct nouns in SemCor that are sense-annotated at least once, 10 times, or 20 times, compared with the corresponding total number of ambiguous nouns in WordNet (last column in the table). Because we could obtain from SemCor the sense distribution of only some hundred 858 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD Table 13 Number of distinct no</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>Miller, George A., Claudia Leacock, Randee Tengi, and Ross Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 303–308, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
<author>Francesco Maria Tucci</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Annotating the MASC Corpus with BabelNet.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>4--214</pages>
<contexts>
<context position="3650" citStr="Moro et al. 2014" startWordPosition="542" endWordPosition="545">onary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amount</context>
</contexts>
<marker>Moro, Navigli, Tucci, Passonneau, 2014</marker>
<rawString>Moro, Andrea, Roberto Navigli, Francesco Maria Tucci, and Rebecca J. Passonneau. 2014. Annotating the MASC Corpus with BabelNet. In Proceedings of LREC, pages 4,214–4,219, Reykjavic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity Linking meets Word Sense Disambiguation: A unified approach.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--231</pages>
<contexts>
<context position="3650" citStr="Moro et al. 2014" startWordPosition="542" endWordPosition="545">onary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amount</context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Moro, Andrea, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: A unified approach. Transactions of the Association for Computational Linguistics, 2:231–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav I Nakov</author>
<author>Marti A Hearst</author>
</authors>
<title>Category-based pseudowords.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003–Short Papers,</booktitle>
<pages>67--69</pages>
<location>Edmonton.</location>
<contexts>
<context position="16679" citStr="Nakov and Hearst 2003" startWordPosition="2497" endWordPosition="2500">t of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an optimistic upper-bound estimate on the performance of WSD classifiers (Nakov and Hearst 2003). Several researchers addressed the issue of producing pseudowords that can model semantic relationships between senses. To this end Nakov and Hearst (2003) used lexical category membership from a medical term hierarchy (extracted from MeSH3 [Medical Subject Headings]) to create “more plausible” pseudowords. By considering the distributions from lexical category co-occurrence, they produced a set of pseudowords that were closer to real ambiguous words in terms of disambiguation difficulty than random 3 http://www.nlm.nih.gov/mesh. 841 Computational Linguistics Volume 40, Number 4 pseudowords. </context>
</contexts>
<marker>Nakov, Hearst, 2003</marker>
<rawString>Nakov, Preslav I. and Marti A. Hearst. 2003. Category-based pseudowords. In HLT-NAACL 2003–Short Papers, pages 67–69, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Semi-automatic extension of large-scale linguistic knowledge bases.</title>
<date>2005</date>
<booktitle>In Proceedings of FLAIRS-05,</booktitle>
<pages>548--553</pages>
<location>Clearwater Beach, FL.</location>
<contexts>
<context position="3696" citStr="Navigli 2005" startWordPosition="550" endWordPosition="551">93 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or str</context>
<context position="12481" citStr="Navigli 2005" startWordPosition="1880" endWordPosition="1881">wide-coverage lexical knowledge bases. However, the largest hand-crafted resource of this kind (i.e., WordNet) dates back to 1990 with subsequent updates, which attests to the high cost of knowledge engineering on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while neglecting much syntagmatic relational information between concepts. As a consequence, over the past few years several automatic techniques have been proposed that enrich WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signatures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013). Enriched knowledge bases have been shown to greatly benefit graph-based approaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied </context>
<context position="87492" citStr="Navigli (2005)" startWordPosition="13674" endWordPosition="13675">fter, we will use IMS and UKB to mean supervised and knowledge-based systems, respectively, since we consider these two systems as state-of-the-art representatives of their corresponding paradigms. 6.5 Enriching the LKB Using Training Data Whereas supervised WSD exploits a training set to perform sense classification, knowledge-based approaches use lexical knowledge bases instead. Therefore, a similar operation to that of providing an increasingly large training set is to enrich basic knowledge bases such as WordNet with additional semantic edges, as has previously been done, among others, by Navigli (2005), Cuadros and Rigau (2008), and Navigli and Lapata (2010). The automatic knowledge injection step, however, is less immediate and natural than the supervised one. In fact, pseudowords cannot be directly used to obtain a ready-to-use set of relation edges. To cope with this issue, in each configuration we used the corresponding training set (on which IMS was trained) to extract knowledge that could be used to enrich the WordNet LKB.13 To this end, given a pseudoword p and for each pseudosense wi ∈ p, we identified the most semantically related words w&apos; to wi using the Dice coefficient: 2c(wi, w</context>
</contexts>
<marker>Navigli, 2005</marker>
<rawString>Navigli, Roberto. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of FLAIRS-05, pages 548–553, Clearwater Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A structural approach to the automatic adjudication of word sense disagreements.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="121836" citStr="Navigli 2008" startWordPosition="19384" endWordPosition="19385">m of 35.9 at polysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise performance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees. B.2 Performance by Pseudosense Node Degree As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate disambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments consist of th</context>
</contexts>
<marker>Navigli, 2008</marker>
<rawString>Navigli, Roberto. 2008. A structural approach to the automatic adjudication of word sense disagreements. Journal of Natural Language Engineering, 14(4):293–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1644" citStr="Navigli 2009" startWordPosition="240" endWordPosition="241">age the most suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a largescale experimental framework for the comparison of state-of-the-art supervised and knowledgebased algorithms. Using this framework, we study the impact of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors which affect their performance. 1. Introduction Word Sense Disambiguation (WSD) is a core research field in computational linguistics dealing with the automatic assignment of senses to words occurring in a given context (Navigli 2009, 2012). There are two major paradigms in WSD: supervised and knowledge-based. Supervised WSD starts from a training set and learns a computational model of the word of interest, which is later used at test time to classify new instances of the same word. Knowledge-based WSD, instead, performs the disambiguation task by using an existing lexical knowledge base—that is, a semantic network to which graph algorithms, for example, can be applied. However, both disambiguation paradigms have to face the so-called knowledge acquisition bottleneck, namely, the * Department of Computer Science, Sapienz</context>
<context position="93557" citStr="Navigli 2009" startWordPosition="14698" endWordPosition="14699">aining data (for instance, “Uni-*” stands for the average performance over Uni-Uni and Uni-Nat configurations). It can be seen that the maximum average performance occurs at K = 125 for the Uni training data and at K = 150 for the Nat training data. Therefore, depending on the sense distribution of training data, we used two different cutting thresholds (K) on the number of related words considered for enriching the corresponding LKB. 6.6 Evaluation Measures It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure (Navigli 2009). Precision calculates the portion of items that are correctly disambiguated from among the total output by the system, and recall measures the portion of the total items in the data set that are correctly disambiguated by the system. F1 is the harmonic mean of precision and recall. Because in our setting all the pseudowords to be disambiguated in the test set are covered in the training data and also included as a node in the LKB, IMS and UKB always provide an answer for each item in the test set. For such a full-coverage case, the values of precision, recall, and F1 will be equal. Hence, in </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Navigli, Roberto. 2009. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A quick tour of word sense disambiguation, induction and related approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM),</booktitle>
<pages>115--129</pages>
<institution>Spindleruv Mlyn.</institution>
<contexts>
<context position="70525" citStr="Navigli 2012" startWordPosition="10865" endWordPosition="10866">o sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), </context>
<context position="82522" citStr="Navigli (2012)" startWordPosition="12901" endWordPosition="12902">tribution is unknown (e.g., in different domains). Given that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distributions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest different distribution. To verify this, we studied the variability of sense distribution across texts belonging to different domains. We started from a data set of sense-annotated documents from 30 different domains provided by Faralli and Navigli (2012). We then estimated the average sense distribution of all nouns across documents, shown in Figure 3 (light columns) for polysemy degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average sense distribution across domains is not skewed, in contrast to the natural sense distribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set. Uni-Uni. This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in </context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Navigli, Roberto. 2012. A quick tour of word sense disambiguation, induction and related approaches. In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM), pages 115–129, Spindleruv Mlyn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study on graph connectivity for unsupervised Word Sense Disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Anaylsis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="4436" citStr="Navigli and Lapata 2010" startWordPosition="658" endWordPosition="661">nhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address this issue by providing two main contributions: • We first focus on novel, flexible techniques for creating new types of artificial words that model real words by preserving their semantics as much as possible. Our semantically aware pseudowords can be used to model any word in the lexicon,1 therefore aiming for wide coverage. We perform different experiments to show that </context>
<context position="12877" citStr="Navigli and Lapata 2010" startWordPosition="1935" endWordPosition="1938"> past few years several automatic techniques have been proposed that enrich WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signatures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013). Enriched knowledge bases have been shown to greatly benefit graph-based approaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied within a domain, but, when the knowledge base is enriched with tens of thousands of semantic relations automatically extracted from Wikipedia, performance comparable to that of state-ofthe-art supervised systems can be obtained in a general all-words setting, too (Ponzetto and Navigli 2010; Moro, Raganato, and Navigli 2014). Recently, a multilingual graph-based WSD approach has been developed </context>
<context position="87549" citStr="Navigli and Lapata (2010)" startWordPosition="13681" endWordPosition="13684">ed and knowledge-based systems, respectively, since we consider these two systems as state-of-the-art representatives of their corresponding paradigms. 6.5 Enriching the LKB Using Training Data Whereas supervised WSD exploits a training set to perform sense classification, knowledge-based approaches use lexical knowledge bases instead. Therefore, a similar operation to that of providing an increasingly large training set is to enrich basic knowledge bases such as WordNet with additional semantic edges, as has previously been done, among others, by Navigli (2005), Cuadros and Rigau (2008), and Navigli and Lapata (2010). The automatic knowledge injection step, however, is less immediate and natural than the supervised one. In fact, pseudowords cannot be directly used to obtain a ready-to-use set of relation edges. To cope with this issue, in each configuration we used the corresponding training set (on which IMS was trained) to extract knowledge that could be used to enrich the WordNet LKB.13 To this end, given a pseudoword p and for each pseudosense wi ∈ p, we identified the most semantically related words w&apos; to wi using the Dice coefficient: 2c(wi, w&apos;) (1) c(wi) + c(w&apos;) where c(wi,w&apos;) is the number of sent</context>
<context position="100142" citStr="Navigli and Lapata (2010)" startWordPosition="15807" endWordPosition="15810">gs by Palmer, Dang, and Fellbaum (2007) that the two are inversely proportional. We also found IMS to be particularly robust on highly polysemous words in the Nat-Nat configuration. The inverse proportionality was approximately logarithmic in all system configurations except for IMS in the Nat-Nat configuration in which the proportionality was linear (Appendix B.1). • The relation between connectivity of a node in the LKB and disambiguation accuracy: we found that the higher the connectivity of a node, the more accurate will be its disambiguation. This corroborates the preliminary findings of Navigli and Lapata (2010) on a much larger scale (Appendix B.2). Previous work has made claims only on significantly smaller amounts of annotated data (e.g., in Navigli and Lapata 2010), whereas we show for the first time that these hold in large-scale experiments with several orders of magnitudes more annotated data. 7.3 UKB Largely Benefits from Semi-Noisy Edges Thanks to our framework, we can go into considerably greater detail on the second point that we verified in Section 7.2. As can be seen in Tables 18 and 19, the enrichment of the WordNet LKB proves to be highly beneficial. The performance of UKB increases si</context>
<context position="121862" citStr="Navigli and Lapata 2010" startWordPosition="19386" endWordPosition="19389">olysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise performance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees. B.2 Performance by Pseudosense Node Degree As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate disambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments consist of the same nodes (i.e., synset</context>
<context position="124155" citStr="Navigli and Lapata 2010" startWordPosition="19758" endWordPosition="19761">from 0.0 to 1.0). Each point in the graph shows the average 875 Node degree Figure B.1 Distribution of nodes by incident edges in three different LKBs: WordNet 3.0 (WN30), the enriched LKB with 800 sentences of uniformly- (Uni) and naturally- (Nat) distributed data sets. degree of the set of nodes (i.e., pseudosenses) on which UKB obtains a recall that falls within the corresponding range. As can be seen in the figure, irrespective of the configuration and training size, the higher the connectivity of a node, the more accurate will be its disambiguation. This is in line with earlier research (Navigli and Lapata 2010), in which it is hypothesized that WSD performance increases when the target sense in the graph tends to have a higher number of incident edges. On the other hand, this Computational Linguistics Volume 40, Number 4 1e+06 100000 10000 1000 100 10 1 Number of nodes WN30 Nat Uni 25-50 0-25 925-950 250-275 225-250 175-200 775-800 650-675 950-975 475-500 350-375 50-75 150-175 125-150 100-125 750-775 725-750 700-725 675-700 625-650 600-625 75-100 900-925 875-900 850-875 825-850 800-825 575-600 550-575 525-550 500-525 450-475 425-450 400-425 375-400 325-350 300-325 275-300 200-225 975-1000 Nat-Nat 80</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Navigli, Roberto and Mirella Lapata. 2010. An experimental study on graph connectivity for unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Anaylsis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="4062" citStr="Navigli and Ponzetto 2012" startWordPosition="606" endWordPosition="609">t efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address t</context>
<context position="13820" citStr="Navigli and Ponzetto 2012" startWordPosition="2064" endWordPosition="2067">ly extracted from Wikipedia, performance comparable to that of state-ofthe-art supervised systems can be obtained in a general all-words setting, too (Ponzetto and Navigli 2010; Moro, Raganato, and Navigli 2014). Recently, a multilingual graph-based WSD approach has been developed that leverages a large multilingual semantic network, called BabelNet (Navigli and Ponzetto 2 http://nlp.comp.nus.edu.sg/sw/models.tar.gz. 840 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2012a), to achieve state-of-the-art results on both general all-words and domainoriented WSD (Navigli and Ponzetto 2012b). Experimental results show that the joint use of multilingual knowledge enables further improvements over monolingual WSD. However, the power of this disambiguation system lies mainly in its usage of the BabelNet multilingual semantic network. In fact, Agirre, Lopez de Lacalle, and Soroa (2014) showed that under similar conditions (i.e., when the same lexical knowledge base was used), the PPR algorithm can outperform the graph-based WSD algorithms used by Navigli and Ponzetto (2012b). 2.3 The Supervision vs. Knowledge Dilemma Unfortunately, as of today we do not have unequivocal insights in</context>
<context position="70583" citStr="Navigli and Ponzetto 2012" startWordPosition="10871" endWordPosition="10874">trategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we</context>
<context position="116353" citStr="Navigli and Ponzetto 2012" startWordPosition="18459" endWordPosition="18462">ky 2010; Erk, Pad´o, and Pad´o 2010), among others. We are releasing to the research community the entire set of 15,935 pseudowords of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet (Navigli and Ponzetto 2012a) and the use of multilingual WSD algorithms (Moro, Raganato, and Navigli 2014). Finally, along the lines of Cuadros and Rigau (2007), our framework could be used in the future to test and compare various LKB enrichment techniques. Appendix A: Reliability of Our Findings In order to verify if the selected subset of pseudowords (cf. Section 6.3) was large enough to provide a reliable estimation of per-polysemy performance, we calculated, for both our systems, confidence intervals of performance values. Table A.1 shows, for the training set consisting of 400 sentences, the 95% confidence interv</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Navigli, Roberto and Simone Paolo Ponzetto. 2012a. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Joining forces pays off: Multilingual joint Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--399</pages>
<contexts>
<context position="4062" citStr="Navigli and Ponzetto 2012" startWordPosition="606" endWordPosition="609">t efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system’s performance. In this article we address t</context>
<context position="13820" citStr="Navigli and Ponzetto 2012" startWordPosition="2064" endWordPosition="2067">ly extracted from Wikipedia, performance comparable to that of state-ofthe-art supervised systems can be obtained in a general all-words setting, too (Ponzetto and Navigli 2010; Moro, Raganato, and Navigli 2014). Recently, a multilingual graph-based WSD approach has been developed that leverages a large multilingual semantic network, called BabelNet (Navigli and Ponzetto 2 http://nlp.comp.nus.edu.sg/sw/models.tar.gz. 840 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2012a), to achieve state-of-the-art results on both general all-words and domainoriented WSD (Navigli and Ponzetto 2012b). Experimental results show that the joint use of multilingual knowledge enables further improvements over monolingual WSD. However, the power of this disambiguation system lies mainly in its usage of the BabelNet multilingual semantic network. In fact, Agirre, Lopez de Lacalle, and Soroa (2014) showed that under similar conditions (i.e., when the same lexical knowledge base was used), the PPR algorithm can outperform the graph-based WSD algorithms used by Navigli and Ponzetto (2012b). 2.3 The Supervision vs. Knowledge Dilemma Unfortunately, as of today we do not have unequivocal insights in</context>
<context position="70583" citStr="Navigli and Ponzetto 2012" startWordPosition="10871" endWordPosition="10874">trategies used in our experiments. 5.1 Uniform Sense Distribution A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we</context>
<context position="116353" citStr="Navigli and Ponzetto 2012" startWordPosition="18459" endWordPosition="18462">ky 2010; Erk, Pad´o, and Pad´o 2010), among others. We are releasing to the research community the entire set of 15,935 pseudowords of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet (Navigli and Ponzetto 2012a) and the use of multilingual WSD algorithms (Moro, Raganato, and Navigli 2014). Finally, along the lines of Cuadros and Rigau (2007), our framework could be used in the future to test and compare various LKB enrichment techniques. Appendix A: Reliability of Our Findings In order to verify if the selected subset of pseudowords (cf. Section 6.3) was large enough to provide a reliable estimation of per-polysemy performance, we calculated, for both our systems, confidence intervals of performance values. Table A.1 shows, for the training set consisting of 400 sentences, the 95% confidence interv</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Navigli, Roberto and Simone Paolo Ponzetto. 2012b. Joining forces pays off: Multilingual joint Word Sense Disambiguation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1,399–1,410, Jeju.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval-2013 task 11: Evaluating Word Sense Induction and Disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<pages>193--201</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="115617" citStr="Navigli and Vannella 2013" startWordPosition="18356" endWordPosition="18359"> by an off-the-shelf knowledge-based WSD system (i.e., UKB). Our pseudoword-based framework enabled the analysis of the conditions and factors that impact the performance of these state-of-the-art WSD systems on a large scale, a study which has never heretofore been possible. We hope our work will pave the way for new research on the generation and exploitation of large-scale sense-annotated corpora. Furthermore, our new type of pseudoword might also be used for a realistic, wide-coverage evaluation of other difficult tasks such as Word Sense Induction (Bordag 2006; Di Marco and Navigli 2013; Navigli and Vannella 2013), Entity Linking (Moro, Raganato, and Navigli 2014) and selectional preference acquisition (Chambers and Jurafsky 2010; Erk, Pad´o, and Pad´o 2010), among others. We are releasing to the research community the entire set of 15,935 pseudowords of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments (http://lcl.uniroma1.it/pseudowords/). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent a</context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Navigli, Roberto and Daniele Vannella. 2013. SemEval-2013 task 11: Evaluating Word Sense Induction and Disambiguation within an end-user application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013), pages 193–201, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lubomir Otrusina</author>
<author>Pavel Smrz</author>
</authors>
<title>A new approach to pseudoword generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1--195</pages>
<location>Valletta.</location>
<contexts>
<context position="18055" citStr="Otrusina and Smrz (2010)" startWordPosition="2694" endWordPosition="2697">cused on the identification of monosemous representatives in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly. However, this method leverages a specific Chinese hierarchical lexicon, in which different levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the WordNet graph in order to find an unambiguous representative for that sense. Unfortunately, as we discuss in detail in the next section, none of these proposals can enable a large-scale evaluation framework for WSD, mainly because they suffer from coverage issues that prevent the creation of wide-coverage sense-annotated data sets. In this article we propose new pseudoword generation techniques that allow for the creation of thousands of artificial words having sufficient occurrence coverage </context>
<context position="24636" citStr="Otrusina and Smrz (2010)" startWordPosition="3709" endWordPosition="3712">, two of which are presented in full detail for the first time in this article, for the generation of semantically aware pseudowords that use WordNet as the reference lexicon. In what follows we focus on nominal pseudowords, and leave the extension to other parts of speech to future work. 3.1 Vicinity-Based Pseudowords As discussed in Section 2, earlier techniques for the generation of semantically aware pseudowords were either inherently restricted to specific hierarchical lexicons utilized in the generation process, or to the number of pseudowords they could generate. An idea put forward by Otrusina and Smrz (2010) was to create pseudowords by combining representatives for each individual sense of a real ambiguous word in WordNet. The 843 Computational Linguistics Volume 40, Number 4 representatives were selected among monosemous relatives (i.e., unambiguous words that are structurally related to a given sense). This method for finding monosemous representatives for senses has been in use since 1998, when it was first proposed for the unsupervised acquisition of sense-tagged corpora (Leacock, Chodorow, and Miller 1998). Specifically, Otrusina and Smrz (2010) exploit WordNet, whose conceptual units are s</context>
<context position="26440" citStr="Otrusina and Smrz (2010)" startWordPosition="3993" endWordPosition="3996">nyms. As an example, consider the ambiguous noun coke, which has three senses (i.e., fuel, drink, and drug) in WordNet 3.0. We show in Table 1, for each of the three senses of coke, the set of nouns in the corresponding synset as well as in the surrounding synsets. Monosemous words are shown in bold in the table. As can be seen, there exist multiple monosemous candidates for each sense (coca cola, pepsi, and pepsi cola for the second sense; nose candy, cocaine, and cocain for the third sense; and dozens of candidates in the direct siblings’ vicinity of the first sense). Among these candidates Otrusina and Smrz (2010) select those whose occurrence frequency ratio in a given text corpus is most similar to that of the senses of the corresponding real word as given by a sense-annotated corpus. However, calculating the occurrence frequency of individual senses of a word requires a large-enough sense-tagged corpus. This dependency on sense-annotated data is a disadvantage of the vicinity-based approach that limits its ability in modeling arbitrary words. Table 1 Synset neighbors of the three senses of coke (We use the sense notation of Navigli [2009] where the irh sense of word w is denoted as wi. We also highl</context>
<context position="32321" citStr="Otrusina and Smrz 2010" startWordPosition="4929" endWordPosition="4932">bility distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative contextbased methods, such as Lin’s (1998) measure, have been shown to require a widecoverage sense-tagged data set in order to calculate similarities on a sense-by-sense basis for all words in the lexicon (Otrusina and Smrz 2010). Also, among WordNet-based approaches, PPR reports state-of-the-art results on semantic similarity (Agirre et al. 2009) and WSD data sets (Agirre, Lopez de Lacalle, and Soroa 2014), thus representing a suitable graph-based measure for finding the most appropriate pseudosenses. In Algorithm 1 we present the procedure for the generation of our similarity-based pseudowords. The algorithm takes as input an ambiguous word w, and generates its corresponding similarity-based pseudoword Pw whose ith pseudosense models the ith sense of w. Additionally, the algorithm provides, for each generated pseudo</context>
<context position="46632" citStr="Otrusina and Smrz 2010" startWordPosition="7190" endWordPosition="7193">en them. Conversely, ambiguous words that have semantically distinct senses (e.g., homonyms) will be relatively easier to disambiguate. Given that our pseudowords directly model real ambiguous words, we ideally expect a pseudoword to preserve the same level 850 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD of semantic similarity between pseudosenses as that of its corresponding real word, and therefore to exhibit a comparable degree of disambiguation difficulty to that of its corresponding real word. We performed this evaluation in the style of earlier work (Otrusina and Smrz 2010; Lu et al. 2006). In order to test a pseudoword generation approach using this style, first, all the sense-tagged words in a manually annotated lexical sample data set are modeled using the approach. Next, a corresponding pseudosense-annotated data set is automatically constructed by sampling sentences from a corpus while maintaining the same number of training and test sentences for each word as that of the original manually tagged data set. A correlation analysis is then carried out to compare the disambiguation performance of a supervised WSD system on a given ambiguous word against its co</context>
<context position="48835" citStr="Otrusina and Smrz (2010)" startWordPosition="7539" endWordPosition="7542">h four data sets, namely: the Senseval-3 data set of real words, and the three artificially sense-tagged data sets for the similarity-based, TS-based, and random pseudowords. Each of the artificially annotated data sets consisted of training and test portions comprising the same number of instances per sense (i.e., the same sense distribution) as that of the original Senseval-3 training and test data sets. Next, for each of our four data sets, we trained a supervised WSD system on the training set and applied it to the corresponding test set. In order to ensure more reliable results we follow Otrusina and Smrz (2010) and report, for all experiments in this evaluation, the average results for five runs. To this end, we randomly sampled the training and test data sets from the combination of all items while preserving the original proportions. Also, in the random setting, we provide the results averaged on a set of 25 different pseudowords modeling a given ambiguous noun. As our WSD system for this experiment, we used IMS (Zhong and Ng 2010), a state-of-the-art supervised WSD system that is based on support vector machines (we will describe IMS in more detail in Section 6.4). Note that we measure disambigua</context>
</contexts>
<marker>Otrusina, Smrz, 2010</marker>
<rawString>Otrusina, Lubomir and Pavel Smrz. 2010. A new approach to pseudoword generation. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 1,195–1,199, Valletta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Palmer, Martha, Hoa Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thanh Phong Pham</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
</authors>
<title>Word Sense Disambiguation with semi-supervised learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence, AAAI’05,</booktitle>
<pages>1--093</pages>
<location>Pittsburgh, PA.</location>
<marker>Pham, Ng, Lee, 2005</marker>
<rawString>Pham, Thanh Phong, Hwee Tou Ng, and Wee Sun Lee. 2005. Word Sense Disambiguation with semi-supervised learning. In Proceedings of the 20th National Conference on Artificial Intelligence, AAAI’05, pages 1,093–1,098, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, disambiguate and walk: A unified approach for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--341</pages>
<location>Sofia.</location>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Pilehvar, Mohammad Taher, David Jurgens, and Roberto Navigli. 2013. Align, disambiguate and walk: A unified approach for measuring semantic similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1,341–1,351, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Paving the way to a large-scale pseudosense-annotated dataset.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</booktitle>
<pages>1--100</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="5748" citStr="Pilehvar and Navigli 2013" startWordPosition="855" endWordPosition="858">rms of disambiguation difficulty, representativeness, and distinguishability of the artificial senses. • We leverage our semantically aware pseudowords to create, for the first time, a large-scale evaluation framework for WSD. Using this framework, we are able to perform an experimental comparison of state-of-the-art systems for supervised and knowledge-based WSD on a very large data set made up of millions of sense-tagged sentences. Our large-scale framework enables us to carry out an in-depth analysis of the factors and conditions that determine the systems’ performance. In our recent work (Pilehvar and Navigli 2013), we presented an approach for the generation of semantically aware pseudowords, called similarity-based pseudowords. At the core of this approach was the Personalized PageRank algorithm (Haveliwala 1 Although in our experiments we focus on nouns only, the same approach can potentially be used for any other open-class part of speech. 838 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2002) on the WordNet graph, which was utilized to find the most semantically similar monosemous representative for a given sense of a real ambiguous word. The main strength of the</context>
</contexts>
<marker>Pilehvar, Navigli, 2013</marker>
<rawString>Pilehvar, Mohammad Taher and Roberto Navigli. 2013. Paving the way to a large-scale pseudosense-annotated dataset. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), pages 1,100–1,109, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich Word Sense Disambiguation rivaling supervised system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--522</pages>
<location>Uppsala.</location>
<contexts>
<context position="3748" citStr="Ponzetto and Navigli 2010" startWordPosition="556" endWordPosition="559">. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004;</context>
<context position="13371" citStr="Ponzetto and Navigli 2010" startWordPosition="2007" endWordPosition="2010">girre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied within a domain, but, when the knowledge base is enriched with tens of thousands of semantic relations automatically extracted from Wikipedia, performance comparable to that of state-ofthe-art supervised systems can be obtained in a general all-words setting, too (Ponzetto and Navigli 2010; Moro, Raganato, and Navigli 2014). Recently, a multilingual graph-based WSD approach has been developed that leverages a large multilingual semantic network, called BabelNet (Navigli and Ponzetto 2 http://nlp.comp.nus.edu.sg/sw/models.tar.gz. 840 Pilehvar and Navigli A Large-Scale Pseudoword-Based Evaluation Framework for WSD 2012a), to achieve state-of-the-art results on both general all-words and domainoriented WSD (Navigli and Ponzetto 2012b). Experimental results show that the joint use of multilingual knowledge enables further improvements over monolingual WSD. However, the power of thi</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Ponzetto, Simone Paolo and Roberto Navigli. 2010. Knowledge-rich Word Sense Disambiguation rivaling supervised system. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1,522–1,531, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2007 task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague.</location>
<contexts>
<context position="3506" citStr="Pradhan et al. 2007" startWordPosition="515" endWordPosition="518">argest manual efforts for providing a widecoverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradig</context>
<context position="8415" citStr="Pradhan et al. 2007" startWordPosition="1263" endWordPosition="1266"> we leverage our pseudowords to generate large sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based WSD is described in Section 6. Experimental results as well as the findings are presented and discussed in Section 7. Finally, we provide concluding remarks in Section 8. 2. Related Work 2.1 Supervised WSD and the Knowledge Acquisition Bottleneck Over the last few decades, WSD systems have been suffering from disappointingly low performance, especially in an all-words setting in which one has to cover the entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a). In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping technique</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Pradhan, Sameer, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007a. SemEval-2007 task-17: English lexical sample, SRL and all words. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard Hovy</author>
</authors>
<marker>Pradhan, Hovy, </marker>
<rawString>Pradhan, Sameer S., Eduard Hovy,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<date>2007</date>
<marker>Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007b.</rawString>
</citation>
<citation valid="false">
<title>OntoNotes: A unified relational semantic representation.</title>
<booktitle>In Proceedings of the 1st International Conference on Semantic Computing (ICSC),</booktitle>
<pages>517--526</pages>
<location>Irvine, CA.</location>
<marker></marker>
<rawString>OntoNotes: A unified relational semantic representation. In Proceedings of the 1st International Conference on Semantic Computing (ICSC), pages 517–526, Irvine, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
<author>C J Van Rijsbergen</author>
</authors>
<title>The impact on retrieval effectiveness of skewed frequency distributions.</title>
<date>1999</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>17--440</pages>
<marker>Sanderson, Van Rijsbergen, 1999</marker>
<rawString>Sanderson, Mark and C. J. Van Rijsbergen. 1999. The impact on retrieval effectiveness of skewed frequency distributions. ACM Transactions on Information Systems, 17:440–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Supercomputing ’92: Proceedings of the 1992 ACM/IEEE Conference on Supercomputing,</booktitle>
<pages>787--796</pages>
<location>Los Alamitos, CA.</location>
<marker>Sch¨utze, 1992</marker>
<rawString>Sch¨utze, Hinrich. 1992. Dimensions of meaning. In Supercomputing ’92: Proceedings of the 1992 ACM/IEEE Conference on Supercomputing, pages 787–796, Los Alamitos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Shen</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Coarse to fine grained sense disambiguation in Wikipedia.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>22--31</pages>
<location>Atlanta, GA.</location>
<marker>Shen, Bunescu, Mihalcea, 2013</marker>
<rawString>Shen, Hui, Razvan Bunescu, and Rada Mihalcea. 2013. Coarse to fine grained sense disambiguation in Wikipedia. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 22–31, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<location>Honolulu, HI.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Snow, Rion, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3),</booktitle>
<pages>41--43</pages>
<location>Barcelona.</location>
<contexts>
<context position="8394" citStr="Snyder and Palmer 2004" startWordPosition="1259" endWordPosition="1262">. We then illustrate how we leverage our pseudowords to generate large sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based WSD is described in Section 6. Experimental results as well as the findings are presented and discussed in Section 7. Finally, we provide concluding remarks in Section 8. 2. Related Work 2.1 Supervised WSD and the Knowledge Acquisition Bottleneck Over the last few decades, WSD systems have been suffering from disappointingly low performance, especially in an all-words setting in which one has to cover the entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a). In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bo</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, Benjamin and Martha Palmer. 2004. The English all-words task. In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3), pages 41–43, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Vannella</author>
<author>David Jurgens</author>
<author>Daniele Scarfini</author>
<author>Domenico Toscani</author>
<author>Roberto Navigli</author>
</authors>
<title>Validating and extending semantic knowledge bases using video games with a purpose.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--294</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="3915" citStr="Vannella et al. (2014)" startWordPosition="585" endWordPosition="588">s in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with differ</context>
</contexts>
<marker>Vannella, Jurgens, Scarfini, Toscani, Navigli, 2014</marker>
<rawString>Vannella, Daniele, David Jurgens, Daniele Scarfini, Domenico Toscani, and Roberto Navigli. 2014. Validating and extending semantic knowledge bases using video games with a purpose. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1,294–1,304, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noortje J Venhuizen</author>
<author>Valerio Basile</author>
<author>Kilian Evang</author>
<author>Johan Bos</author>
</authors>
<title>Gamification for word sense labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Computational Semantics (IWCS),</booktitle>
<pages>397--403</pages>
<location>Potsdam.</location>
<contexts>
<context position="3832" citStr="Venhuizen et al. 2013" startWordPosition="570" endWordPosition="573">Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2</context>
</contexts>
<marker>Venhuizen, Basile, Evang, Bos, 2013</marker>
<rawString>Venhuizen, Noortje J., Valerio Basile, Kilian Evang, and Johan Bos. 2013. Gamification for word sense labeling. In Proceedings of the International Conference on Computational Semantics (IWCS), pages 397–403, Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinglong Wang</author>
<author>John Carroll</author>
</authors>
<title>Word Sense Disambiguation using sense examples automatically acquired from a second language.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>547--554</pages>
<location>Vancouver.</location>
<contexts>
<context position="10069" citStr="Wang and Carroll 2005" startWordPosition="1516" endWordPosition="1519">eacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zho</context>
</contexts>
<marker>Wang, Carroll, 2005</marker>
<rawString>Wang, Xinglong and John Carroll. 2005. Word Sense Disambiguation using sense examples automatically acquired from a second language. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 547–554, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>266--271</pages>
<location>Princeton, NJ.</location>
<contexts>
<context position="16216" citStr="Yarowsky 1993" startWordPosition="2430" endWordPosition="2431">valuation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selectional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). However, constructing a pseudoword by merely combining a random set of unambiguous words picked out to be in the same range of occurrence frequency (Sch¨utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation. For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an optimistic upper-bound estimate on the performance of WSD classifiers (Nakov and Hearst 2003). Several researchers addressed the issue of producing pseudowords that can model semantic relationships between senses. To this end Nako</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David. 1993. One sense per collocation. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 266–271, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="9031" citStr="Yarowsky 1995" startWordPosition="1360" endWordPosition="1361"> In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context). In an effort to address this issue, several approaches to the automatic acquisition of sense-tagged corpora have been proposed. Some of these approaches are based on bootstrapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iteratively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific monosemous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge resources, such as Wikipedia, have al</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised Word Sense Disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word Sense Disambiguation for all words without hard labor.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1--616</pages>
<location>Pasadena, CA.</location>
<contexts>
<context position="3631" citStr="Zhong and Ng 2009" startWordPosition="538" endWordPosition="541">r the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved</context>
<context position="10321" citStr="Zhong and Ng (2009)" startWordPosition="1557" endWordPosition="1560">, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. 839 Computational Linguistics Volume 40, Number 4 An alternative approach to acquiring sense-tagged data is to leverage multilingual resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous noun</context>
</contexts>
<marker>Zhong, Ng, 2009</marker>
<rawString>Zhong, Zhi and Hwee Tou Ng. 2009. Word Sense Disambiguation for all words without hard labor. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI), pages 1,616–1,622, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage Word Sense Disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>78--83</pages>
<location>Uppsala.</location>
<contexts>
<context position="4015" citStr="Zhong and Ng 2010" startWordPosition="600" endWordPosition="603">dably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of annotations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008; Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, however, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the sys</context>
<context position="10683" citStr="Zhong and Ng 2010" startWordPosition="1618" endWordPosition="1621">005; Chan, Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models,2 this approach can only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered. Middle-ground approaches have also been proposed that either mix arbitrary sensetagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and </context>
<context position="49266" citStr="Zhong and Ng 2010" startWordPosition="7612" endWordPosition="7615">data sets, we trained a supervised WSD system on the training set and applied it to the corresponding test set. In order to ensure more reliable results we follow Otrusina and Smrz (2010) and report, for all experiments in this evaluation, the average results for five runs. To this end, we randomly sampled the training and test data sets from the combination of all items while preserving the original proportions. Also, in the random setting, we provide the results averaged on a set of 25 different pseudowords modeling a given ambiguous noun. As our WSD system for this experiment, we used IMS (Zhong and Ng 2010), a state-of-the-art supervised WSD system that is based on support vector machines (we will describe IMS in more detail in Section 6.4). Note that we measure disambiguation difficulty in terms of the system’s recall performance (cf. Section 6.6 for evaluation measures). We present in Figure 1 the scatterplot of the recall performance (hence the disambiguation difficulty) for real words vs. those for the similarity-based, TS-based, and random pseudowords. For each set of pseudowords, we also show the line fitted to the corresponding set of points by means of linear regression. Ideally, this li</context>
<context position="84655" citStr="Zhong and Ng 2010" startWordPosition="13242" endWordPosition="13245">the training data set were split into 10 different subsets of varying size (from 80 sentences [i.e., 10% of training instances] to the full set of 800 sentences in 10 steps) while at the same time preserving the original sense distribution for all these sets. Overall, the data sets comprised about 2 million10 pseudosense-tagged sentences for each of the four configurations. 6.4 Systems We chose state-of-the-art off-the-shelf representatives for the two mainstream WSD paradigms, that is, supervised and knowledge-based WSD. 6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and Ng 2010) as the representative supervised WSD system. IMS is a publicly available English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks.11 The system classifies words in context using linear support vector machines. The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (Lee and Ng 2002). For each of the four configurations (see Section 6.3) and for each pseudoword, IMS was trained with the corresponding training set and the learned word expert model was th</context>
<context position="98545" citStr="Zhong and Ng 2010" startWordPosition="15553" endWordPosition="15556">.1%* to 90.3%* with 800), whereas UKB is less sensitive to training and test distributions (52.3%°–56.5%° with 80 training sentences, and 58.6%0–64.6%0 with 800). The 14 Symbols in the tables are for easier referencing. 867 Computational Linguistics Volume 40, Number 4 performance of IMS (Nat-Nat) with 160 training sentences is in line with competitive results on the Senseval-3 lexical sample data set (Mihalcea, Chklovski, and Kilgarriff 2004) in which there exist around 180 training sentences for each noun on average. In fact the latter are in the 73% ballpark against an MFS recall of 55.2% (Zhong and Ng 2010), whereas IMS obtains 84.4% against 70.5% MFS in our setting. The 15% shift in MFS is due to the sense distribution of our Nat data set, which is more skewed towards frequent senses compared to that of the Senseval-3 lexical sample data set. In addition, the average polysemy degree of our pseudowords is slightly lower than that of the Senseval-3 nouns (average polysemy 5.1 of pseudowords vs. 5.8 of Senseval-3 nouns), which also contributes to higher recall in our experiments. 7.2 Corroboration of Previous Findings on a Large Scale Before moving to a detailed analysis and discussion of our resu</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhong, Zhi and Hwee Tou Ng. 2010. It makes sense: A wide-coverage Word Sense Disambiguation system for free text. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 78–83, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principle of Least-Effort.</title>
<date>1949</date>
<publisher>Addison-Wesley,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="70772" citStr="Zipf 1949" startWordPosition="10905" endWordPosition="10906">word are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution Although the uniform distribution can be useful in specific applications such as dictionary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribution. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, SemCor provides reliable distribution estimates for only some hundred words. The table shows for each degree of polysemy the number of distinct nouns in SemCor that are s</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>Zipf, George K. 1949. Human Behavior and the Principle of Least-Effort. Addison-Wesley, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
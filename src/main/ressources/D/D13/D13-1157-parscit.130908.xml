<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.996228">
Inducing Document Plans for Concept-to-text Generation
</title>
<author confidence="0.970665">
Ioannis Konstas and Mirella Lapata
</author>
<affiliation confidence="0.997447">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.993828">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.997199">
ikonstas@inf.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.879876708333333">
In a language generation system, a content
planner selects which elements must be in-
cluded in the output text and the ordering be-
tween them. Recent empirical approaches per-
form content selection without any ordering
and have thus no means to ensure that the out-
put is coherent. In this paper we focus on
the problem of generating text from a database
and present a trainable end-to-end generation
system that includes both content selection
and ordering. Content plans are represented
intuitively by a set of grammar rules that op-
erate on the document level and are acquired
automatically from training data. We de-
velop two approaches: the first one is inspired
from Rhetorical Structure Theory and repre-
sents the document as a tree of discourse re-
lations between database records; the second
one requires little linguistic sophistication and
uses tree structures to represent global patterns
of database record sequences within a doc-
ument. Experimental evaluation on two do-
mains yields considerable improvements over
the state of the art for both approaches.
</bodyText>
<sectionHeader confidence="0.998926" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895311111111">
Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations including
databases, expert system knowledge bases, simula-
tions of physical systems, or formal meaning rep-
resentations. Generation systems typically follow
a pipeline architecture consisting of three compo-
nents: content planning (selecting and ordering the
parts of the input to be mentioned in the output text),
sentence planning (determining the structure and
lexical content of individual sentences), and surface
realization (verbalizing the chosen content in natu-
ral language). Traditionally, these components are
hand-engineered in order to ensure output of high
quality.
More recently there has been growing interest
in the application of learning methods because of
their promise to make generation more robust and
adaptable. Examples include learning which con-
tent should be present in a document (Duboue and
McKeown, 2002; Barzilay and Lapata, 2005), how it
should be aligned to utterances (Liang et al., 2009),
and how to select a sentence plan among many al-
ternatives (Stent et al., 2004). Beyond isolated com-
ponents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by treating sentence plan-
ning and surface realization as one component (An-
geli et al., 2010), by implementing content selection
without any document planning (Konstas and Lap-
ata, 2012; Angeli et al., 2010; Kim and Mooney,
2010), or by eliminating content planning entirely
(Belz, 2008; Wong and Mooney, 2007).
In this paper we present a trainable end-to-end
generation system that captures all components of
the traditional pipeline, including document plan-
ning. Rather than breaking up the generation pro-
cess into a sequence of local decisions, each learned
separately (Reiter et al., 2005; Belz, 2008; Chen and
Mooney, 2008; Kim and Mooney, 2010), our model
performs content planning (i.e., document planning
and content selection), sentence planning (i.e., lex-
</bodyText>
<page confidence="0.854807">
1503
</page>
<note confidence="0.83226">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.551139818181818">
Database Records
temp(time:6-21, min:9, mean:15, max:21)
wind-spd(time:6-21, min:15, mean:20, max:30)
sky-cover(time:6-9, percent:25-50)
sky-cover(time:9-12, percent:50-75)
wind-dir(time:6-21, mode:SSE)
gust(time:6-21, min:20, mean:30, max:40)
Output Text
Cloudy, with a high around 20. South southeast wind
between 15 and 30 mph. Gusts as high as 40 mph.
Database Records
</figure>
<bodyText confidence="0.925198">
desktop(cmd:lclick, name:start, type:button)
start(cmd:lclick, name:settings, type:button)
start-target(cmd:lclick, name:control panel, type:button)
win-target(cmd:dblclick, name:users and passwords, type:item)
contMenu(cmd:lclick, name:advanced, type:tab)
action-contMenu(cmd:lclick, name:advanced, type:button)
Output Text
Click start, point to settings, and then click control panel. Double-
click users and passwords. On the advanced tab, click advanced.
</bodyText>
<figure confidence="0.997223">
(a) WEATHERGOV (b) WINHELP
</figure>
<figureCaption confidence="0.711889333333333">
Figure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows trou-
bleshooting. Each record has a type (e.g., win-target), and a set of fields. Each field has a value, which
can be categorical (in typewriter), an integer (in bold), or a literal string (in italics).
</figureCaption>
<bodyText confidence="0.999942027777778">
icalization of input entries), and surface realization
jointly. We focus on the problem of generating text
from a database. The input to our model is a set of
database records and collocated descriptions, exam-
ples of which are shown in Figure 1.
Given this input, we define a probabilistic
context-free grammar (PCFG) that captures the
structure of the database and how it can be verbal-
ized. Specifically, we extend the model of Kon-
stas and Lapata (2012) which also uses a PCFG to
perform content selection and surface realization,
but does not capture any aspect of document plan-
ning. We represent content plans with grammar
rules which operate on the document level and are
embedded on top of the original PCFG. We essen-
tially learn a discourse grammar following two ap-
proaches. The first one is linguistically naive but
applicable to multiple languages and domains; it ex-
tracts rules representing global patterns of record
sequences within a sentence and among sentences
from a training corpus. The second approach learns
document plans based on Rhetorical Structure The-
ory (RST; Mann and Thomson, 1988); it therefore
has a solid linguistic foundation, but is resource in-
tensive as it assumes access to a text-level discourse
parser.
We learn document plans automatically using
both representations and develop a tractable decod-
ing algorithm for finding the best output, i.e., deriva-
tion in our grammar. To the best of our knowledge,
this is the first data-driven model to incorporate doc-
ument planning in a joint end-to-end system. Exper-
imental evaluation on the WEATHERGOV (Liang et
al., 2009) and WINHELP (Branavan et al., 2009) do-
mains shows that our approach improves over Kon-
stas and Lapata (2012) by a wide margin.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999896896551724">
Content planning is a fundamental component in a
natural generation system. Not only does it deter-
mine which information-bearing units to talk about,
but also arranges them into a structure that cre-
ates coherent output. It is therefore not surpris-
ing that many content planners have been based
on theories of discourse coherence (Hovy, 1993;
Scott and de Souza, 1990). Other work has re-
lied on generic planners (Dale, 1988) or schemas
(Duboue and McKeown, 2002). In all cases, con-
tent plans are created manually, sometimes through
corpus analysis. A few researchers recognize that
this top-down approach to planning is too inflexible
and adopt a generate-and-rank architecture instead
(Mellish et al., 1998; Karamanis, 2003; Kibble and
Power, 2004). The idea is to produce a large set
of candidate plans and select the best one according
to a ranking function. The latter is typically devel-
oped manually taking into account constraints relat-
ing to discourse coherence and the semantics of the
domain.
Duboue and McKeown (2001) present perhaps
the first empirical approach to content planning.
They use techniques from computational biology
to learn the basic patterns contained within a plan
and the ordering among them. Duboue and McK-
eown (2002) learn a tree-like planner from an
aligned corpus of semantic inputs and correspond-
ing human-authored outputs using evolutionary al-
</bodyText>
<page confidence="0.993988">
1504
</page>
<bodyText confidence="0.999970558823529">
gorithms. More recent data-driven work focuses on
end-to-end systems rather than individual compo-
nents, however without taking document planning
into account. For example, Kim and Mooney (2010)
first define a generative model similar to Liang et
al. (2009) that selects which database records to
talk about and then use an existing surface real-
izer (Wong and Mooney, 2007) to render the cho-
sen records in natural language. Their content plan-
ner has no notion of coherence. Angeli et al. (2010)
adopt a more unified approach that builds on top of
the alignment model of Liang et al. (2009). They
break record selection into a series of locally coher-
ent decisions, by first deciding on what records to
talk about. Each choice is based on a history of
previous decisions, which is encoded in the form
of discriminative features in a log-linear model.
Analogously, they choose fields for each record,
and finally verbalize the input using automatically
extracted domain-specific templates from training
data.
Konstas and Lapata (2012) propose a joint model,
which recasts content selection and surface realiza-
tion into a parsing problem. Their model optimizes
the choice of records, fields and words simultane-
ously, however they still select and order records lo-
cally. We replace their content selection mechanism
(which is based on a simple markovized chaining of
records) with global document representations. A
plan in our model is identified either as a sequence
of sentences, each containing a sequence of records,
or as a tree where the internal nodes denote dis-
course information and the leaf nodes correspond to
records.
</bodyText>
<sectionHeader confidence="0.982641" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999973846153846">
The generator takes as input a set of database
records d and outputs a text g that verbalizes some
of these records. Each record token ri ∈ d, with
1 ≤ i ≤ |d|, has a type ri.t and a set of fields f as-
sociated with it. Fields have different values f.v and
types f.t (i.e., integer, categorical, or literal strings).
For example, in Figure 1b, win-target is a record
type with three fields: cmd (denotes the action the
user must perform on an object on their screen,
e.g., left-click), name (denotes the name of the ob-
ject), and type (denotes the type of the object). The
values of these fields are dblclick, users and pass-
words, and item; name is a literal string, the rest are
</bodyText>
<subsectionHeader confidence="0.460983">
Grammar Rules
</subsectionHeader>
<listItem confidence="0.9961116">
1. S → R(start)
2. R(ri.t) → FS(rj,start) R(rj.t)  |FS(rj,start)
3. FS(r,r. fi) → F(r,r. fj) FS(r,r. fj)  |F(r,r. fj)
4. F(r,r.f) → W(r,r.f) F(r,r.f)  |W(r,r.f)
5. W(r,r.f) → a  |g(f.v)
</listItem>
<figureCaption confidence="0.96817">
Figure 2: Grammar G of the original model. Paren-
theses denote features, and impose constraints on the
grammar.
</figureCaption>
<bodyText confidence="0.987667416666667">
categorical.
During training, our algorithm is given a corpus
consisting of several scenarios, i.e., database records
paired with texts w (see Figure 1). For each sce-
nario, the model first decides on a global document
plan, i.e., it selects which types of records belong to
each sentence (or phrase) and how these sentences
(or phrases) should be ordered. Then it selects ap-
propriate record tokens for each type and progres-
sively chooses the most relevant fields; then, based
on the values of the fields, it generates the final text,
word by word.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="method">
4 Original Model
</sectionHeader>
<bodyText confidence="0.99997145">
Our work builds on the model developed by Kon-
stas and Lapata (2012). The latter is essentially
a PCFG which captures both the structure of the
input database and the way it renders into natural
language. This grammar-based approach lends it-
self well to the incorporation of document planning
which has traditionally assumed tree-like represen-
tations. We first briefly describe the original model
and then present our extensions in Section 5.
Grammar Grammar G in Figure 2 defines a set
of non-recursive CFG rewrite rules that capture the
structure of the database, i.e., the relationship be-
tween records, records and fields, fields and words.
These rules are domain-independent and could be
applied to any database provided it follows the same
structure. Non-terminal symbols are in capitals, the
terminal symbol a corresponds to the vocabulary of
the training set and g(f .v) is a function which gener-
ates integers given the field value f.v. Note that all
non-terminals have features (in parentheses) which
</bodyText>
<page confidence="0.947822">
1505
</page>
<bodyText confidence="0.999897942307692">
act as constraints and impose non-recursion (e.g., in
rule (2) i =� j, so that a record cannot emit itself).
Rule (1) defines the expansion from the start sym-
bol S to the first record R of type start. The rules
in (2) implement content selection, by choosing ap-
propriate records from the database and generating
a sequence. R(ri.t) is the source record, R(rj.t) is
the target record and FS(rj.start) is a place-holder
symbol for the set of fields of record token rj. This
method is locally optimal, since it only keeps track
of the previous type of record for each re-write. The
rules in (3) conclude content selection on the field
level, i.e., after we have chosen a record, we select
and order the corresponding fields. Finally, the rules
in (4) and (5) correspond to surface realization. The
former rule binarizes the sequence of words emitted
by a particular field r. f in an attempt to capture local
dependencies between words, such as multi-word
expressions (e.g., right click, radio button). The lat-
ter rule defines the emission of words and integer
numbers1, given a field type and its value. Note that
the original model lexicalizes field values of cate-
gorical and integer type only.
Training The rules of grammar G are associated
with weights that are learned using the EM algo-
rithm (Dempster et al., 1977). During training, the
records, fields and values of database d and the
words w from the associated text are observed, and
the model learns the mapping between them. Notice
that we use w to denote the gold-standard text and
g to refer to the words generated by the model. The
mapping between the database and the observed text
is unknown and thus the weights of the rules define
a hidden correspondence h between records, fields
and their values.
Decoding Given a trained grammar G and an in-
put scenario from a database d, the model generates
text by finding the most likely derivation, i.e., se-
quence of rewrite rules for the input. Although re-
sembling parsing, the generation task is subtly dif-
ferent. In parsing, we observe a string of words and
our goal is to find the most probable syntactic struc-
ture, i.e., hidden correspondence ˆh. In generation,
1The function g(f.v) : Z → Z, generates an integer in the
following six ways (Liang et al., 2009): identical, rounding
up/down to a multiple of 5, rounding off a multiple of 5 and
adding or subtracting some noise modelled by a geometric dis-
tribution.
however, the string is not observed; instead, we must
find the best text ˆg, by maximizing both over h and g,
where g = g1 ...gN is a sequence of words licensed
by G. More formally:
</bodyText>
<equation confidence="0.999502">
gˆ = f
(argmaxP((g,h))) (1)
</equation>
<bodyText confidence="0.999981666666667">
where f is a function that takes as input a derivation
tree (g,h) and returns ˆg. Konstas and Lapata (2012)
use a modified version of the CYK parser (Kasami,
1965; Younger, 1967) to find ˆg. Specifically, they
intersect grammar G with a n-gram language model
and calculate the most probable generation gˆ as:
</bodyText>
<equation confidence="0.9705">
g = f \arg axp(g) -p(g,h  |d)I (2)
</equation>
<bodyText confidence="0.999978166666667">
where p(g,h|d) is the decoding likelihood for a se-
quence of words g = g1 ...gN of length N and the
hidden correspondence h that emits it, i.e., the likeli-
hood of the grammar for a given database input sce-
nario d. p(g) is a measure of the quality of each out-
put and is provided by the n-gram language model.
</bodyText>
<sectionHeader confidence="0.995802" genericHeader="method">
5 Extensions
</sectionHeader>
<bodyText confidence="0.99991825">
In this section we extend the model of Konstas and
Lapata (2012) by developing two more sophisticated
content selection approaches which are informed by
a global plan of the document to be generated.
</bodyText>
<subsectionHeader confidence="0.999647">
5.1 Planning with Record Sequences
</subsectionHeader>
<bodyText confidence="0.999983285714286">
Grammar Our key idea is to replace the content
selection mechanism of the original model with a
document plan which essentially defines a gram-
mar on record types. We split a document into
sentences, each terminated by a full-stop. Then a
sentence is further split into a sequence of record
types. Contrary to the original model, we observe a
complete sequence2 of record types, split into sen-
tences. This way we learn domain-specific pat-
terns of frequently occurring record type sequences
among the sentences of a document, as well as more
local structures within a sentence. We thus substitute
rules (1)–(2) in Figure 2 with sub-grammar GRSE
based on record type sequences:
</bodyText>
<equation confidence="0.797439">
Definition 1(GRSE grammar)
GRSE = {ER, NRSE, PRSE, D}
</equation>
<footnote confidence="0.954996">
2Note that a sequence is different from a permutation, as we
may allow repetitions or omissions of certain record types.
</footnote>
<page confidence="0.989526">
1506
</page>
<bodyText confidence="0.998266">
where ER is a set of terminal symbols R(r.t), and
NRSE is a set of non-terminal symbols:
</bodyText>
<equation confidence="0.858936">
NRSE = {D, SENT}
</equation>
<bodyText confidence="0.999647">
where D represents the start symbol and SENT a
sequence of records. PRSE is a set of production rules
of the form:
</bodyText>
<figure confidence="0.9274765">
(a) D → SENT(ti, ..., tj) ... SENT(tl, ..., tm)
(b) SENT(ti, ..., tj) → R(ra.ti) ... R(rk.tj) ·
</figure>
<bodyText confidence="0.99602925">
where t is a record type, ti, tj, tl and tm may overlap
and ra, rk are record tokens of type ti and tj respec-
tively. The corresponding weights for the production
rules PRSE are:
</bodyText>
<equation confidence="0.9524836">
Definition 2 (GRSE weights)
(a) p(ti, ..., tj, ... tl, ...,tm  |D)
(b) p(ti) · ... · p(tj) = 1
|s(ti) |· ... · 1
|s(tj)|
</equation>
<bodyText confidence="0.999907454545455">
where s(t) is a function that returns the set of records
with type t (Liang et al., 2009).
Rule (a) defines the expansion from the start sym-
bol D to a sequence of sentences, each represented
by the non-terminal SENT. Similarly to the original
grammar G, we employ the use of features (in paren-
theses) to denote a sequence of record types. The
same record types may recur in different sentences,
but not in the same one. The weight of rule (a) is
simply the joint probability of all the record types
present, ordered and segmented appropriately into
sentences in the document, given the start symbol.
Once record types have been selected (on a per
sentence basis) we move on to rule (b) which de-
scribes how each non-terminal SENT expands to
an ordered sequence of records R, as they are ob-
served within a sentence (see the terminal sym-
bol ‘.’ at the end of the rule). Notice that a record
type ti may correspond to several record tokens ra.
Rules (3)–(5) in grammar G make decisions on these
tokens based on the overall content of the database
and the field/value selection. The weight of this
rule is the product of the weights of each record
type. This is set to the uniform distribution over
{1, ..., |s(t)|} for record type t, where |s(t) |is the
number of records with that type.
Figure 3d shows an example tree for the database
input in Figure 1b, using GRSE and assuming that the
alignments between records and text are given. The
top level of the tree refers to the sequence of record
types as they are observed in the text. The first sen-
tence contains three records with types ‘desktop’,
‘start’ and ‘start-target’, each corresponding to the
textual segments click start, point to settings, and
then click control panel. The next level on the tree,
denotes the choice of record tokens for each sen-
tence, provided that we have decided on the choice
and order of their types (see Figure 3b). In Fig-
ure 3d, the bottom-left sub-tree corresponds to the
choice of the first three records of Figure 1b.
Training A straightforward way to train the ex-
tended model would be to embed the parameters of
GRSE in the original model and then run the EM al-
gorithm using inside-outside at the E-step. Unfortu-
nately, this method will induce a prohibitively large
search space. Rule (a) enumerates all possible com-
binations of record type sequences and the number
grows exponentially even for a few record types and
a small sequence size. To tackle this problem, we ex-
tracted rules for GRSE from the training data, based
on the assumption that there will be far fewer unique
sequences of record types per dataset than exhaus-
tively enumerating all possibilities.
For each scenario, we obtain a word-by-word
alignment between the database records and the cor-
responding text. In our experiments we used Liang
et al.’s (2009) unsupervised model, however any
other semi- or fully supervised method could be
used. As we show in Section 7, the quality of the
alignment inevitably correlates with the quality of
the extracted grammar and the decoder’s output. We
then map the aligned record tokens to their corre-
sponding types, merge adjacent words with the same
type and segment on punctuation (see Figure 3b).
Next, we create the corresponding tree according to
GRSE (Figure 3d) and binarize it. We experimented
both with left and right binarization and adhered to
the latter, as it obtained a more compact set of rules.
Finally, we collectively count the rule weights on the
resulting treebank and extract a rule set, discarding
rules with frequency less than three.
Using the extracted (weighted) GRSE rules, we run
the EM algorithm via inside-outside and learn the
weights for the remaining rules in G. Decoding re-
mains the same as in Konstas and Lapata (2012);
the only requirement is that the extracted grammar
remains binarized in order to guarantee the cubic
</bodyText>
<page confidence="0.860994">
1507
</page>
<bodyText confidence="0.5422175">
desktop1 start1 start-target1 win-target1 contMenu1 action-contMenu1
Click start, point to settings, and then click control panel. Double-click users and passwords. On the advanced tab, click advanced.
</bodyText>
<figure confidence="0.929455961538461">
(a) Record token alignments
[ ] [Click start,]desktop1.t [point to settings, ]start1.t [and then
desktop start start-targetkwin-targetkcontMenu action-contMenuk click control panel.]start−target1.t [Double-click users and
(b) Record type segmentation passwords.]win−target1.t [On the advanced tab,]contMenu1.t [click
advanced.]action−contMenu1.t
(c) Segmentation of text into EDUs
D
Elaboration[N][S]
R(a-c
R(c1.t)
Elaboration[N][S]
Elaboration[N][S] R(w-t1.t)
J
Elaboration[N][S]
SENT(w-t)
SENT(c, a-c)
o i nt
SENT(d, s, s-t)
D
R(d1.t) R(s R(s-t R(w-t1.t) R(c1.t) R(a-c
1 . t)
(d) Document plan using the GRSE grammar
1 .t) ok
1508
Definition 3 (GRST grammar)
GRST = {ER, NRST, PRST, D}
</figure>
<bodyText confidence="0.999974093023256">
where ER is the alphabet of leaf nodes as de-
fined in Section 5.1, NRST is a set of non-terminals
corresponding to rhetorical relations augmented
with nucleus-satellite information (e.g., Elabora-
tion[N][S] stands for the elaboration relation be-
tween the nucleus EDU left-adjoining with the satel-
lite EDU), PRST is the set of production rules of the
form PRST C NRST X {NRST U ER} X {NRST U ER} as-
sociated with a weight for each rule, and D E NRST
is the root symbol. Figure 3e gives the discourse tree
for the database input of Figure 1b, using GRST.
Training In order to obtain the weighted produc-
tions of GRST, we use an existing state-of-the-art dis-
course parser3 (Feng and Hirst, 2012) trained on the
RST-DT corpus (Carlson et al., 2001). The latter
contains a selection of 385 Wall Street Journal arti-
cles which have been annotated using the framework
of RST and an inventory of 78 rhetorical relations,
classified into 18 coarse-grained categories (Carl-
son and Marcu, 2001). Figure 4 gives a comparison
of the distribution of relations extracted for the two
datasets we used, against the gold-standard annota-
tion of RST-DT. The statistics for the RST-DT cor-
pus are taken from Williams and Power (2008). The
relative frequencies of relations on both datasets fol-
low closely the distribution of those in RST-DT, thus
empirically supporting the application of the RST
framework to our data.
We segment each document in our training set
into EDUs based on the record-to-text alignments
given by the model of Liang et al. (2009) (see Fig-
ure 3c). We then run the discourse parser on the
resulting EDUs, and retrieve the corresponding dis-
course tree; the internal nodes are labelled with
one of the RST relations. Finally, we replace the
leaf EDUs with their respective terminal symbols
R(r.t) E ER (Figure 3e) and collect the resulting
grammar productions; their weights are calculated
via maximum likelihood estimation based on their
collective counts in the parse trees licensed by GRST.
Training and decoding of the extended generation
model (after we embed GRST in the original gram-
mar G) is performed identically to Section 5.1.
</bodyText>
<footnote confidence="0.907557">
3Publicly available from http://www.cs.toronto.edu/
˜weifeng/software.html.
</footnote>
<sectionHeader confidence="0.988711" genericHeader="method">
6 Experimental Design
</sectionHeader>
<bodyText confidence="0.999416909090909">
Data Since our aim was to evaluate the planning
component of our model, we used datasets whose
documents are at least a few sentences long. Specif-
ically, we generated weather forecasts and trou-
bleshooting guides for an operating system. For
the first domain (henceforth WEATHERGOV) we used
the dataset of Liang et al. (2009), which consists
of 29,528 weather scenarios for 3,753 major US
cities (collected over four days). The database has
12 record types, each scenario contains on average
36 records, 5.8 out of which are mentioned in the
text. A document has 29.3 words and is four sen-
tences long. The vocabulary is 345 words. We used
25,000 scenarios from WEATHERGOV for training,
1,000 scenarios for development and 3,528 scenar-
ios for testing.
For the second domain (henceforth WINHELP) we
used the dataset of Branavan et al. (2009), which
consists of 128 scenarios. These are articles from
Microsoft’s Help and Support website4 and contain
step-by-step instructions on how to perform tasks on
the Windows 2000 operating system. In its original
format, the database provides a semantic representa-
tion of the textual guide, i.e., it represents the user’s
actions on the operating system’s UI. We semi-
automatically converted this representation into a
schema of records, fields and values, following the
conventions adopted in Branavan et al. (2009).5 The
final database has 13 record types. Each scenario has
9.2 records and each document 51.92 words with 4.3
sentences. The vocabulary is 629 words. We per-
formed 10-fold cross-validation on the entire dataset
for training and testing. Compared to WEATHER-
GOV, WINHELP documents are longer with a larger
vocabulary. More importantly, due to the nature of
the domain, i.e., giving instructions, content selec-
tion is critical not only in terms of what to say but
also in what order.
Grammar Extraction and Parameter Setting
We obtained alignments between database records
and textual segments for both domains and gram-
mars (GRSE and GRST) using the unsupervised model
of Liang et al. (2009). On WEATHERGOV, we ex-
tracted a GRSE grammar with 663 rules (after bi-
</bodyText>
<footnote confidence="0.947923666666667">
4support.microsoft.com
5The dataset can be downloaded from http://homepages.
inf.ed.ac.uk/ikonstas/index.php?page=resources
</footnote>
<page confidence="0.990861">
1509
</page>
<figure confidence="0.999823652173913">
RST-DT
WEATHERGOV
WINHELP
60
40
20
0
Elaboration
Attribution
Joint
Contrast
Explanation
Background
Enablement
Cause
Evaluation
Comparison
Condition
Topic-Comment
Temporal
Explanation
Summary
Topic Change
</figure>
<figureCaption confidence="0.9695805">
Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power,
2008).
</figureCaption>
<bodyText confidence="0.976243583333334">
narization). The WINHELP dataset is considerably
smaller, and as a result the procedure described in
Section 5.1 yields a very sparse grammar. To al-
leviate this, we horizontally markovized the right-
hand side of each rule (Collins, 1999; Klein and
Manning, 2003).6 After markovization, we obtained
a GRSE grammar with 516 rules. On WEATHERGOV,
we extracted 434 rules for GRST. On WINHELP we
could not follow the horizontal markovization pro-
cedure, since the discourse trees are already bina-
rized. Instead, we performed vertical markovization,
i.e., annotated each non-terminal with their parent
node (Johnson, 1998) and obtained a GRST grammar
with 419 rules. The model of Konstas and Lapata
(2012) has two parameters, namely the number of
k-best lists to keep in each derivation, and the or-
der of the language model. We tuned k experimen-
tally on the development set and obtained best re-
sults with 60 for WEATHERGOV and 120 for WIN-
HELP. We used a trigram model for both domains,
trained on each training set.
Evaluation We compared two configurations of
our system, one with a content planning compo-
nent based on record type sequences (GRSE) and
6When horizontally markovizing, we can encode an arbi-
trary amount of context in the intermediate non-terminals that
result from this process; in our case we store h=1 horizontal
siblings plus the mother left-hand side (LHS) non-terminal, in
order to uniquely identify the Markov chain. For example,
A → B C D becomes A → B (A...B), (A...B) → C (A...C),
(A...C) → D.
another one based on RST (GRST). In both cases
content plans were extracted from (noisy) unsuper-
vised alignments. As a baseline, we used the orig-
inal model of Konstas and Lapata (2012). We also
compared our model to Angeli et al.’s system (2010),
which is state of the art on WEATHERGOV.
System output was evaluated automatically, using
the BLEU modified precision score (Papineni et al.,
2002) with the human-written text as reference. In
addition, we evaluated the generated text by eliciting
human judgments. Participants were presented with
a scenario and its corresponding verbalization and
were asked to rate the latter along three dimensions:
fluency (is the text grammatical?), semantic correct-
ness (does the meaning conveyed by the text corre-
spond to the database input?) and coherence (is the
text comprehensible and logically structured?). Par-
ticipants used a five point rating scale where a high
number indicates better performance. We randomly
selected 12 documents from the test set (for each do-
main) and produced output with the system of Kon-
stas and Lapata (2012) (henceforth K&amp;L), our two
models using GRSE and GRST, respectively, and An-
geli et al. (2010) (henceforth ANGELI). We also in-
cluded the original text (HUMAN) as gold-standard.
We obtained ratings for 60 (12 x 5) scenario-text
pairs for each domain. Examples of the documents
shown to the participants are given in Table 1.
The study was conducted over the Internet us-
</bodyText>
<page confidence="0.958663">
1510
</page>
<table confidence="0.999815545454545">
WEATHERGOV WINHELP
GR Showers before noon. Cloudy, with a high near Right-click my network places, and then click prop-
38. Southwest wind between 3 and 8 mph. erties. Right-click local area connection, and click
Chance of precipitation is 55 %. properties. Click to select the file and printer sharing
for Microsoft networks, and then click ok.
Showers likely. Mostly cloudy, with a high around Right-click my network places, and then click proper-
38. South wind between 1 and 8 mph. Chance of ties. Right-click local area connection. Click file and
precipitation is 55 %. printer sharing for Microsoft networks, and click ok.
L A chance of showers. Otherwise, cloudy, with a Right-click my network places, click properties.
K high near 38. Southwest wind between 3 and 8 Right-click local area connection. Click to select the
mph. file and printer sharing for Microsoft networks, and
then click ok.
LI A chance of rain or drizzle after 9am. Mostly Right-click my network places, and then click prop-
E cloudy, with a high near 38. Southwest wind be- erties on the tools menu, and then click proper-
NG tween 3 and 8 mph. Chance of precipitation is 50 ties. Right-click local area connection, and then click
A % properties. Click file and printer sharing for Microsoft
networks, and then click ok.
N A 50 percent chance of showers. Cloudy, with a Right-click my network places, and then click proper-
A high near 38. Southwest wind between 3 and 6 ties. Right-click local area connection, and then click
2 mph. properties. Click to select the file and printer sharing
� for Microsoft networks check box. Click ok.
H
</table>
<tableCaption confidence="0.99994">
Table 1: Human-authored text and system output on WEATHERGOV and WINHELP.
</tableCaption>
<bodyText confidence="0.999811571428571">
ing Amazon Mechanical Turk7, and involved 200
volunteers (100 for WEATHERGOV, and 100 for
WINHELP), all self reported native English speak-
ers. For WINHELP, we made sure participants were
computer-literate and familiar with the Windows op-
erating system by administering a short question-
naire prior to the experiment.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999659625">
The results of the automatic evaluation are summa-
rized in Table 2. Overall, our models outperform
K&amp;L’s system by a wide margin on both datasets.
The two content planners (GRSE and GRST) perform
comparably in terms of BLEU. This suggests that
document plans induced solely from data are of sim-
ilar quality to those informed by RST. This is an
encouraging result given that RST-style discourse
parsers are currently available only for English. AN-
GELI performs better on WEATHERGOV possibly due
to better output quality on the surface level. Their
system defines trigger patterns that specifically lexi-
calize record fields containing numbers. In contrast,
on WINHELP it is difficult to explicitly specify such
patterns, as none of the record fields are numeric; as
a result their system performs poorly compared to
</bodyText>
<footnote confidence="0.953632">
7https://www.mturk.com
</footnote>
<bodyText confidence="0.984621076923077">
the other models.
To assess the impact of the alignment on the
content planner, we also extracted GRSE from
cleaner alignments which we obtained automat-
ically via human-crafted heuristics for each do-
main. The heuristics performed mostly anchor
matching between database records and words in the
text (e.g., the value Lkly of the field rainChance,
matches with the string rain likely in the text).
Using these alignments, GRSE obtained a BLEU
score of 39.23 on WEATHERGOV and 41.35 on WIN-
HELP. These results indicate that improved align-
ments would lead to more accurate grammar rules.
WEATHERGOV seems more sensitive to the align-
ments than WINHELP. This is probably because
the dataset shows more structural variations in the
choice of record types at the document level, and
therefore the grammar extracted from the unsuper-
vised alignments is noisier. Unfortunately, perform-
ing this kind of analysis for GRST would require gold
standard segmentation of our training corpus into
EDUs which we neither have nor can easily approx-
imate via heuristics.
The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system
</bodyText>
<page confidence="0.961882">
1511
</page>
<table confidence="0.993831">
Model WEATHERGOV WINHELP
GRSE 35.60 40.92
GRST 36.54 40.65
K&amp;L 33.70 38.26
ANGELI 38.40 32.21
</table>
<tableCaption confidence="0.9891035">
Table 2: Automatic evaluation of system output us-
ing BLEU-4.
</tableCaption>
<table confidence="0.999866714285714">
Model WEATHERGOV WINHELP
FL SC CO FL SC CO
GRSE 4.25 3.75 4.18 3.59 3.21 3.35
GRST 4.10 3.68 4.10 3.45 3.29 3.22
K&amp;L 3.73 3.25 3.59 3.27 2.97 2.93
ANGELI 3.90 3.44 3.82 3.44 2.79 2.97
HUMAN 4.22 3.72 4.11 4.20 4.41 4.25
</table>
<tableCaption confidence="0.704023">
Table 3: Mean ratings for fluency (FL), semantic
correctness (SC) and coherence (CO) on system out-
put elicited by humans.
</tableCaption>
<bodyText confidence="0.99963725">
type (GRSE, GRST, K&amp;L, ANGELI, and HUMAN) on
fluency, semantic correctness and coherence ratings.
Means differences of 0.2 or more are significant at
the 0.05 level using a post-hoc Tukey test. Interest-
ingly, we observe that document planning improves
system output overall, not only in terms of coher-
ence. Across all dimensions our models are per-
ceived better than K&amp;L and ANGELI. As far as co-
herence is concerned, the two content planners are
rated comparably (differences in the means are not
significant). Both GRSE and GRST are significantly
better than the comparison systems (ANGELI and
K&amp;L). Table 1 illustrates examples of system out-
put along with the gold standard content selection
for reference, for the WEATHERGOV and WINHELP
domains, respectively.
In sum, we observe that integrating document
planning either via GRSE or GRST boosts perfor-
mance. Document plans induced from record
sequences exhibit similar performance, compared
to those generated using expert-derived linguistic
knowledge. Our systems are consistently better than
K&amp;L both in terms of automatic and human eval-
uation and are close or better than the supervised
model of Angeli et al. (2010). We also show that
feeding the system with a grammar of better qual-
ity can achieve state-of-the-art performance, without
further changes to the model.
</bodyText>
<sectionHeader confidence="0.996879" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999980111111111">
In this paper, we have proposed an end-to-end sys-
tem that generates text from database input and cap-
tures all components of the traditional generation
pipeline, including document planning. Document
plans are induced automatically from training data
and are represented intuitively by PCFG rules cap-
turing the structure of the database and the way it
renders to text. We proposed two complementary
approaches to inducing content planners. In a first
linguistically naive approach, a document is mod-
elled as a sequence of sentences and each sentence
as a sequence of records. Our second approach
draws inspiration from Rhetorical Structure Theory
(Mann and Thomson, 1988) and represents a docu-
ment as a tree with intermediate nodes correspond-
ing to discourse relations, and leaf nodes to database
records.
Experiments with both approaches demonstrate
improvements over models that do not incorporate
document planning. In the future, we would like to
tackle more challenging domains, such as NFL re-
caps, financial articles and biographies (Howald et
al., 2013; Schilder et al., 2013). Our models could
also benefit from the development of more sophis-
ticated planners either via grammar refinement or
more expressive grammar formalisms (Cohn et al.,
2010).
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999825333333333">
We are grateful to Percy Liang and Gabor Angeli
for providing us with their code and data. Thanks to
Giorgio Satta and Charles Sutton for helpful com-
ments and suggestions. We also thank the members
of the Probabilistic Models reading group at the Uni-
versity of Edinburgh for useful feedback.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.963083375">
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502–512, Cambridge, MA.
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and
</reference>
<page confidence="0.945241">
1512
</page>
<reference confidence="0.999414773584905">
Empirical Methods in Natural Language Processing,
pages 331–338, Vancouver, British Columbia.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431–455.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82–90, Suntec, Singapore.
L. Carlson and D. Marcu. 2001. Discourse tagging ref-
erence manual. Technical report, Univ. of Southern
California / Information Sciences Institute.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue - Volume 16, SIGDIAL ’01, pages 1–10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128–135, Helsinki, Finland.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11(November):3053–
3096.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Robert Dale. 1988. Generating referring expressions in
a domain of objects and processes. Ph.D. thesis, Uni-
versity of Edinburgh.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1):1–38.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proceedings of the 39th An-
nual Meeting on Association for Computational Lin-
guistics, pages 172–179.
Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89–96, Ramapo Mountains, NY.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 60–68, Jeju
Island, Korea.
Eduard Hovy. 1993. Automated discourse generation
using discourse structure relations. Artificial Intelli-
gence, 63:341–385.
Blake Howald, Ravikumar Kondadadi, and Frank
Schilder. 2013. Domain adaptable semantic clustering
in statistical nlg. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013) – Long Papers, pages 143–154, Potsdam, Ger-
many, March. Association for Computational Linguis-
tics.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613–
632, December.
Nikiforos Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, University of
Edinburgh.
Tadao Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Lab, Bedford, MA.
Rodger Kibble and Richard Power. 2004. Optimising
referential coherence in text generation. Computa-
tional Linguistics, 30(4):401–416.
Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543–551,
Beijing, China.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423–430. Association for Computa-
tional Linguistics Morristown, NJ, USA.
Ioannis Konstas and Mirella Lapata. 2012. Unsupervised
concept-to-text generation with hypergraphs. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 752–
761, Montr´eal, Canada.
Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91–99, Suntec, Singapore.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
William C. Mann and Sandra A. Thomson. 1988.
Rhetorical structure theory. Text, 8(3):243–281.
</reference>
<page confidence="0.560272">
1513
</page>
<reference confidence="0.999878063829787">
Chris Mellish, Alisdair Knott, Jon Oberlander, and Mick
O’Donnell. 1998. Experiments using stochastic
search for text planning. In Proceedings of Interna-
tional Natural Language Generation, pages 98–107,
New Brunswick, NJ.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.
Frank Schilder, Blake Howald, and Ravi Kondadadi.
2013. Gennext: A consolidated domain adaptable nlg
system. In Proceedings of the 14th European Work-
shop on Natural Language Generation, pages 178–
182, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Donia Scott and Clarisse Sieckenius de Souza. 1990.
Getting the message across in RST-based text gener-
ation. In Robert Dale, Chris Mellish, and Michael
Zock, editors, Current Research in Natural Language
Generation, pages 47–73. Academic Press, New York.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of Association for Computational Linguis-
tics, pages 79–86, Barcelona, Spain.
Sandra Williams and Richard Power. 2008. Deriving
rhetorical complexity data from the rst-dt corpus. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08), May.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172–179, Rochester, NY.
Daniel H Younger. 1967. Recognition and parsing for
context-free languages in time n3. Information and
Control, 10(2):189–208.
</reference>
<page confidence="0.994409">
1514
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656833">
<title confidence="0.997784">Inducing Document Plans for Concept-to-text Generation</title>
<author confidence="0.673993">Konstas</author>
<affiliation confidence="0.9944305">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.984161">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998112">ikonstas@inf.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.9996428">In a language generation system, a content planner selects which elements must be included in the output text and the ordering between them. Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. We develop two approaches: the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>502--512</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="2882" citStr="Angeli et al., 2010" startWordPosition="430" endWordPosition="434">e of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e.,</context>
<context position="8527" citStr="Angeli et al. (2010)" startWordPosition="1285" endWordPosition="1288">tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al1504 gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al. (2009) that selects which database records to talk about and then use an existing surface realizer (Wong and Mooney, 2007) to render the chosen records in natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the form of discriminative features in a log-linear model. Analogously, they choose fields for each record, and finally verbalize the input using automatically extracted domain-specific templates from training data. Konstas and Lapata (2012) propose a joint model, which recasts content selection and sur</context>
<context position="29453" citStr="Angeli et al. (2010)" startWordPosition="4777" endWordPosition="4781">rio and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test set (for each domain) and produced output with the system of Konstas and Lapata (2012) (henceforth K&amp;L), our two models using GRSE and GRST, respectively, and Angeli et al. (2010) (henceforth ANGELI). We also included the original text (HUMAN) as gold-standard. We obtained ratings for 60 (12 x 5) scenario-text pairs for each domain. Examples of the documents shown to the participants are given in Table 1. The study was conducted over the Internet us1510 WEATHERGOV WINHELP GR Showers before noon. Cloudy, with a high near Right-click my network places, and then click prop38. Southwest wind between 3 and 8 mph. erties. Right-click local area connection, and click Chance of precipitation is 55 %. properties. Click to select the file and printer sharing for Microsoft networ</context>
<context position="35440" citStr="Angeli et al. (2010)" startWordPosition="5760" endWordPosition="5763">er than the comparison systems (ANGELI and K&amp;L). Table 1 illustrates examples of system output along with the gold standard content selection for reference, for the WEATHERGOV and WINHELP domains, respectively. In sum, we observe that integrating document planning either via GRSE or GRST boosts performance. Document plans induced from record sequences exhibit similar performance, compared to those generated using expert-derived linguistic knowledge. Our systems are consistently better than K&amp;L both in terms of automatic and human evaluation and are close or better than the supervised model of Angeli et al. (2010). We also show that feeding the system with a grammar of better quality can achieve state-of-the-art performance, without further changes to the model. 8 Conclusions In this paper, we have proposed an end-to-end system that generates text from database input and captures all components of the traditional generation pipeline, including document planning. Document plans are induced automatically from training data and are represented intuitively by PCFG rules capturing the structure of the database and the way it renders to text. We proposed two complementary approaches to inducing content plann</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502–512, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>331--338</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="2452" citStr="Barzilay and Lapata, 2005" startWordPosition="361" endWordPosition="364">lecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content plannin</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing, pages 331–338, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="3074" citStr="Belz, 2008" startWordPosition="462" endWordPosition="463">uld be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, Seattle, Wa</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>82--90</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6546" citStr="Branavan et al., 2009" startWordPosition="966" endWordPosition="969">roach learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are</context>
<context position="25057" citStr="Branavan et al. (2009)" startWordPosition="4084" endWordPosition="4087">an operating system. For the first domain (henceforth WEATHERGOV) we used the dataset of Liang et al. (2009), which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days). The database has 12 record types, each scenario contains on average 36 records, 5.8 out of which are mentioned in the text. A document has 29.3 words and is four sentences long. The vocabulary is 345 words. We used 25,000 scenarios from WEATHERGOV for training, 1,000 scenarios for development and 3,528 scenarios for testing. For the second domain (henceforth WINHELP) we used the dataset of Branavan et al. (2009), which consists of 128 scenarios. These are articles from Microsoft’s Help and Support website4 and contain step-by-step instructions on how to perform tasks on the Windows 2000 operating system. In its original format, the database provides a semantic representation of the textual guide, i.e., it represents the user’s actions on the operating system’s UI. We semiautomatically converted this representation into a schema of records, fields and values, following the conventions adopted in Branavan et al. (2009).5 The final database has 13 record types. Each scenario has 9.2 records and each doc</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82–90, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
</authors>
<title>Discourse tagging reference manual.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>Univ. of Southern California / Information Sciences Institute.</institution>
<contexts>
<context position="22966" citStr="Carlson and Marcu, 2001" startWordPosition="3746" endWordPosition="3750">RST C NRST X {NRST U ER} X {NRST U ER} associated with a weight for each rule, and D E NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST. Training In order to obtain the weighted productions of GRST, we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data. We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>L. Carlson and D. Marcu. 2001. Discourse tagging reference manual. Technical report, Univ. of Southern California / Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue - Volume 16, SIGDIAL ’01,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22728" citStr="Carlson et al., 2001" startWordPosition="3709" endWordPosition="3712">rical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST C NRST X {NRST U ER} X {NRST U ER} associated with a weight for each rule, and D E NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST. Training In order to obtain the weighted productions of GRST, we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporti</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue - Volume 16, SIGDIAL ’01, pages 1–10, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>128--135</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="3415" citStr="Chen and Mooney, 2008" startWordPosition="513" endWordPosition="516">reating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Database Records temp(time:6-21, min:9, mean:15, max:21) wind-spd(time:6-21, min:15, mean:20, max:30) sky-cover(time:6-9, percent:25-50) sky-cover(time:9-12, percent:50-75) wind-dir(time:6-21, mode:SSE) gust(time:6-21, min:20, mean:30, max:40) Output Text C</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of International Conference on Machine Learning, pages 128–135, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>3096</pages>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, 11(November):3053– 3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27005" citStr="Collins, 1999" startWordPosition="4377" endWordPosition="4378">p://homepages. inf.ed.ac.uk/ikonstas/index.php?page=resources 1509 RST-DT WEATHERGOV WINHELP 60 40 20 0 Elaboration Attribution Joint Contrast Explanation Background Enablement Cause Evaluation Comparison Condition Topic-Comment Temporal Explanation Summary Topic Change Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power, 2008). narization). The WINHELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for GRST. On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experim</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating referring expressions in a domain of objects and processes.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="7075" citStr="Dale, 1988" startWordPosition="1058" endWordPosition="1059">luation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (20</context>
</contexts>
<marker>Dale, 1988</marker>
<rawString>Robert Dale. 1988. Generating referring expressions in a domain of objects and processes. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the royal statistical society, series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="13560" citStr="Dempster et al., 1977" startWordPosition="2133" endWordPosition="2136">r the corresponding fields. Finally, the rules in (4) and (5) correspond to surface realization. The former rule binarizes the sequence of words emitted by a particular field r. f in an attempt to capture local dependencies between words, such as multi-word expressions (e.g., right click, radio button). The latter rule defines the emission of words and integer numbers1, given a field type and its value. Note that the original model lexicalizes field values of categorical and integer type only. Training The rules of grammar G are associated with weights that are learned using the EM algorithm (Dempster et al., 1977). During training, the records, fields and values of database d and the words w from the associated text are observed, and the model learns the mapping between them. Notice that we use w to denote the gold-standard text and g to refer to the words generated by the model. The mapping between the database and the observed text is unknown and thus the weights of the rules define a hidden correspondence h between records, fields and their values. Decoding Given a trained grammar G and an input scenario from a database d, the model generates text by finding the most likely derivation, i.e., sequenc</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society, series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Empirically estimating order constraints for content planning in generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>172--179</pages>
<contexts>
<context position="7678" citStr="Duboue and McKeown (2001)" startWordPosition="1151" endWordPosition="1154"> planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al1504 gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang e</context>
</contexts>
<marker>Duboue, McKeown, 2001</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2001. Empirically estimating order constraints for content planning in generation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Content planner construction via evolutionary algorithms and a corpus-based fitness function.</title>
<date>2002</date>
<booktitle>In Proceedings of International Natural Language Generation,</booktitle>
<pages>89--96</pages>
<location>Ramapo Mountains, NY.</location>
<contexts>
<context position="2424" citStr="Duboue and McKeown, 2002" startWordPosition="357" endWordPosition="360">ents: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by</context>
<context position="7113" citStr="Duboue and McKeown, 2002" startWordPosition="1062" endWordPosition="1065">V (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirica</context>
</contexts>
<marker>Duboue, McKeown, 2002</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2002. Content planner construction via evolutionary algorithms and a corpus-based fitness function. In Proceedings of International Natural Language Generation, pages 89–96, Ramapo Mountains, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>60--68</pages>
<location>Jeju Island,</location>
<contexts>
<context position="22676" citStr="Feng and Hirst, 2012" startWordPosition="3700" endWordPosition="3703">RST is a set of non-terminals corresponding to rhetorical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST C NRST X {NRST U ER} X {NRST U ER} associated with a weight for each rule, and D E NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST. Training In order to obtain the weighted productions of GRST, we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distri</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60–68, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--341</pages>
<contexts>
<context position="6992" citStr="Hovy, 1993" startWordPosition="1043" endWordPosition="1044">el to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints rela</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Eduard Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence, 63:341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blake Howald</author>
<author>Ravikumar Kondadadi</author>
<author>Frank Schilder</author>
</authors>
<title>Domain adaptable semantic clustering in statistical nlg.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>143--154</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Potsdam, Germany,</location>
<marker>Howald, Kondadadi, Schilder, 2013</marker>
<rawString>Blake Howald, Ravikumar Kondadadi, and Frank Schilder. 2013. Domain adaptable semantic clustering in statistical nlg. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 143–154, Potsdam, Germany, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfg models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>632</pages>
<contexts>
<context position="27384" citStr="Johnson, 1998" startWordPosition="4433" endWordPosition="4434">narization). The WINHELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for GRST. On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development set and obtained best results with 60 for WEATHERGOV and 120 for WINHELP. We used a trigram model for both domains, trained on each training set. Evaluation We compared two configurations of our system, one with a content planning component based on record type sequences (GRSE) and 6When horizontally markovizing, we can encode an arbitrary amount of </context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24(4):613– 632, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
</authors>
<title>Entity Coherence for Descriptive Text Structuring.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="7375" citStr="Karamanis, 2003" startWordPosition="1102" endWordPosition="1103"> information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corr</context>
</contexts>
<marker>Karamanis, 2003</marker>
<rawString>Nikiforos Karamanis. 2003. Entity Coherence for Descriptive Text Structuring. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Lab,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="15038" citStr="Kasami, 1965" startWordPosition="2400" endWordPosition="2401">on g(f.v) : Z → Z, generates an integer in the following six ways (Liang et al., 2009): identical, rounding up/down to a multiple of 5, rounding off a multiple of 5 and adding or subtracting some noise modelled by a geometric distribution. however, the string is not observed; instead, we must find the best text ˆg, by maximizing both over h and g, where g = g1 ...gN is a sequence of words licensed by G. More formally: gˆ = f (argmaxP((g,h))) (1) where f is a function that takes as input a derivation tree (g,h) and returns ˆg. Konstas and Lapata (2012) use a modified version of the CYK parser (Kasami, 1965; Younger, 1967) to find ˆg. Specifically, they intersect grammar G with a n-gram language model and calculate the most probable generation gˆ as: g = f \arg axp(g) -p(g,h |d)I (2) where p(g,h|d) is the decoding likelihood for a sequence of words g = g1 ...gN of length N and the hidden correspondence h that emits it, i.e., the likelihood of the grammar for a given database input scenario d. p(g) is a measure of the quality of each output and is provided by the n-gram language model. 5 Extensions In this section we extend the model of Konstas and Lapata (2012) by developing two more sophisticat</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Tadao Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodger Kibble</author>
<author>Richard Power</author>
</authors>
<title>Optimising referential coherence in text generation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7400" citStr="Kibble and Power, 2004" startWordPosition="1104" endWordPosition="1107">ing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored </context>
</contexts>
<marker>Kibble, Power, 2004</marker>
<rawString>Rodger Kibble and Richard Power. 2004. Optimising referential coherence in text generation. Computational Linguistics, 30(4):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd Conference on Computational Linguistics,</booktitle>
<pages>543--551</pages>
<location>Beijing, China.</location>
<contexts>
<context position="3017" citStr="Kim and Mooney, 2010" startWordPosition="452" endWordPosition="455">nt (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in </context>
<context position="8227" citStr="Kim and Mooney (2010)" startWordPosition="1232" endWordPosition="1235">rse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al1504 gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al. (2009) that selects which database records to talk about and then use an existing surface realizer (Wong and Mooney, 2007) to render the chosen records in natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the </context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>Joohyun Kim and Raymond Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In Proceedings of the 23rd Conference on Computational Linguistics, pages 543–551, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27031" citStr="Klein and Manning, 2003" startWordPosition="4379" endWordPosition="4382">inf.ed.ac.uk/ikonstas/index.php?page=resources 1509 RST-DT WEATHERGOV WINHELP 60 40 20 0 Elaboration Attribution Joint Contrast Explanation Background Enablement Cause Evaluation Comparison Condition Topic-Comment Temporal Explanation Summary Topic Change Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power, 2008). narization). The WINHELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for GRST. On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised concept-to-text generation with hypergraphs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>752--761</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2973" citStr="Konstas and Lapata, 2012" startWordPosition="443" endWordPosition="447">ing which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of </context>
<context position="5366" citStr="Konstas and Lapata (2012)" startWordPosition="775" endWordPosition="779"> a type (e.g., win-target), and a set of fields. Each field has a value, which can be categorical (in typewriter), an integer (in bold), or a literal string (in italics). icalization of input entries), and surface realization jointly. We focus on the problem of generating text from a database. The input to our model is a set of database records and collocated descriptions, examples of which are shown in Figure 1. Given this input, we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be verbalized. Specifically, we extend the model of Konstas and Lapata (2012) which also uses a PCFG to perform content selection and surface realization, but does not capture any aspect of document planning. We represent content plans with grammar rules which operate on the document level and are embedded on top of the original PCFG. We essentially learn a discourse grammar following two approaches. The first one is linguistically naive but applicable to multiple languages and domains; it extracts rules representing global patterns of record sequences within a sentence and among sentences from a training corpus. The second approach learns document plans based on Rheto</context>
<context position="6618" citStr="Konstas and Lapata (2012)" startWordPosition="978" endWordPosition="982">; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers </context>
<context position="9064" citStr="Konstas and Lapata (2012)" startWordPosition="1371" endWordPosition="1374">natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the form of discriminative features in a log-linear model. Analogously, they choose fields for each record, and finally verbalize the input using automatically extracted domain-specific templates from training data. Konstas and Lapata (2012) propose a joint model, which recasts content selection and surface realization into a parsing problem. Their model optimizes the choice of records, fields and words simultaneously, however they still select and order records locally. We replace their content selection mechanism (which is based on a simple markovized chaining of records) with global document representations. A plan in our model is identified either as a sequence of sentences, each containing a sequence of records, or as a tree where the internal nodes denote discourse information and the leaf nodes correspond to records. 3 Pro</context>
<context position="11298" citStr="Konstas and Lapata (2012)" startWordPosition="1755" endWordPosition="1759">ical. During training, our algorithm is given a corpus consisting of several scenarios, i.e., database records paired with texts w (see Figure 1). For each scenario, the model first decides on a global document plan, i.e., it selects which types of records belong to each sentence (or phrase) and how these sentences (or phrases) should be ordered. Then it selects appropriate record tokens for each type and progressively chooses the most relevant fields; then, based on the values of the fields, it generates the final text, word by word. 4 Original Model Our work builds on the model developed by Konstas and Lapata (2012). The latter is essentially a PCFG which captures both the structure of the input database and the way it renders into natural language. This grammar-based approach lends itself well to the incorporation of document planning which has traditionally assumed tree-like representations. We first briefly describe the original model and then present our extensions in Section 5. Grammar Grammar G in Figure 2 defines a set of non-recursive CFG rewrite rules that capture the structure of the database, i.e., the relationship between records, records and fields, fields and words. These rules are domain-i</context>
<context position="14983" citStr="Konstas and Lapata (2012)" startWordPosition="2388" endWordPosition="2391">tructure, i.e., hidden correspondence ˆh. In generation, 1The function g(f.v) : Z → Z, generates an integer in the following six ways (Liang et al., 2009): identical, rounding up/down to a multiple of 5, rounding off a multiple of 5 and adding or subtracting some noise modelled by a geometric distribution. however, the string is not observed; instead, we must find the best text ˆg, by maximizing both over h and g, where g = g1 ...gN is a sequence of words licensed by G. More formally: gˆ = f (argmaxP((g,h))) (1) where f is a function that takes as input a derivation tree (g,h) and returns ˆg. Konstas and Lapata (2012) use a modified version of the CYK parser (Kasami, 1965; Younger, 1967) to find ˆg. Specifically, they intersect grammar G with a n-gram language model and calculate the most probable generation gˆ as: g = f \arg axp(g) -p(g,h |d)I (2) where p(g,h|d) is the decoding likelihood for a sequence of words g = g1 ...gN of length N and the hidden correspondence h that emits it, i.e., the likelihood of the grammar for a given database input scenario d. p(g) is a measure of the quality of each output and is provided by the n-gram language model. 5 Extensions In this section we extend the model of Konst</context>
<context position="20990" citStr="Konstas and Lapata (2012)" startWordPosition="3445" endWordPosition="3448">nt words with the same type and segment on punctuation (see Figure 3b). Next, we create the corresponding tree according to GRSE (Figure 3d) and binarize it. We experimented both with left and right binarization and adhered to the latter, as it obtained a more compact set of rules. Finally, we collectively count the rule weights on the resulting treebank and extract a rule set, discarding rules with frequency less than three. Using the extracted (weighted) GRSE rules, we run the EM algorithm via inside-outside and learn the weights for the remaining rules in G. Decoding remains the same as in Konstas and Lapata (2012); the only requirement is that the extracted grammar remains binarized in order to guarantee the cubic 1507 desktop1 start1 start-target1 win-target1 contMenu1 action-contMenu1 Click start, point to settings, and then click control panel. Double-click users and passwords. On the advanced tab, click advanced. (a) Record token alignments [ ] [Click start,]desktop1.t [point to settings, ]start1.t [and then desktop start start-targetkwin-targetkcontMenu action-contMenuk click control panel.]start−target1.t [Double-click users and (b) Record type segmentation passwords.]win−target1.t [On the advanc</context>
<context position="27467" citStr="Konstas and Lapata (2012)" startWordPosition="4446" endWordPosition="4449">t the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for GRST. On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development set and obtained best results with 60 for WEATHERGOV and 120 for WINHELP. We used a trigram model for both domains, trained on each training set. Evaluation We compared two configurations of our system, one with a content planning component based on record type sequences (GRSE) and 6When horizontally markovizing, we can encode an arbitrary amount of context in the intermediate non-terminals that result from this process; in our cas</context>
<context position="29360" citStr="Konstas and Lapata (2012)" startWordPosition="4761" endWordPosition="4765">valuated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test set (for each domain) and produced output with the system of Konstas and Lapata (2012) (henceforth K&amp;L), our two models using GRSE and GRST, respectively, and Angeli et al. (2010) (henceforth ANGELI). We also included the original text (HUMAN) as gold-standard. We obtained ratings for 60 (12 x 5) scenario-text pairs for each domain. Examples of the documents shown to the participants are given in Table 1. The study was conducted over the Internet us1510 WEATHERGOV WINHELP GR Showers before noon. Cloudy, with a high near Right-click my network places, and then click prop38. Southwest wind between 3 and 8 mph. erties. Right-click local area connection, and click Chance of precipi</context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Ioannis Konstas and Mirella Lapata. 2012. Unsupervised concept-to-text generation with hypergraphs. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752– 761, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>91--99</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2513" citStr="Liang et al., 2009" startWordPosition="372" endWordPosition="375">utput text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper</context>
<context position="6510" citStr="Liang et al., 2009" startWordPosition="960" endWordPosition="963">a training corpus. The second approach learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 20</context>
<context position="8290" citStr="Liang et al. (2009)" startWordPosition="1243" endWordPosition="1246"> (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al1504 gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al. (2009) that selects which database records to talk about and then use an existing surface realizer (Wong and Mooney, 2007) to render the chosen records in natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the form of discriminative features in a log-linear model. Analogou</context>
<context position="14512" citStr="Liang et al., 2009" startWordPosition="2301" endWordPosition="2304">nown and thus the weights of the rules define a hidden correspondence h between records, fields and their values. Decoding Given a trained grammar G and an input scenario from a database d, the model generates text by finding the most likely derivation, i.e., sequence of rewrite rules for the input. Although resembling parsing, the generation task is subtly different. In parsing, we observe a string of words and our goal is to find the most probable syntactic structure, i.e., hidden correspondence ˆh. In generation, 1The function g(f.v) : Z → Z, generates an integer in the following six ways (Liang et al., 2009): identical, rounding up/down to a multiple of 5, rounding off a multiple of 5 and adding or subtracting some noise modelled by a geometric distribution. however, the string is not observed; instead, we must find the best text ˆg, by maximizing both over h and g, where g = g1 ...gN is a sequence of words licensed by G. More formally: gˆ = f (argmaxP((g,h))) (1) where f is a function that takes as input a derivation tree (g,h) and returns ˆg. Konstas and Lapata (2012) use a modified version of the CYK parser (Kasami, 1965; Younger, 1967) to find ˆg. Specifically, they intersect grammar G with a</context>
<context position="17333" citStr="Liang et al., 2009" startWordPosition="2813" endWordPosition="2816">ls: NRSE = {D, SENT} where D represents the start symbol and SENT a sequence of records. PRSE is a set of production rules of the form: (a) D → SENT(ti, ..., tj) ... SENT(tl, ..., tm) (b) SENT(ti, ..., tj) → R(ra.ti) ... R(rk.tj) · where t is a record type, ti, tj, tl and tm may overlap and ra, rk are record tokens of type ti and tj respectively. The corresponding weights for the production rules PRSE are: Definition 2 (GRSE weights) (a) p(ti, ..., tj, ... tl, ...,tm |D) (b) p(ti) · ... · p(tj) = 1 |s(ti) |· ... · 1 |s(tj)| where s(t) is a function that returns the set of records with type t (Liang et al., 2009). Rule (a) defines the expansion from the start symbol D to a sequence of sentences, each represented by the non-terminal SENT. Similarly to the original grammar G, we employ the use of features (in parentheses) to denote a sequence of record types. The same record types may recur in different sentences, but not in the same one. The weight of rule (a) is simply the joint probability of all the record types present, ordered and segmented appropriately into sentences in the document, given the start symbol. Once record types have been selected (on a per sentence basis) we move on to rule (b) whi</context>
<context position="23516" citStr="Liang et al. (2009)" startWordPosition="3838" endWordPosition="3841">lassified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data. We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser on the resulting EDUs, and retrieve the corresponding discourse tree; the internal nodes are labelled with one of the RST relations. Finally, we replace the leaf EDUs with their respective terminal symbols R(r.t) E ER (Figure 3e) and collect the resulting grammar productions; their weights are calculated via maximum likelihood estimation based on their collective counts in the parse trees licensed by GRST. Training and decoding of the extended generation model (after we embed GRST in the original grammar G) is performed identically to Section </context>
<context position="26261" citStr="Liang et al. (2009)" startWordPosition="4274" endWordPosition="4277">ds and each document 51.92 words with 4.3 sentences. The vocabulary is 629 words. We performed 10-fold cross-validation on the entire dataset for training and testing. Compared to WEATHERGOV, WINHELP documents are longer with a larger vocabulary. More importantly, due to the nature of the domain, i.e., giving instructions, content selection is critical not only in terms of what to say but also in what order. Grammar Extraction and Parameter Setting We obtained alignments between database records and textual segments for both domains and grammars (GRSE and GRST) using the unsupervised model of Liang et al. (2009). On WEATHERGOV, we extracted a GRSE grammar with 663 rules (after bi4support.microsoft.com 5The dataset can be downloaded from http://homepages. inf.ed.ac.uk/ikonstas/index.php?page=resources 1509 RST-DT WEATHERGOV WINHELP 60 40 20 0 Elaboration Attribution Joint Contrast Explanation Background Enablement Cause Evaluation Comparison Condition Topic-Comment Temporal Explanation Summary Topic Change Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power, 2008). narization). The WINHELP dataset is considerably smaller, and as a result the procedure des</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91–99, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thomson</author>
</authors>
<title>Rhetorical structure theory.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="6018" citStr="Mann and Thomson, 1988" startWordPosition="881" endWordPosition="884">rform content selection and surface realization, but does not capture any aspect of document planning. We represent content plans with grammar rules which operate on the document level and are embedded on top of the original PCFG. We essentially learn a discourse grammar following two approaches. The first one is linguistically naive but applicable to multiple languages and domains; it extracts rules representing global patterns of record sequences within a sentence and among sentences from a training corpus. The second approach learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the WEATHERGOV (Liang et al., 2009) and WINHELP (Branavan et al., 2009) domains shows that our approach improves over Konstas and Lapata (2012)</context>
<context position="36276" citStr="Mann and Thomson, 1988" startWordPosition="5890" endWordPosition="5893">ystem that generates text from database input and captures all components of the traditional generation pipeline, including document planning. Document plans are induced automatically from training data and are represented intuitively by PCFG rules capturing the structure of the database and the way it renders to text. We proposed two complementary approaches to inducing content planners. In a first linguistically naive approach, a document is modelled as a sequence of sentences and each sentence as a sequence of records. Our second approach draws inspiration from Rhetorical Structure Theory (Mann and Thomson, 1988) and represents a document as a tree with intermediate nodes corresponding to discourse relations, and leaf nodes to database records. Experiments with both approaches demonstrate improvements over models that do not incorporate document planning. In the future, we would like to tackle more challenging domains, such as NFL recaps, financial articles and biographies (Howald et al., 2013; Schilder et al., 2013). Our models could also benefit from the development of more sophisticated planners either via grammar refinement or more expressive grammar formalisms (Cohn et al., 2010). Acknowledgments</context>
</contexts>
<marker>Mann, Thomson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thomson. 1988. Rhetorical structure theory. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alisdair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of International Natural Language Generation,</booktitle>
<pages>98--107</pages>
<location>New Brunswick, NJ.</location>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>Chris Mellish, Alisdair Knott, Jon Oberlander, and Mick O’Donnell. 1998. Experiments using stochastic search for text planning. In Proceedings of International Natural Language Generation, pages 98–107, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="28675" citStr="Papineni et al., 2002" startWordPosition="4655" endWordPosition="4658">ss; in our case we store h=1 horizontal siblings plus the mother left-hand side (LHS) non-terminal, in order to uniquely identify the Markov chain. For example, A → B C D becomes A → B (A...B), (A...B) → C (A...C), (A...C) → D. another one based on RST (GRST). In both cases content plans were extracted from (noisy) unsupervised alignments. As a baseline, we used the original model of Konstas and Lapata (2012). We also compared our model to Angeli et al.’s system (2010), which is state of the art on WEATHERGOV. System output was evaluated automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1494" citStr="Reiter and Dale, 2000" startWordPosition="221" endWordPosition="224">ining data. We develop two approaches: the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input (Reiter and Dale, 2000). Depending on the application and the domain at hand, the input may assume various representations including databases, expert system knowledge bases, simulations of physical systems, or formal meaning representations. Generation systems typically follow a pipeline architecture consisting of three components: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, the</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
<author>Jim Hunter</author>
<author>Ian Davy</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<pages>169</pages>
<contexts>
<context position="3380" citStr="Reiter et al., 2005" startWordPosition="507" endWordPosition="510">he generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Database Records temp(time:6-21, min:9, mean:15, max:21) wind-spd(time:6-21, min:15, mean:20, max:30) sky-cover(time:6-9, percent:25-50) sky-cover(time:9-12, percent:50-75) wind-dir(time:6-21, mode:SSE) gust(time:6-21, min</context>
</contexts>
<marker>Reiter, Sripada, Hunter, Davy, 2005</marker>
<rawString>Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian Davy. 2005. Choosing words in computer-generated weather forecasts. Artificial Intelligence, 167:137– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
<author>Blake Howald</author>
<author>Ravi Kondadadi</author>
</authors>
<title>Gennext: A consolidated domain adaptable nlg system.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation,</booktitle>
<pages>178--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Schilder, Howald, Kondadadi, 2013</marker>
<rawString>Frank Schilder, Blake Howald, and Ravi Kondadadi. 2013. Gennext: A consolidated domain adaptable nlg system. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 178– 182, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia Scott</author>
<author>Clarisse Sieckenius de Souza</author>
</authors>
<title>Getting the message across in RST-based text generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>47--73</pages>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Scott, de Souza, 1990</marker>
<rawString>Donia Scott and Clarisse Sieckenius de Souza. 1990. Getting the message across in RST-based text generation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation, pages 47–73. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentation in spoken dialog systems.</title>
<date>2004</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>79--86</pages>
<location>Barcelona,</location>
<contexts>
<context position="2593" citStr="Stent et al., 2004" startWordPosition="387" endWordPosition="390"> individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all component</context>
</contexts>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>Amanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In Proceedings of Association for Computational Linguistics, pages 79–86, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
<author>Richard Power</author>
</authors>
<title>Deriving rhetorical complexity data from the rst-dt corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<contexts>
<context position="23194" citStr="Williams and Power (2008)" startWordPosition="3785" endWordPosition="3788"> weighted productions of GRST, we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data. We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser on the resulting EDUs, and retrieve the corresponding discourse tree; the internal nodes are labelled with one of the RST relations. Finally, we replace the leaf EDUs with their respective terminal symbols R(r.t) E ER (Figure 3</context>
<context position="26768" citStr="Williams and Power, 2008" startWordPosition="4337" endWordPosition="4340">and textual segments for both domains and grammars (GRSE and GRST) using the unsupervised model of Liang et al. (2009). On WEATHERGOV, we extracted a GRSE grammar with 663 rules (after bi4support.microsoft.com 5The dataset can be downloaded from http://homepages. inf.ed.ac.uk/ikonstas/index.php?page=resources 1509 RST-DT WEATHERGOV WINHELP 60 40 20 0 Elaboration Attribution Joint Contrast Explanation Background Enablement Cause Evaluation Comparison Condition Topic-Comment Temporal Explanation Summary Topic Change Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power, 2008). narization). The WINHELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On WEATHERGOV, we extracted 434 rules for GRST. On WINHELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node</context>
</contexts>
<marker>Williams, Power, 2008</marker>
<rawString>Sandra Williams and Richard Power. 2008. Deriving rhetorical complexity data from the rst-dt corpus. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology and the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>172--179</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="3098" citStr="Wong and Mooney, 2007" startWordPosition="464" endWordPosition="467">ed to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, Seattle, Washington, USA, 18-21 Oct</context>
<context position="8406" citStr="Wong and Mooney, 2007" startWordPosition="1263" endWordPosition="1266">biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary al1504 gorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al. (2009) that selects which database records to talk about and then use an existing surface realizer (Wong and Mooney, 2007) to render the chosen records in natural language. Their content planner has no notion of coherence. Angeli et al. (2010) adopt a more unified approach that builds on top of the alignment model of Liang et al. (2009). They break record selection into a series of locally coherent decisions, by first deciding on what records to talk about. Each choice is based on a history of previous decisions, which is encoded in the form of discriminative features in a log-linear model. Analogously, they choose fields for each record, and finally verbalize the input using automatically extracted domain-specif</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of the Human Language Technology and the Conference of the North American Chapter of the Association for Computational Linguistics, pages 172–179, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing for context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="15054" citStr="Younger, 1967" startWordPosition="2402" endWordPosition="2403">→ Z, generates an integer in the following six ways (Liang et al., 2009): identical, rounding up/down to a multiple of 5, rounding off a multiple of 5 and adding or subtracting some noise modelled by a geometric distribution. however, the string is not observed; instead, we must find the best text ˆg, by maximizing both over h and g, where g = g1 ...gN is a sequence of words licensed by G. More formally: gˆ = f (argmaxP((g,h))) (1) where f is a function that takes as input a derivation tree (g,h) and returns ˆg. Konstas and Lapata (2012) use a modified version of the CYK parser (Kasami, 1965; Younger, 1967) to find ˆg. Specifically, they intersect grammar G with a n-gram language model and calculate the most probable generation gˆ as: g = f \arg axp(g) -p(g,h |d)I (2) where p(g,h|d) is the decoding likelihood for a sequence of words g = g1 ...gN of length N and the hidden correspondence h that emits it, i.e., the likelihood of the grammar for a given database input scenario d. p(g) is a measure of the quality of each output and is provided by the n-gram language model. 5 Extensions In this section we extend the model of Konstas and Lapata (2012) by developing two more sophisticated content selec</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H Younger. 1967. Recognition and parsing for context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
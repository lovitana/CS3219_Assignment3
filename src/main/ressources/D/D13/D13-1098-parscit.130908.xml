<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988613">
Building Event Threads out of Multiple News Articles
</title>
<author confidence="0.914909">
Xavier Tannier
</author>
<affiliation confidence="0.70887">
LIMSI-CNRS
</affiliation>
<address confidence="0.498571">
Univ. Paris-Sud
Orsay, France
</address>
<email confidence="0.997533">
xavier.tannier@limsi.fr
</email>
<author confidence="0.578393">
V´eronique Moriceau
</author>
<affiliation confidence="0.394108666666667">
LIMSI-CNRS
Univ. Paris-Sud
Orsay, France
</affiliation>
<email confidence="0.993337">
moriceau@limsi.fr
</email>
<sectionHeader confidence="0.993722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999579941176471">
We present an approach for building multi-
document event threads from a large corpus
of newswire articles. An event thread is basi-
cally a succession of events belonging to the
same story. It helps the reader to contextual-
ize the information contained in a single arti-
cle, by navigating backward or forward in the
thread from this article. A specific effort is
also made on the detection of reactions to a
particular event.
In order to build these event threads, we use a
cascade of classifiers and other modules, tak-
ing advantage of the redundancy of informa-
tion in the newswire corpus.
We also share interesting comments con-
cerning our manual annotation procedure for
building a training and testing set1.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891166666667">
In this paper, we explore a new way of dealing
with temporal relations between events. Our task
is somewhat between multidocument summariza-
tion and classification of temporal relations between
events. We work with a large collection of En-
glish newswire articles, where each article relates
an event: the main topic of the article is a specific
event, and other older events are mentioned in order
to put it into perspective. Thus, we consider that an
event is associated with an article and that defining
temporal relations between articles is a way to define
temporal relations between events.
</bodyText>
<footnote confidence="0.97783475">
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.
</footnote>
<bodyText confidence="0.99601">
The task is to build a temporal graph of arti-
cles, linked between each other by the following re-
lations:
</bodyText>
<listItem confidence="0.999337">
• Same event, when two documents relate the
same event, or when a document is an update
of another one.
• Continuation, when an event is the continua-
</listItem>
<bodyText confidence="0.981884791666667">
tion or the consequence of a previous one.
We also define a subset of continuation, called
reaction, concerning a document relating the reac-
tion of someone to another event.
Some examples of these three classes will be
given in Section 3.
These relations can be represented by a directed
graph where documents are vertices and relations
are edges (as illustrated in all figures of this article).
Figure 1 shows an example of such a graph.
Press articles, and especially newswire articles,
are characterized by an important redundancy of re-
lated events. An important event2 is likely to be
treated by several successive articles, which will
give more and more details and update some num-
bers (mainly, tragedy casualty updates, as shown in
Figure 2). On the one hand, this redundancy is an
issue since a system must not show duplicate infor-
mation to the user; on the other hand, we show in
this article that it can also be of great help in the
process of extracting temporal graphs.
In what follows, we first review some of the re-
lated work in Section 2. Section 3 presents the anno-
tation procedure and the resulting annotated corpus
</bodyText>
<footnote confidence="0.625222">
2Note that we do not focus intentionally on “important”
events. However, the fact is that minor events do hardly lead
to dense temporal graphs.
</footnote>
<page confidence="0.888254">
958
</page>
<note confidence="0.880675">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 958–967,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9977675">
Figure 1: Example of “temporal graph”: around the
Pope’s death. The associated text is the title of each ar-
ticle. Relations that can be obtained by transitivity have
been hidden for clarity’s sake.
</figureCaption>
<bodyText confidence="0.999960909090909">
used for developing, learning and evaluating the sys-
tem. The simple modules used to predict the same
event, continuation and, possibly, reaction relations
are described in Section 4, and results are given in
Section 5.
We also propose an end-user application to this
work. When a user reads an article, the system will
then be able to provide her with a thread of events
having occurred before or after, helping her to con-
textualize the information she is reading. This appli-
cation is described in Section 6.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999772833333333">
The identification of temporal relations between
events in texts has been the focus of increasing atten-
tion because of its importance in NLP applications
such as information extraction, question-answering
or summarization. The evaluation campaigns Tem-
pEval 2007 (Verhagen et al., 2007) and TempEval
2010 (Verhagen et al., 2010) focused on temporal
relation identification, mainly on temporal relations
between events and times in the same sentence or
in consecutive sentences and between events and the
creation time of documents. In this context, the goal
is to identify the type of a temporal relation which is
</bodyText>
<figureCaption confidence="0.9952456">
Figure 2: Example of “temporal graph”: Madrid attacks,
with many updates of the initial information. Note that
articles gathered in this main pool of articles can be pos-
terior to the continuations and reactions to the described
event.
</figureCaption>
<bodyText confidence="0.999977678571428">
known to be present. Systems having the best results
(accuracy about 0.6) use statistical learning based
on temporal features (modality, tense, aspect, etc.)
(Mani et al., 2006; Chambers et al., 2007). More re-
cently, Mirroshandel and Ghassem-Sani (2012) pro-
posed a new method for temporal relation extraction
by using a bootstrapping method on annotated data
and have a better accuracy than state-of-the-art sys-
tems. Their method is based on the assumption that
similar event pairs in topically related documents
are likely to have the same temporal relations. For
this work, the authors had already some collections
of topically related documents and did not need to
identify them.
In the 2012 i2b2 challenge (i2b, 2012), the
problem was not only to identify the type of tempo-
ral relations, but also to decide whether a temporal
relation existed or not between two elements, either
clinical concepts or temporal expressions. But, as
in TempEval, the temporal analysis were only to be
performed within a single document.
Other works focus on event ordering. For ex-
ample, Fujiki et al. (2003) and Talukdar et al. (2012)
proposed methods for automatic acquisition of event
sequences from texts. They did not use tempo-
ral information present in texts and extracted se-
quences of events (e.g. arrest/escape) from sen-
tences which were already arranged in chronologi-
</bodyText>
<page confidence="0.997927">
959
</page>
<bodyText confidence="0.999976725">
cal order. Chambers and Jurafsky (2008) proposed
a method to learn narrative chains of events related
to a protagonist in a single document. The first
step consists in detecting narrative relations between
events sharing coreferring arguments. Then, a tem-
poral classifier orders partially the connected events
with the before relation.
Concerning the identification of the reaction re-
lation, to our knowledge, there is no work on the
detection of reaction between several documents.
Pouliquen et al. (2007), Krestel et al. (2008) and
Balahur et al. (2009) focused on the identification of
reported speech or opinions in quotations in a docu-
ment, but not on the identification of an event which
is the source of a reaction and which can possibly be
in another document.
As we can see, all these approaches, as well as
traditional information extraction approaches, lean
on information contained by a single document, and
consider an event as a word or a phrase. However,
Ahmed et al. (2011) proposed a framework to group
temporally and tocipally related news articles into
same story clusters in order to reveal the temporal
evolution of stories. But in these topically related
clusters of documents, no temporal relation is de-
tected between articles or events except chronologi-
cal order. On this point of view, our task is closer
to what is done in multidocument summarization,
where a system has to detect redundant excerpts
from various texts on the same topic and present
results in a relevant chronological order. For ex-
ample, Barzilay et al. (2002) propose a system for
multidocument summarization from newswire arti-
cles describing the same event. First, similar text
units from different documents are identified using
statistical techniques and shallow text analysis and
grouped into thematic clusters. Then, in each theme,
sentences which are selected as part of the summary
are ordered using the publication date of the first oc-
currence of events to order sentences.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="method">
3 Resources
</sectionHeader>
<bodyText confidence="0.999919571428571">
We built an annotated collection of English articles,
taken from newswire texts provided by the French
news agency (AFP), spreading over the period 2004-
2012. The entire collection contains about 1.5 mil-
lion articles. Each document is an XML file contain-
ing a title, a creation time (DCT), a set of keywords
and textual content split into paragraphs.
</bodyText>
<subsectionHeader confidence="0.999986">
3.1 Selection of Article Pairs
</subsectionHeader>
<bodyText confidence="0.999629">
Pairs of documents were automatically selected ac-
cording to the following constraints:
</bodyText>
<listItem confidence="0.811225875">
• The article describes an event. Articles such
as timelines, fact files, agendas or summaries
were discarded (all these kinds of articles were
tagged by specific keywords, making the filter-
ing easy).
• The distance between the two DCTs does not
exceed 7 days.
• There are at least 2 words in common in the set
</listItem>
<bodyText confidence="0.967618875">
of keywords and/or 2 proper nouns in common
in the first paragraph of each article.
These last two restrictions are important, but
necessary, in order to give annotators a chance to
find some related articles. Pure random selection of
pairs over a collection of 1.5 million articles would
be impractical.
We assume that the title and the first paragraph
describe the event associated with the document.
This is a realistic hypothesis, since the basic rules
of journalism impose that the first sentence should
summarize the event by informing on the “5 Ws”
(What, Who, When, Where, Why). However, reading
more than the first paragraph is sometimes necessary
to determine whether a relation exists between two
events.
</bodyText>
<subsectionHeader confidence="0.999348">
3.2 Relation Annotation
</subsectionHeader>
<bodyText confidence="0.9841814">
Two annotators were asked to attribute the following
relations between each pair of articles presented by
the annotation interface system.
In a first annotation round, 7 types of relations
were annotated:
</bodyText>
<listItem confidence="0.990414">
• Three relations concerning cases where the two
articles relate the same event or an update:
</listItem>
<bodyText confidence="0.5222382">
– number update, when a document is an
update of numerical data (see top of Fig-
ure 5),
– form update, when the second document
brings only minor corrections,
</bodyText>
<page confidence="0.993257">
960
</page>
<figureCaption confidence="0.99853975">
Figure 3: Examples of relation continuation between two
documents.
Figure 4: Examples of relation continuation-reaction be-
tween two documents.
</figureCaption>
<bodyText confidence="0.995921666666667">
– details, when the second document gives
more details about the events (see bottom
of Figure 5).
</bodyText>
<listItem confidence="0.930150117647059">
• development of same story, when the two docu-
ments relate two events which are included into
a third one;
• continuation, when an event is the continuation
or the consequence of a previous one. Figure 3
shows two examples of such a relation. It is
important to make clear that a continuation re-
lation is more than a simple thematic relation,
it implies a natural prolongation between two
events. For example, two sport events of the
same Olympic Games, or two different attacks
in Iraq, shall not be linked together unless a di-
rect link between both is specified in the arti-
cles.
• reaction, a subset of continuation, when a doc-
ument relates the reaction of someone to an-
other event, as illustrated by the example in
</listItem>
<figureCaption confidence="0.987855">
Figure 4.
Figure 5: Example of relations same-event between two
documents: update on casualties (top) or details (bottom).
</figureCaption>
<bodyText confidence="0.972828052631579">
• nil, when no relation can be identified between
the two documents.
The inter-annotator agreement was calculated
with Cohen’s Kappa measure (Cohen, 1960) across
150 pairs: n = 0.68. The agreement was low for
the first 4 types of relations mostly because the dif-
ference between relations was not clear enough. We
therefore aggregated the number update, form up-
date and details relations into a more generic and
consensual same-event relation (see Figure 5). We
also discarded the development of same story rela-
tion, leaving only same-event, continuation and re-
action.
Annotation guidelines were modified and a sec-
ond annotation round was carried out: only the
same-event, continuation, reaction and nil relations
were annotated. Inter-annotator agreement across
150 pairs was then n = 0.83, which is a good agree-
ment.
</bodyText>
<subsectionHeader confidence="0.9993">
3.3 Relation Set Extension
</subsectionHeader>
<bodyText confidence="0.993354666666667">
This manual annotation would have led to very
sparse temporal graphs without the two following
additional processes:
</bodyText>
<listItem confidence="0.991838714285714">
• When the annotator attributed a “non-nil” rela-
tion to a pair of documents, the annotation sys-
tem suggested other pairs to annotate around
the concerned articles.
• Same-event and continuation relations are tran-
sitive: if A same-event B and B same-event
C, then A same-event C (and respectively for
</listItem>
<page confidence="0.967005">
961
</page>
<table confidence="0.999907166666667">
Pair number Learning Evaluation
Same event 762 458 304
Continuation 1134 748 386
Reaction 182 123 59
Nil 918 614 304
TOTAL 2996 1943 1053
</table>
<tableCaption confidence="0.999899">
Table 1: Characteristics of the corpus.
</tableCaption>
<bodyText confidence="0.998940777777778">
continuation). Then, when the annotation was
done, a transitive closure was performed on the
entire graph, in order to get more relations with
low effort (and to detect and correct some in-
consistencies in the annotations).
Finally, almost 3,000 relations were annotated.
2/3 of the annotated pairs were used for development
and learning phases, while 1/3 were kept for evalua-
tion purpose (cf. Table 1).
</bodyText>
<sectionHeader confidence="0.978685" genericHeader="method">
4 Building Temporal Graphs
</sectionHeader>
<bodyText confidence="0.997272176470588">
As we explained in the introduction, the main pur-
pose of this paper is to show that it is possible to ex-
tract temporal graphs of events from multiple docu-
ments in a news corpus. This is achieved with the
help of redundancy of information in this corpus.
Therefore, we will use a cascade of classifiers and
other modules, each of them using the relations de-
duced by the previous one. All modules predict a
relation between two documents (i.e., two events).
We did not focus on complex algorithms or
classifiers for tuning our results, and most of our fea-
tures are very simple. The idea here is to show that
good results can be obtained in this original and use-
ful task. The process can be separated into 3 main
stages, illustrated in Figure 6:
A. Filtering out pairs that have no relation at all, i.e.
classifying between nil and non-nil relations;
</bodyText>
<listItem confidence="0.85949375">
B. Classifying between same-event and continua-
tion relations;
C. Extracting reactions from the set of continuation
relations.
</listItem>
<bodyText confidence="0.99674425">
All described classifiers use SMO (Platt, 1998),
the SVM optimization implemented into Weka (Hall
et al., 2009), with logistic models fitting (option “-
M”). With this option, the confidence score of each
</bodyText>
<figureCaption confidence="0.999678">
Figure 6: A 3-step classification.
</figureCaption>
<bodyText confidence="0.9986195">
prediction can be used, while SMO alone provides a
constant probability for all instances.
From now on, when considering a pair of doc-
uments, we will refer to the older document as doc-
ument 1, and to the more recent one as document 2.
The relations found between documents will be rep-
resented by a directed graph where documents are
vertices and relations are edges.
</bodyText>
<subsectionHeader confidence="0.975853">
4.1 A. Nils versus non-nils
</subsectionHeader>
<bodyText confidence="0.999995666666667">
We first aim at separating nil relations (no relation
between two events) from other relations. This step
is achieved by two successive classifiers: the first
one (A.1) uses mainly similarity measures between
documents, while the second one (A.2) uses the re-
lations obtained by the first one.
</bodyText>
<subsubsectionHeader confidence="0.469767">
4.1.1 Step A.1: Nil classifier, level 1
</subsubsectionHeader>
<bodyText confidence="0.999729111111111">
Features provided to the SMO classifier at this
first step are based on 3 different similarity measures
applied to pairs of titles, pairs of first sentences,
and pairs of entire documents: cosine similarity (as
implemented by Lucene search engine3), inclusion
similarity (rate of words from element 1 present in
element 2) and overlap similarity (number of words
present in both elements). This classifier is therefore
based on only 9 features.
</bodyText>
<subsectionHeader confidence="0.47765">
4.1.2 Step A.2: Nil classifier, level 2
</subsectionHeader>
<bodyText confidence="0.9945665">
Finding relations on a document implies that the
described event is important enough to be addressed
by several articles (same-event) or to have conse-
quences (continuation). Consequently, if we find
such relations concerning a document, we are more
likely to find more of them, because this means that
</bodyText>
<footnote confidence="0.99086">
3http://lucene.apache.org
</footnote>
<page confidence="0.989583">
962
</page>
<bodyText confidence="0.999002666666667">
the document has some importance. A typical exam-
ple is shown in Figure 7, where an event described
by several documents (on the left) has many contin-
uations. For this reason, we build a second classifier
A.2 using additional features related to the relations
found at step A.1:
</bodyText>
<listItem confidence="0.918082111111111">
• Number of non-nil edges, incoming to or out-
going from document 1 (2 features); the sum of
both numbers (1 extra feature);
• Number of non-nil edges, incoming to or out-
going from document 2 (2 features); the sum of
both numbers (1 extra feature);
• Number of non-nil edges found involving one
of the two documents (i.e., the sum of all edges
described above – 1 feature).
</listItem>
<bodyText confidence="0.999685166666667">
These figures have been computed on training
set for training, and on result of step A.1 classifier
for testing. This new information will basically help
the classifier to be more optimistic toward non-nil
relations for documents having already non-nil rela-
tions.
</bodyText>
<subsectionHeader confidence="0.987927">
4.2 B. Same-event versus Continuation
</subsectionHeader>
<bodyText confidence="0.999986333333333">
We are now working only with non-nil relations
(even if some relations may switch between nil and
non-nil during the transitive closure).
</bodyText>
<subsectionHeader confidence="0.636045">
4.2.1 Step B.1: Relation classifier, level 1
</subsectionHeader>
<bodyText confidence="0.9982745">
Distinction between same-event and continua-
tion is made by the following sets of features:
</bodyText>
<listItem confidence="0.997019">
• Date features:
</listItem>
<bodyText confidence="0.890922076923077">
– Difference between the two document cre-
ation times (DCTs): difference in days, in
hours, in minutes (3 features);
– Whether the creation time of doc. 1 is
mentioned in doc. 2. For this purpose,
we use the date normalization system de-
scribed in Kessler et al. (2012).
– Cosine similarity between the first sen-
tence of doc. 1 and sentences of doc. 2
containing the DCT of doc. 1.
– Cosine similarity between the first sen-
tence of doc. 1 and the most similar sen-
tence of doc. 2.
</bodyText>
<figureCaption confidence="0.9881934">
Figure 7: An example of highly-connected subgraph,
corresponding to the development of an important story.
Same events are grouped by cliques (see Section 4.2.3)
and some redundant relations are not shown for clarity’s
sake.
</figureCaption>
<bodyText confidence="0.99809275">
These last three features come from the
idea that a continuation relation can be
made explicit in text by mentioning the
first event in the second document.
</bodyText>
<listItem confidence="0.954097615384615">
• Temporal features: whether words introducing
temporal relations occur in document 1 or doc-
ument 2. These manually-collected words can
be prepositions (after, before, etc.) or verbs
(follow, confirm, prove, etc.).
• Reaction features: whether verbs introducing
reactions occur in document 1 or document 2
(25 manually-collected verbs as approve, ac-
cept, welcome, vow, etc.).
• Opinion features: whether opinion words occur
in document 1 or document 2. The list of opin-
ion words comes from the MPQA subjectivity
lexicon (Wilson et al., 2005).
</listItem>
<bodyText confidence="0.9987102">
Only same-event relations classified with more
than 90% confidence by the classifier are kept, in
order to ensure a high precision (recall will be im-
proved at next step). This threshold has been set up
on development set.
</bodyText>
<subsectionHeader confidence="0.689937">
4.2.2 Step B.2: Relation classifier, level 2
</subsectionHeader>
<bodyText confidence="0.9999982">
As for step A.2, a second classifier is im-
plemented, using the results of step B.1 with the
same manner as A.2 uses A.1 (collecting numbers
of same-event and continuation relations that have
been found by the previous classifier).
</bodyText>
<subsectionHeader confidence="0.9269015">
4.2.3 Steps B.3 and B.4: Transitive closure by
vote
</subsectionHeader>
<bodyText confidence="0.997171">
As already stated, same-event and continuation
relations are transitive. Same-event is also symmet-
ric (A same-event B ñ B same-event A). In the
</bodyText>
<page confidence="0.997465">
963
</page>
<bodyText confidence="0.999044538461539">
graph formed by documents (vertices) and relations
(edges), it is then possible to find all cliques, i.e. sub-
sets of vertices such that every two vertices in the
subset are connected by a same-event relation, as il-
lustrated by Figure 7.
This step does not involve any learning phase.
Starting from the result of last step, we find all same-
event cliques in the graph by using the Bron and
Kerbosch (1973) algorithm. The transitive closure
process is then illustrated by Figure 8. If the classi-
fier proposed a relation between some documents of
a clique and some other documents (as D1, D2 and
D3), then a vote is necessary:
</bodyText>
<listItem confidence="0.9934736">
• If the document is linked to half or more of the
clique, then all missing links are created (Fig-
ure 8.a);
• Otherwise, the document is entirely discon-
nected from the clique (Figure 8.b).
</listItem>
<bodyText confidence="0.986137047619048">
This vote is done for same-event and contin-
uation relations (resp. steps B.3 and B.4). Only
cliques containing at least 3 nodes are used. A draw-
back of this voting procedure is that the final result
may not be independent of the voting order, in some
cases. However, it is assured that the result is con-
sistent, i.e. that no document will sit in two different
cliques, or that two documents from the same clique
will not have two different relations toward a third
document.
Note that this vote leads to improvements only
if the precision of the initial classifier is sufficiently
good. As we will see in Section 5.2, this is the case
in our situation, but one must keep in mind that a
vote leaning on too imprecise material would lead to
even worse results. Some experiments on the devel-
opment set show us that at least 70% precision was
necessary. Another way to ensure robustness of the
vote would be to apply the transitive closure only
on bigger cliques (e.g., containing more than 3 or
4 nodes).
</bodyText>
<subsectionHeader confidence="0.972498">
4.3 C. Continuation versus Reaction
</subsectionHeader>
<bodyText confidence="0.9994712">
The approach for reaction extraction is different. We
first try to determine which documents describe re-
actions, regardless of which event it is a reaction to.
In the training set, all documents having at least one
incoming reaction edge are considered as reaction
</bodyText>
<figureCaption confidence="0.983409285714286">
Figure 8: Vote for same-event transitive closure. At the
top (a.), four nodes from the 5-node clique are linked to
document D1, which is enough to add D1 to the clique.
At the bottom (b.), only two nodes from the clique are
linked to documents D2 and D3, which is not enough to
add them into the clique. All edges from the clique to D2
and D3 are then deleted.
</figureCaption>
<bodyText confidence="0.998550785714286">
documents, all others are not. This distinction is
then learned with the same model and features as
for step B.1 (Section 4.2.1).
Once reaction documents have been selected,
the question is how to decide to which other doc-
ument(s) it must be linked. For example, in Fig-
ure 1, “Queen Elizabeth expresses deep sorrow” is
a reaction to pope’s death, not to other documents in
the temporal thread (for example, not to other reac-
tions or to “Pope in serious condition”). We did not
manage to build any classifier leading to satisfying
results at this point. We then proposed the two fol-
lowing basic heuristics, applied on all continuation
relations found after step B:
</bodyText>
<listItem confidence="0.9488955">
• A reaction reacts to only one event.
• A reaction reacts to an important event. Then,
among all continuation edges incoming to
the reaction document, we choose the biggest
same-event clique and create reaction edges
instead of continuations. If there is no
clique (only single nodes) or several same-size
cliques, all of them are tagged as reactions.
</listItem>
<bodyText confidence="0.8314815">
This module is called step C.1. Finally, a transitive
closure is performed for reactions (C.2).
</bodyText>
<figure confidence="0.3797665">
ñ
ñ
</figure>
<page confidence="0.89243">
964
</page>
<table confidence="0.9999374">
Relation Precision Recall F1
NIL 0.754 0.821 0.786
same-event 0.832 0.812 0.822
continuation 0.736 0.696 0.715
4 reaction 0.273 0.077 0.120
</table>
<tableCaption confidence="0.993031">
Table 2: Results obtained by the baseline system. Con-
tinuation scores do not consider reactions, only the last
row makes the distinction.
</tableCaption>
<sectionHeader confidence="0.999201" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.956953">
5.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999787">
As a baseline, we propose a single classifier deter-
mining all classes at once, based on the same SMO
classifier with the exact same parameters and all
similarity-based features (on titles, first sentences
and entire documents) described in Section 4.1.1.
Table 2 shows results for this baseline. Unsur-
prisingly, same-event relations are quite well clas-
sified by this baseline, since similarity is the major
clue for this class. Continuation is much lower and
only 3 reactions are well detected.
</bodyText>
<subsectionHeader confidence="0.999336">
5.2 System Evaluation
</subsectionHeader>
<bodyText confidence="0.999309">
Results for all successive steps described in previous
section are shown in Figure 3. The final result of the
entire system is the last one. The first observation
is that redundancy-based steps improve performance
in a significant manner:
</bodyText>
<listItem confidence="0.7572304">
• Classifiers A.2 and B.2, using the number of
incoming or outgoing edges found at previous
steps, lead to very significant improvement.
• Among transitivity closure algorithms (B.3,
B.4, C.2), only same-event transitivity B.3
</listItem>
<bodyText confidence="0.988934">
leads to significant improvement. Furthermore,
as we already noticed, these algorithms must be
used only when a good precision is guaranteed
at previous step. Otherwise, there is a risk of
inferring mostly bad relations. This is why we
biased classifier at step B.1 towards precision.
Finally, if this condition on precision is true,
transitivity closure is a robust way to get new
relations for free.
Results also tell that classification of relations
same-event and continuation is encouraging. Reac-
tion level gets a fair precision but a bad recall. This
</bodyText>
<table confidence="0.999275416666667">
Step Relation Precision Recall F1
A. NIL vs non-NIL classifier
A.1 NIL 0.764 0.815 0.788
non-NIL 0.921 0.896 0.910
A.2 NIL 0.907 0.811 0.857
*** non-NIL 0.925 0.966 0.945
B. Same-event vs continuation classifier
B.1 NIL 0.907 0.811 0.857
same-event 0.870 0.553 0.676
continuation 0.664 0.867 0.752
B.2 NIL 0.947 0.831 0.885
*** same-event 0.894 0.724 0.800
continuation 0.744 0.911 0.819
B.3 NIL 0.884 0.831 0.857
** same-event 0.943 0.819 0.877
continuation 0.797 0.906 0.848
B.4 NIL 0.890 0.831 0.860
* same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
C. Reaction vs continuation
C.1 NIL 0.890 0.831 0.860
C.2 same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
4 reaction 0.778 0.359 0.491
</table>
<tableCaption confidence="0.999355">
Table 3: Results obtained at each step of the classifica-
</tableCaption>
<bodyText confidence="0.9140565">
tion process. The significance of the improvement wrt
previous step (when relevant) is indicated by the Student
t-test (*: non significant; **: p &lt; 0.05 (significant); ***:
p &lt; 0.01 (highly significant)). Steps C.1 and C.2 are
aggregated, since their results are exactly the same.
is not catastrophic since most of the missed reactions
are tagged as continuation, which is still true (only
10% of the reaction relations are mistagged as same-
event). However, there is big room for improvement
on this point.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="evaluation">
6 Application
</sectionHeader>
<bodyText confidence="0.9977146">
As we showed in previous section, results for classi-
fication of same-event and continuation relations be-
tween documents are good enough to use this system
in an application that builds “event threads” around
an input document. The use case is the following:
</bodyText>
<listItem confidence="0.950297">
• The reader reads an article (let’s say, about the
death of John Paul II, article published on Feb.
4th, 2005 (UT) – see Figure 1).
• A link in the page suggests the user to visualize
the event thread around this article.
</listItem>
<page confidence="0.997054">
965
</page>
<figureCaption confidence="0.98741">
Figure 9: An example of temporal thread obtained on the death of John Paul II for user visualization (see corresponding
relation graph in Figure 1).
</figureCaption>
<listItem confidence="0.732477961538462">
• All articles within a period of 7 days around
the event, sharing at least two keywords with
the current document, are collected. All pairs
are given to the system4.
• When same-event cliques are found, only the
longest article (often, the most recent one) of
each clique is presented to the user. However,
the date and time presented to the user are those
of the first article relating the event.
• This leads to a graph with only continuation
and reaction relations. Edges are “cleaned” so
that a unique thread is visible: relations that can
be obtained by transitivity are removed, edges
between two documents are kept only if no doc-
ument can be inserted in-between.
• Nodes are presented in chronological order.
The user can visualize and navigate through
this graph (the event thread shows only titles
but full articles can be accessed by clicking on
the node).
• When found, reactions are isolated from the
main thread.
• Such a temporal thread is potentially infinite. If
the user navigates through the end of the 7-day
window, the system must be run again on the
next time span.
</listItem>
<footnote confidence="0.996207">
4In case of very important events where “all pairs” would be
too much, the temporal window is restrained. However, there is
no real time performance issue in this system.
</footnote>
<figureCaption confidence="0.940833">
Figure 9 presents the result of this process on
the partial temporal graph shown in Figure 1.
</figureCaption>
<sectionHeader confidence="0.997818" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99989844">
This article presents a task of multidocument tem-
poral graph building. We make the assumption that
each news article (after filtering) relates an event,
and we present a system extracting relations be-
tween articles. This system uses simple features and
algorithms but takes advantage of the important re-
dundancy of information in a news corpus, by in-
corporating redundancy information in a cascade of
classifiers, and by using transitivity of relations to
infer new links.
Finally, we present an application presenting
“event threads” to the user, in order to contextual-
ize the information and recomposing the story of an
event.
Now that the task is well defined and that en-
couraging results have been obtained, we envisage to
enrich classifiers by more fine-grained temporal and
lexical information, such as narrative chains (Cham-
bers and Jurafsky, 2008) for continuation relation
or event clustering (Barzilay et al., 2002) for same-
event relation. There is no doubt that reaction de-
tection can be improved a lot, by going beyond sim-
ple lexical features and discovering specific patterns.
We also intend to adapt the described system to other
languages than English.
</bodyText>
<page confidence="0.997678">
966
</page>
<sectionHeader confidence="0.995891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957913580247">
A. Ahmed, Q. Ho, J. Eisenstein, E.P. Xing, A.J. Smola,
and C.H. Teo. 2011. Unified Analysis of Streaming
News. In Proceedings of WWW, Hyderabad, India.
A. Balahur, R. Steinberger, E. van der Goot,
B. Pouliquen, and M. Kabadjov. 2009. Opinion
Mining on Newspaper Quotations. In Proceedings
of International Joint Conference on Web Intelli-
gence and Intelligent Agent Technologies, Milano,
Italy.
R. Barzilay, N. Elhadad, and K.R. McKeown. 2002. In-
ferring Strategies for Sentence Ordering in Multi-
document News Summarization. Journal of Artifi-
cial Intelligence Research, 17:35–55.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications
of the ACM, 16(9):575–577.
N. Chambers and D. Jurafsky. 2008. Unsupervised
Learning of Narrative Event Chains. In Proceedings
of the 46th Annual Meeting of the ACL, Columbus,
USA.
N. Chambers, S. Wang, and D. Jurafsky. 2007. Classify-
ing temporal relations between events. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, Prague,
Czech Republic, June.
J. Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Mea-
surement, 43(6):551–558.
T. Fujiki, H. Nanba, and M. Okumura. 2003. Automatic
Acquisition of Script Knowledge from a Text Col-
lection. In Proceedings of EACL, Budapest, hun-
gary.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explo-
rations, 11(1).
2012. Proceedings of i2b2/VA Shared-Tasks and Work-
shop on Challenges in Natural Language Process-
ing for Clinical Data, Chicago, USA.
R. Kessler, X. Tannier, C. Hag`ege, V. Moriceau, and
A. Bittar. 2012. Finding Salient Dates for Build-
ing Thematic Timelines. In Proceedings of the 50th
Annual Meeting of the ACL, Jeju Island, Republic of
Korea.
R. Krestel, S. Bergler, and R. Witte. 2008. Minding the
Source: Automatic Tagging of Reported Speech in
Newspaper Articles. In Proceedings of LREC, Mar-
rakech, Morocco.
I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Puste-
jovsky. 2006. Machine learning of temporal rela-
tions. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the ACL, Sydney, Australia.
S.A. Mirroshandel and G. Ghassem-Sani. 2012. Towards
Unsupervised Learning of Temporal Relations be-
tween Events. In Journal of Artificial Intelligence
Research, volume 45.
J.C. Platt, 1998. Advances in Kernel Methods - Support
Vector Learning, chapter Fast Training of Support
Vector Machines Using Sequential Minimal Opti-
mization. MIT Press.
B. Pouliquen, R. Steinberger, and C. Best. 2007. Auto-
matic Detection of Quotations in Multilingual News.
In Proceedings of RANLP, Borovets, Bulgaria.
P.P. Talukdar, D. Wijaya, and T. Mitchell. 2012. Ac-
quiring Temporal Constraints between Relations. In
Proceedings of the 21st ACM international confer-
ence on Information and knowledge management,
Hawaii.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007 -
15: TempEval Temporal Relation Identification. In
Proceedings of SemEval workshop at ACL, Prague,
Czech Republic.
M. Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.
2010. SemEval-2010 - 13: TempEval-2. In Pro-
ceedings of SemEval workshop at ACL, Uppsala,
Sweden.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In Proceedings of HLT-EMNLP.
</reference>
<page confidence="0.997635">
967
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286606">
<title confidence="0.999987">Building Event Threads out of Multiple News Articles</title>
<author confidence="0.999316">Xavier Tannier</author>
<affiliation confidence="0.987406">LIMSI-CNRS Univ. Paris-Sud</affiliation>
<address confidence="0.984585">Orsay, France</address>
<email confidence="0.993616">xavier.tannier@limsi.fr</email>
<affiliation confidence="0.81174">V´eronique Univ. Orsay,</affiliation>
<email confidence="0.998042">moriceau@limsi.fr</email>
<abstract confidence="0.975470777777778">We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A specific effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classifiers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for a training and testing</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ahmed</author>
<author>Q Ho</author>
<author>J Eisenstein</author>
<author>E P Xing</author>
<author>A J Smola</author>
<author>C H Teo</author>
</authors>
<title>Unified Analysis of Streaming News.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="7401" citStr="Ahmed et al. (2011)" startWordPosition="1202" endWordPosition="1205">he reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a document, but not on the identification of an event which is the source of a reaction and which can possibly be in another document. As we can see, all these approaches, as well as traditional information extraction approaches, lean on information contained by a single document, and consider an event as a word or a phrase. However, Ahmed et al. (2011) proposed a framework to group temporally and tocipally related news articles into same story clusters in order to reveal the temporal evolution of stories. But in these topically related clusters of documents, no temporal relation is detected between articles or events except chronological order. On this point of view, our task is closer to what is done in multidocument summarization, where a system has to detect redundant excerpts from various texts on the same topic and present results in a relevant chronological order. For example, Barzilay et al. (2002) propose a system for multidocument </context>
</contexts>
<marker>Ahmed, Ho, Eisenstein, Xing, Smola, Teo, 2011</marker>
<rawString>A. Ahmed, Q. Ho, J. Eisenstein, E.P. Xing, A.J. Smola, and C.H. Teo. 2011. Unified Analysis of Streaming News. In Proceedings of WWW, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>R Steinberger</author>
<author>E van der Goot</author>
<author>B Pouliquen</author>
<author>M Kabadjov</author>
</authors>
<title>Opinion Mining on Newspaper Quotations.</title>
<date>2009</date>
<booktitle>In Proceedings of International Joint Conference on Web Intelligence and Intelligent Agent Technologies,</booktitle>
<location>Milano, Italy.</location>
<marker>Balahur, Steinberger, van der Goot, Pouliquen, Kabadjov, 2009</marker>
<rawString>A. Balahur, R. Steinberger, E. van der Goot, B. Pouliquen, and M. Kabadjov. 2009. Opinion Mining on Newspaper Quotations. In Proceedings of International Joint Conference on Web Intelligence and Intelligent Agent Technologies, Milano, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K R McKeown</author>
</authors>
<title>Inferring Strategies for Sentence Ordering in Multidocument News Summarization.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>17--35</pages>
<contexts>
<context position="7965" citStr="Barzilay et al. (2002)" startWordPosition="1294" endWordPosition="1297"> event as a word or a phrase. However, Ahmed et al. (2011) proposed a framework to group temporally and tocipally related news articles into same story clusters in order to reveal the temporal evolution of stories. But in these topically related clusters of documents, no temporal relation is detected between articles or events except chronological order. On this point of view, our task is closer to what is done in multidocument summarization, where a system has to detect redundant excerpts from various texts on the same topic and present results in a relevant chronological order. For example, Barzilay et al. (2002) propose a system for multidocument summarization from newswire articles describing the same event. First, similar text units from different documents are identified using statistical techniques and shallow text analysis and grouped into thematic clusters. Then, in each theme, sentences which are selected as part of the summary are ordered using the publication date of the first occurrence of events to order sentences. 3 Resources We built an annotated collection of English articles, taken from newswire texts provided by the French news agency (AFP), spreading over the period 2004- 2012. The e</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>R. Barzilay, N. Elhadad, and K.R. McKeown. 2002. Inferring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Artificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bron</author>
<author>J Kerbosch</author>
</authors>
<title>Algorithm 457: finding all cliques of an undirected graph.</title>
<date>1973</date>
<journal>Communications of the ACM,</journal>
<volume>16</volume>
<issue>9</issue>
<contexts>
<context position="19845" citStr="Bron and Kerbosch (1973)" startWordPosition="3263" endWordPosition="3266">revious classifier). 4.2.3 Steps B.3 and B.4: Transitive closure by vote As already stated, same-event and continuation relations are transitive. Same-event is also symmetric (A same-event B ñ B same-event A). In the 963 graph formed by documents (vertices) and relations (edges), it is then possible to find all cliques, i.e. subsets of vertices such that every two vertices in the subset are connected by a same-event relation, as illustrated by Figure 7. This step does not involve any learning phase. Starting from the result of last step, we find all sameevent cliques in the graph by using the Bron and Kerbosch (1973) algorithm. The transitive closure process is then illustrated by Figure 8. If the classifier proposed a relation between some documents of a clique and some other documents (as D1, D2 and D3), then a vote is necessary: • If the document is linked to half or more of the clique, then all missing links are created (Figure 8.a); • Otherwise, the document is entirely disconnected from the clique (Figure 8.b). This vote is done for same-event and continuation relations (resp. steps B.3 and B.4). Only cliques containing at least 3 nodes are used. A drawback of this voting procedure is that the final</context>
</contexts>
<marker>Bron, Kerbosch, 1973</marker>
<rawString>C. Bron and J. Kerbosch. 1973. Algorithm 457: finding all cliques of an undirected graph. Communications of the ACM, 16(9):575–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Event Chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="6451" citStr="Chambers and Jurafsky (2008)" startWordPosition="1047" endWordPosition="1050">mporal relations, but also to decide whether a temporal relation existed or not between two elements, either clinical concepts or temporal expressions. But, as in TempEval, the temporal analysis were only to be performed within a single document. Other works focus on event ordering. For example, Fujiki et al. (2003) and Talukdar et al. (2012) proposed methods for automatic acquisition of event sequences from texts. They did not use temporal information present in texts and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronologi959 cal order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the identification of the reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a doc</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>N. Chambers and D. Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proceedings of the 46th Annual Meeting of the ACL, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>S Wang</author>
<author>D Jurafsky</author>
</authors>
<title>Classifying temporal relations between events.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5249" citStr="Chambers et al., 2007" startWordPosition="853" endWordPosition="856"> in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also</context>
</contexts>
<marker>Chambers, Wang, Jurafsky, 2007</marker>
<rawString>N. Chambers, S. Wang, and D. Jurafsky. 2007. Classifying temporal relations between events. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="11618" citStr="Cohen, 1960" startWordPosition="1895" endWordPosition="1896">n two events. For example, two sport events of the same Olympic Games, or two different attacks in Iraq, shall not be linked together unless a direct link between both is specified in the articles. • reaction, a subset of continuation, when a document relates the reaction of someone to another event, as illustrated by the example in Figure 4. Figure 5: Example of relations same-event between two documents: update on casualties (top) or details (bottom). • nil, when no relation can be identified between the two documents. The inter-annotator agreement was calculated with Cohen’s Kappa measure (Cohen, 1960) across 150 pairs: n = 0.68. The agreement was low for the first 4 types of relations mostly because the difference between relations was not clear enough. We therefore aggregated the number update, form update and details relations into a more generic and consensual same-event relation (see Figure 5). We also discarded the development of same story relation, leaving only same-event, continuation and reaction. Annotation guidelines were modified and a second annotation round was carried out: only the same-event, continuation, reaction and nil relations were annotated. Inter-annotator agreement</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 43(6):551–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujiki</author>
<author>H Nanba</author>
<author>M Okumura</author>
</authors>
<title>Automatic Acquisition of Script Knowledge from a Text Collection.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Budapest,</location>
<contexts>
<context position="6140" citStr="Fujiki et al. (2003)" startWordPosition="998" endWordPosition="1001">vent pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also to decide whether a temporal relation existed or not between two elements, either clinical concepts or temporal expressions. But, as in TempEval, the temporal analysis were only to be performed within a single document. Other works focus on event ordering. For example, Fujiki et al. (2003) and Talukdar et al. (2012) proposed methods for automatic acquisition of event sequences from texts. They did not use temporal information present in texts and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronologi959 cal order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before re</context>
</contexts>
<marker>Fujiki, Nanba, Okumura, 2003</marker>
<rawString>T. Fujiki, H. Nanba, and M. Okumura. 2003. Automatic Acquisition of Script Knowledge from a Text Collection. In Proceedings of EACL, Budapest, hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="14423" citStr="Hall et al., 2009" startWordPosition="2359" endWordPosition="2362">focus on complex algorithms or classifiers for tuning our results, and most of our features are very simple. The idea here is to show that good results can be obtained in this original and useful task. The process can be separated into 3 main stages, illustrated in Figure 6: A. Filtering out pairs that have no relation at all, i.e. classifying between nil and non-nil relations; B. Classifying between same-event and continuation relations; C. Extracting reactions from the set of continuation relations. All described classifiers use SMO (Platt, 1998), the SVM optimization implemented into Weka (Hall et al., 2009), with logistic models fitting (option “- M”). With this option, the confidence score of each Figure 6: A 3-step classification. prediction can be used, while SMO alone provides a constant probability for all instances. From now on, when considering a pair of documents, we will refer to the older document as document 1, and to the more recent one as document 2. The relations found between documents will be represented by a directed graph where documents are vertices and relations are edges. 4.1 A. Nils versus non-nils We first aim at separating nil relations (no relation between two events) fr</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>Proceedings of i2b2/VA Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data,</booktitle>
<location>Chicago, USA.</location>
<contexts>
<context position="5302" citStr="(2012)" startWordPosition="863" endWordPosition="863">and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also to decide whether a temporal relation existed or not</context>
<context position="17608" citStr="(2012)" startWordPosition="2890" endWordPosition="2890">elations. 4.2 B. Same-event versus Continuation We are now working only with non-nil relations (even if some relations may switch between nil and non-nil during the transitive closure). 4.2.1 Step B.1: Relation classifier, level 1 Distinction between same-event and continuation is made by the following sets of features: • Date features: – Difference between the two document creation times (DCTs): difference in days, in hours, in minutes (3 features); – Whether the creation time of doc. 1 is mentioned in doc. 2. For this purpose, we use the date normalization system described in Kessler et al. (2012). – Cosine similarity between the first sentence of doc. 1 and sentences of doc. 2 containing the DCT of doc. 1. – Cosine similarity between the first sentence of doc. 1 and the most similar sentence of doc. 2. Figure 7: An example of highly-connected subgraph, corresponding to the development of an important story. Same events are grouped by cliques (see Section 4.2.3) and some redundant relations are not shown for clarity’s sake. These last three features come from the idea that a continuation relation can be made explicit in text by mentioning the first event in the second document. • Tempo</context>
</contexts>
<marker>2012</marker>
<rawString>2012. Proceedings of i2b2/VA Shared-Tasks and Workshop on Challenges in Natural Language Processing for Clinical Data, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kessler</author>
<author>X Tannier</author>
<author>C Hag`ege</author>
<author>V Moriceau</author>
<author>A Bittar</author>
</authors>
<title>Finding Salient Dates for Building Thematic Timelines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the ACL, Jeju Island,</booktitle>
<location>Republic of</location>
<marker>Kessler, Tannier, Hag`ege, Moriceau, Bittar, 2012</marker>
<rawString>R. Kessler, X. Tannier, C. Hag`ege, V. Moriceau, and A. Bittar. 2012. Finding Salient Dates for Building Thematic Timelines. In Proceedings of the 50th Annual Meeting of the ACL, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krestel</author>
<author>S Bergler</author>
<author>R Witte</author>
</authors>
<title>Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="6941" citStr="Krestel et al. (2008)" startWordPosition="1122" endWordPosition="1125"> of events (e.g. arrest/escape) from sentences which were already arranged in chronologi959 cal order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the identification of the reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a document, but not on the identification of an event which is the source of a reaction and which can possibly be in another document. As we can see, all these approaches, as well as traditional information extraction approaches, lean on information contained by a single document, and consider an event as a word or a phrase. However, Ahmed et al. (2011) proposed a framework to group temporally and tocipally related news articles into same story clusters in order to reveal the temporal evolu</context>
</contexts>
<marker>Krestel, Bergler, Witte, 2008</marker>
<rawString>R. Krestel, S. Bergler, and R. Witte. 2008. Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles. In Proceedings of LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Verhagen</author>
<author>B Wellner</author>
<author>C Lee</author>
<author>J Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="5225" citStr="Mani et al., 2006" startWordPosition="849" endWordPosition="852">en events and times in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temp</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Mirroshandel</author>
<author>G Ghassem-Sani</author>
</authors>
<title>Towards Unsupervised Learning of Temporal Relations between Events.</title>
<date>2012</date>
<journal>In Journal of Artificial Intelligence Research,</journal>
<volume>45</volume>
<contexts>
<context position="5302" citStr="Mirroshandel and Ghassem-Sani (2012)" startWordPosition="860" endWordPosition="863"> sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use statistical learning based on temporal features (modality, tense, aspect, etc.) (Mani et al., 2006; Chambers et al., 2007). More recently, Mirroshandel and Ghassem-Sani (2012) proposed a new method for temporal relation extraction by using a bootstrapping method on annotated data and have a better accuracy than state-of-the-art systems. Their method is based on the assumption that similar event pairs in topically related documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also to decide whether a temporal relation existed or not</context>
</contexts>
<marker>Mirroshandel, Ghassem-Sani, 2012</marker>
<rawString>S.A. Mirroshandel and G. Ghassem-Sani. 2012. Towards Unsupervised Learning of Temporal Relations between Events. In Journal of Artificial Intelligence Research, volume 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Platt</author>
</authors>
<title>Advances in Kernel Methods - Support Vector Learning, chapter Fast Training of Support Vector Machines Using Sequential Minimal Optimization.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14359" citStr="Platt, 1998" startWordPosition="2351" endWordPosition="2352">tion between two documents (i.e., two events). We did not focus on complex algorithms or classifiers for tuning our results, and most of our features are very simple. The idea here is to show that good results can be obtained in this original and useful task. The process can be separated into 3 main stages, illustrated in Figure 6: A. Filtering out pairs that have no relation at all, i.e. classifying between nil and non-nil relations; B. Classifying between same-event and continuation relations; C. Extracting reactions from the set of continuation relations. All described classifiers use SMO (Platt, 1998), the SVM optimization implemented into Weka (Hall et al., 2009), with logistic models fitting (option “- M”). With this option, the confidence score of each Figure 6: A 3-step classification. prediction can be used, while SMO alone provides a constant probability for all instances. From now on, when considering a pair of documents, we will refer to the older document as document 1, and to the more recent one as document 2. The relations found between documents will be represented by a directed graph where documents are vertices and relations are edges. 4.1 A. Nils versus non-nils We first aim</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>J.C. Platt, 1998. Advances in Kernel Methods - Support Vector Learning, chapter Fast Training of Support Vector Machines Using Sequential Minimal Optimization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pouliquen</author>
<author>R Steinberger</author>
<author>C Best</author>
</authors>
<title>Automatic Detection of Quotations in Multilingual News.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP, Borovets,</booktitle>
<contexts>
<context position="6918" citStr="Pouliquen et al. (2007)" startWordPosition="1118" endWordPosition="1121">s and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronologi959 cal order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the identification of the reaction relation, to our knowledge, there is no work on the detection of reaction between several documents. Pouliquen et al. (2007), Krestel et al. (2008) and Balahur et al. (2009) focused on the identification of reported speech or opinions in quotations in a document, but not on the identification of an event which is the source of a reaction and which can possibly be in another document. As we can see, all these approaches, as well as traditional information extraction approaches, lean on information contained by a single document, and consider an event as a word or a phrase. However, Ahmed et al. (2011) proposed a framework to group temporally and tocipally related news articles into same story clusters in order to re</context>
</contexts>
<marker>Pouliquen, Steinberger, Best, 2007</marker>
<rawString>B. Pouliquen, R. Steinberger, and C. Best. 2007. Automatic Detection of Quotations in Multilingual News. In Proceedings of RANLP, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>D Wijaya</author>
<author>T Mitchell</author>
</authors>
<title>Acquiring Temporal Constraints between Relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<location>Hawaii.</location>
<contexts>
<context position="6167" citStr="Talukdar et al. (2012)" startWordPosition="1003" endWordPosition="1006">elated documents are likely to have the same temporal relations. For this work, the authors had already some collections of topically related documents and did not need to identify them. In the 2012 i2b2 challenge (i2b, 2012), the problem was not only to identify the type of temporal relations, but also to decide whether a temporal relation existed or not between two elements, either clinical concepts or temporal expressions. But, as in TempEval, the temporal analysis were only to be performed within a single document. Other works focus on event ordering. For example, Fujiki et al. (2003) and Talukdar et al. (2012) proposed methods for automatic acquisition of event sequences from texts. They did not use temporal information present in texts and extracted sequences of events (e.g. arrest/escape) from sentences which were already arranged in chronologi959 cal order. Chambers and Jurafsky (2008) proposed a method to learn narrative chains of events related to a protagonist in a single document. The first step consists in detecting narrative relations between events sharing coreferring arguments. Then, a temporal classifier orders partially the connected events with the before relation. Concerning the iden</context>
</contexts>
<marker>Talukdar, Wijaya, Mitchell, 2012</marker>
<rawString>P.P. Talukdar, D. Wijaya, and T. Mitchell. 2012. Acquiring Temporal Constraints between Relations. In Proceedings of the 21st ACM international conference on Information and knowledge management, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>R Gaizauskas</author>
<author>F Schilder</author>
<author>M Hepple</author>
<author>G Katz</author>
<author>J Pustejovsky</author>
</authors>
<title>SemEval-2007 -15: TempEval Temporal Relation Identification.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval workshop at ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4486" citStr="Verhagen et al., 2007" startWordPosition="731" endWordPosition="734">results are given in Section 5. We also propose an end-user application to this work. When a user reads an article, the system will then be able to provide her with a thread of events having occurred before or after, helping her to contextualize the information she is reading. This application is described in Section 6. 2 Related work The identification of temporal relations between events in texts has been the focus of increasing attention because of its importance in NLP applications such as information extraction, question-answering or summarization. The evaluation campaigns TempEval 2007 (Verhagen et al., 2007) and TempEval 2010 (Verhagen et al., 2010) focused on temporal relation identification, mainly on temporal relations between events and times in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having t</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky. 2007. SemEval-2007 -15: TempEval Temporal Relation Identification. In Proceedings of SemEval workshop at ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>R Sauri</author>
<author>T Caselli</author>
<author>J Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 - 13: TempEval-2. In Proceedings of SemEval workshop at ACL,</booktitle>
<location>Uppsala, Sweden.</location>
<contexts>
<context position="4528" citStr="Verhagen et al., 2010" startWordPosition="738" endWordPosition="741">opose an end-user application to this work. When a user reads an article, the system will then be able to provide her with a thread of events having occurred before or after, helping her to contextualize the information she is reading. This application is described in Section 6. 2 Related work The identification of temporal relations between events in texts has been the focus of increasing attention because of its importance in NLP applications such as information extraction, question-answering or summarization. The evaluation campaigns TempEval 2007 (Verhagen et al., 2007) and TempEval 2010 (Verhagen et al., 2010) focused on temporal relation identification, mainly on temporal relations between events and times in the same sentence or in consecutive sentences and between events and the creation time of documents. In this context, the goal is to identify the type of a temporal relation which is Figure 2: Example of “temporal graph”: Madrid attacks, with many updates of the initial information. Note that articles gathered in this main pool of articles can be posterior to the continuations and reactions to the described event. known to be present. Systems having the best results (accuracy about 0.6) use s</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>M. Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky. 2010. SemEval-2010 - 13: TempEval-2. In Proceedings of SemEval workshop at ACL, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="18742" citStr="Wilson et al., 2005" startWordPosition="3074" endWordPosition="3077">be made explicit in text by mentioning the first event in the second document. • Temporal features: whether words introducing temporal relations occur in document 1 or document 2. These manually-collected words can be prepositions (after, before, etc.) or verbs (follow, confirm, prove, etc.). • Reaction features: whether verbs introducing reactions occur in document 1 or document 2 (25 manually-collected verbs as approve, accept, welcome, vow, etc.). • Opinion features: whether opinion words occur in document 1 or document 2. The list of opinion words comes from the MPQA subjectivity lexicon (Wilson et al., 2005). Only same-event relations classified with more than 90% confidence by the classifier are kept, in order to ensure a high precision (recall will be improved at next step). This threshold has been set up on development set. 4.2.2 Step B.2: Relation classifier, level 2 As for step A.2, a second classifier is implemented, using the results of step B.1 with the same manner as A.2 uses A.1 (collecting numbers of same-event and continuation relations that have been found by the previous classifier). 4.2.3 Steps B.3 and B.4: Transitive closure by vote As already stated, same-event and continuation r</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. In Proceedings of HLT-EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
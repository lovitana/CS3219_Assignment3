<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.990136">
Optimized Event Storyline Generation based on Mixture-Event-Aspect
Model
</title>
<author confidence="0.986416">
Lifu Huang
</author>
<affiliation confidence="0.88540875">
Shenzhen Key Lab for Cloud Computing
Technology and Applications,
Peking University Shenzhen Graduate School,
Shenzhen, Guangdong 518055, P.R.China
</affiliation>
<email confidence="0.906607">
warrior.fu@gmail.com
</email>
<author confidence="0.755542">
Lian’en Huang*
</author>
<affiliation confidence="0.8387785">
Shenzhen Key Lab for Cloud Computing
Technology and Applications,
Peking University Shenzhen Graduate School,
Shenzhen, Guangdong 518055, P.R.China
</affiliation>
<email confidence="0.990577">
hle@net.pku.edu.cn
</email>
<sectionHeader confidence="0.998562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998823136363636">
Recently, much research focuses on event s-
toryline generation, which aims to produce a
concise, global and temporal event summary
from a collection of articles. Generally, each
event contains multiple sub-events and the sto-
ryline should be composed by the componen-
t summaries of all the sub-events. However,
different sub-events have different part-whole
relationship with the major event, which is
important to correspond to users’ interests
but seldom considered in previous work. To
distinguish different types of sub-events, we
propose a mixture-event-aspect model which
models different sub-events into local and
global aspects. Combining these local/global
aspects with summarization requirements to-
gether, we utilize an optimization method to
generate the component summaries along the
timeline. We develop experimental systems on
6 distinctively different datasets. Evaluation
and comparison results indicate the effective-
ness of our proposed method.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988463863636364">
With the rapid growth of the World Wide Web, in-
formation explosion has become an important issue
to modern people. Those who search for informa-
tion from the Internet often get lost and confused
by the overwhelmingly large collection of web doc-
uments. So how to get a concise and global pic-
ture for a given event subject is an urgent prob-
lem to be solved. Although many document un-
derstanding systems have been proposed, such as
∗Corresponding author
multi-document summarization systems, to gener-
ate a compressed summary by extracting the ma-
jor information from the collection of documents,
they ignored the dynamic development information
of an event. Intuitively, each event is long-running
and contains multiple sub-events, including related
events. Users are likely to prefer a summary of all
occurrences of all the sub-events along the timeline
of the event. This motivates us to study the task of
generating event storyline from a collection of web
documents related to an event subject.
The research of event storyline summarization is
popular in recent years. Its task is to summarize a
collection of web documents by extracting represen-
tative information based on all the sub-events and
generate a global summary. Generally, generating
such a global storyline is quite interesting for the fol-
lowing main reasons: (1) It can help people catch the
whole incident based on an overall temporal struc-
tured summary for a given subject, and understand
the cause, climax, development process and result
of an event. (2) It can also make people know what
other events are related, or the effect of this incident
to subsequent events, which can present the evolu-
tion of an event along a timeline.
Though several methods of generating event sto-
ryline have been proposed recently, there are still
some problems unresolved. As event storyline sum-
marization is a process to generate component sum-
maries based on the multiple sub-events, which is d-
ifferent from traditional summarization focusing on
only one subject, so how to exactly extract all the
sub-events is the first challenge. Moreover, user-
s tend to bias to the sub-events which have global
</bodyText>
<page confidence="0.970162">
726
</page>
<note confidence="0.733897">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726–735,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999895881355932">
consistency with the given event subject, so the sub-
events should not be considered equally when gen-
erating the component summaries. It is also a great
challenge to generate a qualified summary based on
the different types of sub-events. The componen-
t summaries should be correlative across differen-
t dates based on the global collection (Yan et al.,
2011a).
Mei and Zhai (Mei and Zhai, 2005) proposed to
use theme or topic to model different sub-events,
which is to some extent similar to our method. To
be different, in this paper we introduce “local/glob-
al” property to distinguish different part-whole rela-
tionship between the sub-events and the major even-
t, which have not been considered before in story-
line generation or summarization, to improve the
quality of the storyline. The local/global proper-
ty corresponds to the elements of an event, such
as the place, characters and other body informa-
tion. These information reflects the relationship be-
tween the sub-events and the major event. Some
sub-events have distinctive body information and lit-
tle relevance with each other. They generally occur
for a local period, which we name as “local-sub-
events”. While other sub-events often share com-
mon properties with each other and have close re-
lationship with the major event and we call them
as “global-sub-events”. Here we give some exam-
ples to illustrate the difference. For the event “Con-
necticut school shooting” which occurred on Dec.14
2012, its sub-events such as “Obama’s speech for
this massacre” or “Gun control Act” have little word
co-occurrences and distinctive event body informa-
tion to each other, while the process and result of this
tragedy can be regarded as global-sub-events which
have a lot of word co-occurrences and share com-
mon properties with the major event.
Inspired by these, to detect different types of sub-
events based on word co-occurrences between sub-
events and the major event, we propose a mixture-
event-aspect (MEA) model to formalize differen-
t types of sub-events into local/global aspects, which
are implicated with clusters of sentences. Then com-
bining the local/global aspects with summarization
requirements together, we utilized an optimization
approach to get the optimal component summaries
along the timeline. We evaluate our method on 6 dis-
tinctively different datasets. Performance compar-
isons among different system-generated storylines
demonstrate the necessity to distinguish differen-
t types of sub-events and also indicates the effective-
ness of the proposed mixture-event-aspect model.
The rest of the paper is organized as follows. We
briefly review the related work in section 2. In sec-
tion 3 we present the details of optimized event s-
toryline generation based on mixture-event-aspect
model. Experiments and results are discussed in
Section 4. Finally we draw a conclusion of this s-
tudy in Section 5.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999615382352941">
Our work is related to several lines of research in the
literature including multi-document summarization
(MDS), topic detection and tracking (TDT), tempo-
ral text mining (TXM) and temporal news summa-
rization (TNS).
Multi-document summarization is a process to
generate a summary by reducing documents in size
while retaining the main information. To date, dif-
ferent features and ranking strategies have been s-
tudied. Radev et al. (Radev et al., 2004) proposed
to implement MEAD as a centroid-based summa-
rizer by combining several predefined features to s-
core the sentence. LexPageRank (Erkan and Radev,
2004) is the representative work which is based on
PageRank (Page et al., 1999) algorithm. Some meth-
ods have been proposed to extend the conventional
graph-based models recently including multi-layer
graph incorporated with different relationship (Wan,
2008), ToPageRank based on the topic information
(Pei et al., 2012) and DivRank (Mei et al., 2010) bal-
ancing the prestige and diversity.
Topic detection and tracking (TDT) aims to group
news articles based on the topics discussed in them,
detect some novel and previously unreported events
and track future events related to the topics (Wang
et al., 2012). Kumaran and Allan (Kumaran and Al-
lan, 2004) showed how performance on new event
detection could be improved by the use of text clas-
sification techniques as well as by using named en-
tities in a new way. Makkonen et al. (Makkonen
et al., 2004) proposed a method that incorporated
simple semantics into TDT by splitting the term s-
pace into groups of terms. Krause et al. Wang et al.
(Wang et al., 2007) and Wang et al. (Wang et al.,
</bodyText>
<page confidence="0.994341">
727
</page>
<bodyText confidence="0.999943">
2009) worked on topic tracking from multiple news
streams. Their methods extracted meaningful top-
ics from multi-source news collections and tracked
different topics as they evolved from one to another
along the timeline.
Our work is also related to temporal text mining
and temporal news summarization. The task of tem-
poral news summarization is to generate news sum-
maries along the timeline from massive data. Chieu
et al. (Chieu and Lee, 2004) built a system that ex-
tracted events relevant to a query from a collection
of related documents and placed such events along
a timeline. Yan et al. (Yan et al., 2011b) designed
an evolutionary timeline summarization approach to
construct a timeline of a topic by optimizing the rel-
evance, coverage, coherence, and diversity. Lin et al.
(Lin et al., 2012) explored the problem of generating
storylines from microblogs for user input queries.
They first proposed a language model with dynamic
pseudo relevance feedback to obtain relevant tweet-
s and then generated storylines via graph optimiza-
tion.
</bodyText>
<sectionHeader confidence="0.982787" genericHeader="method">
3 Approach Details
</sectionHeader>
<bodyText confidence="0.999937428571428">
In this section, we first propose a mixture-event-
aspect model to detect local/global sub-events based
on part-whole relationship with the major event and
then present a new method to estimate the bursty of
each aspect on a certain date. Afterwards we uti-
lize an optimization method based on local/global
aspects to extract the qualified summary.
</bodyText>
<subsectionHeader confidence="0.982099">
3.1 Mixture-Event-Aspect Model
</subsectionHeader>
<bodyText confidence="0.999975821428571">
The key challenge to our storyline generation task
is to detect and distinguish different types of sub-
events contained in the article collection. In the col-
lection, each sentence is assigned with a certain date
and sentences that are assigned with the same date
are grouped into the same sub-collection. Consid-
ering the consistency of content between the sub-
events and the major event, we model different sub-
events into two types: local-sub-event and global-
sub-event, and introduce local/global aspects cor-
respondingly. Generally, local aspects which cor-
respond to local-sub-events have distinctive word-
s distribution from each other and sustain for a lo-
cal context while the global aspects corresponding
to global-sub-events have coincident words distri-
bution with the major event. To capture specific
words, Titov and McDonald (Titov and McDonald,
2008) proposed a multi-grain topic model, relying
on word co-occurrences within short paragraphs and
Li et al. (Li et al., 2010) proposed a entity-aspect
model based on word co-occurrences within single
sentences. Inspired by these ideas, we rely on word
co-occurrences within local period context to detec-
t mixed local and global aspects implicated in the
whole collection. We name this model as “Mixture-
Event-Aspect (MEA)” model which can simultane-
ously detect local/global aspects and cluster sen-
tences and words into different aspects.
</bodyText>
<subsectionHeader confidence="0.607943">
3.1.1 Model Description
</subsectionHeader>
<bodyText confidence="0.99837534375">
Our mixture-event-aspect (MEA) model can be
extended from both the Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999) and Laten-
t Dirichlet Allocation (LDA) (Blei et al., 2003). We
model two distinct types of aspects: global aspect-
s and local aspects, based on their relationship with
the major event. The distribution of global aspects is
fixed for the collection while the distribution of local
aspects is fixed to a local period of sub-collections.
That means a sentence is sampled either from the
mixture of the global aspects or from the local as-
pects specific for the local context. Here we take
the event “Connecticut school shooting” as an exam-
ple. For the sentence “On Sunday, President Obama
came to Connecticut to give a lecture, expressing his
sorrow for ... and calling for an end to such inci-
dents”, the words such as “Obama”, “lecture”, “ex-
press” are only occurred for the local period of two
days and have no co-occurrence with other neigh-
boring period sentences, so we sample the sentence
as a local aspect sentence. But for the sentence “All
schools in Newtown, the northeastern U.S. state of
Connecticut were in lockdown after a shooting was
reported at a local elementary school”, the word-
s such as “Connecticut”, “shooting”, “elementary”
have high co-occurrence frequency in the whole col-
lection, so we sample the sentence as a global aspect
sentence.
To detect aspects, we first divide words into two
types: aspect words and background words. Back-
ground words are commonly used in the whole even-
t corpus while aspect words are clearly associat-
</bodyText>
<page confidence="0.987262">
728
</page>
<bodyText confidence="0.999736444444444">
ed with the aspects of the sentences they occur in.
Stop words are removed using a standard stop word
list. In order to get the distribution of local aspect-
s, we implement a mechanism called “Time Win-
dow” which covers Sp sequential time-based sub-
collections. We associate each time window with
a distribution over local aspects and a distribution
defining preference of local aspects versus global as-
pects.
</bodyText>
<construct confidence="0.81121725">
draw WB ∼ Dir(β), Oc(v) ∼ Dir(λ), π ∼ Dir(γ)
draw Wgl ∼ Dir(β) for Agl times
draw Wloc ∼ Dir(β) for Aloc times
choose a distribution of global aspects 0gl ∼ Dir(αgl)
For each time window v in collection c
choose 0loc c,v∼ Dir(αloc)
choose Pc,v ∼ Beta(αmix)
For each sentence s in collection c
choose window νc,s ∼ Oc
choose 77c,s ∼ Pc,νc,s
if 77c,s = gl, zc,s ∼ 0gl
if 77c,s = loc, zc,s ∼ 0loc
</construct>
<equation confidence="0.5741952">
c,v
For each word w of sentence s in collection c
draw yc,s,n ∼ Multi(π)
draw wc,s,n ∼ Multi(WB) if yc,s,n = 1
draw wc,s,n ∼ Multi(Wzc,s) if yc,s,n = 2
</equation>
<figureCaption confidence="0.999947">
Figure 1: The Collection Generation Process
</figureCaption>
<bodyText confidence="0.999631375">
Formally, let C = {Ct|t = 1, 2, 3, ..., T} be
T time based sub-collections related to the even-
t subject, Ct represents the collection of sentences
which are assigned with the date t. Let v be a time
window containing Sp sequential sub-collections,
v = {Ct|t = i, i + 1, ..., i + Sp − 1}. We draw
a background unigram language model which gen-
erates words for all sub-collections, and draw Agl
global aspect unigram language models for global
aspects and Aloc word distributions for local aspects.
We assume these word distributions have a uniform
Dirichlet prior Dir(β). There is also a multinomi-
al distribution 7r that controls in each sentence how
often the word occurs as a background word or an
aspect word. 7r has a Dirichlet prior with parame-
ter -y. We assign each window v with an distribution
over local aspects and a distribution ρ defining pref-
erence for local aspects versus global aspects. ρ has
a Beta prior αmix. A sentence can be sampled using
any window which is chosen according to a categor-
ical distribution.
In Figure 2 the corresponding graphical model is
presented. This model allows for fast approximate
inference with collapsed Gibbs sampling.
</bodyText>
<figureCaption confidence="0.984901">
Figure 2: Mixture-Event-Aspect Model
</figureCaption>
<bodyText confidence="0.999983875">
Let SC denotes the number of sentences in col-
lection C, Nc,s denotes the number of words in sen-
tence s of collection c, and wc,s,n denotes the nth
word in sentence s. There are two kinds of hidden
variables: zc,s for each sentence to indicate the as-
pect a sentence belongs to, and yc,s,n for each word
to indicate whether a word is generated from the
background model or the aspect model.
</bodyText>
<subsectionHeader confidence="0.517336">
3.1.2 Inference via Gibbs Sampling
</subsectionHeader>
<bodyText confidence="0.999956714285714">
In order to estimate the hidden parameter-
s in the model, we try to maximize distribution
p(z,y|w; α, β, -y, λ), where z, y and w represent the
set of all z, y and w variables, respectively. Giv-
en a sentence s in the collection c, we apply Gibbs
Sampling to estimate the conditional probability for
local/global aspects using the following rules:
</bodyText>
<equation confidence="0.968041857142857">
p(vc,s = vh, ηc,s = gl, zc,s = a|v′, z′, y, w) ∝
nc,vh
(.) + �
r′ ∈gl,loc
IIl= 1 II `0)−1(Ca(l)+i+β)
�E(.)−1
i=0 (Ca (.)+i+Lβ)
</equation>
<figure confidence="0.960182818181818">
nch∗+λ
nc(.)+S∗pλ ·
nc,vh +αmix
gl gl
αmix
′
r
nc +αgl
gl,a
· nc
gl+Aglαgl ·
</figure>
<page confidence="0.895694">
729
</page>
<equation confidence="0.973324166666667">
p(vc,s = vh, ηc,s = loc, zc,s = a|v′, z′, y, w) a
nc,vh +αloc
loc,a
� �
nloc +Alocαloc
c,vh
r′ ∈gl,loc
∏L ∏E(l)−1
i=0 (Ca (l)+i+β)
∏E(.)−1
l=1
i=0 (Ca (.)+i+Lβ)
</equation>
<bodyText confidence="0.999292769230769">
where nch∗ =#{si|vc,si = vi−sp+h∗+1}, denotes
the number of times a sentence si in collection c is
assigned to its h∗th window and for each sentence
si, we have h = i − Sp + 1 + h∗. S∗p is the number
of windows that contain sentence s. nc (.) denotes the
number of sentences in collection c. Sp represents
the number of dates a window covered. nc,vh
gl and
nc,vh
loc are the number of sentences in window vh that
are assigned to global or local aspects. nc,vh (.)is the
number of sentences assigned to window vh. ncgl,a
is the number of sentences in all global aspects that
are assigned to aspect a and nc,vh
loc,a is the number of
local aspect sentences in the window vh are assigned
to aspect a. Agl is the number of global aspects in
collection C while Aloc is the number of local as-
pects. E(l) represents the number of times of word
l occurs in the current sentence and is assigned to
be an aspect word, while E(.) is the total number of
words in the current sentence that are assigned to be
an aspect word.
Then we compute the assignments of the consid-
ered word. We sample the hidden variables yc,s,n for
each word with the following rules:
</bodyText>
<equation confidence="0.969039">
(�) + 2 · γ (�) + L · 0
</equation>
<bodyText confidence="0.999913714285714">
where Cπ(1) and Cπ(2) are the numbers of words as-
signed to be background words and aspect words.
Cπ (·) is the total number of words. CB (·) is the total
number of background words and Ca (·) is the number
of words assigned to aspect a.
By using the samples from Gibbs sampling, we
can effectively make the following estimations:
</bodyText>
<equation confidence="0.97680975">
B — CBw +β a — Caw+β nch∗+λ
φw — CB+L· β, φw — Caw +L· β , Oh∗ = n( )+S∗p λ
Cπy +γv = n
C·π+2·γ, ρ77 nc,v+
c,v loc
B
gl = ngl,a+αglBloc = nloc,a+α
nyl+Aglαgl, a nloc+Alocαloc
</equation>
<bodyText confidence="0.999876">
The hyper-parameters like α, β, γ, λ can be estimat-
ed using standard methods introduced in (Minka,
2000).
</bodyText>
<subsectionHeader confidence="0.999723">
3.2 Bursty Period Detection
</subsectionHeader>
<bodyText confidence="0.999696857142857">
We borrow the definition of “bursty” from (Lappas
et al., 2009) to measure the popularity of the event
on a certain date. Intuitively, each aspect have differ-
ent bursties on different dates. In this section, we try
to obtain the temporal aspect sequences of an event
based on the bursty periods of all the aspects. Dur-
ing its bursty period, one aspect should (1)be more
popular than other aspects (2) be continuously more
popular than other time. Following these intuitions,
we design a method to measure the bursty of each
aspect and get the bursty period.
Let Ak be the kth aspect obtained from the
mixture-event-aspect model, we estimate the bursty
of Ak at a certain date t as follows.
</bodyText>
<equation confidence="0.999138">
bursty(Ak,t) = p(t|Ak) = p(Ak|t) · p(t)
</equation>
<bodyText confidence="0.999980090909091">
where p(Ak|t) is measured by the number of sen-
tences assigned to aspect Ak in date t divided by the
total number of sentences in date t. p(t) is estimated
by the total number of sentences in aspect Ak divid-
ed by the overall number of sentences in the collec-
tion C.
After getting the bursty of aspect Ak at each date,
we can find the most popular date and expand on
both sides to obtain the whole burst period in which
the bursties are higher than the neighboring aspects
and continuous higher than other dates.
</bodyText>
<subsectionHeader confidence="0.999728">
3.3 Optimization-based Storyline Generation
</subsectionHeader>
<bodyText confidence="0.9999696">
With the methods discussed in previous sections, we
can get the local/global aspect sequence. Each as-
pect contains numbers of sentences and we are aim-
ing to select the most representative ones to compose
the final storyline. Considering users’ bias and the
length requirement, different aspects should have d-
ifferent proportions in the last storyline. For glob-
al aspects which correspond more to users’ interest,
they should share a larger proportion in the final sto-
ryline than local aspects. Thus, we use an optimiza-
tion method to determine if a sentence is selected to
be an summary sentence or to be discarded based on
the multiple local/global aspects and finally get the
optimal storyline. We formalize this problem as se-
lecting a subset of sentences S from the aspect Ak
</bodyText>
<equation confidence="0.85918448">
nch∗+λ
nc(.)+S∗pλ �
nc,vh+αmix
loc loc
nc,vh
(.) + ∑
αmix
r′
p(yc,s,n = 1|z, y ) ∝ C(1) + γ C(w�,s,n) + 0
C() + 2 · γ CB(�) + L · 0
·
C(2) + γ Ca
(w�,s,n) + 0
p(yc,s,n = 2|z, y′) ∝ ·
C� Ca
πy =
αmix
r
∑
r∈(gl,loc)
c,v
η +αmix
η
∑ p(Ak|t′)p(t′)
t′
</equation>
<page confidence="0.698323">
730
</page>
<figure confidence="0.885411">
∑arg min
AkEC,SEAk
∑ O(z, s)
zEAk−S,3ES
Table 1: News sources of the 6 datasets
News Sources Number of Articles
CNN 2357
Fox News 1936
to minimize the information loss.
</figure>
<bodyText confidence="0.995510692307692">
where O(z, s) is the cost function which measures
the cost of representing sentence z with sentence s.
Generally, this is an NP-hard problem (Cheung et
al., 2009) but we can use POPSTAR, an implemen-
tation of an approximate solution proposed by Re-
sende and Werneck (Resende and Werneck, 2004).
To model different costs between global or local
aspects and determine the proportions of differen-
t aspects in the final storyline, we utilize a func-
tion ζ(s). When sentences z and s are local as-
pect sentences, ζ(s) = χ, or, ζ(s) = 1 − χ. For-
mally, we incorporate two kinds of decreasing/in-
creasing logistic functions, ℓ1(x) = 1/(1 + ex) and
</bodyText>
<equation confidence="0.9526925">
ℓ2(x)/= ex/(1 + ex), to/ define the cost function as
O(z, s) = ζ(s) · ℓ1(S(S)) · ℓ2(S(z)) · DKL(s, z)
</equation>
<bodyText confidence="0.9999545">
where 5(s) and 5(z) are the ranking scores of sen-
tences s and z among the aspect Ak with LexPageR-
ank algorithm. DKL(s, z) is used to measure the
similarity between sentence s and z with Kullback-
Leibler divergence here.
With this optimization method, we get the repre-
sentative sentences of each aspect for the given event
subject. Combining all the representative sentences
together based on the aspect sequence, we finally
generate the storyline.
</bodyText>
<sectionHeader confidence="0.998458" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.919654">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999993818181818">
To evaluate our framework for event storyline gen-
eration, we conduct our experiments on the dataset-
s amounting to 12418 articles for 6 event subjects
from 6 famous news websites, which provide date
edited by professional editors. Each article consists
of three parts, title, publish-time and news content.
Table 1 and Table 2 give the brief description of the
12418 articles. To generate reference summary, we
invite 12 undergraduate students with good English
ability to read the sentences, and for each event sub-
ject we ask two students to label human storylines.
</bodyText>
<subsectionHeader confidence="0.980277">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.994373">
We use the ROUGE1 (Lin and Hovy, 2003) (Recall
</bodyText>
<footnote confidence="0.877958">
Oriented Understudy for Gisting Evaluation) toolkit
1http://www.isi/edu/licensed-sw/see/rouge/
</footnote>
<table confidence="0.98246925">
New York Times 2178
ABC 2113
Washington Post 1405
Xinhua 2429
</table>
<tableCaption confidence="0.973794">
Table 2: Event subjects of the datasets
</tableCaption>
<table confidence="0.939272857142857">
Event Subjects Number of Articles
Connecticut school shooting 1792
The earthquake in Tokyo, Japan 2046
The U.S. presidential election 2573
Sandy hurricane attacked America 1827
American curiosity rover landed on Mars 1651
The 30th London Olympic Games 2529
</table>
<bodyText confidence="0.9997925">
to evaluate our framework, which has been widely
applied for summarization evaluation. It evaluates
the quality of a summary by counting the overlap-
ping units between the candidate summary and ref-
erence summaries. There are many kinds of ROUGE
metrics to measure the system-generated summa-
rization such as ROUGE-N, ROUGE-L, ROUGE-
W, and ROUGE-U, of which the most important one
is ROUGE-N with 3 sub-metrics: precision, recall,
and F-score.
</bodyText>
<equation confidence="0.997552333333333">
Countmat�h(N − gram)
ROUGE − N =
Count(N − gram)
</equation>
<bodyText confidence="0.999463333333333">
where RS represents the reference summaries. N-
gramERS in the metrics denotes the N-grams in ref-
erence summaries. Countmat h(N − gram) is the
maximum number of N-grams co-occurring in the
candidate summary and in the set of reference sum-
maries. Count(N − gram) is the number of N-
grams in the reference summaries.
The ROUGE toolkit can report separate scores for
1, 2, 3, and 4-gram. In the experimental results we
report three ROUGE F-measure scores: ROUGE-
1, ROUGE-2, ROUGE-W metrics. The higher the
ROUGE scores, the better the summary is.
</bodyText>
<subsectionHeader confidence="0.992184">
4.3 Algorithms for Comparison
</subsectionHeader>
<bodyText confidence="0.999036833333333">
Given a collection of news articles, we first decom-
pose them into sentences, and then assign each sen-
tence with a certain date, afterwards stop-words re-
moving and words stemming are performed. We
choose the following algorithms as baseline system-
s. Specifically, baseline 2 and 3 are summarization
</bodyText>
<figure confidence="0.837814">
∑ ∑
SERS N−gramES
∑ ∑
SERS N−gramES
</figure>
<page confidence="0.993271">
731
</page>
<bodyText confidence="0.999946571428571">
systems which are similar to our storyline genera-
tion system. Then we choose baseline 4, 5 to evalu-
ate the effectiveness of the proposed method. It must
be said that all the systems are required to generate
the same number of summary words with the hu-
man reference. We conduct the same preprocessing
for all algorithms for fairness.
</bodyText>
<listItem confidence="0.963541038461538">
• Random : The method selects sentences ran-
domly from the sentence collection.
• LexPageRank (LexRank): This method applies
the graph-based multi-document summarization al-
gorithm which first constructs a sentence connec-
tivity graph based on the cosine similarity and then
chooses top-ranked sentences with PageRank.
• Chieu : This method was proposed by Chieu
(Chieu and Lee, 2004), utilizing interest and bursti-
ness to rank sentences, and choosing the top-ranked
query related sentences to construct the timeline.
• LDA+LexPageRank (LDALR) : This method
first applies standard LDA to detect latent topics
from the collection and clusters sentences to mul-
tiple aspects, then utilizes PageRank to generate the
most representative component summaries from all
the aspects.
• MEA+LexPageRank (MEALR) : This method
applies the proposed mixture-event-aspect model to
cluster sentences into multiple aspects and then uti-
lizes PageRank to generate the most representative
component summaries from all the aspects.
• MEA+Optimization (MEAOp) : This method
extracts local/global aspects with the proposed
mixture-event-aspect model, and then utilizes the
optimization method to get the qualified summary.
</listItem>
<figureCaption confidence="0.614789">
Figure 3: Overall performance for comparison
</figureCaption>
<tableCaption confidence="0.997884">
Table 3: Results of different systems on 6 subjects
</tableCaption>
<table confidence="0.999872041666667">
Subject1 Subject2
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.234 0.037 0.188 0.242 0.039 0.192
LexRank 0.317 0.045 0.257 0.326 0.051 0.262
Chieu 0.332 0.056 0.277 0.351 0.055 0.283
LDALR 0.356 0.069 0.297 0.369 0.066 0.327
MEALR 0.369 0.072 0.313 0.381 0.076 0.348
MEAOp 0.381 0.075 0.331 0.398 0.081 0.364
Subject3 Subject4
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.258 0.042 0.191 0.234 0.036 0.179
LexRank 0.339 0.049 0.272 0.309 0.043 0.242
Chieu 0.364 0.059 0.296 0.329 0.053 0.264
LDALR 0.383 0.071 0.331 0.347 0.065 0.308
MEALR 0.396 0.076 0.3512 0.368 0.069 0.312
MEAOp 0.419 0.082 0.371 0.376 0.071 0.323
Subject5 Subject6
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.222 0.034 0.166 0.264 0.045 0.195
LexRank 0.309 0.042 0.237 0.349 0.054 0.276
Chieu 0.319 0.049 0.258 0.371 0.062 0.293
LDALR 0.342 0.062 0.291 0.392 0.073 0.325
MEALR 0.372 0.068 0.299 0.406 0.079 0.349
MEAOp 0.384 0.070 0.309 0.427 0.087 0.368
</table>
<subsectionHeader confidence="0.976534">
4.4 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.9999217">
We experiment with all the baselines and our frame-
work on the 6 datasets. We take the average F-
score performance in terms of 3 ROUGE-F scores:
ROUGE-1, ROUGE-2 and ROUGE-SU4. The over-
all results are shown in Figure 3 and details are listed
in Tables 3.
Figure 3 and Table 3 show the performance of
these systems on the same datasets. The local/global
optimization balance parameter χ = 0.5. From Fig-
ure 3 and Table 3 we have following observations:
</bodyText>
<listItem confidence="0.9974703125">
• Generally, the Random gets the worst perfor-
mance;
• The LexRank system outperforms Random al-
gorithm. This is due to the fact that LexRank ranks
all the sentences based on eigenvector centrality and
the global relationship between sentences, which
tends to select the most informative sentences as the
summary.
• The results of Chieu (Chieu and Lee, 2004) sys-
tem are better than those of LexRank. This may
be mainly for the reason that Chieu used the date
dimension to filter away uninteresting sentences by
paraphrasing and defined two different ranking mea-
sures: interest and burtiness, to select top-ranked in-
formative sentences.
• The LDALR system outperforms the Chieu sys-
</listItem>
<page confidence="0.994569">
732
</page>
<bodyText confidence="0.99827275">
tem. This may be for the fact that Chieu’s method is
actually based on flat clustering-based summariza-
tion, which is not as effective as LDA topic model
to extract latent sub-events.
</bodyText>
<figureCaption confidence="0.9596085">
Figure 4: Examine the performance of the balance pa-
rameter X
</figureCaption>
<listItem confidence="0.95352975">
• The MEALR system outperforms the LDAL-
R system. This may be mainly for the reason that
MEALR utilizes the mixture-event-aspect model to
detect the more salient sub-events based on the sub-
whole relationship, which seems to satisfy users’
bias to different sub-events.
• The MEAOp system which utilizes our method
outperforms all the baselines, indicating the effec-
</listItem>
<bodyText confidence="0.93373">
tiveness of detecting different types of sub-events
with mixture-event-aspect model and the necessity
to distinguish different proportions of the compo-
nent summaries based on local/global aspects.
</bodyText>
<subsectionHeader confidence="0.90218">
4.5 Parameter Tuning
</subsectionHeader>
<figureCaption confidence="0.738701333333333">
Figure 5: Aspect sequence of event “Connecticut school
shooting” (X-axis is the number of days after Dec. 14,
2012. Y-axis is bursty(Ak,t))
</figureCaption>
<bodyText confidence="0.999908909090909">
In this section, we compare the performance of
the parameters. The hyper-parameters such as α, Q,
&apos;y, A can be estimated using standard methods intro-
duced by Minka (Minka, 2000). So we mainly ex-
amine the local/global optimization balance parame-
ter X. We try to evaluate the influence of this param-
eter on the three kinds of ROUGE measure results
respectively. Figure 4 shows the performance of the
balance parameters X. It is obvious that when the
balance parameter X is set to 0.7 this method per-
forms best.
</bodyText>
<subsectionHeader confidence="0.969785">
4.6 Sample Output and Case Study
</subsectionHeader>
<figureCaption confidence="0.922496666666667">
Figure 6: Bursties of sub-event “Gun control debate” (X-
axis is the number of days after Dec. 14, 2012. Y-axis is
bursty(Ak,t))
</figureCaption>
<bodyText confidence="0.9996768">
We take the event “Connecticut school shooting”
as an example to show the usefulness of our method.
Figure 5 shows the aspect sequence based on the
bursty periods of all aspects. We select a sub-
event “Gun control debate” and Figure 6 shows the
bursties of this sub-event on the whole timeline. Ta-
ble 4 shows part of the storylines for the event “Con-
necticut school shooting” generated by human and
our method. Through observation, we find that the
peak of the event “Connecticut school shooting” is
around the date when it occurred, and the sub-event
“Gun control debate” has two bursty periods around
the two peaks. Compared with the human summary,
our framework can extract the important sub-events
contained in the collection, and satisfy users’ inter-
est on different sub-events based on the part-whole
relationship with the event subject.
From the sample output and the human storylines,
we also get some observations. (1) The component
summary of global aspect tend to share larger pro-
</bodyText>
<page confidence="0.999085">
733
</page>
<tableCaption confidence="0.999882">
Table 4: Selected part of storyline generated by MEAOp and human
</tableCaption>
<table confidence="0.998321033333333">
Storyline Generated by human:
December 14, 2012 Global Aspect
Shooting massacre occurred in a primary school in Connecticut town, Sandy hoot Newton.
20-year-old Adam lanza broke into the primary school after shotting his mother, and in 10 minutes shot more than 100 times, killing twenty children
and eight adult, including himself.
The youngest death was a children in preschool students.
December 15, 2012 Global Aspect
Photos of the teachers and students shoot in the Connecticut massacre are released as well as the shooter’s.
The shooter was very smart but lonely.
December 16-17, 2012 Local Aspect
President Obama arrived in the locality of school shooting, mourned for the victims and made a speech.
December 18-20, 2012 Local Aspect
American gun control bill was put on the agenda again.
Storyline Generated by MEAOp:
December 14, 2012 Global Aspect
Children and adults gunned down in Connecticut school massacre.
20 children, six adults and the shooter are dead after shooting at Sandy Hoot Elementary School in Newtown, Connecticut.
Three law enforcement officials say Adam Lanza, 20, was the shooter, and that he died apparently by his own hand.
Suspect’s mother, Nancy Lanza, found dead in suspects home in Newtown, law enforcement source says.
Ryan Lanza, older brother of Adam Lanza, questioned by police but not labeled a suspect.
December 15-16, 2012 Global Aspect
Victims’ names released Saturday; all of the slain children were either 6 or 7 years old.
Understanding school lockdowns in regards to Connecticut shooting.
Connecticut gunman recalled as intelligent but remote.
December 17, 2012 Local Aspect
President obama leads interfaith preyer vigil in newtown connecticut.
A tearful Obama says “we’ve endured too many of these tragedies”.
December 18-20, 2012 Local Aspect
Moderate dems join gun control debate call for commission on us violence gains.
Gun debate gains traction as some lawmakers say its time to act.
</table>
<bodyText confidence="0.999770416666667">
portion in the final storyline. This is mainly for
the reason that when researching for an event sub-
ject, users bias more to the information about the
global-sub-events that have closely connection and
coincident properties with the major event based on
the part-whole relationship. So it is really neces-
sary to distinguish different sub-events with distinc-
tive properties. (2) Our system performs better for
the persistent event, such as “The U.S. presidential
election”. This may be for the fact that these events
are usually long running and have more global-sub-
events than local-sub-events.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972368421053">
In this work, we study the task of event storyline
generation and present a novel method. We inno-
vatively introduce the properties of different sub-
events based on word co-occurrences to determine
the part-whole relationship with the major event and
develop a mixture-event-aspect (MEA) model to for-
malize different types of sub-events into local/global
aspects. Based on these local/global aspects, we uti-
lize an optimization method to get the optimal com-
ponent summaries along the aspect sequence. We
conduct experiments with our method and various
baselines on real web datasets. Through our exper-
iments we notice that our method generates over-
all better storyline than other baselines. This indi-
cates the effectiveness to detect different types of
sub-events with the proposed mixture-event-aspect
model and the necessity to distinguish different pro-
portions of the component summaries based on lo-
cal/global aspects.
</bodyText>
<sectionHeader confidence="0.99095" genericHeader="conclusions">
Acknowledgments.
</sectionHeader>
<bodyText confidence="0.99913925">
We thank the anonymous reviewers for their
valuable and constructive comments. This work
is financially supported by NSFC under the grant
NO.61272340 and 60933004.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9988522">
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Jackie Chi Kit Cheung, Giuseppe Carenini, and Ray-
mond T Ng. 2009. Optimization-based content se-
</reference>
<page confidence="0.982814">
734
</page>
<reference confidence="0.999666074074074">
lection for opinion summarization. In Proceedings of
the 2009 Workshop on Language Generation and Sum-
marisation, pages 7–14. ACL.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 425–432. ACM.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of the Fifteenth conference
on Uncertainty in artificial intelligence, pages 289–
296. Morgan Kaufmann Publishers Inc.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 297–304. ACM.
Theodoros Lappas, Benjamin Arai, Manolis Platakis,
Dimitrios Kotsakos, and Dimitrios Gunopulos. 2009.
On burstiness-aware search for document sequences.
In Proceedings of the 15th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 477–486. ACM.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th annual meeting of the Association for Com-
putational Linguistics, pages 640–649. ACL.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71–78. ACL.
Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang
Chen, and Tao Li. 2012. Generating event storylines
from microblogs. In Proceedings of the 21st ACM in-
ternational conference on Information and knowledge
management, pages 175–184. ACM.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3-4):347–
368.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discover-
ing evolutionary theme patterns from text: an explo-
ration of temporal text mining. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 198–207.
ACM.
Q. Mei, J. Guo, and D. Radev. 2010. Divrank: the inter-
play of prestige and diversity in information networks.
In Proceedings of the 16th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 1009–1018. ACM.
Thomas Minka. 2000. Estimating a dirichlet distribu-
tion.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Ter-
ry Winograd. 1999. The pagerank citation ranking:
bringing order to the web.
Yulong Pei, Wenpeng Yin, et al. 2012. Generic multi-
document summarization using topic-oriented infor-
mation. In PRICAI 2012: Trends in Artificial Intel-
ligence, pages 435–446. Springer.
D.R. Radev, H. Jing, M. Sty´s, and D. Tam. 2004.
Centroid-based summarization of multiple documents.
Information Processing &amp; Management, 40(6):919–
938.
Mauricio GC Resende and Renato F Werneck. 2004. A
hybrid heuristic for the p-median problem. Journal of
heuristics, 10(1):59–88.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th international conference on World
Wide Web, pages 111–120. ACM.
X. Wan. 2008. Document-based hits model for multi-
document summarization. PRICAI 2008: Trends in
Artificial Intelligence, pages 454–465.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 784–
793. ACM.
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, pages 192–201. ACM.
Dingding Wang, Tao Li, and Mitsunori Ogihara. 2012.
Generating pictorial storylines via minimum-weight
connected dominating set approximation in multi-view
graphs. Proceddings of AAAI 2012.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, X-
iaoming Li, and Yan Zhang. 2011a. Timeline gener-
ation through evolutionary trans-temporal summariza-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 433–
443. ACL.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 745–754. ACM.
</reference>
<page confidence="0.998578">
735
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556833">
<title confidence="0.932024666666667">Optimized Event Storyline Generation based on Mixture-Event-Aspect Model Lifu</title>
<author confidence="0.913195">Shenzhen Key Lab for Cloud</author>
<affiliation confidence="0.940625">Technology and Peking University Shenzhen Graduate</affiliation>
<address confidence="0.978619">Shenzhen, Guangdong 518055,</address>
<email confidence="0.999796">warrior.fu@gmail.com</email>
<author confidence="0.973938">Shenzhen Key Lab for Cloud</author>
<affiliation confidence="0.959075">Technology and Peking University Shenzhen Graduate</affiliation>
<address confidence="0.994486">Shenzhen, Guangdong 518055,</address>
<email confidence="0.9417">hle@net.pku.edu.cn</email>
<abstract confidence="0.998350782608695">Recently, much research focuses on event storyline generation, which aims to produce a concise, global and temporal event summary from a collection of articles. Generally, each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events. However, different sub-events have different part-whole relationship with the major event, which is important to correspond to users’ interests but seldom considered in previous work. To distinguish different types of sub-events, we propose a mixture-event-aspect model which models different sub-events into local and global aspects. Combining these local/global aspects with summarization requirements together, we utilize an optimization method to generate the component summaries along the timeline. We develop experimental systems on 6 distinctively different datasets. Evaluation and comparison results indicate the effectiveness of our proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="11325" citStr="Blei et al., 2003" startWordPosition="1763" endWordPosition="1766">posed a entity-aspect model based on word co-occurrences within single sentences. Inspired by these ideas, we rely on word co-occurrences within local period context to detect mixed local and global aspects implicated in the whole collection. We name this model as “MixtureEvent-Aspect (MEA)” model which can simultaneously detect local/global aspects and cluster sentences and words into different aspects. 3.1.1 Model Description Our mixture-event-aspect (MEA) model can be extended from both the Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We model two distinct types of aspects: global aspects and local aspects, based on their relationship with the major event. The distribution of global aspects is fixed for the collection while the distribution of local aspects is fixed to a local period of sub-collections. That means a sentence is sampled either from the mixture of the global aspects or from the local aspects specific for the local context. Here we take the event “Connecticut school shooting” as an example. For the sentence “On Sunday, President Obama came to Connecticut to give a lecture, expressing his sorrow for ... and c</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>Optimization-based content selection for opinion summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation,</booktitle>
<pages>7--14</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="20511" citStr="Cheung et al., 2009" startWordPosition="3439" endWordPosition="3442">set of sentences S from the aspect Ak nch∗+λ nc(.)+S∗pλ � nc,vh+αmix loc loc nc,vh (.) + ∑ αmix r′ p(yc,s,n = 1|z, y ) ∝ C(1) + γ C(w�,s,n) + 0 C() + 2 · γ CB(�) + L · 0 · C(2) + γ Ca (w�,s,n) + 0 p(yc,s,n = 2|z, y′) ∝ · C� Ca πy = αmix r ∑ r∈(gl,loc) c,v η +αmix η ∑ p(Ak|t′)p(t′) t′ 730 ∑arg min AkEC,SEAk ∑ O(z, s) zEAk−S,3ES Table 1: News sources of the 6 datasets News Sources Number of Articles CNN 2357 Fox News 1936 to minimize the information loss. where O(z, s) is the cost function which measures the cost of representing sentence z with sentence s. Generally, this is an NP-hard problem (Cheung et al., 2009) but we can use POPSTAR, an implementation of an approximate solution proposed by Resende and Werneck (Resende and Werneck, 2004). To model different costs between global or local aspects and determine the proportions of different aspects in the final storyline, we utilize a function ζ(s). When sentences z and s are local aspect sentences, ζ(s) = χ, or, ζ(s) = 1 − χ. Formally, we incorporate two kinds of decreasing/increasing logistic functions, ℓ1(x) = 1/(1 + ex) and ℓ2(x)/= ex/(1 + ex), to/ define the cost function as O(z, s) = ζ(s) · ℓ1(S(S)) · ℓ2(S(z)) · DKL(s, z) where 5(s) and 5(z) are t</context>
</contexts>
<marker>Cheung, Carenini, Ng, 2009</marker>
<rawString>Jackie Chi Kit Cheung, Giuseppe Carenini, and Raymond T Ng. 2009. Optimization-based content selection for opinion summarization. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 7–14. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Query based event extraction along a timeline.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>425--432</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8731" citStr="Chieu and Lee, 2004" startWordPosition="1361" endWordPosition="1364"> that incorporated simple semantics into TDT by splitting the term space into groups of terms. Krause et al. Wang et al. (Wang et al., 2007) and Wang et al. (Wang et al., 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive data. Chieu et al. (Chieu and Lee, 2004) built a system that extracted events relevant to a query from a collection of related documents and placed such events along a timeline. Yan et al. (Yan et al., 2011b) designed an evolutionary timeline summarization approach to construct a timeline of a topic by optimizing the relevance, coverage, coherence, and diversity. Lin et al. (Lin et al., 2012) explored the problem of generating storylines from microblogs for user input queries. They first proposed a language model with dynamic pseudo relevance feedback to obtain relevant tweets and then generated storylines via graph optimization. 3 </context>
<context position="24776" citStr="Chieu and Lee, 2004" startWordPosition="4140" endWordPosition="4143">evaluate the effectiveness of the proposed method. It must be said that all the systems are required to generate the same number of summary words with the human reference. We conduct the same preprocessing for all algorithms for fairness. • Random : The method selects sentences randomly from the sentence collection. • LexPageRank (LexRank): This method applies the graph-based multi-document summarization algorithm which first constructs a sentence connectivity graph based on the cosine similarity and then chooses top-ranked sentences with PageRank. • Chieu : This method was proposed by Chieu (Chieu and Lee, 2004), utilizing interest and burstiness to rank sentences, and choosing the top-ranked query related sentences to construct the timeline. • LDA+LexPageRank (LDALR) : This method first applies standard LDA to detect latent topics from the collection and clusters sentences to multiple aspects, then utilizes PageRank to generate the most representative component summaries from all the aspects. • MEA+LexPageRank (MEALR) : This method applies the proposed mixture-event-aspect model to cluster sentences into multiple aspects and then utilizes PageRank to generate the most representative component summar</context>
<context position="27444" citStr="Chieu and Lee, 2004" startWordPosition="4567" endWordPosition="4570">l results are shown in Figure 3 and details are listed in Tables 3. Figure 3 and Table 3 show the performance of these systems on the same datasets. The local/global optimization balance parameter χ = 0.5. From Figure 3 and Table 3 we have following observations: • Generally, the Random gets the worst performance; • The LexRank system outperforms Random algorithm. This is due to the fact that LexRank ranks all the sentences based on eigenvector centrality and the global relationship between sentences, which tends to select the most informative sentences as the summary. • The results of Chieu (Chieu and Lee, 2004) system are better than those of LexRank. This may be mainly for the reason that Chieu used the date dimension to filter away uninteresting sentences by paraphrasing and defined two different ranking measures: interest and burtiness, to select top-ranked informative sentences. • The LDALR system outperforms the Chieu sys732 tem. This may be for the fact that Chieu’s method is actually based on flat clustering-based summarization, which is not as effective as LDA topic model to extract latent sub-events. Figure 4: Examine the performance of the balance parameter X • The MEALR system outperforms</context>
</contexts>
<marker>Chieu, Lee, 2004</marker>
<rawString>Hai Leong Chieu and Yoong Keok Lee. 2004. Query based event extraction along a timeline. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 425–432. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<contexts>
<context position="7247" citStr="Erkan and Radev, 2004" startWordPosition="1113" endWordPosition="1116"> Related Work Our work is related to several lines of research in the literature including multi-document summarization (MDS), topic detection and tracking (TDT), temporal text mining (TXM) and temporal news summarization (TNS). Multi-document summarization is a process to generate a summary by reducing documents in size while retaining the main information. To date, different features and ranking strategies have been studied. Radev et al. (Radev et al., 2004) proposed to implement MEAD as a centroid-based summarizer by combining several predefined features to score the sentence. LexPageRank (Erkan and Radev, 2004) is the representative work which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 20</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In Proceedings of EMNLP, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,</booktitle>
<pages>289--296</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="11267" citStr="Hofmann, 1999" startWordPosition="1755" endWordPosition="1756">n short paragraphs and Li et al. (Li et al., 2010) proposed a entity-aspect model based on word co-occurrences within single sentences. Inspired by these ideas, we rely on word co-occurrences within local period context to detect mixed local and global aspects implicated in the whole collection. We name this model as “MixtureEvent-Aspect (MEA)” model which can simultaneously detect local/global aspects and cluster sentences and words into different aspects. 3.1.1 Model Description Our mixture-event-aspect (MEA) model can be extended from both the Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We model two distinct types of aspects: global aspects and local aspects, based on their relationship with the major event. The distribution of global aspects is fixed for the collection while the distribution of local aspects is fixed to a local period of sub-collections. That means a sentence is sampled either from the mixture of the global aspects or from the local aspects specific for the local context. Here we take the event “Connecticut school shooting” as an example. For the sentence “On Sunday, President Obama came to Connecti</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289– 296. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giridhar Kumaran</author>
<author>James Allan</author>
</authors>
<title>Text classification and named entities for new event detection.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>297--304</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7895" citStr="Kumaran and Allan, 2004" startWordPosition="1214" endWordPosition="1218">ork which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) proposed a method that incorporated simple semantics into TDT by splitting the term space into groups of terms. Krause et al. Wang et al. (Wang et al., 2007) and Wang et al. (Wang et al., 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along th</context>
</contexts>
<marker>Kumaran, Allan, 2004</marker>
<rawString>Giridhar Kumaran and James Allan. 2004. Text classification and named entities for new event detection. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 297–304. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Lappas</author>
<author>Benjamin Arai</author>
<author>Manolis Platakis</author>
<author>Dimitrios Kotsakos</author>
<author>Dimitrios Gunopulos</author>
</authors>
<title>On burstiness-aware search for document sequences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>477--486</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17929" citStr="Lappas et al., 2009" startWordPosition="2971" endWordPosition="2974"> aspect words. Cπ (·) is the total number of words. CB (·) is the total number of background words and Ca (·) is the number of words assigned to aspect a. By using the samples from Gibbs sampling, we can effectively make the following estimations: B — CBw +β a — Caw+β nch∗+λ φw — CB+L· β, φw — Caw +L· β , Oh∗ = n( )+S∗p λ Cπy +γv = n C·π+2·γ, ρ77 nc,v+ c,v loc B gl = ngl,a+αglBloc = nloc,a+α nyl+Aglαgl, a nloc+Alocαloc The hyper-parameters like α, β, γ, λ can be estimated using standard methods introduced in (Minka, 2000). 3.2 Bursty Period Detection We borrow the definition of “bursty” from (Lappas et al., 2009) to measure the popularity of the event on a certain date. Intuitively, each aspect have different bursties on different dates. In this section, we try to obtain the temporal aspect sequences of an event based on the bursty periods of all the aspects. During its bursty period, one aspect should (1)be more popular than other aspects (2) be continuously more popular than other time. Following these intuitions, we design a method to measure the bursty of each aspect and get the bursty period. Let Ak be the kth aspect obtained from the mixture-event-aspect model, we estimate the bursty of Ak at a </context>
</contexts>
<marker>Lappas, Arai, Platakis, Kotsakos, Gunopulos, 2009</marker>
<rawString>Theodoros Lappas, Benjamin Arai, Manolis Platakis, Dimitrios Kotsakos, and Dimitrios Gunopulos. 2009. On burstiness-aware search for document sequences. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 477–486. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Jing Jiang</author>
<author>Yinglin Wang</author>
</authors>
<title>Generating templates of entity summaries with an entityaspect model and pattern mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>640--649</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10703" citStr="Li et al., 2010" startWordPosition="1670" endWordPosition="1673">ts and the major event, we model different subevents into two types: local-sub-event and globalsub-event, and introduce local/global aspects correspondingly. Generally, local aspects which correspond to local-sub-events have distinctive words distribution from each other and sustain for a local context while the global aspects corresponding to global-sub-events have coincident words distribution with the major event. To capture specific words, Titov and McDonald (Titov and McDonald, 2008) proposed a multi-grain topic model, relying on word co-occurrences within short paragraphs and Li et al. (Li et al., 2010) proposed a entity-aspect model based on word co-occurrences within single sentences. Inspired by these ideas, we rely on word co-occurrences within local period context to detect mixed local and global aspects implicated in the whole collection. We name this model as “MixtureEvent-Aspect (MEA)” model which can simultaneously detect local/global aspects and cluster sentences and words into different aspects. 3.1.1 Model Description Our mixture-event-aspect (MEA) model can be extended from both the Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LD</context>
</contexts>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>Peng Li, Jing Jiang, and Yinglin Wang. 2010. Generating templates of entity summaries with an entityaspect model and pattern mining. In Proceedings of the 48th annual meeting of the Association for Computational Linguistics, pages 640–649. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>71--78</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="22202" citStr="Lin and Hovy, 2003" startWordPosition="3728" endWordPosition="3731">ate our framework for event storyline generation, we conduct our experiments on the datasets amounting to 12418 articles for 6 event subjects from 6 famous news websites, which provide date edited by professional editors. Each article consists of three parts, title, publish-time and news content. Table 1 and Table 2 give the brief description of the 12418 articles. To generate reference summary, we invite 12 undergraduate students with good English ability to read the sentences, and for each event subject we ask two students to label human storylines. 4.2 Evaluation Metrics We use the ROUGE1 (Lin and Hovy, 2003) (Recall Oriented Understudy for Gisting Evaluation) toolkit 1http://www.isi/edu/licensed-sw/see/rouge/ New York Times 2178 ABC 2113 Washington Post 1405 Xinhua 2429 Table 2: Event subjects of the datasets Event Subjects Number of Articles Connecticut school shooting 1792 The earthquake in Tokyo, Japan 2046 The U.S. presidential election 2573 Sandy hurricane attacked America 1827 American curiosity rover landed on Mars 1651 The 30th London Olympic Games 2529 to evaluate our framework, which has been widely applied for summarization evaluation. It evaluates the quality of a summary by counting </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 71–78. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Lin</author>
<author>Chun Lin</author>
<author>Jingxuan Li</author>
<author>Dingding Wang</author>
<author>Yang Chen</author>
<author>Tao Li</author>
</authors>
<title>Generating event storylines from microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>175--184</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9086" citStr="Lin et al., 2012" startWordPosition="1421" endWordPosition="1424">om one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive data. Chieu et al. (Chieu and Lee, 2004) built a system that extracted events relevant to a query from a collection of related documents and placed such events along a timeline. Yan et al. (Yan et al., 2011b) designed an evolutionary timeline summarization approach to construct a timeline of a topic by optimizing the relevance, coverage, coherence, and diversity. Lin et al. (Lin et al., 2012) explored the problem of generating storylines from microblogs for user input queries. They first proposed a language model with dynamic pseudo relevance feedback to obtain relevant tweets and then generated storylines via graph optimization. 3 Approach Details In this section, we first propose a mixture-eventaspect model to detect local/global sub-events based on part-whole relationship with the major event and then present a new method to estimate the bursty of each aspect on a certain date. Afterwards we utilize an optimization method based on local/global aspects to extract the qualified s</context>
</contexts>
<marker>Lin, Lin, Li, Wang, Chen, Li, 2012</marker>
<rawString>Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang Chen, and Tao Li. 2012. Generating event storylines from microblogs. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 175–184. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juha Makkonen</author>
<author>Helena Ahonen-Myka</author>
<author>Marko Salmenkivi</author>
</authors>
<title>Simple semantics in topic detection and tracking. Information Retrieval,</title>
<date>2004</date>
<pages>7--3</pages>
<contexts>
<context position="8093" citStr="Makkonen et al., 2004" startWordPosition="1252" endWordPosition="1255">rent relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) proposed a method that incorporated simple semantics into TDT by splitting the term space into groups of terms. Krause et al. Wang et al. (Wang et al., 2007) and Wang et al. (Wang et al., 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive da</context>
</contexts>
<marker>Makkonen, Ahonen-Myka, Salmenkivi, 2004</marker>
<rawString>Juha Makkonen, Helena Ahonen-Myka, and Marko Salmenkivi. 2004. Simple semantics in topic detection and tracking. Information Retrieval, 7(3-4):347– 368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Discovering evolutionary theme patterns from text: an exploration of temporal text mining.</title>
<date>2005</date>
<booktitle>In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>198--207</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4158" citStr="Mei and Zhai, 2005" startWordPosition="627" endWordPosition="630">events which have global 726 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726–735, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics consistency with the given event subject, so the subevents should not be considered equally when generating the component summaries. It is also a great challenge to generate a qualified summary based on the different types of sub-events. The component summaries should be correlative across different dates based on the global collection (Yan et al., 2011a). Mei and Zhai (Mei and Zhai, 2005) proposed to use theme or topic to model different sub-events, which is to some extent similar to our method. To be different, in this paper we introduce “local/global” property to distinguish different part-whole relationship between the sub-events and the major event, which have not been considered before in storyline generation or summarization, to improve the quality of the storyline. The local/global property corresponds to the elements of an event, such as the place, characters and other body information. These information reflects the relationship between the sub-events and the major ev</context>
</contexts>
<marker>Mei, Zhai, 2005</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering evolutionary theme patterns from text: an exploration of temporal text mining. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 198–207. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>J Guo</author>
<author>D Radev</author>
</authors>
<title>Divrank: the interplay of prestige and diversity in information networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1009--1018</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7593" citStr="Mei et al., 2010" startWordPosition="1165" endWordPosition="1168">ormation. To date, different features and ranking strategies have been studied. Radev et al. (Radev et al., 2004) proposed to implement MEAD as a centroid-based summarizer by combining several predefined features to score the sentence. LexPageRank (Erkan and Radev, 2004) is the representative work which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) proposed a method that incorporated simple semantics into TDT by splitting the term space into grou</context>
</contexts>
<marker>Mei, Guo, Radev, 2010</marker>
<rawString>Q. Mei, J. Guo, and D. Radev. 2010. Divrank: the interplay of prestige and diversity in information networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1009–1018. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
</authors>
<title>Estimating a dirichlet distribution.</title>
<date>2000</date>
<contexts>
<context position="17836" citStr="Minka, 2000" startWordPosition="2958" endWordPosition="2959"> 0 where Cπ(1) and Cπ(2) are the numbers of words assigned to be background words and aspect words. Cπ (·) is the total number of words. CB (·) is the total number of background words and Ca (·) is the number of words assigned to aspect a. By using the samples from Gibbs sampling, we can effectively make the following estimations: B — CBw +β a — Caw+β nch∗+λ φw — CB+L· β, φw — Caw +L· β , Oh∗ = n( )+S∗p λ Cπy +γv = n C·π+2·γ, ρ77 nc,v+ c,v loc B gl = ngl,a+αglBloc = nloc,a+α nyl+Aglαgl, a nloc+Alocαloc The hyper-parameters like α, β, γ, λ can be estimated using standard methods introduced in (Minka, 2000). 3.2 Bursty Period Detection We borrow the definition of “bursty” from (Lappas et al., 2009) to measure the popularity of the event on a certain date. Intuitively, each aspect have different bursties on different dates. In this section, we try to obtain the temporal aspect sequences of an event based on the bursty periods of all the aspects. During its bursty period, one aspect should (1)be more popular than other aspects (2) be continuously more popular than other time. Following these intuitions, we design a method to measure the bursty of each aspect and get the bursty period. Let Ak be th</context>
<context position="28913" citStr="Minka, 2000" startWordPosition="4803" endWordPosition="4804">which utilizes our method outperforms all the baselines, indicating the effectiveness of detecting different types of sub-events with mixture-event-aspect model and the necessity to distinguish different proportions of the component summaries based on local/global aspects. 4.5 Parameter Tuning Figure 5: Aspect sequence of event “Connecticut school shooting” (X-axis is the number of days after Dec. 14, 2012. Y-axis is bursty(Ak,t)) In this section, we compare the performance of the parameters. The hyper-parameters such as α, Q, &apos;y, A can be estimated using standard methods introduced by Minka (Minka, 2000). So we mainly examine the local/global optimization balance parameter X. We try to evaluate the influence of this parameter on the three kinds of ROUGE measure results respectively. Figure 4 shows the performance of the balance parameters X. It is obvious that when the balance parameter X is set to 0.7 this method performs best. 4.6 Sample Output and Case Study Figure 6: Bursties of sub-event “Gun control debate” (Xaxis is the number of days after Dec. 14, 2012. Y-axis is bursty(Ak,t)) We take the event “Connecticut school shooting” as an example to show the usefulness of our method. Figure 5</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas Minka. 2000. Estimating a dirichlet distribution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: bringing order to the web.</title>
<date>1999</date>
<contexts>
<context position="7321" citStr="Page et al., 1999" startWordPosition="1126" endWordPosition="1129">re including multi-document summarization (MDS), topic detection and tracking (TDT), temporal text mining (TXM) and temporal news summarization (TNS). Multi-document summarization is a process to generate a summary by reducing documents in size while retaining the main information. To date, different features and ranking strategies have been studied. Radev et al. (Radev et al., 2004) proposed to implement MEAD as a centroid-based summarizer by combining several predefined features to score the sentence. LexPageRank (Erkan and Radev, 2004) is the representative work which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: bringing order to the web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulong Pei</author>
<author>Wenpeng Yin</author>
</authors>
<title>Generic multidocument summarization using topic-oriented information.</title>
<date>2012</date>
<booktitle>In PRICAI 2012: Trends in Artificial Intelligence,</booktitle>
<pages>435--446</pages>
<publisher>Springer.</publisher>
<marker>Pei, Yin, 2012</marker>
<rawString>Yulong Pei, Wenpeng Yin, et al. 2012. Generic multidocument summarization using topic-oriented information. In PRICAI 2012: Trends in Artificial Intelligence, pages 435–446. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Sty´s</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing &amp; Management,</journal>
<volume>40</volume>
<issue>6</issue>
<pages>938</pages>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>D.R. Radev, H. Jing, M. Sty´s, and D. Tam. 2004. Centroid-based summarization of multiple documents. Information Processing &amp; Management, 40(6):919– 938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio GC Resende</author>
<author>Renato F Werneck</author>
</authors>
<title>A hybrid heuristic for the p-median problem.</title>
<date>2004</date>
<journal>Journal of heuristics,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="20640" citStr="Resende and Werneck, 2004" startWordPosition="3461" endWordPosition="3464">1) + γ C(w�,s,n) + 0 C() + 2 · γ CB(�) + L · 0 · C(2) + γ Ca (w�,s,n) + 0 p(yc,s,n = 2|z, y′) ∝ · C� Ca πy = αmix r ∑ r∈(gl,loc) c,v η +αmix η ∑ p(Ak|t′)p(t′) t′ 730 ∑arg min AkEC,SEAk ∑ O(z, s) zEAk−S,3ES Table 1: News sources of the 6 datasets News Sources Number of Articles CNN 2357 Fox News 1936 to minimize the information loss. where O(z, s) is the cost function which measures the cost of representing sentence z with sentence s. Generally, this is an NP-hard problem (Cheung et al., 2009) but we can use POPSTAR, an implementation of an approximate solution proposed by Resende and Werneck (Resende and Werneck, 2004). To model different costs between global or local aspects and determine the proportions of different aspects in the final storyline, we utilize a function ζ(s). When sentences z and s are local aspect sentences, ζ(s) = χ, or, ζ(s) = 1 − χ. Formally, we incorporate two kinds of decreasing/increasing logistic functions, ℓ1(x) = 1/(1 + ex) and ℓ2(x)/= ex/(1 + ex), to/ define the cost function as O(z, s) = ζ(s) · ℓ1(S(S)) · ℓ2(S(z)) · DKL(s, z) where 5(s) and 5(z) are the ranking scores of sentences s and z among the aspect Ak with LexPageRank algorithm. DKL(s, z) is used to measure the similarit</context>
</contexts>
<marker>Resende, Werneck, 2004</marker>
<rawString>Mauricio GC Resende and Renato F Werneck. 2004. A hybrid heuristic for the p-median problem. Journal of heuristics, 10(1):59–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10580" citStr="Titov and McDonald, 2008" startWordPosition="1650" endWordPosition="1653">are assigned with the same date are grouped into the same sub-collection. Considering the consistency of content between the subevents and the major event, we model different subevents into two types: local-sub-event and globalsub-event, and introduce local/global aspects correspondingly. Generally, local aspects which correspond to local-sub-events have distinctive words distribution from each other and sustain for a local context while the global aspects corresponding to global-sub-events have coincident words distribution with the major event. To capture specific words, Titov and McDonald (Titov and McDonald, 2008) proposed a multi-grain topic model, relying on word co-occurrences within short paragraphs and Li et al. (Li et al., 2010) proposed a entity-aspect model based on word co-occurrences within single sentences. Inspired by these ideas, we rely on word co-occurrences within local period context to detect mixed local and global aspects implicated in the whole collection. We name this model as “MixtureEvent-Aspect (MEA)” model which can simultaneously detect local/global aspects and cluster sentences and words into different aspects. 3.1.1 Model Description Our mixture-event-aspect (MEA) model can </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, pages 111–120. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Document-based hits model for multidocument summarization. PRICAI 2008: Trends in Artificial Intelligence,</title>
<date>2008</date>
<pages>454--465</pages>
<contexts>
<context position="7500" citStr="Wan, 2008" startWordPosition="1151" endWordPosition="1152">ocess to generate a summary by reducing documents in size while retaining the main information. To date, different features and ranking strategies have been studied. Radev et al. (Radev et al., 2004) proposed to implement MEAD as a centroid-based summarizer by combining several predefined features to score the sentence. LexPageRank (Erkan and Radev, 2004) is the representative work which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) propos</context>
</contexts>
<marker>Wan, 2008</marker>
<rawString>X. Wan. 2008. Document-based hits model for multidocument summarization. PRICAI 2008: Trends in Artificial Intelligence, pages 454–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanhui Wang</author>
<author>ChengXiang Zhai</author>
<author>Xiao Hu</author>
<author>Richard Sproat</author>
</authors>
<title>Mining correlated bursty topic patterns from coordinated text streams.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>784--793</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8251" citStr="Wang et al., 2007" startWordPosition="1281" endWordPosition="1284">opic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) proposed a method that incorporated simple semantics into TDT by splitting the term space into groups of terms. Krause et al. Wang et al. (Wang et al., 2007) and Wang et al. (Wang et al., 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive data. Chieu et al. (Chieu and Lee, 2004) built a system that extracted events relevant to a query from a collection of related documents and placed such events </context>
</contexts>
<marker>Wang, Zhai, Hu, Sproat, 2007</marker>
<rawString>Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard Sproat. 2007. Mining correlated bursty topic patterns from coordinated text streams. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 784– 793. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Wang</author>
<author>Kai Zhang</author>
<author>Xiaoming Jin</author>
<author>Dou Shen</author>
</authors>
<title>Mining common topics from multiple asynchronous text streams.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>192--201</pages>
<publisher>ACM.</publisher>
<marker>Wang, Zhang, Jin, Shen, 2009</marker>
<rawString>Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen. 2009. Mining common topics from multiple asynchronous text streams. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 192–201. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Mitsunori Ogihara</author>
</authors>
<title>Generating pictorial storylines via minimum-weight connected dominating set approximation in multi-view graphs.</title>
<date>2012</date>
<booktitle>Proceddings of AAAI</booktitle>
<contexts>
<context position="7850" citStr="Wang et al., 2012" startWordPosition="1207" endWordPosition="1210">nd Radev, 2004) is the representative work which is based on PageRank (Page et al., 1999) algorithm. Some methods have been proposed to extend the conventional graph-based models recently including multi-layer graph incorporated with different relationship (Wan, 2008), ToPageRank based on the topic information (Pei et al., 2012) and DivRank (Mei et al., 2010) balancing the prestige and diversity. Topic detection and tracking (TDT) aims to group news articles based on the topics discussed in them, detect some novel and previously unreported events and track future events related to the topics (Wang et al., 2012). Kumaran and Allan (Kumaran and Allan, 2004) showed how performance on new event detection could be improved by the use of text classification techniques as well as by using named entities in a new way. Makkonen et al. (Makkonen et al., 2004) proposed a method that incorporated simple semantics into TDT by splitting the term space into groups of terms. Krause et al. Wang et al. (Wang et al., 2007) and Wang et al. (Wang et al., 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics</context>
</contexts>
<marker>Wang, Li, Ogihara, 2012</marker>
<rawString>Dingding Wang, Tao Li, and Mitsunori Ogihara. 2012. Generating pictorial storylines via minimum-weight connected dominating set approximation in multi-view graphs. Proceddings of AAAI 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Congrui Huang</author>
<author>Xiaojun Wan</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Timeline generation through evolutionary trans-temporal summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>433--443</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4121" citStr="Yan et al., 2011" startWordPosition="620" endWordPosition="623">er, users tend to bias to the sub-events which have global 726 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726–735, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics consistency with the given event subject, so the subevents should not be considered equally when generating the component summaries. It is also a great challenge to generate a qualified summary based on the different types of sub-events. The component summaries should be correlative across different dates based on the global collection (Yan et al., 2011a). Mei and Zhai (Mei and Zhai, 2005) proposed to use theme or topic to model different sub-events, which is to some extent similar to our method. To be different, in this paper we introduce “local/global” property to distinguish different part-whole relationship between the sub-events and the major event, which have not been considered before in storyline generation or summarization, to improve the quality of the storyline. The local/global property corresponds to the elements of an event, such as the place, characters and other body information. These information reflects the relationship be</context>
<context position="8897" citStr="Yan et al., 2011" startWordPosition="1392" endWordPosition="1395"> 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive data. Chieu et al. (Chieu and Lee, 2004) built a system that extracted events relevant to a query from a collection of related documents and placed such events along a timeline. Yan et al. (Yan et al., 2011b) designed an evolutionary timeline summarization approach to construct a timeline of a topic by optimizing the relevance, coverage, coherence, and diversity. Lin et al. (Lin et al., 2012) explored the problem of generating storylines from microblogs for user input queries. They first proposed a language model with dynamic pseudo relevance feedback to obtain relevant tweets and then generated storylines via graph optimization. 3 Approach Details In this section, we first propose a mixture-eventaspect model to detect local/global sub-events based on part-whole relationship with the major event</context>
</contexts>
<marker>Yan, Kong, Huang, Wan, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011a. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 433– 443. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>745--754</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4121" citStr="Yan et al., 2011" startWordPosition="620" endWordPosition="623">er, users tend to bias to the sub-events which have global 726 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726–735, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics consistency with the given event subject, so the subevents should not be considered equally when generating the component summaries. It is also a great challenge to generate a qualified summary based on the different types of sub-events. The component summaries should be correlative across different dates based on the global collection (Yan et al., 2011a). Mei and Zhai (Mei and Zhai, 2005) proposed to use theme or topic to model different sub-events, which is to some extent similar to our method. To be different, in this paper we introduce “local/global” property to distinguish different part-whole relationship between the sub-events and the major event, which have not been considered before in storyline generation or summarization, to improve the quality of the storyline. The local/global property corresponds to the elements of an event, such as the place, characters and other body information. These information reflects the relationship be</context>
<context position="8897" citStr="Yan et al., 2011" startWordPosition="1392" endWordPosition="1395"> 727 2009) worked on topic tracking from multiple news streams. Their methods extracted meaningful topics from multi-source news collections and tracked different topics as they evolved from one to another along the timeline. Our work is also related to temporal text mining and temporal news summarization. The task of temporal news summarization is to generate news summaries along the timeline from massive data. Chieu et al. (Chieu and Lee, 2004) built a system that extracted events relevant to a query from a collection of related documents and placed such events along a timeline. Yan et al. (Yan et al., 2011b) designed an evolutionary timeline summarization approach to construct a timeline of a topic by optimizing the relevance, coverage, coherence, and diversity. Lin et al. (Lin et al., 2012) explored the problem of generating storylines from microblogs for user input queries. They first proposed a language model with dynamic pseudo relevance feedback to obtain relevant tweets and then generated storylines via graph optimization. 3 Approach Details In this section, we first propose a mixture-eventaspect model to detect local/global sub-events based on part-whole relationship with the major event</context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 745–754. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000271">
<title confidence="0.999235">
A Walk-based Semantically Enriched Tree Kernel
Over Distributed Word Representations
</title>
<author confidence="0.998956">
Shashank Srivastava&apos; Dirk Hovy2 Eduard Hovy&apos;
</author>
<affiliation confidence="0.8979925">
(1) Carnegie Mellon University, Pittsburgh
(2) Center for Language Technology, University of Copenhagen, Denmark
</affiliation>
<email confidence="0.994488">
{ssrivastava,hovy}@cmu.edu,mail@dirkhovy.com
</email>
<sectionHeader confidence="0.993811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755333333333">
In this paper, we propose a walk-based graph
kernel that generalizes the notion of tree-
kernels to continuous spaces. Our proposed
approach subsumes a general framework for
word-similarity, and in particular, provides a
flexible way to incorporate distributed repre-
sentations. Using vector representations, such
an approach captures both distributional se-
mantic similarities among words as well as the
structural relations between them (encoded as
the structure of the parse tree). We show an ef-
ficient formulation to compute this kernel us-
ing simple matrix operations. We present our
results on three diverse NLP tasks, showing
state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998004611111111">
Capturing semantic similarity between sentences
is a fundamental issue in NLP, with applications in
a wide range of tasks. Previously, tree kernels based
on common substructures have been used to model
similarity between parse trees (Collins and Duffy,
2002; Moschitti, 2004; Moschitti, 2006b). These
kernels encode a high number of latent syntactic
features within a concise representation, and com-
pute the similarity between two parse trees based
on the matching of node-labels (words, POS tags,
etc.), as well as the overlap of tree structures. While
this is sufficient to capture syntactic similarity, it
does not capture semantic similarity very well, even
when using discrete semantic types as node labels.
This constrains the utility of many traditional
tree kernels in two ways: i) two sentences that
are syntactically identical, but have no semantic
similarity can receive a high matching score (see
</bodyText>
<tableCaption confidence="0.7377104">
Table 1, top) while ii) two sentences with only local
syntactic overlap, but high semantic similarity can
receive low scores (see Table 1, bottom).
Table 1: Traditional tree kernels do not capture se-
mantic similarity
</tableCaption>
<bodyText confidence="0.998991533333333">
In contrast, distributional vector representations
of words have been successful in capturing fine-
grained semantics, but lack syntactic knowledge.
Resources such as Wordnet, dictionaries and on-
tologies that encode different semantic perspectives
can also provide additional knowledge infusion.
In this paper, we describe a generic walk-based
graph kernel for dependency parse trees that sub-
sumes general notions of word-similarity, while
focusing on vector representations of words to
capture lexical semantics. Through a convolutional
framework, our approach takes into account the
distributional semantic similarities between words
in a sentence as well as the structure of the parse
tree. Our main contributions are:
</bodyText>
<listItem confidence="0.961330875">
1. We present a new graph kernel for NLP that ex-
tends to distributed word representations, and
diverse word similarity measures.
2. Our proposed approach provides a flexible
framework for incorporating both syntax and
semantics of sentence level constructions.
3. Our generic kernel shows state-of-the-art per-
formance on three eclectic NLP tasks.
</listItem>
<figure confidence="0.996569619047619">
tree pairs semantic syntactic score
love
we toys
crush
they puppies
ჴ
✓ high
green little
kissed
she cat
her
she
her
friend
feline
gave
kiss
a
✓
ჴ
low
</figure>
<page confidence="0.911549">
1411
</page>
<bodyText confidence="0.2775655">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99980375">
Tree kernels in NLP Tree kernels have been ex-
tensively used to capture syntactic information about
parse trees in tasks such as parsing (Collins and
Duffy, 2002), NER (Wang et al., 2010; Cumby and
Roth, 2003), SRL (Moschitti et al., 2008) and rela-
tion extraction (Qian et al., 2008). These kernels are
based on the paradigm that parse trees are similar if
they contain many common substructures, consist-
ing of nodes with identical labels (Vishwanathan and
Smola, 2003; Collins and Duffy, 2002). Moschitti
(2006a) proposed a partial tree kernel that adds flex-
ibility in matching tree substructures. Croce et al.
(2011) introduce a lexical semantic tree kernel that
incorporates continuous similarity values between
node labels, albeit with a different focus than ours
and would not match words with different POS. This
would miss the similarity of “feline friend” and “cat”
in our examples, as it requires matching the adjective
“feline” with “cat”, and verb “kissed” with “kiss”.
Walk based kernels Kernels for structured data
derive from the seminal Convolution Kernel for-
malism by Haussler (1999) for designing kernels
for structured objects through local decompositions.
Our proposed kernel for parse trees is most closely
associated with the random walk-based kernels de-
fined by Gartner et al. (2003) and Kashima et al.
(2003). The walk-based graph kernels proposed by
Gartner et al. (2003) count the common walks be-
tween two input graphs, using the adjacency matrix
of the product graph. This work extends to graphs
with a finite set of edge and node labels by appro-
priately modifying the adjacency matrix. Our kernel
differs from these kernels in two significant ways: (i)
Our method extends beyond label matching to con-
tinuous similarity metrics (this conforms with the
very general formalism for graph kernels in Vish-
wanathan et al. (2010)). (ii) Rather than using the
adjacency matrix to model edge-strengths, we mod-
ify the product graph and the corresponding adja-
cency matrix to model node similarities.
</bodyText>
<sectionHeader confidence="0.993288" genericHeader="method">
3 Vector Tree Kernels
</sectionHeader>
<bodyText confidence="0.99989">
In this section, we describe our kernel and an al-
gorithm to compute it as a simple matrix multiplica-
tion formulation.
</bodyText>
<subsectionHeader confidence="0.998606">
3.1 Kernel description
</subsectionHeader>
<bodyText confidence="0.998919">
The similarity kernel K between two dependency
trees can be defined as:
</bodyText>
<equation confidence="0.999008666666667">
K(T1,T2) = E k(h1, h2)
h1CT1,h2CT2
len(h1)=len(h2)
</equation>
<bodyText confidence="0.999779266666667">
where the summation is over pairs of equal length
walks h1 and h2 on the trees T1 and T2 respec-
tively. The similarity between two n length walks,
k(h1, h2), is in turn given by the pairwise similari-
ties of the corresponding nodes vih in the respective
walks, measured via the node similarity kernel n:
k(h1, h2) =
In the context of parse trees, nodes vh1
i and vh2
i cor-
respond to words in the two parse trees, and thus can
often be conveniently represented as vectors over
distributional/dependency contexts. The vector rep-
resentation allows us several choices for the node
kernel function n. In particular, we consider:
</bodyText>
<listItem confidence="0.909684166666667">
1. Gaussian: n(v1, v2) = exp ( − �v1�v2�2 )
2σ2
2. Positive-Linear: n(v1, v2) = max(vT1 v2, 0)
3. Sigmoid: n(v1, v2) = (1 + tanh(avT1 v2))/2
We note that the kernels above take strictly non-
negative values in [0, 1] (assuming word vector rep-
resentations are normalized). Non-negativity is nec-
essary, since we define the walk kernel to be the
product of the individual kernels. As walk kernels
are products of individual node-kernels, bounded-
ness by 1 ensures that the kernel contribution does
not grow arbitrarily for longer length walks.
</listItem>
<bodyText confidence="0.99996225">
The kernel function K puts a high similarity
weight between parse trees if they contain com-
mon walks with semantically similar words in corre-
sponding positions. Apart from the Gaussian kernel,
the other two kernels are based on the dot-product
of the word vector representations. We observe that
the positive-linear kernel defined above is not a Mer-
cer kernel, since the max operation makes it non-
positive semidefinite (PSD). However, this formu-
lation has desirable properties, most significant be-
ing that all walks with one or more node-pair mis-
matches are strictly penalized and add no score to
</bodyText>
<equation confidence="0.9898858">
�(vh1
i , vh2
i )
n
i:1
</equation>
<page confidence="0.920954">
1412
</page>
<bodyText confidence="0.9997114375">
the tree-kernel. This is a more selective condition
than the other two kernels, where mediocre walk
combinations could also add small contributions to
the score. The sigmoid kernel is also non-PSD, but
is known to work well empirically (Boughorbel et
al., 2005). We also observe while the summation in
the kernel is over equal length walks, the formalism
can allow comparisons over different length paths by
including self-loops at nodes in the tree.
With a notion of similarity between words that
defines the local node kernels, we need computa-
tional machinery to enumerate all pairs of walks
between two trees, and compute the summation
over products in the kernel K(T1, T2) efficiently.
We now show a convenient way to compute this as
a matrix geometric series.
</bodyText>
<subsectionHeader confidence="0.6446535">
3.2 Matrix Formulation for Kernel
Computation
</subsectionHeader>
<bodyText confidence="0.999698">
Walk-based kernels compute the number of com-
mon walks using the adjacency matrix of the prod-
uct graph (Gartner et al., 2003). In our case, this
computation is complicated by the fact that instead
of counting common walks, we need to compute a
product of node-similarities for each walk. Since
we compute similarity scores over nodes, rather than
edges, the product for a walk of length n involves
n + 1 factors.
However, we can still compute the tree kernel K
as a simple sum of matrix products. Given two trees
T (V, E) and T0(V 0, E0), we define a modified prod-
uct graph G(Vp, Ep) with an additional ghost node
u added to the vertex set. The vertex and edge sets
for the modified product graph are given as:
</bodyText>
<equation confidence="0.9334846">
�
0 : ((vi1,vj10),(vi2,vj20)) ∈� Ep
κ(vi2,vj20)
:otherwise
=
</equation>
<bodyText confidence="0.884525588235294">
There is a straightforward bijective mapping from
walks on G starting from u to pairs of walks on T
and
Restricting ourselves to the case when the
first node of a k + 1 length walk is u, the next k
steps allow us to efficiently compute the products of
the node similarities along the k nodes in the corre-
sponding klength walks in T and
Given this ad-
jacency matrix for G, the sum of values of k length
walk kernels is given by the
row of the (k + 1)th
exponent of the weighted adjacency matrix (denoted
as Wk+1). This corresponds to k+1 length walks on
G starting from u and ending at any node. Specif-
ically,
corresponds to the sum of similar-
</bodyText>
<equation confidence="0.9200436875">
ities of all common walks of length n in T and
that end in vi in T and
in
The kernel K for
walks upto length N can
Wu,(vi1,vj10)
κ(vi1,vj10)W(v,u) = 0 � v E Vp
T0.
T0.
uth
Wu,(vi,v0j)
T0
v0j
T0.
now be calculated as :
K(T, T 0) =
</equation>
<bodyText confidence="0.821508">
where
</bodyText>
<equation confidence="0.99826225">
W(vi1,vj10),(vi2,vj20) =
� |VV|
i
Su,i
Vp := {(vi1, vj10) : vi1 E V, vj10 E V 0} U u
Ep := {((vi1, vj10), (vi2, vj20)) : (vi1, vi2) E E,
(vj10, vj20)) E E0}
U{(u, (vi1, vj10)) : vi1 E V, vj10 E V 0}
</equation>
<bodyText confidence="0.9865465">
The modified product graph thus has additional
edges connecting u to all other nodes. In our for-
mulation, u now serves as a starting location for all
random walks on G, and a k + 1 length walk of G
corresponds to a pair of k length walks on T and T0.
We now define the weighted adjacency matrix W for
G, which incorporates the local node kernels.
S=W+W2+...WN+1
We note that in out formulation, longer walks are
naturally discounted, since they involve products of
more factors (generally all less than unity).
The above kernel provides a similarity measure
between any two pairs of dependency parse-trees.
Depending on whether we consider directional re-
lations in the parse tree, the edge set
changes,
while the procedure for the kernel computation re-
mains the same. Finally, to avoid larger trees yield-
ing larger values for the kernel, we normalize the
kernel by the number of edges in the product graph.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.92490925">
We evaluate the Vector Tree Kernel (VTK) on
three NLP tasks. We create dependency trees using
the FANSE parser (Tratz and Hovy, 2011), and
use distribution-based SENNA word embeddings
by Collobert
Ep
et al. (2011) as word representations.
These embeddings provide low-dimensional vector
</bodyText>
<page confidence="0.972106">
1413
</page>
<bodyText confidence="0.999937612903226">
representations of words, while encoding distribu-
tional semantic characteristics. We use LibSVM for
classification. For sake of brevity, we only report
results for the best performing kernel.
We first consider the Cornell Sentence Polarity
dataset by Pang and Lee (2005). The task is to
identify the polarity of a given sentence. The
data consists of 5331 sentences from positive and
negative movie reviews. Many phrases denoting
sentiments are lexically ambiguous (cf. “terribly
entertaining” vs “terribly written”), so simple lexi-
cal approaches are not expected to work well here,
while syntactic context could help disambiguation.
Next, we try our approach on the MSR paraphrase
corpus. The data contains a training set of 4077
pairs of sentences, annotated as paraphrases and
non-paraphrases, and a test-set of 1726 sentence
pairs. Each instance consists of a pair of sentences,
so the VTK cannot be directly used by a kernel
machine for classification. Instead, we generate
16 kernel values based for each pair on different
parameter settings of the kernel, and feed these as
features to a linear SVM.
We finally look at the annotated Metaphor corpus
by (Hovy et al., 2013). The dataset consists of sen-
tences with specified target phrases. The task here is
to classify the target use as literal or metaphorical.
We focus on target phrases by upweighting walks
that pass through target nodes. This is done by
simply multiplying the corresponding entries in the
adjacency matrix by a constant factor.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.939777">
5.1 Sentence Polarity Dataset
</subsectionHeader>
<table confidence="0.999901714285714">
Prec Rec F1 Acc
Albornoz et al 0.63 – – 0.63
WNA+synsets 0.61 – – 0.61
WNA 0.53 – – 0.51
DSM 0.54 0.55 0.55 0.54
SSTK 0.49 0.48 0.48 0.49
VTK 0.65 0.58 0.62 0.67
</table>
<tableCaption confidence="0.999742">
Table 2: Results on Sentence Polarity dataset
</tableCaption>
<bodyText confidence="0.998654285714286">
On the polarity data set, Vector Tree Kernel
(VTK) significantly outperforms the state-of-the-art
method by Carrillo de Albornoz et al. (2010), who
use a hybrid model incorporating databases of af-
fective lexicons, and also explicitly model the ef-
fect of negation and quantifiers (see Table 2). Lex-
ical approaches using pairwise semantic similarity
of SENNA embeddings (DSM), as well as Word-
net Affective Database-based (WNA) labels perform
poorly (Carrillo de Albornoz et al., 2010), showing
the importance of syntax for this particular problem.
On the other hand, a syntactic tree kernel (SSTK)
that ignores distributional semantic similarity be-
tween words, fails as expected.
</bodyText>
<subsectionHeader confidence="0.932414">
5.2 MSR Paraphrase Dataset
</subsectionHeader>
<table confidence="0.999916571428571">
Prec Rec F1 Acc
BASE 0.72 0.86 0.79 0.69
Zhang et al 0.74 0.88 0.81 0.72
Qiu et al 0.73 0.93 0.82 0.72
Malakasiotis 0.74 0.94 0.83 0.74
Finch 0.77 0.90 0.83 0.75
VTK 0.72 0.95 0.82 0.72
</table>
<tableCaption confidence="0.999961">
Table 3: Results on MSR Paraphrase corpus
</tableCaption>
<bodyText confidence="0.99975875">
On the MSR paraphrase corpus, VTK performs
competitively against state-of-the-art-methods. We
expected paraphrasing to be challenging to our
method, since it can involve little syntactic overlap.
However, data analysis reveals that the corpus gener-
ally contains sentence pairs with high syntactic sim-
ilarity. Results for this task are encouraging since
ours is a general approach, while other systems use
multiple task-specific features like semantic role la-
bels, active-passive voice conversion, and synonymy
resolution. In the future, incorporating such features
to VTK should further improve results for this task.
</bodyText>
<subsectionHeader confidence="0.839564">
5.3 Metaphor Identification
</subsectionHeader>
<table confidence="0.9998666">
Acc P R F1
CRF 0.69 0.74 0.50 0.59
SVM+DSM 0.70 0.63 0.80 0.71
SSTK 0.75 0.70 0.80 0.75
VTK 0.76 0.67 0.87 0.76
</table>
<tableCaption confidence="0.999907">
Table 4: Results on Metaphor dataset
</tableCaption>
<bodyText confidence="0.999915">
On the Metaphor corpus, VTK improves the pre-
vious score by Hovy et al. (2013), whose approach
uses an conjunction of lexical and syntactic tree ker-
nels (Moschitti, 2006b), and distributional vectors.
VTK identified several templates of metaphor usage
such as “warm heart” and “cold shoulder”. We look
towards approaches for automatedly mining such
metaphor patterns from a corpus.
</bodyText>
<sectionHeader confidence="0.996681" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996213">
We present a general formalism for walk-based
kernels to evaluate similarity of dependency trees.
</bodyText>
<page confidence="0.982185">
1414
</page>
<bodyText confidence="0.999922222222222">
Our method generalizes tree kernels to take dis-
tributed representations of nodes as input, and cap-
ture both lexical semantics and syntactic structures
of parse trees. Our approach has tunable parame-
ters to look for larger or smaller syntactic constructs.
Our experiments shows state-of-the-art performance
on three diverse NLP tasks. The approach can gen-
eralize to any task involving structural and local sim-
ilarity, and arbitrary node similarity measures.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999488242105263">
Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Bouje-
maa. 2005. Conditionally positive definite kernels for
svm based image recognition. In ICME, pages 113–
116.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerv´as. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 153–161. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association for
computational linguistics, pages 263–270. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034–1046. Association for
Computational Linguistics.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In In Proc. of the International
Conference on Machine Learning, pages 107–114.
Andrew Finch. 2005. Using machine translation evalu-
ation techniques to determine sentence-level semantic
equivalence. In In IWP2005.
Thomas Gartner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient al-
ternatives. In Proceedings of the Annual Conference
on Computational Learning Theory, pages 129–143.
Springer.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical Report Technical Report UCS-
CRL-99-10, UC Santa Cruz.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proceed-
ings of NAACL HLT, Meta4NLP Workshop.
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the Twentieth International Con-
ference on Machine Learning, pages 321–328. AAAI
Press.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, ACLstudent ’09, pages 27–
35, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 335–es. Association for
Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318–329.
Springer.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697–704. Association for Computational Lin-
guistics.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’06, pages 18–26, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142–149.
</reference>
<page confidence="0.826127">
1415
</page>
<reference confidence="0.999338904761905">
Stephen Tratz and Eduard Hovy. 2011. A fast, accu-
rate, non-projective, semantically-enriched parser. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’11, pages
1257–1268, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. V. N. Vishwanathan and Alexander J. Smola. 2003.
Fast kernels for string and tree matching. In Advances
In Neural Information Processing Systems 15, pages
569–576. MIT Press.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kon-
dor, and Karsten M. Borgwardt. 2010. Graph kernels.
J. Mach. Learn. Res., 99:1201–1242, August.
Xinglong Wang, Jun’ichi Tsujii, and Sophia Ananiadou.
2010. Disambiguating the species of biomedical
named entities using natural language parsers. Bioin-
formatics, 26(5):661–667.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In In Proceedings
of the Australasian Language Technology Workshop
2005.
</reference>
<page confidence="0.992677">
1416
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875114">
<title confidence="0.997728">A Walk-based Semantically Enriched Tree Over Distributed Word Representations</title>
<author confidence="0.97093">Eduard</author>
<affiliation confidence="0.942566">(1) Carnegie Mellon University, (2) Center for Language Technology, University of Copenhagen,</affiliation>
<abstract confidence="0.999535875">In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a flexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabri Boughorbel</author>
<author>Jean-Philippe Tarel</author>
<author>Nozha Boujemaa</author>
</authors>
<title>Conditionally positive definite kernels for svm based image recognition.</title>
<date>2005</date>
<booktitle>In ICME,</booktitle>
<pages>113--116</pages>
<contexts>
<context position="7877" citStr="Boughorbel et al., 2005" startWordPosition="1234" endWordPosition="1237">representations. We observe that the positive-linear kernel defined above is not a Mercer kernel, since the max operation makes it nonpositive semidefinite (PSD). However, this formulation has desirable properties, most significant being that all walks with one or more node-pair mismatches are strictly penalized and add no score to �(vh1 i , vh2 i ) n i:1 1412 the tree-kernel. This is a more selective condition than the other two kernels, where mediocre walk combinations could also add small contributions to the score. The sigmoid kernel is also non-PSD, but is known to work well empirically (Boughorbel et al., 2005). We also observe while the summation in the kernel is over equal length walks, the formalism can allow comparisons over different length paths by including self-loops at nodes in the tree. With a notion of similarity between words that defines the local node kernels, we need computational machinery to enumerate all pairs of walks between two trees, and compute the summation over products in the kernel K(T1, T2) efficiently. We now show a convenient way to compute this as a matrix geometric series. 3.2 Matrix Formulation for Kernel Computation Walk-based kernels compute the number of common wa</context>
</contexts>
<marker>Boughorbel, Tarel, Boujemaa, 2005</marker>
<rawString>Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Boujemaa. 2005. Conditionally positive definite kernels for svm based image recognition. In ICME, pages 113– 116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Carrillo de Albornoz</author>
<author>Laura Plaza</author>
<author>Pablo Gerv´as</author>
</authors>
<title>A hybrid approach to emotional sentence polarity and intensity classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>153--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>de Albornoz, Plaza, Gerv´as, 2010</marker>
<rawString>Jorge Carrillo de Albornoz, Laura Plaza, and Pablo Gerv´as. 2010. A hybrid approach to emotional sentence polarity and intensity classification. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 153–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1223" citStr="Collins and Duffy, 2002" startWordPosition="166" endWordPosition="169">s, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semanti</context>
<context position="3691" citStr="Collins and Duffy, 2002" startWordPosition="541" endWordPosition="544">ions. 3. Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. tree pairs semantic syntactic score love we toys crush they puppies ჴ ✓ high green little kissed she cat her she her friend feline gave kiss a ✓ ჴ low 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 263–270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1034--1046</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4147" citStr="Croce et al. (2011)" startWordPosition="615" endWordPosition="618">k Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1034–1046. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>On kernel methods for relational learning. In</title>
<date>2003</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<pages>107--114</pages>
<contexts>
<context position="3738" citStr="Cumby and Roth, 2003" startWordPosition="550" endWordPosition="553"> performance on three eclectic NLP tasks. tree pairs semantic syntactic score love we toys crush they puppies ჴ ✓ high green little kissed she cat her she her friend feline gave kiss a ✓ ჴ low 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different </context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. On kernel methods for relational learning. In In Proc. of the International Conference on Machine Learning, pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In</title>
<date>2005</date>
<booktitle>In IWP2005.</booktitle>
<marker>Finch, 2005</marker>
<rawString>Andrew Finch. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In In IWP2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Gartner</author>
<author>Peter Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On graph kernels: Hardness results and efficient alternatives.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Conference on Computational Learning Theory,</booktitle>
<pages>129--143</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4834" citStr="Gartner et al. (2003)" startWordPosition="720" endWordPosition="723">uous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most closely associated with the random walk-based kernels defined by Gartner et al. (2003) and Kashima et al. (2003). The walk-based graph kernels proposed by Gartner et al. (2003) count the common walks between two input graphs, using the adjacency matrix of the product graph. This work extends to graphs with a finite set of edge and node labels by appropriately modifying the adjacency matrix. Our kernel differs from these kernels in two significant ways: (i) Our method extends beyond label matching to continuous similarity metrics (this conforms with the very general formalism for graph kernels in Vishwanathan et al. (2010)). (ii) Rather than using the adjacency matrix to model e</context>
<context position="8551" citStr="Gartner et al., 2003" startWordPosition="1346" endWordPosition="1349">s over equal length walks, the formalism can allow comparisons over different length paths by including self-loops at nodes in the tree. With a notion of similarity between words that defines the local node kernels, we need computational machinery to enumerate all pairs of walks between two trees, and compute the summation over products in the kernel K(T1, T2) efficiently. We now show a convenient way to compute this as a matrix geometric series. 3.2 Matrix Formulation for Kernel Computation Walk-based kernels compute the number of common walks using the adjacency matrix of the product graph (Gartner et al., 2003). In our case, this computation is complicated by the fact that instead of counting common walks, we need to compute a product of node-similarities for each walk. Since we compute similarity scores over nodes, rather than edges, the product for a walk of length n involves n + 1 factors. However, we can still compute the tree kernel K as a simple sum of matrix products. Given two trees T (V, E) and T0(V 0, E0), we define a modified product graph G(Vp, Ep) with an additional ghost node u added to the vertex set. The vertex and edge sets for the modified product graph are given as: � 0 : ((vi1,vj</context>
</contexts>
<marker>Gartner, Flach, Wrobel, 2003</marker>
<rawString>Thomas Gartner, Peter Flach, and Stefan Wrobel. 2003. On graph kernels: Hardness results and efficient alternatives. In Proceedings of the Annual Conference on Computational Learning Theory, pages 129–143. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report Technical Report UCSCRL-99-10,</tech>
<institution>UC Santa Cruz.</institution>
<contexts>
<context position="4628" citStr="Haussler (1999)" startWordPosition="691" endWordPosition="692">ffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most closely associated with the random walk-based kernels defined by Gartner et al. (2003) and Kashima et al. (2003). The walk-based graph kernels proposed by Gartner et al. (2003) count the common walks between two input graphs, using the adjacency matrix of the product graph. This work extends to graphs with a finite set of edge and node labels by appropriately modifying the adjacency matrix. Our kernel differs from these kernels in two significant ways: (i) Our method extends </context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical Report Technical Report UCSCRL-99-10, UC Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Shashank Srivastava</author>
<author>Sujay Kumar Jauhar</author>
<author>Mrinmaya Sachan</author>
<author>Kartik Goyal</author>
<author>Huiying Li</author>
<author>Whitney Sanders</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying metaphorical word use with tree kernels.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL HLT, Meta4NLP Workshop.</booktitle>
<contexts>
<context position="12665" citStr="Hovy et al., 2013" startWordPosition="2079" endWordPosition="2082">ted to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair of sentences, so the VTK cannot be directly used by a kernel machine for classification. Instead, we generate 16 kernel values based for each pair on different parameter settings of the kernel, and feed these as features to a linear SVM. We finally look at the annotated Metaphor corpus by (Hovy et al., 2013). The dataset consists of sentences with specified target phrases. The task here is to classify the target use as literal or metaphorical. We focus on target phrases by upweighting walks that pass through target nodes. This is done by simply multiplying the corresponding entries in the adjacency matrix by a constant factor. 5 Results 5.1 Sentence Polarity Dataset Prec Rec F1 Acc Albornoz et al 0.63 – – 0.63 WNA+synsets 0.61 – – 0.61 WNA 0.53 – – 0.51 DSM 0.54 0.55 0.55 0.54 SSTK 0.49 0.48 0.48 0.49 VTK 0.65 0.58 0.62 0.67 Table 2: Results on Sentence Polarity dataset On the polarity data set, </context>
<context position="15044" citStr="Hovy et al. (2013)" startWordPosition="2469" endWordPosition="2472">ontains sentence pairs with high syntactic similarity. Results for this task are encouraging since ours is a general approach, while other systems use multiple task-specific features like semantic role labels, active-passive voice conversion, and synonymy resolution. In the future, incorporating such features to VTK should further improve results for this task. 5.3 Metaphor Identification Acc P R F1 CRF 0.69 0.74 0.50 0.59 SVM+DSM 0.70 0.63 0.80 0.71 SSTK 0.75 0.70 0.80 0.75 VTK 0.76 0.67 0.87 0.76 Table 4: Results on Metaphor dataset On the Metaphor corpus, VTK improves the previous score by Hovy et al. (2013), whose approach uses an conjunction of lexical and syntactic tree kernels (Moschitti, 2006b), and distributional vectors. VTK identified several templates of metaphor usage such as “warm heart” and “cold shoulder”. We look towards approaches for automatedly mining such metaphor patterns from a corpus. 6 Conclusion We present a general formalism for walk-based kernels to evaluate similarity of dependency trees. 1414 Our method generalizes tree kernels to take distributed representations of nodes as input, and capture both lexical semantics and syntactic structures of parse trees. Our approach </context>
</contexts>
<marker>Hovy, Srivastava, Jauhar, Sachan, Goyal, Li, Sanders, Hovy, 2013</marker>
<rawString>Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whitney Sanders, and Eduard Hovy. 2013. Identifying metaphorical word use with tree kernels. In Proceedings of NAACL HLT, Meta4NLP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Koji Tsuda</author>
<author>Akihiro Inokuchi</author>
</authors>
<title>Marginalized kernels between labeled graphs.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning,</booktitle>
<pages>321--328</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4860" citStr="Kashima et al. (2003)" startWordPosition="725" endWordPosition="728">ween node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most closely associated with the random walk-based kernels defined by Gartner et al. (2003) and Kashima et al. (2003). The walk-based graph kernels proposed by Gartner et al. (2003) count the common walks between two input graphs, using the adjacency matrix of the product graph. This work extends to graphs with a finite set of edge and node labels by appropriately modifying the adjacency matrix. Our kernel differs from these kernels in two significant ways: (i) Our method extends beyond label matching to continuous similarity metrics (this conforms with the very general formalism for graph kernels in Vishwanathan et al. (2010)). (ii) Rather than using the adjacency matrix to model edge-strengths, we modify t</context>
</contexts>
<marker>Kashima, Tsuda, Inokuchi, 2003</marker>
<rawString>Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. 2003. Marginalized kernels between labeled graphs. In Proceedings of the Twentieth International Conference on Machine Learning, pages 321–328. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, ACLstudent ’09,</booktitle>
<pages>27--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Malakasiotis, 2009</marker>
<rawString>Prodromos Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, ACLstudent ’09, pages 27– 35, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="3768" citStr="Moschitti et al., 2008" startWordPosition="555" endWordPosition="558">ic NLP tasks. tree pairs semantic syntactic score love we toys crush they puppies ჴ ✓ high green little kissed she cat her she her friend feline gave kiss a ✓ ჴ low 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the simil</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1240" citStr="Moschitti, 2004" startWordPosition="170" endWordPosition="171">res both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can </context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 335–es. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>318--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1257" citStr="Moschitti, 2006" startWordPosition="172" endWordPosition="173">tional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can receive a high ma</context>
<context position="4040" citStr="Moschitti (2006" startWordPosition="600" endWordPosition="601">le, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designi</context>
<context position="15135" citStr="Moschitti, 2006" startWordPosition="2485" endWordPosition="2486">ince ours is a general approach, while other systems use multiple task-specific features like semantic role labels, active-passive voice conversion, and synonymy resolution. In the future, incorporating such features to VTK should further improve results for this task. 5.3 Metaphor Identification Acc P R F1 CRF 0.69 0.74 0.50 0.59 SVM+DSM 0.70 0.63 0.80 0.71 SSTK 0.75 0.70 0.80 0.75 VTK 0.76 0.67 0.87 0.76 Table 4: Results on Metaphor dataset On the Metaphor corpus, VTK improves the previous score by Hovy et al. (2013), whose approach uses an conjunction of lexical and syntactic tree kernels (Moschitti, 2006b), and distributional vectors. VTK identified several templates of metaphor usage such as “warm heart” and “cold shoulder”. We look towards approaches for automatedly mining such metaphor patterns from a corpus. 6 Conclusion We present a general formalism for walk-based kernels to evaluate similarity of dependency trees. 1414 Our method generalizes tree kernels to take distributed representations of nodes as input, and capture both lexical semantics and syntactic structures of parse trees. Our approach has tunable parameters to look for larger or smaller syntactic constructs. Our experiments </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006a. Efficient convolution kernels for dependency and constituent syntactic trees. In Machine Learning: ECML 2006, pages 318–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making Tree Kernels Practical for Natural Language Learning. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1257" citStr="Moschitti, 2006" startWordPosition="172" endWordPosition="173">tional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efficient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can receive a high ma</context>
<context position="4040" citStr="Moschitti (2006" startWordPosition="600" endWordPosition="601">le, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designi</context>
<context position="15135" citStr="Moschitti, 2006" startWordPosition="2485" endWordPosition="2486">ince ours is a general approach, while other systems use multiple task-specific features like semantic role labels, active-passive voice conversion, and synonymy resolution. In the future, incorporating such features to VTK should further improve results for this task. 5.3 Metaphor Identification Acc P R F1 CRF 0.69 0.74 0.50 0.59 SVM+DSM 0.70 0.63 0.80 0.71 SSTK 0.75 0.70 0.80 0.75 VTK 0.76 0.67 0.87 0.76 Table 4: Results on Metaphor dataset On the Metaphor corpus, VTK improves the previous score by Hovy et al. (2013), whose approach uses an conjunction of lexical and syntactic tree kernels (Moschitti, 2006b), and distributional vectors. VTK identified several templates of metaphor usage such as “warm heart” and “cold shoulder”. We look towards approaches for automatedly mining such metaphor patterns from a corpus. 6 Conclusion We present a general formalism for walk-based kernels to evaluate similarity of dependency trees. 1414 Our method generalizes tree kernels to take distributed representations of nodes as input, and capture both lexical semantics and syntactic structures of parse trees. Our approach has tunable parameters to look for larger or smaller syntactic constructs. Our experiments </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006b. Making Tree Kernels Practical for Natural Language Learning. In In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="11757" citStr="Pang and Lee (2005)" startWordPosition="1932" endWordPosition="1935"> the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert Ep et al. (2011) as word representations. These embeddings provide low-dimensional vector 1413 representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments are lexically ambiguous (cf. “terribly entertaining” vs “terribly written”), so simple lexical approaches are not expected to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair o</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>697--704</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3812" citStr="Qian et al., 2008" startWordPosition="563" endWordPosition="566"> love we toys crush they puppies ჴ ✓ high green little kissed she cat her she her friend feline gave kiss a ✓ ჴ low 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our ex</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 697–704. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>18--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 18–26, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>142--149</pages>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, accurate, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1257--1268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11328" citStr="Tratz and Hovy, 2011" startWordPosition="1871" endWordPosition="1874"> since they involve products of more factors (generally all less than unity). The above kernel provides a similarity measure between any two pairs of dependency parse-trees. Depending on whether we consider directional relations in the parse tree, the edge set changes, while the procedure for the kernel computation remains the same. Finally, to avoid larger trees yielding larger values for the kernel, we normalize the kernel by the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert Ep et al. (2011) as word representations. These embeddings provide low-dimensional vector 1413 representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments </context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1257–1268, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Alexander J Smola</author>
</authors>
<title>Fast kernels for string and tree matching.</title>
<date>2003</date>
<booktitle>In Advances In Neural Information Processing Systems 15,</booktitle>
<pages>569--576</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3997" citStr="Vishwanathan and Smola, 2003" startWordPosition="592" endWordPosition="595">s in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kern</context>
</contexts>
<marker>Vishwanathan, Smola, 2003</marker>
<rawString>S. V. N. Vishwanathan and Alexander J. Smola. 2003. Fast kernels for string and tree matching. In Advances In Neural Information Processing Systems 15, pages 569–576. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Risi Kondor</author>
<author>Karsten M Borgwardt</author>
</authors>
<date>2010</date>
<journal>Graph kernels. J. Mach. Learn. Res.,</journal>
<pages>99--1201</pages>
<contexts>
<context position="5377" citStr="Vishwanathan et al. (2010)" startWordPosition="810" endWordPosition="814">sely associated with the random walk-based kernels defined by Gartner et al. (2003) and Kashima et al. (2003). The walk-based graph kernels proposed by Gartner et al. (2003) count the common walks between two input graphs, using the adjacency matrix of the product graph. This work extends to graphs with a finite set of edge and node labels by appropriately modifying the adjacency matrix. Our kernel differs from these kernels in two significant ways: (i) Our method extends beyond label matching to continuous similarity metrics (this conforms with the very general formalism for graph kernels in Vishwanathan et al. (2010)). (ii) Rather than using the adjacency matrix to model edge-strengths, we modify the product graph and the corresponding adjacency matrix to model node similarities. 3 Vector Tree Kernels In this section, we describe our kernel and an algorithm to compute it as a simple matrix multiplication formulation. 3.1 Kernel description The similarity kernel K between two dependency trees can be defined as: K(T1,T2) = E k(h1, h2) h1CT1,h2CT2 len(h1)=len(h2) where the summation is over pairs of equal length walks h1 and h2 on the trees T1 and T2 respectively. The similarity between two n length walks, k</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Kondor, Borgwardt, 2010</marker>
<rawString>S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. 2010. Graph kernels. J. Mach. Learn. Res., 99:1201–1242, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinglong Wang</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Disambiguating the species of biomedical named entities using natural language parsers.</title>
<date>2010</date>
<journal>Bioinformatics,</journal>
<volume>26</volume>
<issue>5</issue>
<contexts>
<context position="3715" citStr="Wang et al., 2010" startWordPosition="546" endWordPosition="549">ws state-of-the-art performance on three eclectic NLP tasks. tree pairs semantic syntactic score love we toys crush they puppies ჴ ✓ high green little kissed she cat her she her friend feline gave kiss a ✓ ჴ low 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not matc</context>
</contexts>
<marker>Wang, Tsujii, Ananiadou, 2010</marker>
<rawString>Xinglong Wang, Jun’ichi Tsujii, and Sophia Ananiadou. 2010. Disambiguating the species of biomedical named entities using natural language parsers. Bioinformatics, 26(5):661–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yitao Zhang</author>
<author>Jon Patrick</author>
</authors>
<title>Paraphrase identification by text canonicalization.</title>
<date>2005</date>
<booktitle>In In Proceedings of the Australasian Language Technology Workshop</booktitle>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Yitao Zhang and Jon Patrick. 2005. Paraphrase identification by text canonicalization. In In Proceedings of the Australasian Language Technology Workshop 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
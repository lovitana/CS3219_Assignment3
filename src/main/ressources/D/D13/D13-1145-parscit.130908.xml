<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004059">
<title confidence="0.988891">
Automatic Idiom Identification in Wiktionary
</title>
<author confidence="0.991345">
Grace Muzny and Luke Zettlemoyer
</author>
<affiliation confidence="0.992635">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.960696">
Seattle, WA 98195
</address>
<email confidence="0.999538">
{muznyg,lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999050761904762">
Online resources, such as Wiktionary, provide
an accurate but incomplete source of idiomatic
phrases. In this paper, we study the problem
of automatically identifying idiomatic dictio-
nary entries with such resources. We train
an idiom classifier on a newly gathered cor-
pus of over 60,000 Wiktionary multi-word
definitions, incorporating features that model
whether phrase meanings are constructed
compositionally. Experiments demonstrate
that the learned classifier can provide high
quality idiom labels, more than doubling the
number of idiomatic entries from 7,764 to
18,155 at precision levels of over 65%. These
gains also translate to idiom detection in sen-
tences, by simply using known word sense
disambiguation algorithms to match phrases
to their definitions. In a set of Wiktionary def-
inition example sentences, the more complete
set of idioms boosts detection recall by over
28 percentage points.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997859375">
Idiomatic language is common and provides unique
challenges for language understanding systems. For
example, a diamond in the rough can be the literal
unpolished object or a crude but lovable person. Un-
derstanding such distinctions is important for many
applications, including parsing (Sag et al., 2002) and
machine translation (Shutova et al., 2012).
We use Wiktionary as a large, but incomplete, ref-
erence for idiomatic entries; individual entries can
be marked as idiomatic but, in practice, most are
not. Using these incomplete annotations as super-
vision, we train a binary Perceptron classifier for
identifying idiomatic dictionary entries. We intro-
duce new lexical and graph-based features that use
WordNet and Wiktionary to compute semantic re-
latedness. This allows us to learn, for example, that
the words in the phrase diamond in the rough are
more closely related to the words in its literal defi-
nition than the idiomatic one. Experiments demon-
strate that the classifier achieves precision of over
65% at recall over 52% and that, when used to fill in
missing Wiktionary idiom labels, it more than dou-
bles the number of idioms from 7,764 to 18,155.
These gains also translate to idiom detection in
sentences, by simply using the Lesk word sense
disambiguation (WSD) algorithm (1986) to match
phrases to their definitions. This approach allows
for scalable detection with no restrictions on the syn-
tactic structure or context of the target phrase. In a
set of Wiktionary definition example sentences, the
more complete set of idioms boosts detection recall
by over 28 percentage points.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998465">
To the best of our knowledge, this work represents
the first attempt to identify dictionary entries as id-
iomatic and the first to reduce idiom detection to
identification via a dictionary.
Previous idiom detection systems fall in one
of two paradigms: phrase classification, where a
phrase p is always idiomatic or literal, e.g. (Gedigian
et al., 2006; Shutova et al., 2010), or token classifi-
cation, where each occurrence of a phrase p can be
idiomatic or literal, e.g. (Katz and Giesbrecht, 2006;
</bodyText>
<page confidence="0.921104">
1417
</page>
<bodyText confidence="0.944463125">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Birke and Sarkar, 2006; Li and Sporleder, 2009).
Most previous idiom detection systems have focused
on specific syntactic constructions. For instance,
Shutova et al. (2010) consider subject/verb (cam-
paign surged) and verb/direct-object idioms (stir ex-
citement) while Fazly and Stevenson (2006), Cook
et al. (2007), and Diab and Bhutada (2009) de-
tect verb/noun idioms (blow smoke). Fothergill and
Baldwin (2012) are syntactically unconstrained, but
only study Japanese idioms. Although we focus on
identifying idiomatic dictionary entries, one advan-
tage of our approach is that it enables syntactically
unconstrained token-level detection for any phrase
in the dictionary.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="method">
3 Formal Problem Definitions
</sectionHeader>
<bodyText confidence="0.99928825">
Identification For identification, we assume data
of the form {((pi, di), yi) : i = 1... n} where
pi is the phrase associated with definition di and
yi E {literal, idiomatic}. For example, this would
include both the literal pair ( “leave for dead”, “To
abandon a person or other living creature that is in-
jured or otherwise incapacitated, assuming that the
death of the one abandoned will soon follow.”) and
the idiomatic pair ( “leave for dead”, “To disregard
or bypass as unimportant.” ). Given (pi, di), we aim
to predict yi.
Detection To evaluate identification in the con-
text of detection, we assume data {((pi, ei), yi) :
i = 1... n}. Here, pi is the phrase in exam-
ple sentence ei whose idiomatic status is labeled
yi E {idiomatic, literal}. One such idiomatic pair
is (“heart to heart”, “They sat down and had a
long overdue heart to heart about the future of their
relationship.”). Given (pi, ei), we again aim to pre-
dict yi.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999784714285714">
We gathered phrases, definitions, and example sen-
tences from the English-language Wiktionary dump
from November 13th, 2012.1
Identification Phrase, definition pairs (p, d) were
gathered with the following restrictions: the title of
the Wiktionary entry must be English, p must com-
posed of two or more words w, and (p, d) must be in
</bodyText>
<footnote confidence="0.975048">
1We used the Java Wiktionary Library (Zesch et al., 2008).
</footnote>
<table confidence="0.998658571428572">
Data Set Literal Idiomatic Total
All 56,037 7,764 63,801
Train 47,633 6,600 54,233
Unannotated Dev 2,801 388 3,189
Annotated Dev 2,212 958 3,170
Unannotated Test 5,603 776 6,379
Annotated Test 4,510 1,834 6,344
</table>
<figureCaption confidence="0.939747">
Figure 1: Number of dictionary entries with each class
for the Wiktionary identification data.
</figureCaption>
<table confidence="0.873471">
Data Set Literal Idiomatic Total
Dev 171 330 501
Test 360 695 1055
</table>
<figureCaption confidence="0.9939965">
Figure 2: Number of sentences of each class for the Wik-
tionary detection data.
</figureCaption>
<bodyText confidence="0.999957233333334">
its base form—senses that are not defined as a dif-
ferent tense of a phrase—e.g. the pair ( “weapons of
mass destruction”, “Plural form of weapon of mass
destruction” ) was removed while the pair ( “weapon
of mass destruction”, “A chemical, biological, radio-
logical, nuclear or other weapon that ... ”) was kept.
Each pair (p, d) was assigned label y according
to the idiom labels in Wiktionary, producing the
Train, Unannotated Dev, and Unannotated Test data
sets. In practice, this produces a noisy assignment
because a majority of the idiomatic senses are not
marked. The development and test sets were anno-
tated to correct these potential omissions. Annota-
tors used the definition of an idiom as a “phrase with
a non-compositional meaning” to produce the An-
notated Dev and Annotated Test data sets. Figure 1
presents the data statistics.
We measured inter-annotator agreement on 1,000
examples. Two annotators marked each dictionary
entry as literal, idiomatic, or indeterminable. Less
than one half of one percent could not be deter-
mined2—the computed kappa was 81.85. Given
this high level of agreement, the rest of the data
were only labeled by a single annotator, follow-
ing the methodology used with the VNC-Tokens
Dataset (Cook et al., 2008).
Detection For detection, we gathered the example
sentences provided, when available, for each defi-
nition used in our annotated identification data sets.
These sentences provide a clean source of develop-
</bodyText>
<footnote confidence="0.94264">
2The indeterminable pairs were omitted from the data.
</footnote>
<page confidence="0.990631">
1418
</page>
<bodyText confidence="0.999543">
ment and test data containing idiomatic and literal
phrase usages. In all, there were over 1,300 unique
phrases, half of which had more than one possible
dictionary definition in Wiktionary. Figure 2 pro-
vides the complete statistics.
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="method">
5 Identification Model
</sectionHeader>
<bodyText confidence="0.9979342">
For identification, we use a linear model that pre-
dicts class y∗ E {literal, idiomatic} for an input pair
(p, d) with phrase p and definition d. We assign the
class:
Graph-based features use the graph structure of
WordNet 3.0 to calculate path distances. Let
distance(w, v, rel, n) be the minimum distance via
links of type rel in WordNet from a word w to a
word v, up to a threshold max integer value n, and 0
otherwise. The features compute:
</bodyText>
<listItem confidence="0.650095666666667">
• closest synonym:
distance(w, v, synonym, 5)
• closest antonym:4
</listItem>
<equation confidence="0.5705155">
min
WEp,VEd
y∗ = arg max 9 · 0(p, d, y) min distance(w, v, antonym, 5)
y WEp,VEd
</equation>
<bodyText confidence="0.996379894736842">
given features 0(p, d, y) E R&apos; with associated pa-
rameters 0 E R&apos;.
Learning In this work, we use the averaged Per-
ceptron algorithm (Freund and Schapire, 1999) to
perform learning, which was optimized in terms of
iterations T, bounded by range [1, 100], by maxi-
mizing F-measure on the development set.
The models described correspond to the features
they use. All models are trained on the same, unan-
notated training data.
Features The features that were developed fall
into two categories: lexical and graph-based fea-
tures. The lexical features were motivated by the
intuition that literal phrases are more likely to have
closely related words in d to those in p because lit-
eral phrases do not break the principle of compo-
sitionality. All words compared are stemmed ver-
sions. Let count(w, t) = number of times word w
appears in text t.
</bodyText>
<listItem confidence="0.964064777777778">
• synonym overlap: Let S be the set of syn-
onyms as defined in Wiktionary for all words
in p. Then, we define the synonym overlap =
|S |K1 ∈S count(s, d).
• antonym overlap: Let A be the set of antonyms
as defined in Wiktionary for all words in
p. Then, we define the antonym overlap =
|A |&amp;∈A count(a, d).
• average number of capitals:3 The value of
</listItem>
<bodyText confidence="0.619334">
number of capital letters in p
number of words in p .
</bodyText>
<footnote confidence="0.63073">
3In practice, this feature identifies most proper nouns.
</footnote>
<listItem confidence="0.819776">
• average synonym distance:
• synsets connected by an antonym: This feature in-
dicates whether the following is true. The set of
synsets Synp, all synsets from all words in p, and
</listItem>
<bodyText confidence="0.9100605">
the set of synsets Synd, all synsets from all words
in d, are connected by a shared antonym. This fea-
ture follows an approach described by Budanitsky
et al. (2006).
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999953">
We report identification and detection results, vary-
ing the data labeling and choice of feature sets.
</bodyText>
<subsectionHeader confidence="0.977833">
6.1 Identification
</subsectionHeader>
<bodyText confidence="0.999780555555556">
Random Baseline We use a proportionally ran-
dom baseline for the identification task that classi-
fies according to the proportion of literal definitions
seen in the training data.
Results Figure 3 provides the results for the base-
line, the full approach, and variations with subsets
of the features. Results are reported for the origi-
nal, unannotated test set, and the same test examples
with corrected idiom labels. All models increased
</bodyText>
<footnote confidence="0.8964925">
4The first relation expanded was the antonym relation. All
subsequent expansions were via synonym relations.
</footnote>
<table confidence="0.954576">
1 distance(w, v, synonym, 5)
|p |WEp,VEd
• average hyponym:
1 distance(w, v, hyponym, 5)
|p |WEp,VEd
1419
Data Set Model Rec. Prec. F1
Unannotated Lexical 85.8 21.9 34.9
Graph 62.4 26.6 37.3
Lexical+Graph 70.5 28.1 40.1
Baseline 12.2 11.9 12.0
Annotated Lexical 81.2 49.3 61.4
Graph 64.3 51.3 57.1
Lexical+Graph 75.0 52.9 62.0
Baseline 29.5 12.5 17.6
</table>
<figureCaption confidence="0.997613666666667">
Figure 3: Results for idiomatic definition identification.
Figure 4: Precision and recall with varied features on the
annotated test set.
</figureCaption>
<bodyText confidence="0.999646647058824">
over their corresponding baselines by more than 22
points and both feature families contributed.5
Figure 4 shows the complete precision, recall
curve. We selected our operating point to optimize
F-measure, but we see that the graph features per-
form well across all recall levels and that adding the
lexical features provides consistent improvement in
precision. However, other points are possible, es-
pecially when aiming for high precision to extend
the labels in Wiktionary. For example, the original
7,764 entries can be extended to 18,155 at 65% pre-
cision, 9,594 at 80%, or 27,779 at 52.9%.
Finally, Figures 5 and 6 present qualitative results,
including newly discovered idioms and high scoring
false identifications. Analysis reveals where our sys-
tem has room to improve—errors most often occur
with phrases that are specific to a certain field, such
</bodyText>
<footnote confidence="0.742517333333333">
5We also ran ablations demonstrating that removing each
feature from the Lexical+Graph model hurt performance, but
omit the detailed results for space.
</footnote>
<bodyText confidence="0.977439181818182">
Phrase Definition
feel free You have my permission.
live down To get used to something shameful.
nail down To make something
(e.g. a decision or plan) firm or certain.
make after To chase.
get out To say something with difficulty.
good riddance A welcome departure.
to bad rubbish
as all hell To a great extent or degree; very.
roll around To happen, occur, take place.
</bodyText>
<figureCaption confidence="0.987230307692308">
Figure 5: Newly discovered idioms.
Phrase Definition
put asunder To sunder; disjoin; separate;
disunite; divorce; annul; dissolve.
add up To take a sum.
peel off To remove (an outer layer or
covering, such as clothing).
straighten up To become straight, or straighter.
wild potato The edible root of this plant.
shallow embedding The act of representing one logic
or language with another by
providing a syntactic translation.
Figure 6: High scoring false identifications.
</figureCaption>
<bodyText confidence="0.9912785">
as sports or mathematics, and with phrases whose
words also appear in their definitions.
</bodyText>
<subsectionHeader confidence="0.998264">
6.2 Detection
</subsectionHeader>
<bodyText confidence="0.999968285714286">
Approach We use the Lesk (1986) algorithm to
perform WSD, matching an input phrase p from sen-
tence e to the definition d in Wiktionary that defines
the sense p is being used in. The final classification y
is then assigned to (p, d) by the identification model.
Results Figure 7 shows detection results. The
baseline for this experiment is a model that assigns
the default labels within Wiktionary to the disam-
biguated definition. The Annotated model is the
Lexical+Graph model shown in Figure 3 evaluated
on the annotated data. The +Default setting aug-
ments the identification model by labeling the (p, e)
as idiomatic if either the model or the original label
within Wiktionary identifies it as such.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9948245">
We presented a supervised approach to classifying
definitions as idiomatic or literal that more than dou-
</bodyText>
<page confidence="0.953396">
1420
</page>
<table confidence="0.99902625">
Model Rec. Prec. F1
Default 60.5 1 75.4
Annotated 78.3 76.7 77.5
Annotated+Default 89.2 79.0 83.8
</table>
<figureCaption confidence="0.997793">
Figure 7: Detection results.
</figureCaption>
<bodyText confidence="0.998649181818182">
bles the number of marked idioms in Wiktionary,
even when training on incomplete data. When com-
bined with the Lesk word sense algorithm, this ap-
proach provides a complete idiom detector for any
phrase in the dictionary.
We expect that semi-supervised learning tech-
niques could better recover the missing labels and
boost overall performance. We also think it should
be possible to scale the detection approach, perhaps
with automatic dictionary definition discovery, and
evaluate it on more varied sentence types.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9822396">
The research was supported in part by the Na-
tional Science Foundation (IIS-1115966) and a
Mary Gates Research Scholarship. The authors
thank Nicholas FitzGerald, Sarah Vieweg, and Mark
Yatskar for helpful discussions and feedback.
</bodyText>
<sectionHeader confidence="0.997936" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998535970588235">
J. Birke and A. Sarkar. 2006. A clustering approach
for nearly unsupervised recognition of nonliteral lan-
guage. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13–47.
P. Cook, A. Fazly, and S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the workshop on a broader perspective
on multiword expressions.
P. Cook, A. Fazly, and S. Stevenson. 2008. The
vnc-tokens dataset. In Proceedings of the Language
Resources and Evaluation Conference Workshop To-
wards a Shared Task for Multiword Expressions.
M. Diab and P. Bhutada. 2009. Verb noun construction
mwe token supervised classification. In Proceedings
of the Workshop on Multiword Expressions: Identifica-
tion, Interpretation, Disambiguation and Applications.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
R. Fothergill and T. Baldwin. 2012. Combining re-
sources for mwe-token classification. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
learning, 37(3):277–296.
M. Gedigian, J. Bryant, S. Narayanan, and B. Ciric.
2006. Catching metaphors. In Proceedings of the
Third Workshop on Scalable Natural Language Un-
derstanding.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
Workshop on Multiword Expressions: Identifying and
Exploiting Underlying Properties.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings of Special
Interest Group on the Design of Communication.
L. Li and C. Sporleder. 2009. Classifier combination for
contextual idiom detection without labelled data. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In Computational Linguistics and
Intelligent Text Processing. Springer.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor
identification using verb and noun clustering. In Pro-
ceedings of the International Conference on Computa-
tional Linguistics.
E. Shutova, S. Teufel, and A. Korhonen. 2012. Statisti-
cal metaphor processing. Computational Linguistics,
39(2):301–353.
T. Zesch, C. M¨uller, and I. Gurevych. 2008. Extracting
lexical semantic knowledge from wikipedia and wik-
tionary. In Proceedings of the International Confer-
ence on Language Resources and Evaluation.
</reference>
<page confidence="0.992749">
1421
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924652">
<title confidence="0.998617">Automatic Idiom Identification in Wiktionary</title>
<author confidence="0.944663">Muzny</author>
<affiliation confidence="0.999729">Computer Science &amp; University of</affiliation>
<address confidence="0.996339">Seattle, WA</address>
<abstract confidence="0.999221045454545">Online resources, such as Wiktionary, provide an accurate but incomplete source of idiomatic phrases. In this paper, we study the problem of automatically identifying idiomatic dictionary entries with such resources. We train an idiom classifier on a newly gathered corpus of over 60,000 Wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65%. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Birke</author>
<author>A Sarkar</author>
</authors>
<title>A clustering approach for nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3452" citStr="Birke and Sarkar, 2006" startWordPosition="525" endWordPosition="528">diomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>J. Birke and A. Sarkar. 2006. A clustering approach for nearly unsupervised recognition of nonliteral language. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating wordnetbased measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating wordnetbased measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cook</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the</booktitle>
<contexts>
<context position="3743" citStr="Cook et al. (2007)" startWordPosition="566" endWordPosition="569">, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Formal Problem Definitions Identification For identification, we assume data of the form {((pi, di), yi) : i = 1... n} where pi is the phrase associated with definition di and yi E {literal, idiomatic}. For example, this would include both</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>P. Cook, A. Fazly, and S. Stevenson. 2007. Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. In Proceedings of the workshop on a broader perspective on multiword expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cook</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>The vnc-tokens dataset.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference Workshop Towards</booktitle>
<contexts>
<context position="7165" citStr="Cook et al., 2008" startWordPosition="1135" endWordPosition="1138"> potential omissions. Annotators used the definition of an idiom as a “phrase with a non-compositional meaning” to produce the Annotated Dev and Annotated Test data sets. Figure 1 presents the data statistics. We measured inter-annotator agreement on 1,000 examples. Two annotators marked each dictionary entry as literal, idiomatic, or indeterminable. Less than one half of one percent could not be determined2—the computed kappa was 81.85. Given this high level of agreement, the rest of the data were only labeled by a single annotator, following the methodology used with the VNC-Tokens Dataset (Cook et al., 2008). Detection For detection, we gathered the example sentences provided, when available, for each definition used in our annotated identification data sets. These sentences provide a clean source of develop2The indeterminable pairs were omitted from the data. 1418 ment and test data containing idiomatic and literal phrase usages. In all, there were over 1,300 unique phrases, half of which had more than one possible dictionary definition in Wiktionary. Figure 2 provides the complete statistics. 5 Identification Model For identification, we use a linear model that predicts class y∗ E {literal, idi</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2008</marker>
<rawString>P. Cook, A. Fazly, and S. Stevenson. 2008. The vnc-tokens dataset. In Proceedings of the Language Resources and Evaluation Conference Workshop Towards a Shared Task for Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>P Bhutada</author>
</authors>
<title>Verb noun construction mwe token supervised classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications.</booktitle>
<contexts>
<context position="3772" citStr="Diab and Bhutada (2009)" startWordPosition="571" endWordPosition="574">of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Formal Problem Definitions Identification For identification, we assume data of the form {((pi, di), yi) : i = 1... n} where pi is the phrase associated with definition di and yi E {literal, idiomatic}. For example, this would include both the literal pair ( “leave fo</context>
</contexts>
<marker>Diab, Bhutada, 2009</marker>
<rawString>M. Diab and P. Bhutada. 2009. Verb noun construction mwe token supervised classification. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3723" citStr="Fazly and Stevenson (2006)" startWordPosition="562" endWordPosition="565">10), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Formal Problem Definitions Identification For identification, we assume data of the form {((pi, di), yi) : i = 1... n} where pi is the phrase associated with definition di and yi E {literal, idiomatic}. For example, thi</context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>A. Fazly and S. Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fothergill</author>
<author>T Baldwin</author>
</authors>
<title>Combining resources for mwe-token classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="3840" citStr="Fothergill and Baldwin (2012)" startWordPosition="581" endWordPosition="584">brecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any phrase in the dictionary. 3 Formal Problem Definitions Identification For identification, we assume data of the form {((pi, di), yi) : i = 1... n} where pi is the phrase associated with definition di and yi E {literal, idiomatic}. For example, this would include both the literal pair ( “leave for dead”, “To abandon a person or other living creature that is injur</context>
</contexts>
<marker>Fothergill, Baldwin, 2012</marker>
<rawString>R. Fothergill and T. Baldwin. 2012. Combining resources for mwe-token classification. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="8433" citStr="Freund and Schapire, 1999" startWordPosition="1353" endWordPosition="1356">se p and definition d. We assign the class: Graph-based features use the graph structure of WordNet 3.0 to calculate path distances. Let distance(w, v, rel, n) be the minimum distance via links of type rel in WordNet from a word w to a word v, up to a threshold max integer value n, and 0 otherwise. The features compute: • closest synonym: distance(w, v, synonym, 5) • closest antonym:4 min WEp,VEd y∗ = arg max 9 · 0(p, d, y) min distance(w, v, antonym, 5) y WEp,VEd given features 0(p, d, y) E R&apos; with associated parameters 0 E R&apos;. Learning In this work, we use the averaged Perceptron algorithm (Freund and Schapire, 1999) to perform learning, which was optimized in terms of iterations T, bounded by range [1, 100], by maximizing F-measure on the development set. The models described correspond to the features they use. All models are trained on the same, unannotated training data. Features The features that were developed fall into two categories: lexical and graph-based features. The lexical features were motivated by the intuition that literal phrases are more likely to have closely related words in d to those in p because literal phrases do not break the principle of compositionality. All words compared are </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R.E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gedigian</author>
<author>J Bryant</author>
<author>S Narayanan</author>
<author>B Ciric</author>
</authors>
<title>Catching metaphors.</title>
<date>2006</date>
<booktitle>In Proceedings of the Third Workshop on Scalable Natural Language Understanding.</booktitle>
<contexts>
<context position="3077" citStr="Gedigian et al., 2006" startWordPosition="470" endWordPosition="473">oach allows for scalable detection with no restrictions on the syntactic structure or context of the target phrase. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. 2 Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (sti</context>
</contexts>
<marker>Gedigian, Bryant, Narayanan, Ciric, 2006</marker>
<rawString>M. Gedigian, J. Bryant, S. Narayanan, and B. Ciric. 2006. Catching metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Katz</author>
<author>E Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</booktitle>
<contexts>
<context position="3223" citStr="Katz and Giesbrecht, 2006" startWordPosition="495" endWordPosition="498">inition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. 2 Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill a</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>G. Katz and E. Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of Special Interest Group on the Design of Communication.</booktitle>
<contexts>
<context position="13070" citStr="Lesk (1986)" startWordPosition="2123" endWordPosition="2124">ce. Figure 5: Newly discovered idioms. Phrase Definition put asunder To sunder; disjoin; separate; disunite; divorce; annul; dissolve. add up To take a sum. peel off To remove (an outer layer or covering, such as clothing). straighten up To become straight, or straighter. wild potato The edible root of this plant. shallow embedding The act of representing one logic or language with another by providing a syntactic translation. Figure 6: High scoring false identifications. as sports or mathematics, and with phrases whose words also appear in their definitions. 6.2 Detection Approach We use the Lesk (1986) algorithm to perform WSD, matching an input phrase p from sentence e to the definition d in Wiktionary that defines the sense p is being used in. The final classification y is then assigned to (p, d) by the identification model. Results Figure 7 shows detection results. The baseline for this experiment is a model that assigns the default labels within Wiktionary to the disambiguated definition. The Annotated model is the Lexical+Graph model shown in Figure 3 evaluated on the annotated data. The +Default setting augments the identification model by labeling the (p, e) as idiomatic if either th</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of Special Interest Group on the Design of Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>C Sporleder</author>
</authors>
<title>Classifier combination for contextual idiom detection without labelled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3477" citStr="Li and Sporleder, 2009" startWordPosition="529" endWordPosition="532">o reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Fazly and Stevenson (2006), Cook et al. (2007), and Diab and Bhutada (2009) detect verb/noun idioms (blow smoke). Fothergill and Baldwin (2012) are syntactically unconstrained, but only study Japanese idioms. Although we focus on identifying idiomatic dictionary entries, one advantage of our approach is that it enables syntactically unconstrained token-level detection for any p</context>
</contexts>
<marker>Li, Sporleder, 2009</marker>
<rawString>L. Li and C. Sporleder. 2009. Classifier combination for contextual idiom detection without labelled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1418" citStr="Sag et al., 2002" startWordPosition="201" endWordPosition="204"> These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. 1 Introduction Idiomatic language is common and provides unique challenges for language understanding systems. For example, a diamond in the rough can be the literal unpolished object or a crude but lovable person. Understanding such distinctions is important for many applications, including parsing (Sag et al., 2002) and machine translation (Shutova et al., 2012). We use Wiktionary as a large, but incomplete, reference for idiomatic entries; individual entries can be marked as idiomatic but, in practice, most are not. Using these incomplete annotations as supervision, we train a binary Perceptron classifier for identifying idiomatic dictionary entries. We introduce new lexical and graph-based features that use WordNet and Wiktionary to compute semantic relatedness. This allows us to learn, for example, that the words in the phrase diamond in the rough are more closely related to the words in its literal d</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Computational Linguistics and Intelligent Text Processing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shutova</author>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="3100" citStr="Shutova et al., 2010" startWordPosition="474" endWordPosition="477">e detection with no restrictions on the syntactic structure or context of the target phrase. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. 2 Related Work To the best of our knowledge, this work represents the first attempt to identify dictionary entries as idiomatic and the first to reduce idiom detection to identification via a dictionary. Previous idiom detection systems fall in one of two paradigms: phrase classification, where a phrase p is always idiomatic or literal, e.g. (Gedigian et al., 2006; Shutova et al., 2010), or token classification, where each occurrence of a phrase p can be idiomatic or literal, e.g. (Katz and Giesbrecht, 2006; 1417 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417–1421, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Birke and Sarkar, 2006; Li and Sporleder, 2009). Most previous idiom detection systems have focused on specific syntactic constructions. For instance, Shutova et al. (2010) consider subject/verb (campaign surged) and verb/direct-object idioms (stir excitement) while Faz</context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor identification using verb and noun clustering. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shutova</author>
<author>S Teufel</author>
<author>A Korhonen</author>
</authors>
<title>Statistical metaphor processing.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1465" citStr="Shutova et al., 2012" startWordPosition="208" endWordPosition="211">ion in sentences, by simply using known word sense disambiguation algorithms to match phrases to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. 1 Introduction Idiomatic language is common and provides unique challenges for language understanding systems. For example, a diamond in the rough can be the literal unpolished object or a crude but lovable person. Understanding such distinctions is important for many applications, including parsing (Sag et al., 2002) and machine translation (Shutova et al., 2012). We use Wiktionary as a large, but incomplete, reference for idiomatic entries; individual entries can be marked as idiomatic but, in practice, most are not. Using these incomplete annotations as supervision, we train a binary Perceptron classifier for identifying idiomatic dictionary entries. We introduce new lexical and graph-based features that use WordNet and Wiktionary to compute semantic relatedness. This allows us to learn, for example, that the words in the phrase diamond in the rough are more closely related to the words in its literal definition than the idiomatic one. Experiments d</context>
</contexts>
<marker>Shutova, Teufel, Korhonen, 2012</marker>
<rawString>E. Shutova, S. Teufel, and A. Korhonen. 2012. Statistical metaphor processing. Computational Linguistics, 39(2):301–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>C M¨uller</author>
<author>I Gurevych</author>
</authors>
<title>Extracting lexical semantic knowledge from wikipedia and wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>T. Zesch, C. M¨uller, and I. Gurevych. 2008. Extracting lexical semantic knowledge from wikipedia and wiktionary. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
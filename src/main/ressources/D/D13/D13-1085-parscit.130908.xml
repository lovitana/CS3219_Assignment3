<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019794">
<title confidence="0.99263">
Microblog Entity Linking by Leveraging Extra Posts
</title>
<author confidence="0.999589">
Yuhang Guo, Bing Qin; Ting Liu, Sheng Li
</author>
<affiliation confidence="0.997125666666667">
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.982209">
{yhguo, bqin; tliu, sli}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.992952" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.799874071428571">
Linking name mentions in microblog posts to
a knowledge base, namely microblog entity
linking, is useful for text mining tasks on mi-
croblog. Entity linking in long text has been
well studied in previous works. However few
work has focused on short text such as mi-
croblog post. Microblog posts are short and
noisy. Previous method can extract few fea-
tures from the post context. In this paper we
propose to use extra posts for the microblog
entity linking task. Experimental results show
that our proposed method significantly im-
proves the linking accuracy over traditional
methods by 8.3% and 7.5% respectively.
</figureCaption>
<sectionHeader confidence="0.998829" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999701083333333">
Microblogging services (e.g. Twitter) are attracting
millions of users to share and exchange their ideas
and opinions. Millions of new microblog posts are
generated on such open broadcasting platforms ev-
ery day 1. Microblog provides a fruitful and instant
channel of global information publication and acqui-
sition.
A necessary step for the information acquisition
on microblog is to identify which entities a post is
about. Such identification can be challenging be-
cause the entity mention may be ambiguous. Let’s
begin with a real post from Twitter.
</bodyText>
<footnote confidence="0.8075266">
(1) No excuse for floods tax, says Abbott
URL
*Corresponding author
1See http://blog.twitter.com/2011/06/ 200-million-tweets-
per-day.html.
</footnote>
<bodyText confidence="0.9813958125">
This post is about an Australia political lead-
er, Tony Abbot, and his opinion on flood tax
policy. To understand that this post mentions
Tony Abbot is not trivial because the name Ab-
bot can refer to many people and organization-
s. In the Wikipedia page of Abbott, there list-
s more than 20 Abbotts, such as baseball player
Jim Abbott, actor Bud Abbott and company
Abbott Laboratories, etc..
Given a knowledge base (KB) (e.g. Wikipedia),
entity linking is the task to identify the referent KB
entity of a target name mention in plain text. Most
current entity linking techniques are designed for
long text such as news/blog articles (Mihalcea and
Csomai, 2007; Cucerzan, 2007; Milne and Witten,
2008; Han and Sun, 2011; Zhang et al., 2011; Shen
et al., 2012; Kulkarni et al., 2009; Ratinov et al.,
2011). Entity linking for microblog posts has not
been well studied.
Comparing with news/blog articles, microblog
posts are:
short each post contains no more than 140 charac-
ters;
fresh the new entity-related content may have not
been included in the knowledge base;
informal acronyms and spoken language writing
style are common.
Due to these properties, few feature can be ex-
tracted from a post. Without enough features, pre-
vious entity linking methods may fail. In order to
overcome the feature sparseness, we turn to another
property of microblog:
</bodyText>
<page confidence="0.982212">
863
</page>
<bodyText confidence="0.955644113207547">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863–868,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
over baselines, which means that entity linking sys-
tem on microblog can be improved by leveraging ex-
tra posts. The results also show that GMEL outper-
forms CEMEL significantly.
We summarize our contributions as follows.
redundancy For each day, over 340M short mes-
sages are posted in twitter. Similar information
may be posted in different expressions.
For example, we find the following post,
(2) Julia Gillard and Tony Abbott on
the flood levy just after 8.30am on
@612brisbane!
The content of post (2) is highly related to post
(1). In contrast to the confusing post (1), the text
in post (2) explicitly indicates that the Abbott here
refers to the Australian political leader. This inspires
us to bridge the confusing post and the knowledge
base with other posts.
In this paper, we approach the microblog entity
linking by leveraging extra posts. A straightforward
method is to expand the post context with similar
posts, which we call Context-Expansion-based Mi-
croblog Entity Linking (CEMEL). In this method,
we first construct a query with the given post and
then search for it in a collection of posts. From the
search result, we select the most similar posts for the
context expansion. The disambiguation will benefit
from the extra posts if, hopefully, they are related
to the given post in content and include explicit fea-
tures for the disambiguation.
Furthermore, we propose a Graph-based Mi-
croblog Entity Linking (GMEL) method. In contrast
to CEMEL, the extra posts in GMEL are not directly
added into the context. Instead, they are represented
as nodes in a graph, and weighted by their similarity
with the target post. We use an iterative algorithm
in this graph to propagate the entity weights through
the edges between the post nodes.
We conduct experiments on real microblog da-
ta which we harvested from Twitter. Current enti-
ty linking corpus, such as the TAC-KBP data (M-
cNamee and Dang, 2009), mainly focuses on long
text. And few microblog entity linking corpus is
publicly available. In this work, we manually anno-
tated a microblog entity linking corpus. This corpus
inherit the target names from TAC-KBP2009. So it
is comparable with the TAC-KBP2009 corpus.
Experimental results show that the performance
of previous methods drops on microblog posts com-
paring with on long text. Both of CEMEL and
GMEL can significantly improve the performance
</bodyText>
<listItem confidence="0.995564">
• We propose a context-expansion-based and a
graph-based method for microblog entity link-
ing by leveraging extra posts.
• We annotate a microblog entity linking corpus
which is comparable to an existing long text
corpus.
• We show the inefficiency of previous method
on the microblog corpus and our method can
significantly improve the results.
</listItem>
<sectionHeader confidence="0.895425" genericHeader="method">
2 Task defination
</sectionHeader>
<bodyText confidence="0.999980684210526">
The microblog entity linking task is that, for a name
mention in a microblog post, the system is to find the
referent entity of the name in a knowledge base, or
return a NIL mark if the entity is absence from the
knowledge base. This definition is close to the en-
tity linking task in the TAC-KBP evaluation (Ji and
Grishman, 2011) except for the context of the target
name is microblog post whereas in TAC-KBP the
context is news article or web log.
Several related tasks have been studied on mi-
croblog posts. In Meij et al. (2012)’s work, they
link a post, rather than a name mention in the post,
to relevant Wikipedia concepts. Guo et al. (2013a)
and Liu et al. (2013) define entity linking as to first
detect all the mentions in a post and then link the
mentions to the knowledge base. In contrast, our
definition (as well as the TAC-KBP definition) fo-
cuses on a concerned name mention across different
posts/documents.
</bodyText>
<sectionHeader confidence="0.980812" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.998242857142857">
A typical entity linking system can be broken down
into two steps:
candidate generation This step narrows down the
candidate entity range from any entity in the
world to a limited set.
candidate ranking This step ranks the candidates
and output the top ranked entity as the result.
</bodyText>
<page confidence="0.997346">
864
</page>
<figureCaption confidence="0.9941014">
Figure 1: An example of the GMEL graph. p1 ... p4 are
post nodes and c1 ... c3 are candidate entity nodes. Each
post node is connected to the corresponding candidate n-
odes from the knowledge base. The edges between the
nodes are weighted by the similarity between them.
</figureCaption>
<bodyText confidence="0.984481373134328">
In this paper, we use the candidate generation
method described in Guo et al.(2013). For the candi-
date ranking, we use a Vector Space Model (VSM)
and a Learning to Rank (LTR) as baselines. VSM
is an unsupervised method and LTR is a supervised
method. Both of them have achieved the state-of-
the-art performances in the TAC-KBP evaluations.
The major challenge in microblog entity linking
is the lack of context in the post. An ideal solu-
tion is to expand the context with the posts which
contain the same entity. However, automatically
judging whether a name mention in two documents
refers to the same entity, namely cross document co-
reference, is not trivial. Here our solution is to rank
the posts by their possibility of co-reference to the
target one and select the most possible co-referent
posts for the expansion.
CEMEL is based on the assumption that, given a
name and two posts where the name is mentioned,
the higher similarity between the posts the high-
er possibility of their co-reference and that the co-
referent posts may contains useful features for the
disambiguation. However, two literally similar posts
may not be co-referent. If such non co-referent post
is expanded to the context, noises may be included.
Take the following post as an example.
(3) AG Abbott says that bullets have
crossed the border from Mexico to
Texas at least four times. URL
This post is similar to post (1) because they both
contains “says” and “URL”. But the Abbott in post
(3) refers to the Texas Attorney General Greg Ab-
bott. In this mean, the expanded context in post (3)
could mislead the disambiguation for post (1). Such
noise can be controlled by setting a strict number of
posts to expand the context or weighting the contri-
bution of this post to the target one.
Our CEMEL method consists of the following
steps: First we construct a query with the terms from
the target post. Second we search for the query in a
microblog post collection using a common informa-
tion retrieval model such as the vector space model.
Note that here we limit the searched posts must con-
tain the target name mention. Then we expand the
target post with top N similar posts and use a typical
entity linking method (such as VSM and LTR) with
the expanded context.
Figure 1 illustrates the graph of GMEL. Each n-
ode of this graph represents an candidate entity (e.g.
c1 ... c3) or a post of the given target name (e.g.
p1 ... p4) In this graph, each node represents an en-
tity or a post of the given target name. Between each
pair of post nodes, each pair of entity nodes and each
post node and its candidate entity nodes, there is an
edge. The edge is weighted by the similarity be-
tween the two linked nodes. Entity nodes are labeled
by themselves and candidate nodes are initialized as
unlabeled nodes. For the edges between post node
pairs and entity node pairs, we use cosine similari-
ty. For the edges between a post node and its can-
didate entity nodes, we use the score given by tra-
ditional entity linking methods. We use an iterative
algorithm on this graph to propagate the labels from
the entity nodes to the post nodes. We adapt Label
Propagation (LP) (Zhu and Ghahramani, 2002) and
Modified Adsorption (MAD) (Talukdar and Pereira,
2010) for the iteration over the graph.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999346">
4.1 Data Annotation
</subsectionHeader>
<bodyText confidence="0.999834125">
Till now, few microblog entity linking data is pub-
licly available. In this work, we manually annotate
a data set on microblog posts2. We collect 15.6 mil-
lion microblog posts in Twitter dated from January
23 to February 8, 2011. In order to compare with ex-
isting entity linking on long text, we select a subset
of target names from TAC-KBP2009 and inherit the
knowledge base in the TAC-KBP evaluation. The
</bodyText>
<footnote confidence="0.997719">
2We published this data so that researchers can reproduce
our results.
</footnote>
<page confidence="0.997306">
865
</page>
<figureCaption confidence="0.999223333333333">
Figure 2: Percentage of the co-reference posts in the top
N similar posts
Figure 3: Impact of expansion post number in CEMEL
</figureCaption>
<bodyText confidence="0.9999555625">
TAC-KBP2009 data set includes 513 target names.
We search for all the target names in the post col-
lection and get 26,643 matches. We randomly sam-
ple 120 posts for each of the top 30 most frequently
matched target names and filter out non-English and
overly short (i.e. less than 3 words) posts. Then
we get 2,258 posts for 25 target names and manual-
ly link the target name mentions in the posts to the
TAC-KBP knowledge base.
In order to evaluate the assumption in CEMEL:
similar posts tend to co-reference, we randomly s-
elect 10 posts for 5 target names respectively and
search for the posts in the post collection. From
the search result of each of the 50 posts, we select
the top 20 posts and manually annotate if they co-
reference with the query post.
</bodyText>
<subsectionHeader confidence="0.984098">
4.2 Settings
</subsectionHeader>
<bodyText confidence="0.9893025">
We generate candidates with the method described
in (Guo et al., 2013b) and use Vector Space Mod-
el (VSM) (Varma et al., 2009) and Learning to Rank
(LTR) (Zheng et al., 2010) as the ranking model. We
</bodyText>
<figureCaption confidence="0.994909">
Figure 4: Accuracy of GMEL with different rate of extra
post nodes
</figureCaption>
<bodyText confidence="0.999890941176471">
use Lucene and ListNet with default settings for the
VSM and LTR implementation respectively. We use
bigram feature for VSM and the feature set of (Chen
et al., 2011) for LTR. LTR is evaluated with 10-fold
cross validation. Given a target name, the GMEL
graph includes all the evaluation posts as well as a
set of extra post nodes searched from the post collec-
tion with the query of the target name. We filter out
determiners, interjections, punctuations, emoticon-
s, discourse markers and URLs in the posts with a
twitter part-of-speech tagger (Owoputi et al., 2013).
The similarity between a post and its candidate en-
tities is set with the score given by VSM or LTR
and the similarity between other nodes is set with the
corresponding cosine similarity. We employ junto3
with default settings for the iterative algorithm im-
plementation .
</bodyText>
<subsectionHeader confidence="0.884526">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999902846153846">
Figure 2 shows the relationship between similari-
ty and co-reference. From this figure we can see
that the percentage decreases with the growth of N.
When the N is up to 10, about 60% of the similar
posts co-reference with the query post and the de-
crease speed slows down. The Pearson correlation
coefficient between the percentage and the number
of top N is -0.843, which shows a significant corre-
lation between the two variables (with p-value 0.01
under t-test).
Figure 3 shows the impact of the extra post num-
ber for the context expansion in CEMEL. We can see
that the accuracies of VSM and LTR are improved
</bodyText>
<footnote confidence="0.998335">
3See https://github.com/parthatalukdar/junto
</footnote>
<page confidence="0.995226">
866
</page>
<figureCaption confidence="0.999832333333333">
Figure 5: Label entropy of GMEL with different rate of
extra post nodes
Figure 6: Accuracy of the systems
</figureCaption>
<bodyText confidence="0.999187878787879">
by CEMEL. The improvements peak with 5-10 ex-
tra posts. Then more extra posts will pull down the
accuracy.
Figure 4 shows the accuracy of GMEL. The x-axis
is the rate of the extra post number over the evalu-
ation post number. We can see that the accuracy of
MAD increases with the number of extra post nodes
at first and then turns to be stable. The accuracy of
LP increases at first and drops when more extra posts
are added into the graph.
Figure 5 shows the information entropy of the la-
bels in LP and MAD. The curves show that the pre-
diction of LP tends to converge into a small number
of labels. This is because LP prefers smoothing la-
belings over the graph (Talukdar and Pereira, 2010).
We also evaluate our baselines on TAC-KBP2009
data set (LTR is trained on TAC-KBP2010 data set).
The accuracy of VSM and LTR are 0.8338 and
0.8372 respectively, which are comparable with the
state-of-the-art result (Hachey et al., 2013).
Figure 6 shows the performances of the systems
on the microblog data. We set the optimal expansion
post number of CEMEL and use MAD algorithm for
GMEL with all searched extra post nodes. From this
figure we can see that the results of VSM and LTR
baselines are comparable and both of them are sig-
nificantly lower than that on TAC-KBP2009 data.
CEMEL improves the VSM and LTR baselines by
4.3% and 2.7% respectively. GMEL improves VSM
and LTR by 8.3% and 7.5% respectively. The results
of GMEL are also significantly better than CEMEL.
All of the improvements are significant under Z-test
with p &lt; 0.05.
</bodyText>
<sectionHeader confidence="0.997789" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998846818181818">
In this paper we approach microblog entity linking
by leveraging extra posts. We propose a context-
expansion-based and a graph-based method. Exper-
imental results on our data set show that the per-
formance of traditional method drops on the mi-
croblog data. The graph-based method outperform-
s the context-expansion-based method and both of
them significantly improve the accuracy of tradition-
al methods. In the graph-based method the modified
adsorption algorithm performs better than the label
propagation algorithm.
</bodyText>
<sectionHeader confidence="0.990764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999065">
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61273321, 61073126, 61133012 and the National
863 Leading Technology Research Project via grant
2012AA011102. We would like to thank to Wanx-
iang Che, Ruiji Fu, Yanyan Zhao, Wei Song and
several anonymous reviewers for their constructive
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9914058">
Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.
2011. A toolkit for knowledge base population. In
SIGIR, pages 1267–1268.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Method-
s in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708–716, Prague, Czech Republic, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.990647">
867
</page>
<reference confidence="0.998524813084112">
Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013a. To link or not to link? a study on end-to-
end tweet entity linking. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1020–1030, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng
Li. 2013b. Improving candidate generation for entity
linking. In Elisabeth Mtais, Farid Meziane, Mohamad
Saraee, Vijayan Sugumaran, and Sunil Vadera, edi-
tors, Natural Language Processing and Information
Systems, volume 7934 of Lecture Notes in Computer
Science, pages 225–236. Springer Berlin Heidelberg.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating en-
tity linking with wikipedia. Artificial Intelligence,
194(0):130 – 150. ¡ce:title¿Artificial Intelligence,
Wikipedia and Semi-Structured Resources¡/ce:title¿.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Techologies, pages 945–954, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1148–1158, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ’09,
pages 457–466, New York, NY, USA. ACM.
Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu
Wei, and Yi Lu. 2013. Entity linking for tweets. In
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics.
P. McNamee and H.T. Dang. 2009. Overview of
the tac 2009 knowledge base population track. In
Proceedings of the Second Text Analysis Conference
(TAC2009).
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ’12, pages 563–
572, New York, NY, USA. ACM.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
’07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233–242, New York, NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In CIKM ’08: Proceeding of the 17th
ACM conference on Information and knowledge man-
agement, pages 509–518, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In NAACL2013,
pages 380–390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computation-
al Linguistics: Human Language Technologies, pages
1375–1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-
edge base via semantic knowledge. In Proceedings of
the 21st international conference on World Wide We-
b, WWW ’12, pages 449–458, New York, NY, USA.
ACM.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473–1481, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,
Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-
thi Reddy, Karuna Kumar, and Nitin Maganti. 2009.
Iiit hyderabad at tac 2009. In Proceedings of the Sec-
ond Text Analysis Conference (TAC 2009), Gaithers-
burg, Maryland, USA, November.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011. Entity linking with effective acronym expan-
sion, instance selection, and topic modeling. In Toby
Walsh, editor, IJCAI 2011, pages 1909–1914.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In NAACL2010, pages 483–491, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
</reference>
<page confidence="0.997566">
868
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963521">
<title confidence="0.998951">Microblog Entity Linking by Leveraging Extra Posts</title>
<author confidence="0.99274">Bing Liu Guo</author>
<author confidence="0.99274">Sheng</author>
<affiliation confidence="0.996912">Research Center for Social Computing and Information School of Computer Science and Harbin Institute of Technology,</affiliation>
<abstract confidence="0.998549266666666">Linking name mentions in microblog posts to a knowledge base, namely microblog entity linking, is useful for text mining tasks on microblog. Entity linking in long text has been well studied in previous works. However few work has focused on short text such as microblog post. Microblog posts are short and noisy. Previous method can extract few features from the post context. In this paper we propose to use extra posts for the microblog entity linking task. Experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3% and 7.5% respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Suzanne Tamang</author>
<author>Adam Lee</author>
<author>Heng Ji</author>
</authors>
<title>A toolkit for knowledge base population.</title>
<date>2011</date>
<booktitle>In SIGIR,</booktitle>
<pages>1267--1268</pages>
<contexts>
<context position="12456" citStr="Chen et al., 2011" startWordPosition="2109" endWordPosition="2112"> the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emoticons, discourse markers and URLs in the posts with a twitter part-of-speech tagger (Owoputi et al., 2013). The similarity between a post and its candidate entities is set with the score given by VSM or LTR and the similarity between other nodes is set with the corresponding cosine similarity. We employ j</context>
</contexts>
<marker>Chen, Tamang, Lee, Ji, 2011</marker>
<rawString>Zheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji. 2011. A toolkit for knowledge base population. In SIGIR, pages 1267–1268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>708--716</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2271" citStr="Cucerzan, 2007" startWordPosition="356" endWordPosition="357">ot, and his opinion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to o</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708–716, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Guo</author>
<author>Ming-Wei Chang</author>
<author>Emre Kiciman</author>
</authors>
<title>To link or not to link? a study on end-toend tweet entity linking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1020--1030</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="6520" citStr="Guo et al. (2013" startWordPosition="1058" endWordPosition="1061">ask is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. 864 Figure 1: An example of the </context>
<context position="12093" citStr="Guo et al., 2013" startWordPosition="2043" endWordPosition="2046">d filter out non-English and overly short (i.e. less than 3 words) posts. Then we get 2,258 posts for 25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We </context>
</contexts>
<marker>Guo, Chang, Kiciman, 2013</marker>
<rawString>Stephen Guo, Ming-Wei Chang, and Emre Kiciman. 2013a. To link or not to link? a study on end-toend tweet entity linking. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1020–1030, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhang Guo</author>
<author>Bing Qin</author>
<author>Yuqin Li</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Improving candidate generation for entity linking.</title>
<date>2013</date>
<booktitle>Natural Language Processing and Information Systems,</booktitle>
<volume>7934</volume>
<pages>225--236</pages>
<editor>In Elisabeth Mtais, Farid Meziane, Mohamad Saraee, Vijayan Sugumaran, and Sunil Vadera, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6520" citStr="Guo et al. (2013" startWordPosition="1058" endWordPosition="1061">ask is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. 864 Figure 1: An example of the </context>
<context position="12093" citStr="Guo et al., 2013" startWordPosition="2043" endWordPosition="2046">d filter out non-English and overly short (i.e. less than 3 words) posts. Then we get 2,258 posts for 25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We </context>
</contexts>
<marker>Guo, Qin, Li, Liu, Li, 2013</marker>
<rawString>Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and Sheng Li. 2013b. Improving candidate generation for entity linking. In Elisabeth Mtais, Farid Meziane, Mohamad Saraee, Vijayan Sugumaran, and Sunil Vadera, editors, Natural Language Processing and Information Systems, volume 7934 of Lecture Notes in Computer Science, pages 225–236. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<booktitle>150. ¡ce:title¿Artificial Intelligence, Wikipedia and Semi-Structured Resources¡/ce:title¿.</booktitle>
<volume>194</volume>
<issue>0</issue>
<contexts>
<context position="14833" citStr="Hachey et al., 2013" startWordPosition="2524" endWordPosition="2527">t first and then turns to be stable. The accuracy of LP increases at first and drops when more extra posts are added into the graph. Figure 5 shows the information entropy of the labels in LP and MAD. The curves show that the prediction of LP tends to converge into a small number of labels. This is because LP prefers smoothing labelings over the graph (Talukdar and Pereira, 2010). We also evaluate our baselines on TAC-KBP2009 data set (LTR is trained on TAC-KBP2010 data set). The accuracy of VSM and LTR are 0.8338 and 0.8372 respectively, which are comparable with the state-of-the-art result (Hachey et al., 2013). Figure 6 shows the performances of the systems on the microblog data. We set the optimal expansion post number of CEMEL and use MAD algorithm for GMEL with all searched extra post nodes. From this figure we can see that the results of VSM and LTR baselines are comparable and both of them are significantly lower than that on TAC-KBP2009 data. CEMEL improves the VSM and LTR baselines by 4.3% and 2.7% respectively. GMEL improves VSM and LTR by 8.3% and 7.5% respectively. The results of GMEL are also significantly better than CEMEL. All of the improvements are significant under Z-test with p &lt; 0</context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with wikipedia. Artificial Intelligence, 194(0):130 – 150. ¡ce:title¿Artificial Intelligence, Wikipedia and Semi-Structured Resources¡/ce:title¿.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
</authors>
<title>A generative entitymention model for linking entities with knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Techologies,</booktitle>
<pages>945--954</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Han, Le Sun, 2011</marker>
<rawString>Xianpei Han and Le Sun. 2011. A generative entitymention model for linking entities with knowledge base. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Techologies, pages 945–954, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1148--1158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6204" citStr="Ji and Grishman, 2011" startWordPosition="1000" endWordPosition="1003">roblog entity linking by leveraging extra posts. • We annotate a microblog entity linking corpus which is comparable to an existing long text corpus. • We show the inefficiency of previous method on the microblog corpus and our method can significantly improve the results. 2 Task defination The microblog entity linking task is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Metho</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1148–1158, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’09,</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2376" citStr="Kulkarni et al., 2009" startWordPosition="374" endWordPosition="377">rivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proceedings of the 2013 Con</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’09, pages 457–466, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Yitong Li</author>
<author>Haocheng Wu</author>
<author>Ming Zhou</author>
<author>Furu Wei</author>
<author>Yi Lu</author>
</authors>
<title>Entity linking for tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6544" citStr="Liu et al. (2013)" startWordPosition="1063" endWordPosition="1066"> mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. 864 Figure 1: An example of the GMEL graph. p1 ... p4 ar</context>
</contexts>
<marker>Liu, Li, Wu, Zhou, Wei, Lu, 2013</marker>
<rawString>Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, Furu Wei, and Yi Lu. 2013. Entity linking for tweets. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>H T Dang</author>
</authors>
<title>Overview of the tac 2009 knowledge base population track.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference (TAC2009).</booktitle>
<contexts>
<context position="5056" citStr="McNamee and Dang, 2009" startWordPosition="808" endWordPosition="812">iven post in content and include explicit features for the disambiguation. Furthermore, we propose a Graph-based Microblog Entity Linking (GMEL) method. In contrast to CEMEL, the extra posts in GMEL are not directly added into the context. Instead, they are represented as nodes in a graph, and weighted by their similarity with the target post. We use an iterative algorithm in this graph to propagate the entity weights through the edges between the post nodes. We conduct experiments on real microblog data which we harvested from Twitter. Current entity linking corpus, such as the TAC-KBP data (McNamee and Dang, 2009), mainly focuses on long text. And few microblog entity linking corpus is publicly available. In this work, we manually annotated a microblog entity linking corpus. This corpus inherit the target names from TAC-KBP2009. So it is comparable with the TAC-KBP2009 corpus. Experimental results show that the performance of previous methods drops on microblog posts comparing with on long text. Both of CEMEL and GMEL can significantly improve the performance • We propose a context-expansion-based and a graph-based method for microblog entity linking by leveraging extra posts. • We annotate a microblog</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>P. McNamee and H.T. Dang. 2009. Overview of the tac 2009 knowledge base population track. In Proceedings of the Second Text Analysis Conference (TAC2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Wouter Weerkamp</author>
<author>Maarten de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>563--572</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 563– 572, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2255" citStr="Mihalcea and Csomai, 2007" startWordPosition="352" endWordPosition="355"> political leader, Tony Abbot, and his opinion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fai</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 233–242, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2295" citStr="Milne and Witten, 2008" startWordPosition="358" endWordPosition="361">ion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature spar</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. Learning to link with wikipedia. In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 509–518, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In NAACL2013,</booktitle>
<pages>380--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In NAACL2013, pages 380–390, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1375--1384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2399" citStr="Ratinov et al., 2011" startWordPosition="378" endWordPosition="381"> Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proceedings of the 2013 Conference on Empirical Me</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1375–1384, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Shen</author>
<author>Jianyong Wang</author>
<author>Ping Luo</author>
<author>Min Wang</author>
</authors>
<title>Linden: linking named entities with knowledge base via semantic knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web, WWW ’12,</booktitle>
<pages>449--458</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2353" citStr="Shen et al., 2012" startWordPosition="370" endWordPosition="373">Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proc</context>
</contexts>
<marker>Shen, Wang, Luo, Wang, 2012</marker>
<rawString>Wei Shen, Jianyong Wang, Ping Luo, and Min Wang. 2012. Linden: linking named entities with knowledge base via semantic knowledge. In Proceedings of the 21st international conference on World Wide Web, WWW ’12, pages 449–458, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Fernando Pereira</author>
</authors>
<title>Experiments in graph-based semi-supervised learning methods for class-instance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1473--1481</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10583" citStr="Talukdar and Pereira, 2010" startWordPosition="1775" endWordPosition="1778">es, there is an edge. The edge is weighted by the similarity between the two linked nodes. Entity nodes are labeled by themselves and candidate nodes are initialized as unlabeled nodes. For the edges between post node pairs and entity node pairs, we use cosine similarity. For the edges between a post node and its candidate entity nodes, we use the score given by traditional entity linking methods. We use an iterative algorithm on this graph to propagate the labels from the entity nodes to the post nodes. We adapt Label Propagation (LP) (Zhu and Ghahramani, 2002) and Modified Adsorption (MAD) (Talukdar and Pereira, 2010) for the iteration over the graph. 4 Experiment 4.1 Data Annotation Till now, few microblog entity linking data is publicly available. In this work, we manually annotate a data set on microblog posts2. We collect 15.6 million microblog posts in Twitter dated from January 23 to February 8, 2011. In order to compare with existing entity linking on long text, we select a subset of target names from TAC-KBP2009 and inherit the knowledge base in the TAC-KBP evaluation. The 2We published this data so that researchers can reproduce our results. 865 Figure 2: Percentage of the co-reference posts in th</context>
<context position="14595" citStr="Talukdar and Pereira, 2010" startWordPosition="2486" endWordPosition="2489">re extra posts will pull down the accuracy. Figure 4 shows the accuracy of GMEL. The x-axis is the rate of the extra post number over the evaluation post number. We can see that the accuracy of MAD increases with the number of extra post nodes at first and then turns to be stable. The accuracy of LP increases at first and drops when more extra posts are added into the graph. Figure 5 shows the information entropy of the labels in LP and MAD. The curves show that the prediction of LP tends to converge into a small number of labels. This is because LP prefers smoothing labelings over the graph (Talukdar and Pereira, 2010). We also evaluate our baselines on TAC-KBP2009 data set (LTR is trained on TAC-KBP2010 data set). The accuracy of VSM and LTR are 0.8338 and 0.8372 respectively, which are comparable with the state-of-the-art result (Hachey et al., 2013). Figure 6 shows the performances of the systems on the microblog data. We set the optimal expansion post number of CEMEL and use MAD algorithm for GMEL with all searched extra post nodes. From this figure we can see that the results of VSM and LTR baselines are comparable and both of them are significantly lower than that on TAC-KBP2009 data. CEMEL improves t</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>Partha Pratim Talukdar and Fernando Pereira. 2010. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473–1481, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasudeva Varma</author>
<author>Vijay Bharat</author>
<author>Sudheer Kovelamudi</author>
<author>Praveen Bysani</author>
<author>Santosh GSK</author>
<author>Kiran Kumar N</author>
<author>Kranthi Reddy</author>
<author>Karuna Kumar</author>
<author>Nitin Maganti</author>
</authors>
<title>Iiit hyderabad at tac</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference (TAC 2009),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="12149" citStr="Varma et al., 2009" startWordPosition="2054" endWordPosition="2057">than 3 words) posts. Then we get 2,258 posts for 25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emo</context>
</contexts>
<marker>Varma, Bharat, Kovelamudi, Bysani, GSK, N, Reddy, Kumar, Maganti, 2009</marker>
<rawString>Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi, Praveen Bysani, Santosh GSK, Kiran Kumar N, Kranthi Reddy, Karuna Kumar, and Nitin Maganti. 2009. Iiit hyderabad at tac 2009. In Proceedings of the Second Text Analysis Conference (TAC 2009), Gaithersburg, Maryland, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Yan Chuan Sim</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Entity linking with effective acronym expansion, instance selection, and topic modeling.</title>
<date>2011</date>
<pages>1909--1914</pages>
<editor>In Toby Walsh, editor, IJCAI</editor>
<contexts>
<context position="2334" citStr="Zhang et al., 2011" startWordPosition="366" endWordPosition="369"> this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of </context>
</contexts>
<marker>Zhang, Sim, Su, Tan, 2011</marker>
<rawString>Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan. 2011. Entity linking with effective acronym expansion, instance selection, and topic modeling. In Toby Walsh, editor, IJCAI 2011, pages 1909–1914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to link entities with knowledge base.</title>
<date>2010</date>
<booktitle>In NAACL2010,</booktitle>
<pages>483--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="12197" citStr="Zheng et al., 2010" startWordPosition="2063" endWordPosition="2066"> 25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emoticons, discourse markers and URLs in the posts </context>
</contexts>
<marker>Zheng, Li, Huang, Zhu, 2010</marker>
<rawString>Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge base. In NAACL2010, pages 483–491, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, Technical Report CMUCALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="10524" citStr="Zhu and Ghahramani, 2002" startWordPosition="1767" endWordPosition="1770">ity nodes and each post node and its candidate entity nodes, there is an edge. The edge is weighted by the similarity between the two linked nodes. Entity nodes are labeled by themselves and candidate nodes are initialized as unlabeled nodes. For the edges between post node pairs and entity node pairs, we use cosine similarity. For the edges between a post node and its candidate entity nodes, we use the score given by traditional entity linking methods. We use an iterative algorithm on this graph to propagate the labels from the entity nodes to the post nodes. We adapt Label Propagation (LP) (Zhu and Ghahramani, 2002) and Modified Adsorption (MAD) (Talukdar and Pereira, 2010) for the iteration over the graph. 4 Experiment 4.1 Data Annotation Till now, few microblog entity linking data is publicly available. In this work, we manually annotate a data set on microblog posts2. We collect 15.6 million microblog posts in Twitter dated from January 23 to February 8, 2011. In order to compare with existing entity linking on long text, we select a subset of target names from TAC-KBP2009 and inherit the knowledge base in the TAC-KBP evaluation. The 2We published this data so that researchers can reproduce our result</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, Technical Report CMUCALD-02-107, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
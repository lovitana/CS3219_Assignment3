<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001277">
<title confidence="0.971933">
Appropriately Incorporating Statistical Significance in PMI
</title>
<author confidence="0.703208">
Om P. Damani and Shweta Ghonge
</author>
<affiliation confidence="0.737443">
IIT Bombay
India
</affiliation>
<email confidence="0.984948">
{damani,shwetaghonge}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.995268" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998970642857143">
Two recent measures incorporate the notion of
statistical significance in basic PMI formula-
tion. In some tasks, we find that the new mea-
sures perform worse than the PMI. Our anal-
ysis shows that while the basic ideas in incor-
porating statistical significance in PMI are rea-
sonable, they have been applied slightly inap-
propriately. By fixing this, we get new mea-
sures that improve performance over not just
PMI but on other popular co-occurrence mea-
sures as well. In fact, the revised measures
perform reasonably well compared with more
resource intensive non co-occurrence based
methods also.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856627906977">
The notion of word association is used in many lan-
guage processing and information retrieval appli-
cations and it is important to have low-cost, high-
quality association measures. Lexical co-occurrence
based word association measures are popular be-
cause they are computationally efficient and they can
be applied to any language easily. One of the most
popular co-occurrence measure is Pointwise Mutual
Information (PMI) (Church and Hanks, 1989).
One of the limitations of PMI is that it only works
with relative probabilities and ignores the absolute
amount of evidence. To overcome this, recently two
new measures have been proposed that incorporate
the notion of statistical significance in basic PMI
formulation. In (Washtell and Markert, 2009), sta-
tistical significance is introduced in PMI,ig by mul-
tiplying PMI value with the square root of the ev-
idence. In contrast, in (Damani, 2013), cPMId is
introduced by bounding the probability of observing
a given deviation between a given word pair’s co-
occurrence count and its expected value under a null
model where with each word a global unigram gen-
eration probability is associated. In Table 1, we give
the definitions of PMI, PMI,ig, and cPMId.
While these new measures perform better than
PMI on some of the tasks, on many other tasks,
we find that the new measures perform worse than
the PMI. In Table 3, we show how these measures
perform compared to PMI on four different tasks.
We find that PMI,ig degrades performance in three
out of these four tasks while cPMId degrades per-
formance in two out of these four tasks. The ex-
perimental details and discussion are given in Sec-
tion 4.2.
Our analysis shows that while the basic ideas in
incorporating statistical significance are reasonable,
they have been applied slightly inappropriately. By
fixing this, we get new measures that improve per-
formance over not just PMI, but also on other pop-
ular co-occurrence measures on most of these tasks.
In fact, the revised measures perform reasonably
well compared with more resource intensive non co-
occurrence based methods also.
</bodyText>
<sectionHeader confidence="0.835354" genericHeader="introduction">
2 Adapting PMI for Statistical Significance
</sectionHeader>
<bodyText confidence="0.999707">
In (Washtell and Markert, 2009), it is assumed that
the statistical significance of a word pair association
is proportional to the square root of the evidence.
The question of what constitutes the evidence is an-
swered by taking the lesser of the frequencies of the
two words in the word pair, since at most that many
pairings are possible. Hence the PMI value is multi-
</bodyText>
<page confidence="0.982836">
163
</page>
<note confidence="0.3913455">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 163–169,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.9620745">
Method Formula Revised Formula
PMI (Church and log f(x,y)
Hanks, 1989)
f(x)∗f(y)/W
PMIsig (Washtell log f(x,y) PMIs: log f(x,y) * Vmax(f(x),f(y))
and Markert, 2009) * Vmin(f(x),f(y))
f(x)∗f(y)/W f(x)∗f(y)/W
cPMId (Damani, log
2013) +./a())
d(x)*d(y)/D∗� ( in 50) sPMId: logmax(
/
d(x),d(y))*min(d(x),d(y))/(D+Vmax(d(x),d(y))∗ V (l 2 o)
Terminology:
W Total number of words in the corpus D Total number of documents in the corpus
</table>
<tableCaption confidence="0.95944225">
f(x),f(y) unigram frequencies of x,y respectively in the corpus d(x), d(y) Total number of documents in the corpus containing
at least one occurrence of x and y respectively
f(x, y) Span-constrained (x, y) word pair frequency in the corpus d(x, y) Total number of documents in the corpus having at-least
one span-constrained occurrence of the word pair (x, y)
δ a parameter varying between 0 and 1
Table 1: Definitions of PMI and its statistically significant adaptations. The sub-parts in bold represent the changes
between the original formulas and the revised formulas. The product max(d(x), d(y)) * min(d(x), d(y)) in sPMId
formula can be simplified to f(x) * f(y), however, we left it this way to emphasize the transformation from cPMId.
</tableCaption>
<equation confidence="0.663965">
p
plied by min(f(x), f(y)) to get PMI,ig.
</equation>
<bodyText confidence="0.999319545454545">
In (Damani, 2013), statistical significance is
introduced by bounding the probability of observing
a given number of word-pair occurrences in the
corpus, just by chance, under a null model of inde-
pendent unigram occurrences. For this computation,
one needs to decide what constitutes a random trial
when looking for a word-pair occurrence. Is it the
occurrence of the first word (say x) in the pair, or
the second (say y). In (Damani, 2013), occurrences
of x are arbitrarily chosen to represent the sites of
the random trial. Using Hoeffdings Inequality:
</bodyText>
<equation confidence="0.940825666666667">
P[f(x, y) ? f(x) * f(y)/W + f(x) * t]
G exp(−2 * f(x) * t2)
p
</equation>
<bodyText confidence="0.8699354">
By setting t = lnδ/(−2 * f(x)), we get δ as an
upper bound on probability of observing more than
f(x)*f(y)/W +f(x)*t bigram occurrences in the
corpus, just by chance. Based on this Corpus Level
Significant PMI(cPMI) is defined as:
</bodyText>
<equation confidence="0.9989685">
f(x, y)
cP MI(x, y) = log
p p
f(x) * f(y)/W + f(x) * ln δ/(−2)
</equation>
<bodyText confidence="0.9995595">
In (Damani, 2013), several variants of cPMI are in-
troduced that incorporate different notions of sta-
tistical significance. Of these Corpus Level Signif-
icant PMI based on Document count(cPMId - de-
fined in Table 1) is found to be the best performing,
and hence we consider this variant only in this work.
</bodyText>
<subsectionHeader confidence="0.999691">
2.1 Choice of Random Trial
</subsectionHeader>
<bodyText confidence="0.966230294117647">
While considering statistical significance, one has
to decide what constitutes a random trial. When
looking for a word-pair (x, y)’s occurrences, y can
potentially occur near each occurrence of x, or x
can potentially occur near each occurrence of y.
Which of these two set of occurrences should be
considered the sites of random trial. We believe
that the occurrences of the more frequent of x and y
should be considered, since near each of these occur-
rences the other word could have occurred. Hence
f(x) and f(y) in cPMI definition should be re-
placed with max(f(x), f(y)) and min(f(x), f(y))
respectively. Similarly, d(x) and d(y) in cPMId for-
mula should be replaced with max(d(x), d(y)) and
min(d(x), d(y)) respectively to give a new measure
Significant PMI based on Document count(sPMId).
p
</bodyText>
<subsectionHeader confidence="0.647647">
Using the same logic, min(f(x), f(y))
</subsectionHeader>
<bodyText confidence="0.9796946">
in PMI,ig formula should be replaced with
p
max(f(x), f(y)) to give the formula for a new
measure PMI-significant(PMIs). The definitions of
sPMId and PMIs are also given in Table 1.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.995158714285714">
There are three main types of word association mea-
sures: Knowledge based, Distributional Similarity
based, and Lexical Co-occurrence based.
Based on Firth’s You shall know a word by the
company it keeps (Firth, 1957), distributional sim-
ilarity based measures characterize a word by the
distribution of other words around it and compare
</bodyText>
<equation confidence="0.998923333333333">
f(x) * f(y)/W + f(x) * t
f(x, y)
= log
</equation>
<page confidence="0.995291">
164
</page>
<table confidence="0.999193162162162">
Method Formula
ChiS ware // 2 (f(i ,j)−Ef(i,j))2
�l lx )
K,j Ef(i,j)
Dice (Dice, 1945) f(x,y)
f(x)+f(y)
GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y)
log D−min(log d(x),log d(y))
Jaccard (Jaccard, 1912) f(x,y)
f(x)+f(y)−f(x,y)
LLR (Dunning, 1993) f(x0,y0)
f(x0,
� y0)log
f(x0)f(y0)
x0 ∈ {x, ¬x}
y0 ∈ {y, ¬y}
nPMI (Bouma, 2009) log f(x,y)
f(x)∗f(y)/W
1
log
f(x,y)/W
Ochiai (Janson and Vegelius, 1981) f (x,y)
√f(x)f(y)
PMI2 (Daille, 1994) f(x,y)
log f(x,y)2
log f(x)∗f(y)/W
=
1 f(x)∗f(y)
f(x,y)/W
Simpson (Simpson, 1943) f(x,y)
min(f(x),f(y))
SCI (Washtell and Markert, 2009) f (x,y)
f(x)√f(y)
T-test f(x,y)−Ef(x,y)
�f(x,y)(1− f(x,y)
)
W
</table>
<tableCaption confidence="0.9209025">
Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same
as that in Table 1, except that E in front of a variable name means the expected value of that variable.
</tableCaption>
<table confidence="0.999945290322581">
Task Semantic Sentence Synonym
Relatedness Similarity Selection
Dataset WordSim Li ESL TOEFL
Metric Spearman Rank Pearson Cor- Fraction Fraction
Correlation relation Correct Correct
PMI 0.68 0.69 0.62 0.59
PMI3ig 0.67 0.85 0.58 0.56
cPMId 0.72 0.67 0.56 0.59
PMIs 0.66 0.85 0.66 0.61
sPMId 0.72 0.75 0.70 0.61
ChiSquare (x2) 0.62 0.80 0.62 0.58
Dice 0.58 0.76 0.56 0.57
GoogleDistance 0.53 0.75 0.09 0.19
Jaccard 0.58 0.76 0.56 0.57
LLR 0.50 0.18 0.18 0.27
nPMI 0.72 0.35 0.54 0.54
Ochiai/ PMI2 0.62 0.77 0.62 0.60
SCI 0.65 0.85 0.62 0.60
Simpson 0.59 0.78 0.58 0.57
TTest 0.44 0.63 0.44 0.52
Semantic Net (Li et al., 2006) 0.82
ESA (Gabrilovich and Markovitch, 2007) 0.74
(reimplemented in (Yeh et al., 2009)) 0.71
Distributional Similarity (on web corpus) (Agirre et 0.65
al., 2009))
Context Window based Distributional Similar- 0.60
ity (Agirre et al., 2009))
Latent Semantic Analysis (on web corpus) (Finkel- 0.56
stein et al., 2002)
WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87
PMI-IR3 (using context) (Turney, 2001) 0.73
</table>
<tableCaption confidence="0.99359825">
Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best
co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and
LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known
result for different non co-occurrence based methods is also shown.
</tableCaption>
<page confidence="0.997363">
165
</page>
<bodyText confidence="0.9991687">
two words for distributional similarity (Agirre et
al., 2009; Wandmacher et al., 2008; Bollegala et
al., 2007; Chen et al., 2006). They are also used
for modeling the meaning of a phrase or a sen-
tence (Grefenstette and Sadrzadeh, 2011; Wartena,
2013; Mitchell, 2011; G. Dinu and Baroni, 2013;
Kartsaklis et al., 2013).
Knowledge-based measures use knowledge-
sources like thesauri, semantic networks, or
taxonomies (Milne and Witten, 2008; Hughes
and Ramage, 2007; Gabrilovich and Markovitch,
2007; Yeh et al., 2009; Strube and Ponzetto, 2006;
Finkelstein et al., 2002; Liberman and Markovitch,
2009).
Co-occurrence based measures (Pecina and
Schlesinger, 2006) simply rely on unigram and bi-
gram frequencies of the words in a pair. In this work,
our focus is on the co-occurrence based measures,
since they are resource-light and can easily be used
for resource-scarce languages.
</bodyText>
<subsectionHeader confidence="0.994733">
3.1 Co-occurrence Measures being Compared
</subsectionHeader>
<bodyText confidence="0.999954647058823">
Co-occurrence based measures of association be-
tween two entities are used in several domains like
ecology, psychology, medicine, language process-
ing, etc. To compare the performance of our newly
introduced measures with other co-occurrence mea-
sures, we have selected a number of popu-
lar co-occurrence measures like ChiSquare (X2),
Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and
Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dun-
ning, 1993), Simpson (Simpson, 1943), and T-test
from these domains.
In addition to these popular measures, we
also experiment with other known variations of
PMI like nPMI (Bouma, 2009), PMI2 (Daille,
1994), Ochiai (Janson and Vegelius, 1981), and
SCI (Washtell and Markert, 2009). Since PMI2 is
a monotonic transformation of Ochiai, we present
their results together. In Table 2, we present the def-
initions of these measures. While the motivation
given for SCI in (Washtell and Markert, 2009) is
slightly different, in light of the discussion in Sec-
tion 2.1, we can assume that SCI is PMI adapted for
statistical significance (multiplied by -\/f(y)), where
the site of random trial is taken to be the occurrences
of the second word y, instead of the less frequent
word, as in the case of PMI,ig.
When counting co-occurrences, we only con-
sider the non-overlapping span-constrained occur-
rences. The span of a word-pair’s occurrence is the
direction-independent distance between the occur-
rences of the members of the pair. We consider only
those co-occurrences where span is less than a given
threshold. Therefore, span threshold is a parameter
for all the co-occurrence measures being considered.
</bodyText>
<sectionHeader confidence="0.991489" genericHeader="conclusions">
4 Performance Evaluation
</sectionHeader>
<bodyText confidence="0.999992454545455">
Having introduced the revised measures PMIs and
sPMId, we need to evaluate the performance of these
measures compared to PMI and the original mea-
sures introducing significance. In addition, we also
wish to compare the performance of these measures
with other co-occurrence measures. To compare the
performance of these measures with more resource
heavy non co-occurrence based measures, we have
chosen those tasks and datasets on which published
results exist for distributional similarity and knowl-
edge based word association measures.
</bodyText>
<subsectionHeader confidence="0.993186">
4.1 Task Details
</subsectionHeader>
<bodyText confidence="0.999986869565218">
We evaluate these measures on three tasks: Sen-
tence Similarity(65 sentence-pairs from (Li et al.,
2006)), Synonym Selection(50 questions ESL (Tur-
ney, 2001) and 80 questions TOEFL (Landauer and
Dutnais, 1997) datasets), and, Semantic Related-
ness(353 words Wordsim (Finkelstein et al., 2002)
dataset).
For each of these tasks, gold standard human
judgment results exist. For sentence similarity, fol-
lowing (Li et al., 2006), we evaluate a measure by
the Pearsons correlation between the ranking pro-
duced by the measure and the human ranking. For
synonym selection, we compute the percentage of
correct answers, since there is a unique answer for
each challenge word in the datasets. Semantic relat-
edness has been evaluated by Spearman’s rank cor-
relation with human judgment instead of Pearsons
correlation in literature and we follow the same prac-
tice to make results comparable.
For sentence similarity detection, the algorithm
used by us (Li et al., 2006) assumes that the asso-
ciation scores are between 0 and 1. Hence we nor-
malize the value produced by each measure using
</bodyText>
<page confidence="0.996593">
166
</page>
<table confidence="0.9966919">
Challenge Option y Option z f(x) f(y) f(z) f(x, y) f(x, z) PMIsig PMIsig PMIs PMIs
x (correct) (incorrect) (x, y) (x, z) (x, y) (x, z)
brass metal plastic 15923 125088 24985 228 75 14 24 40 30
twist intertwine curl 11407 153 2047 1 9 7 17 61 41
saucer dish frisbee 2091 12453 1186 5 1 9 14 21 18
mass lump element 90398 1595 43321 14 189 4 10 29 15
applause approval friends 1998 19673 11689 8 6 9 11 29 28
confession statement plea 7687 47299 5232 76 12 18 22 45 26
swing sway bounce 33580 2994 4462 13 17 7 8 24 21
sheet leaf book 20470 20979 586581 20 194 7 2 7 12
</table>
<tableCaption confidence="0.99170925">
Table 4: Details of ESL word-pairs, correctness of whose answers changes between PMIsig and PMIs. Except for the
gray-row, for all other questions, incorrect answers becomes correct on using PMIs instead of PMIsig, and vice-versa
for the gray-row. The association values have been suitably scaled for readability. To save space, of the four choices,
options not selected by either of the methods have been omitted. These results are for a 10 word span.
</tableCaption>
<equation confidence="0.81564475">
max-min normalization:
v − min
v =
max − min
</equation>
<bodyText confidence="0.999937">
where max and min are computed over all associa-
tion scores for the entire task for a given measure.
</bodyText>
<subsectionHeader confidence="0.988525">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999944642857143">
We use a 1.24 Gigawords Wikipedia corpus for get-
ting co-occurrence statistics. Since co-occurrence
methods have span-threshold as a parameter, we fol-
low the standard methodology of five-fold cross val-
idation. Note that, in addition to span-threshold, cP-
MId and sPMId have an additional parameter S.
In Table 3, we present the performance of all the
co-occurrence measures considered on all the tasks.
Note that, except GoogleDistance and LLR, all re-
sults for all co-occurrence measures are statistically
significant at p = .05. For completeness of compari-
son, we also include the best known results from lit-
erature for different non co-occurrence based word
association measures on these tasks.
</bodyText>
<subsectionHeader confidence="0.99996">
4.3 Performance Analysis and Conclusions
</subsectionHeader>
<bodyText confidence="0.999989717948718">
We find that on average, PMIsig and cPMId, the re-
cently introduced measures that incorporate signif-
icance in PMI, do not perform better than PMI on
the given datasets. Both of them perform worse
than PMI on three out of four datasets. By ap-
propriately incorporating significance, we get new
measures PMIs and sPMId that perform better than
PMI(also PMIsig and cPMId respectively) on most
datasets. PMIs improves performance over PMI on
three out of four datasets, while sPMId improves
performance on all four datasets.
The performance improvement of PMIs over
PMIsig and of sPMId over cPMId, is not random.
For example, on the ESL dataset, while the percent-
age of correct answers increases from 58 to 66 from
PMIsig to PMIs, it is not the case that on moving
from PMIsig to PMIs, several correct answers be-
come incorrect and an even larger number of in-
correct answers become correct. As shown in Ta-
ble 4, only one correct answers become incorrect
while seven incorrect answers get corrected. The
same trend holds for most parameters values, and
for moving from cPMId to sPMId. This substanti-
ates the claim that the improvement is not random,
but due to the appropriate incorporation of signifi-
cance, as discussed in Section 2.1.
PMIs and sPMId perform better than not just
PMI, but they perform better than all popular co-
occurrence measures on most of these tasks. When
compared with any other co-occurrence measure,
on three out of four datasets each, both PMIs and
sPMId perform better than that measure. In fact,
PMIs and sPMId perform reasonably well compared
with more resource intensive non co-occurrence
based methods as well. Note that different non co-
occurrence based measures perform well on differ-
ent tasks. We are comparing the performance of a
single measure (say sPMId or PMIs) against the best
measure for each task.
</bodyText>
<sectionHeader confidence="0.982412" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9977915">
We thank Dipak Chaudhari for his help with the im-
plementation.
</bodyText>
<page confidence="0.997303">
167
</page>
<sectionHeader confidence="0.959234" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999253990291262">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In NAACL-HLT 2009,
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In WWW
2007, The World Wide Web Conference, pages 757–
766.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction, from form to
meaning: Processing texts automatically. In GSCL
2009, Proceedings of the Biennial International Con-
ference of the German Society for Computational Lin-
guistics and Language Technology.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In COLING/ACL 2006, Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicogra-
phy. In ACL 1989, Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 76–83.
B. Daille. 1994. Approche mixte pour l’extraction au-
tomatique de terminologie: statistiques lexicales etl-
tres linguistiques. Ph.D. thesis, Universitie Paris 7.
Om P. Damani. 2013. Improving pointwise mutual
information (pmi) by incorporating significant co-
occurrence. In CoNLL 2013, Conference on Compu-
tational Natural Language Learning.
L. R. Dice. 1945. Measures of the amount of ecological
association between species. Ecology, 26:297–302.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Transactions on Information Systems,
20(1):116–131.
J. R. Firth. 1957. A synopsis of linguistics theory. Stud-
ies in Linguistic Analysis, pages 1930–1955.
N. Pham G. Dinu and M. Baroni. 2013. General esti-
mation and evaluation of compositional distributional
semantic models. In CVSC 2013, Proceedings of the
ACL Workshop on Continuous Vector Space Models
and their Compositionality.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI 2007, International
Joint Conference on Artificial Intelligence.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In EMNLP 2011,
Conference on Empirical Methods on Natural Lan-
guage Processing, pages 1394–1404.
T Hughes and D Ramage. 2007. Lexical semantic relat-
edness with random graph walks. In EMNLP 2007,
Conference on Empirical Methods on Natural Lan-
guage Processing.
P. Jaccard. 1912. The distribution of the flora of the
alpine zone. New Phytologist, 11:37–50.
Svante Janson and Jan Vegelius. 1981. Measures of eco-
logical association. Oecologia, 49:371–376.
Dimitrios Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In CoNLL
2013, Conference on Computational Natural Lan-
guage Learning.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211–240.
Rudi L.Cilibrasi and Paul M.B. Vitany. 2007. The google
similarity distance. Psychological review, 19(3).
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138–1150, August.
Sonya Liberman and Shaul Markovitch. 2009. Com-
pact hierarchical explicit semantic representation. In
WikiAI 2009, Proceedings of the IJCAI Workshop
on User-Contributed Knowledge and Artificial Intel-
ligence: An Evolving Synergy, Pasadena, CA, July.
David Milne and Ian H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In ACL 2008, Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, The University of
Edinburgh.
Pavel Pecina and Pavel Schlesinger. 2006. Combin-
ing association measures for collocation extraction. In
ACL 2006, Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
</reference>
<page confidence="0.979473">
168
</page>
<reference confidence="0.99952065625">
Gabriel Recchia and Michael N. Jones. 2009. More data
trumps smarter algorithms: Comparing pointwise mu-
tual information with latent semantic analysis. Behav-
ior Research Methods, 3(41):647–656.
George G. Simpson. 1943. Mammals and the nature of
continents. American Journal of Science, pages 1–31.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI 2006, Conference on Artificial In-
telligence, pages 1419–1424.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In ECML 2001, European
Conference on Machine Learning.
T. Wandmacher, E. Ovchinnikova, and T. Alexandrov.
2008. Does latent semantic analysis reflect human
associations? In ESSLLI 2008, European Summer
School in Logic, Language and Information.
Christian Wartena. 2013. Hsh: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In SemEval 2013, Inter-
national Workshop on Semantic Evaluation.
Justin Washtell and Katja Markert. 2009. A comparison
of windowless and window-based computational asso-
ciation measures as predictors of syntagmatic human
associations. In EMNLP 2009, Conference on Empir-
ical Methods on Natural Language Processing, pages
628–637.
Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre,
and Aitor Soroa. 2009. Wikiwalk: Random walks
on wikipedia for semantic relatedness. In TextGraphs
2009, Proceedings of the ACL workshop on Graph-
based Methods for Natural Language Processing.
</reference>
<page confidence="0.998817">
169
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538456">
<title confidence="0.997131">Appropriately Incorporating Statistical Significance in PMI</title>
<author confidence="0.902473">Om P Damani</author>
<author confidence="0.902473">Shweta</author>
<affiliation confidence="0.579189">IIT</affiliation>
<abstract confidence="0.9969456">Two recent measures incorporate the notion of statistical significance in basic PMI formulation. In some tasks, we find that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical significance in PMI are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL-HLT 2009, Conference of the North American Chapter of</booktitle>
<contexts>
<context position="9069" citStr="Agirre et al., 2009" startWordPosition="1476" endWordPosition="1479">72 0.67 0.56 0.59 PMIs 0.66 0.85 0.66 0.61 sPMId 0.72 0.75 0.70 0.61 ChiSquare (x2) 0.62 0.80 0.62 0.58 Dice 0.58 0.76 0.56 0.57 GoogleDistance 0.53 0.75 0.09 0.19 Jaccard 0.58 0.76 0.56 0.57 LLR 0.50 0.18 0.18 0.27 nPMI 0.72 0.35 0.54 0.54 Ochiai/ PMI2 0.62 0.77 0.62 0.60 SCI 0.65 0.85 0.62 0.60 Simpson 0.59 0.78 0.58 0.57 TTest 0.44 0.63 0.44 0.52 Semantic Net (Li et al., 2006) 0.82 ESA (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al., 2009)) 0.71 Distributional Similarity (on web corpus) (Agirre et 0.65 al., 2009)) Context Window based Distributional Similar- 0.60 ity (Agirre et al., 2009)) Latent Semantic Analysis (on web corpus) (Finkel- 0.56 stein et al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 16</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL-HLT 2009, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<booktitle>In WWW 2007, The World Wide Web Conference,</booktitle>
<pages>757--766</pages>
<contexts>
<context position="9780" citStr="Bollegala et al., 2007" startWordPosition="1581" endWordPosition="1584">ilarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of </context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Measuring semantic similarity between words using web search engines. In WWW 2007, The World Wide Web Conference, pages 757– 766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction, from form to meaning: Processing texts automatically.</title>
<date>2009</date>
<booktitle>In GSCL 2009, Proceedings of the Biennial International Conference of the German Society for Computational Linguistics and Language Technology.</booktitle>
<contexts>
<context position="7677" citStr="Bouma, 2009" startWordPosition="1253" endWordPosition="1254">rence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that variable. Task Semantic Sentence Synonym Relatedness Similarity Selection Datas</context>
<context position="11206" citStr="Bouma, 2009" startWordPosition="1798" endWordPosition="1799">occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multiplied by -\/f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent word, as in the case</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction, from form to meaning: Processing texts automatically. In GSCL 2009, Proceedings of the Biennial International Conference of the German Society for Computational Linguistics and Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Ming-Shun Lin</author>
<author>Yu-Chuan Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In COLING/ACL 2006, Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9800" citStr="Chen et al., 2006" startWordPosition="1585" endWordPosition="1588">es, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair.</context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei. 2006. Novel association measures using web search with double checking. In COLING/ACL 2006, Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1989</date>
<booktitle>In ACL 1989, Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="1204" citStr="Church and Hanks, 1989" startWordPosition="179" endWordPosition="182">ar co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also. 1 Introduction The notion of word association is used in many language processing and information retrieval applications and it is important to have low-cost, highquality association measures. Lexical co-occurrence based word association measures are popular because they are computationally efficient and they can be applied to any language easily. One of the most popular co-occurrence measure is Pointwise Mutual Information (PMI) (Church and Hanks, 1989). One of the limitations of PMI is that it only works with relative probabilities and ignores the absolute amount of evidence. To overcome this, recently two new measures have been proposed that incorporate the notion of statistical significance in basic PMI formulation. In (Washtell and Markert, 2009), statistical significance is introduced in PMI,ig by multiplying PMI value with the square root of the evidence. In contrast, in (Damani, 2013), cPMId is introduced by bounding the probability of observing a given deviation between a given word pair’s cooccurrence count and its expected value un</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. In ACL 1989, Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Approche mixte pour l’extraction automatique de terminologie: statistiques lexicales etltres linguistiques.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitie Paris</institution>
<contexts>
<context position="7788" citStr="Daille, 1994" startWordPosition="1270" endWordPosition="1271">larity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that variable. Task Semantic Sentence Synonym Relatedness Similarity Selection Dataset WordSim Li ESL TOEFL Metric Spearman Rank Pearson Cor- Fraction Fraction Correlation relation Correct Correc</context>
<context position="11227" citStr="Daille, 1994" startWordPosition="1801" endWordPosition="1802">sures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multiplied by -\/f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent word, as in the case of PMI,ig. When coun</context>
</contexts>
<marker>Daille, 1994</marker>
<rawString>B. Daille. 1994. Approche mixte pour l’extraction automatique de terminologie: statistiques lexicales etltres linguistiques. Ph.D. thesis, Universitie Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Om P Damani</author>
</authors>
<title>Improving pointwise mutual information (pmi) by incorporating significant cooccurrence.</title>
<date>2013</date>
<booktitle>In CoNLL 2013, Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1651" citStr="Damani, 2013" startWordPosition="253" endWordPosition="254">efficient and they can be applied to any language easily. One of the most popular co-occurrence measure is Pointwise Mutual Information (PMI) (Church and Hanks, 1989). One of the limitations of PMI is that it only works with relative probabilities and ignores the absolute amount of evidence. To overcome this, recently two new measures have been proposed that incorporate the notion of statistical significance in basic PMI formulation. In (Washtell and Markert, 2009), statistical significance is introduced in PMI,ig by multiplying PMI value with the square root of the evidence. In contrast, in (Damani, 2013), cPMId is introduced by bounding the probability of observing a given deviation between a given word pair’s cooccurrence count and its expected value under a null model where with each word a global unigram generation probability is associated. In Table 1, we give the definitions of PMI, PMI,ig, and cPMId. While these new measures perform better than PMI on some of the tasks, on many other tasks, we find that the new measures perform worse than the PMI. In Table 3, we show how these measures perform compared to PMI on four different tasks. We find that PMI,ig degrades performance in three out</context>
<context position="4683" citStr="Damani, 2013" startWordPosition="744" endWordPosition="745">nstrained (x, y) word pair frequency in the corpus d(x, y) Total number of documents in the corpus having at-least one span-constrained occurrence of the word pair (x, y) δ a parameter varying between 0 and 1 Table 1: Definitions of PMI and its statistically significant adaptations. The sub-parts in bold represent the changes between the original formulas and the revised formulas. The product max(d(x), d(y)) * min(d(x), d(y)) in sPMId formula can be simplified to f(x) * f(y), however, we left it this way to emphasize the transformation from cPMId. p plied by min(f(x), f(y)) to get PMI,ig. In (Damani, 2013), statistical significance is introduced by bounding the probability of observing a given number of word-pair occurrences in the corpus, just by chance, under a null model of independent unigram occurrences. For this computation, one needs to decide what constitutes a random trial when looking for a word-pair occurrence. Is it the occurrence of the first word (say x) in the pair, or the second (say y). In (Damani, 2013), occurrences of x are arbitrarily chosen to represent the sites of the random trial. Using Hoeffdings Inequality: P[f(x, y) ? f(x) * f(y)/W + f(x) * t] G exp(−2 * f(x) * t2) p </context>
</contexts>
<marker>Damani, 2013</marker>
<rawString>Om P. Damani. 2013. Improving pointwise mutual information (pmi) by incorporating significant cooccurrence. In CoNLL 2013, Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecological association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<pages>26--297</pages>
<contexts>
<context position="7403" citStr="Dice, 1945" startWordPosition="1214" endWordPosition="1215">), f(y)) to give the formula for a new measure PMI-significant(PMIs). The definitions of sPMId and PMIs are also given in Table 1. 3 Related Work There are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Defini</context>
<context position="10939" citStr="Dice, 1945" startWordPosition="1759" endWordPosition="1760">6) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, i</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>L. R. Dice. 1945. Measures of the amount of ecological association between species. Ecology, 26:297–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="7597" citStr="Dunning, 1993" startWordPosition="1237" endWordPosition="1238">n measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that</context>
<context position="11032" citStr="Dunning, 1993" startWordPosition="1770" endWordPosition="1772"> focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistic</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="10237" citStr="Finkelstein et al., 2002" startWordPosition="1652" endWordPosition="1655">ferent non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measur</context>
<context position="13085" citStr="Finkelstein et al., 2002" startWordPosition="2086" endWordPosition="2089">performance of these measures with other co-occurrence measures. To compare the performance of these measures with more resource heavy non co-occurrence based measures, we have chosen those tasks and datasets on which published results exist for distributional similarity and knowledge based word association measures. 4.1 Task Details We evaluate these measures on three tasks: Sentence Similarity(65 sentence-pairs from (Li et al., 2006)), Synonym Selection(50 questions ESL (Turney, 2001) and 80 questions TOEFL (Landauer and Dutnais, 1997) datasets), and, Semantic Relatedness(353 words Wordsim (Finkelstein et al., 2002) dataset). For each of these tasks, gold standard human judgment results exist. For sentence similarity, following (Li et al., 2006), we evaluate a measure by the Pearsons correlation between the ranking produced by the measure and the human ranking. For synonym selection, we compute the percentage of correct answers, since there is a unique answer for each challenge word in the datasets. Semantic relatedness has been evaluated by Spearman’s rank correlation with human judgment instead of Pearsons correlation in literature and we follow the same practice to make results comparable. For sentenc</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: the concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistics theory. Studies in Linguistic Analysis,</title>
<date>1957</date>
<pages>1930--1955</pages>
<contexts>
<context position="7154" citStr="Firth, 1957" startWordPosition="1168" endWordPosition="1169">n cPMId formula should be replaced with max(d(x), d(y)) and min(d(x), d(y)) respectively to give a new measure Significant PMI based on Document count(sPMId). p Using the same logic, min(f(x), f(y)) in PMI,ig formula should be replaced with p max(f(x), f(y)) to give the formula for a new measure PMI-significant(PMIs). The definitions of sPMId and PMIs are also given in Table 1. 3 Related Work There are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistics theory. Studies in Linguistic Analysis, pages 1930–1955.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pham G Dinu</author>
<author>M Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In CVSC 2013, Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.</booktitle>
<contexts>
<context position="9962" citStr="Dinu and Baroni, 2013" startWordPosition="1613" endWordPosition="1616">the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurr</context>
</contexts>
<marker>Dinu, Baroni, 2013</marker>
<rawString>N. Pham G. Dinu and M. Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In CVSC 2013, Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI 2007, International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8875" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1447" endWordPosition="1450">atedness Similarity Selection Dataset WordSim Li ESL TOEFL Metric Spearman Rank Pearson Cor- Fraction Fraction Correlation relation Correct Correct PMI 0.68 0.69 0.62 0.59 PMI3ig 0.67 0.85 0.58 0.56 cPMId 0.72 0.67 0.56 0.59 PMIs 0.66 0.85 0.66 0.61 sPMId 0.72 0.75 0.70 0.61 ChiSquare (x2) 0.62 0.80 0.62 0.58 Dice 0.58 0.76 0.56 0.57 GoogleDistance 0.53 0.75 0.09 0.19 Jaccard 0.58 0.76 0.56 0.57 LLR 0.50 0.18 0.18 0.27 nPMI 0.72 0.35 0.54 0.54 Ochiai/ PMI2 0.62 0.77 0.62 0.60 SCI 0.65 0.85 0.62 0.60 Simpson 0.59 0.78 0.58 0.57 TTest 0.44 0.63 0.44 0.52 Semantic Net (Li et al., 2006) 0.82 ESA (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al., 2009)) 0.71 Distributional Similarity (on web corpus) (Agirre et 0.65 al., 2009)) Context Window based Distributional Similar- 0.60 ity (Agirre et al., 2009)) Latent Semantic Analysis (on web corpus) (Finkel- 0.56 stein et al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance </context>
<context position="10166" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1640" endWordPosition="1643">tistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the perfo</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJCAI 2007, International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In EMNLP 2011, Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<contexts>
<context position="9905" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1604" endWordPosition="1607">ation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can ea</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In EMNLP 2011, Conference on Empirical Methods on Natural Language Processing, pages 1394–1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hughes</author>
<author>D Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In EMNLP 2007, Conference on Empirical Methods on Natural Language Processing.</booktitle>
<contexts>
<context position="10132" citStr="Hughes and Ramage, 2007" startWordPosition="1636" endWordPosition="1639">currence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language pro</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>T Hughes and D Ramage. 2007. Lexical semantic relatedness with random graph walks. In EMNLP 2007, Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jaccard</author>
</authors>
<title>The distribution of the flora of the alpine zone. New Phytologist,</title>
<date>1912</date>
<pages>11--37</pages>
<contexts>
<context position="7553" citStr="Jaccard, 1912" startWordPosition="1232" endWordPosition="1233">here are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a va</context>
<context position="11011" citStr="Jaccard, 1912" startWordPosition="1767" endWordPosition="1768">ir. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI </context>
</contexts>
<marker>Jaccard, 1912</marker>
<rawString>P. Jaccard. 1912. The distribution of the flora of the alpine zone. New Phytologist, 11:37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svante Janson</author>
<author>Jan Vegelius</author>
</authors>
<date>1981</date>
<booktitle>Measures of ecological association. Oecologia,</booktitle>
<pages>49--371</pages>
<contexts>
<context position="7750" citStr="Janson and Vegelius, 1981" startWordPosition="1262" endWordPosition="1265">company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that variable. Task Semantic Sentence Synonym Relatedness Similarity Selection Dataset WordSim Li ESL TOEFL Metric Spearman Rank Pearson Cor- Fraction Fracti</context>
<context position="11263" citStr="Janson and Vegelius, 1981" startWordPosition="1804" endWordPosition="1807">tween two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multiplied by -\/f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent word, as in the case of PMI,ig. When counting co-occurrences, we only conside</context>
</contexts>
<marker>Janson, Vegelius, 1981</marker>
<rawString>Svante Janson and Jan Vegelius. 1981. Measures of ecological association. Oecologia, 49:371–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>Separating disambiguation from composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In CoNLL 2013, Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="9988" citStr="Kartsaklis et al., 2013" startWordPosition="1617" endWordPosition="1620">st co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compar</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>Dimitrios Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composition in distributional semantics. In CoNLL 2013, Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dutnais</author>
</authors>
<title>A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="13003" citStr="Landauer and Dutnais, 1997" startWordPosition="2075" endWordPosition="2078">riginal measures introducing significance. In addition, we also wish to compare the performance of these measures with other co-occurrence measures. To compare the performance of these measures with more resource heavy non co-occurrence based measures, we have chosen those tasks and datasets on which published results exist for distributional similarity and knowledge based word association measures. 4.1 Task Details We evaluate these measures on three tasks: Sentence Similarity(65 sentence-pairs from (Li et al., 2006)), Synonym Selection(50 questions ESL (Turney, 2001) and 80 questions TOEFL (Landauer and Dutnais, 1997) datasets), and, Semantic Relatedness(353 words Wordsim (Finkelstein et al., 2002) dataset). For each of these tasks, gold standard human judgment results exist. For sentence similarity, following (Li et al., 2006), we evaluate a measure by the Pearsons correlation between the ranking produced by the measure and the human ranking. For synonym selection, we compute the percentage of correct answers, since there is a unique answer for each challenge word in the datasets. Semantic relatedness has been evaluated by Spearman’s rank correlation with human judgment instead of Pearsons correlation in </context>
</contexts>
<marker>Landauer, Dutnais, 1997</marker>
<rawString>Thomas K Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi L Cilibrasi</author>
<author>Paul M B Vitany</author>
</authors>
<title>The google similarity distance.</title>
<date>2007</date>
<journal>Psychological review,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="7466" citStr="Cilibrasi and Vitany, 2007" startWordPosition="1219" endWordPosition="1222">PMI-significant(PMIs). The definitions of sPMId and PMIs are also given in Table 1. 3 Related Work There are three main types of word association measures: Knowledge based, Distributional Similarity based, and Lexical Co-occurrence based. Based on Firth’s You shall know a word by the company it keeps (Firth, 1957), distributional similarity based measures characterize a word by the distribution of other words around it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this wor</context>
<context position="10986" citStr="Cilibrasi and Vitany, 2007" startWordPosition="1762" endWordPosition="1765">igram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we ca</context>
</contexts>
<marker>Cilibrasi, Vitany, 2007</marker>
<rawString>Rudi L.Cilibrasi and Paul M.B. Vitany. 2007. The google similarity distance. Psychological review, 19(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138–1150, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonya Liberman</author>
<author>Shaul Markovitch</author>
</authors>
<title>Compact hierarchical explicit semantic representation.</title>
<date>2009</date>
<booktitle>In WikiAI 2009, Proceedings of the IJCAI Workshop on User-Contributed Knowledge and Artificial Intelligence: An Evolving Synergy,</booktitle>
<location>Pasadena, CA,</location>
<contexts>
<context position="10269" citStr="Liberman and Markovitch, 2009" startWordPosition="1656" endWordPosition="1659">ased methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of</context>
</contexts>
<marker>Liberman, Markovitch, 2009</marker>
<rawString>Sonya Liberman and Shaul Markovitch. 2009. Compact hierarchical explicit semantic representation. In WikiAI 2009, Proceedings of the IJCAI Workshop on User-Contributed Knowledge and Artificial Intelligence: An Evolving Synergy, Pasadena, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, lowcost measure of semantic relatedness obtained from wikipedia links.</title>
<date>2008</date>
<booktitle>In ACL 2008, Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10107" citStr="Milne and Witten, 2008" startWordPosition="1632" endWordPosition="1635">ll results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psycholog</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. An effective, lowcost measure of semantic relatedness obtained from wikipedia links. In ACL 2008, Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mitchell</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Edinburgh.</institution>
<contexts>
<context position="9936" citStr="Mitchell, 2011" startWordPosition="1610" endWordPosition="1611">s. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarc</context>
</contexts>
<marker>Mitchell, 2011</marker>
<rawString>Jeffrey Mitchell. 2011. Composition in Distributional Models of Semantics. Ph.D. thesis, The University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
<author>Pavel Schlesinger</author>
</authors>
<title>Combining association measures for collocation extraction.</title>
<date>2006</date>
<booktitle>In ACL 2006, Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10330" citStr="Pecina and Schlesinger, 2006" startWordPosition="1663" endWordPosition="1666">imilarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Di</context>
</contexts>
<marker>Pecina, Schlesinger, 2006</marker>
<rawString>Pavel Pecina and Pavel Schlesinger. 2006. Combining association measures for collocation extraction. In ACL 2006, Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Recchia</author>
<author>Michael N Jones</author>
</authors>
<title>More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis.</title>
<date>2009</date>
<journal>Behavior Research Methods,</journal>
<volume>3</volume>
<issue>41</issue>
<contexts>
<context position="9191" citStr="Recchia and Jones, 2009" startWordPosition="1493" endWordPosition="1496"> 0.56 0.57 GoogleDistance 0.53 0.75 0.09 0.19 Jaccard 0.58 0.76 0.56 0.57 LLR 0.50 0.18 0.18 0.27 nPMI 0.72 0.35 0.54 0.54 Ochiai/ PMI2 0.62 0.77 0.62 0.60 SCI 0.65 0.85 0.62 0.60 Simpson 0.59 0.78 0.58 0.57 TTest 0.44 0.63 0.44 0.52 Semantic Net (Li et al., 2006) 0.82 ESA (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al., 2009)) 0.71 Distributional Similarity (on web corpus) (Agirre et 0.65 al., 2009)) Context Window based Distributional Similar- 0.60 ity (Agirre et al., 2009)) Latent Semantic Analysis (on web corpus) (Finkel- 0.56 stein et al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et a</context>
</contexts>
<marker>Recchia, Jones, 2009</marker>
<rawString>Gabriel Recchia and Michael N. Jones. 2009. More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis. Behavior Research Methods, 3(41):647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George G Simpson</author>
</authors>
<title>Mammals and the nature of continents.</title>
<date>1943</date>
<journal>American Journal of Science,</journal>
<pages>1--31</pages>
<contexts>
<context position="7870" citStr="Simpson, 1943" startWordPosition="1282" endWordPosition="1283">nd it and compare f(x) * f(y)/W + f(x) * t f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that variable. Task Semantic Sentence Synonym Relatedness Similarity Selection Dataset WordSim Li ESL TOEFL Metric Spearman Rank Pearson Cor- Fraction Fraction Correlation relation Correct Correct PMI 0.68 0.69 0.62 0.59 PMI3ig 0.67 0.85 0.58 0.56 cPMId 0.72 0.67 0.56 0.59 PMI</context>
<context position="11057" citStr="Simpson, 1943" startWordPosition="1774" endWordPosition="1775">rence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multipli</context>
</contexts>
<marker>Simpson, 1943</marker>
<rawString>George G. Simpson. 1943. Mammals and the nature of continents. American Journal of Science, pages 1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Wikirelate! computing semantic relatedness using wikipedia.</title>
<date>2006</date>
<booktitle>In AAAI 2006, Conference on Artificial Intelligence,</booktitle>
<pages>1419--1424</pages>
<contexts>
<context position="10211" citStr="Strube and Ponzetto, 2006" startWordPosition="1648" endWordPosition="1651">e best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with </context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia. In AAAI 2006, Conference on Artificial Intelligence, pages 1419–1424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In ECML 2001, European Conference on Machine Learning.</booktitle>
<contexts>
<context position="9240" citStr="Turney, 2001" startWordPosition="1502" endWordPosition="1503">.76 0.56 0.57 LLR 0.50 0.18 0.18 0.27 nPMI 0.72 0.35 0.54 0.54 Ochiai/ PMI2 0.62 0.77 0.62 0.60 SCI 0.65 0.85 0.62 0.60 Simpson 0.59 0.78 0.58 0.57 TTest 0.44 0.63 0.44 0.52 Semantic Net (Li et al., 2006) 0.82 ESA (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al., 2009)) 0.71 Distributional Similarity (on web corpus) (Agirre et 0.65 al., 2009)) Context Window based Distributional Similar- 0.60 ity (Agirre et al., 2009)) Latent Semantic Analysis (on web corpus) (Finkel- 0.56 stein et al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the me</context>
<context position="12951" citStr="Turney, 2001" startWordPosition="2068" endWordPosition="2070">ese measures compared to PMI and the original measures introducing significance. In addition, we also wish to compare the performance of these measures with other co-occurrence measures. To compare the performance of these measures with more resource heavy non co-occurrence based measures, we have chosen those tasks and datasets on which published results exist for distributional similarity and knowledge based word association measures. 4.1 Task Details We evaluate these measures on three tasks: Sentence Similarity(65 sentence-pairs from (Li et al., 2006)), Synonym Selection(50 questions ESL (Turney, 2001) and 80 questions TOEFL (Landauer and Dutnais, 1997) datasets), and, Semantic Relatedness(353 words Wordsim (Finkelstein et al., 2002) dataset). For each of these tasks, gold standard human judgment results exist. For sentence similarity, following (Li et al., 2006), we evaluate a measure by the Pearsons correlation between the ranking produced by the measure and the human ranking. For synonym selection, we compute the percentage of correct answers, since there is a unique answer for each challenge word in the datasets. Semantic relatedness has been evaluated by Spearman’s rank correlation wit</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In ECML 2001, European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wandmacher</author>
<author>E Ovchinnikova</author>
<author>T Alexandrov</author>
</authors>
<title>Does latent semantic analysis reflect human associations?</title>
<date>2008</date>
<booktitle>In ESSLLI 2008, European Summer School in Logic, Language and Information.</booktitle>
<contexts>
<context position="9756" citStr="Wandmacher et al., 2008" startWordPosition="1577" endWordPosition="1580">t al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram an</context>
</contexts>
<marker>Wandmacher, Ovchinnikova, Alexandrov, 2008</marker>
<rawString>T. Wandmacher, E. Ovchinnikova, and T. Alexandrov. 2008. Does latent semantic analysis reflect human associations? In ESSLLI 2008, European Summer School in Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Wartena</author>
</authors>
<title>Hsh: Estimating semantic similarity of words and short phrases with frequency normalized distance measures.</title>
<date>2013</date>
<booktitle>In SemEval 2013, International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="9920" citStr="Wartena, 2013" startWordPosition="1608" endWordPosition="1609">urrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence measures are statistically significant at p = .05. For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used fo</context>
</contexts>
<marker>Wartena, 2013</marker>
<rawString>Christian Wartena. 2013. Hsh: Estimating semantic similarity of words and short phrases with frequency normalized distance measures. In SemEval 2013, International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Washtell</author>
<author>Katja Markert</author>
</authors>
<title>A comparison of windowless and window-based computational association measures as predictors of syntagmatic human associations.</title>
<date>2009</date>
<booktitle>In EMNLP 2009, Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>628--637</pages>
<contexts>
<context position="1507" citStr="Washtell and Markert, 2009" startWordPosition="226" endWordPosition="229">ortant to have low-cost, highquality association measures. Lexical co-occurrence based word association measures are popular because they are computationally efficient and they can be applied to any language easily. One of the most popular co-occurrence measure is Pointwise Mutual Information (PMI) (Church and Hanks, 1989). One of the limitations of PMI is that it only works with relative probabilities and ignores the absolute amount of evidence. To overcome this, recently two new measures have been proposed that incorporate the notion of statistical significance in basic PMI formulation. In (Washtell and Markert, 2009), statistical significance is introduced in PMI,ig by multiplying PMI value with the square root of the evidence. In contrast, in (Damani, 2013), cPMId is introduced by bounding the probability of observing a given deviation between a given word pair’s cooccurrence count and its expected value under a null model where with each word a global unigram generation probability is associated. In Table 1, we give the definitions of PMI, PMI,ig, and cPMId. While these new measures perform better than PMI on some of the tasks, on many other tasks, we find that the new measures perform worse than the PM</context>
<context position="2913" citStr="Washtell and Markert, 2009" startWordPosition="462" endWordPosition="465">grades performance in two out of these four tasks. The experimental details and discussion are given in Section 4.2. Our analysis shows that while the basic ideas in incorporating statistical significance are reasonable, they have been applied slightly inappropriately. By fixing this, we get new measures that improve performance over not just PMI, but also on other popular co-occurrence measures on most of these tasks. In fact, the revised measures perform reasonably well compared with more resource intensive non cooccurrence based methods also. 2 Adapting PMI for Statistical Significance In (Washtell and Markert, 2009), it is assumed that the statistical significance of a word pair association is proportional to the square root of the evidence. The question of what constitutes the evidence is answered by taking the lesser of the frequencies of the two words in the word pair, since at most that many pairings are possible. Hence the PMI value is multi163 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 163–169, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Method Formula Revised Formula PMI (Church and log f(x,y) Han</context>
<context position="7925" citStr="Washtell and Markert, 2009" startWordPosition="1287" endWordPosition="1290"> f(x, y) = log 164 Method Formula ChiS ware // 2 (f(i ,j)−Ef(i,j))2 �l lx ) K,j Ef(i,j) Dice (Dice, 1945) f(x,y) f(x)+f(y) GoogleDistance (L.Cilibrasi and Vitany, 2007) max(log d(x),log d(y))−log d(x,y) log D−min(log d(x),log d(y)) Jaccard (Jaccard, 1912) f(x,y) f(x)+f(y)−f(x,y) LLR (Dunning, 1993) f(x0,y0) f(x0, � y0)log f(x0)f(y0) x0 ∈ {x, ¬x} y0 ∈ {y, ¬y} nPMI (Bouma, 2009) log f(x,y) f(x)∗f(y)/W 1 log f(x,y)/W Ochiai (Janson and Vegelius, 1981) f (x,y) √f(x)f(y) PMI2 (Daille, 1994) f(x,y) log f(x,y)2 log f(x)∗f(y)/W = 1 f(x)∗f(y) f(x,y)/W Simpson (Simpson, 1943) f(x,y) min(f(x),f(y)) SCI (Washtell and Markert, 2009) f (x,y) f(x)√f(y) T-test f(x,y)−Ef(x,y) �f(x,y)(1− f(x,y) ) W Table 2: Definition of other co-occurrence measures being compared in this work. The terminology used here is same as that in Table 1, except that E in front of a variable name means the expected value of that variable. Task Semantic Sentence Synonym Relatedness Similarity Selection Dataset WordSim Li ESL TOEFL Metric Spearman Rank Pearson Cor- Fraction Fraction Correlation relation Correct Correct PMI 0.68 0.69 0.62 0.59 PMI3ig 0.67 0.85 0.58 0.56 cPMId 0.72 0.67 0.56 0.59 PMIs 0.66 0.85 0.66 0.61 sPMId 0.72 0.75 0.70 0.61 ChiSqua</context>
<context position="11301" citStr="Washtell and Markert, 2009" startWordPosition="1810" endWordPosition="1813">l domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newly introduced measures with other co-occurrence measures, we have selected a number of popular co-occurrence measures like ChiSquare (X2), Dice (Dice, 1945), GoogleDistance (L.Cilibrasi and Vitany, 2007), Jaccard (Jaccard, 1912), LLR (Dunning, 1993), Simpson (Simpson, 1943), and T-test from these domains. In addition to these popular measures, we also experiment with other known variations of PMI like nPMI (Bouma, 2009), PMI2 (Daille, 1994), Ochiai (Janson and Vegelius, 1981), and SCI (Washtell and Markert, 2009). Since PMI2 is a monotonic transformation of Ochiai, we present their results together. In Table 2, we present the definitions of these measures. While the motivation given for SCI in (Washtell and Markert, 2009) is slightly different, in light of the discussion in Section 2.1, we can assume that SCI is PMI adapted for statistical significance (multiplied by -\/f(y)), where the site of random trial is taken to be the occurrences of the second word y, instead of the less frequent word, as in the case of PMI,ig. When counting co-occurrences, we only consider the non-overlapping span-constrained</context>
</contexts>
<marker>Washtell, Markert, 2009</marker>
<rawString>Justin Washtell and Katja Markert. 2009. A comparison of windowless and window-based computational association measures as predictors of syntagmatic human associations. In EMNLP 2009, Conference on Empirical Methods on Natural Language Processing, pages 628–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Yeh</author>
<author>Daniel Ramage</author>
<author>Chris Manning</author>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Wikiwalk: Random walks on wikipedia for semantic relatedness.</title>
<date>2009</date>
<booktitle>In TextGraphs 2009, Proceedings of the ACL workshop on Graphbased Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="8917" citStr="Yeh et al., 2009" startWordPosition="1454" endWordPosition="1457"> Metric Spearman Rank Pearson Cor- Fraction Fraction Correlation relation Correct Correct PMI 0.68 0.69 0.62 0.59 PMI3ig 0.67 0.85 0.58 0.56 cPMId 0.72 0.67 0.56 0.59 PMIs 0.66 0.85 0.66 0.61 sPMId 0.72 0.75 0.70 0.61 ChiSquare (x2) 0.62 0.80 0.62 0.58 Dice 0.58 0.76 0.56 0.57 GoogleDistance 0.53 0.75 0.09 0.19 Jaccard 0.58 0.76 0.56 0.57 LLR 0.50 0.18 0.18 0.27 nPMI 0.72 0.35 0.54 0.54 Ochiai/ PMI2 0.62 0.77 0.62 0.60 SCI 0.65 0.85 0.62 0.60 Simpson 0.59 0.78 0.58 0.57 TTest 0.44 0.63 0.44 0.52 Semantic Net (Li et al., 2006) 0.82 ESA (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al., 2009)) 0.71 Distributional Similarity (on web corpus) (Agirre et 0.65 al., 2009)) Context Window based Distributional Similar- 0.60 ity (Agirre et al., 2009)) Latent Semantic Analysis (on web corpus) (Finkel- 0.56 stein et al., 2002) WordNet::Similarity (Recchia and Jones, 2009) 0.70 0.87 PMI-IR3 (using context) (Turney, 2001) 0.73 Table 3: 5-fold cross-validation results for different co-occurrence measures. The results for the best, and second best co-occurrence measures for each data-set is shown in bold and underline respectively. Except GoogleDistance and LLR, all results for all co-occurrence</context>
<context position="10184" citStr="Yeh et al., 2009" startWordPosition="1644" endWordPosition="1647"> For each task, the best known result for different non co-occurrence based methods is also shown. 165 two words for distributional similarity (Agirre et al., 2009; Wandmacher et al., 2008; Bollegala et al., 2007; Chen et al., 2006). They are also used for modeling the meaning of a phrase or a sentence (Grefenstette and Sadrzadeh, 2011; Wartena, 2013; Mitchell, 2011; G. Dinu and Baroni, 2013; Kartsaklis et al., 2013). Knowledge-based measures use knowledgesources like thesauri, semantic networks, or taxonomies (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009). Co-occurrence based measures (Pecina and Schlesinger, 2006) simply rely on unigram and bigram frequencies of the words in a pair. In this work, our focus is on the co-occurrence based measures, since they are resource-light and can easily be used for resource-scarce languages. 3.1 Co-occurrence Measures being Compared Co-occurrence based measures of association between two entities are used in several domains like ecology, psychology, medicine, language processing, etc. To compare the performance of our newl</context>
</contexts>
<marker>Yeh, Ramage, Manning, Agirre, Soroa, 2009</marker>
<rawString>Eric Yeh, Daniel Ramage, Chris Manning, Eneko Agirre, and Aitor Soroa. 2009. Wikiwalk: Random walks on wikipedia for semantic relatedness. In TextGraphs 2009, Proceedings of the ACL workshop on Graphbased Methods for Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
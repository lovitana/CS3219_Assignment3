<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.984439">
Understanding and Quantifying Creativity in Lexical Composition
</title>
<author confidence="0.992054">
Polina Kuznetsova Jianfu Chen Yejin Choi
</author>
<affiliation confidence="0.961371">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.927758">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.999283">
{pkuznetsova,jianchen,ychoi}@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998784588235294">
Why do certain combinations of words such
as “disadvantageous peace” or “metal to the
petal” appeal to our minds as interesting ex-
pressions with a sense of creativity, while
other phrases such as “quiet teenager”, or
“geometrical base” not as much? We present
statistical explorations to understand the char-
acteristics of lexical compositions that give
rise to the perception of being original, inter-
esting, and at times even artistic. We first ex-
amine various correlates of perceived creativ-
ity based on information theoretic measures
and the connotation of words, then present ex-
periments based on supervised learning that
give us further insights on how different as-
pects of lexical composition collectively con-
tribute to the perceived creativity.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946153846154">
An essential property of natural language is the gen-
erative capacity that makes it possible for people to
express indefinitely many thoughts through indef-
initely many different ways of composing phrases
and sentences (Chomsky, 1965). The possibility of
novel, creative expressions never seems to exhaust.
Various types of writers, such as novelists, journal-
ists, movie script writers, and creatives in adver-
tising, continue creating novel phrases and expres-
sions that are original while befitting in expressing
the desired meaning in the given situation. Consider
unique phrases such as “geological split personal-
ity”, or “intoxicating Shangri-La of shoes”,1 that
</bodyText>
<note confidence="0.456291">
1Examples from New York Times articles in 2013.
</note>
<bodyText confidence="0.999900735294117">
continue flowing into the online text drawing atten-
tion from readers.
Writers put significant effort in choosing the per-
fect words in completing their compositions, as a
well-chosen combination of words is impactful in
readers’ minds for rendering the precise intended
meaning, as well as stimulating an increased level
of cognitive responses and attention. Metaphors in
particular, one of the quintessential forms of lin-
guistic creativity, have been discussed extensively
by studies across multiple disciplines, e.g., Cog-
nitive Science, Psychology, Linguistics, and Liter-
ature (e.g., Lakoff and Johnson (1980), McCurry
and Hayes (1992), Goatly (1997)). Moreover, re-
cent studies based on fMRI begin to discover bio-
logical evidences that support the impact of creative
phrases on people’s minds. These studies report that
unconventional metaphoric expressions elicit signif-
icantly increased involvement of brain processing
when compared against the effect of conventional
metaphors or literal expressions (e.g., Mashal et al.
(2007), Mashal et al. (2009)).
Several linguistic elements, e.g., syntax, seman-
tics, and pragmatics, are likely to be working to-
gether in order to lead to the perception of creativ-
ity. However, their underlying mechanisms by and
large are yet to be investigated. In this paper, as a
small step toward quantitative understanding of lin-
guistic creativity, we present a focused study on lex-
ical composition two content words.
Being creative, by definition, implies qualities
such as being unique, novel, unfamiliar or uncon-
ventional. But not every unfamiliar combination of
words would appeal as creative. For example, unfa-
</bodyText>
<page confidence="0.916656">
1246
</page>
<note confidence="0.7304425">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1246–1258,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.984974561403509">
miliar biomedical terms, e.g., “cardiac glycosides”,
are only informative without appreciable creativity.
Similarly, less frequent combinations of words, e.g.,
“rotten detergent” or “quiet teenager”, though de-
scribing situations that are certainly uncommon, do
not bring about the sense of creativity. Finally, some
unique combinations of words can be just nonsensi-
cal , e.g., “elegant glycosides”.
Different studies assumed different definitions of
linguistic creativity depending on their context and
end goals (e.g., Chomsky (1976), Zhu et al. (2009),
Gerv´as (2010), Maybin and Swann (2007), Carter
and McCarthy (2004)). In this paper, as an opera-
tional definition, we consider a phrase creative if it
is (a) unconventional or uncommon, and (b) expres-
sive in an interesting, imaginative, or inspirational
way.
A system that can recognize creative expressions
could be of practical use for many aspiring writers
who are often in need of inspirational help in search-
ing for the optimal choice of words. Such a system
can also be integrated into automatic assessment of
writing styles and quality, and utilized to automat-
ically construct a collection of interesting expres-
sions from the web, which may be potentially useful
for enriching natural language generation systems.
With these practical goals in mind, we aim to un-
derstand phrases with linguistic creativity in a broad
scope. Similarly as the work of Zhu et al. (2009),
our study encompasses phrases that evoke the sense
of interestingness and creativity in readers’ minds,
rather than focusing exclusively on clearly but nar-
rowly defined figure of speeches such as metaphors
(e.g., Shutova (2010)), similes (e.g., Veale et al.
(2008), Hao and Veale (2010)), and humors (e.g.,
Mihalcea and Strapparava (2005), Purandare and
Litman (2006)). Unlike the study of Zhu et al.
(2009), however, we concentrate specifically on how
combinations of different words give rise to the
sense of creativity, as this is an angle that has not
been directly studied before. We leave the roles of
syntactic elements as future research.
We first examine various correlates of perceived
creativity based on information theoretic measures
and the connotation of words, then present experi-
ments based on supervised learning that give us fur-
ther insights on how different aspects of lexical com-
position collectively contribute to the perceived cre-
ativity.
2 Theories of Creativity and Hypotheses
Many researchers, from the ancient philosophers to
the modern time scientists, have proposed theories
that attempt to explain the mechanism of creative
process. In this section, we draw connections from
some of these theories developed for general human
creativity to the problem of quantitatively interpret-
ing linguistic creativity in lexical composition.
</bodyText>
<subsectionHeader confidence="0.998484">
2.1 Divergent Thinking and Composition
</subsectionHeader>
<bodyText confidence="0.999835114285714">
Divergent thinking (e.g., McCrae (1987)), which
seeks to generate multiple unstereotypical solutions
to an open ended problem has been considered as
the key element in creative process, which contrasts
with convergent thinking that find a single, cor-
rect solution (e.g., Cropley (2006)). Applying the
same high-level idea to lexical composition, diver-
gent composition that explores an unusual, uncon-
ventional set of words is more likely to be creative.
Note that the key novelty then lies in the composi-
tional operation itself, i.e., the act of putting together
a set of words in an unexpected way, rather than the
rareness of individual words being used. In recent
years there has been a swell of work on composi-
tional distributional semantics that captures the com-
positional aspects of language understanding, such
as sentiment analysis (e.g., Yessenalina and Cardie
(2011), Socher et al. (2011)) and language model-
ing (e.g., Mitchell and Lapata (2009), Baroni and
Zamparelli (2010), Guevara (2011), Clarke (2012),
Rudolph and Giesbrecht (2010)). However, none
has examined the compositional nature in quantify-
ing creativity in lexical composition.
We consider two computational approaches to
capture the notion of creative composition. The first
is via various information theoretic measures, e.g.,
relative entropy reduction, to measure the surprisal
of seeing the next word given the previous word.
The second is via supervised learning, where we ex-
plore different modeling techniques to capture the
statistical regularities in creative compositional op-
erations. In particular, we will explore (1) compo-
sitional operations of vector space models, (2) ker-
nels capturing the non-linear composition of differ-
ent dimensions in the meaning space, (3) the use of
</bodyText>
<page confidence="0.987566">
1247
</page>
<bodyText confidence="0.994075">
neural networks as an alternative to incorporate non-
linearity in vector composition. (See §5).
</bodyText>
<subsectionHeader confidence="0.9407325">
2.2 Latent Memory and Creative Semantic
Subspace
</subsectionHeader>
<bodyText confidence="0.998998621621622">
Although we expect that unconventional composi-
tion has a connection to creativeness of resulting
phrases, that alone does not explain many counter
examples where the composition itself is uncommon
but the resulting expression is not creative due to
lack of interestingness or imagination, e.g., “room
and water”.2 Therefore, we must consider addi-
tional conditions that give rise to creative phrases.
Let S represent the semantic space, i.e., the set of
all possible semantic representation that can be ex-
pressed by a phrase that is composed of two content
words.3 Then we hypothesize that some subsets of
semantic space {SjjSZ C S} are semantically futile
regions for appreciable linguistic creativity, regard-
less of how novel the composition in itself might be.
Such regions may include technical domains such as
law or pharmacology. Similarly, we expect seman-
tically fruitful subsets of semantic space where cre-
ative expressions are more frequently found. For in-
stance, phrases such as “guns and roses” and “metal
to the petal” are semantically close to each other and
yet both can be considered as interesting and cre-
ative (as opposed to one of them losing the sense of
creativity due to its semantic proximity to the other).
This notion of creative semantic subspace con-
nects to theories that suggest that latent memories
serve as motives for creative ideas and that one’s
creativity is largely depending on prior experience
and knowledge one has been exposed to (e.g., Freud
(1908), Necka (1999), Glaskin (2011), Cohen and
Levinthal (1990), Amabile (1997)), a point also
made by Einstein: “The secret to creativity is know-
ing how to hide your sources.”
Figure 5 presents visualized supports for creative
semantic subspace,4 where we observe that phrases
in the neighborhood of legal terms are generally
not creative, while the semantic neighborhood of
</bodyText>
<footnote confidence="0.997895142857143">
2With additional context this example may turn into a cre-
ative one, but for simplicity we focus on phrases with two con-
tent words considered out of context.
3Investigation on recursive composition of more than two
content words and the influence of syntactic packaging is left as
future research.
4See §6 for more detailed discussion.
</footnote>
<table confidence="0.9375556">
Source # of # of Avg Entropy
uniq sent sent
words len
QUOTESraw 29498 49402 28 173.05
GLOSSESraw 20869 7745 53 96.79
</table>
<tableCaption confidence="0.998176">
Table 1: Entropy of word distribution in datasets
</tableCaption>
<table confidence="0.99986025">
# of word pairs percentage
Dataset total #(-) #(+) #(+)/total %
GLOSSES 1912 149 18 0.94
QUOTES 3298 204 35 1.06
</table>
<tableCaption confidence="0.979223">
Table 2: Distribution of creative(+)/common(-) word
pairs over GLOSSES and QUOTES dataset.
</tableCaption>
<bodyText confidence="0.999935">
“kingdom” and “power” is relatively more fruitful
for composing creative (i.e., unique and uncommon
while being imaginative and interesting, per our op-
erational definition of creativity given in §1) word
pairs, e.g., invisible empire”. In our empirical in-
vestigation, this notion of semantically fruitful and
futile semantic subspaces are captured using dis-
tributional semantic space models under supervised
learning framework (§5).
</bodyText>
<subsectionHeader confidence="0.999456">
2.3 Affective Language
</subsectionHeader>
<bodyText confidence="0.999992875">
Another angle we probe is the connection between
creative expressions and the use of affective lan-
guage. This idea is supported in part by previ-
ous research that explored the connection between
figurative languages such as metaphors and senti-
ment (e.g., Fussell and Moss (1998), Rumbell et
al. (2008), Rentoumi et al. (2012)). The focus of
previous work was either on interpretation of the
sentiment in metaphors, or the use of metaphors
in the description of affect. In contrast, we aim
to quantify the correlation between creative expres-
sions (beyond metaphors) and the use of sentiment-
laden words in a more systematic way. This explo-
ration has a connection to the creative semantic sub-
space discussed earlier (§2.2), but pays a more direct
attention to the aspect of sentiment and connotation.
</bodyText>
<sectionHeader confidence="0.991472" genericHeader="method">
3 Creative Language Dataset
</sectionHeader>
<bodyText confidence="0.9997412">
We start our investigation by considering two types
of naturally existing collection of sentences: (1)
quotes and (2) dictionary glosses. We expect that
quotes are likely to be rich in creative expressions,
while dictionary glosses stand in the opposite spec-
</bodyText>
<page confidence="0.981273">
1248
</page>
<figure confidence="0.988642939393939">
Glosses Quotes All
30
25
20
15
10
5
0
% of word pairs
% of word pairs
25
20
15
40
60
50
30
1 5 9 13 17 21 25 29 33 37
bucket
less freq more freq
10
5
0
1 9 17 25 33 41 49 57 65
bucket
20
10
0
11 21 31 41 51 61 71 81 91
bucket
less freq more freq less freq more freq
% of word pairs
(a) (b) (c)
</figure>
<figureCaption confidence="0.9913775">
Figure 1: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of frequencies (x-axis) for GLOSSES, QUOTES and both datasets combined.
</figureCaption>
<figure confidence="0.9933377">
Glosses Quotes All
20
15
10
5
0
% of word pairs
% of word pairs
% of word pairs
40
30
20
10
0
20
15
1 5 9 13 17 21 25 29 33 37
bucket
lower val higher val
25
10
5
0
1 9 17 25 33 41 49 57 65
bucket
11 21 31 41 51 61 71 81 91
bucket
50
lower val higher val lower val higher val
(a) (b) (c)
</figure>
<figureCaption confidence="0.9967455">
Figure 2: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of PMI values (x-axis) for GLOSSES, QUOTES and both datasets combined.
</figureCaption>
<bodyText confidence="0.9912302">
trum of being creative.
QUOTESraw: We crawled inspirational quotes
from “Brainy Quote”.5
GLOSSESraw: We collected glosses from Ox-
ford Dictionary and Merriam-Webster Dictionary.6
Overall we crawled about 8K definitions. Table 1
shows statistics of the dataset.7
Entropy of word distribution We conjecture that
QUOTES and GLOSSES are different in terms of
word variety, which can be quantified by the entropy
</bodyText>
<footnote confidence="0.922975666666667">
5http://www.brainyquote.com/
6http://oxforddictionaries.com/ and http://www.merriam-
webster.com/. We only consider words appearing in both dic-
</footnote>
<bodyText confidence="0.7526794">
tionaries to avoid unusual words such as compound words, e.g.,
“zero-base”.
7QUOTESraw contain 30K unique words and GLOSSESraw
has 20K unique words. QUOTESraw have much arger number
of sentences, while its average sentence is shorter.
of word distributions. To compute the entropy for
each dataset, we use ngram statistics from the corre-
sponding dataset to measure the probability of each
word. As expected, QUOTES dataset has higher
entropy than GLOSSES in Table 1.
</bodyText>
<subsectionHeader confidence="0.9994">
3.1 Creative Word Pairs
</subsectionHeader>
<bodyText confidence="0.999957181818182">
We extract word pairs corresponding to the follow-
ing syntactic patterns: [NN NN], [JJ NN], [NN JJ]
and [JJ JJ]. Not all pairs from QUOTESraw are cre-
ative, and likewise, not all pairs from GLOSSESraw
are uncreative. Therefore, we perform manual an-
notations to a subset of the collected pairs as fol-
lows. We obtain a small subset of pairs by apply-
ing stratified sampling based on bigram frequency
buckets: first we sort word pairs by their bigram
frequencies obtained from Web 1T corpus (Brants
and Franz (2006)), group them into consecutive fre-
</bodyText>
<page confidence="0.971879">
1249
</page>
<bodyText confidence="0.976530104166666">
quency buckets each of which containing 400 word
pairs, then sample 40 word pairs from each bucket.
We label word pairs using Amazon Mechnical
Turk (AMT) (e.g., Snow et al. (2008)). We ask three
turkers to score each pair in 1-5 scale, where 1 is the
least creative and 5 is the most creative. We then
obtain the final creativity scale score by averaging
the scores over 3 users. In addition, we ask turkers
a series of yes/no questions to help turkers to deter-
mine whether the given pair is creative or not.8 We
determine the final label of a word pair based on two
scores, creativity scale score and yes/no question-
based score. If creativity scale score is 4 or 5 and
question-based score is positive, we label the pair as
creative. Similarly, if creativity scale score is 1 or
2 and question-based score is negative, we label the
pair as common. We discard the rest from the final
dataset. This filtering process is akin to the removal
of neural sentiment in the early work of sentiment
analysis (e.g., Pang et al. (2002)).9 Table 2 shows
the statistics of the resulting dataset.
Creative Pairs and their Frequencies: To gain
insights on the stratified sample of word pairs, we
plot the label (E {creative, common}) distribution
of word pairs as a function of simple statistics, such
as a range (bucket) of bigram frequencies or PMI
values of the given pair of words. Both bigram fre-
quencies and PMI scores are computed based on
Google Web 1T corpus Brants and Franz (2006).
Figure 1 shows the results for word frequencies. As
expected, word pairs with high frequencies are much
more likely to be common, while word pairs with
low frequencies can be either of the two. Also as ex-
pected, pairs extracted from QUOTES are relatively
more likely to be creative than those from GLOSSES.
In any case, it is clear that not all rare pairs are cre-
ative.
Creative Pairs and their PMI Scores: Similarly
as above, Figure 2 plots the relation between the
distribution of labels of word pairs and their corre-
sponding PMI. As expected, pairs with high PMI are
more likely to be common, though the trend is not as
8E.g., “is this word combination boring and not original?”
or “does it provoke unusual imagination?”.
9Cohen’s Kappa and Pearson Correlation on the filtered data
are 0.69 and 0.72 respectively. Corresponding scores for the un-
filtered data drop to 0.26 and 0.29 respectively. All the experi-
ments are performed on the filtered data.
</bodyText>
<figure confidence="0.635779416666667">
Common Creative
quiet teenager inglorious success
constant longitude thorny existence
watery juice relaxed symmetry
noble political sardonic destiny
diet cooking dispassionate history
verbal interpretation poetical enthusiasm
unwelcome situation verbal beauty
migratory tuna earth breathe
lousy businessman disadvantageous peace
terrific marriage alchemical marriage
solved issue deep nonsense
</figure>
<tableCaption confidence="0.99247">
Table 3: Sample Creative / Common Word Pairs
</tableCaption>
<bodyText confidence="0.98158385">
skewed as before.
Final Dataset: From our initial annotation study,
it became apparent to us that creative pairs are very
rare, perhaps not surprisingly, even among infre-
quent pairs. In order to build the word pair corpus
with as many creative pairs as possible, we focus on
infrequent word pairs for further annotation, from
which we construct a larger and balanced set of cre-
ative and common word pairs, with 394 word pairs
for each class. The specific construction procedure
is as follows: first combine all of the word pairs
extracted from both QUOTESraw and GLOSSESraw
as a single dataset, sort them by bigram frequency,
group them into consecutive frequency buckets each
of which has 40 word pairs; finally balance each fre-
quency bucket, by discarding word pairs with higher
frequency value from the larger class in that bucket.
Examples of labeled word pairs are shown in Ta-
ble 3. Hereafter we use this balanced dataset of word
pairs for all experiments.10
</bodyText>
<sectionHeader confidence="0.985691" genericHeader="method">
4 Creativity Measures
</sectionHeader>
<subsectionHeader confidence="0.927548">
4.1 Information Measures
</subsectionHeader>
<bodyText confidence="0.999867625">
In this section we explore information theoretic
measures to quantify the surprisal aspect of creative
word pairs, relating to the divergent, compositional
nature of creativity discussed in §2.1.
Entropy of Context Seeing a word w changes our
expectation on what might follow next. Some words
have stronger selective preference (higher entropy)
than others.
</bodyText>
<footnote confidence="0.665845">
10The resulting dataset is available at http://www.cs.
stonybrook.edu/˜pkuznetsova/creativity/
</footnote>
<page confidence="0.934918">
1250
</page>
<figure confidence="0.99865828">
H(w1w2) RH(w1,w2) KL(w1w2,w2)
% of word pairs
90
70
50
30
10
2 4 6 8 10 12 14 16 18 20 22
% of word pairs
90
70
50
30
10
2 4 6 8 10 12 14 16 18 20 22
% of wordpairs
90
70
50
30
10
0 2 4 6 8 10 12 14 16 18 20 22
bucket bucket bucket
(a) (b) (c)
(d) (e) (f)
</figure>
<figureCaption confidence="0.997438">
Figure 3: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of information or polarity measures (x-axis).
</figureCaption>
<figure confidence="0.999809485714286">
bucket bucket bucket
MI(w1,w2)
diff diff
Lsubj(w1,w2)
2 4 6 8 10 12 14 16 18 20 22
50
40
30
20
0 2 4 6 8 10 12 14 16 18 20 22
% of word pairs
80
70
60
% of word pairs
40
80
70
60
50
30
90
70
50
30
10
% of word pairs
2 4 6 8 10 12 14 16 18 20 22
0.06
0.05
0.04
0.03
0.02
0.01
0
</figure>
<figureCaption confidence="0.9943545">
Figure 4: Conditional probability of neighboring words
for “inglorious” (filled / red) and “very” (unfilled / blue).
</figureCaption>
<bodyText confidence="0.956078214285714">
For instance, the entropy after seeing “very”
would be higher than that after seeing “inglorious”,
as the former can be used in a wider variety of con-
text than the later. Figure 4 visualizes relatively
more skewed distribution of “inglorious”. We com-
pute the entropy of future context conditioning on
w1, w2 and w1w2, which we denote as H(w1),
H(w2), H(w1w2) respectively, latter is shown in
Figure 3 – a.11
11As before, language models are drawn from Google Web
Relative Entropy Transformation In order to fo-
cus more directly on the relative change of entropy
as a result of composition, we compute Relative En-
tropy Transformation:
</bodyText>
<equation confidence="0.9999395">
RH(w1,w2) = |H(w1) − H(w1w2) |(1)
H(w1) + H(w1w2)
</equation>
<bodyText confidence="0.999962666666667">
As expected (Figure 3 – b and Table 4), this relative
quantity captures creativity better than the absolute
measure H(w1w2) computed above. The idea be-
hind this measure has a connection to uncertainty
reduction in psycholinguistic literature (e.g., Frank
(2010), Hale (2003), Hale (2006)).
KL divergence To capture unusual combinations
of words, we compare the difference between the
distributional contexts of w1 and w1w2 so that
</bodyText>
<equation confidence="0.930053666666667">
P(wi|w1, w2)
P(wi|w1, w2) log
P(wi|w1)
(2)
Figure (3 – c) shows that KL(w1w2, w1)12 is among
Lconn(w1,w2)
</equation>
<footnote confidence="0.901003666666667">
1T corpus Brants and Franz (2006).
12We also compute KL(w1, w2) in a similar manner as
KL(w1w2, w1)
</footnote>
<figure confidence="0.910413617647059">
Milton
as
best
busy
clear
common
cool
deep
early
end
expensive
final
glad
hard
hot
ingot
last
likely
many
next
own
pleased
professional
rarely
role
seriously
slow
special
success
to
valuable
welcome
�KL(w1w2, w1) =
w;EV
</figure>
<page confidence="0.913192">
1251
</page>
<bodyText confidence="0.479275666666667">
the effective measures in capturing creative pairs.
Mutual Information Finally, we consider mutual
information (Figure 3 – d):
</bodyText>
<equation confidence="0.9999278">
�MI(w1,w2) = P(wi|w1, w2) x
w;EV
P(wi|w1, w2)
log (3)
P(wi |w1) · P(wi |w2)
</equation>
<bodyText confidence="0.9996431875">
Correlation coefficients Pearson coefficients for
all measures are shown in Table 4. Interestingly, in-
formation theoretic measures that compare the dis-
tribution of word’s context, such as RH(w1, w2),
KL(w1w2, w1) and MI(w1, w2), capture the sur-
prisal aspect of creativity better than simple frequen-
cies or PMI scores that do not consider contextual
changes. But even for those cases when the corre-
lation is statistically significant, the values are not
too high. We conjecture that there are two reasons
for this. First, Pearson assumes linear correlations,
hence not sensitive enough to capture non-linear cor-
relations that are evident in graphs shown in Fig-
ure 3. Second, these measures only capture the sur-
prisal aspect of creativity, missing the other impor-
tant qualities: interestingness or imaginativeness.
</bodyText>
<subsectionHeader confidence="0.993955">
4.2 Sentiment and Connotation
</subsectionHeader>
<bodyText confidence="0.9999501875">
Next we investigate the connection between creativ-
ity and sentiment, as illustrated in §2.3. We con-
sider both sentiment (more explicit) and connotation
(more implicit) words,13 and consider them with
or without distinguishing the polarity (i.e., positive,
negative). To determine sentiment and connotation,
we use lexicons provided by OpinionFinder (Wilson
et al. (2005)) and Feng et al. (2013) respectively. We
denote polarity of a word wi as L(wi).14 When wi
has a negative polarity L(wi) is assigned a value of
-1, and when wi is positive L(wi) is equal to 1. We
assume that a word is neutral when it is not in the
lexicon, assigning 0 to L(wi). For a word pair w1w2
we compute absolute difference Ldiff(w1, w2) be-
tween polarities of tokens in a word pair in order to
catch examples such as “inglorious success”.
</bodyText>
<footnote confidence="0.71247575">
13E.g., expressions such as “blue sky” or “white sand” are
not sentiment-laden, but do have positive connotation.
14We denote polarity from OpinionFinder as Lsubj and con-
notation as Lconn
</footnote>
<table confidence="0.999805285714286">
Measure Corr Coeff p-value* adj p-value**
pointwise, noncontextual
Freq(w1w2) 0.014 0.67 0.86
PMI(w1, w2) 0.011 0.75 0.86
information theoretic, contextual
-0.038 0.26 0.49
-0.126 0.00019 0.00083
E(w1, w2) 0.013 0.71 0.86
RH(w1, w2) 0.113 0.00081 0.0024
KL(w1w2, w1) 0.134 7.152-05 0.00054
KL(w1, w2) -0.080 0.018 0.039
MI(w1,w2) 0.125 0.00022 0.00083
sentiment &amp; connotation
Lsubj(w1) 0.006 0.87 0.87
Lsubj(w2) 0.031 0.36 0.60
Ldiff(w1 w2) 0.168 6.67e-07 1.00e-05
subj
Lconn(w1) 0.023 0.49 0.74
Lconn(w2) 0.008 0.80 0.86
Ldiff 0.082 0.015 0.038
conn(w1, w2)
</table>
<tableCaption confidence="0.8757825">
Table 4: Pearson correlation between various measures
and creativity of word pairs. Boldface denotes statistical
significance (p &lt; 0.05).
note *: Two-tailed p-value, 394 word pairs per class
note **: We used Benjamini-Hochberg method to adjust
p-values for multiple tests
</tableCaption>
<bodyText confidence="0.995108">
Table 4 shows Pearson coefficient for sentiment
and connotation based measures. It turns out that
polarity of each word on its own does not have a
high impact on the creativity of a word pair. Rather,
it is the difference between the two words that gives
rise the sense of creativity.
</bodyText>
<subsectionHeader confidence="0.999654">
4.3 Learning to Recognize Creativity
</subsectionHeader>
<bodyText confidence="0.9999225">
Now we put together all measures explored in §4.1
and 4.2 in a supervised-learning framework. As ex-
pected, rather than either one alone, the combination
of various measures leads to the best performance:
</bodyText>
<equation confidence="0.997311285714286">
F12 = [RH(w1, w2); KL(w1, w2); H(w1w2);
Ldiff
����(w1, w2); PMI(wl, w2);
H(w2); KL(w1w2, w1); KL(w2, w1);
di f f
Lsub, (w1, w2); MI(w1, w2);
Freq(w1w2); H(w1)]
</equation>
<bodyText confidence="0.9962125">
Table 5 shows the performance of the above fea-
ture vector with 12 features using libsvm (Chang and
Lin, 2011). We use C-Support Vector Classification
(C-SVC). Performance is reported in accuracy using
5-fold cross validation.15
15Among these 12 features, the feature selection algorithm
</bodyText>
<page confidence="0.995542">
1252
</page>
<sectionHeader confidence="0.9565025" genericHeader="method">
5 Learning Creative Pairs with
Distributional Semantic Vectors
</sectionHeader>
<bodyText confidence="0.999974470588235">
The measures explored in §4 were largely unin-
formed of distributional semantic dimensions of
each word. However, in order to pursue the concep-
tual aspect of creativity illustrated in §2.2, that is, the
notion of semantic subspaces that are inherently fu-
tile or fruitful for creativity, we need to incorporate
semantic representations more directly. We there-
fore explore the use of distributional vector space
models. Another goal of this section will be addi-
tional learning-based investigation to the composi-
tional nature of creative word pairs, complementing
the investigation in §4, which focused on the com-
positional aspect of creativity described in §2.1.
With above goals in mind, in what follows, we ex-
plore three different ways to learn compositional as-
pect of creative word pairs: (1) learning with explicit
compositional vector operations (§5.1), (2) learning
nonlinear composition via kernels (§5.2), (3) learn-
ing nonlinear composition via deep learning (§5.3).
Note that in all these approaches, the notion of cre-
ative semantic subspace is integrated indirectly, as
the feature representation always incorporates the
resulting (composed) vector representations.
Baseline &amp; Configuration We consider the con-
catenation of two word vectors [191; 192] as the base-
line, since it can be viewed as what simple bag-of-
word features would be. Since the size of creative
pair dataset is not at scale yet, we choose to work
with vector space models that are in reduced dimen-
sions. We experimented with both Non-Negative
Sparse Embedding (Murphy et al. (2012)) and neu-
ral semantic vectors of Huang et al. (2012), but re-
port experiments with the latter only as those gave
us slightly better results.
</bodyText>
<subsectionHeader confidence="0.982636">
5.1 Compositional Vector Operations
</subsectionHeader>
<bodyText confidence="0.999117">
We consider the following compositional vector op-
erations inspired by recent studies for composi-
tional distributional semantics (e.g., Guevara (2011),
Clarke (2012), Mitchell and Lapata (2008), Wid-
dows (2008)).
</bodyText>
<listItem confidence="0.988412428571428">
• ADD: 191 + 192
• DIFF: abs(191 − 192)
of Chen and Lin (2005) determines that the most two important
ones are RH(w1, w2) and KL(w1, w2).
• MULT: 191 .* 192
• MIN: min{191, 192}
• MAX: max{191, 192}
</listItem>
<bodyText confidence="0.997515666666667">
All operations take two input vectors E Rn, and
output a vector E Rn. Each operation is applied
element-wise. We then perform binary classifica-
tion over the composed vectors using linear SVM.
Besides using features based on the composed vec-
tors, we also experiment with features based on con-
catenating multiple composed vectors, in the hope to
capture more diverse compositional operations. See
Table 5 for more details and experimental results.
</bodyText>
<subsectionHeader confidence="0.9661675">
5.2 Learning Nonlinear Composition via
Kernels
</subsectionHeader>
<bodyText confidence="0.9990042">
As an alternative to explicit vector compositions, we
also probe implicit operations based on non-linear
combinations of semantic dimensions using kernels
(e.g., Sch¨olkopf and Smola (2002), Shawe-Taylor
and Cristianini (2004)), in particular:
</bodyText>
<listItem confidence="0.999979">
• Polynomial: K(x, y) = (yxT y + r)d, y &gt; 0
• RBF: K(x, y) = exp(−-y IIx − yII2), y &gt; 0
• Laplacian: K(x, y) = exp(−-y IIx − yII), y &gt; 0
</listItem>
<subsectionHeader confidence="0.9965275">
5.3 Learning Non-linear Composition via Deep
Learning
</subsectionHeader>
<bodyText confidence="0.985486714285714">
Yet another alternative to model non-linear com-
position is deep learning. To learn the non-linear
transformation of a pair of semantic vectors, we ex-
plore the use of autoencoders (e.g., Pollack (1990),
Voegtlin and Dominey (2005)). We follow the for-
mulation of vector composition proposed by Socher
et al. (2011) except that we do not stack autoen-
coders for recursion. More specifically, given the
two input words 191, 192 E Rn, we want to learn
a vector space representation of their combination
p E Rn. The recursive auto encoder (RAE) of
Socher et al. (2011) models the composition of a
word pair as a non-linear transformation of their
concatenation [191; 192]:
</bodyText>
<equation confidence="0.997538">
fi1 = f(M1[191; 192] +1b1) (4)
</equation>
<bodyText confidence="0.9876084">
where M1 E Rn 2n. After adding a bias term
1b1 E Rn, a nonlinear element-wise function f such
as tanh is applied to the resulting vector. The repre-
sentation p of the word pair is then fed into a recon-
struction layer to reconstruct the two input vectors,
</bodyText>
<page confidence="0.906363">
1253
</page>
<table confidence="0.997261888888889">
Methods Accuracy
Creativity measures (§4.3)
1�12 62.30
Baseline: vector concatenation (no composition)
[191; 192] 67.51
Explicit vector composition (§5.1)
191 + 192 66.62
abs(191 − 192) 60.03
min{191, 192} 66.08
max{191, 192} 64.97
191 .* 192 56.34
[abs(191 − 192); 191; 192] 69.54
[max{191, 192}; 191; 192] 68.02
Non-linear composition via kernels (§5.2)
Polynomial 65.86
RBF 69.16
Laplacian 68.15
Non-linear composition via deep learning (§5.3)
</table>
<tableCaption confidence="0.99979">
Table 5: Performance comparison of creativity classifiers.
</tableCaption>
<bodyText confidence="0.9457207">
alone does not perform better than vector concate-
nation [191; 192]. However, combining abs(191 − 192)
or max{191; 192} with [191; 192] perform better than
concatenation. Kernels with non-linear transforma-
tion of feature space generally improve performance
over linear SVM, suggesting that kernels capture
some of the interesting compositional aspect of cre-
ativity that is not covered by some of the explicit
vector compositions considered in §5.1. We also ex-
perimented with additional features driven from the
creativity measures explored in §4, but we omit their
results as those did not help improving the perfor-
mance. Unfortunately learning nonlinear composi-
tion with deep learning did not yield better results.
We conjecture that it is due to the small dataset we
were able to obtain for this study, which may have
not been enough to learn the rich parameter space of
the nonlinear transformation matrix.
f(M1[191; 192] +1b1) 67.25
Semantically close y*
</bodyText>
<sectionHeader confidence="0.488221" genericHeader="method">
6 Analysis and Insight
</sectionHeader>
<table confidence="0.946996307692308">
Incorrectly predicted y*
word pairs word pairs
CONFUSION DUE TO WORD SIMILARITY (20/42)
“entire carton” - “whole angst” +
“outdated tax” - “graconian tax” +
“dismissive way” - “amorous way” +
“insidious part” + “leather part” -
CONFUSION DUE TO SUBJECTIVE LABELING (8/42)
“independent + “wonderful -
religion” religion”
WORD SENSE DISAMBIGUATION PROBLEMS (2/42)
“fiscal cliff” “winding lake” +
“opera window” + “work-shop floor” -
</table>
<tableCaption confidence="0.8812682">
Table 6: Error analysis: y* denotes the true label. For
each incorrectly predicted word pair (left column), we
show an example of semantically close word pairs (right
column) with the opposite true label that might have con-
fused learning.
</tableCaption>
<bodyText confidence="0.99971725">
and a softmax layer to predict the probability of the
word pair being creative and not creative. We ini-
tialize the word vectors using the pre-learned vector
space representations in Huang et al. (2012).
</bodyText>
<subsectionHeader confidence="0.99176">
5.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999908766666666">
Table 5 shows the performance comparison of dif-
ferent features sets and algorithms. In all cases,
parameters are tuned from the training portion of
the data. We see that simple vector composition
Error analysis We manually inspected a ran-
domly chosen 42 error cases, and characterize the
potential causes of those errors. Examples of three
types of errors are shown in Table 6. For each incor-
rectly predicted word pair, we also show a seman-
tically close word pair with the opposite true label
that might have confused the learning algorithm.
Visualization To gain additional insight, we
project word pairs represented in their vec-
tor concatenations onto 2-dimensional space us-
ing t-Distributed Stochastic Neighbor Embedding
(van der Maaten and Hinton (2008)). Figure 5
shows some of the interesting regions of the pro-
jection: some regions are relatively futile in hav-
ing creative phrases (e.g., regions involving simple
adjectives such as “good”, “bad”, regions corre-
sponding to legal terms), while some regions are rel-
atively more fruitful (e.g., regions involving abstract
adjectives such as “infinite”, “universal”, “funda-
mental”). There are also many other regions (e.g., in
the vicinity of “true”, “perfect” or “intelligent” in
Figure 5) where the separation between creative and
noncreative phrases are not as prominent. In those
regions, compositional aspects would play a bigger
role in determining creativity than memorizing fruit-
ful semantic subspaces.
</bodyText>
<page confidence="0.99588">
1254
</page>
<figureCaption confidence="0.999267">
Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph.
</figureCaption>
<figure confidence="0.997312095238095">
• • • -------- --------- •
• -------- --------- • -------- --------- •
• • -------- --------- • -------- -
• -------- -------- • -------- ---------
• -------- ---------
• -------- --------
• -------- ---------
• -------- ---------
• -------- ---------
• human architecture
• human spark
•
• good marathon
• good custodian
• bad profession
• human masterpiece
• legal slavery • legal corporation
•
• -------- ---------
• -------- -----
• -------- ---------
•
• -------- -------
• -------- ---------
• intelligent manipulation
• intelligent vocabulary
• perfect fire
• perfect land
• perfect disorder
• bad motivation
• true perversion
• true ambition
• true golfer
• infinite promise
• infinite leisure
• universal aspiration
• -------- ---------
• -------- --------
• universal anguish
•
•
• -------- ---------
• fundamental soul
• theoretical wisdom
• legal trading
• legal progress
• judicial verdict
• pure phenomenology
• technological refinement
• normal professor
• normal adulthood
• absolute barbarism
• logical market
• fundamental key
• -------- ---------
• honest coward
•
• invisible empire
• omnipotent realm
• finite realm
• human incompetence
•
• -------- ---------
</figure>
<sectionHeader confidence="0.998798" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999981379310345">
Among computational approaches that touch on lin-
guistic creativity, many focused on metaphor (e.g.,
Dunn (2013), Krishnakumaran and Zhu (2007),
Mashal et al. (2007), Rumbell et al. (2008), Ren-
toumi et al. (2012), Mashal et al. (2009)). Other lin-
guistic devices and phenomena related to creativity
include irony (e.g., Davidov et al. (2010), Gonz´alez-
Ib´a˜nez et al. (2011), Filatova (2012)), neologism
(e.g., Cartoni (2008)), humor (e.g., Mihalcea and
Strapparava (2005), Purandare and Litman (2006)),
and similes (e.g., Hao and Veale (2010)).
Veale (2011) proposed the new task of creative
text retrieval to harvest expressions that potentially
convey the same meaning as the query phrase in
a fresh or unusual way. Our work contributes to
the retrieval process of recognizing more creative
phrases. Ozbal and Strapparava (2012) explored
automatic creative naming of commercial products
and services, focusing on the generation of creative
phrases within a specific domain. Costello (2002)
investigated the cognitive process that guides peo-
ple’s choice of words when making up a novel noun-
noun compound. In contrast, we present a data-
driven investigation to quantifying creativity in lex-
ical composition. Memorability is loosely related to
linguistic creativity (Danescu-Niculescu-Mizil et al.
(2012)) as some of the creative quotes may be more
memorable, but not all creative phrases are memo-
rable and vice versa.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999965">
We presented the first study that focuses on learn-
ing and quantifying creativity in lexical composi-
tions, exploring statistical techniques motivated by
three different theories and hypotheses of creativ-
ity, ranging from divergent thinking, compositional
structure, creative semantic subspace, and the con-
nection to sentiment and connotation. Our experi-
mental results suggest the viability of learning cre-
ative language, and point to promising directions for
future research.
Acknowledgments This research was supported
in part by the Stony Brook University Office of the
Vice President for Research, and in part by gift from
Google. We thank anonymous reviewers for insight-
ful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998749" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983381">
T. Amabile. 1997. Motivating creativity in organiza-
tions: On doing what you love and loving what you
do. California Management Review, 40(1):39–58.
</reference>
<page confidence="0.904744">
1255
</page>
<reference confidence="0.998363018867924">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193, Stroudsburg, PA, USA.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1. Google Inc.
Ronald Carter and Michael McCarthy. 2004. Talking,
creating: interactional language, creativity, and con-
text. Applied Linguistics, 25(1):62–88.
Bruno Cartoni. 2008. Lexical resources for automatic
translation of constructed neologisms: the case study
of relational adjectives. In LREC.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Yi-Wei Chen and Chih-Jen Lin. 2005. Combining svms
with various feature selection strategies. In Taiwan
University. Springer-Verlag.
Noam Chomsky. 1965. Aspects of the Theory of Syntax,
volume 11. The MIT press.
Carol Chomsky. 1976. Creativity and innovation in child
language. Journal of Education, Boston.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41–71.
W.M. Cohen and D.A. Levinthal. 1990. Absorptive Ca-
pacity: A New Perspective on Learning and Innova-
tion. Administrative Science Quarterly, 35(1).
Fintan J. Costello. 2002. Investigating creative language:
People’s choice of words in the production of novel
noun-noun compounds. In Proceedings of the 24th
Annual Conference of the Cognitive Science Society.
Arthur Cropley. 2006. In praise of convergent thinking.
Creativity Research Journal, 18(3):391–404.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 892–901. Association for Computational Lin-
guistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107–116. Association for
Computational Linguistics.
Jonathan Dunn. 2013. What metaphor identifica-
tion systems can tell us about metaphor-in-language.
Meta4NLP 2013, page 1.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), Sofia,
Bulgaria, Angust. Association for Computational Lin-
guistics.
Elena Filatova. 2012. Irony and sarcasm: Corpus gen-
eration and analysis using crowdsourcing. In LREC,
pages 392–398.
Stefan L Frank. 2010. Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceedings of
the 2010 workshop on cognitive modeling and com-
putational linguistics, pages 81–89. Association for
Computational Linguistics.
Sigmund Freud. 1908. Creative writers and day-
dreaming. Standard edition, 9:143–153.
Susan R. Fussell and Mallie M. Moss. 1998. Figurative
language in descriptions of emotional states. In Social
and cognitive approaches to interpersonal communi-
cation.
Pablo Gerv´as. 2010. Engineering linguistic creativ-
ity: Bird flight and jet planes. In Proceedings of
the NAACL HLT 2010 Second Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 23–
30. Association for Computational Linguistics.
Katie Glaskin. 2011. Dreams, memory, and the ances-
tors: creativity, culture, and the science of sleep. Jour-
nal of the royal anthropological institute, 17(1):44–62.
Andrew Goatly. 1997. The language of metaphors.
Routledge.
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In ACL (Short Papers), pages 581–586.
Citeseer.
Emiliano Guevara. 2011. Computing semantic composi-
tionality in distributional semantics. In Proceedings of
the Ninth International Conference on Computational
Semantics (IWCS 2011), pages 135–144. Citeseer.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643–672.
Yanfen Hao and Tony Veale. 2010. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635–650.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Annual Meeting of the Association for Com-
putational Linguistics (ACL).
</reference>
<page confidence="0.79242">
1256
</page>
<reference confidence="0.999799245283019">
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13–20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
Live by. University of Chicago Press, Chicago.
N. Mashal, M. Faust, T Hendler, and M. Jung-Beeman.
2007. An fmri investigation of the neural correlates
underlying the processing of novel metaphoric expres-
sions. Brain and Language, pages 115 – 126.
N Mashal, M Faust, T Hendler, and M Jung-Beeman.
2009. An fmri study of processing novel metaphoric
sentences. Laterality, (1):30–54.
Janet Maybin and Joan Swann. 2007. Everyday creativ-
ity in language: Textuality, contextuality, and critique.
Applied Linguistics, 28(4):497–517.
Robert R McCrae. 1987. Creativity, divergent thinking,
and openness to experience. Journal of personality
and social psychology, 52(6):1258.
Susan M. McCurry and Steven C. Hayes. 1992. Clinical
and experimental perspectives on metaphorical talk.
Clinical Psychology Review, 12(7):763 – 785.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in automatic hu-
mor recognition. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 531–
538, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236–244.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 1-Volume 1, pages
430–439. Association for Computational Linguistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933–1950.
Edward Necka. 1999. Memory and creativity. Ency-
clopedia of creativity, ed. by MA Runco, SR Pritzker,
2:193–99.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 703–711, Jeju Island, Korea, July.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.
J. B. Pollack. 1990. Recursive distributed representation.
Artificial Intelligence, 46:77–105.
Amruta Purandare and Diane Litman. 2006. Humor:
Prosody analysis and automatic recognition for f* r* i*
e* n* d* s*. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 208–215. Association for Computational Lin-
guistics.
Vassiliki Rentoumi, George A. Vouros, Vangelis
Karkaletsis, and Amalia Moser. 2012. Investigating
metaphorical language in sentiment analysis: A sense-
to-sentiment perspective. ACM Trans. Speech Lang.
Process., 9(3):6:1–6:31, November.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 907–916. Asso-
ciation for Computational Linguistics.
Tim Rumbell, John Barnden, Mark Lee, and Alan
Wallington. 2008. Affect in metaphor: Developments
with wordnet.
Bernhard Sch¨olkopf and Alexander J Smola. 2002.
Learning with kernels. The MIT Press.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
methods for pattern analysis. Cambridge university
press.
Ekaterina Shutova. 2010. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ’10,
pages 688–697, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254–
263. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sen-
timent distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 151–161. Association for Computational
Linguistics.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visualiz-
ing high-dimensional data using t-sne.
Tony Veale, Yanfen Hao, and Guofu Li. 2008. Multilin-
gual harvesting of cross-cultural stereotypes. In ACL,
pages 523–531.
</reference>
<page confidence="0.803777">
1257
</page>
<reference confidence="0.999720161290322">
Tony Veale. 2011. Creative language retrieval: A ro-
bust hybrid of information retrieval and linguistic cre-
ativity. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 278–287, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Thomas Voegtlin and Peter F. Dominey. 2005. Linear
recursive distributed representations. Neural Netw.,
18(7):878–895, September.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the Sec-
ond AAAI Symposium on Quantum Interaction.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34–35. Association for Computational
Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172–182. As-
sociation for Computational Linguistics.
Xiaojin Zhu, Zhiting Xu, and Tushar Khot. 2009. How
creative is your writing? a linguistic creativity mea-
sure from computer science and cognitive psychology
perspectives. In Proceedings of the Workshop on Com-
putational Approaches to Linguistic Creativity, pages
87–93. Association for Computational Linguistics.
</reference>
<page confidence="0.992456">
1258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788769">
<title confidence="0.999959">Understanding and Quantifying Creativity in Lexical Composition</title>
<author confidence="0.999573">Polina Kuznetsova Jianfu Chen Yejin Choi</author>
<affiliation confidence="0.998289">Department of Computer</affiliation>
<author confidence="0.940914">Stony Brook Stony Brook</author>
<author confidence="0.940914">NY</author>
<abstract confidence="0.992811944444445">Why do certain combinations of words such peace” to the to our minds as interesting expressions with a sense of creativity, while phrases such as or base” as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Amabile</author>
</authors>
<title>Motivating creativity in organizations: On doing what you love and loving what you do.</title>
<date>1997</date>
<journal>California Management Review,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="9869" citStr="Amabile (1997)" startWordPosition="1492" endWordPosition="1493">ntly found. For instance, phrases such as “guns and roses” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e.g., Freud (1908), Necka (1999), Glaskin (2011), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources.” Figure 5 presents visualized supports for creative semantic subspace,4 where we observe that phrases in the neighborhood of legal terms are generally not creative, while the semantic neighborhood of 2With additional context this example may turn into a creative one, but for simplicity we focus on phrases with two content words considered out of context. 3Investigation on recursive composition of more than two content words and the influence of syntactic packaging is left as future research. 4See §</context>
</contexts>
<marker>Amabile, 1997</marker>
<rawString>T. Amabile. 1997. Motivating creativity in organizations: On doing what you love and loving what you do. California Management Review, 40(1):39–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7382" citStr="Baroni and Zamparelli (2010)" startWordPosition="1104" endWordPosition="1107">composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, w</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<publisher>Google Inc.</publisher>
<contexts>
<context position="14939" citStr="Brants and Franz (2006)" startWordPosition="2353" endWordPosition="2356">ity of each word. As expected, QUOTES dataset has higher entropy than GLOSSES in Table 1. 3.1 Creative Word Pairs We extract word pairs corresponding to the following syntactic patterns: [NN NN], [JJ NN], [NN JJ] and [JJ JJ]. Not all pairs from QUOTESraw are creative, and likewise, not all pairs from GLOSSESraw are uncreative. Therefore, we perform manual annotations to a subset of the collected pairs as follows. We obtain a small subset of pairs by applying stratified sampling based on bigram frequency buckets: first we sort word pairs by their bigram frequencies obtained from Web 1T corpus (Brants and Franz (2006)), group them into consecutive fre1249 quency buckets each of which containing 400 word pairs, then sample 40 word pairs from each bucket. We label word pairs using Amazon Mechnical Turk (AMT) (e.g., Snow et al. (2008)). We ask three turkers to score each pair in 1-5 scale, where 1 is the least creative and 5 is the most creative. We then obtain the final creativity scale score by averaging the scores over 3 users. In addition, we ask turkers a series of yes/no questions to help turkers to determine whether the given pair is creative or not.8 We determine the final label of a word pair based o</context>
<context position="16454" citStr="Brants and Franz (2006)" startWordPosition="2620" endWordPosition="2623">iscard the rest from the final dataset. This filtering process is akin to the removal of neural sentiment in the early work of sentiment analysis (e.g., Pang et al. (2002)).9 Table 2 shows the statistics of the resulting dataset. Creative Pairs and their Frequencies: To gain insights on the stratified sample of word pairs, we plot the label (E {creative, common}) distribution of word pairs as a function of simple statistics, such as a range (bucket) of bigram frequencies or PMI values of the given pair of words. Both bigram frequencies and PMI scores are computed based on Google Web 1T corpus Brants and Franz (2006). Figure 1 shows the results for word frequencies. As expected, word pairs with high frequencies are much more likely to be common, while word pairs with low frequencies can be either of the two. Also as expected, pairs extracted from QUOTES are relatively more likely to be creative than those from GLOSSES. In any case, it is clear that not all rare pairs are creative. Creative Pairs and their PMI Scores: Similarly as above, Figure 2 plots the relation between the distribution of labels of word pairs and their corresponding PMI. As expected, pairs with high PMI are more likely to be common, th</context>
<context position="21392" citStr="Brants and Franz (2006)" startWordPosition="3466" endWordPosition="3469">n: RH(w1,w2) = |H(w1) − H(w1w2) |(1) H(w1) + H(w1w2) As expected (Figure 3 – b and Table 4), this relative quantity captures creativity better than the absolute measure H(w1w2) computed above. The idea behind this measure has a connection to uncertainty reduction in psycholinguistic literature (e.g., Frank (2010), Hale (2003), Hale (2006)). KL divergence To capture unusual combinations of words, we compare the difference between the distributional contexts of w1 and w1w2 so that P(wi|w1, w2) P(wi|w1, w2) log P(wi|w1) (2) Figure (3 – c) shows that KL(w1w2, w1)12 is among Lconn(w1,w2) 1T corpus Brants and Franz (2006). 12We also compute KL(w1, w2) in a similar manner as KL(w1w2, w1) Milton as best busy clear common cool deep early end expensive final glad hard hot ingot last likely many next own pleased professional rarely role seriously slow special success to valuable welcome �KL(w1w2, w1) = w;EV 1251 the effective measures in capturing creative pairs. Mutual Information Finally, we consider mutual information (Figure 3 – d): �MI(w1,w2) = P(wi|w1, w2) x w;EV P(wi|w1, w2) log (3) P(wi |w1) · P(wi |w2) Correlation coefficients Pearson coefficients for all measures are shown in Table 4. Interestingly, infor</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram corpus version 1.1. Google Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Carter</author>
<author>Michael McCarthy</author>
</authors>
<title>Talking, creating: interactional language, creativity, and context.</title>
<date>2004</date>
<journal>Applied Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="4192" citStr="Carter and McCarthy (2004)" startWordPosition="606" endWordPosition="609">ics miliar biomedical terms, e.g., “cardiac glycosides”, are only informative without appreciable creativity. Similarly, less frequent combinations of words, e.g., “rotten detergent” or “quiet teenager”, though describing situations that are certainly uncommon, do not bring about the sense of creativity. Finally, some unique combinations of words can be just nonsensical , e.g., “elegant glycosides”. Different studies assumed different definitions of linguistic creativity depending on their context and end goals (e.g., Chomsky (1976), Zhu et al. (2009), Gerv´as (2010), Maybin and Swann (2007), Carter and McCarthy (2004)). In this paper, as an operational definition, we consider a phrase creative if it is (a) unconventional or uncommon, and (b) expressive in an interesting, imaginative, or inspirational way. A system that can recognize creative expressions could be of practical use for many aspiring writers who are often in need of inspirational help in searching for the optimal choice of words. Such a system can also be integrated into automatic assessment of writing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially usef</context>
</contexts>
<marker>Carter, McCarthy, 2004</marker>
<rawString>Ronald Carter and Michael McCarthy. 2004. Talking, creating: interactional language, creativity, and context. Applied Linguistics, 25(1):62–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Cartoni</author>
</authors>
<title>Lexical resources for automatic translation of constructed neologisms: the case study of relational adjectives.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="35184" citStr="Cartoni (2008)" startWordPosition="5665" endWordPosition="5666">rism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive proce</context>
</contexts>
<marker>Cartoni, 2008</marker>
<rawString>Bruno Cartoni. 2008. Lexical resources for automatic translation of constructed neologisms: the case study of relational adjectives. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="25360" citStr="Chang and Lin, 2011" startWordPosition="4102" endWordPosition="4105">y of a word pair. Rather, it is the difference between the two words that gives rise the sense of creativity. 4.3 Learning to Recognize Creativity Now we put together all measures explored in §4.1 and 4.2 in a supervised-learning framework. As expected, rather than either one alone, the combination of various measures leads to the best performance: F12 = [RH(w1, w2); KL(w1, w2); H(w1w2); Ldiff ����(w1, w2); PMI(wl, w2); H(w2); KL(w1w2, w1); KL(w2, w1); di f f Lsub, (w1, w2); MI(w1, w2); Freq(w1w2); H(w1)] Table 5 shows the performance of the above feature vector with 12 features using libsvm (Chang and Lin, 2011). We use C-Support Vector Classification (C-SVC). Performance is reported in accuracy using 5-fold cross validation.15 15Among these 12 features, the feature selection algorithm 1252 5 Learning Creative Pairs with Distributional Semantic Vectors The measures explored in §4 were largely uninformed of distributional semantic dimensions of each word. However, in order to pursue the conceptual aspect of creativity illustrated in §2.2, that is, the notion of semantic subspaces that are inherently futile or fruitful for creativity, we need to incorporate semantic representations more directly. We th</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Wei Chen</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Combining svms with various feature selection strategies.</title>
<date>2005</date>
<publisher>Springer-Verlag.</publisher>
<institution>In Taiwan University.</institution>
<contexts>
<context position="27612" citStr="Chen and Lin (2005)" startWordPosition="4451" endWordPosition="4454">not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E Rn. Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional operations. See Table 5 for more details and experimental results. 5.2 Learning Nonl</context>
</contexts>
<marker>Chen, Lin, 2005</marker>
<rawString>Yi-Wei Chen and Chih-Jen Lin. 2005. Combining svms with various feature selection strategies. In Taiwan University. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax,</title>
<date>1965</date>
<volume>11</volume>
<publisher>The MIT press.</publisher>
<contexts>
<context position="1240" citStr="Chomsky, 1965" startWordPosition="178" endWordPosition="179"> original, interesting, and at times even artistic. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity. 1 Introduction An essential property of natural language is the generative capacity that makes it possible for people to express indefinitely many thoughts through indefinitely many different ways of composing phrases and sentences (Chomsky, 1965). The possibility of novel, creative expressions never seems to exhaust. Various types of writers, such as novelists, journalists, movie script writers, and creatives in advertising, continue creating novel phrases and expressions that are original while befitting in expressing the desired meaning in the given situation. Consider unique phrases such as “geological split personality”, or “intoxicating Shangri-La of shoes”,1 that 1Examples from New York Times articles in 2013. continue flowing into the online text drawing attention from readers. Writers put significant effort in choosing the per</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the Theory of Syntax, volume 11. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Chomsky</author>
</authors>
<title>Creativity and innovation in child language.</title>
<date>1976</date>
<journal>Journal of Education,</journal>
<location>Boston.</location>
<contexts>
<context position="4104" citStr="Chomsky (1976)" startWordPosition="594" endWordPosition="595">gton, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics miliar biomedical terms, e.g., “cardiac glycosides”, are only informative without appreciable creativity. Similarly, less frequent combinations of words, e.g., “rotten detergent” or “quiet teenager”, though describing situations that are certainly uncommon, do not bring about the sense of creativity. Finally, some unique combinations of words can be just nonsensical , e.g., “elegant glycosides”. Different studies assumed different definitions of linguistic creativity depending on their context and end goals (e.g., Chomsky (1976), Zhu et al. (2009), Gerv´as (2010), Maybin and Swann (2007), Carter and McCarthy (2004)). In this paper, as an operational definition, we consider a phrase creative if it is (a) unconventional or uncommon, and (b) expressive in an interesting, imaginative, or inspirational way. A system that can recognize creative expressions could be of practical use for many aspiring writers who are often in need of inspirational help in searching for the optimal choice of words. Such a system can also be integrated into automatic assessment of writing styles and quality, and utilized to automatically const</context>
</contexts>
<marker>Chomsky, 1976</marker>
<rawString>Carol Chomsky. 1976. Creativity and innovation in child language. Journal of Education, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="7413" citStr="Clarke (2012)" startWordPosition="1110" endWordPosition="1111">tional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore (1) compositiona</context>
<context position="27503" citStr="Clarke (2012)" startWordPosition="4431" endWordPosition="4432"> can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E Rn. Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture mo</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Cohen</author>
<author>D A Levinthal</author>
</authors>
<title>Absorptive Capacity: A New Perspective on Learning and Innovation.</title>
<date>1990</date>
<journal>Administrative Science Quarterly,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="9853" citStr="Cohen and Levinthal (1990)" startWordPosition="1488" endWordPosition="1491"> expressions are more frequently found. For instance, phrases such as “guns and roses” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e.g., Freud (1908), Necka (1999), Glaskin (2011), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources.” Figure 5 presents visualized supports for creative semantic subspace,4 where we observe that phrases in the neighborhood of legal terms are generally not creative, while the semantic neighborhood of 2With additional context this example may turn into a creative one, but for simplicity we focus on phrases with two content words considered out of context. 3Investigation on recursive composition of more than two content words and the influence of syntactic packaging is left as future </context>
</contexts>
<marker>Cohen, Levinthal, 1990</marker>
<rawString>W.M. Cohen and D.A. Levinthal. 1990. Absorptive Capacity: A New Perspective on Learning and Innovation. Administrative Science Quarterly, 35(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fintan J Costello</author>
</authors>
<title>Investigating creative language: People’s choice of words in the production of novel noun-noun compounds.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="35751" citStr="Costello (2002)" startWordPosition="5749" endWordPosition="5750">ilatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. Memorability is loosely related to linguistic creativity (Danescu-Niculescu-Mizil et al. (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. 8 Conclusion We presented the first study that focuses on learning and quantifying creativity in lexical compositions, exploring statistical techniques motivated by three dif</context>
</contexts>
<marker>Costello, 2002</marker>
<rawString>Fintan J. Costello. 2002. Investigating creative language: People’s choice of words in the production of novel noun-noun compounds. In Proceedings of the 24th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Cropley</author>
</authors>
<title>In praise of convergent thinking.</title>
<date>2006</date>
<journal>Creativity Research Journal,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="6683" citStr="Cropley (2006)" startWordPosition="994" endWordPosition="995">scientists, have proposed theories that attempt to explain the mechanism of creative process. In this section, we draw connections from some of these theories developed for general human creativity to the problem of quantitatively interpreting linguistic creativity in lexical composition. 2.1 Divergent Thinking and Composition Divergent thinking (e.g., McCrae (1987)), which seeks to generate multiple unstereotypical solutions to an open ended problem has been considered as the key element in creative process, which contrasts with convergent thinking that find a single, correct solution (e.g., Cropley (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher e</context>
</contexts>
<marker>Cropley, 2006</marker>
<rawString>Arthur Cropley. 2006. In praise of convergent thinking. Creativity Research Journal, 18(3):391–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Justin Cheng</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>You had me at hello: How phrasing affects memorability.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>892--901</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36062" citStr="Danescu-Niculescu-Mizil et al. (2012)" startWordPosition="5792" endWordPosition="5795">aning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. Memorability is loosely related to linguistic creativity (Danescu-Niculescu-Mizil et al. (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. 8 Conclusion We presented the first study that focuses on learning and quantifying creativity in lexical compositions, exploring statistical techniques motivated by three different theories and hypotheses of creativity, ranging from divergent thinking, compositional structure, creative semantic subspace, and the connection to sentiment and connotation. Our experimental results suggest the viability of learning creative language, and point to promising directions for future researc</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Cheng, Kleinberg, Lee, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, and Lillian Lee. 2012. You had me at hello: How phrasing affects memorability. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 892–901. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>107--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35100" citStr="Davidov et al. (2010)" startWordPosition="5652" endWordPosition="5655">omenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creativ</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107–116. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Dunn</author>
</authors>
<title>What metaphor identification systems can tell us about metaphor-in-language.</title>
<date>2013</date>
<booktitle>Meta4NLP 2013,</booktitle>
<pages>1</pages>
<contexts>
<context position="34872" citStr="Dunn (2013)" startWordPosition="5617" endWordPosition="5618">isure • universal aspiration • -------- --------- • -------- -------- • universal anguish • • • -------- --------- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual</context>
</contexts>
<marker>Dunn, 2013</marker>
<rawString>Jonathan Dunn. 2013. What metaphor identification systems can tell us about metaphor-in-language. Meta4NLP 2013, page 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Sak Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>2</volume>
<institution>Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.</institution>
<contexts>
<context position="23124" citStr="Feng et al. (2013)" startWordPosition="3736" endWordPosition="3739"> that are evident in graphs shown in Figure 3. Second, these measures only capture the surprisal aspect of creativity, missing the other important qualities: interestingness or imaginativeness. 4.2 Sentiment and Connotation Next we investigate the connection between creativity and sentiment, as illustrated in §2.3. We consider both sentiment (more explicit) and connotation (more implicit) words,13 and consider them with or without distinguishing the polarity (i.e., positive, negative). To determine sentiment and connotation, we use lexicons provided by OpinionFinder (Wilson et al. (2005)) and Feng et al. (2013) respectively. We denote polarity of a word wi as L(wi).14 When wi has a negative polarity L(wi) is assigned a value of -1, and when wi is positive L(wi) is equal to 1. We assume that a word is neutral when it is not in the lexicon, assigning 0 to L(wi). For a word pair w1w2 we compute absolute difference Ldiff(w1, w2) between polarities of tokens in a word pair in order to catch examples such as “inglorious success”. 13E.g., expressions such as “blue sky” or “white sand” are not sentiment-laden, but do have positive connotation. 14We denote polarity from OpinionFinder as Lsubj and connotation</context>
</contexts>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
</authors>
<title>Irony and sarcasm: Corpus generation and analysis using crowdsourcing.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>392--398</pages>
<contexts>
<context position="35150" citStr="Filatova (2012)" startWordPosition="5661" endWordPosition="5662">• normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002</context>
</contexts>
<marker>Filatova, 2012</marker>
<rawString>Elena Filatova. 2012. Irony and sarcasm: Corpus generation and analysis using crowdsourcing. In LREC, pages 392–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Uncertainty reduction as a measure of cognitive processing effort.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 workshop on cognitive modeling and computational linguistics,</booktitle>
<pages>81--89</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21083" citStr="Frank (2010)" startWordPosition="3418" endWordPosition="3419">e as H(w1), H(w2), H(w1w2) respectively, latter is shown in Figure 3 – a.11 11As before, language models are drawn from Google Web Relative Entropy Transformation In order to focus more directly on the relative change of entropy as a result of composition, we compute Relative Entropy Transformation: RH(w1,w2) = |H(w1) − H(w1w2) |(1) H(w1) + H(w1w2) As expected (Figure 3 – b and Table 4), this relative quantity captures creativity better than the absolute measure H(w1w2) computed above. The idea behind this measure has a connection to uncertainty reduction in psycholinguistic literature (e.g., Frank (2010), Hale (2003), Hale (2006)). KL divergence To capture unusual combinations of words, we compare the difference between the distributional contexts of w1 and w1w2 so that P(wi|w1, w2) P(wi|w1, w2) log P(wi|w1) (2) Figure (3 – c) shows that KL(w1w2, w1)12 is among Lconn(w1,w2) 1T corpus Brants and Franz (2006). 12We also compute KL(w1, w2) in a similar manner as KL(w1w2, w1) Milton as best busy clear common cool deep early end expensive final glad hard hot ingot last likely many next own pleased professional rarely role seriously slow special success to valuable welcome �KL(w1w2, w1) = w;EV 1251</context>
</contexts>
<marker>Frank, 2010</marker>
<rawString>Stefan L Frank. 2010. Uncertainty reduction as a measure of cognitive processing effort. In Proceedings of the 2010 workshop on cognitive modeling and computational linguistics, pages 81–89. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sigmund Freud</author>
</authors>
<title>Creative writers and daydreaming.</title>
<date>1908</date>
<pages>9--143</pages>
<note>Standard edition,</note>
<contexts>
<context position="9795" citStr="Freud (1908)" startWordPosition="1482" endWordPosition="1483">ful subsets of semantic space where creative expressions are more frequently found. For instance, phrases such as “guns and roses” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e.g., Freud (1908), Necka (1999), Glaskin (2011), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources.” Figure 5 presents visualized supports for creative semantic subspace,4 where we observe that phrases in the neighborhood of legal terms are generally not creative, while the semantic neighborhood of 2With additional context this example may turn into a creative one, but for simplicity we focus on phrases with two content words considered out of context. 3Investigation on recursive composition of more than two content words a</context>
</contexts>
<marker>Freud, 1908</marker>
<rawString>Sigmund Freud. 1908. Creative writers and daydreaming. Standard edition, 9:143–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan R Fussell</author>
<author>Mallie M Moss</author>
</authors>
<title>Figurative language in descriptions of emotional states. In Social and cognitive approaches to interpersonal communication.</title>
<date>1998</date>
<contexts>
<context position="11605" citStr="Fussell and Moss (1998)" startWordPosition="1764" endWordPosition="1767">g imaginative and interesting, per our operational definition of creativity given in §1) word pairs, e.g., invisible empire”. In our empirical investigation, this notion of semantically fruitful and futile semantic subspaces are captured using distributional semantic space models under supervised learning framework (§5). 2.3 Affective Language Another angle we probe is the connection between creative expressions and the use of affective language. This idea is supported in part by previous research that explored the connection between figurative languages such as metaphors and sentiment (e.g., Fussell and Moss (1998), Rumbell et al. (2008), Rentoumi et al. (2012)). The focus of previous work was either on interpretation of the sentiment in metaphors, or the use of metaphors in the description of affect. In contrast, we aim to quantify the correlation between creative expressions (beyond metaphors) and the use of sentimentladen words in a more systematic way. This exploration has a connection to the creative semantic subspace discussed earlier (§2.2), but pays a more direct attention to the aspect of sentiment and connotation. 3 Creative Language Dataset We start our investigation by considering two types </context>
</contexts>
<marker>Fussell, Moss, 1998</marker>
<rawString>Susan R. Fussell and Mallie M. Moss. 1998. Figurative language in descriptions of emotional states. In Social and cognitive approaches to interpersonal communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gerv´as</author>
</authors>
<title>Engineering linguistic creativity: Bird flight and jet planes.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>pages</pages>
<marker>Gerv´as, 2010</marker>
<rawString>Pablo Gerv´as. 2010. Engineering linguistic creativity: Bird flight and jet planes. In Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 23– 30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katie Glaskin</author>
</authors>
<title>Dreams, memory, and the ancestors: creativity, culture, and the science of sleep.</title>
<date>2011</date>
<journal>Journal of the royal anthropological institute,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="9825" citStr="Glaskin (2011)" startWordPosition="1486" endWordPosition="1487">e where creative expressions are more frequently found. For instance, phrases such as “guns and roses” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e.g., Freud (1908), Necka (1999), Glaskin (2011), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources.” Figure 5 presents visualized supports for creative semantic subspace,4 where we observe that phrases in the neighborhood of legal terms are generally not creative, while the semantic neighborhood of 2With additional context this example may turn into a creative one, but for simplicity we focus on phrases with two content words considered out of context. 3Investigation on recursive composition of more than two content words and the influence of syntactic </context>
</contexts>
<marker>Glaskin, 2011</marker>
<rawString>Katie Glaskin. 2011. Dreams, memory, and the ancestors: creativity, culture, and the science of sleep. Journal of the royal anthropological institute, 17(1):44–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goatly</author>
</authors>
<title>The language of metaphors.</title>
<date>1997</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="2371" citStr="Goatly (1997)" startWordPosition="344" endWordPosition="345">drawing attention from readers. Writers put significant effort in choosing the perfect words in completing their compositions, as a well-chosen combination of words is impactful in readers’ minds for rendering the precise intended meaning, as well as stimulating an increased level of cognitive responses and attention. Metaphors in particular, one of the quintessential forms of linguistic creativity, have been discussed extensively by studies across multiple disciplines, e.g., Cognitive Science, Psychology, Linguistics, and Literature (e.g., Lakoff and Johnson (1980), McCurry and Hayes (1992), Goatly (1997)). Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e.g., Mashal et al. (2007), Mashal et al. (2009)). Several linguistic elements, e.g., syntax, semantics, and pragmatics, are likely to be working together in order to lead to the perception of creativity. However, their underlying mechanisms by a</context>
</contexts>
<marker>Goatly, 1997</marker>
<rawString>Andrew Goatly. 1997. The language of metaphors. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: A closer look.</title>
<date>2011</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>581--586</pages>
<publisher>Citeseer.</publisher>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: A closer look. In ACL (Short Papers), pages 581–586. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>Computing semantic compositionality in distributional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics (IWCS</booktitle>
<pages>135--144</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7398" citStr="Guevara (2011)" startWordPosition="1108" endWordPosition="1109">nusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore (</context>
<context position="27488" citStr="Guevara (2011)" startWordPosition="4429" endWordPosition="4430">seline, since it can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E Rn. Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hop</context>
</contexts>
<marker>Guevara, 2011</marker>
<rawString>Emiliano Guevara. 2011. Computing semantic compositionality in distributional semantics. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011), pages 135–144. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>The information conveyed by words in sentences.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="21096" citStr="Hale (2003)" startWordPosition="3420" endWordPosition="3421">w2), H(w1w2) respectively, latter is shown in Figure 3 – a.11 11As before, language models are drawn from Google Web Relative Entropy Transformation In order to focus more directly on the relative change of entropy as a result of composition, we compute Relative Entropy Transformation: RH(w1,w2) = |H(w1) − H(w1w2) |(1) H(w1) + H(w1w2) As expected (Figure 3 – b and Table 4), this relative quantity captures creativity better than the absolute measure H(w1w2) computed above. The idea behind this measure has a connection to uncertainty reduction in psycholinguistic literature (e.g., Frank (2010), Hale (2003), Hale (2006)). KL divergence To capture unusual combinations of words, we compare the difference between the distributional contexts of w1 and w1w2 so that P(wi|w1, w2) P(wi|w1, w2) log P(wi|w1) (2) Figure (3 – c) shows that KL(w1w2, w1)12 is among Lconn(w1,w2) 1T corpus Brants and Franz (2006). 12We also compute KL(w1, w2) in a similar manner as KL(w1w2, w1) Milton as best busy clear common cool deep early end expensive final glad hard hot ingot last likely many next own pleased professional rarely role seriously slow special success to valuable welcome �KL(w1w2, w1) = w;EV 1251 the effectiv</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John Hale. 2003. The information conveyed by words in sentences. Journal of Psycholinguistic Research, 32(2):101–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="21109" citStr="Hale (2006)" startWordPosition="3422" endWordPosition="3423">respectively, latter is shown in Figure 3 – a.11 11As before, language models are drawn from Google Web Relative Entropy Transformation In order to focus more directly on the relative change of entropy as a result of composition, we compute Relative Entropy Transformation: RH(w1,w2) = |H(w1) − H(w1w2) |(1) H(w1) + H(w1w2) As expected (Figure 3 – b and Table 4), this relative quantity captures creativity better than the absolute measure H(w1w2) computed above. The idea behind this measure has a connection to uncertainty reduction in psycholinguistic literature (e.g., Frank (2010), Hale (2003), Hale (2006)). KL divergence To capture unusual combinations of words, we compare the difference between the distributional contexts of w1 and w1w2 so that P(wi|w1, w2) P(wi|w1, w2) log P(wi|w1) (2) Figure (3 – c) shows that KL(w1w2, w1)12 is among Lconn(w1,w2) 1T corpus Brants and Franz (2006). 12We also compute KL(w1, w2) in a similar manner as KL(w1w2, w1) Milton as best busy clear common cool deep early end expensive final glad hard hot ingot last likely many next own pleased professional rarely role seriously slow special success to valuable welcome �KL(w1w2, w1) = w;EV 1251 the effective measures in</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>John Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30(4):643–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanfen Hao</author>
<author>Tony Veale</author>
</authors>
<title>An ironic fist in a velvet glove: Creative mis-representation in the construction of ironic similes.</title>
<date>2010</date>
<journal>Minds and Machines,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="5286" citStr="Hao and Veale (2010)" startWordPosition="783" endWordPosition="786">y, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects </context>
<context position="35302" citStr="Hao and Veale (2010)" startWordPosition="5680" endWordPosition="5683">ealm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven</context>
</contexts>
<marker>Hao, Veale, 2010</marker>
<rawString>Yanfen Hao and Tony Veale. 2010. An ironic fist in a velvet glove: Creative mis-representation in the construction of ironic similes. Minds and Machines, 20(4):635–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="27215" citStr="Huang et al. (2012)" startWordPosition="4388" endWordPosition="4391">hese approaches, the notion of creative semantic subspace is integrated indirectly, as the feature representation always incorporates the resulting (composed) vector representations. Baseline &amp; Configuration We consider the concatenation of two word vectors [191; 192] as the baseline, since it can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E </context>
<context position="31991" citStr="Huang et al. (2012)" startWordPosition="5161" endWordPosition="5164">IVE LABELING (8/42) “independent + “wonderful - religion” religion” WORD SENSE DISAMBIGUATION PROBLEMS (2/42) “fiscal cliff” “winding lake” + “opera window” + “work-shop floor” - Table 6: Error analysis: y* denotes the true label. For each incorrectly predicted word pair (left column), we show an example of semantically close word pairs (right column) with the opposite true label that might have confused learning. and a softmax layer to predict the probability of the word pair being creative and not creative. We initialize the word vectors using the pre-learned vector space representations in Huang et al. (2012). 5.4 Experimental Results Table 5 shows the performance comparison of different features sets and algorithms. In all cases, parameters are tuned from the training portion of the data. We see that simple vector composition Error analysis We manually inspected a randomly chosen 42 error cases, and characterize the potential causes of those errors. Examples of three types of errors are shown in Table 6. For each incorrectly predicted word pair, we also show a semantically close word pair with the opposite true label that might have confused the learning algorithm. Visualization To gain additiona</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saisuresh Krishnakumaran</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Hunting elusive metaphors using lexical resources.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Computational approaches to Figurative Language,</booktitle>
<pages>13--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="34903" citStr="Krishnakumaran and Zhu (2007)" startWordPosition="5619" endWordPosition="5622">rsal aspiration • -------- --------- • -------- -------- • universal anguish • • • -------- --------- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to t</context>
</contexts>
<marker>Krishnakumaran, Zhu, 2007</marker>
<rawString>Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computational approaches to Figurative Language, pages 13–20. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors we Live by.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="2330" citStr="Lakoff and Johnson (1980)" startWordPosition="336" endWordPosition="339">icles in 2013. continue flowing into the online text drawing attention from readers. Writers put significant effort in choosing the perfect words in completing their compositions, as a well-chosen combination of words is impactful in readers’ minds for rendering the precise intended meaning, as well as stimulating an increased level of cognitive responses and attention. Metaphors in particular, one of the quintessential forms of linguistic creativity, have been discussed extensively by studies across multiple disciplines, e.g., Cognitive Science, Psychology, Linguistics, and Literature (e.g., Lakoff and Johnson (1980), McCurry and Hayes (1992), Goatly (1997)). Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e.g., Mashal et al. (2007), Mashal et al. (2009)). Several linguistic elements, e.g., syntax, semantics, and pragmatics, are likely to be working together in order to lead to the perception of creativity. </context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors we Live by. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Mashal</author>
<author>M Faust</author>
<author>T Hendler</author>
<author>M Jung-Beeman</author>
</authors>
<title>An fmri investigation of the neural correlates underlying the processing of novel metaphoric expressions. Brain and Language,</title>
<date>2007</date>
<pages>115--126</pages>
<contexts>
<context position="2751" citStr="Mashal et al. (2007)" startWordPosition="396" endWordPosition="399">rms of linguistic creativity, have been discussed extensively by studies across multiple disciplines, e.g., Cognitive Science, Psychology, Linguistics, and Literature (e.g., Lakoff and Johnson (1980), McCurry and Hayes (1992), Goatly (1997)). Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e.g., Mashal et al. (2007), Mashal et al. (2009)). Several linguistic elements, e.g., syntax, semantics, and pragmatics, are likely to be working together in order to lead to the perception of creativity. However, their underlying mechanisms by and large are yet to be investigated. In this paper, as a small step toward quantitative understanding of linguistic creativity, we present a focused study on lexical composition two content words. Being creative, by definition, implies qualities such as being unique, novel, unfamiliar or unconventional. But not every unfamiliar combination of words would appeal as creative. For</context>
<context position="34925" citStr="Mashal et al. (2007)" startWordPosition="5623" endWordPosition="5626">----- • -------- -------- • universal anguish • • • -------- --------- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process o</context>
</contexts>
<marker>Mashal, Faust, Hendler, Jung-Beeman, 2007</marker>
<rawString>N. Mashal, M. Faust, T Hendler, and M. Jung-Beeman. 2007. An fmri investigation of the neural correlates underlying the processing of novel metaphoric expressions. Brain and Language, pages 115 – 126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Mashal</author>
<author>M Faust</author>
<author>T Hendler</author>
<author>M Jung-Beeman</author>
</authors>
<title>An fmri study of processing novel metaphoric sentences. Laterality,</title>
<date>2009</date>
<contexts>
<context position="2773" citStr="Mashal et al. (2009)" startWordPosition="400" endWordPosition="403">tivity, have been discussed extensively by studies across multiple disciplines, e.g., Cognitive Science, Psychology, Linguistics, and Literature (e.g., Lakoff and Johnson (1980), McCurry and Hayes (1992), Goatly (1997)). Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e.g., Mashal et al. (2007), Mashal et al. (2009)). Several linguistic elements, e.g., syntax, semantics, and pragmatics, are likely to be working together in order to lead to the perception of creativity. However, their underlying mechanisms by and large are yet to be investigated. In this paper, as a small step toward quantitative understanding of linguistic creativity, we present a focused study on lexical composition two content words. Being creative, by definition, implies qualities such as being unique, novel, unfamiliar or unconventional. But not every unfamiliar combination of words would appeal as creative. For example, unfa1246 Pro</context>
<context position="34994" citStr="Mashal et al. (2009)" startWordPosition="5636" endWordPosition="5639">- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) exp</context>
</contexts>
<marker>Mashal, Faust, Hendler, Jung-Beeman, 2009</marker>
<rawString>N Mashal, M Faust, T Hendler, and M Jung-Beeman. 2009. An fmri study of processing novel metaphoric sentences. Laterality, (1):30–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Maybin</author>
<author>Joan Swann</author>
</authors>
<title>Everyday creativity in language: Textuality, contextuality, and critique.</title>
<date>2007</date>
<journal>Applied Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="4164" citStr="Maybin and Swann (2007)" startWordPosition="602" endWordPosition="605">or Computational Linguistics miliar biomedical terms, e.g., “cardiac glycosides”, are only informative without appreciable creativity. Similarly, less frequent combinations of words, e.g., “rotten detergent” or “quiet teenager”, though describing situations that are certainly uncommon, do not bring about the sense of creativity. Finally, some unique combinations of words can be just nonsensical , e.g., “elegant glycosides”. Different studies assumed different definitions of linguistic creativity depending on their context and end goals (e.g., Chomsky (1976), Zhu et al. (2009), Gerv´as (2010), Maybin and Swann (2007), Carter and McCarthy (2004)). In this paper, as an operational definition, we consider a phrase creative if it is (a) unconventional or uncommon, and (b) expressive in an interesting, imaginative, or inspirational way. A system that can recognize creative expressions could be of practical use for many aspiring writers who are often in need of inspirational help in searching for the optimal choice of words. Such a system can also be integrated into automatic assessment of writing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, w</context>
</contexts>
<marker>Maybin, Swann, 2007</marker>
<rawString>Janet Maybin and Joan Swann. 2007. Everyday creativity in language: Textuality, contextuality, and critique. Applied Linguistics, 28(4):497–517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert R McCrae</author>
</authors>
<title>Creativity, divergent thinking, and openness to experience.</title>
<date>1987</date>
<journal>Journal of personality and social psychology,</journal>
<volume>52</volume>
<issue>6</issue>
<contexts>
<context position="6437" citStr="McCrae (1987)" startWordPosition="957" endWordPosition="958">arning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity. 2 Theories of Creativity and Hypotheses Many researchers, from the ancient philosophers to the modern time scientists, have proposed theories that attempt to explain the mechanism of creative process. In this section, we draw connections from some of these theories developed for general human creativity to the problem of quantitatively interpreting linguistic creativity in lexical composition. 2.1 Divergent Thinking and Composition Divergent thinking (e.g., McCrae (1987)), which seeks to generate multiple unstereotypical solutions to an open ended problem has been considered as the key element in creative process, which contrasts with convergent thinking that find a single, correct solution (e.g., Cropley (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual w</context>
</contexts>
<marker>McCrae, 1987</marker>
<rawString>Robert R McCrae. 1987. Creativity, divergent thinking, and openness to experience. Journal of personality and social psychology, 52(6):1258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan M McCurry</author>
<author>Steven C Hayes</author>
</authors>
<title>Clinical and experimental perspectives on metaphorical talk.</title>
<date>1992</date>
<journal>Clinical Psychology Review,</journal>
<volume>12</volume>
<issue>7</issue>
<pages>785</pages>
<contexts>
<context position="2356" citStr="McCurry and Hayes (1992)" startWordPosition="340" endWordPosition="343">wing into the online text drawing attention from readers. Writers put significant effort in choosing the perfect words in completing their compositions, as a well-chosen combination of words is impactful in readers’ minds for rendering the precise intended meaning, as well as stimulating an increased level of cognitive responses and attention. Metaphors in particular, one of the quintessential forms of linguistic creativity, have been discussed extensively by studies across multiple disciplines, e.g., Cognitive Science, Psychology, Linguistics, and Literature (e.g., Lakoff and Johnson (1980), McCurry and Hayes (1992), Goatly (1997)). Moreover, recent studies based on fMRI begin to discover biological evidences that support the impact of creative phrases on people’s minds. These studies report that unconventional metaphoric expressions elicit significantly increased involvement of brain processing when compared against the effect of conventional metaphors or literal expressions (e.g., Mashal et al. (2007), Mashal et al. (2009)). Several linguistic elements, e.g., syntax, semantics, and pragmatics, are likely to be working together in order to lead to the perception of creativity. However, their underlying </context>
</contexts>
<marker>McCurry, Hayes, 1992</marker>
<rawString>Susan M. McCurry and Steven C. Hayes. 1992. Clinical and experimental perspectives on metaphorical talk. Clinical Psychology Review, 12(7):763 – 785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: Investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>531--538</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5338" citStr="Mihalcea and Strapparava (2005)" startWordPosition="790" endWordPosition="793">t a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to th</context>
<context position="35231" citStr="Mihalcea and Strapparava (2005)" startWordPosition="5669" endWordPosition="5672">ental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when ma</context>
</contexts>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 531– 538, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="27531" citStr="Mitchell and Lapata (2008)" startWordPosition="4433" endWordPosition="4436">as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E Rn. Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional ope</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In In Proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>430--439</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7352" citStr="Mitchell and Lapata (2009)" startWordPosition="1100" endWordPosition="1103">ical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositiona</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 430–439. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Pratim Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning effective and interpretable semantic models using non-negative sparse embedding.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1933--1950</pages>
<contexts>
<context position="27163" citStr="Murphy et al. (2012)" startWordPosition="4378" endWordPosition="4381">position via deep learning (§5.3). Note that in all these approaches, the notion of creative semantic subspace is integrated indirectly, as the feature representation always incorporates the resulting (composed) vector representations. Baseline &amp; Configuration We consider the concatenation of two word vectors [191; 192] as the baseline, since it can be viewed as what simple bag-ofword features would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable semantic models using non-negative sparse embedding. In COLING, pages 1933–1950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Necka</author>
</authors>
<title>Memory and creativity. Encyclopedia of creativity,</title>
<date>1999</date>
<pages>2--193</pages>
<editor>ed. by MA Runco, SR Pritzker,</editor>
<contexts>
<context position="9809" citStr="Necka (1999)" startWordPosition="1484" endWordPosition="1485"> semantic space where creative expressions are more frequently found. For instance, phrases such as “guns and roses” and “metal to the petal” are semantically close to each other and yet both can be considered as interesting and creative (as opposed to one of them losing the sense of creativity due to its semantic proximity to the other). This notion of creative semantic subspace connects to theories that suggest that latent memories serve as motives for creative ideas and that one’s creativity is largely depending on prior experience and knowledge one has been exposed to (e.g., Freud (1908), Necka (1999), Glaskin (2011), Cohen and Levinthal (1990), Amabile (1997)), a point also made by Einstein: “The secret to creativity is knowing how to hide your sources.” Figure 5 presents visualized supports for creative semantic subspace,4 where we observe that phrases in the neighborhood of legal terms are generally not creative, while the semantic neighborhood of 2With additional context this example may turn into a creative one, but for simplicity we focus on phrases with two content words considered out of context. 3Investigation on recursive composition of more than two content words and the influen</context>
</contexts>
<marker>Necka, 1999</marker>
<rawString>Edward Necka. 1999. Memory and creativity. Encyclopedia of creativity, ed. by MA Runco, SR Pritzker, 2:193–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gozde Ozbal</author>
<author>Carlo Strapparava</author>
</authors>
<title>A computational approach to the automation of creative naming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>703--711</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="35590" citStr="Ozbal and Strapparava (2012)" startWordPosition="5725" endWordPosition="5728">al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation to quantifying creativity in lexical composition. Memorability is loosely related to linguistic creativity (Danescu-Niculescu-Mizil et al. (2012)) as some of the creative quotes may be more memorable, but not all creative phrases are memorable and vice versa. 8 Conclusion </context>
</contexts>
<marker>Ozbal, Strapparava, 2012</marker>
<rawString>Gozde Ozbal and Carlo Strapparava. 2012. A computational approach to the automation of creative naming. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 703–711, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16002" citStr="Pang et al. (2002)" startWordPosition="2543" endWordPosition="2546">s a series of yes/no questions to help turkers to determine whether the given pair is creative or not.8 We determine the final label of a word pair based on two scores, creativity scale score and yes/no questionbased score. If creativity scale score is 4 or 5 and question-based score is positive, we label the pair as creative. Similarly, if creativity scale score is 1 or 2 and question-based score is negative, we label the pair as common. We discard the rest from the final dataset. This filtering process is akin to the removal of neural sentiment in the early work of sentiment analysis (e.g., Pang et al. (2002)).9 Table 2 shows the statistics of the resulting dataset. Creative Pairs and their Frequencies: To gain insights on the stratified sample of word pairs, we plot the label (E {creative, common}) distribution of word pairs as a function of simple statistics, such as a range (bucket) of bigram frequencies or PMI values of the given pair of words. Both bigram frequencies and PMI scores are computed based on Google Web 1T corpus Brants and Franz (2006). Figure 1 shows the results for word frequencies. As expected, word pairs with high frequencies are much more likely to be common, while word pairs</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representation.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--77</pages>
<contexts>
<context position="28877" citStr="Pollack (1990)" startWordPosition="4663" endWordPosition="4664">explicit vector compositions, we also probe implicit operations based on non-linear combinations of semantic dimensions using kernels (e.g., Sch¨olkopf and Smola (2002), Shawe-Taylor and Cristianini (2004)), in particular: • Polynomial: K(x, y) = (yxT y + r)d, y &gt; 0 • RBF: K(x, y) = exp(−-y IIx − yII2), y &gt; 0 • Laplacian: K(x, y) = exp(−-y IIx − yII), y &gt; 0 5.3 Learning Non-linear Composition via Deep Learning Yet another alternative to model non-linear composition is deep learning. To learn the non-linear transformation of a pair of semantic vectors, we explore the use of autoencoders (e.g., Pollack (1990), Voegtlin and Dominey (2005)). We follow the formulation of vector composition proposed by Socher et al. (2011) except that we do not stack autoencoders for recursion. More specifically, given the two input words 191, 192 E Rn, we want to learn a vector space representation of their combination p E Rn. The recursive auto encoder (RAE) of Socher et al. (2011) models the composition of a word pair as a non-linear transformation of their concatenation [191; 192]: fi1 = f(M1[191; 192] +1b1) (4) where M1 E Rn 2n. After adding a bias term 1b1 E Rn, a nonlinear element-wise function f such as tanh i</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representation. Artificial Intelligence, 46:77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Diane Litman</author>
</authors>
<title>Humor: Prosody analysis and automatic recognition for f* r* i* e* n* d* s*.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5367" citStr="Purandare and Litman (2006)" startWordPosition="794" endWordPosition="797">ressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity. 2 The</context>
<context position="35260" citStr="Purandare and Litman (2006)" startWordPosition="5673" endWordPosition="5676">honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun comp</context>
</contexts>
<marker>Purandare, Litman, 2006</marker>
<rawString>Amruta Purandare and Diane Litman. 2006. Humor: Prosody analysis and automatic recognition for f* r* i* e* n* d* s*. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 208–215. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassiliki Rentoumi</author>
<author>George A Vouros</author>
<author>Vangelis Karkaletsis</author>
<author>Amalia Moser</author>
</authors>
<title>Investigating metaphorical language in sentiment analysis: A senseto-sentiment perspective.</title>
<date>2012</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="11652" citStr="Rentoumi et al. (2012)" startWordPosition="1772" endWordPosition="1775">al definition of creativity given in §1) word pairs, e.g., invisible empire”. In our empirical investigation, this notion of semantically fruitful and futile semantic subspaces are captured using distributional semantic space models under supervised learning framework (§5). 2.3 Affective Language Another angle we probe is the connection between creative expressions and the use of affective language. This idea is supported in part by previous research that explored the connection between figurative languages such as metaphors and sentiment (e.g., Fussell and Moss (1998), Rumbell et al. (2008), Rentoumi et al. (2012)). The focus of previous work was either on interpretation of the sentiment in metaphors, or the use of metaphors in the description of affect. In contrast, we aim to quantify the correlation between creative expressions (beyond metaphors) and the use of sentimentladen words in a more systematic way. This exploration has a connection to the creative semantic subspace discussed earlier (§2.2), but pays a more direct attention to the aspect of sentiment and connotation. 3 Creative Language Dataset We start our investigation by considering two types of naturally existing collection of sentences: </context>
<context position="34972" citStr="Rentoumi et al. (2012)" startWordPosition="5631" endWordPosition="5635"> • • • -------- --------- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and </context>
</contexts>
<marker>Rentoumi, Vouros, Karkaletsis, Moser, 2012</marker>
<rawString>Vassiliki Rentoumi, George A. Vouros, Vangelis Karkaletsis, and Amalia Moser. 2012. Investigating metaphorical language in sentiment analysis: A senseto-sentiment perspective. ACM Trans. Speech Lang. Process., 9(3):6:1–6:31, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>907--916</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7444" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="1112" endWordPosition="1115">ords is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capture the statistical regularities in creative compositional operations. In particular, we will explore (1) compositional operations of vector space mo</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907–916. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Rumbell</author>
<author>John Barnden</author>
<author>Mark Lee</author>
<author>Alan Wallington</author>
</authors>
<date>2008</date>
<note>Affect in metaphor: Developments with wordnet.</note>
<contexts>
<context position="11628" citStr="Rumbell et al. (2008)" startWordPosition="1768" endWordPosition="1771">ting, per our operational definition of creativity given in §1) word pairs, e.g., invisible empire”. In our empirical investigation, this notion of semantically fruitful and futile semantic subspaces are captured using distributional semantic space models under supervised learning framework (§5). 2.3 Affective Language Another angle we probe is the connection between creative expressions and the use of affective language. This idea is supported in part by previous research that explored the connection between figurative languages such as metaphors and sentiment (e.g., Fussell and Moss (1998), Rumbell et al. (2008), Rentoumi et al. (2012)). The focus of previous work was either on interpretation of the sentiment in metaphors, or the use of metaphors in the description of affect. In contrast, we aim to quantify the correlation between creative expressions (beyond metaphors) and the use of sentimentladen words in a more systematic way. This exploration has a connection to the creative semantic subspace discussed earlier (§2.2), but pays a more direct attention to the aspect of sentiment and connotation. 3 Creative Language Dataset We start our investigation by considering two types of naturally existing c</context>
<context position="34948" citStr="Rumbell et al. (2008)" startWordPosition="5627" endWordPosition="5630">--- • universal anguish • • • -------- --------- • fundamental soul • theoretical wisdom • legal trading • legal progress • judicial verdict • pure phenomenology • technological refinement • normal professor • normal adulthood • absolute barbarism • logical market • fundamental key • -------- --------- • honest coward • • invisible empire • omnipotent realm • finite realm • human incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more crea</context>
</contexts>
<marker>Rumbell, Barnden, Lee, Wallington, 2008</marker>
<rawString>Tim Rumbell, John Barnden, Mark Lee, and Alan Wallington. 2008. Affect in metaphor: Developments with wordnet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with kernels.</title>
<date>2002</date>
<publisher>The MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>Bernhard Sch¨olkopf and Alexander J Smola. 2002. Learning with kernels. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel methods for pattern analysis. Cambridge university press.</title>
<date>2004</date>
<contexts>
<context position="28468" citStr="Shawe-Taylor and Cristianini (2004)" startWordPosition="4584" endWordPosition="4587"> is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional operations. See Table 5 for more details and experimental results. 5.2 Learning Nonlinear Composition via Kernels As an alternative to explicit vector compositions, we also probe implicit operations based on non-linear combinations of semantic dimensions using kernels (e.g., Sch¨olkopf and Smola (2002), Shawe-Taylor and Cristianini (2004)), in particular: • Polynomial: K(x, y) = (yxT y + r)d, y &gt; 0 • RBF: K(x, y) = exp(−-y IIx − yII2), y &gt; 0 • Laplacian: K(x, y) = exp(−-y IIx − yII), y &gt; 0 5.3 Learning Non-linear Composition via Deep Learning Yet another alternative to model non-linear composition is deep learning. To learn the non-linear transformation of a pair of semantic vectors, we explore the use of autoencoders (e.g., Pollack (1990), Voegtlin and Dominey (2005)). We follow the formulation of vector composition proposed by Socher et al. (2011) except that we do not stack autoencoders for recursion. More specifically, giv</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel methods for pattern analysis. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Models of metaphor in nlp.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>688--697</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5227" citStr="Shutova (2010)" startWordPosition="775" endWordPosition="776">nto automatic assessment of writing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learn</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 688–697, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254– 263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semisupervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7295" citStr="Socher et al. (2011)" startWordPosition="1091" endWordPosition="1094">y (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different modeling techniques to capt</context>
<context position="28989" citStr="Socher et al. (2011)" startWordPosition="4679" endWordPosition="4682">ic dimensions using kernels (e.g., Sch¨olkopf and Smola (2002), Shawe-Taylor and Cristianini (2004)), in particular: • Polynomial: K(x, y) = (yxT y + r)d, y &gt; 0 • RBF: K(x, y) = exp(−-y IIx − yII2), y &gt; 0 • Laplacian: K(x, y) = exp(−-y IIx − yII), y &gt; 0 5.3 Learning Non-linear Composition via Deep Learning Yet another alternative to model non-linear composition is deep learning. To learn the non-linear transformation of a pair of semantic vectors, we explore the use of autoencoders (e.g., Pollack (1990), Voegtlin and Dominey (2005)). We follow the formulation of vector composition proposed by Socher et al. (2011) except that we do not stack autoencoders for recursion. More specifically, given the two input words 191, 192 E Rn, we want to learn a vector space representation of their combination p E Rn. The recursive auto encoder (RAE) of Socher et al. (2011) models the composition of a word pair as a non-linear transformation of their concatenation [191; 192]: fi1 = f(M1[191; 192] +1b1) (4) where M1 E Rn 2n. After adding a bias term 1b1 E Rn, a nonlinear element-wise function f such as tanh is applied to the resulting vector. The representation p of the word pair is then fed into a reconstruction layer</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semisupervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J P van der Maaten</author>
<author>G E Hinton</author>
</authors>
<title>Visualizing high-dimensional data using t-sne.</title>
<date>2008</date>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing high-dimensional data using t-sne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
<author>Yanfen Hao</author>
<author>Guofu Li</author>
</authors>
<title>Multilingual harvesting of cross-cultural stereotypes.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>523--531</pages>
<contexts>
<context position="5264" citStr="Veale et al. (2008)" startWordPosition="779" endWordPosition="782">ing styles and quality, and utilized to automatically construct a collection of interesting expressions from the web, which may be potentially useful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on </context>
</contexts>
<marker>Veale, Hao, Li, 2008</marker>
<rawString>Tony Veale, Yanfen Hao, and Guofu Li. 2008. Multilingual harvesting of cross-cultural stereotypes. In ACL, pages 523–531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>Creative language retrieval: A robust hybrid of information retrieval and linguistic creativity.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>278--287</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="35317" citStr="Veale (2011)" startWordPosition="5684" endWordPosition="5685">uman incompetence • • -------- --------- 7 Related Work Among computational approaches that touch on linguistic creativity, many focused on metaphor (e.g., Dunn (2013), Krishnakumaran and Zhu (2007), Mashal et al. (2007), Rumbell et al. (2008), Rentoumi et al. (2012), Mashal et al. (2009)). Other linguistic devices and phenomena related to creativity include irony (e.g., Davidov et al. (2010), Gonz´alezIb´a˜nez et al. (2011), Filatova (2012)), neologism (e.g., Cartoni (2008)), humor (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)), and similes (e.g., Hao and Veale (2010)). Veale (2011) proposed the new task of creative text retrieval to harvest expressions that potentially convey the same meaning as the query phrase in a fresh or unusual way. Our work contributes to the retrieval process of recognizing more creative phrases. Ozbal and Strapparava (2012) explored automatic creative naming of commercial products and services, focusing on the generation of creative phrases within a specific domain. Costello (2002) investigated the cognitive process that guides people’s choice of words when making up a novel nounnoun compound. In contrast, we present a datadriven investigation </context>
</contexts>
<marker>Veale, 2011</marker>
<rawString>Tony Veale. 2011. Creative language retrieval: A robust hybrid of information retrieval and linguistic creativity. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 278–287, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Voegtlin</author>
<author>Peter F Dominey</author>
</authors>
<title>Linear recursive distributed representations.</title>
<date>2005</date>
<journal>Neural Netw.,</journal>
<volume>18</volume>
<issue>7</issue>
<contexts>
<context position="28906" citStr="Voegtlin and Dominey (2005)" startWordPosition="4665" endWordPosition="4668">compositions, we also probe implicit operations based on non-linear combinations of semantic dimensions using kernels (e.g., Sch¨olkopf and Smola (2002), Shawe-Taylor and Cristianini (2004)), in particular: • Polynomial: K(x, y) = (yxT y + r)d, y &gt; 0 • RBF: K(x, y) = exp(−-y IIx − yII2), y &gt; 0 • Laplacian: K(x, y) = exp(−-y IIx − yII), y &gt; 0 5.3 Learning Non-linear Composition via Deep Learning Yet another alternative to model non-linear composition is deep learning. To learn the non-linear transformation of a pair of semantic vectors, we explore the use of autoencoders (e.g., Pollack (1990), Voegtlin and Dominey (2005)). We follow the formulation of vector composition proposed by Socher et al. (2011) except that we do not stack autoencoders for recursion. More specifically, given the two input words 191, 192 E Rn, we want to learn a vector space representation of their combination p E Rn. The recursive auto encoder (RAE) of Socher et al. (2011) models the composition of a word pair as a non-linear transformation of their concatenation [191; 192]: fi1 = f(M1[191; 192] +1b1) (4) where M1 E Rn 2n. After adding a bias term 1b1 E Rn, a nonlinear element-wise function f such as tanh is applied to the resulting ve</context>
</contexts>
<marker>Voegtlin, Dominey, 2005</marker>
<rawString>Thomas Voegtlin and Peter F. Dominey. 2005. Linear recursive distributed representations. Neural Netw., 18(7):878–895, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second AAAI Symposium on Quantum Interaction.</booktitle>
<contexts>
<context position="27547" citStr="Widdows (2008)" startWordPosition="4437" endWordPosition="4439">atures would be. Since the size of creative pair dataset is not at scale yet, we choose to work with vector space models that are in reduced dimensions. We experimented with both Non-Negative Sparse Embedding (Murphy et al. (2012)) and neural semantic vectors of Huang et al. (2012), but report experiments with the latter only as those gave us slightly better results. 5.1 Compositional Vector Operations We consider the following compositional vector operations inspired by recent studies for compositional distributional semantics (e.g., Guevara (2011), Clarke (2012), Mitchell and Lapata (2008), Widdows (2008)). • ADD: 191 + 192 • DIFF: abs(191 − 192) of Chen and Lin (2005) determines that the most two important ones are RH(w1, w2) and KL(w1, w2). • MULT: 191 .* 192 • MIN: min{191, 192} • MAX: max{191, 192} All operations take two input vectors E Rn, and output a vector E Rn. Each operation is applied element-wise. We then perform binary classification over the composed vectors using linear SVM. Besides using features based on the composed vectors, we also experiment with features based on concatenating multiple composed vectors, in the hope to capture more diverse compositional operations. See Tab</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second AAAI Symposium on Quantum Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23100" citStr="Wilson et al. (2005)" startWordPosition="3731" endWordPosition="3734">re non-linear correlations that are evident in graphs shown in Figure 3. Second, these measures only capture the surprisal aspect of creativity, missing the other important qualities: interestingness or imaginativeness. 4.2 Sentiment and Connotation Next we investigate the connection between creativity and sentiment, as illustrated in §2.3. We consider both sentiment (more explicit) and connotation (more implicit) words,13 and consider them with or without distinguishing the polarity (i.e., positive, negative). To determine sentiment and connotation, we use lexicons provided by OpinionFinder (Wilson et al. (2005)) and Feng et al. (2013) respectively. We denote polarity of a word wi as L(wi).14 When wi has a negative polarity L(wi) is assigned a value of -1, and when wi is positive L(wi) is equal to 1. We assume that a word is neutral when it is not in the lexicon, assigning 0 to L(wi). For a word pair w1w2 we compute absolute difference Ldiff(w1, w2) between polarities of tokens in a word pair in order to catch examples such as “inglorious success”. 13E.g., expressions such as “blue sky” or “white sand” are not sentiment-laden, but do have positive connotation. 14We denote polarity from OpinionFinder </context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34–35. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7273" citStr="Yessenalina and Cardie (2011)" startWordPosition="1087" endWordPosition="1090"> correct solution (e.g., Cropley (2006)). Applying the same high-level idea to lexical composition, divergent composition that explores an unusual, unconventional set of words is more likely to be creative. Note that the key novelty then lies in the compositional operation itself, i.e., the act of putting together a set of words in an unexpected way, rather than the rareness of individual words being used. In recent years there has been a swell of work on compositional distributional semantics that captures the compositional aspects of language understanding, such as sentiment analysis (e.g., Yessenalina and Cardie (2011), Socher et al. (2011)) and language modeling (e.g., Mitchell and Lapata (2009), Baroni and Zamparelli (2010), Guevara (2011), Clarke (2012), Rudolph and Giesbrecht (2010)). However, none has examined the compositional nature in quantifying creativity in lexical composition. We consider two computational approaches to capture the notion of creative composition. The first is via various information theoretic measures, e.g., relative entropy reduction, to measure the surprisal of seeing the next word given the previous word. The second is via supervised learning, where we explore different model</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 172–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zhiting Xu</author>
<author>Tushar Khot</author>
</authors>
<title>How creative is your writing? a linguistic creativity measure from computer science and cognitive psychology perspectives.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>87--93</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4123" citStr="Zhu et al. (2009)" startWordPosition="596" endWordPosition="599"> October 2013. c�2013 Association for Computational Linguistics miliar biomedical terms, e.g., “cardiac glycosides”, are only informative without appreciable creativity. Similarly, less frequent combinations of words, e.g., “rotten detergent” or “quiet teenager”, though describing situations that are certainly uncommon, do not bring about the sense of creativity. Finally, some unique combinations of words can be just nonsensical , e.g., “elegant glycosides”. Different studies assumed different definitions of linguistic creativity depending on their context and end goals (e.g., Chomsky (1976), Zhu et al. (2009), Gerv´as (2010), Maybin and Swann (2007), Carter and McCarthy (2004)). In this paper, as an operational definition, we consider a phrase creative if it is (a) unconventional or uncommon, and (b) expressive in an interesting, imaginative, or inspirational way. A system that can recognize creative expressions could be of practical use for many aspiring writers who are often in need of inspirational help in searching for the optimal choice of words. Such a system can also be integrated into automatic assessment of writing styles and quality, and utilized to automatically construct a collection o</context>
<context position="5407" citStr="Zhu et al. (2009)" startWordPosition="802" endWordPosition="805">eful for enriching natural language generation systems. With these practical goals in mind, we aim to understand phrases with linguistic creativity in a broad scope. Similarly as the work of Zhu et al. (2009), our study encompasses phrases that evoke the sense of interestingness and creativity in readers’ minds, rather than focusing exclusively on clearly but narrowly defined figure of speeches such as metaphors (e.g., Shutova (2010)), similes (e.g., Veale et al. (2008), Hao and Veale (2010)), and humors (e.g., Mihalcea and Strapparava (2005), Purandare and Litman (2006)). Unlike the study of Zhu et al. (2009), however, we concentrate specifically on how combinations of different words give rise to the sense of creativity, as this is an angle that has not been directly studied before. We leave the roles of syntactic elements as future research. We first examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity. 2 Theories of Creativity and Hypotheses Many </context>
</contexts>
<marker>Zhu, Xu, Khot, 2009</marker>
<rawString>Xiaojin Zhu, Zhiting Xu, and Tushar Khot. 2009. How creative is your writing? a linguistic creativity measure from computer science and cognitive psychology perspectives. In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 87–93. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
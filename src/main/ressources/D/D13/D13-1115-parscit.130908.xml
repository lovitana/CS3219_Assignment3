<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007631">
<title confidence="0.999054">
A Multimodal LDA Model Integrating
Textual, Cognitive and Visual Modalities
</title>
<author confidence="0.999596">
Stephen Roller Sabine Schulte im Walde
</author>
<affiliation confidence="0.9997275">
Department of Computer Science Institut f¨ur Maschinelle Sprachverarbeitung
The University of Texas at Austin Universit¨at Stuttgart
</affiliation>
<email confidence="0.997926">
roller@cs.utexas.edu schulte@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999558130434783">
Recent investigations into grounded models of
language have shown that holistic views of
language and perception can provide higher
performance than independent views. In this
work, we improve a two-dimensional multi-
modal version of Latent Dirichlet Allocation
(Andrews et al., 2009) in various ways. (1) We
outperform text-only models in two different
evaluations, and demonstrate that low-level
visual features are directly compatible with
the existing model. (2) We present a novel
way to integrate visual features into the LDA
model using unsupervised clusters of images.
The clusters are directly interpretable and im-
prove on our evaluation tasks. (3) We provide
two novel ways to extend the bimodal mod-
els to support three or more modalities. We
find that the three-, four-, and five-dimensional
models significantly outperform models using
only one or two modalities, and that nontex-
tual modalities each provide separate, disjoint
knowledge that cannot be forced into a shared,
latent structure.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981837209302">
In recent years, an increasing body of work has been
devoted to multimodal or “grounded” models of lan-
guage where semantic representations of words are
extended to include perceptual information. The un-
derlying hypothesis is that the meanings of words
are explicitly tied to our perception and understand-
ing of the world around us, and textual-information
alone is insufficient for a complete understanding of
language.
The language grounding problem has come in
many different flavors with just as many different ap-
proaches. Some approaches apply semantic parsing,
where words and sentences are mapped to logical
structure meaning (Kate and Mooney, 2007). Oth-
ers provide automatic mappings of natural language
instructions to executable actions, such as interpret-
ing navigation directions (Chen and Mooney, 2011)
or robot commands (Tellex et al., 2011; Matuszek et
al., 2012). Some efforts have tackled tasks such as
automatic image caption generation (Feng and La-
pata, 2010a; Ordonez et al., 2011), text illustration
(Joshi et al., 2006), or automatic location identifica-
tion of Twitter users (Eisenstein et al., 2010; Wing
and Baldridge, 2011; Roller et al., 2012).
Another line of research approaches grounded
language knowledge by augmenting distributional
approaches of word meaning with perceptual infor-
mation (Andrews et al., 2009; Steyvers, 2010; Feng
and Lapata, 2010b; Bruni et al., 2011; Silberer and
Lapata, 2012; Johns and Jones, 2012; Bruni et al.,
2012a; Bruni et al., 2012b; Silberer et al., 2013).
Although these approaches have differed in model
definition, the general goal in this line of research
has been to enhance word meaning with perceptual
information in order to address one of the most com-
mon criticisms of distributional semantics: that the
“meaning of words is entirely given by other words”
(Bruni et al., 2012b).
In this paper, we explore various ways to integrate
new perceptual information through novel computa-
tional modeling of this grounded knowledge into a
multimodal distributional model of word meaning.
The model we rely on was originally developed by
</bodyText>
<page confidence="0.936277">
1146
</page>
<note confidence="0.735261">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998722136363636">
Andrews et al. (2009) and is based on a general-
ization of Latent Dirichlet Allocation. This model
has previously been shown to provide excellent per-
formance on multiple tasks, including prediction of
association norms, word substitution errors, seman-
tic inferences, and word similarity (Andrews et al.,
2009; Silberer and Lapata, 2012). While prior work
has used the model only with feature norms and vi-
sual attributes, we show that low-level image fea-
tures are directly compatible with the model and
provide improved representations of word meaning.
We also show how simple, unsupervised clusters of
images can act as a semantically useful and qualita-
tively interesting set of features. Finally, we describe
two ways to extend the model by incorporating three
or more modalities. We find that each modality pro-
vides useful but disjoint information for describing
word meaning, and that a hybrid integration of mul-
tiple modalities provides significant improvements
in the representations of word meaning. We release
both our code and data to the community for future
research.1
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999192428571429">
The language grounding problem has received sig-
nificant attention in recent years, owed in part to the
wide availability of data sets (e.g. Flickr, Von Ahn
(2006)), computing power, improved computer vi-
sion models (Oliva and Torralba, 2001; Lowe, 2004;
Farhadi et al., 2009; Parikh and Grauman, 2011)
and neurological evidence of ties between the lan-
guage, perceptual and motor systems in the brain
(Pulverm¨uller et al., 2005; Tettamanti et al., 2005;
Aziz-Zadeh et al., 2006).
Many approaches to multimodal research have
succeeded by abstracting away raw perceptual in-
formation and using high-level representations in-
stead. Some works abstract perception via the us-
age of symbolic logic representations (Chen et al.,
2010; Chen and Mooney, 2011; Matuszek et al.,
2012; Artzi and Zettlemoyer, 2013), while others
choose to employ concepts elicited from psycholin-
guistic and cognition studies. Within the latter cat-
egory, the two most common representations have
been association norms, where subjects are given a
</bodyText>
<footnote confidence="0.968396">
1http://stephenroller.com/research/
emnlp13
</footnote>
<bodyText confidence="0.999788354166667">
cue word and name the first (or several) associated
words that come to mind (e.g., Nelson et al. (2004)),
and feature norms, where subjects are given a cue
word and asked to describe typical properties of the
cue concept (e.g., McRae et al. (2005)).
Griffiths et al. (2007) helped pave the path for
cognitive-linguistic multimodal research, showing
that Latent Dirichlet Allocation outperformed La-
tent Semantic Analysis (Deerwester et al., 1990) in
the prediction of association norms. Andrews et al.
(2009) furthered this work by showing that a bi-
modal topic model, consisting of both text and fea-
ture norms, outperformed models using only one
modality on the prediction of association norms,
word substitution errors, and semantic interference
tasks. In a similar vein, Steyvers (2010) showed that
a different feature-topic model improved predictions
on a fill-in-the-blank task. Johns and Jones (2012)
take an entirely different approach by showing that
one can successfully infer held out feature norms
from weighted mixtures based on textual similarity.
Silberer and Lapata (2012) introduce a new method
of multimodal integration based on Canonical Cor-
relation Analysis, and performs a systematic com-
parison between their CCA-based model and others
on association norm prediction, held out feature pre-
diction, and word similarity.
As computer vision techniques have improved
over the past decade, other research has begun di-
rectly using visual information in place of feature
norms. The first work to do this with topic models is
Feng and Lapata (2010b). They use a Bag of Visual
Words (BoVW) model (Lowe, 2004) to create a bi-
modal vocabulary describing documents. The topic
model using the bimodal vocabulary outperforms a
purely textual based model in word association and
word similarity prediction. Bruni et al. (2012a) show
how a BoVW model may be easily combined with
a distributional vector space model of language us-
ing only vector concatenation. Bruni et al. (2012b)
show that the contextual visual words (i.e. the visual
features around an object, rather than of the object
itself) are even more useful at times, suggesting the
plausibility of a sort of distributional hypothesis for
images. More recently, Silberer et al. (2013) show
that visual attribute classifiers, which have been im-
mensely successful in object recognition (Farhadi
et al., 2009), act as excellent substitutes for feature
</bodyText>
<page confidence="0.990632">
1147
</page>
<bodyText confidence="0.999736692307692">
norms. Other work on modeling the meanings of
verbs using video recognition has also begun show-
ing great promise (Mathe et al., 2008; Regneri et al.,
2013).
The Computer Vision community has also bene-
fited greatly from efforts to unify the two modalities.
To name a few examples, Rohrbach et al. (2010)
and Socher et al. (2013) show how semantic infor-
mation from text can be used to improve zero-shot
classification (i.e., classifying never-before-seen ob-
jects), and Motwani and Mooney (2012) show that
verb clusters can be used to improve activity recog-
nition in videos.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999765">
Our experiments use several existing and new data
sets for each of our modalities. We employ a large
web corpus and a large set of association norms. We
also introduce two new overlapping data sets: a col-
lection of feature norms and a collection of images
for a number of German nouns.
</bodyText>
<subsectionHeader confidence="0.998294">
3.1 Textual Modality
</subsectionHeader>
<bodyText confidence="0.9999157">
For our Text modality, we use deWaC, a large Ger-
man web corpus created by the WaCKy group (Ba-
roni et al., 2009) containing approximately 1.7B
word tokens. We filtered the corpus by: removing
words with unprintable characters or encoding trou-
bles; removing all stopwords; removing word types
with a total frequency of less than 500; and remov-
ing documents with a length shorter than 100. The
resulting corpus has 1,038,883 documents consist-
ing of 75,678 word types and 466M word tokens.
</bodyText>
<subsectionHeader confidence="0.998814">
3.2 Cognitive Modalities
</subsectionHeader>
<bodyText confidence="0.999984064516129">
Association Norms (AN) is a collection of asso-
ciation norms collected by Schulte im Walde et al.
(2012). In association norm experiments, subjects
are presented with a cue word and asked to list the
first few words that come to mind. With enough sub-
jects and responses, association norms can provide a
common and detailed view of the meaning compo-
nents of cue words. After removing responses given
only once in the entire study, the data set contains
a total of 95,214 cue-response pairs for 1,012 nouns
and 5,716 response types.
Feature Norms (FN) is our new collection of fea-
ture norms for a group of 569 German nouns. We
present subjects on Amazon Mechanical Turk with
a cue noun and ask them to give between 4 and 8
typical descriptive features of the noun. Subjects
are given ten example responses; one such exam-
ple is a cue of Tisch ‘table’ and a response of hat
Beine ‘has legs’. After collection, subjects who
are obvious spammers or did not follow instructions
are manually filtered. Responses are manually cor-
rected for spelling mistakes and semantically nor-
malized.2 Finally, responses which are only given
once in the study are removed. The final data set
contains 11,714 cue-response pairs for 569 nouns
and 2,589 response types.
Note that the difference between association
norms and feature norms is subtle, but important. In
AN collection, subjects simply name related words
as fast as possible, while in FN collection, subjects
must carefully describe the cue.
</bodyText>
<subsectionHeader confidence="0.995008">
3.3 Visual Modalities
</subsectionHeader>
<bodyText confidence="0.99994204">
BilderNetle (“little ImageNet” in Swabian German)
is our new data set of German noun-to-ImageNet
synset mappings. ImageNet is a large-scale and
widely used image database, built on top of Word-
Net, which maps words into groups of images,
called synsets (Deng et al., 2009). Multiple synsets
exist for each meaning of a word. For example, Im-
ageNet contains two different synsets for the word
mouse: one contains images of the animal, while
the other contains images of the computer periph-
eral. This BilderNetle data set provides mappings
from German noun types to images of the nouns via
ImageNet.
Starting with a set of noun compounds and their
nominal constituents von der Heide and Borgwaldt
(2009), five native German speakers and one native
English speaker (including the authors of this paper)
work together to map German nouns to ImageNet
synsets. With the assistance of a German-English
dictionary, the participants annotate each word with
all its possible meanings. After discussing the an-
notations with the German speakers, the English
speaker manually map the word meanings to synset
senses in ImageNet. Finally, the German speakers
review samples of the images for each word to en-
</bodyText>
<footnote confidence="0.993945666666667">
2For brevity, we include the full details of the spammer iden-
tification, cleansing process and normalization techniques in the
Supplementary Materials.
</footnote>
<page confidence="0.997255">
1148
</page>
<bodyText confidence="0.999581666666667">
sure the pictures accurately reflect the original noun
in question. Not all words or meanings are mapped
to ImageNet, as there are a number of words with-
out entries in ImageNet, but the resulting data set
contains a considerable amount of polysemy. The fi-
nal data set contains 2022 word-synset mappings for
just 309 words. All but three of these words overlap
with our data set of feature norms. After extract-
ing sections of images using bounding boxes when
available by ImageNet (and using the entire image
when bounding boxes are unavailable), the data set
contains 1,305,602 images.
</bodyText>
<subsectionHeader confidence="0.709185">
3.3.1 Image Processing
</subsectionHeader>
<bodyText confidence="0.999971787878788">
After the collection of all the images, we extracted
simple, low-level computer vision features to use as
modalities in our experiments.
First, we compute a simple Bag of Visual Words
(BoVW) model for our images using SURF key-
points (Bay et al., 2008). SURF is a method
for selecting points-of-interest within an image. It
is faster and more forgiving than the commonly
known SIFT algorithm. We compute SURF key-
points for every image in our data set using Sim-
pleCV3 and randomly sample 1% of the keypoints.
The keypoints are clustered into 5,000 visual code-
words (centroids) using k-means clustering (Sculley,
2010), and images are then quantized over the 5,000
codewords. All images for a given word are summed
together to provide an average representation for the
word. We refer to this representation as the SURF
modality.
While this is a standard, basic BoVW model,
each individual codeword on its own may not pro-
vide a large degree of semantic information; typi-
cally a BoVW representation acts predominantly as
a feature space for a classifier, and objects can only
be recognize using collections of codewords. To
test that similar concepts should share similar vi-
sual codewords, we cluster the BoVW representa-
tions for all our images into 500 clusters with k-
means clustering, and represent each word as mem-
bership over the image clusters, forming the SURF
Clusters modality. The number of clusters is chosen
arbitrarily. Ideally, each cluster should have a com-
mon object or clear visual attribute, and words are
express in terms of these visual commonalities.
</bodyText>
<footnote confidence="0.84609">
3http://simplecv.org
</footnote>
<bodyText confidence="0.999956888888889">
We also compute GIST vectors (Oliva and Tor-
ralba, 2001) for every image using LearGIST
(Douze et al., 2009). Unlike SURF descriptors,
GIST produces a single vector representation for an
image. The vector does not find points of interest
in the image, but rather attempts to provide a rep-
resentation for the overall “gist” of the whole im-
age. It is frequently used in tasks like scene iden-
tification, and Deselaers and Ferrari (2011) shows
that distance in GIST space correlates well with se-
mantic distance in WordNet. After computing the
GIST vectors, each textual word is represented as
the centroid GIST vector of all its images, forming
the GIST modality.
Finally, as with the SURF features, we clustered
the GIST representations for our images into 500
clusters, and represented words as membership in
the clusters, forming the GIST Clusters modality.
</bodyText>
<sectionHeader confidence="0.992408" genericHeader="method">
4 Model Definition
</sectionHeader>
<bodyText confidence="0.99997175">
Our experiments are based on the multimodal ex-
tension of Latent Dirichlet Allocation developed by
Andrews et al. (2009). Previously LDA has been
successfully used to infer unsupervised joint topic
distributions over words and feature norms together
(Andrews et al., 2009; Silberer and Lapata, 2012).
It has also been shown to be useful in joint infer-
ence of text with visual attributes obtained using vi-
sual classifiers (Silberer et al., 2013). These mul-
timodal LDA models (hereafter, mLDA) have been
shown to be qualitatively sensible and highly pre-
dictive of several psycholinguistic tasks (Andrews et
al., 2009). However, prior work using mLDA is lim-
ited to two modalities at a time. In this section, we
describe bimodal mLDA and define two methods for
extending it to three or more modalities.
</bodyText>
<subsectionHeader confidence="0.998606">
4.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999831333333333">
Latent Dirichlet Allocation (Blei et al., 2003), or
LDA, is an unsupervised Bayesian probabilistic
model of text documents. It assumes that all docu-
ments are probabilistically generated from a shared
set of K common topics, where each topic is a multi-
nomial distribution over the vocabulary (notated as
Q), and documents are modeled as mixtures of these
shared topics (notated as 0). LDA assumes every
document in the corpus is generated using the fol-
</bodyText>
<page confidence="0.976104">
1149
</page>
<bodyText confidence="0.504928">
lowing generative process:
</bodyText>
<listItem confidence="0.995982333333333">
1. A document-specific topic distribution, θd —
Dir(α) is drawn.
2. For the ith word in the document,
(a) A topic assignment zi — θd is drawn,
(b) and a word wi — βzi is drawn and ob-
served.
</listItem>
<bodyText confidence="0.995933333333333">
The task of Latent Dirichlet Allocation is then to
automatically infer the latent document distribution
θd for each document d E D, and the topic distri-
bution βk for each of the k = 11, ... , K} topics,
given the data. The probability that the ith word of
document d is
</bodyText>
<equation confidence="0.977247">
p(wi, θd) = � p(wi|βk)p(zi = k|θd).
k
</equation>
<subsectionHeader confidence="0.960411">
4.2 Multimodal LDA
</subsectionHeader>
<bodyText confidence="0.9999124">
Andrews et al. (2009) extend LDA to allow for the
inference of document and topic distributions in a
multimodal corpus. In their model, a document con-
sists of a set of (word, feature) pairs,4 rather than just
words, and documents are still modeled as mixtures
of shared topics. Topics consist of multinomial dis-
tributions over words, βk, but are extended to also
include multinomial distributions over features, ψk.
The generative process is amended to include these
feature distributions:
</bodyText>
<listItem confidence="0.999055571428571">
1. A document-specific topic distribution, θd —
Dir(α) is drawn.
2. For the ith (word, feature) pair in the document,
(a) A topic assignment zi — θd is drawn;
(b) a word wi — βzi is drawn;
(c) a feature fi — ψzi is drawn;
(d) the pair (wi, fi) is observed.
</listItem>
<bodyText confidence="0.9637785">
The conditional probability of the ith pair (wi, fi)
is updated appropriately:
</bodyText>
<equation confidence="0.981975">
�p(wi, fi, θd) = p(wi|βk)p(fi|ψk)p(zi = k|θd).
k
</equation>
<bodyText confidence="0.999486222222222">
The key aspect to notice is that the observed
word wi and feature fi are conditionally indepen-
dent given the topic selection, zi. This powerful ex-
tension allows for joint inference over both words
4Here, and elsewhere, feature and f simply refer to a token
from a nontextual modality and should not be confused with the
machine learning sense offeature.
and features, and topics become the key link be-
tween the text and feature modalities.
</bodyText>
<subsectionHeader confidence="0.991879">
4.3 3D Multimodal LDA
</subsectionHeader>
<bodyText confidence="0.999746285714286">
We can easily extend the bimodal LDA model to in-
corporate three or more modalities by simply per-
forming inference over n-tuples instead of pairs, and
still mandating that each modality is conditionally
independent given the topic. We consider the ith tu-
ple (wi, fi, fi&apos;,...) in document d to have a condi-
tional probability of:
</bodyText>
<equation confidence="0.989861333333333">
p(wi, fi, f&apos;i, ... ,θd) =
� p(wi|βi)p(fi|ψk)p(f&apos;i|ψ&apos;i) ··· p(zi = k|θd)
k
</equation>
<bodyText confidence="0.998679888888889">
That is, we simply take the original mLDA model
of Andrews et al. (2009) and generalize it in the
same way they generalize LDA. At first glance, it
seems that the inference task should become more
difficult as the number of modalities increases and
observed tuples become sparser, but the task remains
roughly the same difficulty, as all of the observed
elements of a tuple are conditionally independent
given the topic assignment zi.
</bodyText>
<subsectionHeader confidence="0.993423">
4.4 Hybrid Multimodal LDA
</subsectionHeader>
<bodyText confidence="0.998486136363636">
3D Multimodal LDA assumes that all modalities
share the same latent topic structure, θd. It is pos-
sible, however, that all modalities do not share some
latent structure, but the modalities can still combine
in order to enhance word meaning. The intuition
here is that language usage is guided by all informa-
tion gained in all modalities, but knowledge gained
from one modality may not always relate to another
modality. For example, the color red and the feature
“is sweet” both enhance our understanding of straw-
berries. However, one cannot see that strawberries
are sweet, so one should not correlate the color red
with the feature “is sweet.”
To this end, we define Hybrid Multimodal LDA.
In this setting, we perform separate, bimodal mLDA
inference according to Section 4.2 for each of the
different modalities, and then concatenate the topic
distributions for the words. In this way, Hybrid
mLDA assumes that every modality shares some la-
tent structure with the text in the corpus, but the la-
tent structures are not shared between non-textual
modalities.
</bodyText>
<page confidence="0.961278">
1150
</page>
<bodyText confidence="0.9998185">
For example, to generate a hybrid model for text,
feature norms and SURF, we separately perform bi-
modal mLDA for the text/feature norms modalities
and the text/SURF modalities. This provides us with
two topic-word distributions: βF N
k,w and βSk�,w, and
the hybrid model is simply the concatenation of the
two distributions,
</bodyText>
<equation confidence="0.97757075">
� FN
FN&amp;S = βj, w 1 &lt;j &lt; KFN
βj,w βS KFN &lt; &lt; KFN + KS,
j−KFN ,w
</equation>
<bodyText confidence="0.9998695">
where KFN indicates the number of topics for the
Feature Norm modality, and likewise for KS.
</bodyText>
<sectionHeader confidence="0.694869" genericHeader="method">
4.5 Inference
</sectionHeader>
<bodyText confidence="0.999943857142857">
Analytical inference of the posterior distribution of
mLDA is intractable, and must be approximated.
Prior work using mLDA has used Gibbs Sampling to
approximate the posterior, but we found this method
did not scale with larger values of K, especially
when applied to the relatively large deWaC corpus.
To solve these scaling issues, we implement On-
line Variational Bayesian Inference (Hoffman et al.,
2010; Hoffman et al., 2012) for our models. In
Variational Bayesian Inference (VBI), one approx-
imates the true posterior using simpler distributions
with free variables. The free variables are then op-
timized in an EM-like algorithm to minimize differ-
ence between the true and approximate posteriors.
Online VBI differs from normal VBI by using ran-
domly sampled minibatches in each EM step rather
than the entire data set. Online VBI easily scales
and quickly converges in all of our experiments. A
listing of the inference algorithm may be found in
the Supplementary Materials and the source code is
available as open source.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.998771">
5.1 Generating Multimodal Corpora
</subsectionHeader>
<bodyText confidence="0.99998859375">
In order to evaluate our algorithms, we first need to
generate multimodal corpora for each of our non-
textual modalities. We use the same method as An-
drews et al. (2009) for generating our multimodal
corpora: for each word token in the text corpus,
a feature is selected stochastically from the word’s
feature distribution, creating a word-feature pair.
Words without grounded features are all given the
same placeholder feature, also resulting in a word-
feature pair.5 That is, for the feature norm modal-
ity, we generate (word, feature norm) pairs; for
the SURF modality, we generate (word, codeword)
pairs, etc. The resulting stochastically generated
corpus is used in its corresponding experiments.
The 3D text-feature-association norm corpus is
generated slightly differently: for each word in
the original text corpus, we check the existence
of multimodal features in either modality. If a
word had no features, it is represented as a triple
(word, placeholderFN, placeholderAN). If the
word had only feature norms, but no associations,
it is generated as (word, feature, placeholderAN),
and similarly for association norms without feature
norms. In the case of words with presence in both
modalities, we generate two triples: (word, feature,
placeholderAN) and (word, placeholderFN, associ-
ation). This allows association norms and feature
norms to influence each other via the document mix-
tures θ, but avoids falsely labeling explicit relation-
ships between randomly selected feature norms and
associations.6 Other 3D corpora are generated using
the same general procedure.
</bodyText>
<subsectionHeader confidence="0.981083">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99991375">
We evaluate each of our models with two data sets: a
set of compositionality ratings for a number of Ger-
man noun-noun compounds, and the same associa-
tion norm data set used as one of our training modal-
ities in some settings.
Compositionality Ratings is a data set of com-
positionality ratings originally collected by von der
Heide and Borgwaldt (2009). The data set con-
sists of 450 concrete, depictable German noun com-
pounds along with compositionality ratings with re-
gard to their constituents. For each compound, 30
native German speakers are asked to rate how re-
lated the meaning of the compound is to each of
its constituents on a scale from 1 (highly opaque;
entirely noncompositional) to 7 (highly transparent;
very compositional). The mean of the 30 judgments
</bodyText>
<footnote confidence="0.9933155">
5Placeholder features must be hardcoded to have equal prob-
ability over all topics to prevent all placeholder pairs from ag-
gregating into a single topic.
6We did try generating the random triples without placehold-
ers, but the generated explicit relationships are overwhelmingly
detrimental in the settings we attempted.
</footnote>
<page confidence="0.991809">
1151
</page>
<bodyText confidence="0.999860285714286">
is taken as the gold compositionality rating for each
of the compound-constituent pairs. For example,
Ahornblatt ‘maple leaf’ is rated highly transparent
with respect to its constituents, Ahorn ‘maple’ and
Blatt ‘leaf’, but L¨owenzahn ‘dandelion’ is rated non-
compositional with respect to its constituents, L¨owe
‘lion’ and Zahn ‘tooth’.
We use a subset of the original data, compris-
ing of all two-part noun-noun compounds and their
constituents. This data set consists of 488 com-
positionality ratings (244 compound-head and 244
compound-modifier ratings) for 571 words. 309 of
the targets have images (the entire image data set);
563 have feature norms; and all 571 of have associ-
ation norms.
In order to predict compositional-
ity, for each compound-constituent pair
(wcompound, wconstituent), we compute nega-
tive symmetric KL divergence between the two
words’ topic distributions, where symmetric KL
divergence is defined as
</bodyText>
<equation confidence="0.97329">
sKL(w1||w2) = KL(w1||w2) + KL(w2||w1),
</equation>
<bodyText confidence="0.997340892857143">
and KL divergence is defined as
The values of −sKL for all compound-
constituent word pairs are correlated with the human
judgments of compositionality using Spearman’s p,
a rank-order correlation coefficient. Note that, since
KL divergence is a measure of dissimilarity, we
use negative symmetric KL divergence so that our
p correlation coefficient is positive. For exam-
ple, we compute both −sKL(Ahornblatt, Ahorn)
and −sKL(Ahornblatt, Blatt), and so on for all
488 compound-constituent pairs, and then correlate
these values with the human judgments.
Additionally, we also evaluate using the Associa-
tion Norms data set described in Section 3. Since
it is not sensible to evaluate association norm pre-
diction when they are also used as training data,
we omit this evaluation for this modality. Follow-
ing Andrews et al. (2009), we measure association
norm prediction as an average of percentile ranks.
For all possible pairs of words in our vocabulary,
we compute the negative symmetric KL divergence
between the two words. We then compute the per-
centile ranks of similarity for each word pair, e.g.,
“cat” is more similar to “dog” than 97.3% of the
rest of the vocabulary. We report the weighted mean
percentile ranks for all cue-association pairs, i.e.,
if a cue-association is given more than once, it is
counted more than once.
</bodyText>
<subsectionHeader confidence="0.988322">
5.3 Model Selection and Hyperparameter
Optimization
</subsectionHeader>
<bodyText confidence="0.999993461538461">
In all settings, we fix all Dirichlet priors at 0.1, use
a learning rate 0.7, and use minibatch sizes of 1024
documents. We do not optimize these hyperparame-
ters or vary them over time. The high Dirichlet pri-
ors are chosen to prevent sparsity in topic distribu-
tions, while the other parameters are selected as the
best from Hoffman et al. (2010).
In order to optimize the number of topics K, we
run five trials of each modality for 2000 iterations
for K = 150, 100, 150, 200, 2501 (a total of 25
runs per setup). We select the value or K for each
model which minimizes the average perplexity esti-
mate over the five trials.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999897">
6.1 Predicting Compositionality Ratings
</subsectionHeader>
<bodyText confidence="0.99998880952381">
Table 1 shows our results for each of our selected
models with our compositionality evaluation. The
2D models employing feature norms and associa-
tion norms do significantly better than the text-only
model (two-tailed t-test). This result is consistent
with other works using this model with these fea-
tures (Andrews et al., 2009; Silberer and Lapata,
2012).
We also see that the SURF visual words are able
to provide notable, albeit not significant, improve-
ments over the text-only modality. This confirms
that the low-level BoVW features do carry semantic
information, and are useful to consider individually.
The GIST vectors, on the other hand, perform al-
most exactly the same as the text-only model. These
features, which are usually more useful for compar-
ing overall image likeness than object likeness, do
not individually contain semantic information useful
for compositionality prediction.
The performance of the visual modalities reverses
when we look at our cluster-based models. Text
</bodyText>
<equation confidence="0.996134">
� ln(p(t
KL(w1||w2) = p(t = k |w1) / p(t = k |w1).
k = k |w2) )
</equation>
<page confidence="0.986974">
1152
</page>
<table confidence="0.999538571428571">
Modality K p
Text Only
Text Only (LDA) 200 .204
Bimodal mLDA
Text + Feature Norms 150 .310 ***
Text + Assoc. Norms 200 .328 **
Text + SURF 50 .251
Text + GIST 100 .204
Text + SURF Clusters 200 .159
Text + GIST Clusters 150 .233
3D mLDA
Text + FN + AN 250 .259
Text + FN + SURF 100 .286 *
Text + FN + GC 200 .261 *
Hybrid mLDA
FN, AN 150+200 .390 ***
FN, SURF 150+50 .350 ***
FN, GC 150+150 .340 ***
FN, AN, GC 150+200+150 .395 ***
FN, AN, SURF 150+200+50 .404 ***
FN, AN, SURF, GC 150+200+50+150 .406 ***
</table>
<tableCaption confidence="0.504046571428571">
Table 1: Average rank correlations between
−sKL(wcompoundiwconstituent) and our Composi-
tionality gold standard. The Hybrid models are the
concatenation of the corresponding Bimodal mLDA
models. Stars indicate statistical significance compared
to the text-only setting at the .05, .01 and .001 levels
using a two-tailed t-test.
</tableCaption>
<bodyText confidence="0.985649722222222">
combined with SURF clusters is our worst perform-
ing system, indicating our clusters of images with
common visual words are actively working against
us. The clusters based on GIST, on the other hand,
provide a minor improvement in compositionality
prediction.
All of our 3D models are better than the text-only
model, but they show a performance drop relative
to one or both of their comparable bimodal models.
The model combining text, feature norms, and as-
sociation norms is especially surprising: despite the
excellent performance of each of the bimodal mod-
els, the 3D model performs significantly worse than
either of its components (p &lt; .05). This indicates
that these modalities provide new insight into word
meaning, but cannot be forced into the same latent
structure.
The hybrid models show massive performance in-
</bodyText>
<table confidence="0.9997058125">
Modality K Assoc.
Text Only
Text Only (LDA) 200 .679
Bimodal mLDA
Text + Feature Norms 150 .676
Text + SURF 50 .789 ***
Text + GIST 100 .739 ***
Text + SURF Clusters 200 .618 ***
Text + GIST Clusters 150 .690
3D mLDA
Text + FN + SURF 100 .722 ***
Text + FN + GC 200 .601 ***
Hybrid mLDA
FN, SURF 150+50 .800 ***
FN, GC 150+150 .742 ***
FN, GC, SURF 150+150+50 .804 ***
</table>
<tableCaption confidence="0.929217">
Table 2: Average predicted rank similarity between cue
words and their associates. Stars indicate statistical sig-
</tableCaption>
<bodyText confidence="0.988714157894737">
nificance compared to the text-only modality, with gray
stars indicating the model is statistically worse than the
text model. The Hybrid models are the concatenation of
the corresponding Bimodal mLDA models.
creases across the board. Indeed, our 5 modality
hybrid model obtains a performance nearly twice
that of the text-only model. Not only do all 6 hy-
brid models do significantly better than the text-only
models, they show a highly significant improvement
over their individual components (p &lt; .001 for all
16 comparisons). Furthermore, improvements gen-
erally continue to grow significantly with each addi-
tional modality we incorporate into the hybrid model
(p &lt; .001 for all but the .404 to .406 compari-
son, which is not significant). Clearly, there is a
great deal to learn from combining three, four and
even five modalities, but the modalities are learn-
ing disjoint knowledge which cannot be forced into
a shared, latent structure.
</bodyText>
<subsectionHeader confidence="0.999685">
6.2 Predicting Association Norms
</subsectionHeader>
<bodyText confidence="0.999956857142857">
Table 2 shows the average weighted predicted rank
similarity between all cue words and associates and
trials. Here we see that feature norms do not seem to
be improving performance on the association norms.
This is slightly unexpected, but consistent with the
result that feature norms seem to provide helpful, but
disjoint semantic information as association norms.
</bodyText>
<page confidence="0.97392">
1153
</page>
<bodyText confidence="0.999996142857143">
We see that the image modalities are much more
useful than they are in compositionality prediction.
The SURF modality does extremely well in partic-
ular, but the GIST features also provide statistically
significant improvements over the text-only model.
Since the SURF and GIST image features tend to
capture object-likeness and scene-likeness respec-
tively, it is possible that words which share asso-
ciates are likely related through common settings
and objects that appear with them. This seems to
provide additional evidence of Bruni et al. (2012b)’s
suggestion that something like a distributional hy-
pothesis of images is plausible.
Once again, the clusters of images using SURF
causes a dramatic drop in performance. Combined
with the evidence from the compositionality assess-
ment, this shows that the SURF clusters are actively
confusing the models and not providing semantic in-
formation. GIST clusters, on the other hand, are pro-
viding a marginal improvement over the text-only
model, but the result is not significant. We take a
qualitative look into the GIST clusters in the next
section.
Once again, we see that the 3D models are inef-
fective compared to their bimodal components, but
the hybrid models provide at least as much informa-
tion as their components. The Feature Norms and
GIST Clusters hybrid model significantly improves
over both components.7 The final four-modality hy-
brid significantly outperforms all comparable mod-
els. As with the compositionality evaluation, we
conclude that the image and and feature norm mod-
els are providing disjoint semantic information that
cannot be forced into a shared latent structure, but
still augment each other when combined.
</bodyText>
<sectionHeader confidence="0.909725" genericHeader="evaluation">
7 Qualitative Analysis of Image Clusters
</sectionHeader>
<bodyText confidence="0.999588125">
In all research connecting word meaning with per-
ceptual information, it is desirable that the inferred
representations be directly interpretable. One nice
property of the cluster-based modalities is that we
may represent each cluster as its prototypical im-
ages, and examine whether the prototypes are re-
lated to the topics.
We chose to limit our analysis to the GIST clus-
</bodyText>
<footnote confidence="0.9980165">
7The gain is smaller than compared to SURF Hybrid, but
there is much less variance in the trials.
</footnote>
<bodyText confidence="0.99998512195122">
ters for two primary reasons: first, the SURF clusters
did not perform well in our evaluations, and sec-
ond, preliminary investigation into the SURF clus-
ters show that the majority of SURF clusters are
nearly identical. This indicates our SURF clusters
are likely hindered by poor initialization or param-
eter selection, and may partially explain their poor
performance in evaluations.
We select our single best Text + GIST Clusters
trial from the Compositionality evaluation and look
at the topic distributions for words and image clus-
ters. For each topic, we select the three clusters with
the highest weight for the topic, p(cJOk). We extract
the five images closest to the cluster centroids, and
select two topics whose prototypical images are the
most interesting and informative. Figure 1 shows
these selected topics.
The first example topic contains almost exclu-
sively water-related terms. The first image, extracted
from the most probable cluster, does not at first seem
related to water. Upon further inspection, we find
that many of the water-related pictures are scenic
views of lakes and mountains, often containing a
cloudy sky. It seems that the GIST cluster does
not tend to group images of water, but rather nature
scenes that may contain water. This relationship is
more obvious in the second picture, especially when
one considers the water itself contains reflections of
the trees and mountain.
The second topic contains time-related terms. The
“@card@” term is a special token for all non-zero
and non-one numbers. The second word, “Uhr”, is
polysemous: it can mean clock, an object which tells
the time, or o’clock, as in We meet at 2 o’clock (“Wir
treffen uns um 2 Uhr.”) The three prototypical pic-
tures are not pictures of clocks, but round, detailed
objects similar to clocks. We see GIST has a prefer-
ence toward clustering images based on the predom-
inant shape of the image. Here we see the clusters
of GIST images are not providing a definite seman-
tic relationship, but an overwhelming visual one.
</bodyText>
<sectionHeader confidence="0.995595" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999528">
In this paper, we evaluated the role of low-level im-
age features, SURF and GIST, for their compatibil-
ity with the multimodal Latent Dirichlet Allocation
model of Andrews et al. (2009). We found both fea-
</bodyText>
<page confidence="0.979922">
1154
</page>
<table confidence="0.997818615384615">
Most Probable Words Translations Prototypical Images
Wasser water
Schiff ship
See lake
Meer sea
Meter meter
Fluß river
@card@ (number)
Uhr clock
Freitag Friday
Sonntag Sunday
Samstag Saturday
Montag Monday
</table>
<figureCaption confidence="0.9926755">
Figure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows water-
related words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks.
</figureCaption>
<bodyText confidence="0.999965833333333">
ture sets were directly compatible with multimodal
LDA and provided significant gains in their ability to
predict association norms over traditional text-only
LDA. SURF features also provided significant gains
over text-only LDA in predicting the compositional-
ity of noun compounds.
We also showed that words may be represented
in terms of membership of image clusters based on
the low-level image features. Image clusters based
on GIST features were qualitatively interesting, and
were able to give improvements over the text-only
model.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999982857142857">
We would like to thank the UT Natural Lan-
guage Learning Reading Group and the anonymous
EMNLP reviewers for their most helpful comments
and suggestions. We would also like to thank the
members of the SemRel group at IMS for their con-
siderable help in the construction of BilderNetle.
The authors acknowledge the Texas Advanced
Computing Center (TACC) for providing the grid
computing resources necessary for these results.
The research presented in this paper was funded by
the DFG Collaborative Research Centre SFB 732
(Stephen Roller) and the DFG Heisenberg Fellow-
ship SCHU-2580/1-1 (Sabine Schulte im Walde).
Finally, we showed two methods for extending
multimodal LDA to three or more modalities: the
first as a 3D model with a shared latent structure
between all modalities, and the second where latent
structures were inferred separately for each modal-
ity and joined together into a hybrid model. Al-
though the 3D model was unable to compete with
its bimodal components, we found the hybrid model
consistently improved performance over its compo-
nent modalities. We conclude that the combination
of many modalities provides the best representation
of word meaning, and that each nontextual modal-
ity is discovering disjoint information about word
meaning that cannot be forced into a global latent
structure.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998638733333333">
Mark Andrews, Gabriella Vigliocco, and David Vinson.
2009. Integrating experiential and distributional data
to learn semantic representations. Psychological Re-
view, 116(3):463.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly super-
vised learning of semantic parsers for mapping instruc-
tions to actions. In Transactions of the Association for
Computational Linguistics, volume 1, pages 49–62.
Lisa Aziz-Zadeh, Stephen M. Wilson, Giacomo Rizzo-
latti, and Marco Iacoboni. 2006. Congruent embodied
representations for visually presented actions and lin-
guistic phrases describing actions. Current Biology,
16(18):1818–1823.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
</reference>
<page confidence="0.944643">
1155
</page>
<reference confidence="0.996071424528302">
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van
Gool. 2008. Surf: Speeded up robust features. Com-
puter Vision and Image Understanding, 110(3):346–
359, June.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. Pro-
ceedings of the EMNLP 2011 Geometrical Models for
Natural Language Semantics, pages 22–32.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pages
136–145.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings of the 20th
ACM International Conference on Multimedia, pages
1219–1228.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. Proceedings of the 25th AAAI
Conference on Artificial Intelligence, pages 859–865,
August.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal ofArtificial
Intelligence Research, 37(1):397–436.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391–
407.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Computer Vision and Pat-
tern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 248–255. IEEE.
Thomas Deselaers and Vittorio Ferrari. 2011. Visual and
semantic similarity in imagenet. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
1777–1784.
Matthijs Douze, Herv´e J´egou, Harsimrat Sandhawalia,
Laurent Amsaleg, and Cordelia Schmid. 2009. Eval-
uation of gist descriptors for web-scale image search.
In Proceedings of the ACM International Conference
on Image and Video Retrieval, pages 19:1–19:8.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1277–1287.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In IEEE Conference on Computer Vision and Pattern
Recognition, pages 1778–1785.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1239–1249.
Yansong Feng and Mirella Lapata. 2010b. Visual in-
formation in semantic representation. In Human Lan-
guage Technologies: the 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 91–99.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211.
Matthew Hoffman, David M. Blei, and Francis Bach.
2010. Online learning for latent dirichlet allocation.
Advances in Neural Information Processing Systems,
23:856–864.
Matthew Hoffman, David M. Blei, Chong Wang, and
John Paisley. 2012. Stochastic variational inference.
ArXiv e-prints, June.
Brendan T. Johns and Michael N. Jones. 2012. Percep-
tual inference through global lexical similarity. Topics
in Cognitive Science, 4(1):103–120.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story
picturing engine—a system for automatic text illustra-
tion. ACM Transactions on Multimedia Computing,
Communications, and Applications, 2(1):68–89.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambiguous supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence, volume 7, pages 895–900.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110.
Stefan Mathe, Afsaneh Fazly, Sven Dickinson, and
Suzanne Stevenson. 2008. Learning the abstract mo-
tion semantics of verbs from captioned videos. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, pages 1–8.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and
Dieter Fox. 2012. Learning to parse natural language
commands to a robot control system. In Proceedings
of the 13th International Symposium on Experimental
Robotics.
</reference>
<page confidence="0.864673">
1156
</page>
<reference confidence="0.999863239583333">
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547–559.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Im-
proving video activity recognition using object recog-
nition and text mining. In ECAI, pages 600–605.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 2004. The University of South Florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, &amp; Computers,
36(3):402–407.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42(3):145–175.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Infor-
mation Processing Systems, pages 1143–1151.
Devi Parikh and Kristen Grauman. 2011. Relative at-
tributes. In International Conference on Computer Vi-
sion, pages 503–510. IEEE.
Friedemann Pulverm¨uller, Olaf Hauk, Vadim V. Nikulin,
and Risto J Ilmoniemi. 2005. Functional links be-
tween motor and language systems. European Journal
of Neuroscience, 21(3):793–797.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
2013. Grounding action descriptions in videos. In
Transactions of the Association for Computational
Linguistics, volume 1, pages 25–36.
Marcus Rohrbach, Michael Stark, Gy¨orgy Szarvas, Iryna
Gurevych, and Bernt Schiele. 2010. What helps
where–and why? Semantic relatedness for knowledge
transfer. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 910–917.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1500–1510.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association norms of german
noun compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 632–639, Istanbul, Turkey.
D. Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th International Conference on
World Wide Web, pages 1177–1178.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433, Jeju
Island, Korea, July.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In Proceedings of the 51th Annual Meet-
ing of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert
Bastani, Christopher D. Manning, and Andrew Y. Ng.
2013. Zero-shot learning through cross-modal trans-
fer. International Conference on Learning Represen-
tations.
Mark Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234–243.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. Proceedings of the 25th AAAI
Conference on Artificial Intelligence.
Marco Tettamanti, Giovanni Buccino, Maria Cristina
Saccuman, Vittorio Gallese, Massimo Danna, Paola
Scifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F.
Cappa, and Daniela Perani. 2005. Listening to action-
related sentences activates fronto-parietal motor cir-
cuits. Journal of Cognitive Neuroscience, 17(2):273–
281.
Luis Von Ahn. 2006. Games with a purpose. Computer,
39(6):92–94.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages 51–
74.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, volume 11, pages 955–964.
</reference>
<page confidence="0.995153">
1157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799527">
<title confidence="0.9996445">A Multimodal LDA Model Integrating Textual, Cognitive and Visual Modalities</title>
<author confidence="0.999793">Stephen Roller Sabine Schulte im Walde</author>
<affiliation confidence="0.9990425">Department of Computer Science Institut f¨ur Maschinelle Sprachverarbeitung The University of Texas at Austin Universit¨at Stuttgart</affiliation>
<email confidence="0.836971">roller@cs.utexas.eduschulte@ims.uni-stuttgart.de</email>
<abstract confidence="0.998219333333334">Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="2663" citStr="Andrews et al., 2009" startWordPosition="388" endWordPosition="391">tions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through nove</context>
<context position="3932" citStr="Andrews et al., 2009" startWordPosition="583" endWordPosition="586">ge into a multimodal distributional model of word meaning. The model we rely on was originally developed by 1146 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity (Andrews et al., 2009; Silberer and Lapata, 2012). While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration</context>
<context position="6283" citStr="Andrews et al. (2009)" startWordPosition="946" endWordPosition="949">presentations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new metho</context>
<context position="15624" citStr="Andrews et al. (2009)" startWordPosition="2471" endWordPosition="2474">like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bim</context>
<context position="17356" citStr="Andrews et al. (2009)" startWordPosition="2770" endWordPosition="2773">y document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, θd — Dir(α) is drawn. 2. For the ith word in the document, (a) A topic assignment zi — θd is drawn, (b) and a word wi — βzi is drawn and observed. The task of Latent Dirichlet Allocation is then to automatically infer the latent document distribution θd for each document d E D, and the topic distribution βk for each of the k = 11, ... , K} topics, given the data. The probability that the ith word of document d is p(wi, θd) = � p(wi|βk)p(zi = k|θd). k 4.2 Multimodal LDA Andrews et al. (2009) extend LDA to allow for the inference of document and topic distributions in a multimodal corpus. In their model, a document consists of a set of (word, feature) pairs,4 rather than just words, and documents are still modeled as mixtures of shared topics. Topics consist of multinomial distributions over words, βk, but are extended to also include multinomial distributions over features, ψk. The generative process is amended to include these feature distributions: 1. A document-specific topic distribution, θd — Dir(α) is drawn. 2. For the ith (word, feature) pair in the document, (a) A topic a</context>
<context position="19145" citStr="Andrews et al. (2009)" startWordPosition="3078" endWordPosition="3081">h the machine learning sense offeature. and features, and topics become the key link between the text and feature modalities. 4.3 3D Multimodal LDA We can easily extend the bimodal LDA model to incorporate three or more modalities by simply performing inference over n-tuples instead of pairs, and still mandating that each modality is conditionally independent given the topic. We consider the ith tuple (wi, fi, fi&apos;,...) in document d to have a conditional probability of: p(wi, fi, f&apos;i, ... ,θd) = � p(wi|βi)p(fi|ψk)p(f&apos;i|ψ&apos;i) ··· p(zi = k|θd) k That is, we simply take the original mLDA model of Andrews et al. (2009) and generalize it in the same way they generalize LDA. At first glance, it seems that the inference task should become more difficult as the number of modalities increases and observed tuples become sparser, but the task remains roughly the same difficulty, as all of the observed elements of a tuple are conditionally independent given the topic assignment zi. 4.4 Hybrid Multimodal LDA 3D Multimodal LDA assumes that all modalities share the same latent topic structure, θd. It is possible, however, that all modalities do not share some latent structure, but the modalities can still combine in o</context>
<context position="22346" citStr="Andrews et al. (2009)" startWordPosition="3609" endWordPosition="3613">thm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5 Experimental Setup 5.1 Generating Multimodal Corpora In order to evaluate our algorithms, we first need to generate multimodal corpora for each of our nontextual modalities. We use the same method as Andrews et al. (2009) for generating our multimodal corpora: for each word token in the text corpus, a feature is selected stochastically from the word’s feature distribution, creating a word-feature pair. Words without grounded features are all given the same placeholder feature, also resulting in a wordfeature pair.5 That is, for the feature norm modality, we generate (word, feature norm) pairs; for the SURF modality, we generate (word, codeword) pairs, etc. The resulting stochastically generated corpus is used in its corresponding experiments. The 3D text-feature-association norm corpus is generated slightly di</context>
<context position="26649" citStr="Andrews et al. (2009)" startWordPosition="4276" endWordPosition="4279">ent. Note that, since KL divergence is a measure of dissimilarity, we use negative symmetric KL divergence so that our p correlation coefficient is positive. For example, we compute both −sKL(Ahornblatt, Ahorn) and −sKL(Ahornblatt, Blatt), and so on for all 488 compound-constituent pairs, and then correlate these values with the human judgments. Additionally, we also evaluate using the Association Norms data set described in Section 3. Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality. Following Andrews et al. (2009), we measure association norm prediction as an average of percentile ranks. For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words. We then compute the percentile ranks of similarity for each word pair, e.g., “cat” is more similar to “dog” than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet prio</context>
<context position="28207" citStr="Andrews et al., 2009" startWordPosition="4541" endWordPosition="4544">f topics K, we run five trials of each modality for 2000 iterations for K = 150, 100, 150, 200, 2501 (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al., 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual </context>
<context position="36834" citStr="Andrews et al. (2009)" startWordPosition="5968" endWordPosition="5971">ct which tells the time, or o’clock, as in We meet at 2 o’clock (“Wir treffen uns um 2 Uhr.”) The three prototypical pictures are not pictures of clocks, but round, detailed objects similar to clocks. We see GIST has a preference toward clustering images based on the predominant shape of the image. Here we see the clusters of GIST images are not providing a definite semantic relationship, but an overwhelming visual one. 8 Conclusions In this paper, we evaluated the role of low-level image features, SURF and GIST, for their compatibility with the multimodal Latent Dirichlet Allocation model of Andrews et al. (2009). We found both fea1154 Most Probable Words Translations Prototypical Images Wasser water Schiff ship See lake Meer sea Meter meter Fluß river @card@ (number) Uhr clock Freitag Friday Sonntag Sunday Samstag Saturday Montag Monday Figure 1: Example topics with prototypical images for the Text + GIST Cluster modality. The first topic shows waterrelated words, as well scenes which often appear with water. The second shows clock-like objects, but not clocks. ture sets were directly compatible with multimodal LDA and provided significant gains in their ability to predict association norms over trad</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>49--62</pages>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In Transactions of the Association for Computational Linguistics, volume 1, pages 49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Aziz-Zadeh</author>
<author>Stephen M Wilson</author>
<author>Giacomo Rizzolatti</author>
<author>Marco Iacoboni</author>
</authors>
<title>Congruent embodied representations for visually presented actions and linguistic phrases describing actions.</title>
<date>2006</date>
<journal>Current Biology,</journal>
<volume>16</volume>
<issue>18</issue>
<contexts>
<context position="5195" citStr="Aziz-Zadeh et al., 2006" startWordPosition="784" endWordPosition="787">icant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name t</context>
</contexts>
<marker>Aziz-Zadeh, Wilson, Rizzolatti, Iacoboni, 2006</marker>
<rawString>Lisa Aziz-Zadeh, Stephen M. Wilson, Giacomo Rizzolatti, and Marco Iacoboni. 2006. Congruent embodied representations for visually presented actions and linguistic phrases describing actions. Current Biology, 16(18):1818–1823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="9187" citStr="Baroni et al., 2009" startWordPosition="1418" endWordPosition="1422">ed to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al., 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al. (2012). In association norm experiments, subjects are presented with a cue word and asked to list the f</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Bay</author>
</authors>
<title>Andreas Ess, Tinne Tuytelaars, and Luc Van Gool.</title>
<date>2008</date>
<journal>Computer Vision and Image Understanding,</journal>
<volume>110</volume>
<issue>3</issue>
<pages>359</pages>
<marker>Bay, 2008</marker>
<rawString>Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. 2008. Surf: Speeded up robust features. Computer Vision and Image Understanding, 110(3):346– 359, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="16382" citStr="Blei et al., 2003" startWordPosition="2593" endWordPosition="2596">t al., 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al., 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set of K common topics, where each topic is a multinomial distribution over the vocabulary (notated as Q), and documents are modeled as mixtures of these shared topics (notated as 0). LDA assumes every document in the corpus is generated using the fol1149 lowing generative process: 1. A document-specific topic distribution, θd — Dir(α) is drawn. 2. For the ith word in the document, (a) A topic assignment zi — θd is drawn, (b) and a word wi — β</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>Proceedings of the EMNLP</booktitle>
<pages>22--32</pages>
<contexts>
<context position="2723" citStr="Bruni et al., 2011" startWordPosition="398" endWordPosition="401">irections (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a m</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics, pages 22–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="2793" citStr="Bruni et al., 2012" startWordPosition="410" endWordPosition="413">11; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on w</context>
<context position="7602" citStr="Bruni et al. (2012" startWordPosition="1151" endWordPosition="1154">ison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other</context>
<context position="32971" citStr="Bruni et al. (2012" startWordPosition="5340" endWordPosition="5343"> helpful, but disjoint semantic information as association norms. 1153 We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al. (2012b)’s suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section. Once again, we see that the 3D models are ineffecti</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012a. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Jasper Uijlings</author>
<author>Marco Baroni</author>
<author>Nicu Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Multimedia,</booktitle>
<pages>1219--1228</pages>
<contexts>
<context position="2793" citStr="Bruni et al., 2012" startWordPosition="410" endWordPosition="413">11; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on w</context>
<context position="7602" citStr="Bruni et al. (2012" startWordPosition="1151" endWordPosition="1154">ison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other</context>
<context position="32971" citStr="Bruni et al. (2012" startWordPosition="5340" endWordPosition="5343"> helpful, but disjoint semantic information as association norms. 1153 We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al. (2012b)’s suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section. Once again, we see that the 3D models are ineffecti</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings of the 20th ACM International Conference on Multimedia, pages 1219–1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>Proceedings of the 25th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>859--865</pages>
<contexts>
<context position="2138" citStr="Chen and Mooney, 2011" startWordPosition="308" endWordPosition="311">ion. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and </context>
<context position="5464" citStr="Chen and Mooney, 2011" startWordPosition="824" endWordPosition="827">ata sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave th</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. Proceedings of the 25th AAAI Conference on Artificial Intelligence, pages 859–865, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5441" citStr="Chen et al., 2010" startWordPosition="820" endWordPosition="823">e availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal ofArtificial Intelligence Research, 37(1):397–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="6221" citStr="Deerwester et al., 1990" startWordPosition="936" endWordPosition="939">ition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textua</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11358" citStr="Deng et al., 2009" startWordPosition="1777" endWordPosition="1780">final data set contains 11,714 cue-response pairs for 569 nouns and 2,589 response types. Note that the difference between association norms and feature norms is subtle, but important. In AN collection, subjects simply name related words as fast as possible, while in FN collection, subjects must carefully describe the cue. 3.3 Visual Modalities BilderNetle (“little ImageNet” in Swabian German) is our new data set of German noun-to-ImageNet synset mappings. ImageNet is a large-scale and widely used image database, built on top of WordNet, which maps words into groups of images, called synsets (Deng et al., 2009). Multiple synsets exist for each meaning of a word. For example, ImageNet contains two different synsets for the word mouse: one contains images of the animal, while the other contains images of the computer peripheral. This BilderNetle data set provides mappings from German noun types to images of the nouns via ImageNet. Starting with a set of noun compounds and their nominal constituents von der Heide and Borgwaldt (2009), five native German speakers and one native English speaker (including the authors of this paper) work together to map German nouns to ImageNet synsets. With the assistanc</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Vittorio Ferrari</author>
</authors>
<title>Visual and semantic similarity in imagenet.</title>
<date>2011</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1777--1784</pages>
<contexts>
<context position="15062" citStr="Deselaers and Ferrari (2011)" startWordPosition="2382" endWordPosition="2385"> of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall “gist” of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully</context>
</contexts>
<marker>Deselaers, Ferrari, 2011</marker>
<rawString>Thomas Deselaers and Vittorio Ferrari. 2011. Visual and semantic similarity in imagenet. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1777–1784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthijs Douze</author>
<author>Herv´e J´egou</author>
<author>Harsimrat Sandhawalia</author>
<author>Laurent Amsaleg</author>
<author>Cordelia Schmid</author>
</authors>
<title>Evaluation of gist descriptors for web-scale image search.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACM International Conference on Image and Video Retrieval,</booktitle>
<pages>19--1</pages>
<marker>Douze, J´egou, Sandhawalia, Amsaleg, Schmid, 2009</marker>
<rawString>Matthijs Douze, Herv´e J´egou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. 2009. Evaluation of gist descriptors for web-scale image search. In Proceedings of the ACM International Conference on Image and Video Retrieval, pages 19:1–19:8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1277--1287</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1778--1785</pages>
<contexts>
<context position="4991" citStr="Farhadi et al., 2009" startWordPosition="752" endWordPosition="755">porating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic an</context>
<context position="8142" citStr="Farhadi et al., 2009" startWordPosition="1237" endWordPosition="1240">ased model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity rec</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1778–1785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? Automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1239--1249</pages>
<contexts>
<context position="2300" citStr="Feng and Lapata, 2010" startWordPosition="334" endWordPosition="338">n alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition</context>
<context position="7332" citStr="Feng and Lapata (2010" startWordPosition="1108" endWordPosition="1111">roach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? Automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239–1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="2300" citStr="Feng and Lapata, 2010" startWordPosition="334" endWordPosition="338">n alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition</context>
<context position="7332" citStr="Feng and Lapata (2010" startWordPosition="1108" endWordPosition="1111">roach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Visual information in semantic representation. In Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="6049" citStr="Griffiths et al. (2007)" startWordPosition="914" endWordPosition="917">en et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-bla</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David M Blei</author>
<author>Francis Bach</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>23--856</pages>
<contexts>
<context position="21501" citStr="Hoffman et al., 2010" startWordPosition="3471" endWordPosition="3474">he concatenation of the two distributions, � FN FN&amp;S = βj, w 1 &lt;j &lt; KFN βj,w βS KFN &lt; &lt; KFN + KS, j−KFN ,w where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS. 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al., 2010; Hoffman et al., 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is avai</context>
<context position="27552" citStr="Hoffman et al. (2010)" startWordPosition="4429" endWordPosition="4432">s more similar to “dog” than 97.3% of the rest of the vocabulary. We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once. 5.3 Model Selection and Hyperparameter Optimization In all settings, we fix all Dirichlet priors at 0.1, use a learning rate 0.7, and use minibatch sizes of 1024 documents. We do not optimize these hyperparameters or vary them over time. The high Dirichlet priors are chosen to prevent sparsity in topic distributions, while the other parameters are selected as the best from Hoffman et al. (2010). In order to optimize the number of topics K, we run five trials of each modality for 2000 iterations for K = 150, 100, 150, 200, 2501 (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works usi</context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew Hoffman, David M. Blei, and Francis Bach. 2010. Online learning for latent dirichlet allocation. Advances in Neural Information Processing Systems, 23:856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David M Blei</author>
<author>Chong Wang</author>
<author>John Paisley</author>
</authors>
<title>Stochastic variational inference. ArXiv e-prints,</title>
<date>2012</date>
<contexts>
<context position="21524" citStr="Hoffman et al., 2012" startWordPosition="3475" endWordPosition="3478">e two distributions, � FN FN&amp;S = βj, w 1 &lt;j &lt; KFN βj,w βS KFN &lt; &lt; KFN + KS, j−KFN ,w where KFN indicates the number of topics for the Feature Norm modality, and likewise for KS. 4.5 Inference Analytical inference of the posterior distribution of mLDA is intractable, and must be approximated. Prior work using mLDA has used Gibbs Sampling to approximate the posterior, but we found this method did not scale with larger values of K, especially when applied to the relatively large deWaC corpus. To solve these scaling issues, we implement Online Variational Bayesian Inference (Hoffman et al., 2010; Hoffman et al., 2012) for our models. In Variational Bayesian Inference (VBI), one approximates the true posterior using simpler distributions with free variables. The free variables are then optimized in an EM-like algorithm to minimize difference between the true and approximate posteriors. Online VBI differs from normal VBI by using randomly sampled minibatches in each EM step rather than the entire data set. Online VBI easily scales and quickly converges in all of our experiments. A listing of the inference algorithm may be found in the Supplementary Materials and the source code is available as open source. 5</context>
</contexts>
<marker>Hoffman, Blei, Wang, Paisley, 2012</marker>
<rawString>Matthew Hoffman, David M. Blei, Chong Wang, and John Paisley. 2012. Stochastic variational inference. ArXiv e-prints, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan T Johns</author>
<author>Michael N Jones</author>
</authors>
<title>Perceptual inference through global lexical similarity.</title>
<date>2012</date>
<journal>Topics in Cognitive Science,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2773" citStr="Johns and Jones, 2012" startWordPosition="406" endWordPosition="409">ands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. Th</context>
<context position="6680" citStr="Johns and Jones (2012)" startWordPosition="1006" endWordPosition="1009">ave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work t</context>
</contexts>
<marker>Johns, Jones, 2012</marker>
<rawString>Brendan T. Johns and Michael N. Jones. 2012. Perceptual inference through global lexical similarity. Topics in Cognitive Science, 4(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dhiraj Joshi</author>
<author>James Z Wang</author>
<author>Jia Li</author>
</authors>
<title>The story picturing engine—a system for automatic text illustration.</title>
<date>2006</date>
<journal>ACM Transactions on Multimedia Computing, Communications, and Applications,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="2364" citStr="Joshi et al., 2006" startWordPosition="345" endWordPosition="348">The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance </context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The story picturing engine—a system for automatic text illustration. ACM Transactions on Multimedia Computing, Communications, and Applications, 2(1):68–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning language semantics from ambiguous supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd Conference on Artificial Intelligence,</booktitle>
<volume>7</volume>
<pages>895--900</pages>
<contexts>
<context position="1980" citStr="Kate and Mooney, 2007" startWordPosition="286" endWordPosition="289">dy of work has been devoted to multimodal or “grounded” models of language where semantic representations of words are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distribu</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2007. Learning language semantics from ambiguous supervision. In Proceedings of the 22nd Conference on Artificial Intelligence, volume 7, pages 895–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="4969" citStr="Lowe, 2004" startWordPosition="750" endWordPosition="751">del by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited fr</context>
<context position="7392" citStr="Lowe, 2004" startWordPosition="1121" endWordPosition="1122">s from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More rece</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Mathe</author>
<author>Afsaneh Fazly</author>
<author>Sven Dickinson</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Learning the abstract motion semantics of verbs from captioned videos.</title>
<date>2008</date>
<booktitle>In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8322" citStr="Mathe et al., 2008" startWordPosition="1267" endWordPosition="1270">sing only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We </context>
</contexts>
<marker>Mathe, Fazly, Dickinson, Stevenson, 2008</marker>
<rawString>Stefan Mathe, Afsaneh Fazly, Sven Dickinson, and Suzanne Stevenson. 2008. Learning the abstract motion semantics of verbs from captioned videos. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th International Symposium on Experimental Robotics.</booktitle>
<contexts>
<context position="2201" citStr="Matuszek et al., 2012" startWordPosition="319" endWordPosition="322">e explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni</context>
<context position="5487" citStr="Matuszek et al., 2012" startWordPosition="828" endWordPosition="831">Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-li</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2012. Learning to parse natural language commands to a robot control system. In Proceedings of the 13th International Symposium on Experimental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="6023" citStr="McRae et al. (2005)" startWordPosition="910" endWordPosition="913">ic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predic</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanvi S Motwani</author>
<author>Raymond J Mooney</author>
</authors>
<title>Improving video activity recognition using object recognition and text mining.</title>
<date>2012</date>
<booktitle>In ECAI,</booktitle>
<pages>600--605</pages>
<contexts>
<context position="8682" citStr="Motwani and Mooney (2012)" startWordPosition="1325" endWordPosition="1328">rs, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al., 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unpr</context>
</contexts>
<marker>Motwani, Mooney, 2012</marker>
<rawString>Tanvi S. Motwani and Raymond J. Mooney. 2012. Improving video activity recognition using object recognition and text mining. In ECAI, pages 600–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="5879" citStr="Nelson et al. (2004)" startWordPosition="885" endWordPosition="888">acting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ emnlp13 cue word and name the first (or several) associated words that come to mind (e.g., Nelson et al. (2004)), and feature norms, where subjects are given a cue word and asked to describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word subs</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="4957" citStr="Oliva and Torralba, 2001" startWordPosition="746" endWordPosition="749"> two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts</context>
<context position="14685" citStr="Oliva and Torralba, 2001" startWordPosition="2317" endWordPosition="2321"> feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representations for all our images into 500 clusters with kmeans clustering, and represent each word as membership over the image clusters, forming the SURF Clusters modality. The number of clusters is chosen arbitrarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3http://simplecv.org We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall “gist” of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modali</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1143--1151</pages>
<contexts>
<context position="2324" citStr="Ordonez et al., 2011" startWordPosition="339" endWordPosition="342">for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in th</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Advances in Neural Information Processing Systems, pages 1143–1151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Devi Parikh</author>
<author>Kristen Grauman</author>
</authors>
<title>Relative attributes.</title>
<date>2011</date>
<booktitle>In International Conference on Computer Vision,</booktitle>
<pages>503--510</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5018" citStr="Parikh and Grauman, 2011" startWordPosition="756" endWordPosition="759"> modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within</context>
</contexts>
<marker>Parikh, Grauman, 2011</marker>
<rawString>Devi Parikh and Kristen Grauman. 2011. Relative attributes. In International Conference on Computer Vision, pages 503–510. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Friedemann Pulverm¨uller</author>
<author>Olaf Hauk</author>
<author>Vadim V Nikulin</author>
<author>Risto J Ilmoniemi</author>
</authors>
<title>Functional links between motor and language systems.</title>
<date>2005</date>
<journal>European Journal of Neuroscience,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>Pulverm¨uller, Hauk, Nikulin, Ilmoniemi, 2005</marker>
<rawString>Friedemann Pulverm¨uller, Olaf Hauk, Vadim V. Nikulin, and Risto J Ilmoniemi. 2005. Functional links between motor and language systems. European Journal of Neuroscience, 21(3):793–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>25--36</pages>
<contexts>
<context position="8345" citStr="Regneri et al., 2013" startWordPosition="1271" endWordPosition="1274">catenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new </context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. In Transactions of the Association for Computational Linguistics, volume 1, pages 25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Rohrbach</author>
<author>Michael Stark</author>
<author>Gy¨orgy Szarvas</author>
<author>Iryna Gurevych</author>
<author>Bernt Schiele</author>
</authors>
<title>What helps where–and why? Semantic relatedness for knowledge transfer.</title>
<date>2010</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>910--917</pages>
<contexts>
<context position="8492" citStr="Rohrbach et al. (2010)" startWordPosition="1296" endWordPosition="1299">f) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text mod</context>
</contexts>
<marker>Rohrbach, Stark, Szarvas, Gurevych, Schiele, 2010</marker>
<rawString>Marcus Rohrbach, Michael Stark, Gy¨orgy Szarvas, Iryna Gurevych, and Bernt Schiele. 2010. What helps where–and why? Semantic relatedness for knowledge transfer. In IEEE Conference on Computer Vision and Pattern Recognition, pages 910–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Michael Speriosu</author>
<author>Sarat Rallapalli</author>
<author>Benjamin Wing</author>
<author>Jason Baldridge</author>
</authors>
<title>Supervised text-based geolocation using language models on an adaptive grid.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1500--1510</pages>
<contexts>
<context position="2492" citStr="Roller et al., 2012" startWordPosition="365" endWordPosition="368"> semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that</context>
</contexts>
<marker>Roller, Speriosu, Rallapalli, Wing, Baldridge, 2012</marker>
<rawString>Stephen Roller, Michael Speriosu, Sarat Rallapalli, Benjamin Wing, and Jason Baldridge. 2012. Supervised text-based geolocation using language models on an adaptive grid. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1500–1510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Susanne Borgwaldt</author>
<author>Ronny Jauch</author>
</authors>
<title>Association norms of german noun compounds.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>632--639</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="9690" citStr="Walde et al. (2012)" startWordPosition="1499" endWordPosition="1502">lity For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al., 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al. (2012). In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types. Feature Norms (FN) is our new collection of feature norms for a group of 569 German nouns. We present subjects on Amazon Mechanical Turk with a cue noun and ask them to give</context>
</contexts>
<marker>Walde, Borgwaldt, Jauch, 2012</marker>
<rawString>Sabine Schulte im Walde, Susanne Borgwaldt, and Ronny Jauch. 2012. Association norms of german noun compounds. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 632–639, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
</authors>
<title>Web-scale k-means clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>1177--1178</pages>
<contexts>
<context position="13658" citStr="Sculley, 2010" startWordPosition="2152" endWordPosition="2153">fter the collection of all the images, we extracted simple, low-level computer vision features to use as modalities in our experiments. First, we compute a simple Bag of Visual Words (BoVW) model for our images using SURF keypoints (Bay et al., 2008). SURF is a method for selecting points-of-interest within an image. It is faster and more forgiving than the commonly known SIFT algorithm. We compute SURF keypoints for every image in our data set using SimpleCV3 and randomly sample 1% of the keypoints. The keypoints are clustered into 5,000 visual codewords (centroids) using k-means clustering (Sculley, 2010), and images are then quantized over the 5,000 codewords. All images for a given word are summed together to provide an average representation for the word. We refer to this representation as the SURF modality. While this is a standard, basic BoVW model, each individual codeword on its own may not provide a large degree of semantic information; typically a BoVW representation acts predominantly as a feature space for a classifier, and objects can only be recognize using collections of codewords. To test that similar concepts should share similar visual codewords, we cluster the BoVW representa</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>D. Sculley. 2010. Web-scale k-means clustering. In Proceedings of the 19th International Conference on World Wide Web, pages 1177–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2750" citStr="Silberer and Lapata, 2012" startWordPosition="402" endWordPosition="405">Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional mo</context>
<context position="6861" citStr="Silberer and Lapata (2012)" startWordPosition="1033" endWordPosition="1036">tion of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over the past decade, other research has begun directly using visual information in place of feature norms. The first work to do this with topic models is Feng and Lapata (2010b). They use a Bag of Visual Words (BoVW) model (Lowe, 2004) to create a bimodal vocabulary describing documents. The topic model</context>
<context position="15803" citStr="Silberer and Lapata, 2012" startWordPosition="2497" endWordPosition="2500">ors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al., 2003), or LDA, is an unsup</context>
<context position="28235" citStr="Silberer and Lapata, 2012" startWordPosition="4545" endWordPosition="4548">e trials of each modality for 2000 iterations for K = 150, 100, 150, 200, 2501 (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. 6 Results 6.1 Predicting Compositionality Ratings Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al., 2009; Silberer and Lapata, 2012). We also see that the SURF visual words are able to provide notable, albeit not significant, improvements over the text-only modality. This confirms that the low-level BoVW features do carry semantic information, and are useful to consider individually. The GIST vectors, on the other hand, perform almost exactly the same as the text-only model. These features, which are usually more useful for comparing overall image likeness than object likeness, do not individually contain semantic information useful for compositionality prediction. The performance of the visual modalities reverses when we </context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2839" citStr="Silberer et al., 2013" startWordPosition="418" endWordPosition="421">ave tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by 1146 Proceedings of</context>
<context position="8020" citStr="Silberer et al. (2013)" startWordPosition="1219" endWordPosition="1222">eate a bimodal vocabulary describing documents. The topic model using the bimodal vocabulary outperforms a purely textual based model in word association and word similarity prediction. Bruni et al. (2012a) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classif</context>
<context position="15948" citStr="Silberer et al., 2013" startWordPosition="2523" endWordPosition="2526">e clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully used to infer unsupervised joint topic distributions over words and feature norms together (Andrews et al., 2009; Silberer and Lapata, 2012). It has also been shown to be useful in joint inference of text with visual attributes obtained using visual classifiers (Silberer et al., 2013). These multimodal LDA models (hereafter, mLDA) have been shown to be qualitatively sensible and highly predictive of several psycholinguistic tasks (Andrews et al., 2009). However, prior work using mLDA is limited to two modalities at a time. In this section, we describe bimodal mLDA and define two methods for extending it to three or more modalities. 4.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al., 2003), or LDA, is an unsupervised Bayesian probabilistic model of text documents. It assumes that all documents are probabilistically generated from a shared set of K comm</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Hamsa Sridhar</author>
<author>Osbert Bastani</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>International Conference on Learning Representations.</booktitle>
<contexts>
<context position="8517" citStr="Socher et al. (2013)" startWordPosition="1301" endWordPosition="1304">times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature 1147 norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new overlapping data sets: a collection of feature norms and a collection of images for a number of German nouns. 3.1 Textual Modality For our Text modality, we use deWaC, a la</context>
</contexts>
<marker>Socher, Ganjoo, Sridhar, Bastani, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, and Andrew Y. Ng. 2013. Zero-shot learning through cross-modal transfer. International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
</authors>
<title>Combining feature norms and text data with topic models.</title>
<date>2010</date>
<journal>Acta Psychologica,</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="2679" citStr="Steyvers, 2010" startWordPosition="392" endWordPosition="393">tions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational </context>
<context position="6563" citStr="Steyvers (2010)" startWordPosition="992" endWordPosition="993">o describe typical properties of the cue concept (e.g., McRae et al. (2005)). Griffiths et al. (2007) helped pave the path for cognitive-linguistic multimodal research, showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis (Deerwester et al., 1990) in the prediction of association norms. Andrews et al. (2009) furthered this work by showing that a bimodal topic model, consisting of both text and feature norms, outperformed models using only one modality on the prediction of association norms, word substitution errors, and semantic interference tasks. In a similar vein, Steyvers (2010) showed that a different feature-topic model improved predictions on a fill-in-the-blank task. Johns and Jones (2012) take an entirely different approach by showing that one can successfully infer held out feature norms from weighted mixtures based on textual similarity. Silberer and Lapata (2012) introduce a new method of multimodal integration based on Canonical Correlation Analysis, and performs a systematic comparison between their CCA-based model and others on association norm prediction, held out feature prediction, and word similarity. As computer vision techniques have improved over th</context>
</contexts>
<marker>Steyvers, 2010</marker>
<rawString>Mark Steyvers. 2010. Combining feature norms and text data with topic models. Acta Psychologica, 133(3):234–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis Gopal Banerjee</author>
<author>Seth J Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>Proceedings of the 25th AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2177" citStr="Tellex et al., 2011" startWordPosition="315" endWordPosition="318"> meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Br</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. Proceedings of the 25th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Tettamanti</author>
<author>Giovanni Buccino</author>
<author>Maria Cristina Saccuman</author>
<author>Vittorio Gallese</author>
<author>Massimo Danna</author>
<author>Paola Scifo</author>
<author>Ferruccio Fazio</author>
<author>Giacomo Rizzolatti</author>
<author>Stefano F Cappa</author>
<author>Daniela Perani</author>
</authors>
<title>Listening to actionrelated sentences activates fronto-parietal motor circuits.</title>
<date>2005</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>281</pages>
<contexts>
<context position="5169" citStr="Tettamanti et al., 2005" startWordPosition="780" endWordPosition="783">odalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research.1 2 Related Work The language grounding problem has received significant attention in recent years, owed in part to the wide availability of data sets (e.g. Flickr, Von Ahn (2006)), computing power, improved computer vision models (Oliva and Torralba, 2001; Lowe, 2004; Farhadi et al., 2009; Parikh and Grauman, 2011) and neurological evidence of ties between the language, perceptual and motor systems in the brain (Pulverm¨uller et al., 2005; Tettamanti et al., 2005; Aziz-Zadeh et al., 2006). Many approaches to multimodal research have succeeded by abstracting away raw perceptual information and using high-level representations instead. Some works abstract perception via the usage of symbolic logic representations (Chen et al., 2010; Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), while others choose to employ concepts elicited from psycholinguistic and cognition studies. Within the latter category, the two most common representations have been association norms, where subjects are given a 1http://stephenroller.com/research/ e</context>
</contexts>
<marker>Tettamanti, Buccino, Saccuman, Gallese, Danna, Scifo, Fazio, Rizzolatti, Cappa, Perani, 2005</marker>
<rawString>Marco Tettamanti, Giovanni Buccino, Maria Cristina Saccuman, Vittorio Gallese, Massimo Danna, Paola Scifo, Ferruccio Fazio, Giacomo Rizzolatti, Stefano F. Cappa, and Daniela Perani. 2005. Listening to actionrelated sentences activates fronto-parietal motor circuits. Journal of Cognitive Neuroscience, 17(2):273– 281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<marker>Von Ahn, 2006</marker>
<rawString>Luis Von Ahn. 2006. Games with a purpose. Computer, 39(6):92–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia von der Heide</author>
<author>Susanne Borgwaldt</author>
</authors>
<title>Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium,</booktitle>
<pages>51--74</pages>
<marker>von der Heide, Borgwaldt, 2009</marker>
<rawString>Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie. In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51– 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wing</author>
<author>Jason Baldridge</author>
</authors>
<title>Simple supervised document geolocation with geodesic grids.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>11</volume>
<pages>955--964</pages>
<contexts>
<context position="2470" citStr="Wing and Baldridge, 2011" startWordPosition="361" endWordPosition="364">hes. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distribu</context>
</contexts>
<marker>Wing, Baldridge, 2011</marker>
<rawString>Benjamin Wing and Jason Baldridge. 2011. Simple supervised document geolocation with geodesic grids. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 11, pages 955–964.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.994869">
Exploring the utility of joint morphological and syntactic learning
from child-directed speech
</title>
<author confidence="0.797058">
Stella Frank
</author>
<email confidence="0.829467">
sfrank@inf.ed.ac.uk
</email>
<author confidence="0.522706">
Frank Keller
</author>
<email confidence="0.783873">
keller@inf.ed.ac.uk
</email>
<author confidence="0.49671">
Sharon Goldwater
</author>
<email confidence="0.767838">
sgwater@inf.ed.ac.uk
</email>
<affiliation confidence="0.978138">
ILCC, School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.842229">
Edinburgh, EH8 9AB, UK
</address>
<sectionHeader confidence="0.948172" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818772727273">
Children learn various levels of linguistic
structure concurrently, yet most existing mod-
els of language acquisition deal with only
a single level of structure, implicitly assum-
ing a sequential learning process. Developing
models that learn multiple levels simultane-
ously can provide important insights into how
these levels might interact synergistically dur-
ing learning. Here, we present a model that
jointly induces syntactic categories and mor-
phological segmentations by combining two
well-known models for the individual tasks.
We test on child-directed utterances in English
and Spanish and compare to single-task base-
lines. In the morphologically poorer language
(English), the model improves morphological
segmentation, while in the morphologically
richer language (Spanish), it leads to better
syntactic categorization. These results provide
further evidence that joint learning is useful,
but also suggest that the benefits may be dif-
ferent for typologically different languages.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999299375">
Models of language acquisition seek to infer lin-
guistic structure from data with minimal amounts of
prior knowledge, in order to discover which char-
acteristics of the input data are useful for learn-
ing, and thus potentially utilised by human learners.
Most previous work has focused on learning individ-
ual aspects of linguistic structure. However, children
clearly learn multiple aspects in parallel, rather than
sequentially, implying that models of language ac-
quisition should also incorporate joint learning. Joint
models investigate the interaction between different
levels of linguistic structure during learning. These
interactions are often (but not necessarily) synergis-
tic, enabling better, more robust, learning by making
use of cues from multiple sources. Recent models
using joint learning to model language acquisition
have spanned various domains including phonology,
word segmentation, syntax and semantics (Feldman
et al., 2009; Elsner et al., 2012; Doyle and Levy,
2013; Johnson, 2008; Kwiatkowski et al., 2012).
In this paper we examine the joint learning of
syntactic categories and morphology, which are ac-
quired by children at roughly the same age (Clark,
2003b), implying possible interactions in the learn-
ing process. Both morphology and word order de-
pend on categorising words based on their morpho-
syntactic function. However, previous models of
syntactic category learning have relied principally
on surrounding context, i.e., word order constraints,
whereas models of morphology use word-internal
cues. Our joint model integrates both sources of
information, allowing the model to flexibly weigh
them according to their utility.
Languages differ in the richness of their mor-
phology and strictness of word order. These char-
acteristics appear to be (anti)correlated, with rich
morphology co-occurring with free word order and
vice versa (Blake, 2001; McFadden, 2003). The
timecourse of acquisition is also influenced by lan-
guage typology: learners of morphologically rich
languages become productive in morphology ear-
lier (Xanthos et al., 2011), suggesting that richer
morphology may be more salient for learners than
impoverished morphology. Sentence comprehension
in children also shows cross-linguistic differences
in the cues used to make sense of non-canonical
sentence structure: learners of a morphologically
rich language (Turkish) disregard word order in
</bodyText>
<page confidence="0.978194">
30
</page>
<note confidence="0.732947">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999978475609757">
favour of morphology, whereas learners of En-
glish favour word order (Slobin, 1982; MacWhin-
ney et al., 1984). These interactions between mor-
phology and word order suggest that a joint model
will be better able to support the differences in cue
strength (rich morphology versus strict word order),
and thus be more language-general, than single-task
models.
Both syntactic category and morphology induc-
tion have been the focus of much recent work. (See
Hammarstr¨om and Borin (2011) for an overview
of unsupervised morphology learning, likewise
Christodoulopoulos et al. (2010) for a comparison
of part of speech/syntactic category induction sys-
tems.) However, given the tightly coupled nature of
these two tasks, there has been surprisingly little
work in joint learning of morphology and syntac-
tic categories. Systems for inducing syntactic cat-
egories often make use of morpheme-like features,
such as word-final characters (Smith and Eisner,
2005; Haghighi and Klein, 2006; Berg-Kirkpatrick
et al., 2010; Lee et al., 2010), or model words
at the character-level (Clark, 2003a; Blunsom and
Cohn, 2011), but do not include morphemes ex-
plicitly. Other systems (Dasgupta and Ng, 2007;
Christodoulopoulos et al., 2011) use morphologi-
cal segmentations learned by a separate morphology
model as features in a pipeline approach.
Models of morphology induction generally oper-
ate over a lexicon, i.e. a list of word types, rather
than token corpora (Goldsmith, 2006; Creutz and
Lagus, 2007; Kurimo et al., 2010). These models
find morphological categories on the basis of word-
internal features, without taking syntactic context
into account (which is of course not available in a
lexicon).
Lee et al. (2011) and Sirts and Alum¨ae (2012)
present models that infer morphological segmenta-
tions and syntactic categories jointly, although Lee
et al. (2011) do not evaluate the inferred syntactic
categories. Both make use of a word-type constraint
which limits each word form to a single analysis
(i.e., all instances of ducks are assigned to a single
category and will have the same morpheme analy-
sis, ignoring the gold standard distinction between a
plural noun and third person singular verb). This can
make inference more tractable, and often increases
performance, but does not respect the ambiguity in-
herent in natural language, both over syntactic cat-
egories and morphological analyses. The degree of
ambiguity is language dependent, so that even if a
type-constraint is perhaps relatively unproblematic
in English, it will pose problems in morphologically
richer languages. Furthermore, these two models
make use of an array of heuristics that may not allow
them to be easily generalisable across languages and
datasets (e.g., likelihood scaling (Sirts and Alum¨ae,
2012), sequential suffix matching (Lee et al., 2011)).
In this paper, we present a joint model composed
of two well-known individual models. This allows
us to cleanly investigate the effects of joint learning
and its potential benefits over the single task models.
The simplicity of our models also allows us to avoid
modelling and inference heuristics.
Previous models have used adult-directed written
texts, which differs significantly from the type of
language available to child learners. We test our joint
model on child-directed utterances in English (a
morphologically poor language) and Spanish (with
richer morphology)1. Our results indicate that our
joint model is able to flexibly accommodate lan-
guages with differing levels of morphological rich-
ness. The joint model matches the performance of
single task models on both tasks, demonstrating that
the additional complexity is not a problem (i.e., it
does not add noise). Moreover, the joint model im-
proves performance significantly on the task corre-
sponding to the language’s weaker cue, indicating a
transfer of information from the stronger cue. The
fact that the nature of this improvement varies by
language provides evidence that joint learning can
effectively accommodate typological diversity.
</bodyText>
<sectionHeader confidence="0.971828" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.987189">
The task is to assign word tokens to part of speech
categories and simultaneously segment the tokens
into morphemes. We assume a relatively simple yet
commonly used concatenative morphology which
models a word as a stem plus (possibly null) suffix2.
1There are languages with much richer morphology than
Spanish, but none with a child-directed corpus suitably anno-
tated for evaluation.
2Fullwood and O’Donnell (2013) recently presented a
model of non-concatenative morphology that could be inte-
grated into this model; however, it does not perform well on En-
glish (and presumably other mostly concatenative languages).
</bodyText>
<page confidence="0.999787">
31
</page>
<bodyText confidence="0.9996635">
Since this is an unsupervised model, the inferred cat-
egories and morphemes lack meaningful labels, but
ideally will correspond to gold standard categories
and morphemes.
</bodyText>
<subsectionHeader confidence="0.998728">
2.1 Word Order
</subsectionHeader>
<bodyText confidence="0.9998326">
We model a sequence of words as a Hidden Markov
Model (HMM) with a non-parametric emission dis-
tribution. As usual, the latent states of the HMM rep-
resent syntactic categories. The tag sequence is gen-
erated by a trigram Dirichlet-multinomial distribu-
tion, where transition parameters τ are drawn from
a symmetric Dirichlet distribution with the hyperpa-
rameter αt. Each tag ti in the sequence is then drawn
from the transition distribution conditioned on the
previous two tags:
</bodyText>
<equation confidence="0.9997925">
τ(t,t0) ∼ Dir(αt)
ti = t|ti−1 = t0,ti−2 = t00,τ ∼ Mult(τ(t0,t00))
</equation>
<bodyText confidence="0.969752">
generated from Dirichlet-multinomials conditioned
on the tag t:
</bodyText>
<equation confidence="0.991354666666667">
κ ∼ Dir(ακ)
t|κ ∼ Mult(κ)
σ ∼ Dir(αs)
s|t,σ ∼ Mult(σt)
φ ∼ Dir(αf)
f |t,φ ∼ Mult(φt)
</equation>
<bodyText confidence="0.999756">
The αs are hyperparameters governing the Dirich-
let distributions from which the multinomials κ,σ,φ
are drawn. In turn, t,s, and f are drawn from these
multinomials.
The probability of a word under this model is the
sum of the probabilities of all possible analyses l =
</bodyText>
<equation confidence="0.8896655">
(t,s, f):
P0(w) = ∑ P0(l) = ∑ P(s|t)P(f|t)P(t) (1)
l t,s, f s.t.
s⊕f=w
</equation>
<bodyText confidence="0.99956565">
This model is token-based, permitting different
tokens of the same word type to have different
syntactic categories. Most recent models have in-
cluded a constraint forcing all tokens of a given
type into the same category, which improves per-
formance but often complicates inference. The
Bayesian HMM’s performance is therefore not state-
of-the-art, but is comparable to other token-based
models (Christodoulopoulos et al., 2010) and the
model is easy to extend within the Bayesian frame-
work, allowing us to compare multiple versions.
This part of the model is parametric, operat-
ing over a fixed number of tags T, and is iden-
tical to the formulation of tag transitions in the
Bayesian HMM (Goldwater and Griffiths, 2007).
However, we replace the BHMM’s emission dis-
tribution with the morphologically-informed distri-
butions described below. As in the BHMM, the
emission distributions are conditioned on the tag,
i.e., each tag has its own morphology.
</bodyText>
<subsectionHeader confidence="0.994205">
2.2 Morphology
</subsectionHeader>
<bodyText confidence="0.999956038461539">
The morphology model introduced by Goldwater
et al. (2006) generates morphological analyses for a
set of tokens. These analyses consist of a tag plus a
stem and suffix pair, which are concatenated to form
the observed words. Both stem s and suffix f are
where s ⊕ f = w denotes that the concatenation of
stem and suffix results in the word w.
On its own, this distribution over morphologi-
cal analyses makes independence assumptions that
are too strong: most word tokens of a word type
have the same analysis, but P0 will re-generate
that analysis for every token. To resolve this prob-
lem, a Pitman-Yor process (PYP) is placed over the
generating distribution above. The Pitman-Yor pro-
cess has been found to be useful for representing
the power-law distributions common in natural lan-
guage (Teh, 2006; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011).
The distribution of draws from a Pitman-Yor pro-
cess (which, in our case, determines the distribu-
tion of word tokens with each morphological anal-
ysis) is commonly described using the metaphor of
a Chinese restaurant. A series of customers (tokens
z = z1 ...zN) enter a restaurant with an infinite num-
ber of initially empty tables. Upon entering, each
customer is seated at a table k with probability
</bodyText>
<equation confidence="0.9809978">
p(zi = k|z1 ...zi−1,a,b) = (2)
� nk−a
i−1+b if 1 ≤ k ≤ K
Ka+b if k = K +1
−1+b
</equation>
<page confidence="0.992095">
32
</page>
<figureCaption confidence="0.931861285714286">
Figure 1: Plate diagram depicting the morphology model
(adapted from Goldwater et al. (2006)). Hyperparameters
have been omitted for clarity. The left-hand plate depicts
the base distribution P0; note that the morphological anal-
yses lk are generated deterministically as (tk,sk, fk). The
observed words wi are also deterministic given zi = k and
lk, since wi = sk ® fk.
</figureCaption>
<bodyText confidence="0.99982995">
where nk is the number of customers already sitting
at table k, K is the total number of tables occupied by
the i−1 previous customers, and 0 &lt; a &lt; 1 and b &gt; 0
are hyperparameters of the process. The probability
of being seated at a table increases with the number
of customers already seated at that table, creating a
‘rich-get-richer’ power-law distribution of tokens to
tables; a and b control the amount of reuse of exist-
ing tables, with smaller values leading to more reuse.
Crucially, each table serves a dish generated by
the base distribution P0—i.e., the dish is a morpho-
logical analysis lk = (t,s, f)—and all the customers
seated at the same table share the same dish, which
is generated only once (at the point when that table
is first occupied). The model can thus reuse the anal-
ysis for a particular word and avoid regenerating the
same analysis multiple times. Note that multiple ta-
bles may have identical analyses, lk = lk,. Figure 1
illustrates how the full PYP morphology model gen-
erates the observed sequence of word tokens.
</bodyText>
<subsectionHeader confidence="0.979134">
2.3 Combined Model
</subsectionHeader>
<bodyText confidence="0.9949837">
The full model (Figure 2) combines the latent tag se-
quence with the morphology model. Tag tokens are
generated conditioned on local context, not the base
distribution, as in the morphology model. Instead of
a single PYP generating morphological analyses for
all tokens, as in the Goldwater et al. (2006) model,
we have a separate PYP for each tag type, i.e., each
tag has its own restaurant with its own customers
(the tokens labeled with that tag) and its own mor-
phological analyses. The distribution of customers
</bodyText>
<figureCaption confidence="0.988293142857143">
Figure 2: Plate diagram depicting the joint model. Hyper-
parameters have been omitted for clarity. The L-shaped
plate contains the tokens, while the square plates contain
the morphological analyses. The t are latent tags, zi is an
assignment to a morphological analysis lk = (sk, fk), and
wi is the observed word. T is the number of distinct tags,
and Kt the number of tables used by tag type t.
</figureCaption>
<bodyText confidence="0.996121181818182">
in each of the tag-specific restaurants is still deter-
mined by Equation 2, except that all of the counts
and indices are with respect to only the tokens and
tables assigned to that tag.
Each tag-specific PYP (restaurant) also has a sep-
arate base distribution, P(t)
0 , resulting in distinct dis-
tributions over stems and suffixes for each tag. The
analyses generated by the base distributions consist
of (stem, suffix) pairs; the tag is given by the identity
of the generating PYP.
</bodyText>
<equation confidence="0.9528378">
P(t) P(t) P(s|t)P(f|t)
0 (w) = Y, 0 (l = (s, f)) = Y,
l s, f s.t.
s®f=w
(3)
</equation>
<bodyText confidence="0.9943095">
The full joint posterior distribution of a sequence
of words, tags, and morpheme analyses is shown in
Figure 3. Note that all tag-specific morphology mod-
els share the same Pitman-Yor parameters a and b.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.9998565">
We use Gibbs sampling for inference over the three
sets of discrete variables: tags t, their assignments to
morphological analyses (tables) z, and the analyses
themselves l.
Each iteration of the sampler has two stages: First
the morphological analyses l are sampled, and then
each token samples a new tag and a new assignment
to an analysis/table. Because the table assignments
</bodyText>
<equation confidence="0.959167047619048">
fk lk
tk sk
K
wi
zi
N
ti−2 ti−1 ti zi
sk fk
lk
Kt
T
wi
N
33
P(t,l,z|αt,a,b,αs,αf) =P(t|αt)P(l|t,αs,αf)P(z|a,b) (4)
P(t|αt) = N P(ti|ti−1,ti−2,t1...i−1,αt) = T Γ(Tαt) T Γ(ntt0t00 +αt) (5)
∏ ∏ Γ(ntt0 + Tαt) ∏ Γ(αt)
i=2 t,t0=1 t00=1
T Kt Pt(lk = (s, f)|l1...k−1,αs,αf ) (6)
P(l|t,αs,αf) =∏ ∏
t=1 k=1
</equation>
<figure confidence="0.87308376">
Γ(Sαs)
Γ(mt + Sαs)
Γ(Fαf)
Γ(mt + Fαf )
Γ(mts + αs)
Γ(αs)
S
∏
s=1
=
T
∏
t=1
(7)
Γ(αf )
F
∏
f=1
Γ(mt f + αf)
P(z|a,b) = T Nt P(zi|t,z1...i−1,a,b) (8)
∏ ∏
t=1 i=1
= T Γ(1 +b) Kt (ka + b)Γ(nk − a) (9)
∏ Γ(nt + b) ∏ Γ(1−a)
t=1 k=1
</figure>
<figureCaption confidence="0.993043">
Figure 3: The posterior distribution of our joint model. Because the sequence of words w is deterministic given
analyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w,t,l,z|αt,a,b,αs,αf) is
equal to P(t,l,z|αt,a,b,αs,αf) when lzi = wi for all i, and 0 otherwise. We give equations for the non-zero case. ns
refer to token counts, ms to table counts. We add two dummy tokens at the start, end, and between sentences to pad
the context history.
</figureCaption>
<bodyText confidence="0.9874908">
are conditioned on tags (i.e., a token must be as- multinomial posteriors as follows: mf +αf
signed to a table in the correct PYP restaurant) re- \k \k
sampling the tag requires immediate resampling of p(lk = (s, f) |t, (10)
the table assignment as well. l\k) = ms+αs m\k +Fαf
m\k + Sαs
</bodyText>
<subsectionHeader confidence="0.97252">
3.1 Initialization
</subsectionHeader>
<bodyText confidence="0.999984">
The tags are initialized uniformly at random. For
each token, a segmentation point is chosen uni-
formly at random (we disallow segmentations with
a null stem). If this segmentation is new within the
PYP associated with that token’s tag, a new table is
created for the token in that PYP. If it matches an ex-
isting analysis, zi is sampled from the existing tables
k plus a possible new table k0.
</bodyText>
<subsectionHeader confidence="0.998936">
3.2 Morphological Analyses
</subsectionHeader>
<bodyText confidence="0.999782588235294">
Each lk represents the morphological analysis for the
set of tokens assigned to table k. Resampling the
segmentation point (stem and suffix identity) of the
analysis changes the segmentation of all of the word
tokens assigned to that analysis. Note that the tag is
not included in lk in the combined model, because
the tag identity is dependent on the local contexts of
all the tokens seated at the table.
Analyses are sampled from a product of Dirichlet-
where ms and mf are the number of analyses for
this tag that share a stem or suffix with lk, and m
is the total number of analyses for this tag. S and
F are the total number of stems and suffixes in the
model. l\k indicates that the current analysis lk has
been removed from the distribution and the appro-
priate counts, to create the correct conditioning dis-
tribution for the Gibbs sampler.
</bodyText>
<subsectionHeader confidence="0.992925">
3.3 Tags
</subsectionHeader>
<bodyText confidence="0.9999235">
Tags are sampled from the product of posteri-
ors of the transition and emission distributions.
The transition distribution is a standard Dirichlet-
multinomial posterior. Calculating the emission dis-
tribution probability, i.e. the marginal probability of
the word given the tag, involves summing over the
probability of all the existing tables in the given PYP
that emit the correct word, plus the probability of
a new table being created, which also includes the
probability of a new analysis from P(0t) .
</bodyText>
<page confidence="0.991366">
34
</page>
<bodyText confidence="0.9285395">
More precisely, tags are sampled from the follow-
ing distribution:
</bodyText>
<equation confidence="0.95563575">
p(ti = t|wi = w,t\i,z\i,l,αt,a,b) (11)
∝ p(ti = t|ti−1,ti−2,t\i,αt) X p(w|t,z\i,l)
= p(ti = t|ti−1,ti−2,t\i,αt)
Language A
</equation>
<bodyText confidence="0.82055">
abdc fefh pomo rtut usst
cdcc bcba gghh npop npoo
cdca aaaa fefh hfeg pnon
</bodyText>
<subsectionHeader confidence="0.334853">
Language B
</subsectionHeader>
<bodyText confidence="0.614454">
noom.no usrs.st bbdb.ac cbab.cc cdaa.cc
rttt.uu cbab.aa mnom.oo ccda.bc onmm.om
rruu.ts npop.mm gehg.fh trrt.uu tssu.uu
</bodyText>
<equation confidence="0.9239335">
X( ∑ p(zi = k|t,w,z\i) + p(zi = knew|t,w,z\i))
k s.t. lk=w
Kta+b (t)
nt + b P0 (w))
</equation>
<tableCaption confidence="0.9733812">
Table 1: Example sentences in the synthetic languages.
Words in Category 1 are made of characters a-d, Cate-
gory 2 e-h, Category 3 m-p, Category 4 r-u. Suffixes in
Language B are separated with periods (.) for illustrative
purposes only.
</tableCaption>
<table confidence="0.953025">
= nti−2ti−1t + αt
nti−2ti−1 +Tαt
X( ∑ nk −a
k s.t. lk=w nt +b +
</table>
<bodyText confidence="0.999973571428571">
where lk = w matches tables compatible with w,
i.e., the concatenation of stem and suffix form the
word, slk ® flk = w. nk is the number of words as-
signed to the table k and Kt is the total number of
tables in the PYP for tag t. Note that all counts are
obtained after the removal of the current ti and zi,
i.e., from t\i and z\i.
</bodyText>
<subsectionHeader confidence="0.969143">
3.4 Table Assignments
</subsectionHeader>
<bodyText confidence="0.9960254">
Once a new tag has been sampled for a token, the ta-
ble assignment must be resampled conditioned on
the new tag. The assignment zi is drawn over all
compatible tables in the tag’s PYP (that is, where
lk = w), plus a possible new table:
</bodyText>
<equation confidence="0.998561333333333">
p(zi = k|ti = t,w,z\i,a,b) ∝ (12)
�nk−a if 1 G k G Kt
nt+b
Kta+b
nt+b P(t)
0 (w) if k = Kt + 1
</equation>
<bodyText confidence="0.9170448">
P(0t) is calculated by summing over the probability
of all possible segmentations for a new analysis for
word wi, using Equation 3. If a new table is drawn
(k &gt; Kt) then we also sample a new analysis for that
table from P(0t) .
</bodyText>
<sectionHeader confidence="0.991133" genericHeader="method">
4 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.993459119047619">
An important argument for joint learning is that it
affords increased flexibility and robustness across a
wider range of input data. A model that relies on
word order cannot learn syntactic categories from a
morphologically complex language with free word
order; likewise a model attempting to categorise
words using morphology alone will fail on a lan-
guage without morphology. An effective joint model
will be able to make use of the different cues in both
language types in a flexible way.
In order to test the proposed model, we run two
experiments on synthetic languages, which simulate
languages in which either word order or morphology
is the sole cue. Most natural languages fall between
these extremes, but these experiments show that our
model can capture the full spectrum.
Language A is a strict word order language lack-
ing morphology. It has a vocabulary of 200 word
types, split into four different categories. The 50
word types in each category are created by com-
bining four letters, with replacement, into four-letter
words, with a different set of letters used in each cat-
egory3. Words within a category may thus share be-
ginning or ending characters, which could be posited
as stems or suffixes by the model, but since only
50 of 256 possible strings are used, there will be
no strong evidence for consistent stem and suffixes
(i.e. stems appearing with multiple suffixes and vice
versa). Each sentence in Language A consists of five
words in one of twenty possible category sequences.
In these sequences, each category is either followed
by itself or the next category (i.e. [2,2,2,3,4] is valid
but [2,4,3,1,4] is not). Word order is thus strongly
constrained by category membership.
Language B has free word order, with category
membership signalled by suffixes. Words are cre-
3We achieved the same results with a language using the
same four characters in all categories, but using different char-
acters makes the categories human-readable. The model does
not have a orthographic/phonological component and so will
not recognise the within-category similarity, other than possi-
bly positing spurious stems or suffixes.
</bodyText>
<page confidence="0.993851">
35
</page>
<figure confidence="0.997556666666667">
Log Probability
0 200 400 600 800 1000
Iteration
</figure>
<figureCaption confidence="0.992449">
Figure 4: Log probability of the sampler state over 1000
iterations on Languages A and B.
</figureCaption>
<bodyText confidence="0.987842875">
ated by the concatenation of a stem and a suffix,
where the stems are the same as the words in lan-
guage A (50 stems in each of four categories). One
of six category-specific suffixes is appended to each
stem, resulting in 300 word types per category. Each
suffix is two letters long, created by combining three
possible letters (the same letters used to create the
stems), thus making mis-segmentation possible (for
instance, up to three of the suffixes could have the
same final letter). Sentences are again five words
long, but the sequence of categories is drawn at ran-
dom, resulting in uniformly random word order. See
Table 1 for example sentences in both languages.
We create a 5000 word corpus for each language,
and run our model on these corpora. Hyperparame-
ters are set to the same values in both languages4.
We run the sampler on each dataset for 1000 it-
erations with simulated annealing. In both cases,
the correct solution is found by iteration 500. Fig-
ure 4 shows that the morphology component con-
tinues to increase the log probability by increasing
the number of tokens seated at a table. Note that
the correct solution in Language A involves learn-
ing a very peaked transition distribution as well as an
even more extreme distribution over suffixes (where
only the null suffix has high probability), whereas
the same distributions in Language B are much flat-
ter. The fact that the same hyperparameter setting is
4The PYP parameters are set to a = 0.1,b = 1.0 and the
HMM transition parameter at = 1.0; the parameters in the base
distribution are as,af = 0.001,ak = 0.5.
able to correctly identify the two language extremes
indicates that the model is robust to hyperparameter
values.
These experiments demonstrate that our joint
model is able to learn correctly even when only ei-
ther morphology or word order is informative in a
language. We now turn to acquisition data from nat-
ural languages in which both morphology and word
order are useful cues but to varying degrees.
</bodyText>
<sectionHeader confidence="0.992014" genericHeader="method">
5 CDS Experiments
</sectionHeader>
<subsectionHeader confidence="0.966271">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999983387096774">
We use two corpora, Eve (Brown, 1973) and Or-
nat (Ornat, 1994), from the CHILDES database
(MacWhinney, 2000). These corpora consist of the
child-directed utterances heard by two children,
the former learning English and the latter Spanish.
These have been annotated for part of speech cate-
gories and morphemes.
The CHILDES corpora are tagged with a very rich
set of part of speech tags (74 tags), which we col-
lapse to a smaller set of tags5. The Eve corpus has
61224 tokens and is thus larger than the Spanish cor-
pus, which has 40497 tokens. However, the English
corpus has only 17 gold suffix types, while Spanish
has 83. The increased richness of Spanish morphol-
ogy also has an effect on the number of word types in
the corpus: the Spanish dataset has 3046 word types,
whereas the larger English dataset has only 1957.
Morphology is annotated using a stem-affix en-
coding which does not directly correspond to our
segmentation-based model. The word running is an-
notated as run-ING, jumping as jump-ING; the anno-
tation is thus agnostic about ortho-morphemic seg-
mentation (i.e., whether to segment as run.ning or
runn.ing), whereas the model is forced to choose
a segmentation point. Syncretic suffixes (sharing
an identical surface form) are disambiguated: sings
is annotated as sing-3S, plums as plum-PL. Con-
versely, the annotation scheme merges allomorphs
into a single suffix: infinitive verbs in Spanish,
for instance, are encoded as ending with -INF,
corresponding to -ar, -er, and -ir surface forms.
</bodyText>
<footnote confidence="0.85340325">
5These are 13 for English (ADJ, ADV, AUX, CONJ, DET,
INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10
for Spanish, since the gold standard does not distinguish AUX,
PART or INF.
</footnote>
<figure confidence="0.992855785714286">
-10000
-20000
-30000
-40000
-50000
-60000
70000
0
LangA Total
LangA Transitions
LangA Morphology
LangB Total
LangB Transitions
LangB Morphology
</figure>
<page confidence="0.992855">
36
</page>
<bodyText confidence="0.999953076923077">
We ignore irregular/non-affixing forms annotated
with &amp; (e.g. was, annotated as be&amp;PAST) and
use only hyphen-separated suffixes to evaluate.
Where multiple suffixes are concatenated together
(e.g., dog-DIM-PL) we treat this as a single suffix
(-DIM-PL) for evaluation purposes.
In Spanish, many words are annotated as having
a suffix of effectively zero length, e.g. the imper-
ative gusta is annotated as gusta-2S&amp;IMP. We re-
place these suffixes (where the stem is equal to the
word) with a null suffix, excluding them from eval-
uation, as they are impossible for a segmentation-
based model to find.
</bodyText>
<subsectionHeader confidence="0.992658">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999935181818182">
Tags are evaluated using VM (Rosenberg and
Hirschberg, 2007), as has become standard for this
task (Christodoulopoulos et al., 2010). VM is a mea-
sure of the normalised cross-entropy between gold
and proposed clusters; it ranges between 0 and 100,
with higher scores being better.
We also use VM to evaluate the morphological
segmentation: all tokens with a common suffix are
clustered together, and these clusters are compared
against the gold suffix clusters6. Using a clustering
metric avoids the need to evaluate against a gold seg-
mentation point (which the annotation lacks). Tag
membership is added to the non-null model suffixes,
so that a final -s suffix found in tag 2 is distinguished
from the same suffix found in tag 8 (creating suffixes
-s-T8 and -s-T2), analogous to the gold annotation
distinction between syncretic morphemes -PL and
-3S.
Note that ceiling performance of our model on
Suffix VM will be below 100, since our model can-
not cluster allomorphs, which are represented by a
single abstract morpheme in the gold standard.
</bodyText>
<subsectionHeader confidence="0.998606">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9969768">
We test the full model, MORTAG, against a number
of variations to investigate the advantages of jointly
modelling the two tasks.
Two variants remove the transition distributions,
and thus local syntactic context, from the model.
</bodyText>
<footnote confidence="0.992229666666667">
6We also evaluated stem morpheme clusters and found near-
ceiling performance due to the high number of null-suffix words
in both corpora.
</footnote>
<bodyText confidence="0.999775565217392">
MORTAGNOTRANS is the full model without tran-
sitions between tag tokens; morphology PYP draws
remain conditioned on token tags. We add a Dirich-
let prior over tags (at = 0.1) to encourage tag spar-
sity (analogous to the transition distribution in the
full model). MORCLUSTERS is the original model
of Goldwater et al. (2006), in which tags (called
clusters in the original) are drawn by P0.
MORTAGNOSEG is a variant in which the only
available suffix is the null suffix; thus segmentations
are trivial and only tags are inferred. This model
is approximately equivalent to a simple Bayesian
HMM but with the addition of PYPs within the
emission distribution. We also evaluate against tags
found by the BHMM, with a Dirichlet-multinomial
emission distribution and no morphology.
MORTAGTRUETAGS is the full model but with all
tags fixed to their gold values. This model gives us
oracle-type results for morphology. (Due to the an-
notation scheme used in CHILDES, oracle morpho-
logical segmentations are unavailable, so we were
unable to test a model with gold morphology and in-
ferred tags.)
</bodyText>
<subsectionHeader confidence="0.980588">
5.4 Experimental Procedure
</subsectionHeader>
<bodyText confidence="0.999992869565218">
Hyperparameter values for the Pitman-Yor process
were found using grid search on a development set
(Section 10 of Eve and Section 8 of Ornat; these sec-
tions are removed from the dataset we report results
on). We use the values which give the best Suffix
VM performance on the development data; however
we stress that the development results did not vary
greatly over a wide range of hyperparameter values,
and only deteriorated significantly at extreme values
of a.
There are a number of other hyperparameters in
the model which we set to fixed values. The transi-
tion hyperparameter at is set to 0.1 in all models.
We set the hyperparameters for the stem and suf-
fix distributions in the morphology base distribution
P0 to 0.001 for both as and af; ak over tags in the
MORCLUSTERS model is set to 0.5. The number of
possible stems and suffixes is given by the dataset: in
the Eve dataset there are 5339 candidate stems and
6617 candidate suffixes; in the Ornat dataset these
numbers are 8649 and 6598, respectively. The num-
ber of tags available to the model is set to the number
of gold tags in the data.
</bodyText>
<page confidence="0.997764">
37
</page>
<table confidence="0.947141285714286">
Tag VM Suffix VM
MORTAG 59.1(1.9) 41.9(10.0)
MORCLUSTERS 22.4(1.0)* 28.0(11.9)*
MORTAGNOTRANS 19.3(1.2)* 24.4(5.2)*
MORTAGNOSEG 59.4(1.7) −
BHMM 56.2(2.3)* −
MORTAGTRUETAGS − 42.5(5.2)
</table>
<tableCaption confidence="0.988154">
Table 2: English Eve corpus results. Standard deviations
are in parentheses; * denotes a significant difference from
</tableCaption>
<table confidence="0.936199375">
the MORTAG model.
Tag VM Suffix VM
MORTAG 43.4(2.6) 41.4(2.5)
MORCLUSTERS 20.3(2.5)* 46.5(3.2)
MORTAGNOTRANS 14.4(1.7)* 36.4(2.0)*
MORTAGNOSEG 39.6(3.7)* −
BHMM 36.4(0.7)* −
MORTAGTRUETAGS − 59.8(0.4)*
</table>
<tableCaption confidence="0.996474333333333">
Table 3: Spanish Ornat corpus results. Standard devia-
tions are in parentheses; * denotes a significant difference
from the MORTAG model.
</tableCaption>
<bodyText confidence="0.999683714285714">
Sampling is run for 5000 iterations with anneal-
ing. Inspection of the posterior log-likelihood indi-
cates that the models converge after about 1000 it-
erations. We run inference over all models ten times
and report the average performance. Significance is
reported using the non-parametric Wilcoxon rank-
sum test with a significance level of p &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.928829">
5.5 Results: English
</subsectionHeader>
<bodyText confidence="0.999955258064516">
Results on the English Eve corpus are shown in Ta-
ble 2. We use PYP parameters a = 0.3 and b = 10,
though we found similar performance over a wide
range of values of a and b. Our results show a clear
improvement in the morphological segmentations
found by the joint model and stable tagging perfor-
mance across all models with context information.
The syntactic clusters found by models using
only morphological patterns, MORTAGNOTRANS
and MORCLUSTERS, are clearly inferior and lead to
low Tag VM results. The models with local syntac-
tic context all perform approximately equally well
in terms of finding tags. We find no improvement on
tagging performance in English when adding mor-
phology, compared to the MORTAGNOSEG base-
line in which words are not segmented. However, we
do see a small but significant improvement over the
BHMM for both of these models, due to the replace-
ment of the multinomial emission distribution in the
BHMM with the PYP.
Morphological segmentations, as measured by
Suffix VM, clearly improve with the addition of lo-
cal contexts (and the ensuing better tags): the full
model outperforms the baselines without syntactic
contexts. On this dataset, the joint MORTAG model
even matches the performance of the model us-
ing oracle tags. The standard deviation over Suf-
fix VM scores is quite large for MORTAG and
MORCLUSTERS; this is due to frequent words hav-
ing two high probability segmentations (most no-
tably is, which in some runs was segmented as i.s).
</bodyText>
<subsectionHeader confidence="0.821619">
5.6 Results: Spanish
</subsectionHeader>
<bodyText confidence="0.9996656">
For the Spanish Ornat corpus, we found slightly dif-
ferent optimal PYP hyperparameters and set a = 0.1
and b = 0.1. Results are shown in Table 3.
The Spanish results pattern in the opposite way
as English. Here we see a statistically significant
improvement in tagging performance of the full
joint model over both models without morphology
(MORTAGNOSEG and BHMM). Models without
context information again find much worse tags,
mainly because (as in English) function words are
not identifiable by suffixes.
However, the full model does not find better mor-
phological segmentations than the MORCLUSTERS
model, despite better tags (the two models’ Suffix
VM scores are not statistically significantly differ-
ent). We also see that the difference between the seg-
mentations found by the model using gold tags and
estimated tags is quite large. This is due to the ora-
cle model finding the rarer suffixes which were not
distinguished by the models with noisier tags. This
demonstrates the importance of syntactic categorisa-
tion for the morpheme induction task, and suggests
that a more sophisticated tagging model (with better
performance) may yet improve morpheme segmen-
tation performance in Spanish.
</bodyText>
<page confidence="0.998593">
38
</page>
<sectionHeader confidence="0.998497" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995523809524">
We have presented a model of joint syntactic cate-
gory and morphology induction. Operating within a
generative Bayesian framework means that combin-
ing single-task components is straightforward and
well-founded. Our model is token-based, allowing
for syntactic and morphemic ambiguity.
To our knowledge, this is the first joint model to
be tested on child-directed speech data, which is less
complex than the newswire corpora used by previ-
ous joint models. Child-directed speech may be sim-
ple enough for joint learning not to be necessary: our
results indicate the contrary, namely that joint learn-
ing is indeed helpful when learning from realistic
acquisition data.
We tested this model on two languages with dif-
ferent morphological characteristics. On English, a
language with relatively little morphology, espe-
cially in child directed speech, we found that bet-
ter categorisation of words yielded much better mor-
phology in terms of suffixes learned. Conversely, in
Spanish we saw less difference on the morphology
task between models with categories inferred solely
from morphemic patterns and models that also used
local syntactic context for categorisation. However,
in Spanish we saw an improvement in the tagging
task when morphology information was included.
This suggests that English and Spanish make dif-
ferent word-order and morphology trade-offs. In En-
glish, local context provides at least as much in-
formation as morphology in terms of determining
the correct syntactic category, but knowing a good
estimate of the correct syntactic category is use-
ful for determining a word’s morphology. In Span-
ish, a word’s morphology can more easily be deter-
mined simply by looking at frequent suffixes within
a purely morphological system. On the other hand,
word order is freer, making local syntactic context
unreliable, so taking morphological information into
account can improve tagging. These differences be-
tween languages demonstrate the benefits of joint
learning, which enables the learner to more flexibly
utilise the information available in the input data.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999490108695652">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. Painless unsuper-
vised learning with features. In Proceedings of the
North American Association for Computational
Linguistics (NAACL), 2010.
Barry J. Blake. Case. Cambridge University Press,
2001.
Phil Blunsom and Trevor Cohn. A hierarchical
Pitman-Yor process HMM for unsupervised part
of speech induction. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2011.
Roger Brown. A first language: The early stages.
Harvard University Press, Cambridge, MA, 1973.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Christos Christodoulopoulos, Sharon Goldwater,
and Mark Steedman. A Bayesian mixture model
for part-of-speech induction using multiple fea-
tures. In Proceedings of the 16th Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), 2011.
Alexander Clark. Combining distributional and
morphological information for part of speech in-
duction. In Proceedings of the 10th annual Meet-
ing of the European Association for Computa-
tional Linguistics (EACL), 2003a.
Eve V. Clark. First Language Acquisition. Cam-
bridge University Press, 2003b.
Mathias Creutz and Krista Lagus. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Transactions on Speech and
Language Processing, 4(1):1–34, 2007.
Sajib Dasgupta and Vincent Ng. Unsupervised part-
of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the 12th Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), 2007.
Gabriel Doyle and Roger Levy. Combining multi-
ple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL-HLT 2013, pages
117–126, 2013.
</reference>
<page confidence="0.988263">
39
</page>
<reference confidence="0.99865802247191">
Micha Elsner, Sharon Goldwater, and Jacob Eisen-
stein. Bootstrapping a unified model of lexical
and phonetic acquisition. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2012.
Naomi Feldman, Thomas Griffiths, and James Mor-
gan. Learning phonetic categories by learning a
lexicon. In Proceedings of the 31st Annual Con-
ference of the Cognitive Science Society (CogSci),
2009.
Michelle A. Fullwood and Timothy J. O’Donnell.
Learning non-concatenative morphology. In Pro-
ceedings of the Workshop on Cognitive Modeling
and Computational Linguistics, 2013.
John Goldsmith. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(4):353–371, December 2006.
Sharon Goldwater and Thomas L. Griffiths. A
fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In Ad-
vances in Neural Information Processing Systems
18, 2006.
Aria Haghighi and Dan Klein. Prototype-driven
grammar induction. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2006.
Harald Hammarstr¨om and Lars Borin. Unsupervised
learning of morphology. Computational Linguis-
tics, 37(2):309–350, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2008.
Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.
Proceedings of the MorphoChallenge 2010 work-
shop. Technical Report TKK-ICS-R37, Aalto
University School of Science and Technology, Es-
poo, Finland, 2010.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. A probabilistic
model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL), 2012.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Simple type-level unsupervised POS tagging.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
2010.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning, 2011.
Brian MacWhinney. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ, 2000.
Brian MacWhinney, Elizabeth Bates, and Reinhold
Kliegl. Cue validity and sentence interpretation
in English, German, and Italian. Journal of Ver-
bal Learning and Verbal Behavior, 23:127–150,
1984.
Thomas McFadden. On morphological case and
word-order freedom. In Proceedings of the An-
nual Meeting of the Berkeley Linguistics Society,
volume 29, pages 295–306, 2003.
S. Lopez Ornat. La adquisicion de la lengua espag-
nola. Siglo XXI, Madrid, 1994.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
12th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Alum¨ae. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguistics
(NAACL), 2012.
Dan Slobin. Universal and particular in the acqui-
sition of language. In Eric Wanner and Lila R.
Gleitman, editors, Language acquisition: the state
</reference>
<page confidence="0.96795">
40
</page>
<reference confidence="0.997149545454545">
of the art, pages 128–170. Cambridge University
Press, 1982.
Noah A. Smith and Jason Eisner. Contrastive esti-
mation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL), 2005.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
2006.
Aris Xanthos, Sabine Laaha, Steven Gillis, Ursula
Stephany, Ayhan Aksu-Koc¸, Anastasia Christofi-
dou, Natalia Gagarina, Gordana Hrzica, F. Ni-
han Ketrez, Marianne Kilani-Schoch, Katharina
Korecky-Kr¨oll, Melita Kovaˇcevi´c, Klaus Laalo,
Marijan Palmovi´c, Barbara Pfeiler, Maria D.
Voeikova, and Wolfgang U. Dressler. On the role
of morphological richness in the early develop-
ment of noun and verb inflection. First Language,
31(4):461–479, 2011.
</reference>
<page confidence="0.999446">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719031">
<title confidence="0.9702745">Exploring the utility of joint morphological and syntactic from child-directed speech</title>
<author confidence="0.972889">Stella</author>
<email confidence="0.990149">sfrank@inf.ed.ac.uk</email>
<author confidence="0.980883">Frank</author>
<email confidence="0.983894">keller@inf.ed.ac.uk</email>
<author confidence="0.896409">Sharon</author>
<email confidence="0.993641">sgwater@inf.ed.ac.uk</email>
<affiliation confidence="0.9706225">ILCC, School of University of</affiliation>
<address confidence="0.991192">Edinburgh, EH8 9AB, UK</address>
<abstract confidence="0.998206347826087">Children learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the benefits may be different for typologically different languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cote</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL),</booktitle>
<contexts>
<context position="4879" citStr="Berg-Kirkpatrick et al., 2010" startWordPosition="696" endWordPosition="699">gory and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syn</context>
</contexts>
<marker>Berg-Kirkpatrick, Bouchard-Cote, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote, John DeNero, and Dan Klein. Painless unsupervised learning with features. In Proceedings of the North American Association for Computational Linguistics (NAACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Case</author>
</authors>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<marker>Case, 2001</marker>
<rawString>Barry J. Blake. Case. Cambridge University Press, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="4976" citStr="Blunsom and Cohn, 2011" startWordPosition="712" endWordPosition="715">1) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) an</context>
<context position="11560" citStr="Blunsom and Cohn, 2011" startWordPosition="1759" endWordPosition="1762">em s and suffix f are where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. To resolve this problem, a Pitman-Yor process (PYP) is placed over the generating distribution above. The Pitman-Yor process has been found to be useful for representing the power-law distributions common in natural language (Teh, 2006; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011). The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. A series of customers (tokens z = z1 ...zN) enter a restaurant with an infinite number of initially empty tables. Upon entering, each customer is seated at a table k with probability p(zi = k|z1 ...zi−1,a,b) = (2) � nk−a i−1+b if 1 ≤ k ≤ K Ka+b if k = K +1 −1+b 32 Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al. (2006)). Hyperparameters have b</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
</authors>
<title>A first language: The early stages.</title>
<date>1973</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="24676" citStr="Brown, 1973" startWordPosition="4074" endWordPosition="4075">eters are set to a = 0.1,b = 1.0 and the HMM transition parameter at = 1.0; the parameters in the base distribution are as,af = 0.001,ak = 0.5. able to correctly identify the two language extremes indicates that the model is robust to hyperparameter values. These experiments demonstrate that our joint model is able to learn correctly even when only either morphology or word order is informative in a language. We now turn to acquisition data from natural languages in which both morphology and word order are useful cues but to varying degrees. 5 CDS Experiments 5.1 Data We use two corpora, Eve (Brown, 1973) and Ornat (Ornat, 1994), from the CHILDES database (MacWhinney, 2000). These corpora consist of the child-directed utterances heard by two children, the former learning English and the latter Spanish. These have been annotated for part of speech categories and morphemes. The CHILDES corpora are tagged with a very rich set of part of speech tags (74 tags), which we collapse to a smaller set of tags5. The Eve corpus has 61224 tokens and is thus larger than the Spanish corpus, which has 40497 tokens. However, the English corpus has only 17 gold suffix types, while Spanish has 83. The increased r</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Roger Brown. A first language: The early stages. Harvard University Press, Cambridge, MA, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="4450" citStr="Christodoulopoulos et al. (2010)" startWordPosition="632" endWordPosition="635">r 2013. c�2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984). These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng,</context>
<context position="10169" citStr="Christodoulopoulos et al., 2010" startWordPosition="1526" endWordPosition="1529">drawn from these multinomials. The probability of a word under this model is the sum of the probabilities of all possible analyses l = (t,s, f): P0(w) = ∑ P0(l) = ∑ P(s|t)P(f|t)P(t) (1) l t,s, f s.t. s⊕f=w This model is token-based, permitting different tokens of the same word type to have different syntactic categories. Most recent models have included a constraint forcing all tokens of a given type into the same category, which improves performance but often complicates inference. The Bayesian HMM’s performance is therefore not stateof-the-art, but is comparable to other token-based models (Christodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions. This part of the model is parametric, operating over a fixed number of tags T, and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology. 2.2 Morphology The morphology model introduced by Goldwater et al. (2006) gener</context>
<context position="27214" citStr="Christodoulopoulos et al., 2010" startWordPosition="4483" endWordPosition="4486">parated suffixes to evaluate. Where multiple suffixes are concatenated together (e.g., dog-DIM-PL) we treat this as a single suffix (-DIM-PL) for evaluation purposes. In Spanish, many words are annotated as having a suffix of effectively zero length, e.g. the imperative gusta is annotated as gusta-2S&amp;IMP. We replace these suffixes (where the stem is equal to the word) with a null suffix, excluding them from evaluation, as they are impossible for a segmentationbased model to find. 5.2 Evaluation Tags are evaluated using VM (Rosenberg and Hirschberg, 2007), as has become standard for this task (Christodoulopoulos et al., 2010). VM is a measure of the normalised cross-entropy between gold and proposed clusters; it ranges between 0 and 100, with higher scores being better. We also use VM to evaluate the morphological segmentation: all tokens with a common suffix are clustered together, and these clusters are compared against the gold suffix clusters6. Using a clustering metric avoids the need to evaluate against a gold segmentation point (which the annotation lacks). Tag membership is added to the non-null model suffixes, so that a final -s suffix found in tag 2 is distinguished from the same suffix found in tag 8 (c</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>A Bayesian mixture model for part-of-speech induction using multiple features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 16th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="5089" citStr="Christodoulopoulos et al., 2011" startWordPosition="729" endWordPosition="732">comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2011</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. A Bayesian mixture model for part-of-speech induction using multiple features. In Proceedings of the 16th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th annual Meeting of the European Association for Computational Linguistics (EACL),</booktitle>
<contexts>
<context position="2469" citStr="Clark, 2003" startWordPosition="348" endWordPosition="349"> levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility. Languages differ in the richness of their morphology and strictness of word order. These characteristics appear to be (anti)co</context>
<context position="4950" citStr="Clark, 2003" startWordPosition="710" endWordPosition="711">and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexi</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. Combining distributional and morphological information for part of speech induction. In Proceedings of the 10th annual Meeting of the European Association for Computational Linguistics (EACL), 2003a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eve V Clark</author>
</authors>
<title>First Language Acquisition.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="2469" citStr="Clark, 2003" startWordPosition="348" endWordPosition="349"> levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility. Languages differ in the richness of their morphology and strictness of word order. These characteristics appear to be (anti)co</context>
<context position="4950" citStr="Clark, 2003" startWordPosition="710" endWordPosition="711">and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexi</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Eve V. Clark. First Language Acquisition. Cambridge University Press, 2003b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5355" citStr="Creutz and Lagus, 2007" startWordPosition="771" endWordPosition="774">make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al. (2011) do not evaluate the inferred syntactic categories. Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will have the same morpheme a</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1):1–34, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised partof-speech acquisition for resource-scarce languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="5055" citStr="Dasgupta and Ng, 2007" startWordPosition="725" endWordPosition="728">os et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentation</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. Unsupervised partof-speech acquisition for resource-scarce languages. In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
<author>Roger Levy</author>
</authors>
<title>Combining multiple information types in Bayesian word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT 2013,</booktitle>
<pages>117--126</pages>
<contexts>
<context position="2274" citStr="Doyle and Levy, 2013" startWordPosition="314" endWordPosition="317">earn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, a</context>
</contexts>
<marker>Doyle, Levy, 2013</marker>
<rawString>Gabriel Doyle and Roger Levy. Combining multiple information types in Bayesian word segmentation. In Proceedings of NAACL-HLT 2013, pages 117–126, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Sharon Goldwater</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Bootstrapping a unified model of lexical and phonetic acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="2252" citStr="Elsner et al., 2012" startWordPosition="310" endWordPosition="313">r, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sou</context>
</contexts>
<marker>Elsner, Goldwater, Eisenstein, 2012</marker>
<rawString>Micha Elsner, Sharon Goldwater, and Jacob Eisenstein. Bootstrapping a unified model of lexical and phonetic acquisition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Feldman</author>
<author>Thomas Griffiths</author>
<author>James Morgan</author>
</authors>
<title>Learning phonetic categories by learning a lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci),</booktitle>
<contexts>
<context position="2231" citStr="Feldman et al., 2009" startWordPosition="306" endWordPosition="309">stic structure. However, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint mode</context>
</contexts>
<marker>Feldman, Griffiths, Morgan, 2009</marker>
<rawString>Naomi Feldman, Thomas Griffiths, and James Morgan. Learning phonetic categories by learning a lexicon. In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle A Fullwood</author>
<author>Timothy J O’Donnell</author>
</authors>
<title>Learning non-concatenative morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<marker>Fullwood, O’Donnell, 2013</marker>
<rawString>Michelle A. Fullwood and Timothy J. O’Donnell. Learning non-concatenative morphology. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>An algorithm for the unsupervised learning of morphology.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="5331" citStr="Goldsmith, 2006" startWordPosition="769" endWordPosition="770">categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al. (2011) do not evaluate the inferred syntactic categories. Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will </context>
</contexts>
<marker>Goldsmith, 2006</marker>
<rawString>John Goldsmith. An algorithm for the unsupervised learning of morphology. Natural Language Engineering, 12(4):353–371, December 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="10460" citStr="Goldwater and Griffiths, 2007" startWordPosition="1578" endWordPosition="1581">fferent syntactic categories. Most recent models have included a constraint forcing all tokens of a given type into the same category, which improves performance but often complicates inference. The Bayesian HMM’s performance is therefore not stateof-the-art, but is comparable to other token-based models (Christodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions. This part of the model is parametric, operating over a fixed number of tags T, and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology. 2.2 Morphology The morphology model introduced by Goldwater et al. (2006) generates morphological analyses for a set of tokens. These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words. Both stem s and suffix f are where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<contexts>
<context position="10763" citStr="Goldwater et al. (2006)" startWordPosition="1623" endWordPosition="1626">istodoulopoulos et al., 2010) and the model is easy to extend within the Bayesian framework, allowing us to compare multiple versions. This part of the model is parametric, operating over a fixed number of tags T, and is identical to the formulation of tag transitions in the Bayesian HMM (Goldwater and Griffiths, 2007). However, we replace the BHMM’s emission distribution with the morphologically-informed distributions described below. As in the BHMM, the emission distributions are conditioned on the tag, i.e., each tag has its own morphology. 2.2 Morphology The morphology model introduced by Goldwater et al. (2006) generates morphological analyses for a set of tokens. These analyses consist of a tag plus a stem and suffix pair, which are concatenated to form the observed words. Both stem s and suffix f are where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. To resolve this problem, a Pitman-Yor process (PYP) is placed over the generating distribution</context>
<context position="12135" citStr="Goldwater et al. (2006)" startWordPosition="1864" endWordPosition="1867">ter and Griffiths, 2007; Blunsom and Cohn, 2011). The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. A series of customers (tokens z = z1 ...zN) enter a restaurant with an infinite number of initially empty tables. Upon entering, each customer is seated at a table k with probability p(zi = k|z1 ...zi−1,a,b) = (2) � nk−a i−1+b if 1 ≤ k ≤ K Ka+b if k = K +1 −1+b 32 Figure 1: Plate diagram depicting the morphology model (adapted from Goldwater et al. (2006)). Hyperparameters have been omitted for clarity. The left-hand plate depicts the base distribution P0; note that the morphological analyses lk are generated deterministically as (tk,sk, fk). The observed words wi are also deterministic given zi = k and lk, since wi = sk ® fk. where nk is the number of customers already sitting at table k, K is the total number of tables occupied by the i−1 previous customers, and 0 &lt; a &lt; 1 and b &gt; 0 are hyperparameters of the process. The probability of being seated at a table increases with the number of customers already seated at that table, creating a ‘ri</context>
<context position="13778" citStr="Goldwater et al. (2006)" startWordPosition="2148" endWordPosition="2151">irst occupied). The model can thus reuse the analysis for a particular word and avoid regenerating the same analysis multiple times. Note that multiple tables may have identical analyses, lk = lk,. Figure 1 illustrates how the full PYP morphology model generates the observed sequence of word tokens. 2.3 Combined Model The full model (Figure 2) combines the latent tag sequence with the morphology model. Tag tokens are generated conditioned on local context, not the base distribution, as in the morphology model. Instead of a single PYP generating morphological analyses for all tokens, as in the Goldwater et al. (2006) model, we have a separate PYP for each tag type, i.e., each tag has its own restaurant with its own customers (the tokens labeled with that tag) and its own morphological analyses. The distribution of customers Figure 2: Plate diagram depicting the joint model. Hyperparameters have been omitted for clarity. The L-shaped plate contains the tokens, while the square plates contain the morphological analyses. The t are latent tags, zi is an assignment to a morphological analysis lk = (sk, fk), and wi is the observed word. T is the number of distinct tags, and Kt the number of tables used by tag t</context>
<context position="28828" citStr="Goldwater et al. (2006)" startWordPosition="4747" endWordPosition="4750">o investigate the advantages of jointly modelling the two tasks. Two variants remove the transition distributions, and thus local syntactic context, from the model. 6We also evaluated stem morpheme clusters and found nearceiling performance due to the high number of null-suffix words in both corpora. MORTAGNOTRANS is the full model without transitions between tag tokens; morphology PYP draws remain conditioned on token tags. We add a Dirichlet prior over tags (at = 0.1) to encourage tag sparsity (analogous to the transition distribution in the full model). MORCLUSTERS is the original model of Goldwater et al. (2006), in which tags (called clusters in the original) are drawn by P0. MORTAGNOSEG is a variant in which the only available suffix is the null suffix; thus segmentations are trivial and only tags are inferred. This model is approximately equivalent to a simple Bayesian HMM but with the addition of PYPs within the emission distribution. We also evaluate against tags found by the BHMM, with a Dirichlet-multinomial emission distribution and no morphology. MORTAGTRUETAGS is the full model but with all tags fixed to their gold values. This model gives us oracle-type results for morphology. (Due to the </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems 18, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="4848" citStr="Haghighi and Klein, 2006" startWordPosition="692" endWordPosition="695">odels. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordintern</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. Prototype-driven grammar induction. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
<author>Lars Borin</author>
</authors>
<title>Unsupervised learning of morphology.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<marker>Hammarstr¨om, Borin, 2011</marker>
<rawString>Harald Hammarstr¨om and Lars Borin. Unsupervised learning of morphology. Computational Linguistics, 37(2):309–350, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="2289" citStr="Johnson, 2008" startWordPosition="318" endWordPosition="319">in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the mod</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
</authors>
<title>workshop.</title>
<date>2010</date>
<booktitle>Proceedings of the MorphoChallenge</booktitle>
<tech>Technical Report TKK-ICS-R37,</tech>
<institution>Aalto University School of Science and Technology,</institution>
<location>Espoo, Finland,</location>
<contexts>
<context position="5377" citStr="Kurimo et al., 2010" startWordPosition="775" endWordPosition="778">e features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al. (2011) do not evaluate the inferred syntactic categories. Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will have the same morpheme analysis, ignoring the </context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, 2010</marker>
<rawString>Mikko Kurimo, Sami Virpioja, and Ville T. Turunen. Proceedings of the MorphoChallenge 2010 workshop. Technical Report TKK-ICS-R37, Aalto University School of Science and Technology, Espoo, Finland, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Sharon Goldwater</author>
<author>Luke Zettelmoyer</author>
<author>Mark Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<contexts>
<context position="2316" citStr="Kwiatkowski et al., 2012" startWordPosition="320" endWordPosition="323">ther than sequentially, implying that models of language acquisition should also incorporate joint learning. Joint models investigate the interaction between different levels of linguistic structure during learning. These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources. Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics (Feldman et al., 2009; Elsner et al., 2012; Doyle and Levy, 2013; Johnson, 2008; Kwiatkowski et al., 2012). In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age (Clark, 2003b), implying possible interactions in the learning process. Both morphology and word order depend on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them a</context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettelmoyer, Steedman, 2012</marker>
<rawString>Tom Kwiatkowski, Sharon Goldwater, Luke Zettelmoyer, and Mark Steedman. A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised POS tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="4898" citStr="Lee et al., 2010" startWordPosition="700" endWordPosition="703">ave been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. Simple type-level unsupervised POS tagging. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Modeling syntactic context improves morphological segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<contexts>
<context position="5573" citStr="Lee et al. (2011)" startWordPosition="806" endWordPosition="809">nsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). Lee et al. (2011) and Sirts and Alum¨ae (2012) present models that infer morphological segmentations and syntactic categories jointly, although Lee et al. (2011) do not evaluate the inferred syntactic categories. Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will have the same morpheme analysis, ignoring the gold standard distinction between a plural noun and third person singular verb). This can make inference more tractable, and often increases performance, but does not respect the ambiguity inheren</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2011</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. Modeling syntactic context improves morphological segmentation. In Proceedings of Fifteenth Conference on Computational Natural Language Learning, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates,</title>
<date>2000</date>
<location>Mahwah, NJ,</location>
<contexts>
<context position="24746" citStr="MacWhinney, 2000" startWordPosition="4085" endWordPosition="4086"> at = 1.0; the parameters in the base distribution are as,af = 0.001,ak = 0.5. able to correctly identify the two language extremes indicates that the model is robust to hyperparameter values. These experiments demonstrate that our joint model is able to learn correctly even when only either morphology or word order is informative in a language. We now turn to acquisition data from natural languages in which both morphology and word order are useful cues but to varying degrees. 5 CDS Experiments 5.1 Data We use two corpora, Eve (Brown, 1973) and Ornat (Ornat, 1994), from the CHILDES database (MacWhinney, 2000). These corpora consist of the child-directed utterances heard by two children, the former learning English and the latter Spanish. These have been annotated for part of speech categories and morphemes. The CHILDES corpora are tagged with a very rich set of part of speech tags (74 tags), which we collapse to a smaller set of tags5. The Eve corpus has 61224 tokens and is thus larger than the Spanish corpus, which has 40497 tokens. However, the English corpus has only 17 gold suffix types, while Spanish has 83. The increased richness of Spanish morphology also has an effect on the number of word</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates, Mahwah, NJ, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Elizabeth Bates</author>
<author>Reinhold Kliegl</author>
</authors>
<title>Cue validity and sentence interpretation in English, German, and Italian.</title>
<date>1984</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>23</volume>
<contexts>
<context position="3982" citStr="MacWhinney et al., 1984" startWordPosition="561" endWordPosition="565">suggesting that richer morphology may be more salient for learners than impoverished morphology. Sentence comprehension in children also shows cross-linguistic differences in the cues used to make sense of non-canonical sentence structure: learners of a morphologically rich language (Turkish) disregard word order in 30 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984). These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two ta</context>
</contexts>
<marker>MacWhinney, Bates, Kliegl, 1984</marker>
<rawString>Brian MacWhinney, Elizabeth Bates, and Reinhold Kliegl. Cue validity and sentence interpretation in English, German, and Italian. Journal of Verbal Learning and Verbal Behavior, 23:127–150, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas McFadden</author>
</authors>
<title>On morphological case and word-order freedom.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Berkeley Linguistics Society,</booktitle>
<volume>29</volume>
<pages>295--306</pages>
<contexts>
<context position="3178" citStr="McFadden, 2003" startWordPosition="451" endWordPosition="452">end on categorising words based on their morphosyntactic function. However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues. Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility. Languages differ in the richness of their morphology and strictness of word order. These characteristics appear to be (anti)correlated, with rich morphology co-occurring with free word order and vice versa (Blake, 2001; McFadden, 2003). The timecourse of acquisition is also influenced by language typology: learners of morphologically rich languages become productive in morphology earlier (Xanthos et al., 2011), suggesting that richer morphology may be more salient for learners than impoverished morphology. Sentence comprehension in children also shows cross-linguistic differences in the cues used to make sense of non-canonical sentence structure: learners of a morphologically rich language (Turkish) disregard word order in 30 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41</context>
</contexts>
<marker>McFadden, 2003</marker>
<rawString>Thomas McFadden. On morphological case and word-order freedom. In Proceedings of the Annual Meeting of the Berkeley Linguistics Society, volume 29, pages 295–306, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lopez Ornat</author>
</authors>
<title>La adquisicion de la lengua espagnola. Siglo XXI,</title>
<date>1994</date>
<location>Madrid,</location>
<contexts>
<context position="24700" citStr="Ornat, 1994" startWordPosition="4079" endWordPosition="4080">,b = 1.0 and the HMM transition parameter at = 1.0; the parameters in the base distribution are as,af = 0.001,ak = 0.5. able to correctly identify the two language extremes indicates that the model is robust to hyperparameter values. These experiments demonstrate that our joint model is able to learn correctly even when only either morphology or word order is informative in a language. We now turn to acquisition data from natural languages in which both morphology and word order are useful cues but to varying degrees. 5 CDS Experiments 5.1 Data We use two corpora, Eve (Brown, 1973) and Ornat (Ornat, 1994), from the CHILDES database (MacWhinney, 2000). These corpora consist of the child-directed utterances heard by two children, the former learning English and the latter Spanish. These have been annotated for part of speech categories and morphemes. The CHILDES corpora are tagged with a very rich set of part of speech tags (74 tags), which we collapse to a smaller set of tags5. The Eve corpus has 61224 tokens and is thus larger than the Spanish corpus, which has 40497 tokens. However, the English corpus has only 17 gold suffix types, while Spanish has 83. The increased richness of Spanish morph</context>
</contexts>
<marker>Ornat, 1994</marker>
<rawString>S. Lopez Ornat. La adquisicion de la lengua espagnola. Siglo XXI, Madrid, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="27142" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="4472" endWordPosition="4475">nnotated with &amp; (e.g. was, annotated as be&amp;PAST) and use only hyphen-separated suffixes to evaluate. Where multiple suffixes are concatenated together (e.g., dog-DIM-PL) we treat this as a single suffix (-DIM-PL) for evaluation purposes. In Spanish, many words are annotated as having a suffix of effectively zero length, e.g. the imperative gusta is annotated as gusta-2S&amp;IMP. We replace these suffixes (where the stem is equal to the word) with a null suffix, excluding them from evaluation, as they are impossible for a segmentationbased model to find. 5.2 Evaluation Tags are evaluated using VM (Rosenberg and Hirschberg, 2007), as has become standard for this task (Christodoulopoulos et al., 2010). VM is a measure of the normalised cross-entropy between gold and proposed clusters; it ranges between 0 and 100, with higher scores being better. We also use VM to evaluate the morphological segmentation: all tokens with a common suffix are clustered together, and these clusters are compared against the gold suffix clusters6. Using a clustering metric avoids the need to evaluate against a gold segmentation point (which the annotation lacks). Tag membership is added to the non-null model suffixes, so that a final -s suffi</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A hierarchical Dirichlet process model for joint part-of-speech and morphology induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Kairit Sirts and Tanel Alum¨ae. A hierarchical Dirichlet process model for joint part-of-speech and morphology induction. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Slobin</author>
</authors>
<title>Universal and particular in the acquisition of language.</title>
<date>1982</date>
<pages>128--170</pages>
<editor>In Eric Wanner and Lila R. Gleitman, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="3956" citStr="Slobin, 1982" startWordPosition="559" endWordPosition="560">t al., 2011), suggesting that richer morphology may be more salient for learners than impoverished morphology. Sentence comprehension in children also shows cross-linguistic differences in the cues used to make sense of non-canonical sentence structure: learners of a morphologically rich language (Turkish) disregard word order in 30 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30–41, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics favour of morphology, whereas learners of English favour word order (Slobin, 1982; MacWhinney et al., 1984). These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coup</context>
</contexts>
<marker>Slobin, 1982</marker>
<rawString>Dan Slobin. Universal and particular in the acquisition of language. In Eric Wanner and Lila R. Gleitman, editors, Language acquisition: the state of the art, pages 128–170. Cambridge University Press, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="4822" citStr="Smith and Eisner, 2005" startWordPosition="688" endWordPosition="691">eral, than single-task models. Both syntactic category and morphology induction have been the focus of much recent work. (See Hammarstr¨om and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al. (2010) for a comparison of part of speech/syntactic category induction systems.) However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories. Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010), or model words at the character-level (Clark, 2003a; Blunsom and Cohn, 2011), but do not include morphemes explicitly. Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach. Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010). These models find morphological categories </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="11504" citStr="Teh, 2006" startWordPosition="1753" endWordPosition="1754">enated to form the observed words. Both stem s and suffix f are where s ⊕ f = w denotes that the concatenation of stem and suffix results in the word w. On its own, this distribution over morphological analyses makes independence assumptions that are too strong: most word tokens of a word type have the same analysis, but P0 will re-generate that analysis for every token. To resolve this problem, a Pitman-Yor process (PYP) is placed over the generating distribution above. The Pitman-Yor process has been found to be useful for representing the power-law distributions common in natural language (Teh, 2006; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011). The distribution of draws from a Pitman-Yor process (which, in our case, determines the distribution of word tokens with each morphological analysis) is commonly described using the metaphor of a Chinese restaurant. A series of customers (tokens z = z1 ...zN) enter a restaurant with an infinite number of initially empty tables. Upon entering, each customer is seated at a table k with probability p(zi = k|z1 ...zi−1,a,b) = (2) � nk−a i−1+b if 1 ≤ k ≤ K Ka+b if k = K +1 −1+b 32 Figure 1: Plate diagram depicting the morphology model (adapt</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aris Xanthos</author>
<author>Sabine Laaha</author>
<author>Steven Gillis</author>
<author>Ursula Stephany</author>
<author>Ayhan Aksu-Koc¸</author>
<author>Anastasia Christofidou</author>
<author>Natalia Gagarina</author>
<author>Gordana Hrzica</author>
<author>F Nihan Ketrez</author>
</authors>
<title>On the role of morphological richness in the early development of noun and verb inflection.</title>
<date>2011</date>
<journal>First Language,</journal>
<volume>31</volume>
<issue>4</issue>
<institution>Marianne Kilani-Schoch, Katharina Korecky-Kr¨oll, Melita Kovaˇcevi´c, Klaus Laalo, Marijan Palmovi´c, Barbara Pfeiler,</institution>
<location>Maria</location>
<marker>Xanthos, Laaha, Gillis, Stephany, Aksu-Koc¸, Christofidou, Gagarina, Hrzica, Ketrez, 2011</marker>
<rawString>Aris Xanthos, Sabine Laaha, Steven Gillis, Ursula Stephany, Ayhan Aksu-Koc¸, Anastasia Christofidou, Natalia Gagarina, Gordana Hrzica, F. Nihan Ketrez, Marianne Kilani-Schoch, Katharina Korecky-Kr¨oll, Melita Kovaˇcevi´c, Klaus Laalo, Marijan Palmovi´c, Barbara Pfeiler, Maria D. Voeikova, and Wolfgang U. Dressler. On the role of morphological richness in the early development of noun and verb inflection. First Language, 31(4):461–479, 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001585">
<title confidence="0.996263">
A multi-Teraflop Constituency Parser using GPUs
</title>
<author confidence="0.80624">
John Canny David Hall Dan Klein
</author>
<address confidence="0.7412935">
UC Berkeley
Berkeley, CA, 94720
</address>
<email confidence="0.998715">
canny@berkeley.edu, dlwh,klein@cs.berkeley.edu
</email>
<sectionHeader confidence="0.998502" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.96224725">
Constituency parsing with rich grammars re-
mains a computational challenge. Graph-
ics Processing Units (GPUs) have previously
been used to accelerate CKY chart evalua-
tion, but gains over CPU parsers were mod-
est. In this paper, we describe a collection of
new techniques that enable chart evaluation at
close to the GPU’s practical maximum speed
(a Teraflop), or around a half-trillion rule eval-
uations per second. Net parser performance
on a 4-GPU system is over 1 thousand length-
30 sentences/second (1 trillion rules/sec), and
400 general sentences/second for the Berkeley
Parser Grammar. The techniques we introduce
include grammar compilation, recursive sym-
bol blocking, and cache-sharing.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987853125">
Constituency parsing with high accuracy (e.g. latent
variable) grammars remains a computational chal-
lenge. The O(Gs3) complexity of full CKY pars-
ing for a grammar with G rules and sentence length
s, is daunting. Even with a host of pruning heuris-
tics, the high cost of constituency parsing limits its
uses. The most recent Berkeley latent variable gram-
mar for instance, has 1.7 million rules and requires
about a billion rule evaluations for inside scoring of
a single length-30 sentence. GPUs have previously
been used to accelerate CKY evaluation, but gains
over CPU parsers were modest. e.g. in Yi et al.
(2011) a GPU parser is described for the Berkeley
Parser grammar which achieves 5 sentences per sec-
ond on the first 1000 sentences of Penn Treebank
section 22 Marcus et al. (1993), which is compa-
rable with the best CPU parsers Petrov and Klein
(2007). Our parser achieves 120 sentences/second
per GPU for this sentence set, and over 250 sen-
tences/sec on length &lt; 30 sentences. These results
use a Berkeley Grammar approximately twice as big
as Yi et al. (2011), an apparent 50x improvement.
On a 4-GPU system, we achieve 1000 sentences/sec
for length &lt; 30 sentences. This is 2 orders of mag-
nitude faster than CPU implementations that rely
heavily on pruning, and 5 orders of magnitude faster
than full CKY evaluation on a CPU.
Key to these results is a collection of new tech-
niques that enable GPU parsing at close to the
GPU’s practical maximum speed (a Teraflop for re-
cent GPUs), or around a half-trillion rule evaluations
per second. The techniques are:
</bodyText>
<listItem confidence="0.999041153846154">
1. Grammar compilation, which allows register-
to-register code for application of grammar
rules. This gives an order of magnitude (10x)
speedup over alternative approaches that use
shared memory.
2. Symbol/rule blocking of the grammar to re-
spect register, constant and instruction cache
limits. This is precondition for 1 above, and
the details of the partitioning have a big (&gt; 4x)
effect on performance.
3. Sub-block partitioning to distribute rules across
the stream processors of the GPU and allow L2
cache acceleration. A factor of 2 improvement.
</listItem>
<bodyText confidence="0.9931755">
The code generated by our parser comes close to the
theoretical limits of the GPU. 80% of grammar rules
</bodyText>
<page confidence="0.946861">
1898
</page>
<note confidence="0.890028333333333">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898–1907,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Data from Anonymous (2012) and NVIDIA (2012).
</note>
<bodyText confidence="0.7642175">
are evaluated using a single-cycle register-to-register
instruction.
</bodyText>
<sectionHeader confidence="0.973385" genericHeader="method">
2 GPU Design Principles
</sectionHeader>
<bodyText confidence="0.999489837209303">
In this paper, we focus on the architecture of recent
NVIDIA® GPUs, though many of the principles we
describe here can be applied to other GPUs (e.g.
those made by AMD R�.) The current NVIDIA®
KeplerTM series GPU contains between 2 and 16
“stream processors” or SMX’s which share an L2
cache interfacing to the GPUs main memory Anony-
mous (2013). The SMXs in turn comprise 192
cores which share a memory which is partitioned
into “shared memory” and L1 cache. Shared mem-
ory supports relatively fast communication between
threads in an SMX. Communication between SMXs
has to pass through slower main memory.
The execution of instructions within SMXs is vir-
tualized and pipelined - i.e. it is not a simple task to
count processors, although there are nominally 192
in the KeplerTM series. Register storage is not at-
tached to cores, instead registers are associated in
blocks of 63 or 255 (depending on KeplerTM sub-
architecture) with running threads. Because of this,
it is usually easier for the programmer to think of
the SMXes as 1024 thread processors. These 1024
threads are grouped into 32 groups of 32 threads
called warps. Each warp of threads shares a program
counter and executes code in lock-step. However,
execution is not SIMD - all threads do not execute all
instructions. When the warp encounters a branching
instruction, all branches that are satisfied by some
thread will be executed in sequence. Each thread
only executes the instructions for its own branch,
and idles for the others. NVIDIA® calls this model
SIMT (Single Instruction, Multiple Threads). Ex-
ecution of diverging branches by a warp is called
warp divergence. While it simplifies programming,
warp divergence understandably hurts performance
and our first goal is to avoid it.
GPUs are generally optimized for single-
precision floating point arithmetic in support of
rendering and simulation. Table 1 shows instruction
throughput (number of instructions that are executed
per cycle on each SMX). The KeplerTM series has
two architectural sub-generations (3.0 and 3.5) with
significant differences in double-precision support.
</bodyText>
<table confidence="0.999410625">
Instruction type Architecture
3.0 3.5
Shared memory word access 32 32
FP arithmetic +,-,*,FMA 192 192
DP arithmetic +,-,*,FMA 8 64
Integer +,- 160 160
Integer *,FMA 32 32
float sin, exp, log,... 32 32
</table>
<tableCaption confidence="0.960116">
Table 1: Instructions per cycle per SMX in generation 3.0
and 3.5 KeplerTM devices
</tableCaption>
<bodyText confidence="0.991564157894737">
In the table, FP is floating point, DP is double
precision, and FMA is a single-cycle floating-piont
fused multiply-add used in most matrix and vector
operations (A &lt;-- A+B *C). Note next that floating
point (single precision) operations are extremely fast
and there is an FPU for each of the 192 processors.
Double precision floating point is 3x slower on high-
end 3.5 GPUS, and much slower (24x) on the com-
modity 3.0 machines. While integer addition is fast,
integer multiply is much slower. Perhaps most sur-
prising is the speed of single-precision transcenden-
tal function evaluation, log, exp, sin, cos, tan, etc.,
which are as fast as shared memory accesses or in-
teger multiplication, and which amount to a quarter-
trillion transcendental evaluations per second on a
GTX-680/K10.
PCFG grammar evaluation nominally requires
two multiplications and an addition per rule (section
4) which can be written:
</bodyText>
<equation confidence="0.99537">
Sij,m = E Sik,nS(k+1)j,pCmnp (1)
k=1...j; n,pEQ
</equation>
<bodyText confidence="0.9997235">
i.e. the CKY node scores are sums of products of
pairs of scores and a weight. This suggests that at
least in principle, it’s possible to achieve a trillion
rule evaluations per second on a 680 or K10 device,
using a * and an FMA operation for each rule. That
assumes we are doing register-to-register operations
however. If we worked through shared memory (first
line of the table), we would be limited to about 80
billion evaluations/sec, 20 times slower. The anal-
ysis underscores that high performance for parsing
on a GPU is really a challenge of data movement.
We next review the different storage types and their
</bodyText>
<page confidence="0.996521">
1899
</page>
<bodyText confidence="0.999765">
bandwidths, since prudent use of, and movement be-
tween storage types is the key to performance.
</bodyText>
<subsectionHeader confidence="0.999417">
2.1 Memory Types and Speeds
</subsectionHeader>
<bodyText confidence="0.999144435897436">
There are six types of storage on the GPU which
matter for us. For each type, we give the capac-
ity and aggregate bandwidth on a typical device (a
GTX-680 or K10 running at 1GHz).
Register files These are virtualized and associated
with threads rather than processors. 256kB per
SMX. Each thread in architecture 3.0 devices can
access 63 32-bit registers, or 255 registers for 3.5
devices. Aggregate bandwidth 40 TB/s.
Shared memory/L1 cache is shared by all threads
in an SMX. 64kB per SMX partitioned into shared
memory and cache functions. Aggregate bandwidth
about 1 TB/s.
Constant memory Each SMX has a 48kB read/only
cache separate from the L1 cache. It can store gram-
mar constants and has much higher bandwidth than
shared memory. Broadast bandwidth 13 TB/s.
Instruction cache is 8 KB per SMX. Aggregate
bandwidth 13 TB/s.
L2 cache is 0.5-1.5 MB, shared between all SMXs.
Aggregate bandwidth 500 GB/s.
Global memory is 2-6GB typically, and is shared
by all SMXs. GPUs use a particularly fast form of
SDRAM (compared to CPUs) but it is still much
slower than the other memory types above. Ag-
gregate bandwidth about 160 GB/s.
There is one more very important principle: Coa-
lesced main memory access. From the above it can
be seen that main memory access is much slower
than other memories and can easily bottleneck the
calculations. The figure above (160 GB/s) for main
memory access assumes such access is coalesced.
Each thread in the GPU has a thread and a block
number which determines where it runs on the hard-
ware. Consecutively-numbered threads should ac-
cess consecutive main memory locations for fast
memory access.
These parameters suggest a set of design princi-
ples for peak performance:
</bodyText>
<listItem confidence="0.945965769230769">
1. Maximize use of registers for symbol scores,
and minimize use of shared memory (in fact we
will not use it at all).
2. Maximize use of constant memory for rule
weights, and minimize use of shared memory.
3. Partition the rule set into blocks that respect the
limits on number of registers, constant memory
(needed for grammar rules probabilities) and
instruction cache limits.
4. Minimize main memory access and use L2
cache to speed it up.
Lets look in more detail at how to achieve this.
3 Anatomy of an Efficient GPU Parser
</listItem>
<bodyText confidence="0.99989221875">
High performance on the GPU requires us to mini-
mize code divergence. This suggests that we do not
use a lexicalized grammar or a grammar that is sen-
sitive to the position of a span within the sentence.
These kinds of grammars—while highly accurate—
have irregular memory access patterns that conflict
with SIMD execution. Instead, an unlexicalized ap-
proach like that of Johnson (2011) or Klein and
Manning (2003), or a latent variable approach like
that of Matsuzaki et al. (2005) or Petrov et al. (2006)
are more appropriate. We opt for the latter kind: la-
tent variable grammars are fairly small, and their ac-
curacies rival lexicalized approaches.
Our GPU-ized inside algorithm maintains two
data structures: parse charts that store scores for
each labeled span, as usual, and a “workspace” that
is used to actually perform the updates of the in-
side algorithm. Schematically, this memory lay-
out is represented in Figure 1. A queue is main-
tained CPU-side that enqueues work items of the
form (s, p,l, r), where s is a sentence, and p, l, and
r specify the index in the parse chart for parent, left
child, and right child, respectively. The outer loop
proceeds in increasing span length (or height of par-
ent node scores to be computed). Next the algorithm
iterates over the available sentences. Then it iterates
over the parent nodes at the current length in that
sentences, and finally over all split points for the cur-
rent parent node. In each case, work items are sent to
the queue with that span for all possible split points.
When the queue is full—or when there are no
more work items of that length—the queue is flushed
</bodyText>
<page confidence="0.975454">
1900
</page>
<figureCaption confidence="0.77681575">
Figure 1: The architecture of the system. Parse charts are
stored in triangular arrays laid out consecutively in mem-
ory. Scores for left and right children are transposed and
copied into the “workspace” array, and the inside updates
are calculated for the parent. Scores are then pushed back
to the appropriate cell in the parse charts, maxing them
with scores that are already there. Transposition ensures
that reads and writes are coalesced.
</figureCaption>
<bodyText confidence="0.999947">
to the GPU, which executes three steps. First, the
scores for each left and right child are copied into
the corresponding column in the workspace. Then
inside updates are applied in parallel for all cells to
get parent scores. Then parents are entered back to
their appropriate cells in the parse charts. This is
typically a many-to one atomic reduction (either a
sum for probability scores, or a max for max-sum
log probability scores). This process repeats until
all span lengths have been processed.
</bodyText>
<subsectionHeader confidence="0.999784">
3.1 The Inside Updates
</subsectionHeader>
<bodyText confidence="0.999976791666667">
The high-level goal of our parser is to use SIMD
parallelism to evaluate the same rule across many
spans (1024 threads are currently used to process
8192 spans in each kernel). This approaches allows
us to satisfy the GPU performance desiderata from
the previous section. As discussed in section 5 each
GPU kernel actually processes a small subset of the
symbols and rules for the grammar, and kernels are
executed in sequence until the entire grammar has
been processed. Each thread iterates over the rules
in the same order, reading in symbols from the left
child and right child arrays in main memory as nec-
essary.
The two-dimensional work arrays must be stored
in “symbol-major” order for this to work. That is,
the parent VP for one work item is stored next to the
parent VP for the next work item, while the VP sym-
bol for the first work item is stored on the next “row”
of the work array. The reason the workspace cells
are stored in “symbol-major” order is to maximize
coalesced access: each thread in the SMX accesses
the same symbol for a different work item in paral-
lel, and those work items are in consecutive memory
locations.
</bodyText>
<subsectionHeader confidence="0.99905">
3.2 The Copy-transpose Operations
</subsectionHeader>
<bodyText confidence="0.999933">
Unlike the workspace arrays, the arrays for the parse
charts are stored in “span-major” order, transposed
from how they are stored in the workspace arrays.
That is, for a given span, the NP symbol is next
to the same span’s VP symbol (for example). This
order accelerates both symbol loading and Viterbi
search later on. It requires a transpose-copy in-
stead of “non-transposed” copy to move from chart
to workspace arrays and back again, but note that a
non-transposed copy (or direct access to the chart
by the GPU compute kernel) would probably be
slower. The reason is that any linear ordering of
cells in the triangle table will produce short seg-
ments (less than 32 words and often less than 16)
of consecutive memory locations. This will lead to
many non-coalesced memory accesses. By contrast
the span-major representation always uses vectors
whose lengths equals the number of symbols (500-
1000), and these can be accessed almost entirely
with coalesced operations. The copy-transpose op-
erations are quite efficient (the transpose itself is
much faster than the I/O), and come close to the 160
GB/s GPU main memory limit.
The reverse copy-transpose (from parent
workspace cells to chart) is typically many-to-one,
since parent scores derive from multiple splits. They
are implemented using atomic reduce operations
(either atomic sum or atomic max) to ensure data
consistency.
At the heart of our approach is the use of grammar
compilation and symbol/rule blocking, described
next.
</bodyText>
<page confidence="0.989939">
1901
</page>
<sectionHeader confidence="0.997811" genericHeader="method">
4 Grammar Compilation
</sectionHeader>
<bodyText confidence="0.997456">
Each rule in a probabilistic context-free grammar
can be evaluated with an update of the form:
</bodyText>
<equation confidence="0.991002">
Sij,m = 11 Sik,nS(k+1)j,pcmnp (2)
k=1...j; n,pEQ
</equation>
<bodyText confidence="0.999978384615385">
where Sij,m is the score for symbol m as a generator
of the span of words from position i to j in the in-
put sentence, cmnp is the probability that symbol m
generates the binary symbol pair n, p, and Q is the
set of symbols. The scores will be stored in a CKY
chart indexed by the span ij and the symbol m.
To evaluate (2) as fast as possible, we want to
use register variables which are limited in number.
The location indices i, j, k can be moved outside the
GPU kernel to reduce the variable count. We use
symbols P, L and R for respectively the score of the
parent, left child and right child in the CKY chart.
Then the core relation in (2) can be written as:
</bodyText>
<equation confidence="0.9899905">
�Pm = LnRpcmnp (3)
n,pEQ
</equation>
<bodyText confidence="0.99987">
In the KeplerTM architecture, register arguments are
non-indexed, i.e. one cannot access register 3 as an
array variable R[i] with i=31. So in order to use
register storage for maximum speed, we must open-
code the grammar. Symbols like L3, R17 are en-
coded as variables L003 and R017, and each rule
must appear as a line of C code:
</bodyText>
<equation confidence="0.994469666666667">
P043 += L003 *R017 *0.023123f;
P019 += L012 *R123 *6.21354e-7f;
: : : :
</equation>
<bodyText confidence="0.962265">
Open-coding the grammar likely has a host of per-
formance advantages. It allows both compiler and
hardware to “see” what arguments are coming and
schedule the operations earlier than a “grammar
as data” approach. Note that we show here the
sum-product code for computing inner/outer symbol
probabilities. For Viterbi parse extraction we replace
+,* with max,+ and work on log scores.
L and R variables must be loaded from main
memory, while P-values are initialized to zero and
then atomically combined (sum or max) with P-
values in memory. Loads are performed as late as
</bodyText>
<footnote confidence="0.6322445">
1Even if indexing were possible, it is extremely unlikely that
such accesses could complete in a single cycle
</footnote>
<bodyText confidence="0.9907885">
possible, that is, a load instruction will immediately
precede the first use of a symbol:
</bodyText>
<equation confidence="0.9696315">
float R031 = right[tid+65*stride];
P001 += L001*R031*1.338202e-001f;
</equation>
<bodyText confidence="0.999934666666667">
where tid is the thread ID plus an offset, and stride
is the row dimension of the workspace (typically
8192), and right is the main memory array of right
symbol scores. Similarly, atomic updates to P-
values occur as early as possible, right after the last
update to a value:
</bodyText>
<equation confidence="0.9787495">
G020 += L041*R008*6.202160e-001f;
atomicAdd(&amp;par[tid+6*stride],G020);
</equation>
<bodyText confidence="0.9999652">
These load/store strategies minimize the active life
of each variable and allow reuse of register variables
for symbols whose lifetimes do not overlap. This
will be critical to successful blocking, described in
the next section.
</bodyText>
<subsectionHeader confidence="0.996404">
4.1 Common subexpressions
</subsectionHeader>
<bodyText confidence="0.999992875">
One interesting discovery made by the compiler was
that the same L,R pair is repeated in several rules. In
hindsight, this is obvious because the symbols in this
grammar are splits of base symbols, and so splits of
the parent symbol will be involved in rules with each
pair of L,R splits. The compiler recognized this by
turning the L,R pair into a common subexpression
in a register. i.e. the compiler converts
</bodyText>
<equation confidence="0.99716525">
P008 += L041 *R008 *6.200769e -001f;
P009 += L041 *R008 *6.201930e -001f;
P010 += L041 *R008 *6.202160e -001f;
into
float LRtmp = L041*R008;
P008 += LRtmp*6.200769e-001f;
P009 += LRtmp*6.201930e-001f;
P010 += LRtmp*6.202160e-001f;
</equation>
<bodyText confidence="0.999960222222222">
and inspection of the resulting assembly code shows
that each rule is compiled into a single fused
multiply-add of LRtmp and a value from con-
stant memory into the P symbol register. This al-
lows grammar evaluation to approach the theoretical
Gflop limit of the GPU. For this to occur, the rules
need to be sorted with matching L,R pairs consecu-
tive. The compiler does not discover this constraint
otherwise or reorder instructions to make it possible.
</bodyText>
<page confidence="0.987958">
1902
</page>
<subsectionHeader confidence="0.995261">
4.2 Exploiting L2 cache
</subsectionHeader>
<bodyText confidence="0.999995909090909">
Finally, we have to generate code to evaluate distinct
minor cube rulesets on each of the 8 SMXes con-
currently in order to benefit from the L2 cache, as
described in the next section. CUDATM (NVIDIA’s
GPU programming Platform) does not allow direct
control of SMX target, but we can achieve this by
running the kernel as 8 thread blocks and then test-
ing the block ID within the kernel and dispatching to
one of 8 blocks of rules. The CUDATM scheduler
will execute each thread block on a different SMX
which gives the desired distribution of code.
</bodyText>
<sectionHeader confidence="0.994104" genericHeader="method">
5 Symbol and Rule Blocking
</sectionHeader>
<bodyText confidence="0.997655333333333">
The grammar formula (3) is very sparse. i.e. most
productions are impossible and most c.,,p are zero.
For the Berkeley grammar used here, only 0.2% of
potential rules occur. Normally this would be bad
news for performance because it suggests low vari-
able re-use. However, the update relation is a tensor
rather than a matrix product. The re-use rate is deter-
mined by the number of rules in which a particular
symbol occurs, which is actually very high (more
than 1000 on average).
The number of symbols is about 1100 in this
grammar, and only a fraction can be stored in a
thread’s register set at one time (which is either 63 or
255 registers). To compute all productions we will
need to break the calculation into smaller groups of
variables that can fit in the available register space.
We can visualize this geometrically in figure 2.
The vectors of symbols P, L and R form the lead-
ing edges of this cube. The cube will be partitioned
into smaller subcubes indexed by subsets of those
symbols, and containing all the rules that apply be-
tween those symbols. The partitioning is chosen so
that the symbols in that subset can fit into available
register storage. In addition, the partitioning is cho-
sen to induce the same number of rules in each cube
- otherwise different code paths in the kernel will run
longer than others, and reduce overall performance.
This figure is a simplification - in order to balance
the number of rules in each subcube, the partition-
ing is not uniform in number of symbols as the figure
suggests.
As can be seen in figure 2, cube partitioning has
two levels. The original P-L-R cube is first par-
</bodyText>
<figureCaption confidence="0.9972775">
Figure 2: Partition of the cube of symbol combinations
into major subcubes (left) and minor subcubes (right).
</figureCaption>
<bodyText confidence="0.999958057142857">
titioned into “major” cubes, which are then parti-
tioned into “minor” cubes (2x2x2 in the figure). A
major cube holds the symbols and rules that are ex-
ecuted in a single GPU kernel. The minor cubes in a
major cube hold the symbols and rules that are exe-
cuted in a particular SMX. For the GTX-680 or K10
with 8 SMXs, this allows different SMXs to concur-
rently work on different 2x2x2 subcubes in a major
cube. This arrangement substantially reduces main
memory bandwidth through the L2 cache (which is
shared between SMXes). Each symbol in a major
cube will be loaded just once from main memory,
but loaded into (up to) 4 different SMXes through
the L2 cache. Subcube division for caching in our
experiments roughly doubled the kernel’s speed.
However, simple partitioning will not work. e.g.
if we blocked into groups of 20 P, L, R symbols
(in order to fit into 60 registers), we would need
1100/20 = 55 blocks along each edge, and a total of
553 pz� 160, 000 cells. Each symbol would need to
be loaded 552 = 3025 times, there would be almost
no symbol re-use. Throughput would be limited by
main memory speed to about 100 Gflops, an order
of magnitude slower than our target. Instead, we
use a rule partitioning scheme that creates as small a
symbol footprint as possible in each cube. We use a
spectral method to do this.
Before describing the spectral method we men-
tion an optimization that drops the symbol count by
2. Symbols are either terminal or non-terminal, and
in the Berkeley latent variable grammar there are
roughly equal numbers of them (503 non-terminals
and 631 terminals). All binary rules involve a non-
terminal parent. L and R symbols may be either ter-
minal or non-terminal, so there are 4 distinct types
</bodyText>
<equation confidence="0.72776">
L
P R
</equation>
<page confidence="0.909774">
1903
</page>
<bodyText confidence="0.999981357142857">
of rules depending on the L, R, types. We handle
each of these cases with a different kernel, which
rougly halves the number of rules along each edge
(it is either 503 or 631 along each edge). Further-
more, these kernels are called in different contexts,
and a different number of times. e.g. XX (L, R, both
non-terminal) kernels are called O(s3) times for sen-
tences of length s because both L, R children can oc-
cur at every position in the chart. XT and TX kernels
(with one terminal and one non-terminal symbol) are
called only O(s2) times since one of L or R must be
at the base of the chart. Finally TT kernels (both L
and R are terminals) will be called O(s) times. Per-
formance is therefore dominated by the XX kernel.
</bodyText>
<subsectionHeader confidence="0.980407">
5.1 Spectral Partitioning
</subsectionHeader>
<bodyText confidence="0.999833493506494">
We explored a number of partitioning schemes for
both symbol and rule partitioning. In the end we
settled on a spectral symbol partitioning scheme.
Each symbol is a node in the graph to be parti-
tioned. Each node is assigned a feature vector de-
signed to match it to other nodes with similar sym-
bols occuring in many rules. There was considerable
evolution of this feature set to improve partitioning.
In the end the vector for a particular P symbol is
a = (a1, 0.1 * a2, 0.1 * a3) where a1 is a vector
whose elements are indexed by L, R pairs and whose
values represent the number of rules involving both
those symbols (and the parent symbol P), a2 encodes
L symbols and counts the number of rules contain-
ing that L symbol and P, and a3 encodes the R sym-
bols and counts rules containing that R symbol and
P. This feature vector produces a high similarity be-
tween P symbols that exactly share many L,R pairs
and lower similarity for shared L and R.
A spectral clustering/partitioning algorithm ap-
proximately minimizes the total edge weight of
graph cuts. In our case, the total weight of a cut is
to first order the product of the number of L,R pairs
that occur on each side of the cut, and to second or-
der the count of individual L and R pairs that span
the cut. Let S and T be the counts for a particular
LR pair or feature, then we are trying to minimize
the product S*T while keeping the sum S+T, which
is the total occurences of the feature on both sides of
the partition, constant. Such a product is minimized
when one of S or T is zero. Since many symbols
are involved, this typically does not happen to an in-
dividual symbol, but this heuristic is successful at
making the individual symbol or LR pair distribu-
tions across the cuts as unbalanced as possible. i.e.
one side of the cut has very few instances of a given
symbol. The number of instances of a symbol is an
upper bound on the number of subcells in which than
symbol occurs, and therefore on the number of times
it needs to be loaded from memory. Repeating this
operation recursively to produce a 3d cell decom-
position also concentrates each symbol in relatively
few cells, and so tends to reduce the total register
count per cell.
In a bit more detail, from the vectors a above we
construct a matrix A whose columns are the fea-
ture vectors for each P symbol. Next we construct
the symmetric normalized Laplacian L for the adja-
cency matrix ATA. We then compute the eigende-
composition of L, and extract the eigenvector cor-
responding to the second-smallest eigenvalue. Each
node in the graph is assign a real weight from the
corresponding element of this eigenvector. We sort
by these weights, and partition the symbols using
this sort order. We tried both recursive binary par-
titioning, and partitioning into k intervals using the
original sort order, and obtained better results with
the latter.
Partitioning is applied in order P, L, R to gener-
ate the major cubes of the rule/symbol partition, and
then again to generate minor cubes. This partition-
ing is far more efficient than a naive partitioning.
The XX ruleset for our Berkeley grammar has about
343,000 rules over a 5033 cube of non-terminal sym-
bols. The optimal PxLxR cube decomposition (op-
timal in net kernel throughput) for this ruleset was
6x2x2 for major cubes, and then 2x2x2 for minor
cubes. This requires 6x2x2=24 GPU kernels, each
of which encodes 2x2x2=8 code blocks (recall that
each of the 8 SMXs executes a different code block
from the same kernel)2. Most importantly the reload
rate (the mean number of major cells containing a
given symbol, or the mean number of times a sym-
bol needs to be reloaded from main memory) drops
to about 6 (vs. 3000 for naive partitioning). This
is very significant. Each symbol is used on average
343,000/501 Pz� 6000 times overall by the XX ker-
</bodyText>
<footnote confidence="0.9922535">
2This cube decomposition also respects the constant cache
and instruction cache limits
</footnote>
<page confidence="0.997249">
1904
</page>
<bodyText confidence="0.999968555555556">
nel. Dropping the reload factor to 6 means that for
every 1 main memory load of a symbol, there are
approximately 1000 register or L2 cache reuses. A
little further calculation shows that L2 cache items
are used a little more than twice, so the register reuse
rate within kernel code blocks is close to 500 on av-
erage. This is what allows teraflop range speeds.
Note that while the maximum number of registers
per thread in the GTX-680 or K10 is 63, the aver-
age number of variables per minor cube is over 80
for our best-performing kernel, showing a number
of variables have non-overlapping lifetimes. Sorting
rules lexicographically by (L,R,P) does a good job
of minimizing variable lifetime overlap. However
the CUDATM compiler reorders variables anyway
with slightly worse performance on average (there
seems to be no way around this, other than generat-
ing assembly code directly).
</bodyText>
<sectionHeader confidence="0.995264" genericHeader="method">
6 GPU Viterbi Parse Extraction
</sectionHeader>
<bodyText confidence="0.999919685185185">
In sequential programs for chart generation, it is pos-
sible to compute and save a pointer to the best split
point and score at each node in the chart. However,
here the scores at each node are computed with fine-
grained parallelism. The best split point and score
cannot be computed until all scores are available.
Thus there is a separate Viterbi step after chart scor-
ing.
The gap between GPU and CPU performance
is large enough that CPU Viterbi search was a
bottleneck, even though it requires asymptotically
less work (O(Gs2) worst case, O(Gs) typical) vs
O(Gs3) to compute the CKY scores. Therefore
we wrote a non-recusive GPU-based Viterbi search.
Current GPUs support “high-level” recursion, but
there is no stack in the SMX. A recursive pro-
gram must create software stack space in either
shared memory or main memory which serious per-
formance impact on small function calls. Instead,
we use an iterative version of Viterbi parse extrac-
tion which uses pre-allocated array storage to store
its output, and such that the partially-complete out-
put array encodes all the information the algorithm
needs to proceed - i.e. the output array is also the
algorithm’s work queue.
Ignoring unaries for the moment, a binary parse
tree for a sentence of length n has 2n − 1 nodes,
including preterminals, internal nodes, and the root.
We can uniquely represent a tree as an array with
2n − 1 elements. In this representation, each index
corresponds to a node in prefix (depth-first) order.
For example, the root is always at position 0, and the
second node will correspond to the root’s left child.
If this second node has a left child, it will be the third
node, otherwise the third node will be the second’s
right sibling.
We can uniquely identify the topology of the tree
by storing the “width” of each node in this array,
where the width is the number of words governed
by that constituent. For a node at position p, its left
child will always be at p + 1, and its right child will
always be at p + 2 · wt, where wt is the width of the
left child. The symbol for each node can obviously
be stored with the height. For unaries, we require
exactly one unary rule per node, with the possibil-
ity that it is the identity rule, and so we store two
nodes: one for the “pre-unary” symbol, and one for
the “post-unary.” (Identity unary transitions are re-
moved in post-processing.)
Algorithm 1 Non-recursive Viterbi implementation.
The algorithm proceeds left-to-right in depth-first
order along the array representing the tree.
Input: Sentence length n, parse chart V[i,j]
Output: Array tree of size 2·n−2
</bodyText>
<equation confidence="0.962528375">
tree[0].preunary ← ROOT
tree[0].width ← n
i ← 0 &gt; Current leftmost position for span
for p ← 0 to 2·n−2 do
j ← i + tree[p].width &gt; Rightmost position
postu ← BestUnary(V, tree[p].preunary, i, j)
tree[p].postunary ← parent
if tree[p].width = 1 then
i ← i + 1
else
lc, rc, k ← BestBinary(V, parent, i, j)
tree[p + 1].preunary ← lc
tree[p + 1].width ← k − i
tree[p + 2·(k−i)].width ← j - k
tree[p + 2·(k−i)].preunary ← rc
end if
</equation>
<subsectionHeader confidence="0.30578">
end for
</subsectionHeader>
<bodyText confidence="0.898422">
Armed with this representation, we are ready to
describe algorithm 1. The algorithm proceeds in
</bodyText>
<page confidence="0.980796">
1905
</page>
<bodyText confidence="0.999994414634147">
left-to-right order along the array. First, the sym-
bol of the root is recorded. Then, for each node in
the tree, we search for the best unary rule continuing
it. If the node is a terminal, then no more nodes can
contain the current word, and so we advance the po-
sition of the left most child. Otherwise, if the node
is a non-terminal, we then find its left and right chil-
dren, entering their respective symbols and widths
into the array representing the tree.
The GPU implementation follows the algorithm
outline above although is somewhat technical. Each
parse tree is handled by a separate thread block
(thread blocks are groups of threads that can com-
municate through shared memory, and run on a sin-
gle SMX). Each thread block includes a number of
threads which are used to rapidly (in partly parallel
fashion) iterate through rulesets and symbol vectors
for the BestBinary and BestUnary operations using
coalesced memory accesses. Each thread block first
loads the complete set of L and R scores for the
current split being explored. Recall that these are
in consecutive memory locations using the “span-
major” ordering, so these loads are coalesced. Then
the thread block parallel-iterates through the rules
for the current parent symbol, which will be in a con-
tiguous block of memory since the rules are sorted
by parent symbol, and again are coalesced. The
thread block therefore needs storage for all the L, R
symbol scores and in addition working storage pro-
portional to the number of threads (to hold the best
child symbol and its score from each thread). The
number of threads is chosen to maximize speed: too
few will cause each thread to do more work and to
run more slowly. Too many will limit the number of
thread blocks (since the total threads concurrently
running on an SMX is 1024) that can run concur-
rently. We found 128 to be optimum.
With these techniques, Viterbi search consumes
approximately 1% of the parser’s running time. Its
throughput is around 10 Gflops, and it is 50-100x
faster than a CPU reference implementation.
</bodyText>
<sectionHeader confidence="0.999825" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99996675">
The parser was tested in an desktop computer with
one Intel E5-2650 processor, 64 GB ram, and
2 GTX-690 dual GPUs (effectively 4 GTX-680
GPUs). The high-level parser code is written in a
matrix library in the Scala language, which access
GPU code through JNI and using the JCUDA wrap-
per library for CUDATM.
XX-kernel throughput was 900 Gflops per GPU
for sum-product calculation (which uses a single
FMA for most rules) and 700 Gflops per GPU for
max-sum calculations (which requires two instruc-
tions for most rules). Net parser throughput in-
cluding max-sum CKY evaluation, Viterbi scoring
traspose-copy etc was between 500 and 600 gi-
gaflops per GPU, or about 2 teraflops total. Parsing
max-length-30 sentences from the Penn Treebank
test set ran at 250 sentences/sec per GPU, or 1000
sentences/sec total. General sentences were parsed
at about half this rate, 120 sentences/sec per GPU,
or 480 sentences/sec for the system.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.987702103448276">
We described a new approach to GPU constituency
parsing with surprisingly fast performance, close
to the theoretical limits of the GPU and similar to
dense matrix multiplication which achieves the de-
vices highest practical throughput. The resulting
parser parses 1000 length-30 sentences per second
in a 4-GPU computer. The parser has immediate ap-
plication to parsing and eventually to parser training.
The two highest-priority extensions are:
Addition of pruning: coarse-to-fine score pruning
should be applicable to our GPU design as it is to
CPU parsers. GPU pruning will not be as granu-
lar as CPU pruning and is unlikely to yield as large
speedups (4-5 orders of magnitude are common for
CPU parser pruning). But on the other hand, we
hardly need speedups that large, and 1-2 orders of
magnitude would be very useful.
Direct generation of assembly code. Currently our
code generator produces (&gt; 1.7 million lines, about
same as the number of rules) C source code which
must be compiled into GPU binary code. While it
takes only 8 seconds to generate the source code, it
takes more than an hour to compile it. The com-
piler evidently applies a number of optimizations
that we cannot disable, and this takes time. This
is an obstacle to e.g. using this framework to train
a parser where there would be frequent updates to
the grammar. However, since symbol variables cor-
respond almost one-to-one with registers (modulo
</bodyText>
<page confidence="0.976086">
1906
</page>
<bodyText confidence="0.999955857142857">
lifetime overlap and reuse, which our code gener-
ator is slightly better at than the compiler), there is
no reason for our code generator not to generate as-
sembly code directly. Presumably assembly code is
much faster to translate into kernel modules than C
source, and hopefully this will lead to much faster
kernel generation.
</bodyText>
<subsectionHeader confidence="0.994239">
8.1 Code Release
</subsectionHeader>
<bodyText confidence="0.99972325">
The code will be released under a BSD-style open
source license once its dependencies are fully in-
tegrated. Pre- and Final releases will be here
https://github.com/jcanny/BIDParse
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865795454546">
Anonymous. 2012. CUDA C PROGRAMMING
GUIDE. Technical Report PG-02829-001-v5.0. In-
cluded with CUDATM Toolkit.
Anonymous. 2013. NVIDIA’s next generation CUDA
compute architecture: KeplerTM GK110. Technical
report. Included with CUDATM Tootkit.
Mark Johnson. 2011. Parsing in parallel on mul-
tiple cores and gpus. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2011, pages 29–37, Canberra, Australia, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’05), pages
75–82, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
NVIDIA. 2012. private communication.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404–411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on gpus.
In Proceedings of the 2011 Conference on Parsing
Technologies, Dublin, Ireland, October.
</reference>
<page confidence="0.99534">
1907
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867532">
<title confidence="0.99943">A multi-Teraflop Constituency Parser using GPUs</title>
<author confidence="0.999974">John Canny David Hall Dan Klein</author>
<affiliation confidence="0.999679">UC Berkeley</affiliation>
<address confidence="0.999973">Berkeley, CA, 94720</address>
<email confidence="0.999744">canny@berkeley.edu,dlwh,klein@cs.berkeley.edu</email>
<abstract confidence="0.99128194117647">Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPU’s practical maximum speed (a Teraflop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length- 30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anonymous</author>
</authors>
<title>CUDA C PROGRAMMING GUIDE.</title>
<date>2012</date>
<tech>Technical Report PG-02829-001-v5.0. Included with CUDATM Toolkit.</tech>
<contexts>
<context position="3325" citStr="Anonymous (2012)" startWordPosition="528" endWordPosition="529">struction cache limits. This is precondition for 1 above, and the details of the partitioning have a big (&gt; 4x) effect on performance. 3. Sub-block partitioning to distribute rules across the stream processors of the GPU and allow L2 cache acceleration. A factor of 2 improvement. The code generated by our parser comes close to the theoretical limits of the GPU. 80% of grammar rules 1898 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898–1907, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Data from Anonymous (2012) and NVIDIA (2012). are evaluated using a single-cycle register-to-register instruction. 2 GPU Design Principles In this paper, we focus on the architecture of recent NVIDIA® GPUs, though many of the principles we describe here can be applied to other GPUs (e.g. those made by AMD R�.) The current NVIDIA® KeplerTM series GPU contains between 2 and 16 “stream processors” or SMX’s which share an L2 cache interfacing to the GPUs main memory Anonymous (2013). The SMXs in turn comprise 192 cores which share a memory which is partitioned into “shared memory” and L1 cache. Shared memory supports relat</context>
</contexts>
<marker>Anonymous, 2012</marker>
<rawString>Anonymous. 2012. CUDA C PROGRAMMING GUIDE. Technical Report PG-02829-001-v5.0. Included with CUDATM Toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anonymous</author>
</authors>
<title>NVIDIA’s next generation CUDA compute architecture: KeplerTM GK110.</title>
<date>2013</date>
<tech>Technical report. Included with CUDATM Tootkit.</tech>
<contexts>
<context position="3782" citStr="Anonymous (2013)" startWordPosition="602" endWordPosition="604">Language Processing, pages 1898–1907, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Data from Anonymous (2012) and NVIDIA (2012). are evaluated using a single-cycle register-to-register instruction. 2 GPU Design Principles In this paper, we focus on the architecture of recent NVIDIA® GPUs, though many of the principles we describe here can be applied to other GPUs (e.g. those made by AMD R�.) The current NVIDIA® KeplerTM series GPU contains between 2 and 16 “stream processors” or SMX’s which share an L2 cache interfacing to the GPUs main memory Anonymous (2013). The SMXs in turn comprise 192 cores which share a memory which is partitioned into “shared memory” and L1 cache. Shared memory supports relatively fast communication between threads in an SMX. Communication between SMXs has to pass through slower main memory. The execution of instructions within SMXs is virtualized and pipelined - i.e. it is not a simple task to count processors, although there are nominally 192 in the KeplerTM series. Register storage is not attached to cores, instead registers are associated in blocks of 63 or 255 (depending on KeplerTM subarchitecture) with running thread</context>
</contexts>
<marker>Anonymous, 2013</marker>
<rawString>Anonymous. 2013. NVIDIA’s next generation CUDA compute architecture: KeplerTM GK110. Technical report. Included with CUDATM Tootkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing in parallel on multiple cores and gpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>29--37</pages>
<location>Canberra, Australia,</location>
<contexts>
<context position="10141" citStr="Johnson (2011)" startWordPosition="1660" endWordPosition="1661">t memory (needed for grammar rules probabilities) and instruction cache limits. 4. Minimize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of t</context>
</contexts>
<marker>Johnson, 2011</marker>
<rawString>Mark Johnson. 2011. Parsing in parallel on multiple cores and gpus. In Proceedings of the Australasian Language Technology Association Workshop 2011, pages 29–37, Canberra, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10169" citStr="Klein and Manning (2003)" startWordPosition="1663" endWordPosition="1666">or grammar rules probabilities) and instruction cache limits. 4. Minimize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p,l, r), where s</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>COMPUTATIONAL LINGUISTICS,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1661" citStr="Marcus et al. (1993)" startWordPosition="256" endWordPosition="259"> G rules and sentence length s, is daunting. Even with a host of pruning heuristics, the high cost of constituency parsing limits its uses. The most recent Berkeley latent variable grammar for instance, has 1.7 million rules and requires about a billion rule evaluations for inside scoring of a single length-30 sentence. GPUs have previously been used to accelerate CKY evaluation, but gains over CPU parsers were modest. e.g. in Yi et al. (2011) a GPU parser is described for the Berkeley Parser grammar which achieves 5 sentences per second on the first 1000 sentences of Penn Treebank section 22 Marcus et al. (1993), which is comparable with the best CPU parsers Petrov and Klein (2007). Our parser achieves 120 sentences/second per GPU for this sentence set, and over 250 sentences/sec on length &lt; 30 sentences. These results use a Berkeley Grammar approximately twice as big as Yi et al. (2011), an apparent 50x improvement. On a 4-GPU system, we achieve 1000 sentences/sec for length &lt; 30 sentences. This is 2 orders of magnitude faster than CPU implementations that rely heavily on pruning, and 5 orders of magnitude faster than full CKY evaluation on a CPU. Key to these results is a collection of new techniqu</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>75--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="10237" citStr="Matsuzaki et al. (2005)" startWordPosition="1675" endWordPosition="1678">mize main memory access and use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p,l, r), where s is a sentence, and p, l, and r specify the index in the parse chart</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 75–82, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NVIDIA</author>
</authors>
<title>private communication.</title>
<date>2012</date>
<contexts>
<context position="3343" citStr="NVIDIA (2012)" startWordPosition="531" endWordPosition="532">s. This is precondition for 1 above, and the details of the partitioning have a big (&gt; 4x) effect on performance. 3. Sub-block partitioning to distribute rules across the stream processors of the GPU and allow L2 cache acceleration. A factor of 2 improvement. The code generated by our parser comes close to the theoretical limits of the GPU. 80% of grammar rules 1898 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898–1907, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Data from Anonymous (2012) and NVIDIA (2012). are evaluated using a single-cycle register-to-register instruction. 2 GPU Design Principles In this paper, we focus on the architecture of recent NVIDIA® GPUs, though many of the principles we describe here can be applied to other GPUs (e.g. those made by AMD R�.) The current NVIDIA® KeplerTM series GPU contains between 2 and 16 “stream processors” or SMX’s which share an L2 cache interfacing to the GPUs main memory Anonymous (2013). The SMXs in turn comprise 192 cores which share a memory which is partitioned into “shared memory” and L1 cache. Shared memory supports relatively fast communi</context>
</contexts>
<marker>NVIDIA, 2012</marker>
<rawString>NVIDIA. 2012. private communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="1732" citStr="Petrov and Klein (2007)" startWordPosition="269" endWordPosition="272">ning heuristics, the high cost of constituency parsing limits its uses. The most recent Berkeley latent variable grammar for instance, has 1.7 million rules and requires about a billion rule evaluations for inside scoring of a single length-30 sentence. GPUs have previously been used to accelerate CKY evaluation, but gains over CPU parsers were modest. e.g. in Yi et al. (2011) a GPU parser is described for the Berkeley Parser grammar which achieves 5 sentences per second on the first 1000 sentences of Penn Treebank section 22 Marcus et al. (1993), which is comparable with the best CPU parsers Petrov and Klein (2007). Our parser achieves 120 sentences/second per GPU for this sentence set, and over 250 sentences/sec on length &lt; 30 sentences. These results use a Berkeley Grammar approximately twice as big as Yi et al. (2011), an apparent 50x improvement. On a 4-GPU system, we achieve 1000 sentences/sec for length &lt; 30 sentences. This is 2 orders of magnitude faster than CPU implementations that rely heavily on pruning, and 5 orders of magnitude faster than full CKY evaluation on a CPU. Key to these results is a collection of new techniques that enable GPU parsing at close to the GPU’s practical maximum spee</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="10261" citStr="Petrov et al. (2006)" startWordPosition="1680" endWordPosition="1683"> use L2 cache to speed it up. Lets look in more detail at how to achieve this. 3 Anatomy of an Efficient GPU Parser High performance on the GPU requires us to minimize code divergence. This suggests that we do not use a lexicalized grammar or a grammar that is sensitive to the position of a span within the sentence. These kinds of grammars—while highly accurate— have irregular memory access patterns that conflict with SIMD execution. Instead, an unlexicalized approach like that of Johnson (2011) or Klein and Manning (2003), or a latent variable approach like that of Matsuzaki et al. (2005) or Petrov et al. (2006) are more appropriate. We opt for the latter kind: latent variable grammars are fairly small, and their accuracies rival lexicalized approaches. Our GPU-ized inside algorithm maintains two data structures: parse charts that store scores for each labeled span, as usual, and a “workspace” that is used to actually perform the updates of the inside algorithm. Schematically, this memory layout is represented in Figure 1. A queue is maintained CPU-side that enqueues work items of the form (s, p,l, r), where s is a sentence, and p, l, and r specify the index in the parse chart for parent, left child,</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngmin Yi</author>
<author>Chao-Yue Lai</author>
<author>Slav Petrov</author>
<author>Kurt Keutzer</author>
</authors>
<title>Efficient parallel cky parsing on gpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Parsing Technologies,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1488" citStr="Yi et al. (2011)" startWordPosition="225" endWordPosition="228">n Constituency parsing with high accuracy (e.g. latent variable) grammars remains a computational challenge. The O(Gs3) complexity of full CKY parsing for a grammar with G rules and sentence length s, is daunting. Even with a host of pruning heuristics, the high cost of constituency parsing limits its uses. The most recent Berkeley latent variable grammar for instance, has 1.7 million rules and requires about a billion rule evaluations for inside scoring of a single length-30 sentence. GPUs have previously been used to accelerate CKY evaluation, but gains over CPU parsers were modest. e.g. in Yi et al. (2011) a GPU parser is described for the Berkeley Parser grammar which achieves 5 sentences per second on the first 1000 sentences of Penn Treebank section 22 Marcus et al. (1993), which is comparable with the best CPU parsers Petrov and Klein (2007). Our parser achieves 120 sentences/second per GPU for this sentence set, and over 250 sentences/sec on length &lt; 30 sentences. These results use a Berkeley Grammar approximately twice as big as Yi et al. (2011), an apparent 50x improvement. On a 4-GPU system, we achieve 1000 sentences/sec for length &lt; 30 sentences. This is 2 orders of magnitude faster th</context>
</contexts>
<marker>Yi, Lai, Petrov, Keutzer, 2011</marker>
<rawString>Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt Keutzer. 2011. Efficient parallel cky parsing on gpus. In Proceedings of the 2011 Conference on Parsing Technologies, Dublin, Ireland, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
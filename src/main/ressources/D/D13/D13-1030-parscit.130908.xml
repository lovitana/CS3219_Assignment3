<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.996586">
Interpreting Anaphoric Shell Nouns using Antecedents of
Cataphoric Shell Nouns as Training Data
</title>
<author confidence="0.993834">
Varada Kolhatkar Heike Zinsmeister
</author>
<affiliation confidence="0.9987265">
Department of Computer Science Institut f¨ur Germanistik
University of Toronto Universit¨at Hamburg
</affiliation>
<email confidence="0.987109">
varada@cs.toronto.edu heike.zinsmeister@uni-hamburg.de
</email>
<author confidence="0.995601">
Graeme Hirst
</author>
<affiliation confidence="0.9986445">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.998475">
gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.998137" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994139705882353">
Interpreting anaphoric shell nouns
(ASNs) such as this issue and this fact
is essential to understanding virtually
any substantial natural language text.
One obstacle in developing methods
for automatically interpreting ASNs is
the lack of annotated data. We tackle
this challenge by exploiting cataphoric
shell nouns (CSNs) whose construction
makes them particularly easy to interpret
(e.g., the fact that X). We propose an ap-
proach that uses automatically extracted
antecedents of CSNs as training data to
interpret ASNs. We achieve precisions
in the range of 0.35 (baseline = 0.21) to
0.72 (baseline = 0.44), depending upon
the shell noun.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995089125">
Anaphors such as this fact and this issue encapsulate
complex abstract entities such as propositions, facts,
and events. An example is shown below.
(1) Here is another bit of advice: Environmental
Defense, a national advocacy group, notes that
“Mowing the lawn with a gas mower produces
as much pollution in half an hour as driving a
car 172 miles.” This fact may help to explain
the recent surge in the sales of the good old-
fashioned push mowers or the battery-powered
mowers.
Here, the anaphor this fact is interpreted with the
help of the clausal antecedent marked in bold. The
antecedent here is complex because it involves a
number of entities and events (e.g., mowing the
lawn, a gas mower) and relationships between them,
and is abstract because the antecedent itself is not a
purely physical entity.
The distinguishing property of these anaphors is
that they contain semantically rich abstract nouns
(e.g., fact in (1)) which characterize and label their
corresponding antecedents. Linguists and philoso-
phers have studied such abstract nouns for decades
(Vendler, 1968; Halliday and Hasan, 1976; Francis,
1986; Ivanic, 1991; Asher, 1993). Our work is in-
spired by one such study, namely that of Schmid
(2000). Following Schmid, we refer to these abstract
nouns as shell nouns, as they serve as conceptual
shells for complex chunks of information. Accord-
ingly, we refer to the anaphoric occurrences of shell
nouns (e.g., this fact in (1)) as anaphoric shell nouns
(ASNs).
An important reason for studying ASNs is their
ubiquity in all kinds of text. Schmid (2000) ob-
served that shell nouns such as fact, idea, point, and
problem were among the 100 most frequently oc-
curring nouns in a corpus of 225 million words of
British English. Moreover, ASNs can play several
roles in organizing a discourse such as encapsulation
of complex information, cohesion, and topic bound-
ary marking. So correct interpretation of ASNs can
be an important step for correct interpretation of a
discourse, and in a number of NLP applications such
as text summarization, information extraction, and
non-factoid question answering.
Despite their importance, ASNs have not received
much attention in Computational Linguistics, and
research in this field remains in its earliest stages. At
</bodyText>
<page confidence="0.966683">
300
</page>
<note confidence="0.7328295">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 300–310,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.987784714285714">
present, the major obstacle is that there is very little
annotated data available that could be used to train a
supervised machine learning system for robustly in-
terpreting these anaphors, and manual annotation is
an expensive and time-consuming task.
We tackle this challenge by exploiting a category
of examples, as shown in (2), whose construction is
particularly easy to interpret.
(2) Congress has focused almost solely on the fact
that special education is expensive – and that
it takes away money from regular education.
Here, in contrast with (1), the fact is not anaphoric
in the traditional sense, but is an easy case of a
forward-looking anaphor — a cataphor. While the
resolution process of this fact in (1) is quite chal-
lenging as it requires the use of semantics and world
knowledge, it is fairly easy to interpret the fact in
(2) based on the syntactic structure alone. We refer
to these easy-to-interpret cataphoric occurrences of
shell nouns as cataphoric shell nouns (CSNs). The
interpretation of both ASNs and CSNs will be re-
ferred to as antecedent.1 The antecedent of the fact
in (2) is given in the post-nominal that clause. We
use the term shell concept to refer to the general no-
tion of a shell noun, i.e., the semantic type of the
antecedent. For example, the notion of an issue is an
important problem which requires a solution.
In this work, we propose an approach to interpret
ASNs that exploits unlabelled but easy-to-interpret
CSN examples to extract characteristic features as-
sociated with the antecedent of different shell con-
cepts. We evaluate our approach using crowdsourc-
ing. Our results show that these unlabelled CSN ex-
amples provide useful linguistic properties that help
in interpreting ASNs.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99548012195122">
The resolution of anaphors to non-nominal an-
tecedents has been well analyzed taking discourse
structure and semantic types into account (Web-
ber, 1991; Passonneau, 1989; Asher, 1993). Most
work in machine anaphora resolution, however,
is restricted to anaphora that involve nominal an-
tecedents only (Poesio et al., 2011).
1Sadly, the more-logical term for the interpretation of a
CSN, succedent, does not actually exist.
There are some notable exceptions which have
tackled the challenge of interpreting non-nominal
antecedents (Eckert and Strube, 2000; Strube and
M¨uller, 2003; Byron, 2004; M¨uller, 2008). These
approaches are limited as they either rely heavily
on domain-specific syntactic and semantic annota-
tion or prepossessing, or mark only verbal proxies
for non-nominal antecedents.
Recently, Kolhatkar and Hirst (2012) presented
a machine-learning based resolution system for this
issue anaphora, identifying full syntactic phrases as
antecedents. Although they achieved promising re-
sults, their approach was limited in two respects.
First, it focused on only one type of shell noun
anaphora (issue anaphora). Second, their training
data was restricted to MEDLINE abstracts in which
this issue is used in a rather systematic way. Further-
more, their work is based on manually labelled ASN
antecedents, whereas we use automatically identi-
fied CSN antecedents, which we interpret as explic-
itly expressed antecedents in comparison to the more
implicitly expressed ASN antecedents.
Using explicitly expressed structure in the text
to identify implicit structure is not new. The same
idea has been applied before in computational lin-
guistics. Marcu and Echihabi (2002) identified im-
plicit discourse relations using explicit ones. Mark-
ert and Nissim (2005) used Hearst’s (1992) explicit
patterns to learn lexical semantic relations for NP-
coreference and other-anaphora resolution from the
web. Although our work focuses on a different topic,
the methodology is in the same vein.
</bodyText>
<sectionHeader confidence="0.802453" genericHeader="method">
3 Hypothesis of this work
</sectionHeader>
<bodyText confidence="0.999933692307692">
The hypothesis of this work is that CSN antecedents
and ASN antecedents share some linguistic proper-
ties and hence linguistic knowledge encoded in CSN
antecedents will help in interpreting ASNs. Accord-
ingly, we examine which features present in CSN
antecedents are relevant in interpreting ASNs.
The motivation and intuition behind this hypoth-
esis is as follows. The antecedents of both ASNs
and CSNs represent the corresponding shell con-
cept. So are there any characteristic features asso-
ciated with this shell concept? Do speakers of En-
glish follow certain patterns of syntactic shape or
words, for instance, when they state facts, decisions,
</bodyText>
<page confidence="0.998925">
301
</page>
<figureCaption confidence="0.99986">
Figure 1: Overview of our approach
</figureCaption>
<bodyText confidence="0.999830176470588">
or issues? There is an abundance of data for CSN
antecedents and if we are able to capture particu-
lar linguistic characteristic features associated with
a shell concept using this data, we can use this in-
formation to interpret ASNs. For instance, exam-
ple (2) demonstrates characteristic properties of an-
tecedents of the shell noun fact including that (a)
they are propositions and are generally expressed
with clauses or sentences rather than noun phrases,
and (b) they are generally expressed in the present
tense. Observe that these properties also hold for
the antecedent of this fact in example (1).
We test our hypothesis by building machine learn-
ing models that are trained on automatically ex-
tracted CSN antecedents and then applying these
models to recover ASN antecedents. Figure 1 shows
an overview of our methodology.
</bodyText>
<sectionHeader confidence="0.997157" genericHeader="method">
4 Background
</sectionHeader>
<bodyText confidence="0.999960944444445">
Formal definition Shell-nounhood is a functional
notion; it is defined by the use of an abstract noun
rather than the inherent properties of the noun itself
(Schmid, 2000). An abstract noun is a shell noun
when the speaker decides to use it as a shell noun.
Shell noun categorization Schmid (2000) gives
a list of 670 English nouns which are frequently
used as shell nouns. He divides them into six
broad semantic classes: factual, linguistic, mental,
modal, eventive, and circumstantial. Table 1 shows
this classification, along with example shell nouns
for each category. For this work, we selected six
frequently occurring shell nouns covering four of
Schmid’s six classes: fact and reason from factual,
issue and decision from mental, question from lin-
guistic, and possibility from modal. These shell
nouns tend to have antecedents that lie within a sin-
gle sentence. We excluded eventive and circumstan-
</bodyText>
<subsectionHeader confidence="0.949157">
Class Description Examples
</subsectionHeader>
<bodyText confidence="0.955335">
factual states of affairs fact, reason
</bodyText>
<table confidence="0.6373046">
linguistic linguistic acts question
mental ideas issue, decision
modal judgements possibility
eventive events act, reaction
circumstantial situations situation, way
</table>
<tableCaption confidence="0.73588425">
Table 1: Schmid’s classification of shell nouns. The
nouns given in the Example column tend to occur fre-
quently with the respective class. The shell nouns used in
this work are shown in boldface.
</tableCaption>
<subsectionHeader confidence="0.971049">
Pattern Example
</subsectionHeader>
<bodyText confidence="0.936999303030303">
N-to Several people at the group said the deci-
sion to write the letters was not controver-
sial internally.
N-be-to The principal reason is to create a repre-
sentative government rather than to select
the most talented person.
N-that Mr. Shoval left open the possibility that
Israel would move into other West Bank
cities.
N-be-that The simple and reassuring fact is that a
future generation of leaders is seeking new
challenges during challenging times.
N-wh There is now some question whether the
country was ever really in a recession.
N-be-wh Of course, the central, and probably in-
soluble, issue is whether animal testing is
cruel.
Table 2: Easy-to-interpret CSN patterns given by Schmid
(2000). In the Example column, the patterns are marked
in boldface and the antecedents are marked in italics.
tial classes because the shell nouns in these classes
tend to have rather unclear and long antecedents.2
Shell noun patterns Schmid (2000) also provides
a number of lexico-grammatical patterns for shell
nouns. In Section 1, we noted two such patterns:
this-N (this fact in example (1)) and N-that (fact that
in example (2)). We also noted that CSNs with pat-
tern N-that are fairly easy to interpret compared to
the ASN pattern this-N. Table 2 shows some other
easy-to-interpret CSN patterns given by Schmid.
Generally, for all these patterns, the antecedent is
2These observations are based on an exploratory pilot anno-
tation we carried out on sample data of 150 ASN instances.
</bodyText>
<figure confidence="0.996880470588235">
Training Testing
Training data
The CSN corpus
(211,722 instances)
CSN
antecedent
extractor
CSN
antecedent
models
Ranked ASN
antecedent
candidates
The ASN corpus
(2,323 instances)
Crowdsourcing
evaluation
</figure>
<page confidence="0.989461">
302
</page>
<bodyText confidence="0.999371">
quite easy to extract with a few predefined rules.
Shell antecedent properties Antecedents of
CSNs and ASNs share some properties while they
are distinguished by others. The distinguishing
property is that CSNs, by their construction, have
their antecedents in the same sentence, as shown
in example (2). On the other hand, ASNs can
have long-distance as well as short-distance an-
tecedents.3 The common properties are as follows.
First, antecedents of both ASNs and CSNs represent
the corresponding shell concept, e.g., the notion
of a fact or an issue. Second, in both cases, the
antecedents are complex abstract entities, which
involve a number of entities and relationships
between them. Finally, in both cases, there is no
one-to-one correspondence between the syntactic
type of an antecedent and semantic type of its
referent (Webber, 1991). For instance, a semantic
type such as fact can be expressed with different
syntactic shapes such as a clause, a verb phrase, or
a complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
</bodyText>
<sectionHeader confidence="0.888645" genericHeader="method">
5 Training phase
</sectionHeader>
<bodyText confidence="0.99987475">
As shown in Figure 1, the goal of the training phase
is to build training data from CSNs and their an-
tecedents and train models which can be used for
resolving ASNs.
</bodyText>
<subsectionHeader confidence="0.990125">
5.1 The CSN corpus
</subsectionHeader>
<bodyText confidence="0.9843287">
We automatically constructed a corpus, a subset of
the New York times (NYT) corpus4, which contains
211,722 sentences following CSN patterns from Ta-
ble 2. We considered part-of-speech information5
while looking for the patterns. For instance, in-
stead of the pattern N-that, we actually looked for
{shell noun NN that IN}.
3In our annotated sample data, we observed ASN an-
tecedents as close as the same sentence and as far as 7 sentences
away from the anaphor.
</bodyText>
<footnote confidence="0.9914105">
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
5http://nlp.stanford.edu/software/tagger.
shtml
</footnote>
<subsectionHeader confidence="0.997837">
5.2 Antecedent extractor for CSNs
</subsectionHeader>
<bodyText confidence="0.999513465116279">
The goal of the antecedent extractor is to create auto-
matically labelled CSN antecedent data. Recall that
antecedents of CSNs can be extracted using simple
predefined rules that are based on the syntactic struc-
ture alone. For instance, the antecedent extraction
rule for example (2) would be: if the example fol-
lows the patternfact-that, extract the post-nominal
that clause as the antecedent. To come up with a list
of such extraction rules, we systematically analyzed
a sample of examples (about 20 examples) of each
pattern for each shell noun. Table 3 summarizes the
resulting antecedent extraction rules.
The actual antecedent extraction works as fol-
lows. First, we parsed the examples from the CSN
corpus using the Stanford parser.6 Then for each ex-
ample, we applied rules from Table 3 depending on
the shell noun and the pattern it follows to extract
an appropriate syntactic constituent as the CSN an-
tecedent. For instance, for the noun fact following
the N-that pattern, as in example (2), we first looked
for the NP constituent containing the shell noun fact,
and then extracted the sentential constituent follow-
ing the NP constituent as the CSN antecedent. Al-
though, in most of the cases, the antecedent is given
in the post-nominal wh, that, or infinitive clauses,
sometimes it is not present in the immediately fol-
lowing clause but is given only as a predicate, as
shown in (3).
(3) The primary reason that the archdiocese cannot
pay teachers more is that its students cannot af-
ford higher tuition.
In such cases, we looked for the pattern (VP (VB
be verb) X) in the right sibling of the NP contain-
ing the pattern shell noun-that and extracted X as
the CSN antecedent.
Two contradictory goals need to be achieved
while extracting antecedents of CSNs. The first re-
quires only considering CSNs with high-confidence
patterns, whereas the second requires considering as
many patterns as possible to allow a wide variety of
antecedent examples with different linguistic prop-
erties (e.g., syntactic shape). Our antecedent extrac-
tor tries to find a balance between the two goals.
</bodyText>
<footnote confidence="0.991034">
6http://nlp.stanford.edu/software/lex-parser.
shtml
</footnote>
<page confidence="0.992836">
303
</page>
<table confidence="0.962717714285714">
fact reason issue decision question possibility
N-to – – – inf clause predicate inf clause
N-be-to – inf clause inf clause inf clause inf/wh clause inf clause
N-that that clause predicate predicate – predicate that clause
N-be-that that clause that clause that clause that clause that clause that clause
N-wh – predicate wh clause wh clause wh clause –
N-be-wh wh clause wh clause wh clause wh clause wh clause wh clause
</table>
<tableCaption confidence="0.9863205">
Table 3: Content extraction patterns for CSNs. Patterns in boldface are the prominent patterns for the respective shell
noun. inf clause = infinitive clause. Discarded patterns are denoted by –.
</tableCaption>
<bodyText confidence="0.999834875">
To address the first goal, we filter examples fol-
lowing noisy patterns, i.e., the patterns that do not
unambiguously encode antecedents of that CSN. For
instance, the pattern N-to is a highly preferred pat-
tern for decision, as shown in (4). The antecedent
extraction rule here is relatively simple: if the exam-
ple follows the pattern decision-to, extract the post-
nominal infinitive clause as the correct antecedent.
</bodyText>
<listItem confidence="0.944048666666667">
(4) President Jacques Chirac’s arrogant decision to
defy the world and go ahead with two nuclear
bomb tests in Polynesia deserves contempt.
</listItem>
<bodyText confidence="0.864808111111111">
But the same pattern is noisy for reason. In (5), for
example, the actual reason is not given anywhere in
the sentence. So we discard the examples following
the pattern N-to for reason.
(5) Investors have had reason to worry about stocks.
We also discard examples with negative determiners,
as in (6), because in such cases, the extraction rules
do not precisely give the antecedent of the given
CSN.
</bodyText>
<listItem confidence="0.6589395">
(6) He was careful to repeat anew that he had made
no decision to go to war.
</listItem>
<bodyText confidence="0.999939363636364">
For the N-wh pattern, we exclude certain wh
words for certain nouns. For example, we exclude
the wh word which for question as the Penn Tree-
bank tagset7 does not distinguish between which as a
relative pronoun and as a question. We are interested
in the latter but not the former. Other discarded wh
words include which and when for fact; all wh words
except when and why for reason, all wh words except
how and whether for issue; which, whom, when, and
why for decision; which and when for question; and
all wh words for possibility.
</bodyText>
<footnote confidence="0.961267">
7The Stanford tagger we employ uses the Penn Treebank
tagset (Marcus et al., 1993).
</footnote>
<bodyText confidence="0.998995769230769">
To address the second goal of allowing a wide
variety of antecedent examples, we try to include
as many patterns as possible for each shell noun,
as shown in Table 3. For instance, the patterns
question-to and question-be-to will have infinitive
clauses as antecedents (marked as VP or S+VP
by the parser), whereas for the examples follow-
ing patterns question-wh and question-be-wh the
antecedent will be in the wh clauses (marked as
SBAR). For the pattern question-that, the antecedent
will be in the predicate (similar to example (3)),
which can be a prepositional phrase, a noun phrase
or a clause.
</bodyText>
<subsectionHeader confidence="0.974469">
5.3 Models for CSN antecedents
</subsectionHeader>
<bodyText confidence="0.999984714285714">
The antecedent extractor gives labels for each in-
stance in the CSN corpus. Using this labelled data,
we train machine learning ranking models for dif-
ferent shell concepts that capture the characteristic
features associated with that shell concept. The fol-
lowing sections describe each step of our ranking
models in detail.
</bodyText>
<subsubsectionHeader confidence="0.703612">
5.3.1 Candidate extraction
</subsubsectionHeader>
<bodyText confidence="0.999993857142857">
The first step is to extract the set of eligible an-
tecedent candidates C = {C1,C2,...,CkI for the CSN
instance ai. To train a machine learning model we
need positive and negative examples. We already
have positive examples for antecedent candidates —
the true antecedents given by the antecedent extrac-
tor. But we also need negative examples of an-
tecedent candidates. By their construction, CSNs
have their antecedents in the same sentence. So
we extract all syntactic constituents of this sentence,
given by the Stanford parser. All the syntactic con-
stituents, except the true antecedent, are considered
as negative examples. With this candidate extraction
method, we end up with many more negative exam-
</bodyText>
<page confidence="0.996636">
304
</page>
<bodyText confidence="0.996585333333333">
ples than positive examples, but that is exactly what
we expect with ASN antecedent candidates, i.e., the
test data on which we will be applying our models.
</bodyText>
<sectionHeader confidence="0.533437" genericHeader="method">
5.3.2 Features
</sectionHeader>
<bodyText confidence="0.999695137931035">
Although our problem is similar to anaphora res-
olution, we cannot make use of the usual anaphora
or coreference resolution features such as agreement
or string matching (Soon et al., 2001) because of the
nature of ASN and CSN antecedents. We came up
with a set of features based on the properties that
were common in both ASN and CSN antecedents,
according to our judgement.
Syntactic type of the candidate (S) We observed
that each shell noun prefers specific CSN patterns
and each pattern involves a particular syntactic type.
For instance, decision prefers the pattern N-to and
consequently realizes as its antecedents more verb
phrases than, for example, noun phrases. We employ
two versions of syntactic type: fine-grained syntac-
tic type given by the Stanford parser (e.g., NP-TMP,
RRC) and coarse-grained syntactic type (e.g., NP,
VP, S, PP) in which we consider ten basic syntactic
categories and map all fine-grained syntactic types
to these categories.
Context features (C) Context features allow our
models to learn about the contextual clues that signal
the antecedent. This class contains two features: (a)
coarse-grained syntactic type of left and right sib-
lings of the candidate, and (b) part-of-speech tag of
the preceding and following words of the candidate.
Embedding level features (E) These features
(M¨uller, 2008) encode the embedding level of the
candidate within its sentence. We consider two em-
bedding level features: top embedding level and im-
mediate embedding level. Top embedding level is
the level of embedding of the given candidate with
respect to its top clause (the root node), and immedi-
ate embedding level is the level of embedding with
respect to its immediate clause (the closest ancestor
of type S or SBAR). The intuition behind this fea-
ture is that if the candidate is deep in the parse tree,
it is possibly not salient enough to be an antecedent.
As we consider all syntactic constituents as poten-
tial candidates, there are many that clearly cannot be
antecedents. This feature will allow us to get rid of
this noise.
Subordinating conjunctions (SC) As we can see
in Table 2, subordinating conjunctions are common
with CSN and ASN antecedents. Vendler (1968)
points out that the shell noun fact prefers a that-
clause, and question and issue prefer a wh-question
clause. We observed that the pattern because X is
common with reason. The subordinating conjunc-
tion feature encodes these preferences for different
shell nouns. The feature checks whether the candi-
date follows the pattern SBAR —* (IN sconj) (S ...),
where sconj is a subordinating conjunction.
Verb features (V) A prominent property of CSN
and ASN antecedents is that they tend to contain
verbs. All examples from Table 2, for example, con-
tain verbs. Moreover, certain shell nouns have tense
and aspect preferences. For instance, for shell noun
fact, lexical verbs in past and present tenses predom-
inate (Schmid, 2000), whereas modal forms are ex-
tremely common for possibility. We use three verb
features that capture this idea: (a) presence of verbs
in general, (b) whether the main verb is finite or non-
finite, and (c) presence of modals.
Length features (L) The intuition behind these
features is that CSN and ASN antecedents tend to be
long, especially for nouns such as fact. We consider
two length features: (a) length of the candidate in
words, and (b) relative length of the candidate with
respect to the sentence containing the antecedent.
Lexical features (LX) Our extractor gives us a
large number of antecedent examples for each shell
noun. A natural question is whether certain words
tend to occur more frequently in the antecedent than
non-antecedent parts of the sentence. To deal with
this question, we extracted all antecedent unigrams
(i.e., unigrams occurring in antecedent part of the
sentence) and non-antecedent unigrams (i.e., uni-
grams occurring in non-antecedent parts of the sen-
tence) for each shell noun. Then for all antecedent
unigrams for a particular shell noun, we computed
term goodness in terms of information gain (Yang
and Pedersen, 1997) and considered the first 50
highly ranked unigrams as the lexical features for
that noun. Note that, in contrast with the other fea-
tures, these lexical features are tailored for each shell
noun and are extracted a priori.
</bodyText>
<page confidence="0.997936">
305
</page>
<subsectionHeader confidence="0.493788">
5.3.3 Candidate ranking models
</subsectionHeader>
<bodyText confidence="0.999990619047619">
Now that we have the set of candidate antecedents
and a set of features, we are ready to train CSN an-
tecedent models. We follow the candidate-ranking
models proposed by Denis and Baldridge (2008) be-
cause they allow us to evaluate how good an an-
tecedent candidate is relative to all other candidates.
For every shell noun, we gather automatically ex-
tracted antecedent data given by the extractor for all
instances of that shell noun. Then for each instance
in this data, we extract the set C as explained in
Section 5.3.1. For each candidate Ci E C, we ex-
tract a feature vector to create a corresponding set of
feature vectors, Cf = {Cf1,Cf2,...,Cfk}. For every
CSN ai and a set of feature vectors corresponding
to its eligible candidates Cf = {Cf1,Cf2,...,Cfk},
we create training examples (ai,Cfi,rank),bCfi E
Cf. The rank is 1 if Ci is same as the true an-
tecedent, i.e., the automatically extracted antecedent
for that CSN, otherwise the rank is 2. We use the
svm rank learn call of SVMrank (Joachims, 2002)
for training the candidate-ranking models.
</bodyText>
<sectionHeader confidence="0.910635" genericHeader="method">
6 Testing phase
</sectionHeader>
<bodyText confidence="0.9998225">
In this phase, we use the learned candidate ranking
models to identify the antecedents of ASNs.
</bodyText>
<subsectionHeader confidence="0.995473">
6.1 The ASN corpus
</subsectionHeader>
<bodyText confidence="0.9999908">
We started with about 450 instances for each of the
six selected shell nouns (2,700 total instances), con-
taining the pattern {this shell noun}. The instances
were extracted from the NYT. Each instance con-
tains three paragraphs from the corresponding NYT
article: the paragraph containing the ASN and two
preceding paragraphs as context. After automati-
cally removing duplicates and ASNs with a non-
abstract sense (e.g., this issue with a publication-
related sense), we were left with 2,323 instances.
</bodyText>
<subsectionHeader confidence="0.999115">
6.2 Antecedent identification
</subsectionHeader>
<bodyText confidence="0.999926673913044">
Candidate extraction The search space of ASN
antecedents is quite large for two reasons: ASNs
tend to have long-distance as well as short-distance
antecedents, and there is no clear restriction on the
syntactic type of the antecedents. In the ASN cor-
pus, each sentence on average had 49.5 distinct syn-
tactic constituents given by the Stanford parser. If
we consider n preceding sentences, the sentence
containing the anaphor, and one following sentence8
as sources for antecedents, then the average num-
ber of antecedent candidates will be 49.5 x (n + 2).
This is large compared to the search space of ordi-
nary nominal anaphora. In our previous work (Kol-
hatkar et al., 2013), we have developed methods that
identify the sentence containing the antecedent of
the ASN before identifying the precise antecedent.
In brief, given a set of a fixed number of sentences
around the sentence containing an ASN, these meth-
ods reliably identify the sentence containing the an-
tecedent. In this paper, we treat these methods as a
black box.
Given the sentence containing the antecedent,
we extract all syntactic constituents given by the
Stanford parser from that sentence as potential an-
tecedent candidates as for the training phase. In
the training phase, the antecedent is always con-
tained in the set of syntactic constituents given by
the Stanford parser because the extractor obtains the
appropriate antecedent using the syntactic informa-
tion. But in the testing phase, we cannot guarantee
that the true antecedent occurs in the extracted syn-
tactic constituents due to the parser’s errors. So for
robust candidate extraction, we extract all distinct
constituents from the 30-best parses instead of only
considering the best parse, which increases the aver-
age number of candidates from 49.5 to 55.2.
Feature extraction and candidate ranking
Given the antecedent candidates, feature extraction
and candidate ranking are essentially the same as
for the training phase, except of course we do not
know the true antecedent. Once we have the feature
vectors for each antecedent candidate, the appro-
priate trained model, i.e., the model trained for the
corresponding shell noun, is invoked and the can-
didates are ranked using the svm rank classify
call of SVMrank.
</bodyText>
<sectionHeader confidence="0.996244" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.9937535">
We evaluate the ranked candidates of ASN instances
using crowdsourcing.
</bodyText>
<footnote confidence="0.951943">
8The ASN corpus contains a few cataphoric examples that
do not follow the standard patterns of the CSNs shown in Table
2, but actually refer to an antecedent in the following sentence
(e.g., Mr. Dukakis put this question to him: X).
</footnote>
<page confidence="0.998325">
306
</page>
<bodyText confidence="0.999965822222222">
Interface We chose to use CrowdFlower9 as our
crowdsourcing interface because of its integrated
quality-control mechanism. For instance, it throws
gold questions randomly at the workers and the
workers who do not answer them correctly are not
allowed to continue.
We presented to the crowd evaluators the ASN
instances from the ASN corpus. Recall that each
ASN instance is made up of the paragraph contain-
ing the ASN and two preceding paragraphs as con-
text. We displayed the first 10 highly-ranked candi-
dates (ordered randomly) given by our testing phase
and asked the evaluators to choose the best answer
that represents the ASN antecedent. We encouraged
the evaluators to select None when they did not agree
with any of the displayed answers. We also asked
them how satisfied they were with the displayed an-
swers. We provided them with three options: unsat-
isfied, satisfied, and partially satisfied.
Our job contained 2,323 evaluation units. We
asked for 8 judgements per instance and paid 6
cents per evaluation unit. As we were interested
in the verdict of native speakers of English, we
limited the allowed demographic region to English-
speaking countries.
Results Among the 2,323 ASN instances, 96% of
them were labelled as satisfied, 3% as partially satis-
fied and 1% as unsatisfied. Only 2% of the instances
were labelled as None. As expected, evaluators were
unsatisfied or partially satisfied with the options of
these instances. These results suggest that our res-
olution models trained on automatically extracted
antecedents of CSNs bring the relevant candidates
of ASN antecedents to the top, i.e., within first 10
highly-ranked candidates. This itself is a positive re-
sult given the large search space of ASN antecedent
candidates (more than 55 candidates on average).
Among the evaluation units, more than half of the
evaluators agreed on an answer for 1,810 units. We
used these instances for further analysis.
To examine which CSN antecedent features are
relevant in identifying ASN antecedents, we carried
out ablation experiments with all feature class com-
binations. We compared the rankings given by our
ranker to the crowd’s answer using precision at n
</bodyText>
<footnote confidence="0.850654">
9http://crowdflower.com/
</footnote>
<bodyText confidence="0.9866601875">
(P@n).10 More specifically, we count the number of
instances where the crowd’s answers occur within
our ranker’s first n choices. P@n then is this count
divided by the total number of instances. Note that
P@1 is equivalent to the standard precision.
We compared our results against two baselines:
preceding sentence and chance. The preceding sen-
tence baseline chooses the previous sentence as the
correct antecedent. The chance baseline chooses a
candidate from a uniform random distribution over
the set of 10 top-ranked candidates.
The results are shown in Table 4. Although dif-
ferent feature combinations gave the best results for
different shell nouns, the features that occur fre-
quently in many best-performing combinations were
embedding level (E), lexical (LX), and subordinat-
ing conjunction (SC) features. The SC features were
particularly effective for issue and question, where
we expected patterns such as whether X.
Surprisingly, the syntactic type features (S) did
not show up very often in the best-performing fea-
ture combinations, suggesting that the ASN an-
tecedents had a greater variety of syntactic types
than what was available in our CSN training data.
The context features (C) did not appear in any of
the best-performing feature combinations. In fact,
they resulted in a sharp decline in the precision. For
instance, for question, adding the context features
to the best-performing combination {E,SC,V,L,LX}
resulted in a drop of 16 percentage points. This
result was not surprising because although the an-
tecedents of ASNs and CSNs share similar proper-
ties such as common words, we know that their con-
text is generally different.
We did not observe specific features associated
with Schmid’s semantic categories. An exception
was the E features which were particularly effective
for the factual nouns fact and reason: the results
with them alone gave high precision (0.68 for fact
and 0.72 for reason). That said, the E features were
present in most of the best-performing combinations
even for the shell nouns in other semantic categories.
10CrowdFlower gives us a unique answer for each instance,
which we take to be the crowd’s answer. During annotation, ev-
ery annotator is presented with a few gold questions randomly
and each annotator is assigned a trust score based on her per-
formance on these gold questions. The unique answer for an
instance is the answer with the highest sum of trusts.
</bodyText>
<page confidence="0.992506">
307
</page>
<table confidence="0.997879222222222">
fact (43,000 train and 472 test instances) reason (4,520 train and 443 test instances)
Features P@1 P@2 P@3 P@4 Features P@1 P@2 P@3 P@4
{E,L,LX} .70* .85 .91 .94 {E,V,L} .72* .86 .90 .93
{E,V,L,LX} .68* .86 .92 .95 {E,V} .72* .85 .90 .92
{E,SC,L,LX} .66* .83 .92 .95 {E,SC,LX} .69* .84 .90 .94
PSbaseline .40 – – – PSbaseline .44 – – –
issue (3,000 train and 303 test instances) decision (42,332 train and 390 test instances)
Features P@1 P@2 P@3 P@4 Features P@1 P@2 P@3 P@4
{SC,L} .47* .59 .71 .78 {E,LX} .35* .53 .67 .76
{SC,L,LX} .46* .60 .70 .81 {E,SC,LX} .30* .48 .65 .75
{S,E,SC,L,LX}.40* .61 .72 .81 {E,SC,V,L,LX}.27 .44 .57 .69
PSbaseline .30 – – – PSbaseline .21 – – –
question (9,336 train and 440 test instances) possibility (11,735 train and 278 test instances)
Features P@1 P@2 P@3 P@4 Features P@1 P@2 P@3 P@4
{E,SC,V,L,LX} .70* .82 .87 .90 {SC,L,LX} .56* .75 .87 .92
{E,SC,LX} .68* .83 .88 .91 {E,SC} .56* .76 .87 .91
{E,SC,V,LX} .69* .80 .87 .91 {E,L,LX} .54* .76 .86 .91
PSbaseline .25 – – – PSbaseline .34 – – –
</table>
<tableCaption confidence="0.996868">
Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing
</tableCaption>
<bodyText confidence="0.971797166666667">
feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the
column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked
with *(two-sample x2 test: p &lt; 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,
and P@4, respectively.
The only previous work with which our results
could be compared is that of Kolhatkar and Hirst
(2012). The work reports precision in the range
of 0.41 to 0.61 in resolving this issue anaphora in
the Medline domain. In our case, for this issue in-
stances from the NYT corpus, we achieved precision
in the range of 0.40 to 0.47. Furthermore, we ap-
plied our models to resolve this issue instances from
Kolhatkar and Hirst’s (2012) work.11 Even with
models trained on automatically labelled CSN an-
tecedents, we achieved similar results to Kolhatkar
and Hirst’s results: P@1 of 0.45, P@2 of 0.59, P@3
of 0.65, and P@4 of 0.67. These results show the
domain robustness of our methods with respect to
the shell noun issue. Recall that Kolhatkar and Hirst
(2012) looked at only very specific cases of this is-
sue and used manually annotated data (Section 2),
as opposed to the automatically extracted CSN an-
tecedent data we use.
11We thank an anonymous reviewer for suggesting this to us.
</bodyText>
<sectionHeader confidence="0.989792" genericHeader="discussions">
8 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999986142857143">
The goal of this paper was to examine to what ex-
tent CSNs help in interpreting ASNs. Based on the
evaluators’ satisfaction level and very few None re-
sponses, we conclude that our models trained on
CSN antecedents were able to bring the relevant
ASN antecedent candidates into the top 10 candi-
dates.
When we applied the models trained on CSN an-
tecedents to interpret ASNs, we achieved precision
in the range of 0.35 to 0.72. The precision results as
high as 0.72 for reason and 0.70 for fact and ques-
tion support our hypothesis that the linguistic knowl-
edge provided by CSN antecedents helps in identify-
ing the antecedents of ASNs. We observed different
behaviour for different nouns. The mental nouns is-
sue and decision in general were harder to interpret
than other shell nouns. The models trained on CSNs
achieved precisions of 0.35 for decision and 0.47 for
issue. So there is still much room for improvement.
That said, for the same nouns, the antecedents were
in the first four ranks about 76% to 81% of the times,
</bodyText>
<page confidence="0.996541">
308
</page>
<bodyText confidence="0.997892018518519">
suggesting that in future research, our models can be
used as base models to reduce the large search space
of ASN antecedent candidates.
We observed a wide range of performance for dif-
ferent shell nouns. One reason is that the size of the
training data was different for different shell nouns.
After excluding the noisy examples (Section 5.2),
there were about 43,000 training examples for fact,
but only about 3,000 for issue. In addition, a par-
ticular shell concept itself can be difficult, e.g., the
very idea of what counts as an issue is more fuzzy
than what counts as a fact.
One limitation of our approach is that it only
learns the properties that are present in CSN an-
tecedents. However, ASN antecedents have addi-
tional properties which are not always captured by
CSN antecedents. For instance, for the shell noun
decision, most of the training examples were infini-
tive phrases of the form to X. But antecedents of the
ASN decision were mostly court decisions and were
expressed with full sentences.
Moreover, although the models trained on CSN
antecedents are able to encode characteristic fea-
tures associated with the general shell concept, they
are unable to distinguish between two competing
candidates both containing the characteristic fea-
tures of that shell concept. For instance, our ap-
proach will not be able to handle the constructed ex-
amples in (7).
(7) The teacher erased the solutions before John had
time to copy them out, as he had momentarily
been distracted by a band playing outside.
a) This fact infuriated him, as the teacher al-
ways erased the board quickly and John sus-
pected it was just to punish anyone who was
lost in thought, even for a moment.
b) This fact infuriated the teacher, who had al-
ready told John several times to focus on
class work.
Here, both propositions possess properties of the
shell concept fact. Understanding the context of the
anaphor itself is crucial in correctly identifying the
fact in each case, which cannot be learnt from CSN
antecedents due to their specific context patterns.
A number of extensions are planned for this work.
First, we plan to use both kinds of data, CSN and
ASN antecedent data, which will give us a basis
for developing a better performing ASN resolver.
We also plan to incorporate contextual features (e.g.,
right-frontier rule (Webber, 1991) and context rank-
ing (Eckert and Strube, 2000)). Finally, we will ex-
amine whether a model trained for one shell noun
can be generalized to other shell nouns from the
same semantic category.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999876">
We thank the anonymous reviewers for their
constructive comments. This material is based
upon work supported by the United States Air
Force and the Defense Advanced Research Projects
Agency under Contract No. FA8650-09-C-0179,
Ontario/Baden-W¨urttemberg Faculty Research Ex-
change, and the University of Toronto.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887161290322">
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660–669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51–89.
Gill Francis. 1986. Anaphoric Nouns. Discourse Anal-
ysis Monographs 11, Birmingham: English Language
Research, University of Birmingham.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Pub Group.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539–545, Nantes, France. Associa-
tion for Computational Linguistics.
Roz Ivanic. 1991. Nouns in search of a context: A study
of nouns with both open- and closed-system character-
istics. International Review of Applied Linguistics in
Language Teaching, 29:93–114.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
</reference>
<page confidence="0.990745">
309
</page>
<reference confidence="0.999242810344828">
on Knowledge Discovery and Data Mining (KDD),
pages 133–142.
Varada Kolhatkar and Graeme Hirst. 2012. Resolv-
ing “this-issue” anaphora. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1255–1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst.
2013. Annotating anaphoric shell nouns with their an-
tecedents. In Proceedings of the 7th Linguistic Anno-
tation Workshop and Interoperability with Discourse,
pages 112–121, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368–375, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313–330.
Christoph M¨uller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universit¨at T¨ubingen.
Rebecca J. Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 51–
59, Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-J¨org Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics 34. Mouton de Gruyter, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521–544.
Michael Strube and Christoph M¨uller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168–175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton de Gruyter, The Hague.
Bonnie Lynn Webber. 1991. Structure and ostension
in the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107–135.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of the 14th International Conference on
Machine Learning, pages 412–420, Nashville, TN.
</reference>
<page confidence="0.998595">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672442">
<title confidence="0.932062">Interpreting Anaphoric Shell Nouns using Antecedents Cataphoric Shell Nouns as Training Data</title>
<author confidence="0.98172">Varada Kolhatkar Heike Zinsmeister</author>
<affiliation confidence="0.999705">Department of Computer Science Institut f¨ur Germanistik University of Toronto Universit¨at Hamburg</affiliation>
<email confidence="0.91873">varada@cs.toronto.eduheike.zinsmeister@uni-hamburg.de</email>
<author confidence="0.876011">Graeme</author>
<affiliation confidence="0.9997845">Department of Computer University of</affiliation>
<email confidence="0.999098">gh@cs.toronto.edu</email>
<abstract confidence="0.998147388888889">shell nouns such as issue fact is essential to understanding virtually any substantial natural language text. One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data. We tackle challenge by exploiting nouns whose construction makes them particularly easy to interpret fact that We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs. We achieve precisions in the range of 0.35 (baseline = 0.21) to 0.72 (baseline = 0.44), depending upon the shell noun.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse.</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="2182" citStr="Asher, 1993" startWordPosition="326" endWordPosition="327">p of the clausal antecedent marked in bold. The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 225 million words of British English. Moreov</context>
<context position="5435" citStr="Asher, 1993" startWordPosition="851" endWordPosition="852">ant problem which requires a solution. In this work, we propose an approach to interpret ASNs that exploits unlabelled but easy-to-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts. We evaluate our approach using crowdsourcing. Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal a</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Nicholas Asher. 1993. Reference to Abstract Objects in Discourse. Kluwer Academic Publishers, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
</authors>
<title>Resolving pronominal reference to abstract entities.</title>
<date>2004</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Rochester, New York: University of Rochester.</location>
<contexts>
<context position="5844" citStr="Byron, 2004" startWordPosition="911" endWordPosition="912">ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal antecedents. Recently, Kolhatkar and Hirst (2012) presented a machine-learning based resolution system for this issue anaphora, identifying full syntactic phrases as antecedents. Although they achieved promising results, their approach was limited in two respects. First, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data was restricted to MEDLINE abstracts in wh</context>
</contexts>
<marker>Byron, 2004</marker>
<rawString>Donna K. Byron. 2004. Resolving pronominal reference to abstract entities. Ph.D. thesis, Rochester, New York: University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="24565" citStr="Denis and Baldridge (2008)" startWordPosition="3929" endWordPosition="3932">sentence) for each shell noun. Then for all antecedent unigrams for a particular shell noun, we computed term goodness in terms of information gain (Yang and Pedersen, 1997) and considered the first 50 highly ranked unigrams as the lexical features for that noun. Note that, in contrast with the other features, these lexical features are tailored for each shell noun and are extracted a priori. 305 5.3.3 Candidate ranking models Now that we have the set of candidate antecedents and a set of features, we are ready to train CSN antecedent models. We follow the candidate-ranking models proposed by Denis and Baldridge (2008) because they allow us to evaluate how good an antecedent candidate is relative to all other candidates. For every shell noun, we gather automatically extracted antecedent data given by the extractor for all instances of that shell noun. Then for each instance in this data, we extract the set C as explained in Section 5.3.1. For each candidate Ci E C, we extract a feature vector to create a corresponding set of feature vectors, Cf = {Cf1,Cf2,...,Cfk}. For every CSN ai and a set of feature vectors corresponding to its eligible candidates Cf = {Cf1,Cf2,...,Cfk}, we create training examples (ai,C</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660–669, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Eckert</author>
<author>Michael Strube</author>
</authors>
<title>Dialogue acts, synchronizing units, and anaphora resolution.</title>
<date>2000</date>
<journal>Journal of Semantics,</journal>
<pages>17--51</pages>
<contexts>
<context position="5805" citStr="Eckert and Strube, 2000" startWordPosition="903" endWordPosition="906">ul linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal antecedents. Recently, Kolhatkar and Hirst (2012) presented a machine-learning based resolution system for this issue anaphora, identifying full syntactic phrases as antecedents. Although they achieved promising results, their approach was limited in two respects. First, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data wa</context>
</contexts>
<marker>Eckert, Strube, 2000</marker>
<rawString>Miriam Eckert and Michael Strube. 2000. Dialogue acts, synchronizing units, and anaphora resolution. Journal of Semantics, 17:51–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gill Francis</author>
</authors>
<title>Anaphoric Nouns. Discourse Analysis Monographs 11,</title>
<date>1986</date>
<institution>English Language Research, University of Birmingham.</institution>
<location>Birmingham:</location>
<contexts>
<context position="2154" citStr="Francis, 1986" startWordPosition="322" endWordPosition="323">t is interpreted with the help of the clausal antecedent marked in bold. The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 225 million word</context>
</contexts>
<marker>Francis, 1986</marker>
<rawString>Gill Francis. 1986. Anaphoric Nouns. Discourse Analysis Monographs 11, Birmingham: English Language Research, University of Birmingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Pub Group.</publisher>
<contexts>
<context position="2139" citStr="Halliday and Hasan, 1976" startWordPosition="318" endWordPosition="321">Here, the anaphor this fact is interpreted with the help of the clausal antecedent marked in bold. The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 2</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman Pub Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora. In</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nantes, France.</location>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In In Proceedings of the 14th International Conference on Computational Linguistics, pages 539–545, Nantes, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roz Ivanic</author>
</authors>
<title>Nouns in search of a context: A study of nouns with both open- and closed-system characteristics.</title>
<date>1991</date>
<booktitle>International Review of Applied Linguistics in Language Teaching,</booktitle>
<pages>29--93</pages>
<contexts>
<context position="2168" citStr="Ivanic, 1991" startWordPosition="324" endWordPosition="325">d with the help of the clausal antecedent marked in bold. The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 225 million words of British E</context>
</contexts>
<marker>Ivanic, 1991</marker>
<rawString>Roz Ivanic. 1991. Nouns in search of a context: A study of nouns with both open- and closed-system characteristics. International Review of Applied Linguistics in Language Teaching, 29:93–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>133--142</pages>
<contexts>
<context position="25379" citStr="Joachims, 2002" startWordPosition="4074" endWordPosition="4075"> for all instances of that shell noun. Then for each instance in this data, we extract the set C as explained in Section 5.3.1. For each candidate Ci E C, we extract a feature vector to create a corresponding set of feature vectors, Cf = {Cf1,Cf2,...,Cfk}. For every CSN ai and a set of feature vectors corresponding to its eligible candidates Cf = {Cf1,Cf2,...,Cfk}, we create training examples (ai,Cfi,rank),bCfi E Cf. The rank is 1 if Ci is same as the true antecedent, i.e., the automatically extracted antecedent for that CSN, otherwise the rank is 2. We use the svm rank learn call of SVMrank (Joachims, 2002) for training the candidate-ranking models. 6 Testing phase In this phase, we use the learned candidate ranking models to identify the antecedents of ASNs. 6.1 The ASN corpus We started with about 450 instances for each of the six selected shell nouns (2,700 total instances), containing the pattern {this shell noun}. The instances were extracted from the NYT. Each instance contains three paragraphs from the corresponding NYT article: the paragraph containing the ASN and two preceding paragraphs as context. After automatically removing duplicates and ASNs with a nonabstract sense (e.g., this is</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Varada Kolhatkar</author>
<author>Graeme Hirst</author>
</authors>
<title>Resolving “this-issue” anaphora.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1255--1265</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6083" citStr="Kolhatkar and Hirst (2012)" startWordPosition="942" endWordPosition="945"> anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal antecedents. Recently, Kolhatkar and Hirst (2012) presented a machine-learning based resolution system for this issue anaphora, identifying full syntactic phrases as antecedents. Although they achieved promising results, their approach was limited in two respects. First, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data was restricted to MEDLINE abstracts in which this issue is used in a rather systematic way. Furthermore, their work is based on manually labelled ASN antecedents, whereas we use automatically identified CSN antecedents, which we interpret as explicitly expressed antecedents in co</context>
<context position="34882" citStr="Kolhatkar and Hirst (2012)" startWordPosition="5623" endWordPosition="5626">6 .91 PSbaseline .25 – – – PSbaseline .34 – – – Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked with *(two-sample x2 test: p &lt; 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3, and P@4, respectively. The only previous work with which our results could be compared is that of Kolhatkar and Hirst (2012). The work reports precision in the range of 0.41 to 0.61 in resolving this issue anaphora in the Medline domain. In our case, for this issue instances from the NYT corpus, we achieved precision in the range of 0.40 to 0.47. Furthermore, we applied our models to resolve this issue instances from Kolhatkar and Hirst’s (2012) work.11 Even with models trained on automatically labelled CSN antecedents, we achieved similar results to Kolhatkar and Hirst’s results: P@1 of 0.45, P@2 of 0.59, P@3 of 0.65, and P@4 of 0.67. These results show the domain robustness of our methods with respect to the shel</context>
</contexts>
<marker>Kolhatkar, Hirst, 2012</marker>
<rawString>Varada Kolhatkar and Graeme Hirst. 2012. Resolving “this-issue” anaphora. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1255–1265, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Varada Kolhatkar</author>
<author>Heike Zinsmeister</author>
<author>Graeme Hirst</author>
</authors>
<title>Annotating anaphoric shell nouns with their antecedents.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>112--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="26755" citStr="Kolhatkar et al., 2013" startWordPosition="4294" endWordPosition="4298">edents is quite large for two reasons: ASNs tend to have long-distance as well as short-distance antecedents, and there is no clear restriction on the syntactic type of the antecedents. In the ASN corpus, each sentence on average had 49.5 distinct syntactic constituents given by the Stanford parser. If we consider n preceding sentences, the sentence containing the anaphor, and one following sentence8 as sources for antecedents, then the average number of antecedent candidates will be 49.5 x (n + 2). This is large compared to the search space of ordinary nominal anaphora. In our previous work (Kolhatkar et al., 2013), we have developed methods that identify the sentence containing the antecedent of the ASN before identifying the precise antecedent. In brief, given a set of a fixed number of sentences around the sentence containing an ASN, these methods reliably identify the sentence containing the antecedent. In this paper, we treat these methods as a black box. Given the sentence containing the antecedent, we extract all syntactic constituents given by the Stanford parser from that sentence as potential antecedent candidates as for the training phase. In the training phase, the antecedent is always conta</context>
</contexts>
<marker>Kolhatkar, Zinsmeister, Hirst, 2013</marker>
<rawString>Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst. 2013. Annotating anaphoric shell nouns with their antecedents. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 112–121, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>368--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6927" citStr="Marcu and Echihabi (2002)" startWordPosition="1069" endWordPosition="1072">st, it focused on only one type of shell noun anaphora (issue anaphora). Second, their training data was restricted to MEDLINE abstracts in which this issue is used in a rather systematic way. Furthermore, their work is based on manually labelled ASN antecedents, whereas we use automatically identified CSN antecedents, which we interpret as explicitly expressed antecedents in comparison to the more implicitly expressed ASN antecedents. Using explicitly expressed structure in the text to identify implicit structure is not new. The same idea has been applied before in computational linguistics. Marcu and Echihabi (2002) identified implicit discourse relations using explicit ones. Markert and Nissim (2005) used Hearst’s (1992) explicit patterns to learn lexical semantic relations for NPcoreference and other-anaphora resolution from the web. Although our work focuses on a different topic, the methodology is in the same vein. 3 Hypothesis of this work The hypothesis of this work is that CSN antecedents and ASN antecedents share some linguistic properties and hence linguistic knowledge encoded in CSN antecedents will help in interpreting ASNs. Accordingly, we examine which features present in CSN antecedents are</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 368–375, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="18173" citStr="Marcus et al., 1993" startWordPosition="2885" endWordPosition="2888">r. For the N-wh pattern, we exclude certain wh words for certain nouns. For example, we exclude the wh word which for question as the Penn Treebank tagset7 does not distinguish between which as a relative pronoun and as a question. We are interested in the latter but not the former. Other discarded wh words include which and when for fact; all wh words except when and why for reason, all wh words except how and whether for issue; which, whom, when, and why for decision; which and when for question; and all wh words for possibility. 7The Stanford tagger we employ uses the Penn Treebank tagset (Marcus et al., 1993). To address the second goal of allowing a wide variety of antecedent examples, we try to include as many patterns as possible for each shell noun, as shown in Table 3. For instance, the patterns question-to and question-be-to will have infinitive clauses as antecedents (marked as VP or S+VP by the parser), whereas for the examples following patterns question-wh and question-be-wh the antecedent will be in the wh clauses (marked as SBAR). For the pattern question-that, the antecedent will be in the predicate (similar to example (3)), which can be a prepositional phrase, a noun phrase or a clau</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
</authors>
<title>Fully Automatic Resolution of It, This and That in Unrestricted Multi-Party Dialog.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at T¨ubingen.</institution>
<marker>M¨uller, 2008</marker>
<rawString>Christoph M¨uller. 2008. Fully Automatic Resolution of It, This and That in Unrestricted Multi-Party Dialog. Ph.D. thesis, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Getting at discourse referents.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>51--59</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="5421" citStr="Passonneau, 1989" startWordPosition="849" endWordPosition="850">issue is an important problem which requires a solution. In this work, we propose an approach to interpret ASNs that exploits unlabelled but easy-to-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts. We evaluate our approach using crowdsourcing. Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for</context>
</contexts>
<marker>Passonneau, 1989</marker>
<rawString>Rebecca J. Passonneau. 1989. Getting at discourse referents. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 51– 59, Vancouver, British Columbia, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Simone Ponzetto</author>
<author>Yannick Versley</author>
</authors>
<title>Computational models of anaphora resolution: A survey.</title>
<date>2011</date>
<publisher>Unpublished.</publisher>
<contexts>
<context position="5573" citStr="Poesio et al., 2011" startWordPosition="870" endWordPosition="873">-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts. We evaluate our approach using crowdsourcing. Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only verbal proxies for non-nominal antecedents. Recently, Kolhatkar and Hirst (2012) presented a machine-learning based resolution system for this issue anaphora, identifying</context>
</contexts>
<marker>Poesio, Ponzetto, Versley, 2011</marker>
<rawString>Massimo Poesio, Simone Ponzetto, and Yannick Versley. 2011. Computational models of anaphora resolution: A survey. Unpublished.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-J¨org Schmid</author>
</authors>
<title>English Abstract Nouns As Conceptual Shells: From Corpus to Cognition. Topics in English Linguistics 34. Mouton de Gruyter,</title>
<date>2000</date>
<location>Berlin.</location>
<contexts>
<context position="2252" citStr="Schmid (2000)" startWordPosition="340" endWordPosition="341">mplex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 225 million words of British English. Moreover, ASNs can play several roles in organizing a discourse such as enca</context>
<context position="8960" citStr="Schmid, 2000" startWordPosition="1396" endWordPosition="1397">ssed with clauses or sentences rather than noun phrases, and (b) they are generally expressed in the present tense. Observe that these properties also hold for the antecedent of this fact in example (1). We test our hypothesis by building machine learning models that are trained on automatically extracted CSN antecedents and then applying these models to recover ASN antecedents. Figure 1 shows an overview of our methodology. 4 Background Formal definition Shell-nounhood is a functional notion; it is defined by the use of an abstract noun rather than the inherent properties of the noun itself (Schmid, 2000). An abstract noun is a shell noun when the speaker decides to use it as a shell noun. Shell noun categorization Schmid (2000) gives a list of 670 English nouns which are frequently used as shell nouns. He divides them into six broad semantic classes: factual, linguistic, mental, modal, eventive, and circumstantial. Table 1 shows this classification, along with example shell nouns for each category. For this work, we selected six frequently occurring shell nouns covering four of Schmid’s six classes: fact and reason from factual, issue and decision from mental, question from linguistic, and po</context>
<context position="10834" citStr="Schmid (2000)" startWordPosition="1692" endWordPosition="1693">troversial internally. N-be-to The principal reason is to create a representative government rather than to select the most talented person. N-that Mr. Shoval left open the possibility that Israel would move into other West Bank cities. N-be-that The simple and reassuring fact is that a future generation of leaders is seeking new challenges during challenging times. N-wh There is now some question whether the country was ever really in a recession. N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel. Table 2: Easy-to-interpret CSN patterns given by Schmid (2000). In the Example column, the patterns are marked in boldface and the antecedents are marked in italics. tial classes because the shell nouns in these classes tend to have rather unclear and long antecedents.2 Shell noun patterns Schmid (2000) also provides a number of lexico-grammatical patterns for shell nouns. In Section 1, we noted two such patterns: this-N (this fact in example (1)) and N-that (fact that in example (2)). We also noted that CSNs with pattern N-that are fairly easy to interpret compared to the ASN pattern this-N. Table 2 shows some other easy-to-interpret CSN patterns given </context>
<context position="22957" citStr="Schmid, 2000" startWordPosition="3666" endWordPosition="3667">e. We observed that the pattern because X is common with reason. The subordinating conjunction feature encodes these preferences for different shell nouns. The feature checks whether the candidate follows the pattern SBAR —* (IN sconj) (S ...), where sconj is a subordinating conjunction. Verb features (V) A prominent property of CSN and ASN antecedents is that they tend to contain verbs. All examples from Table 2, for example, contain verbs. Moreover, certain shell nouns have tense and aspect preferences. For instance, for shell noun fact, lexical verbs in past and present tenses predominate (Schmid, 2000), whereas modal forms are extremely common for possibility. We use three verb features that capture this idea: (a) presence of verbs in general, (b) whether the main verb is finite or nonfinite, and (c) presence of modals. Length features (L) The intuition behind these features is that CSN and ASN antecedents tend to be long, especially for nouns such as fact. We consider two length features: (a) length of the candidate in words, and (b) relative length of the candidate with respect to the sentence containing the antecedent. Lexical features (LX) Our extractor gives us a large number of antece</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Hans-J¨org Schmid. 2000. English Abstract Nouns As Conceptual Shells: From Corpus to Cognition. Topics in English Linguistics 34. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="20226" citStr="Soon et al., 2001" startWordPosition="3218" endWordPosition="3221">ract all syntactic constituents of this sentence, given by the Stanford parser. All the syntactic constituents, except the true antecedent, are considered as negative examples. With this candidate extraction method, we end up with many more negative exam304 ples than positive examples, but that is exactly what we expect with ASN antecedent candidates, i.e., the test data on which we will be applying our models. 5.3.2 Features Although our problem is similar to anaphora resolution, we cannot make use of the usual anaphora or coreference resolution features such as agreement or string matching (Soon et al., 2001) because of the nature of ASN and CSN antecedents. We came up with a set of features based on the properties that were common in both ASN and CSN antecedents, according to our judgement. Syntactic type of the candidate (S) We observed that each shell noun prefers specific CSN patterns and each pattern involves a particular syntactic type. For instance, decision prefers the pattern N-to and consequently realizes as its antecedents more verb phrases than, for example, noun phrases. We employ two versions of syntactic type: fine-grained syntactic type given by the Stanford parser (e.g., NP-TMP, R</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Christoph M¨uller</author>
</authors>
<title>A machine learning approach to pronoun resolution in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<marker>Strube, M¨uller, 2003</marker>
<rawString>Michael Strube and Christoph M¨uller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 168–175, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Adjectives and Nominalizations. Mouton de Gruyter, The Hague.</title>
<date>1968</date>
<contexts>
<context position="2113" citStr="Vendler, 1968" startWordPosition="316" endWordPosition="317">owered mowers. Here, the anaphor this fact is interpreted with the help of the clausal antecedent marked in bold. The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity. The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in (1)) which characterize and label their corresponding antecedents. Linguists and philosophers have studied such abstract nouns for decades (Vendler, 1968; Halliday and Hasan, 1976; Francis, 1986; Ivanic, 1991; Asher, 1993). Our work is inspired by one such study, namely that of Schmid (2000). Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information. Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs). An important reason for studying ASNs is their ubiquity in all kinds of text. Schmid (2000) observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurr</context>
<context position="22236" citStr="Vendler (1968)" startWordPosition="3548" endWordPosition="3549">op clause (the root node), and immediate embedding level is the level of embedding with respect to its immediate clause (the closest ancestor of type S or SBAR). The intuition behind this feature is that if the candidate is deep in the parse tree, it is possibly not salient enough to be an antecedent. As we consider all syntactic constituents as potential candidates, there are many that clearly cannot be antecedents. This feature will allow us to get rid of this noise. Subordinating conjunctions (SC) As we can see in Table 2, subordinating conjunctions are common with CSN and ASN antecedents. Vendler (1968) points out that the shell noun fact prefers a thatclause, and question and issue prefer a wh-question clause. We observed that the pattern because X is common with reason. The subordinating conjunction feature encodes these preferences for different shell nouns. The feature checks whether the candidate follows the pattern SBAR —* (IN sconj) (S ...), where sconj is a subordinating conjunction. Verb features (V) A prominent property of CSN and ASN antecedents is that they tend to contain verbs. All examples from Table 2, for example, contain verbs. Moreover, certain shell nouns have tense and a</context>
</contexts>
<marker>Vendler, 1968</marker>
<rawString>Zeno Vendler. 1968. Adjectives and Nominalizations. Mouton de Gruyter, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Lynn Webber</author>
</authors>
<title>Structure and ostension in the interpretation of discourse deixis.</title>
<date>1991</date>
<booktitle>In Language and Cognitive Processes,</booktitle>
<pages>107--135</pages>
<contexts>
<context position="5403" citStr="Webber, 1991" startWordPosition="846" endWordPosition="848"> notion of an issue is an important problem which requires a solution. In this work, we propose an approach to interpret ASNs that exploits unlabelled but easy-to-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts. We evaluate our approach using crowdsourcing. Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs. 2 Related work The resolution of anaphors to non-nominal antecedents has been well analyzed taking discourse structure and semantic types into account (Webber, 1991; Passonneau, 1989; Asher, 1993). Most work in machine anaphora resolution, however, is restricted to anaphora that involve nominal antecedents only (Poesio et al., 2011). 1Sadly, the more-logical term for the interpretation of a CSN, succedent, does not actually exist. There are some notable exceptions which have tackled the challenge of interpreting non-nominal antecedents (Eckert and Strube, 2000; Strube and M¨uller, 2003; Byron, 2004; M¨uller, 2008). These approaches are limited as they either rely heavily on domain-specific syntactic and semantic annotation or prepossessing, or mark only </context>
<context position="12668" citStr="Webber, 1991" startWordPosition="1980" endWordPosition="1981">n, have their antecedents in the same sentence, as shown in example (2). On the other hand, ASNs can have long-distance as well as short-distance antecedents.3 The common properties are as follows. First, antecedents of both ASNs and CSNs represent the corresponding shell concept, e.g., the notion of a fact or an issue. Second, in both cases, the antecedents are complex abstract entities, which involve a number of entities and relationships between them. Finally, in both cases, there is no one-to-one correspondence between the syntactic type of an antecedent and semantic type of its referent (Webber, 1991). For instance, a semantic type such as fact can be expressed with different syntactic shapes such as a clause, a verb phrase, or a complex sentence. Conversely, a syntactic shape, such as a clause, can function as several semantic types, including fact, proposition, and event. 5 Training phase As shown in Figure 1, the goal of the training phase is to build training data from CSNs and their antecedents and train models which can be used for resolving ASNs. 5.1 The CSN corpus We automatically constructed a corpus, a subset of the New York times (NYT) corpus4, which contains 211,722 sentences f</context>
</contexts>
<marker>Webber, 1991</marker>
<rawString>Bonnie Lynn Webber. 1991. Structure and ostension in the interpretation of discourse deixis. In Language and Cognitive Processes, pages 107–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning,</booktitle>
<pages>412--420</pages>
<location>Nashville, TN.</location>
<contexts>
<context position="24112" citStr="Yang and Pedersen, 1997" startWordPosition="3852" endWordPosition="3855">Lexical features (LX) Our extractor gives us a large number of antecedent examples for each shell noun. A natural question is whether certain words tend to occur more frequently in the antecedent than non-antecedent parts of the sentence. To deal with this question, we extracted all antecedent unigrams (i.e., unigrams occurring in antecedent part of the sentence) and non-antecedent unigrams (i.e., unigrams occurring in non-antecedent parts of the sentence) for each shell noun. Then for all antecedent unigrams for a particular shell noun, we computed term goodness in terms of information gain (Yang and Pedersen, 1997) and considered the first 50 highly ranked unigrams as the lexical features for that noun. Note that, in contrast with the other features, these lexical features are tailored for each shell noun and are extracted a priori. 305 5.3.3 Candidate ranking models Now that we have the set of candidate antecedents and a set of features, we are ready to train CSN antecedent models. We follow the candidate-ranking models proposed by Denis and Baldridge (2008) because they allow us to evaluate how good an antecedent candidate is relative to all other candidates. For every shell noun, we gather automatica</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the 14th International Conference on Machine Learning, pages 412–420, Nashville, TN.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
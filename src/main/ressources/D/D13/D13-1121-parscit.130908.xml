<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999452">
Automatic Knowledge Acquisition for Case Alternation
between the Passive and Active Voices in Japanese
</title>
<author confidence="0.997865">
Ryohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura1
</author>
<affiliation confidence="0.873361">
1 Precision and Intelligence Laboratory, Tokyo Institute of Technology
2 Graduate School of Informatics, Kyoto University
</affiliation>
<email confidence="0.997995">
{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928090909091">
We present a method for automatically acquir-
ing knowledge for case alternation between
the passive and active voices in Japanese. By
leveraging several linguistic constraints on al-
ternation patterns and lexical case frames ob-
tained from a large Web corpus, our method
aligns a case frame in the passive voice to a
corresponding case frame in the active voice
and finds an alignment between their cases.
We then apply the acquired knowledge to a
case alternation task and prove its usefulness.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966505882353">
Predicate-argument structure analysis is one of the
fundamental techniques for many natural language
applications such as recognition of textual entail-
ment, information retrieval, and machine transla-
tion. In Japanese, the relationship between a pred-
icate and its argument is usually represented by us-
ing case particles1 (Kawahara and Kurohashi, 2006;
Taira et al., 2008; Yoshikawa et al., 2011). However,
since case particles vary depending on the voices,
we have to take case alternation into account to rep-
resent predicate-argument structure. There are thus
two major types of representations: one uses surface
cases, and the other uses normalized-cases for the
base form of predicates. For example, while the Ky-
oto University Text Corpus (Kawahara et al., 2004),
one of the major Japanese corpora that contains an-
notations of predicate-argument structures, adopts
</bodyText>
<footnote confidence="0.925634666666667">
1Japanese is a head-final language. Word order does not
mark syntactic relations. Instead, postpositional case particles
function as case markers.
</footnote>
<bodyText confidence="0.973291076923077">
the former representation, the NAIST Text Corpora
(Iida et al., 2007), another major Japanese corpus,
adopts the latter representation.
Examples (1) and (2) describe the same event in
the passive and active voices, respectively. When
we use surface cases to represent the relationship be-
tween the predicate and its argument in Example (1),
the case of “女 (woman)” is ga2 and the case of “男
(man)” is ni.2 On the other hand, when we use the
normalized-cases for the base form, the case of “女
(woman)” is wo2 and the case of “男 (man)” is ga,
which are the same as the surface cases in the active
voice as in Example (2).
</bodyText>
<listItem confidence="0.257611">
(1) 女が 男に 突き落とされた.
woman-ga man-ni was pushed down
</listItem>
<bodyText confidence="0.898395294117647">
(A woman was pushed down by a man.)
(2) 男が 女を 突き落とした.
man-ga woman-wo pushed down
(A man pushed down a woman.)
Both representations have their own advantages.
Surface case analysis is easier than normalized-case
analysis, especially when we consider omitted ar-
guments, which are also called zero anaphors (Na-
gao and Hasida, 1998). In Japanese, zero anaphora
frequently occurs, and the omitted unnormalized-
case of a zero anaphor is often the same as the
surface case of its antecedent (Sasano and Kuro-
hashi, 2011). Therefore, surface case analysis suits
zero anaphora resolution. On the other hand, when
2Ga, wo, and ni are typical Japanese postpositional case par-
ticles. In most cases, they indicate nominative, accusative, and
dative, respectively.
</bodyText>
<page confidence="0.83148">
1213
</page>
<note confidence="0.7312">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999135523809524">
we focus on the resulting predicate argument struc-
tures, the normalized-case structure is more useful.
Specifically, since a normalized-case structure rep-
resents the same meaning in the same representa-
tion, normalized-case analysis is useful for recog-
nizing textual entailment and information retrieval.
Therefore, we need a system that first analyzes
surface cases and then alternates the surface cases
with normalized-cases. In particular, we focus on
the transformation of the passive voice into the ac-
tive voice in this paper. Passive-to-active voice
transformation in English can be performed system-
atically, which does not depend on lexical infor-
mation in most cases. However, in Japanese, the
method of transformation depends on lexical infor-
mation. For example, while the case particle ni in
Example (1) is alternated with ga in the active voice,
the case particle ni in Example (3) is not alternated in
the active voice as in Example (4) even though both
their predicates are“突き落とされた (be pushed
down).”
</bodyText>
<equation confidence="0.4759915">
(3) 女が 海に 突き落とされた.
woman-ga sea-ni was pushed down
(A woman was pushed down into the sea.)
(4) 女を 海に 突き落とした.
</equation>
<bodyText confidence="0.905929105263158">
woman-wo sea-ni pushed down
(φ pushed down a woman into the sea.)
The ni case in Example (1) indicates agent. On
the other hand, the ni case in Example (3) indicates
direction. To determine the difference is important
for many NLP applications including machine trans-
lation. In fact, Google Translate (GT)3 translates
Examples (1) and (3) as “Woman was pushed down
in the man” and “Woman was pushed down in the
sea,” respectively, which may be because GT cannot
distinguish between the roles of ni in Examples (1)
and (3).
(5) 賞が 男に 贈られた.
prize-ga man-ni was awarded
(A prize was awarded to a man.)
In example (5), although the ni-case argument
“男 (man)” is the same as in Example (1), the case
particle ni indicates recipient and is not alternated
in the active voice. These examples show that case
</bodyText>
<footnote confidence="0.755234">
3http://translate.google.com, accessed 2013-2-20.
</footnote>
<bodyText confidence="0.999939071428571">
alternation between the passive and active voices in
Japanese depends on not only predicates but also ar-
guments, and we have to consider their combina-
tions. Since it is impractical to manually describe
the case alternation rules for all combinations of
predicates and arguments, we have to acquire such
knowledge automatically.
Thus, in this paper, we present a method for ac-
quiring the knowledge for case alternation between
the passive and active voices in Japanese. Our
method leverages several linguistic constraints on al-
ternation patterns and lexical case frames obtained
from a large Web corpus, which are constructed for
each meaning and voice of each predicate.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930129032258">
Levin (1993) grouped English verbs into classes on
the basis of their shared meaning components and
syntactic behavior, defined in terms of diathesis al-
ternations. Hence, diathesis alternations have been
the topic of interest for a number of researchers
in the field of automatic verb classification, which
aims to induce possible verb frames from corpora
(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-
nis et al. 2008; Schulte im Walde et al. 2008; Li and
Brew 2008; Sun and Korhonen 2009; Theijssen et al.
2012). Baroni and Lenci (2010) used distributional
slot similarity to distinguish between verbs undergo-
ing the causative-inchoative alternations, and verbs
that do not alternate.
There is some work on passive-to-active voice
transformation in Japanese. Baldwin and Tanaka
(2000) empirically identified the range and fre-
quency of basic verb alternation, including active-
passive alternation, in Japanese. They automatically
extracted alternation types by using hand-crafted
case frames but did not evaluate the quality. Kondo
et al. (2001) dealt with case alternation between the
passive and active voices as a subtask of paraphras-
ing a simple sentence. They manually introduced
case alternation rules on the basis of verb types and
case patterns and transformed passive sentences into
active sentences.
Murata et al. (2006) developed a machine-
learning-based method for Japanese case alterna-
tion. They extracted 3,576 case particles in passive
sentences from the Kyoto University Text Corpus
</bodyText>
<page confidence="0.991645">
1214
</page>
<table confidence="0.581642857142857">
Case particle Grammatical function
ga nominative
wo accusative
ni dative
de locative, instrumental
kara ablative
no genitive
</table>
<tableCaption confidence="0.994177">
Table 1: Examples of Japanese postpositional case parti-
cles and their typical grammatical functions.
</tableCaption>
<bodyText confidence="0.999652142857143">
and tagged their cases in the active voice. Then,
they trained SVM classifiers using the tagged cor-
pus. Their features for training SVM were made
by using several lexical resources such as IPAL
(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo
(NLRI, 1993), and the output of Kondo et al.’s
method.
</bodyText>
<sectionHeader confidence="0.993907" genericHeader="method">
3 Lexicalized Case Frames
</sectionHeader>
<bodyText confidence="0.99995764">
To acquire knowledge for case alternation, we ex-
ploit lexicalized case frames that are automatically
constructed from 6.9 billion Web sentences by using
Kawahara and Kurohashi (2002)’s method. In short,
their method first parses the input sentences, and
then constructs case frames by collecting reliable
modifier-head relations from the resulting parses.
These case frames are constructed for each predi-
cate like PropBank frames (Palmer et al., 2005), for
each meaning of the predicate like FrameNet frames
(Fillmore et al., 2003), and for each voice. However,
neither pseudo-semantic role labels such as Arg1 in
PropBank nor information about frames defined in
FrameNet are included in these case frames. Each
case frame describes surface cases that each predi-
cate has and instances that can fill a case slot, which
is fully lexicalized like the subcategorization lexicon
VALEX (Korhonen et al., 2006).
We list some Japanese postpositional case parti-
cles with their typical grammatical functions in Ta-
ble 1 and show examples of case frames in Table
2.4 Ideally, one case frame is constructed for each
meaning and voice of the target predicate. However,
since Kawahara and Kurohashi’s method is unsuper-
vised, several case frames are actually constructed
</bodyText>
<footnote confidence="0.995876">
4Niyotte in Table 2 is a Japanese functional phrase that in-
dicates agent in this case. We treat niyotte as a case particle in
this paper for the sake of simplicity.
</footnote>
<equation confidence="0.973877625">
Case Frame: “突き落とされる-4 (be pushed down-4)”
{ 女性 (woman):5, 僕 (I):2, 女 (woman):2, · · · }-ga
{ 海 (sea):229, 川 (bottom):115, 池 (pond):51, · · · }-ni
{ 継母(stepmother):2, ペガサス(Pegasus):2, · · · }-niyotte
Case Frame: “突き落とされる-5 (be pushed down-5)”
{ 京子 (Kyoko):3, 監督 (manager):1, · · · }-ga
{ 誰か (someone):143, 何者か (somebody):85, · · · }-ni
{ 階段 (stair):20, 船 (ship):7, 崖 (cliff):7, · · · }-kara
Case Frame: “突き落とす-2 (push down-2)”
{ 男 (man):14, 獅子 (lion):5, 虎 (tiger):3, · · · }-ga
{ 子(child):316, 子供(child):81, 人(person):51, · · · }-wo
{ 海 (sea):580, 谷 (ravine):576, 川 (river):352 · · · }-ni
Case Frame: “突き落とす-4 (push down-4)”
{ 誰か (someone):14, ライオン (lion):5, · · · }-ga
{ 人 (person):257, 私 (I):214, 子 (child):137, · · · }-wo
{ 崖 (cliff):53, 階段 (stair):28, · · · }-kara
</equation>
<tableCaption confidence="0.963254">
Table 2: Examples of case frames for “突き落とされ
る (be pushed down)” and “突き落とす (push down).”
Words in curly braces denote instances that can fill cor-
responding cases and the numbers following these words
denote their frequency in the corpus.
</tableCaption>
<bodyText confidence="0.9998379">
for each meaning and voice. For example, 59 and
eight case frames were respectively constructed for
the predicate in the passive voice “突き落とされる
(be pushed down)” and in the active voice “突き落
とす (push down)” from 6.9 billion Web sentences.
Table 2 shows the 4th and 5th case frames for “突き
落とされる (be pushed down)” and the 2nd and 4th
case frames for “突き落とす (push down).”
Table 3 shows an example of case frames for
“殴る (hit),” which includes no-case. Here, the
Japanese postpositional case particle “no” roughly
corresponds to “of,” that is, “X no Y” means “Y of
X,” and thus no-case is not an argument of the target
predicate. While Kawahara and Kurohashi’s method
basically collects arguments of the target predicate,
the phrase of no-case that modifies the direct object
of the predicate is also collected as no-case. This
is because, as we will show in the next section, this
phrase can be represented as ga-case in the passive
voice.
</bodyText>
<page confidence="0.679738">
1215
</page>
<equation confidence="0.9251122">
Case Frame: “殴る-2 (hit-2)”
{ 男 (man):51, 拳 (fist):30, 誰か (someone):23, · · · }-ga
{ 自分 (myself):360, 私 (I):223, · · · }-no
{ 頭 (head):5424, 顔 (face):3215, · · · }-wo
{ 拳 (fist):316, 平手 (palm):157, 拳骨 (fist):126, · · · }-de
</equation>
<tableCaption confidence="0.995656">
Table 3: An example of case frames for “殴る (hit).”
</tableCaption>
<sectionHeader confidence="0.840685" genericHeader="method">
4 Passive-Active Transformation in
Japanese
</sectionHeader>
<bodyText confidence="0.999980571428572">
Morphologically speaking, the passive voice in
Japanese is expressed by using the auxiliary verbs
“れる (reru)” and “られる (rareru),” whose past
forms are “れた (reta)” and “られた (rareta),” re-
spectively. For example, the verb in the base form
“突き落とす (tsukiotosu, push down)” is trans-
formed into the past passive form “突き落とされ
た (tsukiotosa-reta, was pushed down).” Case al-
ternations accompany passive-active transformation
in Japanese. There are only two case alternations
at most in passive-active transformation. One is the
case represented as ga in the passive voice, and the
other is the case represented as ga in the active voice.
Japanese passive sentences can be classified into
three types in accordance with what is represented
as ga-case in the passive voice: direct passive, in-
direct passive, and possessor passive.
In direct passive sentence, the object of the pred-
icate in the active voice is represented as ga-case.
Examples (1), (3), and (5) are all direct passive sen-
tences. The case that is represented as ga in the ac-
tive voice is usually represented as ni, niyotte, kara,
or de in the passive sentence. In the first sentence of
Examples (6) and (7),5 ga-cases in the active voice
are represented as niyotte and kara, respectively. On
the other hand, ga-case in the passive sentence is al-
ternated with wo or ni as shown with broken lines in
the second sentence of Examples (6) and (7).
</bodyText>
<equation confidence="0.388754333333333">
(6) P: 原因が 男 によって 特定された.
cause-ga man-niyotte was identified
(The cause was identified by a man.)
A: 男 が 原因を 特定した.
man-ga cause-wo identified
(A man identified the cause.)
</equation>
<bodyText confidence="0.9528145">
5“P” denotes a passive sentence and “A” denotes the corre-
sponding active sentence in these examples.
</bodyText>
<equation confidence="0.854025">
(7) P: 男が 女 から 話しかけられた.
man-ga woman-kara was talked to
(A man was talked to by a woman.)
A: 女 が 男に話しかけた.
</equation>
<bodyText confidence="0.96376575">
woman-ga man-ni talked to
....
(A woman talked to a man.)
Indirect passive is also called adversative pas-
sive, in which an indirectly influenced agent is repre-
sented with ga. For example, “私 (I),” the argument
represented with ga in the first sentence of Exam-
ple (8), does not appear in the active voice, i.e. the
second sentence of Example (8). In the case of in-
direct passive, ga-case in the active sentence is al-
ways alternated with ni-case in the passive sentence
as shown with solid lines in Examples (8).
</bodyText>
<equation confidence="0.4488004">
(8) P: 私が 子供 に 泣かれた.
I-ga child-ni was cried
(I’ve got a child crying.)
A: 子供 が 泣いた.(A child cried.)
child-ga cried
</equation>
<bodyText confidence="0.785501785714286">
Possessor passive is similar to indirect passive in
that the argument represented with ga-case does not
appear as an argument of the predicate in the ac-
tive voice. Therefore, possessor passive is some-
times treated as a kind of indirect passive. How-
ever, in the case of possessor passive, the argument
appears in the active sentence as a possessor of the
direct object. For example, the ga-case argument
“女 (woman)” in the passive sentence of Example
(9) does not appear as an argument of the predicate
“殴った (hit)” in the active sentence but appears in
the phrase that modifies the direct object “頭 (head)”
with the case particle no, which indicates that “女
(woman)” is the possessor of “頭 (head).”
</bodyText>
<equation confidence="0.652799666666667">
(9) P: 女が 男 に 頭を 殴られた.
woman-ga man-ni head-wo was hit
(A woman was hit on the head by a man.)
A: 男 が 女の 頭を 殴った.
man-ga woman-no head-wo hit
(A man hit the head of a woman.)
</equation>
<bodyText confidence="0.996808666666667">
In conclusion, the number of case alternation pat-
terns accompanying passive-active transformation in
Japanese is limited. Ga-case in the passive voice can
</bodyText>
<page confidence="0.94252">
1216
</page>
<bodyText confidence="0.9998222">
be alternated only with either wo, ni, or no, or does
not appear in the active voice. Ga-case in the active
voice can be represented only by ni, niyotte, kara,
or de in the passive voice. Hence, it is sufficient to
consider only their combinations.
</bodyText>
<sectionHeader confidence="0.995721" genericHeader="method">
5 Knowledge Acquisition for Case
Alternation
</sectionHeader>
<subsectionHeader confidence="0.999008">
5.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.99618">
Our objective is to acquire knowledge for case al-
ternation between the passive and active voices in
Japanese. We leverage lexical case frames obtained
from a large Web corpus by using Kawahara and
Kurohashi (2002)’s method and align cases of a case
frame in the passive voice and cases of a case frame
in the active voice. As described in Section 2, sev-
eral case frames are constructed for each voice of
each predicate. Our task consists of the following
two subtasks:
</bodyText>
<listItem confidence="0.91000175">
1. Identify a corresponding case frame in the ac-
tive voice.
2. Find an alignment between cases of case
frames in the passive and active voice.
</listItem>
<bodyText confidence="0.999554818181818">
Figure 1 shows the overview of our task. If a case
frame in the passive voice is input, we identify a cor-
responding case frame in the active voice, and find
an alignment between cases by using the algorithm
described in Section 5.3. In this example, an active
case frame “突き落とす-4 (push down-4)” is iden-
tified as a corresponding case frame for the input
passive case frame “突き落とされる-5 (be pushed
down-5)” and ga, ni, and kara-cases in the passive
case frame are aligned to wo, ga, and kara-cases in
the active case frame, respectively.
</bodyText>
<subsectionHeader confidence="0.948847">
5.2 Clues for Knowledge Acquisition
</subsectionHeader>
<bodyText confidence="0.999652">
We exploit three clues for corresponding case frame
identification and case alignment as follows:
</bodyText>
<listItem confidence="0.9998154">
1. Semantic similarity between the instances of
the aligned cases: simSEM .
2. Case distribution similarity between the corre-
sponding case frames: simDIST.
3. Preference of alternation patterns: fPP .
</listItem>
<figureCaption confidence="0.999868">
Figure 1: The overview of our task.
</figureCaption>
<bodyText confidence="0.99971925">
Semantic similarity The instances of the aligned
cases should be similar. For example, the instances
of the ga-case of the case frame “突き落とされ
る-5 (be pushed down-5)” and the wo-case of the
case frame “突き落とす-4 (push down-4),” which
are considered to be aligned and represent patient,
are similar. Thus, we exploit semantic similarity
simSEM between the instances of the corresponding
cases.
We first define an asymmetric similarity measure
between C1 and C2, each of which is a set of case
slot instances, as follows:
</bodyText>
<equation confidence="0.87091">
sima(C1,C2) = |C1 |i1∈C1 maCx(sim(i1,i2)),
</equation>
<bodyText confidence="0.997185">
where sim(i1, i2) is the similarity between instances.
In this study, we apply a distributional similarity
measure (Lin, 1998), which was computed from
the Web corpus used to construct the case frames.
We next define a symmetric similarity measure be-
tween C1 and C2 as an average of sima(C1, C2) and
sima(C2, C1).
</bodyText>
<equation confidence="0.736152">
1
sims(C1, C2)= 2(sima(C1, C2)+sima(C2, C1)).
</equation>
<bodyText confidence="0.99866">
Then we define semantic similarity of a case
alignment A between case frames CF1 and CF2.
</bodyText>
<equation confidence="0.97181575">
�N
1
simSEM (A) = N
i=1
</equation>
<table confidence="0.90903496">
,QSXW: D FDVH IUDPH LQ WKH SDVVLYH YRLFH
&amp;DVH )UDPH: &amp;quot;OML-&apos;()-5 (EH SXVKHG GRZQ-5)&amp;quot;
^ *&apos;T-(.\RWR):3, +,(PDQDJHU):1, ...`-ga
RR 4RPHRQH):143, -. 4VRPHERG\):85, •••`-ni
%&amp;(VWDLU):20, /(VKLS):7, $�FOLII):7, •••`-kara
���
2. )LQG DQ DOLJQPHQW
EHWZHHQ FDVHV
&amp;DVH )UDPH: &amp;quot;90L$--1 (SXVK GRZQ-1)&amp;quot;
^ ~~~D ZUG����� ���ZRUGV���� ~`JD
&amp;DVH )UDPH� �������� �SXVK GRZQ����
���� ��� `ZR
^ ~~PDQ~~~~~ ��OLRQ���� ~`JD
^ ���ERWWP������
&amp;DVH )UDPH� �������� GRZQ����
���KHOO����� ��`�QL
�SXVK
^ ~~OG~~ ~~~L~~ `Z
^ �����UDSLVW��� ~~~D ZRUG���� ~`JD
&amp;DVH
���
^ ����� ��UDYLQH���� ��ULYH�����
)UDPH� �������� �SXVK ��`�QL
GRZQ����
^ ������ ���SHRSOH����� ����IXQ����� ~`ZR
</table>
<figure confidence="0.932483444444445">
^ � �VRPHRQH����� !�&amp;quot;# �OLRQ���� ���`�ga
WP~~~~~~~
���
^ ��SHUVRQ������
���IHDU������
��,������ ��`QL
��FKLOG������ •••`-wo
%&amp; �VWDLU�����
���
^ $ �FOLII����� •••`-kara
&amp;DVH IUDPHV
IRU ´SXVK GRZQµ LQ WKH DFWLYH YRLFH
1. ,GHQWLI\ D
FRUUHVSRQGLQJ
FDVH IUDPH
^ fil(
^ La,&lt;.
sims(C1,i, C2,a(i)),
</figure>
<page confidence="0.971497">
1217
</page>
<bodyText confidence="0.999958214285715">
where N denotes the number of case slots of CF1,
C1,i denotes a set of instances of the i-th case slot of
CF1, and C2,a(i) denotes the set of the aligned case
instances of CF2. A denotes the alignment {c1,1→
c2,a(1), c1,2→c2,a(2), ... , c1,N→c2,a(N)} where cn,i
denotes the case name that corresponds to Cn,i.
Case distribution similarity Although arguments
are often omitted in Japanese, arguments that are
usually mentioned explicitly in the passive voice
will be also explicitly mentioned in the active voice.
Hence, the frequency distribution of cases can be a
clue for case alignment. In this study, we exploit the
following cosine similarity of frequency distribution
as case distribution similarity:
</bodyText>
<equation confidence="0.9508335">
simDIST (A)=cos((|C1,1|, ... , |C1,N|),
(|C2,a(1)|, . . . , |C2,a(N)|)).
</equation>
<bodyText confidence="0.999973">
As an example, consider the alignment between a
passive case6 “選ばれる-1 (be selected-1)” and the
corresponding active case frame “選ぶ-13 (select-
13)” in Table 4. The alignment A1 = {ga →
wo, ni → ni, NIL → ga} is considered to be correct.
However, if we consider only the semantic similar-
ity, an alignment A2 = {ga → ni, ni → ga, wo →
wo} is selected, because the alignment A2 has the
highest semantic similarity. On the other hand, the
case distribution similarity
</bodyText>
<equation confidence="0.994564">
simDIST (A1) = cos((17722,122273,0),
(33338, 800, 382)) ≈ 0.167
is much larger than
simDIST (A2) = cos((17722,122273,96),
(800, 382, 33338)) ≈ 0.016.
</equation>
<bodyText confidence="0.999514571428571">
Thus, the alignment A1 would be selected by con-
sidering the case distribution similarity.
Preference of alternation patterns Some alter-
nation patterns often appear, and others do not.
For example, as Murata et al. (2006) reported,
whereas 96.47% of ga-case is alternated with wo-
case in passive-active transformation in Japanese,
</bodyText>
<footnote confidence="0.995143333333333">
6This case frame should not have wo-case. However, since
we constructed case frames automatically, some case frames
have improper cases.
</footnote>
<equation confidence="0.9397939">
Case Frame: “選ばれる-1 (be selected-1)”
{ 選手
(player):1119, 作品 (work):983, · · · 1-ga:17722
{代表
(representative):18295, · · · 1-ni:122273
{ 作品 (work):5, 市長 (mayor):3, · · · 1-wo:96
Case Frame: “選ぶ-13 (select-13)”
{ 私 (I):14, 先生 (teacher):18, · · · 1-ga:382
{ 優秀賞 (award):42, シングル (single):17, · · · 1-ni:800
{ 曲 (tune):16666, 作品 (work):9967, · · · 1-wo:33338
</equation>
<tableCaption confidence="0.778249">
Table 4: Case frames “選ばれる-1 (be selected-1)”
</tableCaption>
<bodyText confidence="0.881861571428572">
and “選ぶ-13 (select-13).” The numbers following case
names denote the total numbers of case slot instances.
only 27.38% of ni-case is alternated with ga-case.
Therefore, when we can use development data, we
exploit a weighting factor fPP (A) that is deter-
mined on the development data and takes into ac-
count the preference of alternation patterns. We de-
fine fPP (A) as follows:
fPP (A)=w(ga→cga to)×w(cto ga→ga), (i)
where cga to is the case in the active voice to which
ga-case in the passive voice is aligned, cto ga is the
case in the passive voice which is aligned to ga-
case in the active voice, and w(c1 →c2) denotes the
weight of the case alternation “c1 →c2.”
</bodyText>
<subsectionHeader confidence="0.985138">
5.3 Algorithm
</subsectionHeader>
<bodyText confidence="0.999391727272727">
Algorithm 1 presents our algorithm for identifying
a corresponding case frame and finding an align-
ment between cases in pseudo-code. Our algo-
rithm first makes all possible combinations of a
case frame in the active voice (cfactive), a case in
the active voice to which ga-case in the passive
voice is aligned (cga to), and a case in the passive
voice which is aligned to ga-case in the active voice
(cto ga) on the basis of the linguistic constraints,
and then evaluates the score for the combinations
{cfactive, cga to, cto ga} by the following equation:
</bodyText>
<equation confidence="0.358553">
score=simSEM (A)×simDIST (A)α×fPP (A), (ii)
</equation>
<bodyText confidence="0.998709">
where α is a parameter that controls the impact of
the case distribution similarity.7 When we can use
</bodyText>
<footnote confidence="0.97849">
7Since fPP (A) is defined with a set of weights of case alter-
nation patterns, fPP (A) contains these weights implicitly, and
thus there is only a single explicit weight in equation (ii).
</footnote>
<page confidence="0.994512">
1218
</page>
<bodyText confidence="0.8658276">
Algorithm 1: Identifying a corresponding case
frame and finding an alignment between cases.
Input: a case frame in the passive voice: cfpassive, and
a set of case frames in the active voice: CFSactive
Output: a case frame and an alignment between cases: A
</bodyText>
<listItem confidence="0.793451083333333">
1: max score = 0, A = ()
2: for each cfactive E CFSactive
3: for each cga to E {wo, ni, no, NIL}
4: for each cto ga E {ni, niyotte, kara, de, NIL}
5: if (!occur(cga to, cto ga)) then continue
6: A = (cfactive, cga to, cto ga)
7: score=simSEM (A&apos;) xsimDIST (A&apos;)αxfPP (A&apos;)
8: if (score &gt; max score) then
9: (max score, A) = (score, A&apos;)
10: end for
11: end for
12: end for
</listItem>
<bodyText confidence="0.999807">
development data, we tune α on the development
data; otherwise we set α = 1. Since some combi-
nations of cga to and cto ga never occur, our algo-
rithm filters them out in line 5 of the algorithm. Af-
ter checking all combinations, the combination with
the highest score is output.
</bodyText>
<sectionHeader confidence="0.782208" genericHeader="evaluation">
6 Evaluation of the Acquired Knowledge
</sectionHeader>
<bodyText confidence="0.9999143">
We applied our algorithm to the case frames that
are automatically constructed from a corpus consist-
ing of about 6.9 billion Japanese sentences from the
Web. Of course, these case frames contain improper
ones, that is, several frames mix several meanings
or usages of the predicates. Thus, it is difficult to
evaluate the acquired knowledge itself. Instead, we
evaluate the usefulness of the acquired knowledge
on a case alternation task between the passive and
active voices.
</bodyText>
<subsectionHeader confidence="0.999811">
6.1 Setting and Algorithm for Case Alternation
</subsectionHeader>
<bodyText confidence="0.999993555555556">
We basically used the same data as Murata et
al. (2006). As mentioned in Section 2, they extracted
3,576 case particles in passive sentences from the
Kyoto University Text Corpus, and tagged their
cases in the active voice. Since they treated posses-
sor passive as a kind of indirect passive, they did not
adopt the case alternation between ga and no. In ad-
dition, their data included some annotation errors.
We thus modified 21 annotations,8 five of which
</bodyText>
<footnote confidence="0.9899025">
8The modified version of the data is publicly available at
http://alaginrc.nict.go.jp/case/src/kaku1.1.tar.gz.
</footnote>
<bodyText confidence="0.99972725">
were changed to the case alternation between ga and
no. Note that there were some cases where multiple
possible case particles were tagged to one instance.
We adopted evaluation metrics called “Eval. B” by
Murata et al., that is, we judged the output to be cor-
rect when the output was included in possible an-
swers. We performed experiments on the following
three types of data settings.
</bodyText>
<listItem confidence="0.98951975">
1. Experiments without either development or
training data.
2. Experiments with development data.
3. Experiments with training data.
</listItem>
<bodyText confidence="0.920623304347826">
Experiments without either development or
training data In the first setting, we aligned the
input passive case frame to one of the active case
frames of the same predicate only by using simSEM
and simDIST with the parameter α = 1. Therefore,
this setting is fully unsupervised. In this setting, the
input surface cases are alternated as follows:
1. If a passive sentence is input, perform syntac-
tic and surface case structure analysis by us-
ing Kawahara and Kurohashi (2006)’s model.9
Their model identified a proper case frame for
each predicate, and assigned arguments in the
input sentence to case slots of the case frame.
2. By using the acquired knowledge for case alter-
nation, alternate input surface cases with cases
in the active voice.
We call this model Model 1. For example, if Ex-
ample (10) is input, the ga-case argument is assigned
to the ga-case of the case frame “突き落とされる-5
(be pushed down-5).” Since this case is aligned to
the wo-case of the case frame “突き落とす-4 (push
down-4)” as shown in Figure 1, this ga-case is alter-
nated with wo-case.
</bodyText>
<equation confidence="0.862467">
(10) 女 が 突き落とされた.
</equation>
<bodyText confidence="0.6745045">
woman-ga was pushed down
(A woman was pushed down.)
</bodyText>
<footnote confidence="0.953903">
9KNP: http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
</footnote>
<page confidence="0.987718">
1219
</page>
<listItem confidence="0.897869214285714">
Algorithm 2: Pseudo-code of the hill-climbing
algorithm for tuning the parameter vector x.
1: x = (1.0, 1.0,..., 1.0)
2: acc = faccuracy(x), pre acc = 0
3: while acc &gt; pre acc
4: pre acc = acc
5: for i E {0,...,|x |− 11
6: acc+ = faccuracy(x0,..., xi + 0.1,... ,x|x|−1)
7: acc− = faccuracy(x0, ..., xi − 0.1, ..., x|x|−1)
8: if acc+ &gt;acc and acc+ &gt;acc− then xi =xi+0.1
8: else if acc− &gt; acc then xi = xi − 0.1
9: acc = faccuracy(x)
10: end for
11: end while
</listItem>
<bodyText confidence="0.994059375">
Experiments with development data In the sec-
ond setting, we aligned the input passive case frame
to one of the active case frames of the same pred-
icate by using simSEM, simDIST , and fPP with α
tuned on the development data. In advance, we di-
vided the tagged data into two parts just as Murata
et al. (2006) did, both of which contained 1,788 case
particles, and performed 2-fold cross-validation. We
used one part for development and the other for test-
ing, and vice versa.
We tuned w(ga → cga to), w(cto ga → ga) in
Equation (i), and α in Equation (ii) by a simple
hill-climbing strategy. Since the candidate cases for
cga to are ni, niyotte, kara, de, and NIL, and the can-
didate cases for cto ga are wo, ni, no, and NIL, we
defined parameter vector x as follows:
x=(w(ga→ni),w(ga→niyotte),w(ga→kara),
w(ga→de),w(ga→NIL),w(wo→ga),
w(ni→ga),w(wo→no),w(NIL→ga), α).
Algorithm 2 shows the hill-climbing algorithm for
tuning the parameter vector x, where faccuracy(x) is
a function that returns the case alternation accuracy
on the development data with parameter x. This al-
gorithm varies one parameter at a time with a step-
size of 0.1 until there is no accuracy improvement
in the development data. After acquiring knowledge
for case alternation with the tuned parameter, we ap-
plied the same method for case alternation as the first
setting. We call this model Model 2.
Experiments with training data In the third set-
ting, we also performed 2-fold cross validation, that
is, we used one part of the divided tagged corpus
</bodyText>
<table confidence="0.9992919">
Model Accuracy
Model 1S Parameter 0.902 (3,224/3,576)
simSEM simDIST tuning
✓
Model 1D ✓ 0.857 (3,063/3,576)
Model 1 ✓ ✓ 0.906 (3,239/3,576)
Model 2S ✓ ✓ 0.928 (3,320/3,576)
Model 2D ✓ ✓ 0.927 (3,314/3,576)
Model 2 ✓ ✓ ✓ 0.938 (3,353/3,576)
Baseline 0.883 (3,159/3,576)
</table>
<tableCaption confidence="0.9961845">
Table 5: Experimental results of case alternation without
training data.
</tableCaption>
<bodyText confidence="0.999994642857143">
for training and the other for testing, and vice versa.
Although we basically applied Murata et al. (2006)’s
method, which is based on SVMs, we added the out-
put of Model 2 as a new feature.
Specifically, we first tuned the parameter vector x
on the training data and acquired the knowledge for
case alternation with the tuned parameter. By us-
ing the acquired knowledge, we alternated the input
cases in both the training and test data and obtained
the resulting case of Model 2. Note that, we did not
use any annotations for the test data in this process.
We then trained the SVMs on the training data and
applied them to the test data using the resulting case
as a new feature. We call this model Model 3.
</bodyText>
<subsectionHeader confidence="0.982635">
6.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9988120625">
Table 5 shows the results of the experiments without
training data. Baseline is a system that outputs the
most frequently alternated cases in the development
data, which was also used by Murata et al. (2006).
The baseline score was higher than that reported by
Murata et al. because we modified 21 annotations.
We also performed experiments without using case
distribution similarity or semantic similarity. We
call these models in the first setting Model 1S and
Model 1D, and these models in the second setting
Model 2S and Model 2D, respectively.
Although Models 1S, 1D, and 1 were fully un-
supervised models, Models 1S and 1 significantly10
outperformed the baseline model (p-values of Mc-
Nemar (1947)’s test were smaller than 0.00001). On
the other hand, the difference between Models 1S
</bodyText>
<footnote confidence="0.8666555">
10In this paper, we call a difference significant if the p-value
of McNemar (1947)’s test is less than 0.01.
</footnote>
<page confidence="0.938643">
1220
</page>
<table confidence="0.860333">
Model Accuracy
(Murata et al., 2006) 0.944 (3,376/3,576)
Model 3 0.956 (3,417/3,576)
</table>
<tableCaption confidence="0.9920865">
Table 6: Comparison between Murata et al. (2006)’s
method and our method with training data.
</tableCaption>
<bodyText confidence="0.997353">
and 1 is not statistically significant, and thus the ef-
fect of the case distribution similarity was not con-
firmed by these experiments.
Models 2S, 2D, and 2 were models with parame-
ter tuning. Parameter tuning significantly improved
the performance. In addition, the difference between
Models 2S and 2 and the difference between Models
2D and 2 were both significant (p-values of McNe-
mar’s test were 0.00032 and 0.00039, respectively),
and thus we confirmed the usefulness of the two sim-
ilarity measures. The parameter α that controls the
impact of the case distribution similarity was tuned
to 0.3, which means semantic similarity between the
instances of the aligned cases is more important than
case distribution similarity for this task.
Table 6 compares Murata et al.’s method and our
method with training data. We used Murata et al.’s
method without feature selection because it achieved
the highest performance on this setting. Their
method’s score was higher than that they reported,
again due to the corpus modification. The difference
between their method and our method was signifi-
cant (p-value of McNemar’s test was 0.00011), and
we confirmed the usefulness of the acquired knowl-
edge for case alternation.
Table 7 shows an example of case alternation be-
tween the passive and active voices. When the pas-
sive sentence was input, the argument “松樹さん
が(Mr. Matsuki-ga)” was first assigned to ga-case
of the case frame “殴られる-2 (be hit-2).” Since this
case was aligned to no-case of the case frame “殴
る-2 (hit-2),” the input ga-case was alternated with
no-case. On the other hand, the cases of the other ar-
guments “バットで (bat-de)” and “頭を (head-wo)”
were output as they were in the passive sentence.
We now list three error causes observed in our ex-
periments of the case alternation task:
1) The passive voice in Japanese is expressed by us-
ing the auxiliary verbs “れる (reru)” and “られる
(rareru).” However, these auxiliary verbs can rep-
</bodyText>
<table confidence="0.798668384615385">
Input Text:
· · · 松樹さんが 金属バットで 頭を 殴られ、· · ·
Mr. Matsuki-ga metal bat-de head-wo was hit
(... Mr. Matsuki was hit on the head with a metal bat ... )
Identified passive case frame:
Case Frame: “殴られる-2 (be hit-2)”
{ 何者か (someone):2, 部員 (member):1, · · · }-niyotte
{ 女性 (woman):5, 女児 (girl)):4, · · · }-ga
{ 頭 (head):3944, 顔 (face):1186, · · · }-wo
{ 鈍器 (blunt weapon):84, バット (bat):45, · · · }-de
Corresponding active case frame and case alignment:
Case alignment: {niyotte→ga, ga →no, wo→wo, de→de}
Case Frame: “殴る-2 (hit-2)”
</table>
<equation confidence="0.988443">
{ 男 (man):51, 拳 (fist):30, 誰か (someone):23, · · · }-ga
{ 自分 (myself):360, 私 (I):223, · · · }-no
{
頭 (head):5424, 顔 (face):3215, · · · }-wo
{ 拳 (fist):316, 平手 (palm):157, 拳 (fist):43, · · · }-de
</equation>
<tableCaption confidence="0.979837">
Table 7: An example of case alternation. The input ga-
case was alternated with no-case.
</tableCaption>
<bodyText confidence="0.999318">
resent several other meanings, such as honorific and
possibility. Since Kawahara and Kurohashi (2002)’s
method does not distinguish between these mean-
ings, our case frames sometimes contain improper
cases such as wo-case in case frame “選ばれる-1
(be selected-1)” in Table 4.
2) In some passive sentences, there are two surface
ni-cases as in Example (11). However, our method
does not assume such sentences, and thus cannot
deal with them properly.
</bodyText>
<equation confidence="0.8791785">
(11) 男 に オフィス に 派遣された.
man-ni office-ni was sent
</equation>
<bodyText confidence="0.995481692307692">
(φ was sent to the office by a man.)
3) Agent of a predicate can be represented by us-
ing several types of case particles in the passive
voice. For example, “会社 (company)” in Exam-
ple (12) is the agent of “雇用した (employed),”
which can be represented by either of ni, niyotte,
and kara in the passive voice. Since Kawahara and
Kurohashi (2002)’s method can not recognize the
exchangeablity of case particles, some case frames
contain several cases of the same semantic role.
However, since our method enforces a one-to-one
alignments, only one of these cases is properly
aligned to the corresponding case in the active voice.
</bodyText>
<page confidence="0.958471">
1221
</page>
<figure confidence="0.822831333333333">
(12) -v&apos;2&amp;quot;Z t 7bi 93,�_- EfflUk.
company-ga man-wo employed
(The company employed a man.)
</figure>
<subsectionHeader confidence="0.9799805">
6.3 Application to Alternation between the
Causative and Active Voices
</subsectionHeader>
<bodyText confidence="0.999996696969697">
To confirm the applicability of our framework to
other types of alternation than the active-passive al-
ternation, we applied our framework to case alter-
nation between the causative and active voices. The
causative voice in Japanese is a grammatical voice
and is expressed by using the auxiliary verbs “•L;6
(seru)” and “cam•L;6 (saseru).” We basically used
the same algorithm as Algorithm 1 for acquiring
the knowledge for case alternation, but used differ-
ent constraints on case alternation patterns because
possible case alternation patterns are different from
those of active-passive alternation. Specifically, we
replaced the third and fourth lines of Algorithm 1
with “for each cto ga E tNIL, nil” and “for each
cga to E two, nil,” respectively, based on linguistic
analysis of active-causative alternation in Japanese.
We used a part of the data created by Murata and
Isahara (2003) to evaluate the usefulness of the ac-
quired knowledge. Their data consists of 4,671 case
particles in passive or causative sentences from the
Kyoto University Text Corpus with their cases in
the active voice. We first extracted 524 case par-
ticles that were extracted from causative sentences.
Since the annotation quality was not very high, we
manually checked all tags and modified inappropri-
ate ones. We then performed 2-fold cross valida-
tion experiments. Table 8 shows experimental re-
sults. Baseline is a system that outputs the most fre-
quently alternated cases in the training data. The dif-
ference between Murata et al. (2006)’s model11 and
our method was significant (p-value of McNemar’s
test was 0.0019), and we confirmed the applicability
of our framework to active-causative alternation.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="conclusions">
7 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999916666666667">
We have presented a method for automatically ac-
quiring knowledge for case alternation between the
passive and active voices in Japanese. Our method
</bodyText>
<footnote confidence="0.961253">
11In this experiment, we used the same features as those used
by Murata and Isahara (2003).
</footnote>
<table confidence="0.99429325">
Model Accuracy
Baseline 0.781 (409/524)
Murata et al. (2006)’s model 0.836 (438/524)
Our method with training data 0.872 (457/524)
</table>
<tableCaption confidence="0.9422345">
Table 8: Experimental results of case alternation between
the causative and active voices.
</tableCaption>
<bodyText confidence="0.9990140625">
aligned an input case frame in the passive voice to
a corresponding case frame in the active voice and
found an alignment between their cases. We then
applied the acquired knowledge to a case alternation
task and proved its usefulness.
The knowledge we have to manually construct is
only the knowledge of linguistic constraints on case
alternation patterns. The other types of knowledge
are automatically acquired from a large raw cor-
pus. Thus, although this paper focused on the active-
passive alternation in Japanese, our framework is ap-
plicable to the other types of case alternation and to
other languages, especially similar languages such
as Korean. We plan to apply our framework to other
types of case alternation such as case alternation be-
tween intransitive and transitive verbs.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9965505">
This work was supported by JSPS KAKENHI Grant
Number 23800025 and 25730131.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998681105263158">
Timothy Baldwin and Hozumi Tanaka. 2000. Verb alter-
nations and Japanese – how, what and where? In Proc.
of PACLIC 14, pages 3–14.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistic, 36(4):673–721.
Charles J. Fillmore, Christopher R. Johnson, and Miriam.
R. L. Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235–250.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Proc.
of ACL’07 Workshop: Linguistic Annotation Work-
shop, pages 132–139.
IPA. 1987. Japanese verbs : A guide to the IPA lexicon
of basic Japanese verbs.
Eric Joanis, Suzanne Stevenson, and David James. 2008.
A general feature space for automatic verb classifica-
tion. Natural Language Engineering, 14(3):337–367.
</reference>
<page confidence="0.813539">
1222
</page>
<reference confidence="0.999768451219512">
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust Japanese
case analysis. In Proc. of COLING’02, pages 425–
431.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese syn-
tactic and case structure analysis. In Proc. of HLT-
NAACL’06, pages 176–183.
Daisuke Kawahara, Ryohei Sasano, and Sadao Kuro-
hashi. 2004. Toward text understanding: Integrat-
ing relevance-tagged corpora and automatically con-
structed case frames. In Proc. of LREC’04, pages
1833–1836.
Keiko Kondo, Satoshi Sato, and Manabu Okumura.
2001. Paraphrasing by case alternation (in Japanese).
Journal of Information Processing Society of Japan,
42(3):465–477.
Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for nat-
ural language processing applications. In Proc.of
LREC’06, pages 3000–3006.
Mirella Lapata and Chris Brew. 2004. Verb class dis-
ambiguation using informative priors. Computational
Linguistics, 30(1):45–73.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Proc. of
ACL-HLT’08, pages 434–442.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL-COLING’98, pages
768–774.
Diana McCarthy. 2000. Using semantic preferences to
identify verbal participation in role switching alterna-
tions. In Proc. of NAACL’00.
Quinn McNemar. 1947. Note on the sampling error of
the difference between correlated proportions or per-
centages. Psychometrika, 12:153–157.
Masaki Murata and Hitoshi Isahara. 2003. Conver-
sion of Japanese passive/causative sentences into ac-
tive sentences using machine learning. In Proc. of CI-
CLing’03, pages 115–125.
Masaki Murata, Toshiyuki Kanamaru, Tamotsu Shirado,
and Hitoshi Isahara. 2006. Machine-learning-based
transformation of passive Japanese sentences into ac-
tive by separating training data into each input particle.
In Proc. of COLING-ACL’06, pages 587–594.
Katashi Nagao and Koiti Hasida. 1998. Automatic text
summarization based on the global document annota-
tion. In Proc. of ACL’98, pages 917–921.
NLRI. 1993. Bunrui Goi Hyo (in Japanese). Shuuei
Publishing.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1):71–
105.
Ryohei Sasano and Sadao Kurohashi. 2011. A discrim-
inative approach to japanese zero anaphora resolution
with large-scale lexicalized case frames. In Proc. of
IJCNLP’11, pages 758–766.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining EM
training and the MDL principle for an automatic verb
classification incorporating selectional preferences. In
Proc. of ACL-HLT’08, pages 496–504.
Lin Sun and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired selectional
preferences. In Proc. of EMNLP’09, pages 638–647.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008.
A Japanese predicate argument structure analysis us-
ing decision lists. In Proc. of EMNLP’08, pages 523–
532.
Daphne Theijssen, Lou Boves, Hans van Halteren, and
Nelleke Oostdijk. 2012. Evaluating automatic anno-
tation: automatically detecting and enriching instances
of the dative alternation. Language Resources and
Evaluation, 46(4):565–600.
Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Mat-
sumoto. 2011. Jointly extracting Japanese predicate-
argument relation with markov logic. In Proc. of IJC-
NLP’11, pages 1125–1133.
</reference>
<page confidence="0.947524">
1223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880218">
<title confidence="0.987663">Automatic Knowledge Acquisition for Case between the Passive and Active Voices in Japanese</title>
<author confidence="0.948395">Daisuke Sadao Manabu</author>
<affiliation confidence="0.966512">and Intelligence Laboratory, Tokyo Institute of School of Informatics, Kyoto University</affiliation>
<abstract confidence="0.99945625">We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Verb alternations and Japanese – how, what and where?</title>
<date>2000</date>
<booktitle>In Proc. of PACLIC 14,</booktitle>
<pages>3--14</pages>
<contexts>
<context position="6946" citStr="Baldwin and Tanaka (2000)" startWordPosition="1088" endWordPosition="1091">s. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese c</context>
</contexts>
<marker>Baldwin, Tanaka, 2000</marker>
<rawString>Timothy Baldwin and Hozumi Tanaka. 2000. Verb alternations and Japanese – how, what and where? In Proc. of PACLIC 14, pages 3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistic,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="6698" citStr="Baroni and Lenci (2010)" startWordPosition="1055" endWordPosition="1058">, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasi</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistic, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<marker>Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam. R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3):235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Mamoru Komachi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating a Japanese text corpus with predicate-argument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proc. of ACL’07 Workshop: Linguistic Annotation Workshop,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1947" citStr="Iida et al., 2007" startWordPosition="281" endWordPosition="284">o take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1Japanese is a head-final language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. the former representation, the NAIST Text Corpora (Iida et al., 2007), another major Japanese corpus, adopts the latter representation. Examples (1) and (2) describe the same event in the passive and active voices, respectively. When we use surface cases to represent the relationship between the predicate and its argument in Example (1), the case of “女 (woman)” is ga2 and the case of “男 (man)” is ni.2 On the other hand, when we use the normalized-cases for the base form, the case of “女 (woman)” is wo2 and the case of “男 (man)” is ga, which are the same as the surface cases in the active voice as in Example (2). (1) 女が 男に 突き落とされた. woman-ga man-ni was pushed down</context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with predicate-argument and coreference relations. In Proc. of ACL’07 Workshop: Linguistic Annotation Workshop, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IPA</author>
</authors>
<title>Japanese verbs : A guide to the IPA lexicon of basic Japanese verbs.</title>
<date>1987</date>
<contexts>
<context position="8094" citStr="IPA, 1987" startWordPosition="1263" endWordPosition="1264">(2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus 1214 Case particle Grammatical function ga nominative wo accusative ni dative de locative, instrumental kara ablative no genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classifiers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method first parses the input sentences, and then constructs case frames by collecting reliable modifier-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the pr</context>
</contexts>
<marker>IPA, 1987</marker>
<rawString>IPA. 1987. Japanese verbs : A guide to the IPA lexicon of basic Japanese verbs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
<author>David James</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="6578" citStr="Joanis et al. 2008" startWordPosition="1032" endWordPosition="1036">ges several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the q</context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>Eric Joanis, Suzanne Stevenson, and David James. 2008. A general feature space for automatic verb classification. Natural Language Engineering, 14(3):337–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Fertilization of case frame dictionary for robust Japanese case analysis.</title>
<date>2002</date>
<booktitle>In Proc. of COLING’02,</booktitle>
<pages>425--431</pages>
<contexts>
<context position="8397" citStr="Kawahara and Kurohashi (2002)" startWordPosition="1308" endWordPosition="1311">blative no genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classifiers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method first parses the input sentences, and then constructs case frames by collecting reliable modifier-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames defined in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instanc</context>
<context position="15865" citStr="Kawahara and Kurohashi (2002)" startWordPosition="2596" endWordPosition="2599">ompanying passive-active transformation in Japanese is limited. Ga-case in the passive voice can 1216 be alternated only with either wo, ni, or no, or does not appear in the active voice. Ga-case in the active voice can be represented only by ni, niyotte, kara, or de in the passive voice. Hence, it is sufficient to consider only their combinations. 5 Knowledge Acquisition for Case Alternation 5.1 Task Definition Our objective is to acquire knowledge for case alternation between the passive and active voices in Japanese. We leverage lexical case frames obtained from a large Web corpus by using Kawahara and Kurohashi (2002)’s method and align cases of a case frame in the passive voice and cases of a case frame in the active voice. As described in Section 2, several case frames are constructed for each voice of each predicate. Our task consists of the following two subtasks: 1. Identify a corresponding case frame in the active voice. 2. Find an alignment between cases of case frames in the passive and active voice. Figure 1 shows the overview of our task. If a case frame in the passive voice is input, we identify a corresponding case frame in the active voice, and find an alignment between cases by using the algo</context>
<context position="33703" citStr="Kawahara and Kurohashi (2002)" startWordPosition="5603" endWordPosition="5606">女児 (girl)):4, · · · }-ga { 頭 (head):3944, 顔 (face):1186, · · · }-wo { 鈍器 (blunt weapon):84, バット (bat):45, · · · }-de Corresponding active case frame and case alignment: Case alignment: {niyotte→ga, ga →no, wo→wo, de→de} Case Frame: “殴る-2 (hit-2)” { 男 (man):51, 拳 (fist):30, 誰か (someone):23, · · · }-ga { 自分 (myself):360, 私 (I):223, · · · }-no { 頭 (head):5424, 顔 (face):3215, · · · }-wo { 拳 (fist):316, 平手 (palm):157, 拳 (fist):43, · · · }-de Table 7: An example of case alternation. The input gacase was alternated with no-case. resent several other meanings, such as honorific and possibility. Since Kawahara and Kurohashi (2002)’s method does not distinguish between these meanings, our case frames sometimes contain improper cases such as wo-case in case frame “選ばれる-1 (be selected-1)” in Table 4. 2) In some passive sentences, there are two surface ni-cases as in Example (11). However, our method does not assume such sentences, and thus cannot deal with them properly. (11) 男 に オフィス に 派遣された. man-ni office-ni was sent (φ was sent to the office by a man.) 3) Agent of a predicate can be represented by using several types of case particles in the passive voice. For example, “会社 (company)” in Example (12) is the agent of “雇用</context>
</contexts>
<marker>Kawahara, Kurohashi, 2002</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case analysis. In Proc. of COLING’02, pages 425– 431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL’06,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="1213" citStr="Kawahara and Kurohashi, 2006" startWordPosition="170" endWordPosition="173">ed from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1Japanese is a head-final language. Word order does not mark syntactic relations. </context>
<context position="26012" citStr="Kawahara and Kurohashi (2006)" startWordPosition="4282" endWordPosition="4285">pes of data settings. 1. Experiments without either development or training data. 2. Experiments with development data. 3. Experiments with training data. Experiments without either development or training data In the first setting, we aligned the input passive case frame to one of the active case frames of the same predicate only by using simSEM and simDIST with the parameter α = 1. Therefore, this setting is fully unsupervised. In this setting, the input surface cases are alternated as follows: 1. If a passive sentence is input, perform syntactic and surface case structure analysis by using Kawahara and Kurohashi (2006)’s model.9 Their model identified a proper case frame for each predicate, and assigned arguments in the input sentence to case slots of the case frame. 2. By using the acquired knowledge for case alternation, alternate input surface cases with cases in the active voice. We call this model Model 1. For example, if Example (10) is input, the ga-case argument is assigned to the ga-case of the case frame “突き落とされる-5 (be pushed down-5).” Since this case is aligned to the wo-case of the case frame “突き落とす-4 (push down-4)” as shown in Figure 1, this ga-case is alternated with wo-case. (10) 女 が 突き落とされた.</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. In Proc. of HLTNAACL’06, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Toward text understanding: Integrating relevance-tagged corpora and automatically constructed case frames.</title>
<date>2004</date>
<booktitle>In Proc. of LREC’04,</booktitle>
<pages>1833--1836</pages>
<contexts>
<context position="1628" citStr="Kawahara et al., 2004" startWordPosition="236" endWordPosition="239">al entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1Japanese is a head-final language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. the former representation, the NAIST Text Corpora (Iida et al., 2007), another major Japanese corpus, adopts the latter representation. Examples (1) and (2) describe the same event in the passive and active voices, respectively. When we use surface cases to represent the relationship between the predicate and its argument in Example (1), the case o</context>
</contexts>
<marker>Kawahara, Sasano, Kurohashi, 2004</marker>
<rawString>Daisuke Kawahara, Ryohei Sasano, and Sadao Kurohashi. 2004. Toward text understanding: Integrating relevance-tagged corpora and automatically constructed case frames. In Proc. of LREC’04, pages 1833–1836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiko Kondo</author>
<author>Satoshi Sato</author>
<author>Manabu Okumura</author>
</authors>
<title>Paraphrasing by case alternation (in Japanese).</title>
<date>2001</date>
<journal>Journal of Information Processing Society of Japan,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="7205" citStr="Kondo et al. (2001)" startWordPosition="1125" endWordPosition="1128">e im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus 1214 Case particle Grammatical function ga nominative wo accusative ni dative de locative, instrumental kara ablative no genitive Table 1: Examples</context>
</contexts>
<marker>Kondo, Sato, Okumura, 2001</marker>
<rawString>Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001. Paraphrasing by case alternation (in Japanese). Journal of Information Processing Society of Japan, 42(3):465–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Ted Briscoe</author>
</authors>
<title>A large subcategorization lexicon for natural language processing applications.</title>
<date>2006</date>
<booktitle>In Proc.of LREC’06,</booktitle>
<pages>3000--3006</pages>
<contexts>
<context position="9118" citStr="Korhonen et al., 2006" startWordPosition="1420" endWordPosition="1423"> by collecting reliable modifier-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames defined in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can fill a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2.4 Ideally, one case frame is constructed for each meaning and voice of the target predicate. However, since Kawahara and Kurohashi’s method is unsupervised, several case frames are actually constructed 4Niyotte in Table 2 is a Japanese functional phrase that indicates agent in this case. We treat niyotte as a case particle in this paper for the sake of simplicity. Case Frame: “突き落とされる-4 (be pushed down-4)” { 女性 (woman):5, 僕 (I):2, 女 (woman):2, · </context>
</contexts>
<marker>Korhonen, Krymolowski, Briscoe, 2006</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Ted Briscoe. 2006. A large subcategorization lexicon for natural language processing applications. In Proc.of LREC’06, pages 3000–3006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="6558" citStr="Lapata and Brew 2004" startWordPosition="1028" endWordPosition="1031">ese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but di</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>Mirella Lapata and Chris Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6172" citStr="Levin (1993)" startWordPosition="970" endWordPosition="971">icates but also arguments, and we have to consider their combinations. Since it is impractical to manually describe the case alternation rules for all combinations of predicates and arguments, we have to acquire such knowledge automatically. Thus, in this paper, we present a method for acquiring the knowledge for case alternation between the passive and active voices in Japanese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoi</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianguo Li</author>
<author>Chris Brew</author>
</authors>
<title>Which are the best features for automatic verb classification.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT’08,</booktitle>
<pages>434--442</pages>
<contexts>
<context position="6626" citStr="Li and Brew 2008" startWordPosition="1043" endWordPosition="1046">patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alte</context>
</contexts>
<marker>Li, Brew, 2008</marker>
<rawString>Jianguo Li and Chris Brew. 2008. Which are the best features for automatic verb classification. In Proc. of ACL-HLT’08, pages 434–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of ACL-COLING’98,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="17855" citStr="Lin, 1998" startWordPosition="2929" endWordPosition="2930">r example, the instances of the ga-case of the case frame “突き落とされ る-5 (be pushed down-5)” and the wo-case of the case frame “突き落とす-4 (push down-4),” which are considered to be aligned and represent patient, are similar. Thus, we exploit semantic similarity simSEM between the instances of the corresponding cases. We first define an asymmetric similarity measure between C1 and C2, each of which is a set of case slot instances, as follows: sima(C1,C2) = |C1 |i1∈C1 maCx(sim(i1,i2)), where sim(i1, i2) is the similarity between instances. In this study, we apply a distributional similarity measure (Lin, 1998), which was computed from the Web corpus used to construct the case frames. We next define a symmetric similarity measure between C1 and C2 as an average of sima(C1, C2) and sima(C2, C1). 1 sims(C1, C2)= 2(sima(C1, C2)+sima(C2, C1)). Then we define semantic similarity of a case alignment A between case frames CF1 and CF2. �N 1 simSEM (A) = N i=1 ,QSXW: D FDVH IUDPH LQ WKH SDVVLYH YRLFH &amp;DVH )UDPH: &amp;quot;OML-&apos;()-5 (EH SXVKHG GRZQ-5)&amp;quot; ^ *&apos;T-(.\RWR):3, +,(PDQDJHU):1, ...`-ga RR 4RPHRQH):143, -. 4VRPHERG\):85, •••`-ni %&amp;(VWDLU):20, /(VKLS):7, $�FOLII):7, •••`-kara ��� 2. )LQG DQ DOLJQPHQW EHWZHHQ FDVHV</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of ACL-COLING’98, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Using semantic preferences to identify verbal participation in role switching alternations.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL’00.</booktitle>
<contexts>
<context position="6536" citStr="McCarthy 2000" startWordPosition="1026" endWordPosition="1027">voices in Japanese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-craf</context>
</contexts>
<marker>McCarthy, 2000</marker>
<rawString>Diana McCarthy. 2000. Using semantic preferences to identify verbal participation in role switching alternations. In Proc. of NAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<tech>Psychometrika,</tech>
<pages>12--153</pages>
<contexts>
<context position="30480" citStr="McNemar (1947)" startWordPosition="5057" endWordPosition="5059"> that outputs the most frequently alternated cases in the development data, which was also used by Murata et al. (2006). The baseline score was higher than that reported by Murata et al. because we modified 21 annotations. We also performed experiments without using case distribution similarity or semantic similarity. We call these models in the first setting Model 1S and Model 1D, and these models in the second setting Model 2S and Model 2D, respectively. Although Models 1S, 1D, and 1 were fully unsupervised models, Models 1S and 1 significantly10 outperformed the baseline model (p-values of McNemar (1947)’s test were smaller than 0.00001). On the other hand, the difference between Models 1S 10In this paper, we call a difference significant if the p-value of McNemar (1947)’s test is less than 0.01. 1220 Model Accuracy (Murata et al., 2006) 0.944 (3,376/3,576) Model 3 0.956 (3,417/3,576) Table 6: Comparison between Murata et al. (2006)’s method and our method with training data. and 1 is not statistically significant, and thus the effect of the case distribution similarity was not confirmed by these experiments. Models 2S, 2D, and 2 were models with parameter tuning. Parameter tuning significant</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12:153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Conversion of Japanese passive/causative sentences into active sentences using machine learning.</title>
<date>2003</date>
<booktitle>In Proc. of CICLing’03,</booktitle>
<pages>115--125</pages>
<contexts>
<context position="35772" citStr="Murata and Isahara (2003)" startWordPosition="5941" endWordPosition="5944">xpressed by using the auxiliary verbs “•L;6 (seru)” and “cam•L;6 (saseru).” We basically used the same algorithm as Algorithm 1 for acquiring the knowledge for case alternation, but used different constraints on case alternation patterns because possible case alternation patterns are different from those of active-passive alternation. Specifically, we replaced the third and fourth lines of Algorithm 1 with “for each cto ga E tNIL, nil” and “for each cga to E two, nil,” respectively, based on linguistic analysis of active-causative alternation in Japanese. We used a part of the data created by Murata and Isahara (2003) to evaluate the usefulness of the acquired knowledge. Their data consists of 4,671 case particles in passive or causative sentences from the Kyoto University Text Corpus with their cases in the active voice. We first extracted 524 case particles that were extracted from causative sentences. Since the annotation quality was not very high, we manually checked all tags and modified inappropriate ones. We then performed 2-fold cross validation experiments. Table 8 shows experimental results. Baseline is a system that outputs the most frequently alternated cases in the training data. The differenc</context>
</contexts>
<marker>Murata, Isahara, 2003</marker>
<rawString>Masaki Murata and Hitoshi Isahara. 2003. Conversion of Japanese passive/causative sentences into active sentences using machine learning. In Proc. of CICLing’03, pages 115–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Toshiyuki Kanamaru</author>
<author>Tamotsu Shirado</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Machine-learning-based transformation of passive Japanese sentences into active by separating training data into each input particle.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL’06,</booktitle>
<pages>587--594</pages>
<contexts>
<context position="7490" citStr="Murata et al. (2006)" startWordPosition="1170" endWordPosition="1173">assive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus 1214 Case particle Grammatical function ga nominative wo accusative ni dative de locative, instrumental kara ablative no genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classifiers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1</context>
<context position="20763" citStr="Murata et al. (2006)" startWordPosition="3389" endWordPosition="3392">sidered to be correct. However, if we consider only the semantic similarity, an alignment A2 = {ga → ni, ni → ga, wo → wo} is selected, because the alignment A2 has the highest semantic similarity. On the other hand, the case distribution similarity simDIST (A1) = cos((17722,122273,0), (33338, 800, 382)) ≈ 0.167 is much larger than simDIST (A2) = cos((17722,122273,96), (800, 382, 33338)) ≈ 0.016. Thus, the alignment A1 would be selected by considering the case distribution similarity. Preference of alternation patterns Some alternation patterns often appear, and others do not. For example, as Murata et al. (2006) reported, whereas 96.47% of ga-case is alternated with wocase in passive-active transformation in Japanese, 6This case frame should not have wo-case. However, since we constructed case frames automatically, some case frames have improper cases. Case Frame: “選ばれる-1 (be selected-1)” { 選手 (player):1119, 作品 (work):983, · · · 1-ga:17722 {代表 (representative):18295, · · · 1-ni:122273 { 作品 (work):5, 市長 (mayor):3, · · · 1-wo:96 Case Frame: “選ぶ-13 (select-13)” { 私 (I):14, 先生 (teacher):18, · · · 1-ga:382 { 優秀賞 (award):42, シングル (single):17, · · · 1-ni:800 { 曲 (tune):16666, 作品 (work):9967, · · · 1-wo:3333</context>
<context position="24506" citStr="Murata et al. (2006)" startWordPosition="4039" endWordPosition="4042">. 6 Evaluation of the Acquired Knowledge We applied our algorithm to the case frames that are automatically constructed from a corpus consisting of about 6.9 billion Japanese sentences from the Web. Of course, these case frames contain improper ones, that is, several frames mix several meanings or usages of the predicates. Thus, it is difficult to evaluate the acquired knowledge itself. Instead, we evaluate the usefulness of the acquired knowledge on a case alternation task between the passive and active voices. 6.1 Setting and Algorithm for Case Alternation We basically used the same data as Murata et al. (2006). As mentioned in Section 2, they extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus, and tagged their cases in the active voice. Since they treated possessor passive as a kind of indirect passive, they did not adopt the case alternation between ga and no. In addition, their data included some annotation errors. We thus modified 21 annotations,8 five of which 8The modified version of the data is publicly available at http://alaginrc.nict.go.jp/case/src/kaku1.1.tar.gz. were changed to the case alternation between ga and no. Note that there were some cases </context>
<context position="27489" citStr="Murata et al. (2006)" startWordPosition="4554" endWordPosition="4557">acc = 0 3: while acc &gt; pre acc 4: pre acc = acc 5: for i E {0,...,|x |− 11 6: acc+ = faccuracy(x0,..., xi + 0.1,... ,x|x|−1) 7: acc− = faccuracy(x0, ..., xi − 0.1, ..., x|x|−1) 8: if acc+ &gt;acc and acc+ &gt;acc− then xi =xi+0.1 8: else if acc− &gt; acc then xi = xi − 0.1 9: acc = faccuracy(x) 10: end for 11: end while Experiments with development data In the second setting, we aligned the input passive case frame to one of the active case frames of the same predicate by using simSEM, simDIST , and fPP with α tuned on the development data. In advance, we divided the tagged data into two parts just as Murata et al. (2006) did, both of which contained 1,788 case particles, and performed 2-fold cross-validation. We used one part for development and the other for testing, and vice versa. We tuned w(ga → cga to), w(cto ga → ga) in Equation (i), and α in Equation (ii) by a simple hill-climbing strategy. Since the candidate cases for cga to are ni, niyotte, kara, de, and NIL, and the candidate cases for cto ga are wo, ni, no, and NIL, we defined parameter vector x as follows: x=(w(ga→ni),w(ga→niyotte),w(ga→kara), w(ga→de),w(ga→NIL),w(wo→ga), w(ni→ga),w(wo→no),w(NIL→ga), α). Algorithm 2 shows the hill-climbing algori</context>
<context position="29150" citStr="Murata et al. (2006)" startWordPosition="4825" endWordPosition="4828">this model Model 2. Experiments with training data In the third setting, we also performed 2-fold cross validation, that is, we used one part of the divided tagged corpus Model Accuracy Model 1S Parameter 0.902 (3,224/3,576) simSEM simDIST tuning ✓ Model 1D ✓ 0.857 (3,063/3,576) Model 1 ✓ ✓ 0.906 (3,239/3,576) Model 2S ✓ ✓ 0.928 (3,320/3,576) Model 2D ✓ ✓ 0.927 (3,314/3,576) Model 2 ✓ ✓ ✓ 0.938 (3,353/3,576) Baseline 0.883 (3,159/3,576) Table 5: Experimental results of case alternation without training data. for training and the other for testing, and vice versa. Although we basically applied Murata et al. (2006)’s method, which is based on SVMs, we added the output of Model 2 as a new feature. Specifically, we first tuned the parameter vector x on the training data and acquired the knowledge for case alternation with the tuned parameter. By using the acquired knowledge, we alternated the input cases in both the training and test data and obtained the resulting case of Model 2. Note that, we did not use any annotations for the test data in this process. We then trained the SVMs on the training data and applied them to the test data using the resulting case as a new feature. We call this model Model 3.</context>
<context position="30718" citStr="Murata et al., 2006" startWordPosition="5096" endWordPosition="5099">erformed experiments without using case distribution similarity or semantic similarity. We call these models in the first setting Model 1S and Model 1D, and these models in the second setting Model 2S and Model 2D, respectively. Although Models 1S, 1D, and 1 were fully unsupervised models, Models 1S and 1 significantly10 outperformed the baseline model (p-values of McNemar (1947)’s test were smaller than 0.00001). On the other hand, the difference between Models 1S 10In this paper, we call a difference significant if the p-value of McNemar (1947)’s test is less than 0.01. 1220 Model Accuracy (Murata et al., 2006) 0.944 (3,376/3,576) Model 3 0.956 (3,417/3,576) Table 6: Comparison between Murata et al. (2006)’s method and our method with training data. and 1 is not statistically significant, and thus the effect of the case distribution similarity was not confirmed by these experiments. Models 2S, 2D, and 2 were models with parameter tuning. Parameter tuning significantly improved the performance. In addition, the difference between Models 2S and 2 and the difference between Models 2D and 2 were both significant (p-values of McNemar’s test were 0.00032 and 0.00039, respectively), and thus we confirmed t</context>
<context position="36402" citStr="Murata et al. (2006)" startWordPosition="6044" endWordPosition="6047">te the usefulness of the acquired knowledge. Their data consists of 4,671 case particles in passive or causative sentences from the Kyoto University Text Corpus with their cases in the active voice. We first extracted 524 case particles that were extracted from causative sentences. Since the annotation quality was not very high, we manually checked all tags and modified inappropriate ones. We then performed 2-fold cross validation experiments. Table 8 shows experimental results. Baseline is a system that outputs the most frequently alternated cases in the training data. The difference between Murata et al. (2006)’s model11 and our method was significant (p-value of McNemar’s test was 0.0019), and we confirmed the applicability of our framework to active-causative alternation. 7 Conclusions and Future Directions We have presented a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. Our method 11In this experiment, we used the same features as those used by Murata and Isahara (2003). Model Accuracy Baseline 0.781 (409/524) Murata et al. (2006)’s model 0.836 (438/524) Our method with training data 0.872 (457/524) Table 8: Experimental resu</context>
</contexts>
<marker>Murata, Kanamaru, Shirado, Isahara, 2006</marker>
<rawString>Masaki Murata, Toshiyuki Kanamaru, Tamotsu Shirado, and Hitoshi Isahara. 2006. Machine-learning-based transformation of passive Japanese sentences into active by separating training data into each input particle. In Proc. of COLING-ACL’06, pages 587–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katashi Nagao</author>
<author>Koiti Hasida</author>
</authors>
<title>Automatic text summarization based on the global document annotation.</title>
<date>1998</date>
<booktitle>In Proc. of ACL’98,</booktitle>
<pages>917--921</pages>
<contexts>
<context position="2877" citStr="Nagao and Hasida, 1998" startWordPosition="444" endWordPosition="448">he case of “男 (man)” is ni.2 On the other hand, when we use the normalized-cases for the base form, the case of “女 (woman)” is wo2 and the case of “男 (man)” is ga, which are the same as the surface cases in the active voice as in Example (2). (1) 女が 男に 突き落とされた. woman-ga man-ni was pushed down (A woman was pushed down by a man.) (2) 男が 女を 突き落とした. man-ga woman-wo pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for </context>
</contexts>
<marker>Nagao, Hasida, 1998</marker>
<rawString>Katashi Nagao and Koiti Hasida. 1998. Automatic text summarization based on the global document annotation. In Proc. of ACL’98, pages 917–921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NLRI</author>
</authors>
<title>Bunrui Goi Hyo (in Japanese).</title>
<date>1993</date>
<publisher>Shuuei Publishing.</publisher>
<contexts>
<context position="8146" citStr="NLRI, 1993" startWordPosition="1271" endWordPosition="1272"> Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus 1214 Case particle Grammatical function ga nominative wo accusative ni dative de locative, instrumental kara ablative no genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classifiers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method first parses the input sentences, and then constructs case frames by collecting reliable modifier-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003)</context>
</contexts>
<marker>NLRI, 1993</marker>
<rawString>NLRI. 1993. Bunrui Goi Hyo (in Japanese). Shuuei Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>105</pages>
<contexts>
<context position="8666" citStr="Palmer et al., 2005" startWordPosition="1348" endWordPosition="1351">veral lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method first parses the input sentences, and then constructs case frames by collecting reliable modifier-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames defined in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can fill a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1):71– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames.</title>
<date>2011</date>
<booktitle>In Proc. of IJCNLP’11,</booktitle>
<pages>758--766</pages>
<contexts>
<context position="3060" citStr="Sasano and Kurohashi, 2011" startWordPosition="475" endWordPosition="479">ame as the surface cases in the active voice as in Example (2). (1) 女が 男に 突き落とされた. woman-ga man-ni was pushed down (A woman was pushed down by a man.) (2) 男が 女を 突き落とした. man-ga woman-wo pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics we focus on the resulting predicate argument structures, the normalized-case structure is more useful. Specifically, since a normalized-case structure repres</context>
</contexts>
<marker>Sasano, Kurohashi, 2011</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2011. A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames. In Proc. of IJCNLP’11, pages 758–766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Christian Hying</author>
<author>Christian Scheible</author>
<author>Helmut Schmid</author>
</authors>
<title>Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT’08,</booktitle>
<pages>496--504</pages>
<contexts>
<context position="6608" citStr="Walde et al. 2008" startWordPosition="1039" endWordPosition="1042">nts on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) de</context>
</contexts>
<marker>Walde, Hying, Scheible, Schmid, 2008</marker>
<rawString>Sabine Schulte im Walde, Christian Hying, Christian Scheible, and Helmut Schmid. 2008. Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. In Proc. of ACL-HLT’08, pages 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP’09,</booktitle>
<pages>638--647</pages>
<contexts>
<context position="6649" citStr="Sun and Korhonen 2009" startWordPosition="1047" endWordPosition="1050">al case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, defined in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the field of automatic verb classification, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identified the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the pas</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In Proc. of EMNLP’09, pages 638–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Sanae Fujita</author>
<author>Masaaki Nagata</author>
</authors>
<title>A Japanese predicate argument structure analysis using decision lists.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP’08,</booktitle>
<pages>523--532</pages>
<contexts>
<context position="1233" citStr="Taira et al., 2008" startWordPosition="174" endWordPosition="177">r method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1Japanese is a head-final language. Word order does not mark syntactic relations. Instead, postpositio</context>
</contexts>
<marker>Taira, Fujita, Nagata, 2008</marker>
<rawString>Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008. A Japanese predicate argument structure analysis using decision lists. In Proc. of EMNLP’08, pages 523– 532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Theijssen</author>
<author>Lou Boves</author>
<author>Hans van Halteren</author>
<author>Nelleke Oostdijk</author>
</authors>
<title>Evaluating automatic annotation: automatically detecting and enriching instances of the dative alternation.</title>
<date>2012</date>
<journal>Language Resources and Evaluation,</journal>
<volume>46</volume>
<issue>4</issue>
<marker>Theijssen, Boves, van Halteren, Oostdijk, 2012</marker>
<rawString>Daphne Theijssen, Lou Boves, Hans van Halteren, and Nelleke Oostdijk. 2012. Evaluating automatic annotation: automatically detecting and enriching instances of the dative alternation. Language Resources and Evaluation, 46(4):565–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly extracting Japanese predicateargument relation with markov logic.</title>
<date>2011</date>
<booktitle>In Proc. of IJCNLP’11,</booktitle>
<pages>1125--1133</pages>
<contexts>
<context position="1258" citStr="Yoshikawa et al., 2011" startWordPosition="178" endWordPosition="181">se frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1Japanese is a head-final language. Word order does not mark syntactic relations. Instead, postpositional case particles functi</context>
</contexts>
<marker>Yoshikawa, Asahara, Matsumoto, 2011</marker>
<rawString>Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Matsumoto. 2011. Jointly extracting Japanese predicateargument relation with markov logic. In Proc. of IJCNLP’11, pages 1125–1133.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
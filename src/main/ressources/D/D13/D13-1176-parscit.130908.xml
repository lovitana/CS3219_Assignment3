<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.989402">
Recurrent Continuous Translation Models
</title>
<author confidence="0.993312">
Nal Kalchbrenner Phil Blunsom
</author>
<affiliation confidence="0.9977335">
Department of Computer Science
University of Oxford
</affiliation>
<email confidence="0.99493">
inal.kalchbrenner,phil.blunsoml@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.998582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999439545454545">
We introduce a class of probabilistic con-
tinuous translation models called Recur-
rent Continuous Translation Models that are
purely based on continuous representations
for words, phrases and sentences and do not
rely on alignments or phrasal translation units.
The models have a generation and a condi-
tioning aspect. The generation of the transla-
tion is modelled with a target Recurrent Lan-
guage Model, whereas the conditioning on the
source sentence is modelled with a Convolu-
tional Sentence Model. Through various ex-
periments, we show first that our models ob-
tain a perplexity with respect to gold transla-
tions that is &gt; 43% lower than that of state-
of-the-art alignment-based translation models.
Secondly, we show that they are remarkably
sensitive to the word order, syntax, and mean-
ing of the source sentence despite lacking
alignments. Finally we show that they match a
state-of-the-art system when rescoring n-best
lists of translations.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998712045454546">
In most statistical approaches to machine transla-
tion the basic units of translation are phrases that are
composed of one or more words. A crucial com-
ponent of translation systems are models that esti-
mate translation probabilities for pairs of phrases,
one phrase being from the source language and the
other from the target language. Such models count
phrase pairs and their occurrences as distinct if the
surface forms of the phrases are distinct. Although
distinct phrase pairs often share significant similari-
ties, linguistic or otherwise, they do not share statis-
tical weight in the models’ estimation of their trans-
lation probabilities. Besides ignoring the similar-
ity of phrase pairs, this leads to general sparsity is-
sues. The estimation is sparse or skewed for the
large number of rare or unseen phrase pairs, which
grows exponentially in the length of the phrases, and
the generalisation to other domains is often limited.
Continuous representations have shown promise
at tackling these issues. Continuous representations
for words are able to capture their morphological,
syntactic and semantic similarity (Collobert and We-
ston, 2008). They have been applied in continu-
ous language models demonstrating the ability to
overcome sparsity issues and to achieve state-of-the-
art performance (Bengio et al., 2003; Mikolov et
al., 2010). Word representations have also shown
a marked sensitivity to conditioning information
(Mikolov and Zweig, 2012). Continuous repre-
sentations for characters have been deployed in
character-level language models demonstrating no-
table language generation capabilities (Sutskever et
al., 2011). Continuous representations have also
been constructed for phrases and sentences. The rep-
resentations are able to carry similarity and task de-
pendent information, e.g. sentiment, paraphrase or
dialogue labels, significantly beyond the word level
and to accurately predict labels for a highly diverse
range of unseen phrases and sentences (Grefenstette
et al., 2011; Socher et al., 2011; Socher et al., 2012;
Hermann and Blunsom, 2013; Kalchbrenner and
Blunsom, 2013).
Phrase-based continuous translation models were
first proposed in (Schwenk et al., 2006) and re-
</bodyText>
<page confidence="0.886424">
1700
</page>
<note confidence="0.729133">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999948954545455">
cently further developed in (Schwenk, 2012; Le et
al., 2012). The models incorporate a principled way
of estimating translation probabilities that robustly
extends to rare and unseen phrases. They achieve
significant Bleu score improvements and yield se-
mantically more suggestive translations. Although
wide-reaching in their scope, these models are lim-
ited to fixed-size source and target phrases and sim-
plify the dependencies between the target words tak-
ing into account restricted target language modelling
information.
We describe a class of continuous translation
models called Recurrent Continuous Translation
Models (RCTM) that map without loss of generality
a sentence from the source language to a probabil-
ity distribution over the sentences in the target lan-
guage. We define two specific RCTM architectures.
Both models adopt a recurrent language model for
the generation of the target translation (Mikolov et
al., 2010). In contrast to other n-gram approaches,
the recurrent language model makes no Markov as-
sumptions about the dependencies of the words in
the target sentence.
The two RCTMs differ in the way they condi-
tion the target language model on the source sen-
tence. The first RCTM uses the convolutional sen-
tence model (Kalchbrenner and Blunsom, 2013) to
transform the source word representations into a rep-
resentation for the source sentence. The source sen-
tence representation in turn constraints the genera-
tion of each target word. The second RCTM intro-
duces an intermediate representation. It uses a trun-
cated variant of the convolutional sentence model to
first transform the source word representations into
representations for the target words; the latter then
constrain the generation of the target sentence. In
both cases, the convolutional layers are used to gen-
erate combined representations for the phrases in a
sentence from the representations of the words in the
sentence.
An advantage of RCTMs is the lack of latent
alignment segmentations and the sparsity associated
with them. Connections between source and target
words, phrases and sentences are learnt only implic-
itly as mappings between their continuous represen-
tations. As we see in Sect. 5, these mappings of-
ten carry remarkably precise morphological, syntac-
tic and semantic information. Another advantage is
that the probability of a translation under the models
is efficiently computable requiring a small number
of matrix-vector products that is linear in the length
of the source and the target sentence. Further, trans-
lations can be generated directly from the probabil-
ity distribution of the RCTM without any external
resources.
We evaluate the performance of the models in four
experiments. Since the translation probabilities of
the RCTMs are tractable, we can measure the per-
plexity of the models with respect to the reference
translations. The perplexity of the models is signifi-
cantly lower than that of IBM Model 1 and is &gt; 43%
lower than the perplexity of a state-of-the-art variant
of the IBM Model 2 (Brown et al., 1993; Dyer et
al., 2013). The second and third experiments aim to
show the sensitivity of the output of the RCTM II
to the linguistic information in the source sentence.
The second experiment shows that under a random
permutation of the words in the source sentences,
the perplexity of the model with respect to the refer-
ence translations becomes significantly worse, sug-
gesting that the model is highly sensitive to word
position and order. The third experiment inspects
the translations generated by the RCTM II. The
generated translations demonstrate remarkable mor-
phological, syntactic and semantic agreement with
the source sentence. Finally, we test the RCTMs
on the task of rescoring n-best lists of translations.
The performance of the RCTM probabilities joined
with a single word penalty feature matches the per-
formance of the state-of-the-art translation system
cdec that makes use of twelve features including
five alignment-based translation models (Dyer et al.,
2010).
We proceed as follows. We begin in Sect. 2 by
describing the general modelling framework under-
lying the RCTMs. In Sect. 3 we describe the RCTM
I and in Sect. 4 the RCTM II. Section 5 is dedicated
to the four experiments and we conclude in Sect. 6.1
</bodyText>
<sectionHeader confidence="0.996002" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.999185">
We begin by describing the modelling framework
underlying RCTMs. An RCTM estimates the proba-
bility P(f|e) of a target sentence f = f1, ..., f,,t being
a translation of a source sentence e = e1, ..., ek. Let
</bodyText>
<footnote confidence="0.906409">
1Code and models available at nal.co
</footnote>
<page confidence="0.99313">
1701
</page>
<bodyText confidence="0.9993065">
us denote by fi:j the substring of words fi,..., fj. Us-
ing the following identity,
</bodyText>
<equation confidence="0.999494">
P(f|e) = �m P(fi|f1:i−1,e) (1)
i=1
</equation>
<bodyText confidence="0.999787516129032">
an RCTM estimates P(f|e) by directly computing
for each target position i the conditional probability
P(fi|f1:i−1, e) of the target word fi occurring in the
translation at position i, given the preceding target
words f1:i−1 and the source sentence e. We see that
an RCTM is sensitive not just to the source sentence
e but also to the preceding words f1:i−1 in the target
sentence; by doing so it incorporates a model of the
target language itself.
To model the conditional probability P(f|e), an
RCTM comprises both a generative architecture for
the target sentence and an architecture for condition-
ing the latter on the source sentence. To fully cap-
ture Eq. 1, we model the generative architecture with
a recurrent language model (RLM) based on a re-
current neural network (Mikolov et al., 2010). The
prediction of the i-th word fi in a RLM depends on
all the preceding words f1:i−1 in the target sentence
ensuring that conditional independence assumptions
are not introduced in Eq. 1. Although the predic-
tion is most strongly influenced by words closely
preceding fi, long-range dependencies from across
the whole sentence can also be exhibited. The con-
ditioning architectures are model specific and are
treated in Sect. 3-4. Both the generative and con-
ditioning aspects of the models deploy continuous
representations for the constituents and are trained
as a single joint architecture. Given the modelling
framework underlying RCTMs, we now proceed to
describe in detail the recurrent language model un-
derlying the generative aspect.
</bodyText>
<subsectionHeader confidence="0.989374">
2.1 Recurrent Language Model
</subsectionHeader>
<bodyText confidence="0.9988004">
A RLM models the probability P(f) that the se-
quence of words f occurs in a given language. Let
f = f1,..., fm be a sequence of m words, e.g. a sen-
tence in the target language. Analogously to Eq. 1,
using the identity,
</bodyText>
<equation confidence="0.998712">
P(f) = �m P(fi|f1:i−1) (2)
i=1
</equation>
<bodyText confidence="0.9789855">
the model explicitly computes without simpli-
fying assumptions the conditional distributions
</bodyText>
<figure confidence="0.446102">
fi-1 P(fi ) fi P(f i+ 1) fi+1 P(f i+2)
</figure>
<figureCaption confidence="0.958796833333333">
Figure 1: A RLM (left) and its unravelling to depth 3
(right). The recurrent transformation is applied to the hid-
den layer hi_1 and the result is summed to the represen-
tation for the current word fi. After a non-linear transfor-
mation, a probability distribution over the next word fi+1
is predicted.
</figureCaption>
<bodyText confidence="0.997996125">
P(fi|f1:i−1). The architecture of a RLM comprises
a vocabulary V that contains the words fi of the
language as well as three transformations: an in-
put vocabulary transformation I ∈ Rq×|V |, a re-
current transformation R ∈ Rq×q and an output
vocabulary transformation O ∈ R|V |×q. For each
word fk ∈ V , we indicate by i(fk) its index in V
and by v(fk) ∈ R|V |×1 an all zero vector with only
</bodyText>
<equation confidence="0.932042">
v(fk)i(��) = 1.
</equation>
<bodyText confidence="0.99997975">
For a word fi, the result of I · v(fi) ∈ Rq×1 is
the input continuous representation of fi. The pa-
rameter q governs the size of the word representa-
tion. The prediction proceeds by successively ap-
plying the recurrent transformation R to the word
representations and predicting the next word at each
step. In detail, the computation of each P(fi|f1:i−1)
proceeds recursively. For 1 &lt; i &lt; m,
</bodyText>
<equation confidence="0.924249833333333">
h1 = Q(I · v(f1)) (3a)
hi+1 = Q(R · hi + I · v(fi+1)) (3b)
oi+1 = O · hi (3c)
and the conditional distribution is given by,
P(fi = v|f1:i−1) = vxp (oi,v) (4)
Ev=1 exp(oi,v)
</equation>
<bodyText confidence="0.999944833333334">
In Eq. 3, Q is a nonlinear function such as tanh. Bias
values bh and bo are included in the computation. An
illustration of the RLM is given in Fig. 1.
The RLM is trained by backpropagation through
time (Mikolov et al., 2010). The error in the pre-
dicted distribution calculated at the output layer is
</bodyText>
<equation confidence="0.859582875">
I O
R
h
I
hi-1 h h
i i+1
O
R
</equation>
<page confidence="0.946184">
1702
</page>
<bodyText confidence="0.9998576">
backpropagated through the recurrent layers and cu-
mulatively added to the errors of the previous predic-
tions for a given number d of steps. The procedure is
equivalent to standard backpropagation over a RLM
that is unravelled to depth d as in Fig. 1.
RCTMs may be thought of as RLMs, in which
the predicted distributions for each word fi are con-
ditioned on the source sentence e. We next define
two conditioning architectures each giving rise to a
specific RCTM.
</bodyText>
<sectionHeader confidence="0.9926835" genericHeader="method">
3 Recurrent Continuous Translation
Model I
</sectionHeader>
<bodyText confidence="0.99996828">
The RCTM I uses a convolutional sentence model
(CSM) in the conditioning architecture. The CSM
creates a representation for a sentence that is pro-
gressively built up from representations of the n-
grams in the sentence. The CSM embodies a hierar-
chical structure. Although it does not make use of an
explicit parse tree, the operations that generate the
representations act locally on small n-grams in the
lower layers of the model and act increasingly more
globally on the whole sentence in the upper layers
of the model. The lack of the need for a parse tree
yields two central advantages over sentence models
that require it (Grefenstette et al., 2011; Socher et
al., 2012). First, it makes the model robustly appli-
cable to a large number of languages for which accu-
rate parsers are not available. Secondly, the transla-
tion probability distribution over the target sentences
does not depend on the chosen parse tree.
The RCTM I conditions the probability of each
target word fi on the continuous representation of the
source sentence e generated through the CSM. This
is accomplished by adding the sentence representa-
tion to each hidden layer hi in the target recurrent
language model. We next describe the procedure in
more detail, starting with the CSM itself.
</bodyText>
<subsectionHeader confidence="0.997962">
3.1 Convolutional Sentence Model
</subsectionHeader>
<bodyText confidence="0.999879166666667">
The CSM models the continuous representation of
a sentence based on the continuous representations
of the words in the sentence. Let e = e1...ek be
a sentence in a language and let v(ei) E Rq,1 be
the continuous representation of the word ei. Let
Ee E Rq,k be the sentence matrix for e defined by,
</bodyText>
<equation confidence="0.6980725">
Ee:,i = v(ei) (5)
the cat sat on the mat
</equation>
<figureCaption confidence="0.996004">
Figure 2: A CSM for a six word source sentence e and the
computed sentence representation e. K2, K3 are weight
matrices and L3 is a top weight matrix. To the right, an
instance of a one-dimensional convolution between some
weight matrix Ki and a generic matrix M that could for
instance correspond to Ee2. The color coding of weights
indicates weight sharing.
</figureCaption>
<bodyText confidence="0.999464772727273">
The main component of the architecture of the CSM
is a sequence of weight matrices (Ki)2&lt;i&lt;r that cor-
respond to the kernels or filters of the convolution
and can be thought of as learnt feature detectors.
From the sentence matrix Ee the CSM computes a
continuous vector representation e E Rq,1 for the
sentence e by applying a sequence of convolutions
to Ee whose weights are given by the weight matri-
ces. The weight matrices and the sequence of con-
volutions are defined next.
We denote by (Ki)2&lt;i&lt;r a sequence of weight
matrices where each Ki E Rq,i is a matrix of i
.\/
columns and r = F 2N], where N is the length of
the longest source sentence in the training set. Each
row of Ki is a vector of i weights that is treated as
the kernel or filter of a one-dimensional convolution.
Given for instance a matrix M E Rq,j where the
number of columns j &gt; i, each row of Ki can be
convolved with the corresponding row in M, result-
ing in a matrix Ki * M, where * indicates the con-
volution operation and (Ki * M) E Rq,(j−i+1). For
</bodyText>
<equation confidence="0.990174">
i = 3, the value (Ki * M):,a is computed by:
Ki:,1 0 M:,a + Ki:,2 0 M:,a+1 + Ki:,3 0 M:,a+2 (6)
</equation>
<bodyText confidence="0.971451666666667">
where 0 is component-wise vector product. Ap-
plying the convolution kernel Ki yields a matrix
(Ki*M) that has i−1 columns less than the original
matrix M.
Given a source sentence of length k, the CSM
convolves successively with the sentence matrix Ee
</bodyText>
<equation confidence="0.953732">
e
K 3
K 2
L 3
K ` K ` :,2 K `
:,1 :,3
(K`* M):,1
E e
M M M
:,1 :,2 :,3
</equation>
<page confidence="0.804235">
1703
</page>
<bodyText confidence="0.997572">
the sequence of weight matrices (Ki)2≤i≤r, one af-
ter the other starting with K2 as follows:
</bodyText>
<equation confidence="0.997735">
E 1 = E (7a)
E i+1 = Q(Ki+1 * E i) (7b)
</equation>
<bodyText confidence="0.998186470588235">
After a few convolution operations, E i is either a
vector in Rq×1, in which case we obtained the de-
sired representation, or the number of columns in
E i is smaller than the number i + 1 of columns in
the next weight matrix Ki+1. In the latter case, we
equally obtain a vector in Rq×1 by simply apply-
ing a top weight matrix Lj that has the same num-
ber of columns as E i. We thus obtain a sentence
representation e E Rq×1 for the source sentence e.
Note that the convolution operations in Eq. 7b are
interleaved with non-linear functions Q. Note also
that, given the different levels at which the weight
matrices Ki and Li are applied, the top weight
matrix Lj comes from an additional sequence of
weight matrices (Li)2≤i≤r distinct from (Ki)2≤i≤r.
Fig. 2 depicts an instance of the CSM and of a one-
dimensional convolution.2
</bodyText>
<subsectionHeader confidence="0.989129">
3.2 RCTM I
</subsectionHeader>
<bodyText confidence="0.999961">
As defined in Sect. 2, the RCTM I models the condi-
tional probability P(f|e) of a sentence f = f1, ..., fm
in a target language F being the translation of a sen-
tence e = e1, ..., ek in a source language E. Accord-
ing to Eq. 1, the RCTM I explicitly computes the
conditional distributions P(fi|f1:i−1, e). The archi-
tecture of the RCTM I comprises a source vocabu-
lary V E and a target vocabulary V F, two sequences
of weight matrices (Ki)2≤i≤r and (Li)2≤i≤r that
are part of the constituent CSM, transformations
</bodyText>
<equation confidence="0.639705">
I E Rq×|V F|, R E Rq×q and O E R|V F|×q that are
</equation>
<bodyText confidence="0.994622833333333">
part of the constituent RLM and a sentence transfor-
mation S E Rq×q. We write e = csm(e) for the
output of the CSM with e as the input sentence.
The computation of the RCTM I is a simple mod-
ification to the computation of the RLM described in
Eq. 3. It proceeds recursively as follows:
</bodyText>
<equation confidence="0.99959025">
s = S · csm(e)
h1 = Q(I · v(f1) + s)
hi+1 = Q(R · hi + I · v(fi+1) + s)
oi+1 = O · hi
</equation>
<footnote confidence="0.7368965">
2For a formal treatment of the construction, see (Kalchbren-
ner and Blunsom, 2013).
</footnote>
<bodyText confidence="0.999891214285714">
and the conditional distributions P(fi+1|f1:i, e) are
obtained from oi as in Eq. 4. Q is a nonlinear func-
tion and bias values are included throughout the
computation. Fig. 3 illustrates an RCTM I.
Two aspects of the RCTM I are to be remarked.
First, the length of the target sentence is predicted
by the target RLM itself that by its architecture has
a bias towards shorter sentences. Secondly, the rep-
resentation of the source sentence e constraints uni-
formly all the target words, contrary to the fact that
the target words depend more strongly on certain
parts of the source sentence and less on other parts.
The next model proposes an alternative formulation
of these aspects.
</bodyText>
<sectionHeader confidence="0.9909155" genericHeader="method">
4 Recurrent Continuous Translation
Model II
</sectionHeader>
<bodyText confidence="0.997505111111111">
The central idea behind the RCTM II is to first es-
timate the length m of the target sentence indepen-
dently of the main architecture. Given m and the
source sentence e, the model constructs a represen-
tation for the n-grams in e, where n is set to 4. Note
that each level of the CSM yields n-gram represen-
tations of e for a specific value of n. The 4-gram
representation of e is thus constructed by truncat-
ing the CSM at the level that corresponds to n = 4.
The procedure is then inverted. From the 4-gram
representation of the source sentence e, the model
builds a representation of a sentence that has the
predicted length m of the target. This is similarly
accomplished by truncating the inverted CSM for a
sentence of length m.
We next describe in detail the Convolutional n-
gram Model (CGM). Then we return to specify the
RCTM II.
</bodyText>
<subsectionHeader confidence="0.997161">
4.1 Convolutional n-gram model
</subsectionHeader>
<bodyText confidence="0.999942727272727">
The CGM is obtained by truncating the CSM at the
level where n-grams are represented for the chosen
value of n. A column g of a matrix E i obtained
according to Eq. 7 represents an n-gram from the
source sentence e. The value of n corresponds to
the number of word vectors from which the n-gram
representation g is constructed; equivalently, n is
the span of the weights in the CSM underneath g
(see Fig. 2-3). Note that any column in a matrix
E i represents an n-gram with the same span value
n. We denote by gram(E i) the size of the n-grams
</bodyText>
<page confidence="0.995124">
1704
</page>
<figure confidence="0.969903125">
RCTM I
RCTM II
P( f  |e )
e
S
e
csm
P( f  |m, e )
e
S
T
Eg
F g
F
icgm
cgm
</figure>
<figureCaption confidence="0.996416">
Figure 3: A graphical depiction of the two RCTMs. Arrows represent full matrix transformations while lines are
vector transformations corresponding to columns of weight matrices.
</figureCaption>
<bodyText confidence="0.997439705882353">
represented by EZ. For example, for a sufficiently
long sentence e, gram(E2) = 2, gram(E3) = 4,
gram(E4) = 7. We denote by cgm(e, n) that matrix
EZ from the CSM that represents the n-grams of the
source sentence e.
The CGM can also be inverted to obtain a repre-
sentation for a sentence from the representation of
its n-grams. We denote by icgm the inverse CGM,
which depends on the size of the n-gram represen-
tation cgm(e, n) and on the target sentence length
m. The transformation icgm unfolds the n-gram
representation onto a representation of a target sen-
tence with m words. The architecture corresponds
to an inverted CGM or, equivalently, to an inverted
truncated CSM (Fig. 3). Given the transformations
cgm and icgm, we now detail the computation of the
RCTM II.
</bodyText>
<subsectionHeader confidence="0.850255">
4.2 RCTM II
</subsectionHeader>
<bodyText confidence="0.893394">
The RCTM II models the conditional probability
P(f|e) by factoring it as follows:
</bodyText>
<equation confidence="0.99650425">
P(f|e) = P(f|m, e) · P(m|e) (9a)
M
P(fi+1|f1:i, m, e) · P(m|e) (9b)
i=1
</equation>
<bodyText confidence="0.991061444444445">
and computing the distributions P(fi+1|f1:i, m, e)
and P(m|e). The architecture of the RCTM II
comprises all the elements of the RCTM I together
with the following additional elements: a translation
transformation Tqxq and two sequences of weight
matrices (Ji)2&lt;i&lt;, and (Hi)2&lt;i&lt;, that are part of
the icgm3.
The computation of the RCTM II proceeds recur-
sively as follows:
</bodyText>
<equation confidence="0.999990833333333">
E9 = cgm(e, 4)
F9:,. = Q(T · E9 :,.)
F = icgm(F9, m)
h1 = Q(I · v(f1) + S · F:,1)
hi+1 = Q(R · hi + I · v(fi+1) + S · F:,i+1)
oi+1 = O · hi
</equation>
<bodyText confidence="0.9995544">
and the conditional distributions P(fi+1|f1:i, e) are
obtained from oi as in Eq. 4. Note how each re-
constructed vector F:,i is added successively to the
corresponding layer hi that predicts the target word
fi. The RCTM II is illustrated in Fig. 3.
</bodyText>
<footnote confidence="0.557078666666667">
3Just like r the value s is small and depends on the length
of the source and target sentences in the training set. See
Sect. 5.1.2.
</footnote>
<page confidence="0.987209">
1705
</page>
<bodyText confidence="0.9974125">
For the separate estimation of the length of the
translation, we estimate the conditional probability
</bodyText>
<equation confidence="0.988223">
P(m|e) by letting,
P(m|e) = P(m|k) = Poisson(Ak) (11)
</equation>
<bodyText confidence="0.99995175">
where k is the length of the source sentence e and
Poisson(A) is a Poisson distribution with mean A.
This concludes the description of the RCTM II. We
now turn to the experiments.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999970571428571">
We report on four experiments. The first experiment
considers the perplexities of the models with respect
to reference translations. The second and third ex-
periments test the sensitivity of the RCTM II to the
linguistic aspects of the source sentences. The fi-
nal experiment tests the rescoring performance of
the two models.
</bodyText>
<subsectionHeader confidence="0.989193">
5.1 Training
</subsectionHeader>
<bodyText confidence="0.999808666666667">
Before turning to the experiments, we describe the
data sets, hyper parameters and optimisation algo-
rithms used for the training of the RCTMs.
</bodyText>
<subsubsectionHeader confidence="0.79236">
5.1.1 Data sets
</subsubsectionHeader>
<bodyText confidence="0.999908782608696">
The training set used for all the experiments com-
prises a bilingual corpus of 144953 pairs of sen-
tences less than 80 words in length from the news
commentary section of the Eighth Workshop on Ma-
chine Translation (WMT) 2013 training data. The
source language is English and the target language
is French. The English sentences contain about
4.1M words and the French ones about 4.5M words.
Words in both the English and French sentences
that occur twice or less are substituted with the
(unknown) token. The resulting vocabularies V E
and V F contain, respectively, 25403 English words
and 34831 French words.
For the experiments we use four different test sets
comprised of the Workshop on Machine Transla-
tion News Test (WMT-NT) sets for the years 2009,
2010, 2011 and 2012. They contain, respectively,
2525, 2489, 3003 and 3003 pairs of English-French
sentences. For the perplexity experiments unknown
words occurring in these data sets are replaced with
the (unknown) token. The respective 2008 WMT-
NT set containing 2051 pairs of English-French sen-
tences is used as the validation set throughout.
</bodyText>
<subsectionHeader confidence="0.521887">
5.1.2 Model hyperparameters
</subsectionHeader>
<bodyText confidence="0.999993125">
The parameter q that defines the size of the En-
glish vectors v(ei) for ei E V E, the size of the hid-
den layer hi and the size of the French vectors v(fi)
for v(fi) E V F is set to q = 256. This yields a
relatively small recurrent matrix and corresponding
models. To speed up training, we factorize the target
vocabulary V F into 256 classes following the proce-
dure in (Mikolov et al., 2011).
The RCTM II uses a convolutional n-gram model
CGM where n is set to 4. For the RCTM I, the num-
ber of weight matrices r for the CSM is 15, whereas
in the RCTM II the number r of weight matrices for
the CGM is 7 and the number s of weight matrices
for the inverse CGM is 9. If a test sentence is longer
than all training sentences and a larger weight matrix
is required by the model, the larger weight matrix is
easily factorized into two smaller weight matrices
whose weights have been trained. For instance, if a
weight matrix of 10 weights is required, but weight
matrices have been trained only up to weight 9, then
one can factorize the matrix of 10 weights with one
of 9 and one of 2. Across all test sets the proportion
of sentence pairs that require larger weight matrices
to be factorized into smaller ones is &lt; 0.1%.
</bodyText>
<subsubsectionHeader confidence="0.731768">
5.1.3 Objective and optimisation
</subsubsectionHeader>
<bodyText confidence="0.999955904761905">
The objective function is the average of the sum
of the cross-entropy errors of the predicted words
and the true words in the French sentences. The En-
glish sentences are taken as input in the prediction
of the French sentences, but they are not themselves
ever predicted. An l2 regularisation term is added to
the objective. The training of the model proceeds by
back-propagation through time. The cross-entropy
error calculated at the output layer at each step is
back-propagated through the recurrent structure for
a number d of steps; for all models we let d = 6.
The error accumulated at the hidden layers is then
further back-propagated through the transformation
S and the CSM/CGM to the input vectors v(ei) of
the English input sentence e. All weights, includ-
ing the English vectors, are randomly initialised and
inferred during training.
The objective is minimised using mini-batch
adaptive gradient descent (Adagrad) (Duchi et al.,
2011). The training of an RCTM takes about 15
hours on 3 multicore CPUs. While our experiments
</bodyText>
<page confidence="0.965977">
1706
</page>
<table confidence="0.996806285714286">
WMT-NT 2009 2010 2011 2012
KN-5 218 213 222 225
RLM 178 169 178 181
IBM 1 207 200 188 197
FA-IBM 2 153 146 135 144
RCTM I 143 134 140 142
RCTM II 86 77 76 77
</table>
<tableCaption confidence="0.999848">
Table 1: Perplexity results on the WMT-NT sets.
</tableCaption>
<bodyText confidence="0.985869333333333">
are relatively small, we note that in principle our
models should scale similarly to RLMs which have
been applied to hundreds of millions of words.
</bodyText>
<subsectionHeader confidence="0.999747">
5.2 Perplexity of gold translations
</subsectionHeader>
<bodyText confidence="0.99998852">
Since the computation of the probability of a trans-
lation under one of the RCTMs is efficient, we can
compute the perplexities of the RCTMs with respect
to the reference translations in the test sets. The per-
plexity measure is an indication of the quality that
a model assigns to a translation. We compare the
perplexities of the RCTMs with the perplexity of the
IBM Model 1 (Brown et al., 1993) and of the Fast-
Aligner (FA-IBM 2) model that is a state-of-the-art
variant of IBM Model 2 (Dyer et al., 2013). We add
as baselines the unconditional target RLM and a 5-
gram target language model with modified Kneser-
Nay smoothing (KN-5). The results are reported in
Tab. 1. The RCTM II obtains a perplexity that is
&gt; 43% lower than that of the alignment based mod-
els and that is 40% lower than the perplexity of the
RCTM I. The low perplexity of the RCTMs suggests
that continuous representations and the transforma-
tions between them make up well for the lack of ex-
plicit alignments. Further, the difference in perplex-
ity between the RCTMs themselves demonstrates
the importance of the conditioning architecture and
suggests that the localised 4-gram conditioning in
the RCTM II is superior to the conditioning with the
whole source sentence of the RCTM I.
</bodyText>
<subsectionHeader confidence="0.998605">
5.3 Sensitivity to source sentence structure
</subsectionHeader>
<bodyText confidence="0.99936375">
The second experiment aims at showing the sensi-
tivity of the RCTM II to the order and position of
words in the English source sentence. To this end,
we randomly permute in the training and testing sets
</bodyText>
<table confidence="0.8638035">
WMT-NT PERM 2009 2010 2011 2012
RCTM II 174 168 175 178
</table>
<tableCaption confidence="0.977683666666667">
Table 2: Perplexity results of the RCTM II on the WMT-
NT sets where the words in the English source sentences
are randomly permuted.
</tableCaption>
<bodyText confidence="0.999929333333333">
the words in the English source sentence. The re-
sults on the permuted data are reported in Tab. 2. If
the RCTM II were roughly comparable to a bag-of-
words approach, there would be no difference under
the permutation of the words. By contrast, the dif-
ference of the results reported in Tab. 2 with those
reported in Tab. 1 is very significant, clearly indicat-
ing the sensitivity to word order and position of the
translation model.
</bodyText>
<subsubsectionHeader confidence="0.739347">
5.3.1 Generating from the RCTM II
</subsubsectionHeader>
<bodyText confidence="0.999962133333333">
To show that the RCTM II is sensitive not only to
word order, but also to other syntactic and semantic
traits of the sentence, we generate and inspect can-
didate translations for various English source sen-
tences. The generation proceeds by sampling from
the probability distribution of the RCTM II itself and
does not depend on any other external resources.
Given an English source sentence e, we let m be
the length of the gold translation and we search the
distribution computed by the RCTM II over all sen-
tences of length m. The number of possible target
sentences of length m amounts to IV &apos; = 34831&apos;
where V = V F is the French vocabulary; directly
considering all possible translations is intractable.
We proceed as follows: we sample with replace-
ment 2000 sentences from the distribution of the
RCTM II, each obtained by predicting one word at
a time. We start by predicting a distribution for the
first target word, restricting that distribution to the
top 5 most probable words and sampling the first
word of a candidate translation from the restricted
distribution of 5 words. We proceed similarly for
the remaining words. Each sampled sentence has a
well-defined probability assigned by the model and
can thus be ranked. Table 3 gives various English
source sentences and some candidate French trans-
lations generated by the RCTM II together with their
ranks.
The results in Tab. 3 show the remarkable syn-
tactic agreements of the candidate translations; the
</bodyText>
<page confidence="0.982198">
1707
</page>
<note confidence="0.242495">
English source sentence French gold translation RCTM II candidate translation Rank
</note>
<tableCaption confidence="0.723825833333333">
the patient is sick. le patient est malade . le patient est insuffisante . 1
le patient est mort . 4
la patient est insuffisante . 23
the patient is dead. le patient est mort . le patient est mort . 1
le patient est d´epass´e . 4
the patient is ill. le patient est malade . le patient est mal . 3
the patients are sick. les patients sont malades . les patients sont confront´es . 2
les patients sont corrompus . 5
the patients are dead. les patients sont morts . les patients sont morts . 1
the patients are ill. les patients sont malades . les patients sont confront´es . 5
the patient was ill. le patient ´etait malade . le patient ´etait mal . 2
the patients are not dead. les patients ne sont pas morts . les patients ne sont pas morts . 1
the patients are not sick. les patients ne sont pas malades . les patients ne sont pas (unknown) . 1
les patients ne sont pas mal . 6
the patients were saved. les patients ont ´et´e sauv´es . les patients ont ´et´e sauv´ees . 6
Table 3: English source sentences, respective translations in French and candidate translations generated from the
RCTM II and ranked out of 2000 samples according to their decreasing probability. Note that end of sentence dots (.)
are generated as part of the translation.
</tableCaption>
<table confidence="0.9996455">
WMT-NT 2009 2010 2011 2012
RCTM I + WP 19.7 21.1 22.5 21.5
RCTM II + WP 19.8 21.1 22.5 21.7
cdec (12 features) 19.9 21.2 22.6 21.8
</table>
<tableCaption confidence="0.995903">
Table 4: Bleu scores on the WMT-NT sets of each RCTM
</tableCaption>
<bodyText confidence="0.962765333333333">
linearly interpolated with a word penalty WP. The cdec
system includes WP as well as five translation models and
two language modelling features, among others.
large majority of the candidate translations are fully
well-formed French sentences. Further, subtle syn-
tactic features such as the singular or plural ending
of nouns and the present and past tense of verbs are
well correlated between the English source and the
French candidate targets. Finally, the meaning of
the English source is well transferred on the French
candidate targets; where a correlation is unlikely or
the target word is not in the French vocabulary, a se-
mantically related word or synonym is selected by
the model. All of these traits suggest that the RCTM
II is able to capture a significant amount of both
syntactic and semantic information from the English
source sentence and successfully transfer it onto the
French translation.
</bodyText>
<subsectionHeader confidence="0.985402">
5.4 Rescoring and BLEU Evaluation
</subsectionHeader>
<bodyText confidence="0.999982846153846">
The fourth experiment tests the ability of the RCTM
I and the RCTM II to choose the best translation
among a large number of candidate translations pro-
duced by another system. We use the cdec sys-
tem to generate a list of 1000 best candidate trans-
lations for each English sentence in the four WMT-
NT sets. We compare the rescoring performance of
the RCTM I and the RCTM II with that of the cdec
itself. cdec employs 12 engineered features includ-
ing, among others, 5 translation models, 2 language
model features and a word penalty feature (WP). For
the RCTMs we simply interpolate the log probabil-
ity assigned by the models to the candidate transla-
tions with the word penalty feature WP, tuned on the
validation data. The results of the experiment are
reported in Tab. 4.
While there is little variance in the resulting Bleu
scores, the performance of the RCTMs shows that
their probabilities correlate with translation qual-
ity. Combining a monolingual RLM feature with
the RCTMs does not improve the scores, while re-
ducing cdec to just one core translation probability
and language model features drops its score by two
to five tenths. These results indicate that the RCTMs
have been able to learn both translation and language
modelling distributions.
</bodyText>
<page confidence="0.988892">
1708
</page>
<sectionHeader confidence="0.999569" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977">
We have introduced Recurrent Continuous Transla-
tion Models that comprise a class of purely contin-
uous sentence-level translation models. We have
shown the translation capabilities of these models
and the low perplexities that they obtain with respect
to reference translations. We have shown the ability
of these models at capturing syntactic and semantic
information and at estimating during reranking the
quality of candidate translations.
The RCTMs offer great modelling flexibility due
to the sensitivity of the continuous representations to
conditioning information. The models also suggest
a wide range of potential advantages and extensions,
from being able to include discourse representations
beyond the single sentence and multilingual source
representations, to being able to model morpholog-
ically rich languages through character-level recur-
rences.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928518987342">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7–12. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of ibm model 2. In Proc. of NAACL.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional
Semantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics. Forthcoming.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Convolutional Neural Networks for Discourse Com-
positionality. In Proceedings of the Workshop on Con-
tinuous Vector Space Models and their Composition-
ality, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In HLT-NAACL, pages 39–48.
Tomas Mikolov and Geoffrey Zweig. 2012. Context de-
pendent recurrent neural network language model. In
SLT, pages 234–239.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cer-
nock´y, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Takao
Kobayashi, Keikichi Hirose, and Satoshi Nakamura,
editors, INTERSPEECH, pages 1045–1048. ISCA.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model. In
ICASSP, pages 5528–5531. IEEE.
Holger Schwenk, Daniel D´echelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In ACL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071–1080.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801–809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic Compositional-
ity Through Recursive Matrix-Vector Spaces. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Ilya Sutskever, James Martens, and Geoffrey E. Hinton.
2011. Generating text with recurrent neural networks.
In Lise Getoor and Tobias Scheffer, editors, ICML,
pages 1017–1024. Omnipress.
</reference>
<page confidence="0.996125">
1709
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.991375">
<title confidence="0.998558">Recurrent Continuous Translation Models</title>
<author confidence="0.997815">Nal Kalchbrenner Phil Blunsom</author>
<affiliation confidence="0.9998575">Department of Computer University of Oxford</affiliation>
<email confidence="0.997657">inal.kalchbrenner,phil.blunsoml@cs.ox.ac.uk</email>
<abstract confidence="0.999885391304348">We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translathat is than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a system when rescoring lists of translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2450" citStr="Bengio et al., 2003" startWordPosition="369" endWordPosition="372">eads to general sparsity issues. The estimation is sparse or skewed for the large number of rare or unseen phrase pairs, which grows exponentially in the length of the phrases, and the generalisation to other domains is often limited. Continuous representations have shown promise at tackling these issues. Continuous representations for words are able to capture their morphological, syntactic and semantic similarity (Collobert and Weston, 2008). They have been applied in continuous language models demonstrating the ability to overcome sparsity issues and to achieve state-of-theart performance (Bengio et al., 2003; Mikolov et al., 2010). Word representations have also shown a marked sensitivity to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a h</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML.</booktitle>
<contexts>
<context position="2278" citStr="Collobert and Weston, 2008" startWordPosition="342" endWordPosition="346"> linguistic or otherwise, they do not share statistical weight in the models’ estimation of their translation probabilities. Besides ignoring the similarity of phrase pairs, this leads to general sparsity issues. The estimation is sparse or skewed for the large number of rare or unseen phrase pairs, which grows exponentially in the length of the phrases, and the generalisation to other domains is often limited. Continuous representations have shown promise at tackling these issues. Continuous representations for words are able to capture their morphological, syntactic and semantic similarity (Collobert and Weston, 2008). They have been applied in continuous language models demonstrating the ability to overcome sparsity issues and to achieve state-of-theart performance (Bengio et al., 2003; Mikolov et al., 2010). Word representations have also shown a marked sensitivity to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="26033" citStr="Duchi et al., 2011" startWordPosition="4495" endWordPosition="4498">e training of the model proceeds by back-propagation through time. The cross-entropy error calculated at the output layer at each step is back-propagated through the recurrent structure for a number d of steps; for all models we let d = 6. The error accumulated at the hidden layers is then further back-propagated through the transformation S and the CSM/CGM to the input vectors v(ei) of the English input sentence e. All weights, including the English vectors, are randomly initialised and inferred during training. The objective is minimised using mini-batch adaptive gradient descent (Adagrad) (Duchi et al., 2011). The training of an RCTM takes about 15 hours on 3 multicore CPUs. While our experiments 1706 WMT-NT 2009 2010 2011 2012 KN-5 218 213 222 225 RLM 178 169 178 181 IBM 1 207 200 188 197 FA-IBM 2 153 146 135 144 RCTM I 143 134 140 142 RCTM II 86 77 76 77 Table 1: Perplexity results on the WMT-NT sets. are relatively small, we note that in principle our models should scale similarly to RLMs which have been applied to hundreds of millions of words. 5.2 Perplexity of gold translations Since the computation of the probability of a translation under one of the RCTMs is efficient, we can compute the p</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Adam Lopez</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Juri Ganitkevitch</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7536" citStr="Dyer et al., 2010" startWordPosition="1145" endWordPosition="1148">orse, suggesting that the model is highly sensitive to word position and order. The third experiment inspects the translations generated by the RCTM II. The generated translations demonstrate remarkable morphological, syntactic and semantic agreement with the source sentence. Finally, we test the RCTMs on the task of rescoring n-best lists of translations. The performance of the RCTM probabilities joined with a single word penalty feature matches the performance of the state-of-the-art translation system cdec that makes use of twelve features including five alignment-based translation models (Dyer et al., 2010). We proceed as follows. We begin in Sect. 2 by describing the general modelling framework underlying the RCTMs. In Sect. 3 we describe the RCTM I and in Sect. 4 the RCTM II. Section 5 is dedicated to the four experiments and we conclude in Sect. 6.1 2 Framework We begin by describing the modelling framework underlying RCTMs. An RCTM estimates the probability P(f|e) of a target sentence f = f1, ..., f,,t being a translation of a source sentence e = e1, ..., ek. Let 1Code and models available at nal.co 1701 us denote by fi:j the substring of words fi,..., fj. Using the following identity, P(f|e</context>
</contexts>
<marker>Dyer, Weese, Setiawan, Lopez, Ture, Eidelman, Ganitkevitch, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2. In</title>
<date>2013</date>
<booktitle>Proc. of NAACL.</booktitle>
<contexts>
<context position="6580" citStr="Dyer et al., 2013" startWordPosition="998" endWordPosition="1001">ucts that is linear in the length of the source and the target sentence. Further, translations can be generated directly from the probability distribution of the RCTM without any external resources. We evaluate the performance of the models in four experiments. Since the translation probabilities of the RCTMs are tractable, we can measure the perplexity of the models with respect to the reference translations. The perplexity of the models is significantly lower than that of IBM Model 1 and is &gt; 43% lower than the perplexity of a state-of-the-art variant of the IBM Model 2 (Brown et al., 1993; Dyer et al., 2013). The second and third experiments aim to show the sensitivity of the output of the RCTM II to the linguistic information in the source sentence. The second experiment shows that under a random permutation of the words in the source sentences, the perplexity of the model with respect to the reference translations becomes significantly worse, suggesting that the model is highly sensitive to word position and order. The third experiment inspects the translations generated by the RCTM II. The generated translations demonstrate remarkable morphological, syntactic and semantic agreement with the so</context>
<context position="27023" citStr="Dyer et al., 2013" startWordPosition="4681" endWordPosition="4684"> similarly to RLMs which have been applied to hundreds of millions of words. 5.2 Perplexity of gold translations Since the computation of the probability of a translation under one of the RCTMs is efficient, we can compute the perplexities of the RCTMs with respect to the reference translations in the test sets. The perplexity measure is an indication of the quality that a model assigns to a translation. We compare the perplexities of the RCTMs with the perplexity of the IBM Model 1 (Brown et al., 1993) and of the FastAligner (FA-IBM 2) model that is a state-of-the-art variant of IBM Model 2 (Dyer et al., 2013). We add as baselines the unconditional target RLM and a 5- gram target language model with modified KneserNay smoothing (KN-5). The results are reported in Tab. 1. The RCTM II obtains a perplexity that is &gt; 43% lower than that of the alignment based models and that is 40% lower than the perplexity of the RCTM I. The low perplexity of the RCTMs suggests that continuous representations and the transformations between them make up well for the lack of explicit alignments. Further, the difference in perplexity between the RCTMs themselves demonstrates the importance of the conditioning architectu</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proc. of NAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Edward Grefenstette</author>
</authors>
<location>Mehrnoosh Sadrzadeh, Stephen</location>
<marker>Grefenstette, </marker>
<rawString>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<location>CoRR, abs/1101.0309.</location>
<marker>Clark, Pulman, 2011</marker>
<rawString>Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. CoRR, abs/1101.0309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The Role of Syntax in Vector Space Models of Compositional Semantics.</title>
<date>2013</date>
<journal>Association for Computational Linguistics. Forthcoming.</journal>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3197" citStr="Hermann and Blunsom, 2013" startWordPosition="475" endWordPosition="478">d Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically m</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, August. Association for Computational Linguistics. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent Convolutional Neural Networks for Discourse Compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3230" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="479" endWordPosition="482">representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Alth</context>
<context position="4803" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="714" endWordPosition="717">ithout loss of generality a sentence from the source language to a probability distribution over the sentences in the target language. We define two specific RCTM architectures. Both models adopt a recurrent language model for the generation of the target translation (Mikolov et al., 2010). In contrast to other n-gram approaches, the recurrent language model makes no Markov assumptions about the dependencies of the words in the target sentence. The two RCTMs differ in the way they condition the target language model on the source sentence. The first RCTM uses the convolutional sentence model (Kalchbrenner and Blunsom, 2013) to transform the source word representations into a representation for the source sentence. The source sentence representation in turn constraints the generation of each target word. The second RCTM introduces an intermediate representation. It uses a truncated variant of the convolutional sentence model to first transform the source word representations into representations for the target words; the latter then constrain the generation of the target sentence. In both cases, the convolutional layers are used to generate combined representations for the phrases in a sentence from the represent</context>
<context position="17639" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="3001" endWordPosition="3005">et vocabulary V F, two sequences of weight matrices (Ki)2≤i≤r and (Li)2≤i≤r that are part of the constituent CSM, transformations I E Rq×|V F|, R E Rq×q and O E R|V F|×q that are part of the constituent RLM and a sentence transformation S E Rq×q. We write e = csm(e) for the output of the CSM with e as the input sentence. The computation of the RCTM I is a simple modification to the computation of the RLM described in Eq. 3. It proceeds recursively as follows: s = S · csm(e) h1 = Q(I · v(f1) + s) hi+1 = Q(R · hi + I · v(fi+1) + s) oi+1 = O · hi 2For a formal treatment of the construction, see (Kalchbrenner and Blunsom, 2013). and the conditional distributions P(fi+1|f1:i, e) are obtained from oi as in Eq. 4. Q is a nonlinear function and bias values are included throughout the computation. Fig. 3 illustrates an RCTM I. Two aspects of the RCTM I are to be remarked. First, the length of the target sentence is predicted by the target RLM itself that by its architecture has a bias towards shorter sentences. Secondly, the representation of the source sentence e constraints uniformly all the target words, contrary to the fact that the target words depend more strongly on certain parts of the source sentence and less on</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Convolutional Neural Networks for Discourse Compositionality. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>39--48</pages>
<contexts>
<context position="3592" citStr="Le et al., 2012" startWordPosition="529" endWordPosition="532">gnificantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Although wide-reaching in their scope, these models are limited to fixed-size source and target phrases and simplify the dependencies between the target words taking into account restricted target language modelling information. We describe a class of continuous translation models called Recurrent Continuous Translation Models (RCTM) that map without loss of gener</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In HLT-NAACL, pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In SLT,</booktitle>
<pages>234--239</pages>
<contexts>
<context position="2586" citStr="Mikolov and Zweig, 2012" startWordPosition="388" endWordPosition="391">s exponentially in the length of the phrases, and the generalisation to other domains is often limited. Continuous representations have shown promise at tackling these issues. Continuous representations for words are able to capture their morphological, syntactic and semantic similarity (Collobert and Weston, 2008). They have been applied in continuous language models demonstrating the ability to overcome sparsity issues and to achieve state-of-theart performance (Bengio et al., 2003; Mikolov et al., 2010). Word representations have also shown a marked sensitivity to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Bl</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In SLT, pages 234–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<pages>1045--1048</pages>
<editor>In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, INTERSPEECH,</editor>
<publisher>ISCA.</publisher>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, INTERSPEECH, pages 1045–1048. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In ICASSP,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<marker>Mikolov, Kombrink, Burget, Cernock´y, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In ICASSP, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel D´echelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous space language models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<marker>Schwenk, D´echelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel D´echelotte, and Jean-Luc Gauvain. 2006. Continuous space language models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1071--1080</pages>
<contexts>
<context position="3574" citStr="Schwenk, 2012" startWordPosition="527" endWordPosition="528">ogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Although wide-reaching in their scope, these models are limited to fixed-size source and target phrases and simplify the dependencies between the target words taking into account restricted target language modelling information. We describe a class of continuous translation models called Recurrent Continuous Translation Models (RCTM) that map wit</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING (Posters), pages 1071–1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24,</booktitle>
<pages>801--809</pages>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="3149" citStr="Socher et al., 2011" startWordPosition="467" endWordPosition="470">ty to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant </context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3170" citStr="Socher et al., 2012" startWordPosition="471" endWordPosition="474">formation (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvemen</context>
<context position="12894" citStr="Socher et al., 2012" startWordPosition="2093" endWordPosition="2096">in the conditioning architecture. The CSM creates a representation for a sentence that is progressively built up from representations of the ngrams in the sentence. The CSM embodies a hierarchical structure. Although it does not make use of an explicit parse tree, the operations that generate the representations act locally on small n-grams in the lower layers of the model and act increasingly more globally on the whole sentence in the upper layers of the model. The lack of the need for a parse tree yields two central advantages over sentence models that require it (Grefenstette et al., 2011; Socher et al., 2012). First, it makes the model robustly applicable to a large number of languages for which accurate parsers are not available. Secondly, the translation probability distribution over the target sentences does not depend on the chosen parse tree. The RCTM I conditions the probability of each target word fi on the continuous representation of the source sentence e generated through the CSM. This is accomplished by adding the sentence representation to each hidden layer hi in the target recurrent language model. We next describe the procedure in more detail, starting with the CSM itself. 3.1 Convol</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Lise Getoor and</booktitle>
<pages>1017--1024</pages>
<editor>Tobias Scheffer, editors, ICML,</editor>
<publisher>Omnipress.</publisher>
<contexts>
<context position="2763" citStr="Sutskever et al., 2011" startWordPosition="410" endWordPosition="413">ntinuous representations for words are able to capture their morphological, syntactic and semantic similarity (Collobert and Weston, 2008). They have been applied in continuous language models demonstrating the ability to overcome sparsity issues and to achieve state-of-theart performance (Bengio et al., 2003; Mikolov et al., 2010). Word representations have also shown a marked sensitivity to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Confere</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E. Hinton. 2011. Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer, editors, ICML, pages 1017–1024. Omnipress.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
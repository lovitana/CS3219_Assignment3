<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.974855">
This Text Has the Scent of Starbucks:
A Laplacian Structured Sparsity Model for
Computational Branding Analytics
</title>
<author confidence="0.999643">
William Yang Wang Edward Lin and John Kominek
</author>
<affiliation confidence="0.9943655">
School of Computer Science Voci Technologies, Inc.
Carnegie Mellon University Pittsburgh, PA 15217
</affiliation>
<email confidence="0.998393">
ww@cmu.edu {ed.lin,john.kominek}@vocitec.com
</email>
<sectionHeader confidence="0.996656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9979056">
We propose a Laplacian structured sparsity
model to study computational branding ana-
lytics. To do this, we collected customer re-
views from Starbucks, Dunkin’ Donuts, and
other coffee shops across 38 major cities
in the Midwest and Northeastern regions of
USA. We study the brand related language
use through these reviews, with focuses on
the brand satisfaction and gender factors. In
particular, we perform three tasks: auto-
matic brand identification from raw text, joint
brand-satisfaction prediction, and joint brand-
gender-satisfaction prediction. This work ex-
tends previous studies in text classification by
incorporating the dependency and interaction
among local features in the form of structured
sparsity in a log-linear model. Our quantita-
tive evaluation shows that our approach which
combines the advantages of graphical model-
ing and sparsity modeling techniques signifi-
cantly outperforms various standard and state-
of-the-art text classification algorithms. In ad-
dition, qualitative analysis of our model re-
veals important features of the language uses
associated with the specific brands.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984975609756">
In marketing science, branding is a modern market-
ing strategy of creating a unique image for a prod-
uct in the customers’ mind. Establishing the brand
in the broad social context is just as important as
building a good product (Makens, 1965; Lederer
and Hill, 2001; Kim et al., 2013). In fact, blind
taste test experiments have frequently shown how
branding directly leads to the success of products
and companies. Most notably is a continued study
sponsored by Pepsi, known as the Pepsi Challenge1,
where Pepsi demonstrates how even though people
preferred the taste of Pepsi, Coca-Cola’s branding
has made it more popular. Even now, Microsoft
uses similar blind taste tests2 to compare search en-
gines, Bing and Google, showing that although par-
ticipants prefer Bing’s results, Google’s brand might
have strengthened over the years. These studies all
suggest that brand and its associations play impor-
tant roles in the customers’ perceptions and deci-
sions.
To accommodate the market change, companies
frequently adjust branding strategies by analyzing
how their customers receive and respond to brand-
ing messages. So far, such analysis is often done
by using surveys and focus groups (Moon and
Quelch, 2006), which is expensive and not time-
efficient. Recently, with the advance of machine
learning techniques, researchers from the chemistry
and vision communities started to pay attention to
the problem of automatic brand identification from
smell (Luo et al., 2004) and images (Pelisson et al.,
2003). In contrast, even though textual data that
contains hidden branding information is abundantly
available in many forms over the Web, automatic
discovery and computational analysis on such data
are not well studied in the past.
Computational branding analytics (CBA) seeks to
extract information, trends, and demographics about
a brand on the basis of free-form text, e.g. from
blogs, Twitter comments, reviews, or forum posts.
As described in Section 3, in this study we use a sub-
</bodyText>
<footnote confidence="0.9999645">
1http://en.wikipedia.org/wiki/Pepsi Challenge
2http://www.bingiton.com/
</footnote>
<page confidence="0.858214">
1325
</page>
<note confidence="0.7417445">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1325–1336,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9979255">
set of online Yelp reviews that discuss coffee shops.
The main reason is that this source has the advan-
tage of providing ground truth of multi-labeled data:
each review has meta-information defining a 5-star
rating, the object of the review, and the reviewer’s
name (from which we infer gender). For the pur-
pose of this paper we decompose CBA into three
sub-problems.
</bodyText>
<listItem confidence="0.984045666666667">
• How well can the brand being discussed be
identified by the raw text?
• How well can the joint value of brand and rat-
ing be predicted?
• How well can the joint value of brand, rating,
and gender be predicted?
</listItem>
<bodyText confidence="0.999984761904762">
There are two reasons why one may want to con-
struct text-based classifiers of brand, rating, and gen-
der, when such information is present in the review
header. The first is that trained classifiers can then be
applied to other data sources, such as blogs, where
what is available is only the review itself. The sec-
ond is that by “opening the hood” to the classifier
one can examine which words exhibit high affilia-
tion with the predicted variables. This can be done,
for example, to contrast the preferences of males and
females with respect to evaluating the qualities of a
coffee shop. Examples of such insights are provided
in Section 5.5.
In this paper, we propose a Laplacian structured
sparsity model for computational branding analyt-
ics. Our main contributions are two-fold: first, in
the novel task of automatic brand identification from
text, we show that by incorporating the dependency
structure and graphical interactions among local
features, our model significantly outperforms vari-
ous text classification algorithms such as the stan-
dard logistic regression, principle component anal-
ysis (PCA), linear kernel support vector machine
(SVM), sparse, non-sparse, and mixed-penalty log-
linear models. These improvements could also be
seen from a joint brand-satisfaction prediction task
and a gender-specific joint brand-satisfaction predic-
tion task. In addition, our Laplacian augmented Li-
ball projection experiment shows that the advantage
of Laplacian structured sparsity is robust across dif-
ferent parameter settings in a Li-constrained prob-
lem. Secondly, the qualitative analysis of our ma-
chine learning model shows the interesting features
and language use that relate to brand and its associ-
ated pragmatics.
In the next section, we outline related work in
CBA, sparsity, and spectral graph learning. In Sec-
tion 3, we describe the corpus in this study. The
Laplacian structured sparsity model is introduced in
Section 4. The experimental setup and results are
presented in Section 5. A short discussion is fol-
lowed in Section 6 and we conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999935552631579">
Early work on statistical brand analysis in the
marketing community dates back to the work of
Kuehn (1962), where he first hypothesizes that
brand choice could be described as a learning pro-
cess. Guadagni and Little (1983) further empiri-
cally tested the hypothesis by building a calibrated
multinomial logistic regression model to predict the
purchase of ground coffee, using the data from the
optical scanning of product code in supermarkets.
Outside the marketing community, statistical brand
analysis is rarely seen. More recently, a study (Luo
et al., 2004) applies neural networks to identify
cigarette brands, with the hope of detecting illegal
cigarettes from smell features. In image process-
ing, researchers have studied the problem of brand
identification from image using histogram compar-
ison (Pelisson et al., 2003). However, to the best
of our knowledge, even though textual data is vastly
available, the problems of automatic brand identi-
fication from raw text and computational branding
analytics, are new.
Although the domain of our data is on branding,
our work also aligns with previous work in text and
language classification. Over the years, logistic re-
gression and linear kernel SVM have shown to be
very successful in various regression and classifi-
cation tasks in NLP (Chahuneau et al., 2012; Bi-
adsy et al., 2011). Recently, sparse discriminative
methods that model the sparse nature of text be-
come attractive, because unlike dense models, they
are less likely to overfit to the training data, easier
to interpret, and often lead to state-of-the-art results.
For example, Eisenstein et al. (2011b) use the Li
sparsity model to discover sociolinguistic patterns.
Wang et al. (2012a) compare lasso, ridge, and elas-
tic net models to predict impoliteness behaviors in
teenager conversations. Martins et al. (2011) inves-
tigate the tree-structured overlapping group lasso for
</bodyText>
<page confidence="0.986908">
1326
</page>
<bodyText confidence="0.9999356">
structured prediction problems. Chen et al. (2013)
study the use of element-wise, group-wise, and hi-
erarchical sparsity models for dialogue act classif-
cation. Sparse inducing priors are also investigated
and shown to be effective in generative models for
topic modeling (Eisenstein et al., 2011a; Wang et
al., 2012b; Paul and Dredze, 2012).
Besides lacking sparsity, since the traditional dis-
criminative methods in NLP often use interdepen-
dent features such as n-grams tokens, and part-of-
speech tags, they also suffer from the problem of not
explicitly modeling the complex dependency struc-
ture and interaction of local features from a global
perspective. To solve this problem, graph meth-
ods seem to be a good solution, because they are
simple, generalizable, and are often used to model
such complex dependency structures (Cohen, 2012).
However, combining the sparse modeling and spec-
tral graphical modeling approaches in a principled
way is challenging. Belkin et al. (2006) and Wein-
berger et al. (2007) are among the first to investi-
gate graph Laplacians as a manifold regularization
method for statistical learning. Recently, Gao et
al. (2012) propose a histogram intersection based
kNN method to construct a Laplacian matrix for a
least-square sparse coding problem in image pro-
cessing. Unfortunately, this method might be too
specific to the SIFT-based image coding tasks, thus
might not be applicable to the text classification
problem that utilizes n-gram lexical features.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.999617333333333">
We collected Yelp reviews from 1,860 Starbucks,
Dunkin’ Donuts3, and other coffee shops all over
the Midwest and Northeast regions in the period of
2009. A detail statistics of our data can be found
in Table 1. The Midwest region includes 12 states4
and 19 major cities, and the Northeast region in-
cludes 9 states5 and 19 major cities. For each region,
we divide the coffee shops into 60% training, 20%
development, and 20% test, and there are no over-
laps of coffee shops among these subsets. There are
three values for the brand label: Starbucks, Dunkin’
Donuts, and all other coffee shop brands. The ma-
</bodyText>
<footnote confidence="0.9858706">
3We chose these two brands because they are reported as
the leading coffee shops by WSJ (Ovide, 2011) and Forbes (Di-
Carlo, 2004).
4IL, WI, SD, ND, MN, MO, OH, NE, KS, IA, IN, and MI.
5CT, ME, MA, NH, RI, VT, NJ, NY and PA.
</footnote>
<table confidence="0.879256">
Coffee Shops Reviews
Train Dev. Test Train Dev. Test
1 451 150 150 3,513 1,087 1,424
2 665 222 222 6,982 2,530 2,358
T. 1,116 372 372 10,495 3,617 3,782
</table>
<tableCaption confidence="0.918561">
Table 1: Dataset statistics. 1: midwest region. 2: north-
east region. T.: total.
</tableCaption>
<bodyText confidence="0.994545733333333">
jority class is “all other coffee shop brands”, and
the majority baseline is shown in Table 2. In the
task of joint brand-satisfaction prediction, we utilize
the review scores to approximate user satisfaction:
scores 1-2 as the unsatisfactory label, 3 as moder-
ate, and 4-5 as satisfactory. Since the Yelp reviews
do not reveal the reviewer’s gender, we use a similar
method that U.S. Census Bureau used (OConnell
and Gooding, 2006): we first automatically match
the first name of the reviewer with the prior name-
gender distributions in the census records, then man-
ually examine the no-match cases and a subsample
of the matched cases. For those who we cannot
determine the gender, the review will be dropped
from the gender-specific brand-satisfaction predic-
tion task. After filtering, there are 8,528 documents
for training, 2,928 for development, and, 3,046 for
testing. Since the focus of this paper is not on fea-
ture engineering, we use unigram features to repre-
sent each review. Below is an example of positive
review from a male Starbucks customer from Mid-
west.
My favorite place for my iced vanilla lattes.
They have screwed up my order before: instead
of a grande, I got a venti. Not a fan of their
pastries though. I got a donut once, and ended
up feeding it to a pigeon in city garden. Friendly
and fast service. Not open Sundays.
The coffee shop dataset is freely available6 for re-
search purposes.
</bodyText>
<sectionHeader confidence="0.99714" genericHeader="method">
4 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.999941">
4.1 Problem Formulation and Predictive Tasks
</subsectionHeader>
<bodyText confidence="0.999845">
The automatic brand identification problem could
be considered as a traditional multiclass classifica-
</bodyText>
<footnote confidence="0.9672">
6http://www.cs.cmu.edu/˜yww/data/emnlp2013.zip
</footnote>
<page confidence="0.992308">
1327
</page>
<bodyText confidence="0.9993455">
tion problem where the estimated label Y could
be drawn from Mult(F), where F is the parame-
ter for the multinomial distribution. To solve this,
a simple but accurate solution is to decompose the
multiclass problem into multiple binary classifica-
tion problems (Rifkin and Klautau, 2004) by train-
ing k one-vs-all binary classifiers, and then use the
argmax criteria to select the best hypothesis from the
k posteriors. As for a binary classifier, we need to
infer the posterior from a Bernoulli distribution that
is parametrized by �θy. Similarly, we can derive k
binary classifiers:
</bodyText>
<equation confidence="0.9473705">
�θ(1)
y ,
</equation>
<bodyText confidence="0.999816">
So, instead of drawing Y from a multinomial distri-
bution Mult(F), we can draw the final label Y� that
has the largest posterior across all k classifiers:
</bodyText>
<equation confidence="0.990228">
Y = argmax
Y,i=1,2,...,k
</equation>
<bodyText confidence="0.976498533333333">
where ~Xt is the testing vector, and Pr(Y  |�θ(i)
y,~Xt) is
the posterior probability given the learned classifiers
and the testing vector.
In this paper, we investigate three multiclass clas-
sification tasks: first, we perform a 3-way classi-
fication task for automatic brand identification. In
the task of brand-satisfaction prediction, we model
the brand and the satisfaction label at the same
time (Chahuneau et al., 2012): we perform the task
of jointly predicting aggregate brand-satisfaction
score for a review using 9-way classification. Sim-
ilarly, we perform 18-way classification for the
gender-specific joint brand-satisfaction prediction
task.
</bodyText>
<subsectionHeader confidence="0.987263">
4.2 The Log-Linear Framework and Its
Regularized Variants
</subsectionHeader>
<bodyText confidence="0.99059">
If we consider the standard logistic regression model
as the binary classifier in this log-linear framework7,
then each classifier can be written as:
</bodyText>
<equation confidence="0.9968075">
�θy = 1 + exp � W~ � ~Xj�
exp ( W~T ~Xj ) (3)
</equation>
<bodyText confidence="0.803859">
here, ~Xj is the j-th observed feature vector, label
y ∈ {0, 1}, and W~ is a vector of the coefficients. To
</bodyText>
<footnote confidence="0.731917">
7We thank Jacob Eisenstein for the initial derivation of the
logistic regression model.
</footnote>
<bodyText confidence="0.99977">
estimate the model parameters in equation (3), we
only need to set the weights W~. We can obtain the
following log likelihood, and its gradient function
by taking the first-order partial derivative of W~:
</bodyText>
<equation confidence="0.999282666666667">
` = � yj log Oyj + (1 − yj) log(1 − θyj ) (4)
j
�=
j
� �θyj − (�θyj)2�
= ~X, (6)
</equation>
<bodyText confidence="0.993799272727273">
since the log likelihood objective function (4) is con-
cave, using standard gradient ascent with maximum
likelihood estimation can solve the problem. How-
ever, this model does not penalize the noisy features
and unreliable features that might overfit to the train-
ing data. To address this issue, we introduce the L1
norm from lasso technique (Tibshirani, 1996) to reg-
ularize the above likelihood function. Thus, instead
of maximizing the likelihood, we can minimize the
loss function of the negative log-likelihood with a
linear penalty:
</bodyText>
<equation confidence="0.9942195">
� �
min − ` + λ1 ||W~  ||(7)
</equation>
<bodyText confidence="0.999401333333333">
where λ1 is the regularization coefficient. The bene-
fit of L1 penalty in a discriminative model is sim-
ilar to the double exponential distribution of the
sparse priors in generative models (Eisenstein et al.,
2011a): they both push the weights of many noisy
features into zeros, revealing only the important fea-
tures. However, since the L1 penalty can intro-
duce discontinuities to the original convex function,
we can also consider an alternative non-sparse ridge
estimator (Le Cessie and Van Houwelingen, 1992)
with log loss and L2 norm, and has the convex prop-
erty:
</bodyText>
<equation confidence="0.998128">
� − ` + λ2 ||W~ ||2�
min (8)
</equation>
<bodyText confidence="0.997823666666667">
Another option that balances the sparsity and
smoothness would be the elastic net model (Zou and
Hastie, 2005) that uses the composite penalty:
</bodyText>
<equation confidence="0.9967115">
�− ` + λ1 ||W~  ||+ λ2 ||W~ ||2�
min (9)
</equation>
<subsectionHeader confidence="0.99395">
4.3 The Laplacian Structured Sparsity Model
</subsectionHeader>
<bodyText confidence="0.999692">
So far, none of the above element-wise penalty mod-
els in the previous subsection takes into account the
</bodyText>
<figure confidence="0.952676076923077">
�θ(2) �θ(k)
y ,..., y . (1)
Pr(Y |�θ(i) ~Xt) (2)
y ,
∂`
∂ W~
∂k
∂ W~
� �
∂�θ
j
j
−
j
∂
θ
j
−θ
j
y
1
(5)
W
y
1
y
</figure>
<page confidence="0.971976">
1328
</page>
<bodyText confidence="0.999830875">
dependency structure of the local features. Inspired
by Gao et al.(2012), we group the local features that
have similar distributions together. The intuition is
that, for features that have very similar empirical dis-
tributions in the training set, their weights should not
be drastically different after the learning process in
the same task. In our new objective function, it is
desirable to introduce a new component that struc-
turally penalize these cases where features that are
very similar to each other, but have learned com-
pletely different weights, probably due to the noise
or the data sparsity issue in the training data.
The Objective Function: To do this, we first define
an inter-feature affinity matrix A, where A(p,q) mea-
sures the similarity between a pair of features p and
q. In the spectral graph theory, this affinity matrix
can be viewed as a weighted undirected graph G =
(V, E), where each node Vp denotes a feature p, and
each edge E(p,q) indicates the closeness among the
features p and q. We also introduce a weighted di-
agonal degree matrix D, of which each element in
the diagonal D(p,p) is the sum of all weighted con-
nections of node Vp: D(p,p) = PQq�1 A(p,q). We
propose the following objective function:
</bodyText>
<equation confidence="0.93723675">
�
min − � + �1 ||W�  ||+ �2 ||W� ||2 (10)
�
Wp − Wq||2A(p,q) (11)
</equation>
<bodyText confidence="0.999972333333333">
We then denote a graph Laplacian matrix L = D −
A (Belkin and Niyogi, 2001), and rewrite the objec-
tive function as:
</bodyText>
<equation confidence="0.9931205">
mint − e+ A1  ||VV_ II+ A2|#V||2 (12)
\ �+ α( W� �L W� ) (13)
</equation>
<bodyText confidence="0.999935916666667">
where α is the regularization parameter for the
Laplacian structured sparsity term. Intuitively, the
objective function can be interpreted as the sum of
a negative log loss function, the sparsity-inducing
penalty, the quadratic penalty, and the Laplacian
structured penalty. Or, another view of this new
model could be seen as a Laplacian augmented elas-
tic net model where structured sparsity and feature
interaction are considered.
The Laplacian Matrix: In this model, a key aspect
is to derive the Laplacian matrix L. We propose the
following three steps to learn the Laplacian matrix:
</bodyText>
<figureCaption confidence="0.826020666666667">
Figure 1: An example of the graph G, the corresponding
affinity matrix A, and the corresponding Laplacian matrix
L.
</figureCaption>
<listItem confidence="0.787242">
1. Construct the distance matrix Dist. To con-
</listItem>
<bodyText confidence="0.938881666666667">
struct the distance matrix between each fea-
ture, we first transpose the instance-feature ma-
trix, I = P ��j, and assume that each feature
j
(e.g. unigram in our task) is a random variable
that has a multinomial distribution over the in-
stances in the training set. Then, we compare
each pair of features, and calculate the inter-
feature distance matrix Dist with Euclidean
distance as a measure, and use the k-nearest
neighbors (kNN) method (Beyer et al., 1999)
to select the top neighbors of each feature.
</bodyText>
<listItem confidence="0.994634">
2. Derive the affinity matrix A. To assign the
weight on the edge E(p,q) for each connected
nodes (the kNN of V in Dist), we use the
cosine similarity cosine(Vp, Vq) metric (Wang
and Hirschberg, 2011).
3. Generate the degree matrix D and Lapla-
</listItem>
<bodyText confidence="0.894234375">
cian matrix L. As discussed earlier, we sum
up the symmetric affinity matrix by row, and
obtain a diagonal degree matrix D, and we fur-
ther define a Laplacian matrix L = D − A.
To calculate the above matrices in an efficient man-
ner, we partition the covariate into blocks, and pro-
cess each block in parallel (Chen et al., 2011). An
intuitive example of the graph G, its associated affin-
ity matrix A, and Laplacian matrix L, is shown in
Figure 1.
Parameter Estimation: Regarding the optimiza-
tion of objective function in (12-13), a notable prob-
lem is that the sparsity inducing L1 term is non-
differentiable, whereas this is not the case for the L2
norm and the Laplacian structured sparsity term. If
we first take the derivative of the latter two terms,
</bodyText>
<equation confidence="0.8961925">
X+ α ||
(p,q)
</equation>
<page confidence="0.909258">
1329
</page>
<bodyText confidence="0.9972675">
and we can derive the following gradient compo-
nents:
</bodyText>
<equation confidence="0.9391576">
= 2λ2 W~ + α( W~TLT + W~TL) (15)
= 2λ2 W~ + α(LT + L) W~ (16)
since our Laplacian matrix is symmetric, we can
rewrite (16) as
2(λ2 W~ + αL W~) (17)
</equation>
<bodyText confidence="0.99816935">
Then, we combine the gradient of the log loss func-
tion in (5) with (17), and apply a bound-constrained
re-formulation (Schmidt et al., 2007) and the lim-
ited memory BFGS (L-BFGS) method (Liu and No-
cedal, 1989) to solve the L1 regularized problem.
The L-BFGS method has relatively low space com-
plexity, and does not require the calculation of full
Hessian matrix, thus it is often used for L1 optimiza-
tion problems.
Augmented Laplacian for an L1-Constrained
Problem: Instead of formulating the L1-regularized
problem by adding the L1 norm, an alternative so-
lution is to formulate a L1-constrained problem by
fixing the sum of all weights τ in the weight vector
W~. The reason is because adding the L1 norm will
make the objective function not continuously differ-
entiable, where as the L1 constraint could be just a
simple linear constraint (Lee et al., 2006). Thus, the
alternative L1-constrained problem could be defined
as:
</bodyText>
<equation confidence="0.851044">
Xmin(−`), s.t. ~Wp ≤ τ (18)
p
</equation>
<bodyText confidence="0.9999545">
To test the robustness of Laplacian structured spar-
sity term in the setup of a L1-constrained problem,
we can incorporate the Laplacian penalty term into
the above formula, and derive:
</bodyText>
<equation confidence="0.984353666666667">
min − ` + α( W~ TL W~ ) , s.t.
� � X ~Wp ≤ τ (19)
p
</equation>
<bodyText confidence="0.9855035">
Note that the Laplacian matrix is positive-
semidefinite,
</bodyText>
<equation confidence="0.992862333333333">
W~TL W~ = W~T X L(p,q) W~ (20)
(p,q)
X= W~TL(p,q) W~ (21)
(p,q)
X=  ||~Wp − ~Wq||2A(p,q) (22)
(p,q)
</equation>
<bodyText confidence="0.99997675">
because this graph Laplacian penalty can be viewed
as a quadratic term, and the objective function
in equation 19 is now convex differentiable and
will produce sparse estimates, so that we are able
to use a limited-memory projected quasi-Newton
method (Schmidt et al., 2009) to solve the dual form
of this problem. The Lagrangian dual form of the
problem in equation 19 can be written as:
</bodyText>
<equation confidence="0.999021333333333">
L( W~, ξ) = −` + α( W~ TL W~) (23)
+ β X
p !~Wp − τ − ξ W~ (24)
</equation>
<bodyText confidence="0.9997416">
where β ∈ R is a Lagrange multiplier, and ξ ∈ Rp�
is a p-dimensional vector of non-negative Lagrange
multipliers. And then we can take first-order partial
derivative with regard to W~, and set it to zero to de-
rive the optimality:
</bodyText>
<equation confidence="0.983765">
!
� yj
�θyj − (�θyj)2� X~ − 1 − yj �θyj 1 −�θyj
(25)
+ 2αL W~ + β − ξ = 0 (26)
</equation>
<bodyText confidence="0.998826333333333">
To speed up the training, we use the linear-time L1-
ball projection method from Duchi et al. (2008) in
our implementation.
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999975263157895">
We first compare our model to various baselines in
the 3-way automatic brand identification task. Be-
sides the logistic regression, lasso, ridge and elas-
tic net model that we introduced in Section 4.2, we
also compare with a PCA-based logistic regression
model where the dimensions of the feature space is
reduced in half before the classification. A state-of-
the-art linear kernel SVM model (Chang and Lin,
2011) is also taken into the comparison. In the
second part, we perform 9-way joint classification
of the brand-satisfaction labels. Similarly, we also
perform a 18-way joint classification of the brand-
gender-satisfaction labels. To test the robustness
of our model, we vary the levels of sparsity of our
Laplacian augmented method in a L1-constrained
problem. Finally, we analyze the identified features
for CBA. Throughout this section, we use classifi-
cation accuracy to report the results. We tune the
regularization parameters of log-linear models and
</bodyText>
<equation confidence="0.998553857142857">
∂(λ2 ||W~ ||2+ α( W~TL W~ ))
(14)
∂ W~
X= −
j
∂L
∂ W~
</equation>
<page confidence="0.971339">
1330
</page>
<table confidence="0.999085888888889">
Method Dev. Test
Majority class 75.67 78.08
Logistic regression 91.98 91.06
Linear SVM 92.45 91.75
PCA 91.67 91.20
Lasso 92.81 91.96
Ridge 92.56 91.67
Elastic net 92.81 91.83
Laplacian structured sparsity 93.17* 92.44*
</table>
<tableCaption confidence="0.991738">
Table 2: The automatic brand identification (3-way) per-
formances. The best result is highlighted in bold. * indi-
cates p &lt; .001 comparing to the second best result.
</tableCaption>
<table confidence="0.999771555555555">
Method Dev. Test
Majority class 55.43 55.18
Logistic regression 65.80 65.80
Linear SVM 67.67 65.44
PCA 63.92 62.53
Lasso 68.37 66.84
Ridge 67.79 65.55
Elastic net 68.79 66.82
Laplacian structured sparsity 69.56* 67.32*
</table>
<tableCaption confidence="0.990544">
Table 3: The joint brand-satisfaction prediction (9-way)
performances. The best result is highlighted in bold. *
indicates p &lt; .001 comparing to the second best result.
</tableCaption>
<bodyText confidence="0.999939833333333">
the cost parameter of the SVM on the development
set, and report results on both the development set
and the held-out test set. The parameter for kNN
was set to 5 according to previous literature (Gao et
al., 2012). A paired two-tailed t-test is used to test
the statistical differences among various models.
</bodyText>
<subsectionHeader confidence="0.996832">
5.1 Automatic Brand Identification from Text
</subsectionHeader>
<bodyText confidence="0.999987533333333">
Given any piece of raw text from the Web (e.g.
blogs, tweets, news, or forum posts), the first task
for CBA is to identify which brand this text is re-
lated to. Our customer review data set is useful for
this task, because the ground truth of the brand label
is attached to each review. Table 2 shows the re-
sult of our model in this automatic brand identifica-
tion task. In this 3-way classification task, the over-
all results indicate that it is relatively easy to iden-
tify the related brand from customer reviews. When
evaluating our Laplacian structured sparsity model,
our proposed model obtains the best performances
of 93.17% and 92.44%, which are statistically bet-
ter than the second best results (p &lt; .001) in both
datasets.
</bodyText>
<subsectionHeader confidence="0.999692">
5.2 Joint Brand-Satisfaction Prediction
</subsectionHeader>
<bodyText confidence="0.999979111111111">
In our training data set, we observe a subtle correla-
tion between the brand and satisfaction labels (r =
0.09, p &lt; .001), which suggests us that it might be
interesting to perform a joint prediction task for the
brand-satisfaction labels. This task is also attractive
from the business perspective, because it would be
very useful for the companies to directly identify
user’s level of satisfaction about their brands. Ta-
ble 3 shows that we achieve 69.56% accuracy on the
</bodyText>
<table confidence="0.999074666666667">
Method Dev. Test
Majority class 28.24 27.68
Logistic regression 36.03 35.16
Linear SVM 41.05 39.49
PCA 35.35 34.44
Lasso 40.74 39.53
Ridge 40.98 38.94
Elastic net 41.15 38.96
Laplacian structured sparsity 41.22* 40.22*
</table>
<tableCaption confidence="0.72877075">
Table 4: The joint brand-gender-satisfaction prediction
(18-way) performances. The best result is highlighted in
bold. * indicates p &lt; .001 comparing to the second best
result.
</tableCaption>
<bodyText confidence="0.999912333333333">
development set, and 67.32% accuracy on the test
set using our proposed Laplacian structured model
(p &lt; .001 comparing to the second best results).
</bodyText>
<subsectionHeader confidence="0.987047">
5.3 Joint Brand-Gender-Satisfaction
Prediction
</subsectionHeader>
<bodyText confidence="0.999906625">
Another big interest in the marketing community is
to predict subgroup preferences of specific brands.
In this direction, we perform a 18-way joint brand-
gender-satisfaction prediction using the gender la-
bels that we described in Section 3. Table 4
shows that our proposed Laplacian structured spar-
sity model obtains a test accuracy of 40.22%, signif-
icantly better than the second best result (p &lt; .001).
</bodyText>
<page confidence="0.995722">
1331
</page>
<figureCaption confidence="0.9973575">
Figure 2: Automatic brand identification test perfor-
mance varying the level of sparsity T in a Ll constrained
problem.
Figure 3: Joint brand-satisfaction prediction test perfor-
mance varying the level of sparsity T in a Ll constrained
problem.
</figureCaption>
<subsectionHeader confidence="0.676308">
5.4 Varying the Level of Sparsity in a
L1-Constrained Problem
</subsectionHeader>
<bodyText confidence="0.999988636363636">
To test the robustness of the Laplacian structured
sparsity component, we exponentially increase the
sum of weights T to vary the level of sparsity in a
L1-constrained setup. When T increases, the non-
zero weights in the model also increases. Figures 2
and 3 show that the Laplacian augmented L1-ball
projection statistically outperform the L1-ball pro-
jection baseline in all levels of sparsity (p &lt; .001).
In Figure 4, Laplacian augmented L1-ball projection
is also statistically better than the L1-ball projection
(p &lt; .001), except when T = 32 and T = 64.
</bodyText>
<subsectionHeader confidence="0.982622">
5.5 Exploratory Data Analysis
</subsectionHeader>
<bodyText confidence="0.999908166666667">
We outline the top 15 keywords from the Laplacian
structured sparsity model that are associated with the
Starbucks and Dunkin’ Donuts brands in the auto-
matic brand identification task in the Table 5. First
of all, it is observed that our model has discovered
synonyms for both brands: “sbux”, “dd”, “dds”.
</bodyText>
<figureCaption confidence="0.996482666666667">
Figure 4: Joint brand-gender-satisfaction prediction test
performance varying the level of sparsity T in a Ll con-
strained problem.
</figureCaption>
<bodyText confidence="0.999990514285714">
Also, the results imply that Starbucks’ unique cup
size branding strategy, “venti”, “grande”, “tall”, has
resonated with their customers as the words promi-
nently show up as top features in reviews. Aligned
with previous study in marketing science (Moon and
Quelch, 2006), an informative set of features re-
lated to Starbucks store decorations showed up in
our model: “store”, “restroom”, “public”, “bath-
room”, and “spacious”. In contrast, these features
stopped to show up on the list of Dunkin’ Donuts.
Instead, TV and game (sports), which are indeed
important features of dining at Dunkin’ Donut, ap-
peared. Note that Baskin-Robbins, which is a sub-
brand of Dunkin’ Brands Group, Inc., also appeared
as informative features to predict Dunkin’ Donuts.
To understand the preferences of different gen-
der subgroups towards the two brands, we contrast
in Table 6 and Table 7 the top features that identify
the satisfied female and male customers in the joint
brand-gender-satisfaction prediction task.
Interestingly, it seems that the female customers
identify Starbucks as a place for “studying”, with
“fireplace” as the top preference of the spots in the
store, and “winter” is also a high-ranked feature.
Also, the adjective “super” was frequently men-
tioned by the female Starbucks customers (but not
the males). As for Dunkin’ Donuts, the top-ranked
keywords are still mainly associated with its names,
but it seems the snack “Munchkins” is highly pre-
ferred by the female customers. Not surprisingly,
the cue words that the male customers identify the
Starbucks brand do not always agree with those of
the females. For example, instead of “fireplace”,
they prefer staying at the “patio”, and drink the cof-
fee from the “clover” brewing system. Interestingly,
</bodyText>
<page confidence="0.972443">
1332
</page>
<table confidence="0.999927764705883">
Starbucks weight Dunkin’ weight
starbucks 1.9365 dd 2.4224
sbux 1.0152 dunkin 1.7781
venti 0.8216 donuts 1.6989
corporate 0.7032 dunks 1.6455
store 0.6580 dds 1.4936
particular 0.6512 donut 1.3979
tall 0.5496 dunkins 1.3729
restroom 0.5447 glazed 0.9975
tourists 0.5431 robbins 0.9402
public 0.5260 baskin 0.8578
lines 0.4956 sugar 0.6475
drink 0.4787 d 0.6327
bathroom 0.4721 ice 0.5835
spacious 0.4629 stale 0.5404
location 0.4611 game 0.5049
grande 0.4563 tv 0.5010
</table>
<tableCaption confidence="0.9844315">
Table 5: Top features that identify the Starbucks and
Dunkin’ Donuts brands from the best model.
</tableCaption>
<bodyText confidence="0.988686545454545">
.
on the Dunkin’s side, “munchkins” also disappeared
and replaced by “glazed” (donuts). However, both
males and females agreed that “fast” or “quick” ser-
vice was an important feature of creating satisfac-
tion, which echoes with the result from self-reported
customer surveys (Moon and Quelch, 2006).
The word “name” is a prominent indicator for the
female customers of Starbucks: at first we were puz-
zled, but after we digged into the database, we found
reviews such as:
</bodyText>
<listItem confidence="0.998236">
• “... and the baristas are one of the nicest they
always ask for your name, so you never end up
with coffee meant for the guy behind you.”
• “... she asked me my name and i told her and
she excidetly proclaimed melissa and wrote my
name on the cup. This place was probably one
of the better starbucks ive been to.”
• “... all of their employees are really friendly,
</listItem>
<bodyText confidence="0.867987125">
and embarrassingly enough most know me by
name and know my typical drink order grande
nonfat misto with a flavor shot of white mocha.
This is actually very helpful.”
The above examples show how our system effec-
tively serves as a salient keyword spotter. And that
as a keyword spotter one can use it to extract sur-
rounding context and feed that through to the next
</bodyText>
<table confidence="0.999376545454545">
Starbucks weight Dunkin’ weight
starbucks 0.5013 dd 0.6931
chain 0.4268 dds 0.5620
winter 0.3382 dunkin 0.5344
fireplace 0.3089 donuts 0.4270
studying 0.2972 donut 0.3732
particular 0.2967 dunks 0.3687
super 0.2786 morning 0.3077
name 0.2543 quick 0.3012
know 0.2443 how 0.2940
because 0.2263 munchkins 0.2758
</table>
<tableCaption confidence="0.997525333333333">
Table 6: Top features that jointly identify the satisfied
female customers and the Starbucks and Dunkin’ Donuts
brands from the best model.
</tableCaption>
<table confidence="0.999902916666667">
Starbucks weight Dunkin’ weight
starbucks 0.6632 dd 0.7491
throw 0.3514 dunkin 0.6075
know 0.2959 dds 0.5333
store 0.2885 donuts 0.5326
fix 0.2498 donut 0.3215
particular 0.2487 dunks 0.3158
sbux 0.2462 morning 0.3095
patio 0.2349 rush 0.3030
prefer 0.2324 fast 0.2979
clover 0.2215 moving 0.2520
corporate 0.2153 glazed 0.2326
</table>
<tableCaption confidence="0.905684666666667">
Table 7: Top features that jointly identify the satisfied
male customers and the Starbucks and Dunkin’ Donuts
brands from the best model.
</tableCaption>
<bodyText confidence="0.998568833333333">
stage of analysis, including examination by humans.
This is extremely practical and useful, because it
provides actionable items. For example, analysts
can advise managers to revise their training manual
and tell store employees to remember the names of
your frequent female customers.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="method">
6 Discussions
</sectionHeader>
<bodyText confidence="0.9997435">
In our preliminary experiments, we have also ex-
perimented with the setup where the two keywords
“starbucks” and “dunkin” were removed from the
list of features. This change resulted in a uniformed
2% decrease in performances across all the models
in Table 2, which did not affect the comparisons.
</bodyText>
<page confidence="0.957464">
1333
</page>
<bodyText confidence="0.999989944444445">
However, we kept these two keywords in our final
experiments, because the reviewers sometimes men-
tion “Dunkin” in Starbucks reviews, and vice versa.
Removing the two keywords could be problematic,
since it changes the natural distribution of the data.
Regarding the alternative problem setups, our pre-
liminary experiments showed that instead of using
one-vs-all binary classifiers, a direct 9-way multi-
class classification of joint brand-satisfaction labels
using logistic regression only resulted an accuracy
of 62%. We also did not adopt the hierarchical clas-
sification pipeline, where instead of performing joint
classification, multiple layers of classifiers could be
trained to classify brand, gender, and satisfaction la-
bels incrementally. This is because the hierarchical
classifiers suffered from the error propagation prob-
lem, and the second/third layer classifier could not
correct the errors from the previous layers (Bennett
and Nguyen, 2009).
Our proposed method to generate inter-feature
affinity matrix captures interesting dependency of
features in this dataset. For example, although the
words “frappuccino” and “slurping”, “furniture” and
“mismatched’ are semantically very different, our
method actually group them together due to the sub-
tle interactions of these word pairs in our tasks. The
example in Figure 1 is also very specific to our
dataset. This is very useful, because the word se-
mantic similarity might be context-dependent, and
our method learns and adapts the semantic similar-
ity on the fly, hinges on the particular training set.
On the other end of the spectrum, even though our
method is desirable in our task, one might need to be
cautious when working on very small data sets with
only a handful of samples. This is because small
samples typically have large variances in feature dis-
tributions, and that the generated Laplacian matrix
might not be as reliable as in our study. To alleviate
this potential problem, one might consider building
the Laplacian matrix using external resources such
as WordNet or FrameNet, even though this approach
could also introduce biases due to the mismatched
task domains.
We also observed that the accuracy of the auto-
matic brand identification task was high, indicating
the promising future of CBA for hidden brand infor-
mation from other genres of text over the Web. Al-
though the performances of joint brand-satisfaction
and joint brand-gender-satisfaction predictions are
relatively lower, there is still much room for im-
provements: for example, using the syntactic, se-
mantic, and meta-data features could potentially en-
rich the proposed model. Also, it is possible to con-
sider the higher order n-gram features for better ex-
ploratory data analysis. However, since the focus of
this paper is a proof of concept for Laplacian struc-
tured sparsity models and computational branding
analytics, we have not yet explored various multi-
view representations to augment our model.
Why does Laplacian structured sparsity model
work better in these classication tasks? Similar to
the application in image classifcation (Gao et al.,
2010), one advantage of Laplacian regularization in
text classification is that our model can explicitly
model the dependency of local features. Another
reason is the expressiveness of our model: our model
allows one to express the feature interactions in a
structured manner. Thirdly, by embedding the struc-
ture in the regularization term, our model is more
flexible: one can now control the structured penalty
by tuning the regularization parameter on the devel-
opment set.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999983947368421">
We introduce a Laplacian structured sparsity model
for computational branding analytics (CBA). In the
automatic brand identification, our model achieves
the best result, dominating many competitive base-
lines. We also introduce the tasks of joint brand-
satisfaction and brand-gender-satisfaction predic-
tions, and show that the Laplacian structured spar-
sity do well in these tasks. A closer evaluation that
varying the levels of sparsity in a Li constrained
problem also indicates that the Laplacian augmented
Li-ball projection model can provide state-of-the-
art results. By examining the weights of the de-
rived Laplacian structured sparsity model, interest-
ing indicators of brands and theirs gender-specific
customer satisfaction associations are also discov-
ered. In the future, we would like to investigate other
methods for generating robust inter-feature Lapla-
cians that include deep syntactic and semantic fea-
tures.
</bodyText>
<sectionHeader confidence="0.959131" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9957845">
The authors would like to thank the anonymous re-
viewers for valuable comments.
</bodyText>
<page confidence="0.994101">
1334
</page>
<sectionHeader confidence="0.982585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999087171428571">
M. Belkin and P. Niyogi. 2001. Laplacian eigenmaps
and spectral techniques for embedding and cluster-
ing. Advances in neural information processing sys-
tems (NIPS).
M. Belkin, P. Niyogi, and V. Sindhwani. 2006. Mani-
fold regularization: A geometric framework for learn-
ing from labeled and unlabeled examples. The Journal
of Machine Learning Research (JMLR).
P. N. Bennett and N. Nguyen. 2009. Refined experts:
improving classification in large taxonomies. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval.
K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft.
1999. When is nearest neighbor meaningful? Pro-
ceedings of the International Conference on Database
Theory (ICDT).
F. Biadsy, W.Y. Wang, A. Rosenberg, and J. Hirschberg.
2011. Intoxication detection using phonetic, phono-
tactic and prosodic cues. In Proceedings of the 12th
Annual Conference of the International Speech Com-
munication Association (Interspeech 2011).
V. Chahuneau, K. Gimpel, B.R. Routledge, L. Scherlis,
and N.A. Smith. 2012. Word salad: Relating food
prices and descriptions.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library for
support vector machines. ACM Transactions on Intel-
ligent Systems and Technology (TIST).
W. Y. Chen, Y. Song, H. Bai, C. J. Lin, and E. Y. Chang.
2011. Parallel spectral clustering in distributed sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI).
Y. N. Chen, W. Y. Wang, and A. I. Rudnicky. 2013.
An empirical investigation of sparse log-linear models
for improved dialogue act classification. In Proceed-
ings of the 38th International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2013).
W. W. Cohen. 2012. Learning similarity measures based
on random walks. In Procceedings of the 21nd ACM
International Conference on Information and Knowl-
edge Management (CIKM).
L. DiCarlo. 2004. Dunkin’ donuts vs. starbucks. In
Forbes.com - Monday Matchup.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the l1-ball for learn-
ing in high dimensions. In Proceedings of the 25th in-
ternational conference on Machine learning (ICML).
J. Eisenstein, A. Ahmed, and E. Xing. 2011a. Sparse
additive generative models of text. Proceedings of the
28th International Conference on Machine Learning
(ICML 2011).
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011b. Dis-
covering sociolinguistic associations with structured
sparsity. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT).
S. Gao, I. W. Tsang, L. T. Chia, and P. Zhao. 2010. Local
features are not lonely–laplacian sparse coding for im-
age classification. In 2010 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR).
S. Gao, I. Tsang, and L. Chia. 2012. Laplacian sparse
coding, hypergraph laplacian sparse coding, and ap-
plications. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI).
P. M. Guadagni and J. D. C. Little. 1983. A logit model
of brand choice calibrated on scanner data. Marketing
science.
M. K. Kim, K. Lopetcharat, and M. A. Drake. 2013. In-
fluence of packaging information on consumer liking
of chocolate milk. Journal of dairy science.
A.A. Kuehn. 1962. Consumer brand choice as a learning
process.
S. Le Cessie and JC Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied statistics.
Chris Lederer and Sam Hill. 2001. See your brands
through your customers eyes. Harvard Business Re-
view, 79(6):125–133.
S.I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. 2006. Effi-
cient l1 regularized logistic regression. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI).
D.C. Liu and J. Nocedal. 1989. On the limited memory
bfgs method for large scale optimization. Mathemati-
cal programming.
D. Luo, H.G. Hosseini, and J.R. Stewart. 2004. Appli-
cation of ann with extracted parameters from an elec-
tronic nose in cigarette brand identification. Sensors
and Actuators B: Chemical.
J. C. Makens. 1965. Effect of brand preference upon
consumers perceived taste of turkey meat. Journal of
Applied Psychology.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Structured sparsity in
structured prediction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Y. Moon and J. Quelch. 2006. Starbucks: delivering
customer service. Harvard Business School.
M. OConnell and G. Gooding. 2006. The use of first
names to evaluate reports of gender and its effect
on the distribution of married and unmarried couple
households. In Proceedings of the Annual Meetings of
the Population Association of America.
S. Ovide. 2011. Face off! dunkin’ donuts vs. starbucks.
In Deal Journal - Wall Street Journal Blogs.
</reference>
<page confidence="0.815005">
1335
</page>
<reference confidence="0.998810720930232">
M. Paul and M. Dredze. 2012. Factorial lda: Sparse
multi-dimensional text models. In Advances in Neural
Information Processing Systems (NIPS).
F. Pelisson, D. Hall, O. Riff, and J. Crowley. 2003. Brand
identification using gaussian derivative histograms.
Computer Vision Systems.
R. Rifkin and A. Klautau. 2004. In defense of one-vs-
all classification. The Journal of Machine Learning
Research (JMLR).
M. Schmidt, G. Fung, and R. Rosales. 2007. Fast opti-
mization methods for l1 regularization: A comparative
study and two new approaches. Machine Learning.
M. Schmidt, E. Van Den Berg, M. Friedlander, and
K. Murphy. 2009. Optimizing costly functions
with simple constraints: A limited-memory projected
quasi-newton algorithm. In Proceedings of Confer-
ence on Artificial Intelligence and Statistics (AIStats).
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological).
W. Y. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fusion
learning. In Proceedings of the 12th annual SIGdial
Meeting on Discourse and Dialogue (SIGDIAL 2011).
W. Y. Wang, S. Finkelstein, A. Ogan, A. W. Black, and
J. Cassell. 2012a. “love ya, jerkface”: using sparse
log-linear models to build positive (and impolite) re-
lationships with teens. In Proceedings of the 13th
annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL 2012).
W. Y. Wang, E. Mayfield, S. Naidu, and J. Dittmar.
2012b. Historical analysis of legal opinions with a
sparse mixed-effects latent variable model. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
K. Q. Weinberger, F. Sha, Q. Zhu, and L. K. Saul. 2007.
Graph laplacian regularization for large-scale semidef-
inite programming. Advances in neural information
processing systems (NIPS).
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology).
</reference>
<page confidence="0.99274">
1336
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942965">
<title confidence="0.989783333333333">This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics</title>
<author confidence="0.999996">William Yang Wang Edward Lin</author>
<author confidence="0.999996">John Kominek</author>
<affiliation confidence="0.999991">School of Computer Science Voci Technologies, Inc.</affiliation>
<address confidence="0.995643">Carnegie Mellon University Pittsburgh, PA 15217</address>
<abstract confidence="0.999039038461539">We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which the advantages of modelmodeling significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Belkin</author>
<author>P Niyogi</author>
</authors>
<title>Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in neural information processing systems (NIPS).</title>
<date>2001</date>
<contexts>
<context position="17569" citStr="Belkin and Niyogi, 2001" startWordPosition="2862" endWordPosition="2865">ilarity between a pair of features p and q. In the spectral graph theory, this affinity matrix can be viewed as a weighted undirected graph G = (V, E), where each node Vp denotes a feature p, and each edge E(p,q) indicates the closeness among the features p and q. We also introduce a weighted diagonal degree matrix D, of which each element in the diagonal D(p,p) is the sum of all weighted connections of node Vp: D(p,p) = PQq�1 A(p,q). We propose the following objective function: � min − � + �1 ||W� ||+ �2 ||W� ||2 (10) � Wp − Wq||2A(p,q) (11) We then denote a graph Laplacian matrix L = D − A (Belkin and Niyogi, 2001), and rewrite the objective function as: mint − e+ A1 ||VV_ II+ A2|#V||2 (12) \ �+ α( W� �L W� ) (13) where α is the regularization parameter for the Laplacian structured sparsity term. Intuitively, the objective function can be interpreted as the sum of a negative log loss function, the sparsity-inducing penalty, the quadratic penalty, and the Laplacian structured penalty. Or, another view of this new model could be seen as a Laplacian augmented elastic net model where structured sparsity and feature interaction are considered. The Laplacian Matrix: In this model, a key aspect is to derive th</context>
</contexts>
<marker>Belkin, Niyogi, 2001</marker>
<rawString>M. Belkin and P. Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in neural information processing systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Belkin</author>
<author>P Niyogi</author>
<author>V Sindhwani</author>
</authors>
<title>Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="9228" citStr="Belkin et al. (2006)" startWordPosition="1424" endWordPosition="1427">sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective. To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012). However, combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging. Belkin et al. (2006) and Weinberger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning. Recently, Gao et al. (2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing. Unfortunately, this method might be too specific to the SIFT-based image coding tasks, thus might not be applicable to the text classification problem that utilizes n-gram lexical features. 3 Datasets We collected Yelp reviews from 1,860 Starbucks, Dunkin’ Donuts3, and other coffee s</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>M. Belkin, P. Niyogi, and V. Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Bennett</author>
<author>N Nguyen</author>
</authors>
<title>Refined experts: improving classification in large taxonomies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="34103" citStr="Bennett and Nguyen, 2009" startWordPosition="5610" endWordPosition="5613">that instead of using one-vs-all binary classifiers, a direct 9-way multiclass classification of joint brand-satisfaction labels using logistic regression only resulted an accuracy of 62%. We also did not adopt the hierarchical classification pipeline, where instead of performing joint classification, multiple layers of classifiers could be trained to classify brand, gender, and satisfaction labels incrementally. This is because the hierarchical classifiers suffered from the error propagation problem, and the second/third layer classifier could not correct the errors from the previous layers (Bennett and Nguyen, 2009). Our proposed method to generate inter-feature affinity matrix captures interesting dependency of features in this dataset. For example, although the words “frappuccino” and “slurping”, “furniture” and “mismatched’ are semantically very different, our method actually group them together due to the subtle interactions of these word pairs in our tasks. The example in Figure 1 is also very specific to our dataset. This is very useful, because the word semantic similarity might be context-dependent, and our method learns and adapts the semantic similarity on the fly, hinges on the particular trai</context>
</contexts>
<marker>Bennett, Nguyen, 2009</marker>
<rawString>P. N. Bennett and N. Nguyen. 2009. Refined experts: improving classification in large taxonomies. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Beyer</author>
<author>J Goldstein</author>
<author>R Ramakrishnan</author>
<author>U Shaft</author>
</authors>
<title>When is nearest neighbor meaningful?</title>
<date>1999</date>
<booktitle>Proceedings of the International Conference on Database Theory (ICDT).</booktitle>
<contexts>
<context position="18878" citStr="Beyer et al., 1999" startWordPosition="3083" endWordPosition="3086">Figure 1: An example of the graph G, the corresponding affinity matrix A, and the corresponding Laplacian matrix L. 1. Construct the distance matrix Dist. To construct the distance matrix between each feature, we first transpose the instance-feature matrix, I = P ��j, and assume that each feature j (e.g. unigram in our task) is a random variable that has a multinomial distribution over the instances in the training set. Then, we compare each pair of features, and calculate the interfeature distance matrix Dist with Euclidean distance as a measure, and use the k-nearest neighbors (kNN) method (Beyer et al., 1999) to select the top neighbors of each feature. 2. Derive the affinity matrix A. To assign the weight on the edge E(p,q) for each connected nodes (the kNN of V in Dist), we use the cosine similarity cosine(Vp, Vq) metric (Wang and Hirschberg, 2011). 3. Generate the degree matrix D and Laplacian matrix L. As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D − A. To calculate the above matrices in an efficient manner, we partition the covariate into blocks, and process each block in parallel (Che</context>
</contexts>
<marker>Beyer, Goldstein, Ramakrishnan, Shaft, 1999</marker>
<rawString>K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. 1999. When is nearest neighbor meaningful? Proceedings of the International Conference on Database Theory (ICDT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>W Y Wang</author>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>Intoxication detection using phonetic, phonotactic and prosodic cues.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Annual Conference of the International Speech Communication Association (Interspeech</booktitle>
<contexts>
<context position="7689" citStr="Biadsy et al., 2011" startWordPosition="1188" endWordPosition="1192">have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003). However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen e</context>
</contexts>
<marker>Biadsy, Wang, Rosenberg, Hirschberg, 2011</marker>
<rawString>F. Biadsy, W.Y. Wang, A. Rosenberg, and J. Hirschberg. 2011. Intoxication detection using phonetic, phonotactic and prosodic cues. In Proceedings of the 12th Annual Conference of the International Speech Communication Association (Interspeech 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Chahuneau</author>
<author>K Gimpel</author>
<author>B R Routledge</author>
<author>L Scherlis</author>
<author>N A Smith</author>
</authors>
<title>Word salad: Relating food prices and descriptions.</title>
<date>2012</date>
<contexts>
<context position="7667" citStr="Chahuneau et al., 2012" startWordPosition="1184" endWordPosition="1187">processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003). However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured predi</context>
<context position="13621" citStr="Chahuneau et al., 2012" startWordPosition="2158" endWordPosition="2161">assifiers: �θ(1) y , So, instead of drawing Y from a multinomial distribution Mult(F), we can draw the final label Y� that has the largest posterior across all k classifiers: Y = argmax Y,i=1,2,...,k where ~Xt is the testing vector, and Pr(Y |�θ(i) y,~Xt) is the posterior probability given the learned classifiers and the testing vector. In this paper, we investigate three multiclass classification tasks: first, we perform a 3-way classification task for automatic brand identification. In the task of brand-satisfaction prediction, we model the brand and the satisfaction label at the same time (Chahuneau et al., 2012): we perform the task of jointly predicting aggregate brand-satisfaction score for a review using 9-way classification. Similarly, we perform 18-way classification for the gender-specific joint brand-satisfaction prediction task. 4.2 The Log-Linear Framework and Its Regularized Variants If we consider the standard logistic regression model as the binary classifier in this log-linear framework7, then each classifier can be written as: �θy = 1 + exp � W~ � ~Xj� exp ( W~T ~Xj ) (3) here, ~Xj is the j-th observed feature vector, label y ∈ {0, 1}, and W~ is a vector of the coefficients. To 7We than</context>
</contexts>
<marker>Chahuneau, Gimpel, Routledge, Scherlis, Smith, 2012</marker>
<rawString>V. Chahuneau, K. Gimpel, B.R. Routledge, L. Scherlis, and N.A. Smith. 2012. Word salad: Relating food prices and descriptions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST).</booktitle>
<contexts>
<context position="22796" citStr="Chang and Lin, 2011" startWordPosition="3803" endWordPosition="3806">θyj − (�θyj)2� X~ − 1 − yj �θyj 1 −�θyj (25) + 2αL W~ + β − ξ = 0 (26) To speed up the training, we use the linear-time L1- ball projection method from Duchi et al. (2008) in our implementation. 5 Experiments We first compare our model to various baselines in the 3-way automatic brand identification task. Besides the logistic regression, lasso, ridge and elastic net model that we introduced in Section 4.2, we also compare with a PCA-based logistic regression model where the dimensions of the feature space is reduced in half before the classification. A state-ofthe-art linear kernel SVM model (Chang and Lin, 2011) is also taken into the comparison. In the second part, we perform 9-way joint classification of the brand-satisfaction labels. Similarly, we also perform a 18-way joint classification of the brandgender-satisfaction labels. To test the robustness of our model, we vary the levels of sparsity of our Laplacian augmented method in a L1-constrained problem. Finally, we analyze the identified features for CBA. Throughout this section, we use classification accuracy to report the results. We tune the regularization parameters of log-linear models and ∂(λ2 ||W~ ||2+ α( W~TL W~ )) (14) ∂ W~ X= − j ∂L </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C.C. Chang and C.J. Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Y Chen</author>
<author>Y Song</author>
<author>H Bai</author>
<author>C J Lin</author>
<author>E Y Chang</author>
</authors>
<title>Parallel spectral clustering in distributed systems.</title>
<date>2011</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</journal>
<contexts>
<context position="19493" citStr="Chen et al., 2011" startWordPosition="3197" endWordPosition="3200">99) to select the top neighbors of each feature. 2. Derive the affinity matrix A. To assign the weight on the edge E(p,q) for each connected nodes (the kNN of V in Dist), we use the cosine similarity cosine(Vp, Vq) metric (Wang and Hirschberg, 2011). 3. Generate the degree matrix D and Laplacian matrix L. As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D − A. To calculate the above matrices in an efficient manner, we partition the covariate into blocks, and process each block in parallel (Chen et al., 2011). An intuitive example of the graph G, its associated affinity matrix A, and Laplacian matrix L, is shown in Figure 1. Parameter Estimation: Regarding the optimization of objective function in (12-13), a notable problem is that the sparsity inducing L1 term is nondifferentiable, whereas this is not the case for the L2 norm and the Laplacian structured sparsity term. If we first take the derivative of the latter two terms, X+ α || (p,q) 1329 and we can derive the following gradient components: = 2λ2 W~ + α( W~TLT + W~TL) (15) = 2λ2 W~ + α(LT + L) W~ (16) since our Laplacian matrix is symmetric,</context>
</contexts>
<marker>Chen, Song, Bai, Lin, Chang, 2011</marker>
<rawString>W. Y. Chen, Y. Song, H. Bai, C. J. Lin, and E. Y. Chang. 2011. Parallel spectral clustering in distributed systems. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y N Chen</author>
<author>W Y Wang</author>
<author>A I Rudnicky</author>
</authors>
<title>An empirical investigation of sparse log-linear models for improved dialogue act classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<contexts>
<context position="8301" citStr="Chen et al. (2013)" startWordPosition="1280" endWordPosition="1283"> 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspec</context>
</contexts>
<marker>Chen, Wang, Rudnicky, 2013</marker>
<rawString>Y. N. Chen, W. Y. Wang, and A. I. Rudnicky. 2013. An empirical investigation of sparse log-linear models for improved dialogue act classification. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Learning similarity measures based on random walks.</title>
<date>2012</date>
<booktitle>In Procceedings of the 21nd ACM International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="9088" citStr="Cohen, 2012" startWordPosition="1405" endWordPosition="1406">tive in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective. To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012). However, combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging. Belkin et al. (2006) and Weinberger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning. Recently, Gao et al. (2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing. Unfortunately, this method might be too specific to the SIFT-based image coding tasks, thus might not be applicable to the text classification p</context>
</contexts>
<marker>Cohen, 2012</marker>
<rawString>W. W. Cohen. 2012. Learning similarity measures based on random walks. In Procceedings of the 21nd ACM International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L DiCarlo</author>
</authors>
<title>Dunkin’ donuts vs. starbucks.</title>
<date>2004</date>
<booktitle>In Forbes.com - Monday Matchup.</booktitle>
<contexts>
<context position="10477" citStr="DiCarlo, 2004" startWordPosition="1634" endWordPosition="1636">east regions in the period of 2009. A detail statistics of our data can be found in Table 1. The Midwest region includes 12 states4 and 19 major cities, and the Northeast region includes 9 states5 and 19 major cities. For each region, we divide the coffee shops into 60% training, 20% development, and 20% test, and there are no overlaps of coffee shops among these subsets. There are three values for the brand label: Starbucks, Dunkin’ Donuts, and all other coffee shop brands. The ma3We chose these two brands because they are reported as the leading coffee shops by WSJ (Ovide, 2011) and Forbes (DiCarlo, 2004). 4IL, WI, SD, ND, MN, MO, OH, NE, KS, IA, IN, and MI. 5CT, ME, MA, NH, RI, VT, NJ, NY and PA. Coffee Shops Reviews Train Dev. Test Train Dev. Test 1 451 150 150 3,513 1,087 1,424 2 665 222 222 6,982 2,530 2,358 T. 1,116 372 372 10,495 3,617 3,782 Table 1: Dataset statistics. 1: midwest region. 2: northeast region. T.: total. jority class is “all other coffee shop brands”, and the majority baseline is shown in Table 2. In the task of joint brand-satisfaction prediction, we utilize the review scores to approximate user satisfaction: scores 1-2 as the unsatisfactory label, 3 as moderate, and 4-5</context>
</contexts>
<marker>DiCarlo, 2004</marker>
<rawString>L. DiCarlo. 2004. Dunkin’ donuts vs. starbucks. In Forbes.com - Monday Matchup.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>T Chandra</author>
</authors>
<title>Efficient projections onto the l1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="22347" citStr="Duchi et al. (2008)" startWordPosition="3731" endWordPosition="3734">n method (Schmidt et al., 2009) to solve the dual form of this problem. The Lagrangian dual form of the problem in equation 19 can be written as: L( W~, ξ) = −` + α( W~ TL W~) (23) + β X p !~Wp − τ − ξ W~ (24) where β ∈ R is a Lagrange multiplier, and ξ ∈ Rp� is a p-dimensional vector of non-negative Lagrange multipliers. And then we can take first-order partial derivative with regard to W~, and set it to zero to derive the optimality: ! � yj �θyj − (�θyj)2� X~ − 1 − yj �θyj 1 −�θyj (25) + 2αL W~ + β − ξ = 0 (26) To speed up the training, we use the linear-time L1- ball projection method from Duchi et al. (2008) in our implementation. 5 Experiments We first compare our model to various baselines in the 3-way automatic brand identification task. Besides the logistic regression, lasso, ridge and elastic net model that we introduced in Section 4.2, we also compare with a PCA-based logistic regression model where the dimensions of the feature space is reduced in half before the classification. A state-ofthe-art linear kernel SVM model (Chang and Lin, 2011) is also taken into the comparison. In the second part, we perform 9-way joint classification of the brand-satisfaction labels. Similarly, we also perf</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. 2008. Efficient projections onto the l1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>A Ahmed</author>
<author>E Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>Proceedings of the 28th International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="7971" citStr="Eisenstein et al. (2011" startWordPosition="1232" endWordPosition="1235">nding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Pau</context>
<context position="15363" citStr="Eisenstein et al., 2011" startWordPosition="2455" endWordPosition="2458">ver, this model does not penalize the noisy features and unreliable features that might overfit to the training data. To address this issue, we introduce the L1 norm from lasso technique (Tibshirani, 1996) to regularize the above likelihood function. Thus, instead of maximizing the likelihood, we can minimize the loss function of the negative log-likelihood with a linear penalty: � � min − ` + λ1 ||W~ ||(7) where λ1 is the regularization coefficient. The benefit of L1 penalty in a discriminative model is similar to the double exponential distribution of the sparse priors in generative models (Eisenstein et al., 2011a): they both push the weights of many noisy features into zeros, revealing only the important features. However, since the L1 penalty can introduce discontinuities to the original convex function, we can also consider an alternative non-sparse ridge estimator (Le Cessie and Van Houwelingen, 1992) with log loss and L2 norm, and has the convex property: � − ` + λ2 ||W~ ||2� min (8) Another option that balances the sparsity and smoothness would be the elastic net model (Zou and Hastie, 2005) that uses the composite penalty: �− ` + λ1 ||W~ ||+ λ2 ||W~ ||2� min (9) 4.3 The Laplacian Structured Spa</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>J. Eisenstein, A. Ahmed, and E. Xing. 2011a. Sparse additive generative models of text. Proceedings of the 28th International Conference on Machine Learning (ICML 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT).</booktitle>
<contexts>
<context position="7971" citStr="Eisenstein et al. (2011" startWordPosition="1232" endWordPosition="1235">nding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Pau</context>
<context position="15363" citStr="Eisenstein et al., 2011" startWordPosition="2455" endWordPosition="2458">ver, this model does not penalize the noisy features and unreliable features that might overfit to the training data. To address this issue, we introduce the L1 norm from lasso technique (Tibshirani, 1996) to regularize the above likelihood function. Thus, instead of maximizing the likelihood, we can minimize the loss function of the negative log-likelihood with a linear penalty: � � min − ` + λ1 ||W~ ||(7) where λ1 is the regularization coefficient. The benefit of L1 penalty in a discriminative model is similar to the double exponential distribution of the sparse priors in generative models (Eisenstein et al., 2011a): they both push the weights of many noisy features into zeros, revealing only the important features. However, since the L1 penalty can introduce discontinuities to the original convex function, we can also consider an alternative non-sparse ridge estimator (Le Cessie and Van Houwelingen, 1992) with log loss and L2 norm, and has the convex property: � − ` + λ2 ||W~ ||2� min (8) Another option that balances the sparsity and smoothness would be the elastic net model (Zou and Hastie, 2005) that uses the composite penalty: �− ` + λ1 ||W~ ||+ λ2 ||W~ ||2� min (9) 4.3 The Laplacian Structured Spa</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>J. Eisenstein, N. A. Smith, and E. P. Xing. 2011b. Discovering sociolinguistic associations with structured sparsity. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gao</author>
<author>I W Tsang</author>
<author>L T Chia</author>
<author>P Zhao</author>
</authors>
<title>Local features are not lonely–laplacian sparse coding for image classification.</title>
<date>2010</date>
<booktitle>In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="36248" citStr="Gao et al., 2010" startWordPosition="5950" endWordPosition="5953">ch room for improvements: for example, using the syntactic, semantic, and meta-data features could potentially enrich the proposed model. Also, it is possible to consider the higher order n-gram features for better exploratory data analysis. However, since the focus of this paper is a proof of concept for Laplacian structured sparsity models and computational branding analytics, we have not yet explored various multiview representations to augment our model. Why does Laplacian structured sparsity model work better in these classication tasks? Similar to the application in image classifcation (Gao et al., 2010), one advantage of Laplacian regularization in text classification is that our model can explicitly model the dependency of local features. Another reason is the expressiveness of our model: our model allows one to express the feature interactions in a structured manner. Thirdly, by embedding the structure in the regularization term, our model is more flexible: one can now control the structured penalty by tuning the regularization parameter on the development set. 7 Conclusions We introduce a Laplacian structured sparsity model for computational branding analytics (CBA). In the automatic bran</context>
</contexts>
<marker>Gao, Tsang, Chia, Zhao, 2010</marker>
<rawString>S. Gao, I. W. Tsang, L. T. Chia, and P. Zhao. 2010. Local features are not lonely–laplacian sparse coding for image classification. In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gao</author>
<author>I Tsang</author>
<author>L Chia</author>
</authors>
<title>Laplacian sparse coding, hypergraph laplacian sparse coding, and applications.</title>
<date>2012</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</journal>
<contexts>
<context position="9399" citStr="Gao et al. (2012)" startWordPosition="1452" endWordPosition="1455">lem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective. To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012). However, combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging. Belkin et al. (2006) and Weinberger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning. Recently, Gao et al. (2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing. Unfortunately, this method might be too specific to the SIFT-based image coding tasks, thus might not be applicable to the text classification problem that utilizes n-gram lexical features. 3 Datasets We collected Yelp reviews from 1,860 Starbucks, Dunkin’ Donuts3, and other coffee shops all over the Midwest and Northeast regions in the period of 2009. A detail statistics of our data can be found in Table 1. The Midwest region includes 12 states4 and </context>
<context position="24391" citStr="Gao et al., 2012" startWordPosition="4062" endWordPosition="4065">d best result. Method Dev. Test Majority class 55.43 55.18 Logistic regression 65.80 65.80 Linear SVM 67.67 65.44 PCA 63.92 62.53 Lasso 68.37 66.84 Ridge 67.79 65.55 Elastic net 68.79 66.82 Laplacian structured sparsity 69.56* 67.32* Table 3: The joint brand-satisfaction prediction (9-way) performances. The best result is highlighted in bold. * indicates p &lt; .001 comparing to the second best result. the cost parameter of the SVM on the development set, and report results on both the development set and the held-out test set. The parameter for kNN was set to 5 according to previous literature (Gao et al., 2012). A paired two-tailed t-test is used to test the statistical differences among various models. 5.1 Automatic Brand Identification from Text Given any piece of raw text from the Web (e.g. blogs, tweets, news, or forum posts), the first task for CBA is to identify which brand this text is related to. Our customer review data set is useful for this task, because the ground truth of the brand label is attached to each review. Table 2 shows the result of our model in this automatic brand identification task. In this 3-way classification task, the overall results indicate that it is relatively easy </context>
</contexts>
<marker>Gao, Tsang, Chia, 2012</marker>
<rawString>S. Gao, I. Tsang, and L. Chia. 2012. Laplacian sparse coding, hypergraph laplacian sparse coding, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Guadagni</author>
<author>J D C Little</author>
</authors>
<title>A logit model of brand choice calibrated on scanner data. Marketing science.</title>
<date>1983</date>
<contexts>
<context position="6575" citStr="Guadagni and Little (1983)" startWordPosition="1014" endWordPosition="1017">to brand and its associated pragmatics. In the next section, we outline related work in CBA, sparsity, and spectral graph learning. In Section 3, we describe the corpus in this study. The Laplacian structured sparsity model is introduced in Section 4. The experimental setup and results are presented in Section 5. A short discussion is followed in Section 6 and we conclude in Section 7. 2 Related Work Early work on statistical brand analysis in the marketing community dates back to the work of Kuehn (1962), where he first hypothesizes that brand choice could be described as a learning process. Guadagni and Little (1983) further empirically tested the hypothesis by building a calibrated multinomial logistic regression model to predict the purchase of ground coffee, using the data from the optical scanning of product code in supermarkets. Outside the marketing community, statistical brand analysis is rarely seen. More recently, a study (Luo et al., 2004) applies neural networks to identify cigarette brands, with the hope of detecting illegal cigarettes from smell features. In image processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2</context>
</contexts>
<marker>Guadagni, Little, 1983</marker>
<rawString>P. M. Guadagni and J. D. C. Little. 1983. A logit model of brand choice calibrated on scanner data. Marketing science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Kim</author>
<author>K Lopetcharat</author>
<author>M A Drake</author>
</authors>
<title>Influence of packaging information on consumer liking of chocolate milk.</title>
<date>2013</date>
<journal>Journal of dairy science.</journal>
<contexts>
<context position="1709" citStr="Kim et al., 2013" startWordPosition="249" endWordPosition="252">pproach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands. 1 Introduction In marketing science, branding is a modern marketing strategy of creating a unique image for a product in the customers’ mind. Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013). In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge1, where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing’s results, Google’s brand might have strengthened over the years. These studies all suggest that brand and its asso</context>
</contexts>
<marker>Kim, Lopetcharat, Drake, 2013</marker>
<rawString>M. K. Kim, K. Lopetcharat, and M. A. Drake. 2013. Influence of packaging information on consumer liking of chocolate milk. Journal of dairy science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Kuehn</author>
</authors>
<title>Consumer brand choice as a learning process.</title>
<date>1962</date>
<contexts>
<context position="6459" citStr="Kuehn (1962)" startWordPosition="997" endWordPosition="998">ve analysis of our machine learning model shows the interesting features and language use that relate to brand and its associated pragmatics. In the next section, we outline related work in CBA, sparsity, and spectral graph learning. In Section 3, we describe the corpus in this study. The Laplacian structured sparsity model is introduced in Section 4. The experimental setup and results are presented in Section 5. A short discussion is followed in Section 6 and we conclude in Section 7. 2 Related Work Early work on statistical brand analysis in the marketing community dates back to the work of Kuehn (1962), where he first hypothesizes that brand choice could be described as a learning process. Guadagni and Little (1983) further empirically tested the hypothesis by building a calibrated multinomial logistic regression model to predict the purchase of ground coffee, using the data from the optical scanning of product code in supermarkets. Outside the marketing community, statistical brand analysis is rarely seen. More recently, a study (Luo et al., 2004) applies neural networks to identify cigarette brands, with the hope of detecting illegal cigarettes from smell features. In image processing, re</context>
</contexts>
<marker>Kuehn, 1962</marker>
<rawString>A.A. Kuehn. 1962. Consumer brand choice as a learning process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Le Cessie</author>
<author>JC Van Houwelingen</author>
</authors>
<title>Ridge estimators in logistic regression. Applied statistics.</title>
<date>1992</date>
<marker>Le Cessie, Van Houwelingen, 1992</marker>
<rawString>S. Le Cessie and JC Van Houwelingen. 1992. Ridge estimators in logistic regression. Applied statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Lederer</author>
<author>Sam Hill</author>
</authors>
<title>See your brands through your customers eyes.</title>
<date>2001</date>
<journal>Harvard Business Review,</journal>
<volume>79</volume>
<issue>6</issue>
<contexts>
<context position="1690" citStr="Lederer and Hill, 2001" startWordPosition="245" endWordPosition="248">luation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands. 1 Introduction In marketing science, branding is a modern marketing strategy of creating a unique image for a product in the customers’ mind. Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013). In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge1, where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing’s results, Google’s brand might have strengthened over the years. These studies all suggest that</context>
</contexts>
<marker>Lederer, Hill, 2001</marker>
<rawString>Chris Lederer and Sam Hill. 2001. See your brands through your customers eyes. Harvard Business Review, 79(6):125–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S I Lee</author>
<author>H Lee</author>
<author>P Abbeel</author>
<author>A Y Ng</author>
</authors>
<title>Efficient l1 regularized logistic regression.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="20995" citStr="Lee et al., 2006" startWordPosition="3465" endWordPosition="3468">oblem. The L-BFGS method has relatively low space complexity, and does not require the calculation of full Hessian matrix, thus it is often used for L1 optimization problems. Augmented Laplacian for an L1-Constrained Problem: Instead of formulating the L1-regularized problem by adding the L1 norm, an alternative solution is to formulate a L1-constrained problem by fixing the sum of all weights τ in the weight vector W~. The reason is because adding the L1 norm will make the objective function not continuously differentiable, where as the L1 constraint could be just a simple linear constraint (Lee et al., 2006). Thus, the alternative L1-constrained problem could be defined as: Xmin(−`), s.t. ~Wp ≤ τ (18) p To test the robustness of Laplacian structured sparsity term in the setup of a L1-constrained problem, we can incorporate the Laplacian penalty term into the above formula, and derive: min − ` + α( W~ TL W~ ) , s.t. � � X ~Wp ≤ τ (19) p Note that the Laplacian matrix is positivesemidefinite, W~TL W~ = W~T X L(p,q) W~ (20) (p,q) X= W~TL(p,q) W~ (21) (p,q) X= ||~Wp − ~Wq||2A(p,q) (22) (p,q) because this graph Laplacian penalty can be viewed as a quadratic term, and the objective function in equation</context>
</contexts>
<marker>Lee, Lee, Abbeel, Ng, 2006</marker>
<rawString>S.I. Lee, H. Lee, P. Abbeel, and A.Y. Ng. 2006. Efficient l1 regularized logistic regression. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization. Mathematical programming.</title>
<date>1989</date>
<contexts>
<context position="20347" citStr="Liu and Nocedal, 1989" startWordPosition="3356" endWordPosition="3360">sparsity inducing L1 term is nondifferentiable, whereas this is not the case for the L2 norm and the Laplacian structured sparsity term. If we first take the derivative of the latter two terms, X+ α || (p,q) 1329 and we can derive the following gradient components: = 2λ2 W~ + α( W~TLT + W~TL) (15) = 2λ2 W~ + α(LT + L) W~ (16) since our Laplacian matrix is symmetric, we can rewrite (16) as 2(λ2 W~ + αL W~) (17) Then, we combine the gradient of the log loss function in (5) with (17), and apply a bound-constrained re-formulation (Schmidt et al., 2007) and the limited memory BFGS (L-BFGS) method (Liu and Nocedal, 1989) to solve the L1 regularized problem. The L-BFGS method has relatively low space complexity, and does not require the calculation of full Hessian matrix, thus it is often used for L1 optimization problems. Augmented Laplacian for an L1-Constrained Problem: Instead of formulating the L1-regularized problem by adding the L1 norm, an alternative solution is to formulate a L1-constrained problem by fixing the sum of all weights τ in the weight vector W~. The reason is because adding the L1 norm will make the objective function not continuously differentiable, where as the L1 constraint could be ju</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D.C. Liu and J. Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Luo</author>
<author>H G Hosseini</author>
<author>J R Stewart</author>
</authors>
<title>Application of ann with extracted parameters from an electronic nose in cigarette brand identification. Sensors and Actuators B:</title>
<date>2004</date>
<publisher>Chemical.</publisher>
<contexts>
<context position="2894" citStr="Luo et al., 2004" startWordPosition="432" endWordPosition="435"> suggest that brand and its associations play important roles in the customers’ perceptions and decisions. To accommodate the market change, companies frequently adjust branding strategies by analyzing how their customers receive and respond to branding messages. So far, such analysis is often done by using surveys and focus groups (Moon and Quelch, 2006), which is expensive and not timeefficient. Recently, with the advance of machine learning techniques, researchers from the chemistry and vision communities started to pay attention to the problem of automatic brand identification from smell (Luo et al., 2004) and images (Pelisson et al., 2003). In contrast, even though textual data that contains hidden branding information is abundantly available in many forms over the Web, automatic discovery and computational analysis on such data are not well studied in the past. Computational branding analytics (CBA) seeks to extract information, trends, and demographics about a brand on the basis of free-form text, e.g. from blogs, Twitter comments, reviews, or forum posts. As described in Section 3, in this study we use a sub1http://en.wikipedia.org/wiki/Pepsi Challenge 2http://www.bingiton.com/ 1325 Proceed</context>
<context position="6914" citStr="Luo et al., 2004" startWordPosition="1065" endWordPosition="1068"> Section 6 and we conclude in Section 7. 2 Related Work Early work on statistical brand analysis in the marketing community dates back to the work of Kuehn (1962), where he first hypothesizes that brand choice could be described as a learning process. Guadagni and Little (1983) further empirically tested the hypothesis by building a calibrated multinomial logistic regression model to predict the purchase of ground coffee, using the data from the optical scanning of product code in supermarkets. Outside the marketing community, statistical brand analysis is rarely seen. More recently, a study (Luo et al., 2004) applies neural networks to identify cigarette brands, with the hope of detecting illegal cigarettes from smell features. In image processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003). However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years,</context>
</contexts>
<marker>Luo, Hosseini, Stewart, 2004</marker>
<rawString>D. Luo, H.G. Hosseini, and J.R. Stewart. 2004. Application of ann with extracted parameters from an electronic nose in cigarette brand identification. Sensors and Actuators B: Chemical.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Makens</author>
</authors>
<title>Effect of brand preference upon consumers perceived taste of turkey meat.</title>
<date>1965</date>
<journal>Journal of Applied Psychology.</journal>
<contexts>
<context position="1666" citStr="Makens, 1965" startWordPosition="243" endWordPosition="244">antitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands. 1 Introduction In marketing science, branding is a modern marketing strategy of creating a unique image for a product in the customers’ mind. Establishing the brand in the broad social context is just as important as building a good product (Makens, 1965; Lederer and Hill, 2001; Kim et al., 2013). In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge1, where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing’s results, Google’s brand might have strengthened over the years. These </context>
</contexts>
<marker>Makens, 1965</marker>
<rawString>J. C. Makens. 1965. Effect of brand preference upon consumers perceived taste of turkey meat. Journal of Applied Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8185" citStr="Martins et al. (2011)" startWordPosition="1264" endWordPosition="1267">own to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Moon</author>
<author>J Quelch</author>
</authors>
<title>Starbucks: delivering customer service.</title>
<date>2006</date>
<publisher>Harvard Business School.</publisher>
<contexts>
<context position="2634" citStr="Moon and Quelch, 2006" startWordPosition="393" endWordPosition="396">a’s branding has made it more popular. Even now, Microsoft uses similar blind taste tests2 to compare search engines, Bing and Google, showing that although participants prefer Bing’s results, Google’s brand might have strengthened over the years. These studies all suggest that brand and its associations play important roles in the customers’ perceptions and decisions. To accommodate the market change, companies frequently adjust branding strategies by analyzing how their customers receive and respond to branding messages. So far, such analysis is often done by using surveys and focus groups (Moon and Quelch, 2006), which is expensive and not timeefficient. Recently, with the advance of machine learning techniques, researchers from the chemistry and vision communities started to pay attention to the problem of automatic brand identification from smell (Luo et al., 2004) and images (Pelisson et al., 2003). In contrast, even though textual data that contains hidden branding information is abundantly available in many forms over the Web, automatic discovery and computational analysis on such data are not well studied in the past. Computational branding analytics (CBA) seeks to extract information, trends, </context>
<context position="28373" citStr="Moon and Quelch, 2006" startWordPosition="4705" endWordPosition="4708">associated with the Starbucks and Dunkin’ Donuts brands in the automatic brand identification task in the Table 5. First of all, it is observed that our model has discovered synonyms for both brands: “sbux”, “dd”, “dds”. Figure 4: Joint brand-gender-satisfaction prediction test performance varying the level of sparsity T in a Ll constrained problem. Also, the results imply that Starbucks’ unique cup size branding strategy, “venti”, “grande”, “tall”, has resonated with their customers as the words prominently show up as top features in reviews. Aligned with previous study in marketing science (Moon and Quelch, 2006), an informative set of features related to Starbucks store decorations showed up in our model: “store”, “restroom”, “public”, “bathroom”, and “spacious”. In contrast, these features stopped to show up on the list of Dunkin’ Donuts. Instead, TV and game (sports), which are indeed important features of dining at Dunkin’ Donut, appeared. Note that Baskin-Robbins, which is a subbrand of Dunkin’ Brands Group, Inc., also appeared as informative features to predict Dunkin’ Donuts. To understand the preferences of different gender subgroups towards the two brands, we contrast in Table 6 and Table 7 t</context>
<context position="30727" citStr="Moon and Quelch, 2006" startWordPosition="5072" endWordPosition="5075">lazed 0.9975 tourists 0.5431 robbins 0.9402 public 0.5260 baskin 0.8578 lines 0.4956 sugar 0.6475 drink 0.4787 d 0.6327 bathroom 0.4721 ice 0.5835 spacious 0.4629 stale 0.5404 location 0.4611 game 0.5049 grande 0.4563 tv 0.5010 Table 5: Top features that identify the Starbucks and Dunkin’ Donuts brands from the best model. . on the Dunkin’s side, “munchkins” also disappeared and replaced by “glazed” (donuts). However, both males and females agreed that “fast” or “quick” service was an important feature of creating satisfaction, which echoes with the result from self-reported customer surveys (Moon and Quelch, 2006). The word “name” is a prominent indicator for the female customers of Starbucks: at first we were puzzled, but after we digged into the database, we found reviews such as: • “... and the baristas are one of the nicest they always ask for your name, so you never end up with coffee meant for the guy behind you.” • “... she asked me my name and i told her and she excidetly proclaimed melissa and wrote my name on the cup. This place was probably one of the better starbucks ive been to.” • “... all of their employees are really friendly, and embarrassingly enough most know me by name and know my t</context>
</contexts>
<marker>Moon, Quelch, 2006</marker>
<rawString>Y. Moon and J. Quelch. 2006. Starbucks: delivering customer service. Harvard Business School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M OConnell</author>
<author>G Gooding</author>
</authors>
<title>The use of first names to evaluate reports of gender and its effect on the distribution of married and unmarried couple households.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meetings of the Population</booktitle>
<publisher>Association of America.</publisher>
<contexts>
<context position="11236" citStr="OConnell and Gooding, 2006" startWordPosition="1770" endWordPosition="1773">Train Dev. Test 1 451 150 150 3,513 1,087 1,424 2 665 222 222 6,982 2,530 2,358 T. 1,116 372 372 10,495 3,617 3,782 Table 1: Dataset statistics. 1: midwest region. 2: northeast region. T.: total. jority class is “all other coffee shop brands”, and the majority baseline is shown in Table 2. In the task of joint brand-satisfaction prediction, we utilize the review scores to approximate user satisfaction: scores 1-2 as the unsatisfactory label, 3 as moderate, and 4-5 as satisfactory. Since the Yelp reviews do not reveal the reviewer’s gender, we use a similar method that U.S. Census Bureau used (OConnell and Gooding, 2006): we first automatically match the first name of the reviewer with the prior namegender distributions in the census records, then manually examine the no-match cases and a subsample of the matched cases. For those who we cannot determine the gender, the review will be dropped from the gender-specific brand-satisfaction prediction task. After filtering, there are 8,528 documents for training, 2,928 for development, and, 3,046 for testing. Since the focus of this paper is not on feature engineering, we use unigram features to represent each review. Below is an example of positive review from a m</context>
</contexts>
<marker>OConnell, Gooding, 2006</marker>
<rawString>M. OConnell and G. Gooding. 2006. The use of first names to evaluate reports of gender and its effect on the distribution of married and unmarried couple households. In Proceedings of the Annual Meetings of the Population Association of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ovide</author>
</authors>
<title>Face off! dunkin’ donuts vs. starbucks.</title>
<date>2011</date>
<journal>In Deal Journal - Wall Street Journal Blogs.</journal>
<contexts>
<context position="10450" citStr="Ovide, 2011" startWordPosition="1630" endWordPosition="1631">ver the Midwest and Northeast regions in the period of 2009. A detail statistics of our data can be found in Table 1. The Midwest region includes 12 states4 and 19 major cities, and the Northeast region includes 9 states5 and 19 major cities. For each region, we divide the coffee shops into 60% training, 20% development, and 20% test, and there are no overlaps of coffee shops among these subsets. There are three values for the brand label: Starbucks, Dunkin’ Donuts, and all other coffee shop brands. The ma3We chose these two brands because they are reported as the leading coffee shops by WSJ (Ovide, 2011) and Forbes (DiCarlo, 2004). 4IL, WI, SD, ND, MN, MO, OH, NE, KS, IA, IN, and MI. 5CT, ME, MA, NH, RI, VT, NJ, NY and PA. Coffee Shops Reviews Train Dev. Test Train Dev. Test 1 451 150 150 3,513 1,087 1,424 2 665 222 222 6,982 2,530 2,358 T. 1,116 372 372 10,495 3,617 3,782 Table 1: Dataset statistics. 1: midwest region. 2: northeast region. T.: total. jority class is “all other coffee shop brands”, and the majority baseline is shown in Table 2. In the task of joint brand-satisfaction prediction, we utilize the review scores to approximate user satisfaction: scores 1-2 as the unsatisfactory la</context>
</contexts>
<marker>Ovide, 2011</marker>
<rawString>S. Ovide. 2011. Face off! dunkin’ donuts vs. starbucks. In Deal Journal - Wall Street Journal Blogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>M Dredze</author>
</authors>
<title>Factorial lda: Sparse multi-dimensional text models.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="8590" citStr="Paul and Dredze, 2012" startWordPosition="1325" endWordPosition="1328">011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective. To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012). However, combining the sparse modeling and spectral graphical modeling approaches in a principled wa</context>
</contexts>
<marker>Paul, Dredze, 2012</marker>
<rawString>M. Paul and M. Dredze. 2012. Factorial lda: Sparse multi-dimensional text models. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pelisson</author>
<author>D Hall</author>
<author>O Riff</author>
<author>J Crowley</author>
</authors>
<title>Brand identification using gaussian derivative histograms. Computer Vision Systems.</title>
<date>2003</date>
<contexts>
<context position="2929" citStr="Pelisson et al., 2003" startWordPosition="438" endWordPosition="441">sociations play important roles in the customers’ perceptions and decisions. To accommodate the market change, companies frequently adjust branding strategies by analyzing how their customers receive and respond to branding messages. So far, such analysis is often done by using surveys and focus groups (Moon and Quelch, 2006), which is expensive and not timeefficient. Recently, with the advance of machine learning techniques, researchers from the chemistry and vision communities started to pay attention to the problem of automatic brand identification from smell (Luo et al., 2004) and images (Pelisson et al., 2003). In contrast, even though textual data that contains hidden branding information is abundantly available in many forms over the Web, automatic discovery and computational analysis on such data are not well studied in the past. Computational branding analytics (CBA) seeks to extract information, trends, and demographics about a brand on the basis of free-form text, e.g. from blogs, Twitter comments, reviews, or forum posts. As described in Section 3, in this study we use a sub1http://en.wikipedia.org/wiki/Pepsi Challenge 2http://www.bingiton.com/ 1325 Proceedings of the 2013 Conference on Empi</context>
<context position="7179" citStr="Pelisson et al., 2003" startWordPosition="1104" endWordPosition="1107"> and Little (1983) further empirically tested the hypothesis by building a calibrated multinomial logistic regression model to predict the purchase of ground coffee, using the data from the optical scanning of product code in supermarkets. Outside the marketing community, statistical brand analysis is rarely seen. More recently, a study (Luo et al., 2004) applies neural networks to identify cigarette brands, with the hope of detecting illegal cigarettes from smell features. In image processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003). However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attr</context>
</contexts>
<marker>Pelisson, Hall, Riff, Crowley, 2003</marker>
<rawString>F. Pelisson, D. Hall, O. Riff, and J. Crowley. 2003. Brand identification using gaussian derivative histograms. Computer Vision Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rifkin</author>
<author>A Klautau</author>
</authors>
<title>In defense of one-vsall classification.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="12712" citStr="Rifkin and Klautau, 2004" startWordPosition="2009" endWordPosition="2012"> city garden. Friendly and fast service. Not open Sundays. The coffee shop dataset is freely available6 for research purposes. 4 Our Approach 4.1 Problem Formulation and Predictive Tasks The automatic brand identification problem could be considered as a traditional multiclass classifica6http://www.cs.cmu.edu/˜yww/data/emnlp2013.zip 1327 tion problem where the estimated label Y could be drawn from Mult(F), where F is the parameter for the multinomial distribution. To solve this, a simple but accurate solution is to decompose the multiclass problem into multiple binary classification problems (Rifkin and Klautau, 2004) by training k one-vs-all binary classifiers, and then use the argmax criteria to select the best hypothesis from the k posteriors. As for a binary classifier, we need to infer the posterior from a Bernoulli distribution that is parametrized by �θy. Similarly, we can derive k binary classifiers: �θ(1) y , So, instead of drawing Y from a multinomial distribution Mult(F), we can draw the final label Y� that has the largest posterior across all k classifiers: Y = argmax Y,i=1,2,...,k where ~Xt is the testing vector, and Pr(Y |�θ(i) y,~Xt) is the posterior probability given the learned classifiers</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>R. Rifkin and A. Klautau. 2004. In defense of one-vsall classification. The Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmidt</author>
<author>G Fung</author>
<author>R Rosales</author>
</authors>
<title>Fast optimization methods for l1 regularization: A comparative study and two new approaches.</title>
<date>2007</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="20279" citStr="Schmidt et al., 2007" startWordPosition="3344" endWordPosition="3347">on of objective function in (12-13), a notable problem is that the sparsity inducing L1 term is nondifferentiable, whereas this is not the case for the L2 norm and the Laplacian structured sparsity term. If we first take the derivative of the latter two terms, X+ α || (p,q) 1329 and we can derive the following gradient components: = 2λ2 W~ + α( W~TLT + W~TL) (15) = 2λ2 W~ + α(LT + L) W~ (16) since our Laplacian matrix is symmetric, we can rewrite (16) as 2(λ2 W~ + αL W~) (17) Then, we combine the gradient of the log loss function in (5) with (17), and apply a bound-constrained re-formulation (Schmidt et al., 2007) and the limited memory BFGS (L-BFGS) method (Liu and Nocedal, 1989) to solve the L1 regularized problem. The L-BFGS method has relatively low space complexity, and does not require the calculation of full Hessian matrix, thus it is often used for L1 optimization problems. Augmented Laplacian for an L1-Constrained Problem: Instead of formulating the L1-regularized problem by adding the L1 norm, an alternative solution is to formulate a L1-constrained problem by fixing the sum of all weights τ in the weight vector W~. The reason is because adding the L1 norm will make the objective function not</context>
</contexts>
<marker>Schmidt, Fung, Rosales, 2007</marker>
<rawString>M. Schmidt, G. Fung, and R. Rosales. 2007. Fast optimization methods for l1 regularization: A comparative study and two new approaches. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmidt</author>
<author>E Van Den Berg</author>
<author>M Friedlander</author>
<author>K Murphy</author>
</authors>
<title>Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of Conference on Artificial Intelligence and Statistics (AIStats).</booktitle>
<marker>Schmidt, Van Den Berg, Friedlander, Murphy, 2009</marker>
<rawString>M. Schmidt, E. Van Den Berg, M. Friedlander, and K. Murphy. 2009. Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. In Proceedings of Conference on Artificial Intelligence and Statistics (AIStats).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological).</journal>
<contexts>
<context position="14945" citStr="Tibshirani, 1996" startWordPosition="2386" endWordPosition="2387">rs in equation (3), we only need to set the weights W~. We can obtain the following log likelihood, and its gradient function by taking the first-order partial derivative of W~: ` = � yj log Oyj + (1 − yj) log(1 − θyj ) (4) j �= j � �θyj − (�θyj)2� = ~X, (6) since the log likelihood objective function (4) is concave, using standard gradient ascent with maximum likelihood estimation can solve the problem. However, this model does not penalize the noisy features and unreliable features that might overfit to the training data. To address this issue, we introduce the L1 norm from lasso technique (Tibshirani, 1996) to regularize the above likelihood function. Thus, instead of maximizing the likelihood, we can minimize the loss function of the negative log-likelihood with a linear penalty: � � min − ` + λ1 ||W~ ||(7) where λ1 is the regularization coefficient. The benefit of L1 penalty in a discriminative model is similar to the double exponential distribution of the sparse priors in generative models (Eisenstein et al., 2011a): they both push the weights of many noisy features into zeros, revealing only the important features. However, since the L1 penalty can introduce discontinuities to the original c</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>R. Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological).</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Y Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL</booktitle>
<contexts>
<context position="19124" citStr="Wang and Hirschberg, 2011" startWordPosition="3127" endWordPosition="3130">ce-feature matrix, I = P ��j, and assume that each feature j (e.g. unigram in our task) is a random variable that has a multinomial distribution over the instances in the training set. Then, we compare each pair of features, and calculate the interfeature distance matrix Dist with Euclidean distance as a measure, and use the k-nearest neighbors (kNN) method (Beyer et al., 1999) to select the top neighbors of each feature. 2. Derive the affinity matrix A. To assign the weight on the edge E(p,q) for each connected nodes (the kNN of V in Dist), we use the cosine similarity cosine(Vp, Vq) metric (Wang and Hirschberg, 2011). 3. Generate the degree matrix D and Laplacian matrix L. As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D − A. To calculate the above matrices in an efficient manner, we partition the covariate into blocks, and process each block in parallel (Chen et al., 2011). An intuitive example of the graph G, its associated affinity matrix A, and Laplacian matrix L, is shown in Figure 1. Parameter Estimation: Regarding the optimization of objective function in (12-13), a notable problem is that the</context>
</contexts>
<marker>Wang, Hirschberg, 2011</marker>
<rawString>W. Y. Wang and J. Hirschberg. 2011. Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning. In Proceedings of the 12th annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL 2011). W. Y. Wang, S. Finkelstein, A. Ogan, A. W. Black, and J. Cassell. 2012a. “love ya, jerkface”: using sparse log-linear models to build positive (and impolite) relationships with teens. In Proceedings of the 13th annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Y Wang</author>
<author>E Mayfield</author>
<author>S Naidu</author>
<author>J Dittmar</author>
</authors>
<title>Historical analysis of legal opinions with a sparse mixed-effects latent variable model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="8055" citStr="Wang et al. (2012" startWordPosition="1245" endWordPosition="1248">igns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the Li sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for 1326 structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative </context>
</contexts>
<marker>Wang, Mayfield, Naidu, Dittmar, 2012</marker>
<rawString>W. Y. Wang, E. Mayfield, S. Naidu, and J. Dittmar. 2012b. Historical analysis of legal opinions with a sparse mixed-effects latent variable model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Q Weinberger</author>
<author>F Sha</author>
<author>Q Zhu</author>
<author>L K Saul</author>
</authors>
<title>Graph laplacian regularization for large-scale semidefinite programming. Advances in neural information processing systems (NIPS).</title>
<date>2007</date>
<contexts>
<context position="9257" citStr="Weinberger et al. (2007)" startWordPosition="1429" endWordPosition="1433">tional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of not explicitly modeling the complex dependency structure and interaction of local features from a global perspective. To solve this problem, graph methods seem to be a good solution, because they are simple, generalizable, and are often used to model such complex dependency structures (Cohen, 2012). However, combining the sparse modeling and spectral graphical modeling approaches in a principled way is challenging. Belkin et al. (2006) and Weinberger et al. (2007) are among the first to investigate graph Laplacians as a manifold regularization method for statistical learning. Recently, Gao et al. (2012) propose a histogram intersection based kNN method to construct a Laplacian matrix for a least-square sparse coding problem in image processing. Unfortunately, this method might be too specific to the SIFT-based image coding tasks, thus might not be applicable to the text classification problem that utilizes n-gram lexical features. 3 Datasets We collected Yelp reviews from 1,860 Starbucks, Dunkin’ Donuts3, and other coffee shops all over the Midwest and</context>
</contexts>
<marker>Weinberger, Sha, Zhu, Saul, 2007</marker>
<rawString>K. Q. Weinberger, F. Sha, Q. Zhu, and L. K. Saul. 2007. Graph laplacian regularization for large-scale semidefinite programming. Advances in neural information processing systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zou</author>
<author>T Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology).</journal>
<contexts>
<context position="15857" citStr="Zou and Hastie, 2005" startWordPosition="2540" endWordPosition="2543">tive model is similar to the double exponential distribution of the sparse priors in generative models (Eisenstein et al., 2011a): they both push the weights of many noisy features into zeros, revealing only the important features. However, since the L1 penalty can introduce discontinuities to the original convex function, we can also consider an alternative non-sparse ridge estimator (Le Cessie and Van Houwelingen, 1992) with log loss and L2 norm, and has the convex property: � − ` + λ2 ||W~ ||2� min (8) Another option that balances the sparsity and smoothness would be the elastic net model (Zou and Hastie, 2005) that uses the composite penalty: �− ` + λ1 ||W~ ||+ λ2 ||W~ ||2� min (9) 4.3 The Laplacian Structured Sparsity Model So far, none of the above element-wise penalty models in the previous subsection takes into account the �θ(2) �θ(k) y ,..., y . (1) Pr(Y |�θ(i) ~Xt) (2) y , ∂` ∂ W~ ∂k ∂ W~ � � ∂�θ j j − j ∂ θ j −θ j y 1 (5) W y 1 y 1328 dependency structure of the local features. Inspired by Gao et al.(2012), we group the local features that have similar distributions together. The intuition is that, for features that have very similar empirical distributions in the training set, their weights</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>H. Zou and T. Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
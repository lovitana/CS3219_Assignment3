<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000718">
<title confidence="0.531898">
Fish transporters and miracle homes:
How compositional distributional semantics can help NP parsing
</title>
<author confidence="0.979486">
Angelilii Lazaridou, Eva Maria Vecchi and Marco Baroni
</author>
<affiliation confidence="0.995482">
Center for Mind/Brain Sciences
University of Trento, Italy
</affiliation>
<email confidence="0.986855">
first.last@unitn.it
</email>
<sectionHeader confidence="0.997263" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999672">
In this work, we argue that measures that have
been shown to quantify the degree of semantic
plausibility of phrases, as obtained from their
compositionally-derived distributional seman-
tic representations, can resolve syntactic am-
biguities. We exploit this idea to choose the
correct parsing of NPs (e.g., (live fish) trans-
porter rather than live (fish transporter)). We
show that our plausibility cues outperform
a strong baseline and significantly improve
performance when used in combination with
state-of-the-art features.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999832388888889">
Live fish transporter: A transporter of live fish
or rather a fish transporter that is not dead?
While our intuition, based on the meaning of this
phrase, prefers the former interpretation, the Stan-
ford parser, which lacks semantic features, incor-
rectly predicts the latter as the correct parse.1 The
correct syntactic parsing of sentences is clearly
steered by semantic information (as formal syn-
tacticians have pointed out at least since Fillmore
(1968)), and consequently the semantic plausibil-
ity of alternative parses can provide crucial evidence
about their validity.
An emerging line of parsing research capitalizes
on the advances of compositional distributional se-
mantics (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Socher et al.,
2012). Information related to compositionally-
derived distributional representations of phrases is
</bodyText>
<footnote confidence="0.929325">
1http://nlp.stanford.edu:8080/parser/
index.jsp
</footnote>
<bodyText confidence="0.99972875">
integrated at various stages of the parsing process
to improve overall performance.2 We are aware of
two very recent studies exploiting the semantic in-
formation provided by distributional models to re-
solve syntactic ambiguity: Socher et al. (2013) and
Le et al. (2013).
Socher et al. (2013) present a recursive neural net-
work architecture which jointly learns semantic rep-
resentations and syntactic categories of phrases. By
annotating syntactic categories with their distribu-
tional representation, the method emulates lexical-
ized approaches (Collins, 2003) and captures sim-
ilarity more flexibly than solutions based on hard
clustering (Klein and Manning, 2003; Petrov et al.,
2006). Thus, their approach mainly aims at improv-
ing parsing by capturing a richer, data-driven cate-
gorial structure.
On the other hand, Le et al. (2013) work with the
output of the parser. Their hypothesis is that parses
that lead to less semantically plausible interpreta-
tions will be penalized by a reranker that looks at
the composed semantic representation of the parse.
Their method achieves an improvement of 0.2% in
F-score. However, as the authors also remark, be-
cause of their experimental setup, they cannot con-
clude that the improvement is truly due to the se-
mantic composition component, a crucial issue that
is deferred to further investigation.
This work aims at corroborating the hypothesis
that the semantic plausibility of a phrase can in-
deed determine its correct parsing. We develop a
system based on simple and intuitive measures, ex-
</bodyText>
<footnote confidence="0.96829225">
2Distributional representations approximate word and
phrase meaning by vectors that record the contexts in which
they are likely to appear in corpora; for a review see, e.g., Tur-
ney and Pantel (2010).
</footnote>
<page confidence="0.915196">
1908
</page>
<note confidence="0.538884">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913,
</note>
<page confidence="0.27263">
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</page>
<figure confidence="0.9821817">
Type of NP # Example
A (N N) 1296 local phone company
343
164
424
2227
(A N) N crude oil sector
N (N N) miracle home run
(N N) N blood pressure medicine
Total -
</figure>
<tableCaption confidence="0.972873">
Table 1: NP dataset
</tableCaption>
<bodyText confidence="0.994202789473684">
tracted from the compositional distributional repre-
sentations of phrases, that have been shown to corre-
late with semantic plausibility (Vecchi et al., 2011).
We develop a controlled experimental setup, fo-
cusing on a single syntactic category, that is, noun
phrases (NP), where our task can be formalized as
(left or right) bracketing. Unlike previous work,
we compare our compositional semantic component
against features based on n-gram statistics, which
can arguably also capture some semantic informa-
tion in terms of frequent occurrences of meaningful
phrases. Inspired by previous literature demonstrat-
ing the power of metrics based on Pointwise Mu-
tual Information (PMI) in NP bracketing (Nakov and
Hearst, 2005; Pitler et al., 2010; Vadas and Curran,
2011), we test an approach exploiting PMI features,
and show that plausibility features relying on com-
posed representations can significantly boost accu-
racy over PMI.
</bodyText>
<sectionHeader confidence="0.992086" genericHeader="introduction">
2 Setup
</sectionHeader>
<bodyText confidence="0.999967642857143">
Noun phrase dataset To construct our dataset,
we used the Penn TreeBank (Marcus et al., 1993),
which we enriched with the annotation provided by
Vadas and Curran (2007a), since the original tree-
bank does not distinguish different structures inside
the NPs and always marks them as right bracketed,
e.g., local (phone company) but also blood (pressure
medicine). We focus on NPs formed by three ele-
ments, where the first can be an adjective (A) or a
noun (N), the other two are nouns. Table 1 summa-
rizes the characteristics of the dataset.3
Distributional semantic space As our source cor-
pus we use the concatenation of ukWaC, the English
Wikipedia (2009 dump) and the BNC, with a total of
</bodyText>
<footnote confidence="0.9097735">
3The dataset is available from: http://clic.cimec.
unitn.it/composes
</footnote>
<bodyText confidence="0.999733028571429">
about 2.8 billion tokens.4 We collect co-occurrence
statistics for the top 8K Ns and 4K As, plus any
other word from our NP dataset that was below this
rank. Our context elements are composed of the top
10K content words (adjectives, adverbs, nouns and
verbs). We use a standard bag-of-words approach,
counting within-sentence collocates for every target
word. We apply (non-negative) Pointwise Mutual
Information as weighting scheme and dimensional-
ity reduction using Non-negative Matrix Factoriza-
tion, setting the number of reduced-space dimen-
sions to 300.5
Composition functions We experiment with vari-
ous composition functions, chosen among those sen-
sitive to internal structure (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010),
namely dilation (dil), weighted additive (wadd), lex-
ical function (lexfunc) and full additive (fulladd).6
For model implementation and (unsupervised) es-
timation, we rely on the freely available DISSECT
toolkit (Dinu et al., 2013).7 For all methods, vectors
were normalized before composing, both in training
and in generation. Table 2 presents a summary de-
scription of the composition methods we used.
Following previous literature (Mitchell and Lap-
ata, 2010), and the general intuition that adjectival
modification is quite a different process from noun
combination (Gagn´e and Spalding, 2009; McNally,
2013), we learn different parameters for noun-noun
(NN) and adjective-noun (AN) phrases. As an ex-
ample of the learned parameters, for the wadd model
the ratio of parameters w1 and w2 is 1:2 for ANs,
whereas for NNs it is almost 1:1, confirming the in-
tuition that a non-head noun plays a stronger role in
composition than an adjective modifier.
</bodyText>
<footnote confidence="0.989482307692308">
4http://wacky.sslmit.unibo.it,http://en.
wikipedia.org,http://www.natcorp.ox.ac.uk
5For tuning the parameters of the semantic space, we com-
puted the correlation of cosines produced with a variety of pa-
rameter settings (SVD/NMF/no reduction, PMI/Local MI/raw
counts/log transform, 150 to 300 dimensions in steps of 50) with
the word pair similarity ratings in the MEN dataset: http:
//clic.cimec.unitn.it/˜elia.bruni/MEN
6We do not consider the popular multiplicative model, as it
produces identical representations for NPs irrespective of their
internal structure.
7http://clic.cimec.unitn.it/composes/
toolkit/
</footnote>
<page confidence="0.991605">
1909
</page>
<subsectionHeader confidence="0.412779">
Model Composition function Parameters
</subsectionHeader>
<equation confidence="0.71372675">
wadd w1~u + w2~v w1, w2
dil ||~u||22~v + (λ − 1)(~u,~v�~u λ
fulladd W1~u + W2~v W1, W2 E Rm&amp;quot;m
lexfunc Au~v Au E Rm&amp;quot;m
</equation>
<tableCaption confidence="0.988193">
Table 2: Composition functions of inputs (u, v).
</tableCaption>
<bodyText confidence="0.967991903225806">
Recursive composition In this study we also ex-
periment with recursive composition; to the best
of our knowledge, this is the first time that these
composition functions have been explicitly used in
this manner. For example, given the left brack-
eted NP (blood pressure) medicine, we want to
obtain its compositional semantic representation,
�
blood pressure medicine. First, basic composition
is applied, in which −−−�
blood and −−−−−−�
pressure are com-
bined with one of the composition functions. Fol-
lowing that, we apply recursive composition; the
−−−−−−−−−−�
output of basic composition, i.e., blood pressure,
is fed to the function again to be composed with the
representation of −−−−−−�
medicine.
The latter step is straightforward for all com-
position functions except lexfunc applied to left-
bracketed NPs, where the first step should return a
matrix representing the left constituent (blood pres-
sure in the running example). To cope with this nui-
sance, we apply the lexfunc method to basic compo-
sition only, while recursive representations are de-
−−−−−−−−−−�
rived by summing (e.g., blood pressure is obtained
by multiplying the blood matrix by the pressure vec-
−−−−−−�
tor, and it is then summed to medicine).
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.98452375">
Semantic plausibility measures We use mea-
sures of semantic plausibility computed on com-
posed semantic representations introduced by Vec-
chi et al. (2011). The rationale is that the correct
(wrong) bracketing will lead to semantically more
(less) plausible phrases. Thus, a measure able to dis-
criminate semantically plausible from implausible
phrases should also indicate the most likely parse.
Considering, for example, the alternative parses of
miracle home run, we observe that home run is
a more semantically plausible phrase than miracle
home. Furthermore, we might often refer to a base-
ball player’s miracle home run, but we doubt that
even a miracle home can run! Given the com-
posed representation of an AN (or NN), Vecchi et
al. (2011) define the following measures:
</bodyText>
<listItem confidence="0.9982704">
• Density, quantified as the average cosine of a
phrase with its (top 10) nearest neighbors, cap-
tures the intuition that a deviant phrase should
be isolated in the semantic space.
• Cosine of phrase and head N aims to capture
the fact that the meaning of a deviant AN (or
NN) will tend to diverge from the meaning of
the head noun.
• Vector length should capture anomalous vec-
tors.
</listItem>
<bodyText confidence="0.999577958333334">
Since length, as already observed by Vecchi et al.,
is strongly affected by independent factors such as
input vector normalization and the estimation pro-
cedure, we introduce entropy as a measure of vec-
tor quality. The intuition is that meaningless vec-
tors, whose dimensions contain mostly noise, should
have high entropy.
NP Parsing as Classification Parsing NPs con-
sisting of three elements can be treated as bi-
nary classification; given blood pressure medicine,
we predict whether it is left- ((blood pres-
sure) medicine) or right-bracketed (blood (pressure
medicine)).
We conduct experiments using an SVM with Ra-
dial Basis Function kernel as implemented in the
scikit-learn toolkit.8 Our dataset is split into 10 folds
in which the ratio between the two classes is kept
constant. We tune the SVM complexity parameter
C on the first fold and we report accuracy results on
the remaining nine folds after cross-validation.
Features Given a composition function f, we de-
fine the following feature sets, illustrated with the
usual blood pressure medicine example, which are
used to build different classifiers:
</bodyText>
<listItem confidence="0.965112375">
• fbasic consists of the semantic plausibility
measures described above computed for the
two-word phrases resulting from alternative
bracketings, i.e., 3 measures for each bracket-
ing, evaluated on blood pressure and pressure
medicine respectively, for a total of 6 features.
• frec contains 6 features computed on the vec-
tors resulting from the recursive compositions
</listItem>
<footnote confidence="0.966032">
8http://scikit-learn.org/
</footnote>
<page confidence="0.987491">
1910
</page>
<subsectionHeader confidence="0.938355">
Features Accuracy
</subsectionHeader>
<bodyText confidence="0.70022775">
right
pos
(blood pressure) medicine and blood (pressure
medicine).
</bodyText>
<listItem confidence="0.997089333333333">
• fplausibility concatenates fbasic and frec.
• pmi contains the PMI scores extracted from
our corpus for blood pressure and pressure
medicine.9
• pmi + fplausibility concatenates pmi and
fplausibility.
</listItem>
<bodyText confidence="0.8159946">
Baseline Model Given the skewed bracketing dis-
tribution in our dataset, we implement the following
majority baselines: a) right classifies all phrases
as right-bracketed; b) pos classifies NNN as left-
bracketed (Lauer, 1995), ANN as right-bracketed.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.987111875">
Table 3 omits results for dil and fulladd since they
were outperformed by the right baseline. That
wadd- and lexfunc-based plausibility features per-
form well above this baseline is encouraging, since
it represents the typical default behaviour of parsers
for NPs, although note that these features perform
comparably to the pos baseline, which would be
quite simple to embed in a parser (for English, at
least). For both models, using both basic and recur-
sive features leads to a boost in performance over
basic features alone. Note that recursive features
(frec) achieve at least equal or better performance
than basic ones (fbasic). We expect indeed that in
many cases the asymmetry in plausibility will be
9Several approaches to computing PMI for these purposes
have been proposed in the literature including the dependency
model (Lauer, 1995) and the adjacency model (Marcus, 1980).
We implement the latter since it has been shown to perform
better (Vadas and Curran, 2007b) on NPs extracted from Penn
TreeBank.
sharper when considering the whole NP rather than
its sub-parts; a pressure medicine is still a conceiv-
able concept, but blood (pressure medicine) makes
no sense whatsoever. Finally, wadd outperforms
both the more informative baseline pos and lexfunc.
The difference between wadd and lexfunc is signif-
icant (p &lt; 0.05)10 only when they are trained with
recursive composition features, probably due to our
suboptimal adaptation of the latter to recursive com-
position (see Section 2).
The pmi approach outperforms the best
plausibility-based feature set waddplausibility.
However, the two make only a small proportion of
common errors (29% of the total waddplausibility
errors, 32% for pmi), suggesting that they are com-
plementary. Indeed the pmi + waddplausibility
combination significantly outperforms pmi alone
(p &lt; 0.001), indicating that plausibility features
can improve NP bracketing on top of the pow-
erful PMI-based approach. The same effect can
also be observed in the combination of pmi +
lexfuncplausibility, which again significantly
outperforms pmi alone (p &lt; 0.05). This behaviour
further suggests that the different types of errors are
not a result of the parameters or type of composi-
tion applied, but rather highlights fundamental dif-
ferences in the kind of information that PMI and
composition models are able to capture.
One hypothesis is that compositional models are
more robust for low-frequency NPs, for which
PMI estimates will be less accurate; results on
those low-frequency trigrams only (20% of the NP
dataset, operationalized as those consisting of bi-
grams with frequency &lt; 100) revealed indeed that
waddplausibility performed 8.1% better in terms
of accuracy than pmi.
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998945">
Our pilot study showed that semantic plausibility,
as measured on compositional distributional repre-
sentations, can improve syntactic parsing of NPs.
Our results further suggest that state-of-the-art PMI
features and the ones extracted from compositional
representations are complementary, and thus, when
combined, can lead to significantly better results.
Besides paving the way to a more general integration
</bodyText>
<figure confidence="0.953755166666666">
10Significance values are based on t-tests.
65.6
77.3
lexfuncbasic 74.6
lexfuncrec 74.0
lexfuncplausibility 76.2
waddbasic 75.9
waddrec 78.2
waddplausibility 78.7
pmi 81.2
pmi+lexfuncplausibility 82.9
pmi+waddplausibility 85.6
</figure>
<tableCaption confidence="0.985607">
Table 3: Evaluation of feature sets from Section 3
</tableCaption>
<page confidence="0.98851">
1911
</page>
<bodyText confidence="0.999913833333333">
of compositional distributional semantics in syntac-
tic parsing, the proposed methodology provides a
new way to evaluate composition functions.
The relatively simple-minded wadd approach out-
performed more complex models such as lexfunc.
We plan to experiment next with more linguistically
motivated ways to adapt the latter to recursive com-
position, including hybrid methods where ANs and
NNs are treated differently. We would also like
to consider more sophisticated semantic plausibility
measures (e.g., supervised ones), and apply them to
other ambiguous syntactic constructions.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988836">
We thank Georgiana Dinu and German Kruszewski
for helpful discussions and the reviewers for use-
ful feedback. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.998889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99935213580247">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589–637.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Composi-
tion Toolkit. In Proceedings of the System Demonstra-
tions of ACL 2013, Sofia, Bulgaria.
Charles Fillmore. 1968. The case for case. In Emmon
Bach and Robert Harms, editors, Universals in Lin-
guistic Theory, pages 1–89. Holt, Rinehart and Win-
ston, New York.
Christina Gagn´e and Thomas Spalding. 2009. Con-
stituent integration during the processing of compound
words: Does it involve the use of relational structures?
Journal of Memory and Language, 60:20–35.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37, Up-
psala, Sweden.
Dan Klein and Christopher D Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423–430. Association for Computational Linguistics.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, pages 47–54, Cambridge, MA.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings of
the ACL 2013 Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313–330.
Mitchell P Marcus. 1980. Theory of syntactic recogni-
tion for natural languages. MIT press.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Preslav Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17–24, Stroudsburg, PA, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433–440, Stroudsburg, PA, USA.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of the
COLING, pages 886–894, Beijing, China.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Korea.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Proceedings of ACL, Sofia, Bul-
garia.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.
David Vadas and James Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL, pages 240–247, Prague, Czech Republic.
David Vadas and James R Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the PACLING, pages 104–112.
David Vadas and James R. Curran. 2011. Parsing
noun phrases in the penn treebank. Comput. Linguist.,
37(4):753–809.
</reference>
<page confidence="0.903023">
1912
</page>
<reference confidence="0.992289166666667">
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1–9, Portland,
OR.
</reference>
<page confidence="0.983976">
1913
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955327">
<title confidence="0.9860315">Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing</title>
<author confidence="0.998408">Eva Maria Vecchi Lazaridou</author>
<affiliation confidence="0.9993835">Center for Mind/Brain University of Trento,</affiliation>
<email confidence="0.999321">first.last@unitn.it</email>
<abstract confidence="0.998864769230769">In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the parsing of NPs (e.g., transthan We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1494" citStr="Baroni and Zamparelli, 2010" startWordPosition="212" endWordPosition="215">t is not dead? While our intuition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntac</context>
<context position="6217" citStr="Baroni and Zamparelli, 2010" startWordPosition="946" endWordPosition="949"> 4K As, plus any other word from our NP dataset that was below this rank. Our context elements are composed of the top 10K content words (adjectives, adverbs, nouns and verbs). We use a standard bag-of-words approach, counting within-sentence collocates for every target word. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300.5 Composition functions We experiment with various composition functions, chosen among those sensitive to internal structure (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>Computational linguistics,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="2256" citStr="Collins, 2003" startWordPosition="318" endWordPosition="319">rases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al., 2006). Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. On the other hand, Le et al. (2013) work with the output of the parser. Their hypothesis is that parses that lead to less semantically plausible interpretations will be penalized by a reranker that looks at the composed semantic representation of the parse. Their method achieves an improvement of 0.2% in F-score. However, as the authors also remark, because of</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT: DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the System Demonstrations of ACL 2013,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="6491" citStr="Dinu et al., 2013" startWordPosition="985" endWordPosition="988">ly (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300.5 Composition functions We experiment with various composition functions, chosen among those sensitive to internal structure (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Gagn´e and Spalding, 2009; McNally, 2013), we learn different parameters for noun-noun (NN) and adjective-noun (AN) phrases. As an example of the learned parameters, for the wadd model the ratio of parameters w1 and w2 is 1:2 for ANs, whereas for NNs it is almo</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT: DIStributional SEmantics Composition Toolkit. In Proceedings of the System Demonstrations of ACL 2013, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The case for case.</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory,</booktitle>
<pages>1--89</pages>
<editor>In Emmon Bach and Robert Harms, editors,</editor>
<location>Holt, Rinehart and Winston, New York.</location>
<contexts>
<context position="1240" citStr="Fillmore (1968)" startWordPosition="179" endWordPosition="180">ur plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features. 1 Introduction Live fish transporter: A transporter of live fish or rather a fish transporter that is not dead? While our intuition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the s</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Charles Fillmore. 1968. The case for case. In Emmon Bach and Robert Harms, editors, Universals in Linguistic Theory, pages 1–89. Holt, Rinehart and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Gagn´e</author>
<author>Thomas Spalding</author>
</authors>
<title>Constituent integration during the processing of compound words: Does it involve the use of relational structures?</title>
<date>2009</date>
<journal>Journal of Memory and Language,</journal>
<pages>60--20</pages>
<marker>Gagn´e, Spalding, 2009</marker>
<rawString>Christina Gagn´e and Thomas Spalding. 2009. Constituent integration during the processing of compound words: Does it involve the use of relational structures? Journal of Memory and Language, 60:20–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="1509" citStr="Guevara, 2010" startWordPosition="216" endWordPosition="217">ition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories </context>
<context position="6232" citStr="Guevara, 2010" startWordPosition="950" endWordPosition="951">rom our NP dataset that was below this rank. Our context elements are composed of the top 10K content words (adjectives, adverbs, nouns and verbs). We use a standard bag-of-words approach, counting within-sentence collocates for every target word. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300.5 Composition functions We experiment with various composition functions, chosen among those sensitive to internal structure (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Ga</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2359" citStr="Klein and Manning, 2003" startWordPosition="332" endWordPosition="335">arsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al., 2006). Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. On the other hand, Le et al. (2013) work with the output of the parser. Their hypothesis is that parses that lead to less semantically plausible interpretations will be penalized by a reranker that looks at the composed semantic representation of the parse. Their method achieves an improvement of 0.2% in F-score. However, as the authors also remark, because of their experimental setup, they cannot conclude that the improvement is truly due to the semantic compo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: Some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>47--54</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="12432" citStr="Lauer, 1995" startWordPosition="1909" endWordPosition="1910">res computed on the vectors resulting from the recursive compositions 8http://scikit-learn.org/ 1910 Features Accuracy right pos (blood pressure) medicine and blood (pressure medicine). • fplausibility concatenates fbasic and frec. • pmi contains the PMI scores extracted from our corpus for blood pressure and pressure medicine.9 • pmi + fplausibility concatenates pmi and fplausibility. Baseline Model Given the skewed bracketing distribution in our dataset, we implement the following majority baselines: a) right classifies all phrases as right-bracketed; b) pos classifies NNN as leftbracketed (Lauer, 1995), ANN as right-bracketed. 4 Results and Discussion Table 3 omits results for dil and fulladd since they were outperformed by the right baseline. That wadd- and lexfunc-based plausibility features perform well above this baseline is encouraging, since it represents the typical default behaviour of parsers for NPs, although note that these features perform comparably to the pos baseline, which would be quite simple to embed in a parser (for English, at least). For both models, using both basic and recursive features leads to a boost in performance over basic features alone. Note that recursive f</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the noun compound: Some empirical results. In Proceedings of the Annual Meeting on Association for Computational Linguistics, pages 47–54, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le</author>
<author>Willem Zuidema</author>
<author>Remko Scha</author>
</authors>
<title>Learning from errors: Using vector-based compositional semantics for parse reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1967" citStr="Le et al. (2013)" startWordPosition="277" endWordPosition="280">lidity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al., 2006). Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. On the other hand, Le et al. (2013) work with the output of the parser. T</context>
</contexts>
<marker>Le, Zuidema, Scha, 2013</marker>
<rawString>Phong Le, Willem Zuidema, and Remko Scha. 2013. Learning from errors: Using vector-based compositional semantics for parse reranking. In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="4841" citStr="Marcus et al., 1993" startWordPosition="731" endWordPosition="734"> against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. Table 1 summarizes the characteristics of the dataset.3 Distributional semantic space As our source corpus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3The</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>Theory of syntactic recognition for natural languages.</title>
<date>1980</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="13368" citStr="Marcus, 1980" startWordPosition="2057" endWordPosition="2058"> note that these features perform comparably to the pos baseline, which would be quite simple to embed in a parser (for English, at least). For both models, using both basic and recursive features leads to a boost in performance over basic features alone. Note that recursive features (frec) achieve at least equal or better performance than basic ones (fbasic). We expect indeed that in many cases the asymmetry in plausibility will be 9Several approaches to computing PMI for these purposes have been proposed in the literature including the dependency model (Lauer, 1995) and the adjacency model (Marcus, 1980). We implement the latter since it has been shown to perform better (Vadas and Curran, 2007b) on NPs extracted from Penn TreeBank. sharper when considering the whole NP rather than its sub-parts; a pressure medicine is still a conceivable concept, but blood (pressure medicine) makes no sense whatsoever. Finally, wadd outperforms both the more informative baseline pos and lexfunc. The difference between wadd and lexfunc is significant (p &lt; 0.05)10 only when they are trained with recursive composition features, probably due to our suboptimal adaptation of the latter to recursive composition (see</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Mitchell P Marcus. 1980. Theory of syntactic recognition for natural languages. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise McNally</author>
</authors>
<title>Modification. In Maria Aloni and Paul Dekker, editors, Cambridge Handbook of Semantics.</title>
<date>2013</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="6871" citStr="McNally, 2013" startWordPosition="1043" endWordPosition="1044">10), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Gagn´e and Spalding, 2009; McNally, 2013), we learn different parameters for noun-noun (NN) and adjective-noun (AN) phrases. As an example of the learned parameters, for the wadd model the ratio of parameters w1 and w2 is 1:2 for ANs, whereas for NNs it is almost 1:1, confirming the intuition that a non-head noun plays a stronger role in composition than an adjective modifier. 4http://wacky.sslmit.unibo.it,http://en. wikipedia.org,http://www.natcorp.ox.ac.uk 5For tuning the parameters of the semantic space, we computed the correlation of cosines produced with a variety of parameter settings (SVD/NMF/no reduction, PMI/Local MI/raw cou</context>
</contexts>
<marker>McNally, 2013</marker>
<rawString>Louise McNally. 2013. Modification. In Maria Aloni and Paul Dekker, editors, Cambridge Handbook of Semantics. Cambridge University Press, Cambridge, UK. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1536" citStr="Mitchell and Lapata, 2010" startWordPosition="218" endWordPosition="221"> the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating s</context>
<context position="6260" citStr="Mitchell and Lapata, 2010" startWordPosition="952" endWordPosition="955">set that was below this rank. Our context elements are composed of the top 10K content words (adjectives, adverbs, nouns and verbs). We use a standard bag-of-words approach, counting within-sentence collocates for every target word. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300.5 Composition functions We experiment with various composition functions, chosen among those sensitive to internal structure (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Gagn´e and Spalding, 2009; McN</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>17--24</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4532" citStr="Nakov and Hearst, 2005" startWordPosition="681" endWordPosition="684">n to correlate with semantic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of CoNLL, pages 17–24, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL,</booktitle>
<pages>433--440</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2381" citStr="Petrov et al., 2006" startWordPosition="336" endWordPosition="339"> overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al., 2006). Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. On the other hand, Le et al. (2013) work with the output of the parser. Their hypothesis is that parses that lead to less semantically plausible interpretations will be penalized by a reranker that looks at the composed semantic representation of the parse. Their method achieves an improvement of 0.2% in F-score. However, as the authors also remark, because of their experimental setup, they cannot conclude that the improvement is truly due to the semantic composition component, a cr</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLINGACL, pages 433–440, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
</authors>
<title>Using web-scale n-grams to improve base NP parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING,</booktitle>
<pages>886--894</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4553" citStr="Pitler et al., 2010" startWordPosition="685" endWordPosition="688">ntic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, w</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Church. 2010. Using web-scale n-grams to improve base NP parsing performance. In Proceedings of the COLING, pages 886–894, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1558" citStr="Socher et al., 2012" startWordPosition="222" endWordPosition="225">, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories wi</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1946" citStr="Socher et al. (2013)" startWordPosition="272" endWordPosition="275">l evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories with their distributional representation, the method emulates lexicalized approaches (Collins, 2003) and captures similarity more flexibly than solutions based on hard clustering (Klein and Manning, 2003; Petrov et al., 2006). Thus, their approach mainly aims at improving parsing by capturing a richer, data-driven categorial structure. On the other hand, Le et al. (2013) work with the ou</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="3429" citStr="Turney and Pantel (2010)" startWordPosition="506" endWordPosition="510">re. However, as the authors also remark, because of their experimental setup, they cannot conclude that the improvement is truly due to the semantic composition component, a crucial issue that is deferred to further investigation. This work aims at corroborating the hypothesis that the semantic plausibility of a phrase can indeed determine its correct parsing. We develop a system based on simple and intuitive measures, ex2Distributional representations approximate word and phrase meaning by vectors that record the contexts in which they are likely to appear in corpora; for a review see, e.g., Turney and Pantel (2010). 1908 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Type of NP # Example A (N N) 1296 local phone company 343 164 424 2227 (A N) N crude oil sector N (N N) miracle home run (N N) N blood pressure medicine Total - Table 1: NP dataset tracted from the compositional distributional representations of phrases, that have been shown to correlate with semantic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing o</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>240--247</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4915" citStr="Vadas and Curran (2007" startWordPosition="743" endWordPosition="746">pture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. Table 1 summarizes the characteristics of the dataset.3 Distributional semantic space As our source corpus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3The dataset is available from: http://clic.cimec. unitn.it/composes about 2.8</context>
<context position="13459" citStr="Vadas and Curran, 2007" startWordPosition="2071" endWordPosition="2074">uite simple to embed in a parser (for English, at least). For both models, using both basic and recursive features leads to a boost in performance over basic features alone. Note that recursive features (frec) achieve at least equal or better performance than basic ones (fbasic). We expect indeed that in many cases the asymmetry in plausibility will be 9Several approaches to computing PMI for these purposes have been proposed in the literature including the dependency model (Lauer, 1995) and the adjacency model (Marcus, 1980). We implement the latter since it has been shown to perform better (Vadas and Curran, 2007b) on NPs extracted from Penn TreeBank. sharper when considering the whole NP rather than its sub-parts; a pressure medicine is still a conceivable concept, but blood (pressure medicine) makes no sense whatsoever. Finally, wadd outperforms both the more informative baseline pos and lexfunc. The difference between wadd and lexfunc is significant (p &lt; 0.05)10 only when they are trained with recursive composition features, probably due to our suboptimal adaptation of the latter to recursive composition (see Section 2). The pmi approach outperforms the best plausibility-based feature set waddplaus</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007a. Adding noun phrase structure to the Penn Treebank. In Proceedings of ACL, pages 240–247, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Large-scale supervised models for noun phrase bracketing.</title>
<date>2007</date>
<booktitle>In Proceedings of the PACLING,</booktitle>
<pages>104--112</pages>
<contexts>
<context position="4915" citStr="Vadas and Curran (2007" startWordPosition="743" endWordPosition="746">pture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. Table 1 summarizes the characteristics of the dataset.3 Distributional semantic space As our source corpus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3The dataset is available from: http://clic.cimec. unitn.it/composes about 2.8</context>
<context position="13459" citStr="Vadas and Curran, 2007" startWordPosition="2071" endWordPosition="2074">uite simple to embed in a parser (for English, at least). For both models, using both basic and recursive features leads to a boost in performance over basic features alone. Note that recursive features (frec) achieve at least equal or better performance than basic ones (fbasic). We expect indeed that in many cases the asymmetry in plausibility will be 9Several approaches to computing PMI for these purposes have been proposed in the literature including the dependency model (Lauer, 1995) and the adjacency model (Marcus, 1980). We implement the latter since it has been shown to perform better (Vadas and Curran, 2007b) on NPs extracted from Penn TreeBank. sharper when considering the whole NP rather than its sub-parts; a pressure medicine is still a conceivable concept, but blood (pressure medicine) makes no sense whatsoever. Finally, wadd outperforms both the more informative baseline pos and lexfunc. The difference between wadd and lexfunc is significant (p &lt; 0.05)10 only when they are trained with recursive composition features, probably due to our suboptimal adaptation of the latter to recursive composition (see Section 2). The pmi approach outperforms the best plausibility-based feature set waddplaus</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R Curran. 2007b. Large-scale supervised models for noun phrase bracketing. In Proceedings of the PACLING, pages 104–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrases in the penn treebank.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="4578" citStr="Vadas and Curran, 2011" startWordPosition="689" endWordPosition="692">cchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an </context>
</contexts>
<marker>Vadas, Curran, 2011</marker>
<rawString>David Vadas and James R. Curran. 2011. Parsing noun phrases in the penn treebank. Comput. Linguist., 37(4):753–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>(Linear) maps of the impossible: Capturing semantic anomalies in distributional space.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>1--9</pages>
<location>Portland, OR.</location>
<contexts>
<context position="3973" citStr="Vecchi et al., 2011" startWordPosition="596" endWordPosition="599">ikely to appear in corpora; for a review see, e.g., Turney and Pantel (2010). 1908 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Type of NP # Example A (N N) 1296 local phone company 343 164 424 2227 (A N) N crude oil sector N (N N) miracle home run (N N) N blood pressure medicine Total - Table 1: NP dataset tracted from the compositional distributional representations of phrases, that have been shown to correlate with semantic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, </context>
<context position="9411" citStr="Vecchi et al. (2011)" startWordPosition="1429" endWordPosition="1433">tions except lexfunc applied to leftbracketed NPs, where the first step should return a matrix representing the left constituent (blood pressure in the running example). To cope with this nuisance, we apply the lexfunc method to basic composition only, while recursive representations are de−−−−−−−−−−� rived by summing (e.g., blood pressure is obtained by multiplying the blood matrix by the pressure vec−−−−−−� tor, and it is then summed to medicine). 3 Experiments Semantic plausibility measures We use measures of semantic plausibility computed on composed semantic representations introduced by Vecchi et al. (2011). The rationale is that the correct (wrong) bracketing will lead to semantically more (less) plausible phrases. Thus, a measure able to discriminate semantically plausible from implausible phrases should also indicate the most likely parse. Considering, for example, the alternative parses of miracle home run, we observe that home run is a more semantically plausible phrase than miracle home. Furthermore, we might often refer to a baseball player’s miracle home run, but we doubt that even a miracle home can run! Given the composed representation of an AN (or NN), Vecchi et al. (2011) define the</context>
</contexts>
<marker>Vecchi, Baroni, Zamparelli, 2011</marker>
<rawString>Eva Maria Vecchi, Marco Baroni, and Roberto Zamparelli. 2011. (Linear) maps of the impossible: Capturing semantic anomalies in distributional space. In Proceedings of the ACL Workshop on Distributional Semantics and Compositionality, pages 1–9, Portland, OR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
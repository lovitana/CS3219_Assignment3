<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.950387">
Exploiting Zero Pronouns to Improve Chinese Coreference Resolution
</title>
<author confidence="0.998424">
Fang Kong
</author>
<affiliation confidence="0.9998355">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9768165">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998806">
dcskf@nus.edu.sg
</email>
<author confidence="0.959982">
Hwee Tou Ng
</author>
<affiliation confidence="0.999701">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.97682">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998997">
nght@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.996658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999359285714286">
Coreference resolution plays a critical role
in discourse analysis. This paper focuses
on exploiting zero pronouns to improve Chi-
nese coreference resolution. In particular, a
simplified semantic role labeling framework
is proposed to identify clauses and to detect
zero pronouns effectively, and two effective
methods (refining syntactic parser and refining
learning example generation) are employed to
exploit zero pronouns for Chinese coreference
resolution. Evaluation on the CoNLL-2012
shared task data set shows that zero pronouns
can significantly improve Chinese coreference
resolution.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975425">
As one of the most important tasks in discourse
analysis, coreference resolution aims to link a given
mention (i.e., entity or event) to its co-referring ex-
pression in a text and has been a focus of research in
natural language processing (NLP) for decades.
Over the last decade, various machine learning
techniques have been applied to coreference reso-
lution and have performed reasonably well (Soon
et al., 2001; Ng and Cardie, 2002; Fernandes et al.,
2012). Current techniques rely primarily on surface
level features such as string match, syntactic features
such as apposition, and shallow semantic features
such as number, gender, semantic class, etc.
Despite similarities between Chinese and English,
there are differences that have a significant impact
on coreference resolution. In this paper, we focus
on exploiting one of the key characteristics of Chi-
nese text, zero pronouns (ZPs), to improve Chinese
coreference resolution. In particular, a simplified se-
mantic role labeling (SRL) framework is proposed
to identify Chinese clauses and to detect zero pro-
nouns effectively, and two effective methods are em-
ployed to exploit zero pronouns for Chinese corefer-
ence resolution. Experimental results show the ef-
fectiveness of our approach in improving the perfor-
mance of Chinese coreference resolution. Our work
is novel in that it is the first work that incorporates
the use of zero pronouns to significantly improve
Chinese coreference resolution
The rest of this paper is organized as follows.
Section 2 describes our baseline Chinese corefer-
ence resolution system. Section 3 motivates how
the detection of zero pronouns can improve Chinese
coreference resolution, using an illustrating exam-
ple. Section 4 presents our approach to detect zero
pronouns. Section 5 proposes two methods to ex-
ploit zero pronouns to improve Chinese coreference
resolution, based on a corpus study and preliminary
experiments. Section 6 briefly outlines the related
work. Finally, we conclude our work in Section 7.
</bodyText>
<sectionHeader confidence="0.984815" genericHeader="introduction">
2 Chinese Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9994108">
According to Webber (1978), coreference resolu-
tion can be decomposed into two complementary
subtasks: (1) anaphoricity determination: decid-
ing whether a given noun phrase (NP) is anaphoric
or not; and (2) anaphora resolution: linking to-
gether multiple mentions of a given entity in the
world. Our Chinese coreference resolution system
also contains these two components. Using the train-
ing data set of CoNLL-2012 shared task, we first
train an anaphoricity classifier to determine whether
</bodyText>
<page confidence="0.959055">
278
</page>
<note confidence="0.733916">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 278–288,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999089333333333">
a mention is anaphoric or not, and then employ
an independently-trained coreference resolution sys-
tem to resolve those mentions which are classi-
fied as anaphoric. The lack of gender and number
makes both anaphoricity determination and corefer-
ence resolution in Chinese more difficult.
</bodyText>
<subsectionHeader confidence="0.986387">
2.1 Anaphoricity Determination
</subsectionHeader>
<bodyText confidence="0.9972915">
Since only the mentions that take part in coreference
chains are annotated in the CoNLL-2012 shared
task data set, we first generate a high-recall, low-
precision mention extraction module to extract as
many mentions as possible. The mention extrac-
tion module relies mainly on syntactic parse trees.
We extract all NP nodes, QP (quantifier phrase, i.e.,
complex amount/measure phrase) nodes, and all ter-
minals with part-of-speech tags PN (pronoun) and
NR (proper noun) in parse trees to form a mention
candidate set. Then, we employ some rules to re-
move unlikely mentions, e.g., those which contain
(1) measure words such as ‘-*/one year’ and
‘-Oji/one time’; (2) named entities whose cat-
egories are PERCENT, MONEY, QUANTITY, and
CARDINAL; (3) interrogative pronouns such as ‘
&apos;f+A/what’ and ‘ê?/where’.
After pruning, we employ a learning-based
method to train an independent classifier to deter-
mine whether the remaining mentions are anaphoric.
Table 1 lists all the features employed in our
anaphoricity determination system.
</bodyText>
<subsectionHeader confidence="0.99934">
2.2 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.994601375">
Our Chinese coreference resolution system adopts
the same learning-based model and the same set of
12 features as Soon et al. (2001). Considering the
special characteristics of conversation and web texts
(i.e., a large proportion of personal pronouns and the
organization of a text into several parts1) and prepar-
ing for dealing with zero pronouns, we add some
features shown in Table 2.2
</bodyText>
<footnote confidence="0.917367875">
1A text in the CoNLL-2012 data set is broken down into
different “parts”.
2AN denotes anaphor, CA denotes antecedent candidate, IP
denotes a simple clause, and CP denotes a clause headed by a
complementizer. For the feature ANPronounRanking, the rel-
ative ranking of a given pronoun is based on its semantic role
and surface position, and we assign the highest rank to zero pro-
nouns, similar to Kong et al. (2009).
</footnote>
<table confidence="0.989480333333333">
R P F
GS 76.32 87.14 81.37
Auto 64.87 78.42 71.00
</table>
<tableCaption confidence="0.750728">
Table 3: Performance of anaphoricity determination on
the CoNLL-2012 test set
</tableCaption>
<table confidence="0.9999571875">
R P F
Mention Detection 65.26 67.20 66.22
MUC 51.64 61.82 56.27
AM BCUBED 73.40 80.38 76.73
CEAF 53.16 45.66 49.13
Average 60.71
Mention Detection 82.01 69.58 75.29
MUC 76.21 66.18 70.84
GMB BCUBED 76.15 86.59 81.04
CEAF 59.75 50.52 54.75
Average 68.88
Mention Detection 79.80 100.00 88.77
MUC 80.86 85.48 83.11
GM BCUBED 73.66 91.94 81.79
CEAF 67.54 64.87 66.18
Average 77.02
</table>
<tableCaption confidence="0.9636845">
Table 4: Performance of our Chinese coreference resolu-
tion system on the CoNLL-2012 test set
</tableCaption>
<subsectionHeader confidence="0.966798">
2.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.99993405">
All experiments in this section are conducted on the
CoNLL-2012 shared task data set. The SVM-light
toolkit (Joachims, 1999) with radial basis kernel
and default learning parameters is employed in both
anaphoricity determination and coreference resolu-
tion.
Table 3 reports the performance of anaphoricity
determination on the CoNLL-2012 test set using
gold-standard parse trees (GS) and automatic parse
trees (Auto). All performance figures in this paper
are given in percentages. The results show that using
both gold parse trees and automatic parse trees, our
anaphoricity determination system achieves higher
precision than recall. In comparison with using gold
parse trees, precision decreases by about 9% and re-
call 11% on automatic parse trees.
Table 4 reports the performance of our Chinese
coreference resolution system on the CoNLL-2012
test set under three different experimental settings:
with automatic mentions (AM), with gold mention
</bodyText>
<page confidence="0.998404">
279
</page>
<figureCaption confidence="0.991480842105263">
Feature Description
NPType Type of the current mention (pronoun, demonstrative, proper NP).
NPNumber Number of the current mention (singular, plural).
NPGender Gender of the current mention (male, female).
IsHeadWord Whether the current mention is the same as its headword.
StrMatch Whether there is a string match between the current mention and another phrase in the
previous context.
AliasMatch Whether the current mention is a name alias or abbreviation of another phrase in the
previous context.
Appositive Whether the current mention and another phrase in the previous context are in an
appositive relation.
NestIn Whether another NP is nested in the current mention.
NestOut Whether the current mention is nested in another NP.
FirstNP Whether the current mention is the first NP of the sentence.
FrontDistance The number of words between the current mention and the nearest previous clause.
BackDistance The number of words between the current mention and the nearest following clause.
WordSense Whether the current mention and another phrase in the previous context have the same
word sense. Word sense annotation is provided in the CoNLL-2012 data set, based on
the IMS software (Zhong and Ng, 2010).
</figureCaption>
<tableCaption confidence="0.995112">
Table 1: Features employed in our anaphoricity determination system
</tableCaption>
<bodyText confidence="0.999408736842105">
Feature Description
AN/CAPronounType Whether the anaphor or the antecedent candidate is a zero pronoun, first per-
son, second person, third person, neutral pronoun, or others. In our corefer-
ence resolution system, a zero pronoun is viewed as a kind of special pro-
noun.
AN/CAGrammaticalRole Whether the anaphor or the antecedent candidate is a subject, object, or oth-
ers.
AN/CAOwnerClauseType Whether the anaphor or the antecedent candidate is in a matrix clause, an
independent clause, a subordinate clause, or none of the above.
AN/CARootPath Whether the path of nodes from the anaphor (or the antecedent candidate) to
the root of the parse tree contains NP, IP, CP, or VP.
ANPronounRanking Whether the anaphor is a pronoun and is ranked highest among the pronouns
(including zero pronouns) of the sentence.
AN/CAClosestNP Whether the antecedent candidate is the closest preceding NP of the anaphor.
AN/CAPartDistance This feature captures the distance (in parts) between the antecedent candidate
and the anaphor. If they are in the same part, the value is 0; if they are one
part apart, the value is 1; and so on.
AN/CASameSpeaker Whether the antecedent candidate and the anaphor appear in sentences spo-
ken by the same person.
</bodyText>
<tableCaption confidence="0.958158">
Table 2: Additional features employed in our Chinese coreference resolution system
</tableCaption>
<page confidence="0.995202">
280
</page>
<bodyText confidence="0.767001">
boundaries (GMB), and with gold mentions (GM).
From the results, we find that:
</bodyText>
<listItem confidence="0.992496428571429">
• Using automatic mentions, our system achieves
56.27, 76.73, and 49.13 in F-measure on MUC,
BCUBED, and CEAF evaluation metrics, re-
spectively.
• Using gold mention boundaries improves the
performance of our system by 14.57, 4.31, and
5.62 in F-measure, due to large gains in both
recall and precision. We also find that using
gold mention boundaries can boost the recall
of mention detection. As described above, our
anaphoricity determination model relies mainly
on the parser. Using gold mention boundaries
can improve the parser performance. Thus
our coreference resolution system can benefit
much from using gold mention boundaries (es-
pecially the recall).
• Employing gold mentions further boosts our
system significantly. In comparison with using
gold mention boundaries, the performance im-
provement is attributed more to an increase in
precision.
</listItem>
<bodyText confidence="0.999936875">
In comparison with the three best systems of
CoNLL-2012 in the Chinese closed track (shown in
Table 5), considering average F-measure, we find
that using automatic mentions, our system is only
inferior to that of Chen and Ng (2012); using gold
mention boundaries, our system achieves the best
performance; and using gold mentions, our system is
only a little worse than that of Chen and Ng (2012).
</bodyText>
<sectionHeader confidence="0.988362" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.998581533333333">
In order to analyze the impact of zero pronouns on
Chinese coreference resolution, we first use the re-
leased OntoNotes v5.0 data (i.e., the training and de-
velopment portions of the CoNLL-2012 shared task)
in a corpus study.
Statistics show that anaphoric zero pronouns ac-
count for 10.7% of the mentions in coreference
chains in the training data, while in the develop-
ment data, the proportion is 11.3%. The experi-
mental results of our Chinese coreference resolution
system (i.e., the baseline) show that using both gold
mention boundaries and gold mentions significantly
improves system performance, especially for recall,
largely due to improved parser performance. We
then analyze the impact of zero pronouns on Chi-
nese syntactic parsing. As a preliminary exploration,
we integrate Chinese zero pronouns into the Berke-
ley parser (Petrov et al., 2006), experimenting with
gold-standard or automatically determined zero pro-
nouns kept or stripped off (using gold-standard word
segmentation provided in the CoNLL-2012 data).
The results indicate that given gold-standard zero
pronouns, parsing performance improves by 1.8%
in F-measure. Using automatically determined zero
pronouns by our zero pronoun detector to be intro-
duced in Section 4, parsing performance also im-
proves by 1.4% in F-measure.
In order to illustrate the impact of zero pronouns
on parsing performance, consider the following ex-
ample:3
</bodyText>
<equation confidence="0.5884205">
Example (1):
4ITZAì4—*���M�
#�����*��,#����›�
;,A.�
...
A*itV]AìJJLAk 911A404*14
AìW°Tå�Alf,_MXk#pA*°Tå
fbs†�
</equation>
<bodyText confidence="0.999491571428571">
(In future, we have a reconstruction
plan.
Divide the park into seven regions, and
bring some more attractions.
Now we wait for approval of the gov-
ernment before implementing this plan
again. It is expected that work can start
next year.)
Without considering zero pronouns, the parse tree
of the second sentence output by the Berkeley parser
is shown in Figure 1.
Prior to parsing, using our zero pronoun detector
to be introduced in Section 4, the presence of zero
pronouns (denoted by #) can be detected. Figure 2
</bodyText>
<footnote confidence="0.628180666666667">
3In this paper, zero pronouns are denoted by “#” and men-
tions in the same coreference chain are shown in bold for all
examples.
</footnote>
<page confidence="0.987827">
281
</page>
<table confidence="0.9997198125">
MD MUC BCUBED CEAF Avg
(Chen and Ng, 2012) 71.64 62.21 73.55 50.97 62.24
AM (Yuan et al., 2012) 68.15 60.33 72.90 48.83 60.69
(Bj¨orkelund and Farkas, 2012) 66.37 58.61 73.10 48.19 59.97
Our baseline system (without ZPs) 66.22 56.27 76.73 49.13 60.71
Our refined system (with auto ZPs) 70.33 59.58 78.15 51.47 63.07
(Chen and Ng, 2012) 80.45 71.43 77.04 57.17 68.55
GMB (Yuan et al., 2012) 74.02 66.44 75.02 51.81 64.42
(Bj¨orkelund and Farkas, 2012) 71.02 63.56 74.52 50.20 62.76
Our baseline system (without ZPs) 75.29 70.84 81.04 54.75 68.88
Our refined system (with auto ZPs) 75.77 72.62 81.45 58.04 70.70
(Chen and Ng, 2012) 91.73 83.77 81.15 68.38 77.77
GM (Yuan et al., 2012) 89.95 82.79 79.79 65.58 76.05
(Bj¨orkelund and Farkas, 2012) 83.47 76.85 76.30 56.61 69.92
Our baseline system (without ZPs) 88.77 83.11 81.79 66.18 77.02
Our refined system (with auto ZPs) 91.49 83.46 82.43 65.88 77.26
</table>
<tableCaption confidence="0.969315">
Table 5: Performance (F-measure) of the three best Chinese coreference resolution systems on the CoNLL-2012 test
set
</tableCaption>
<bodyText confidence="0.999959818181818">
shows the new parse tree, which includes the de-
tected zero pronouns, output by the Berkeley parser
on the same sentence. Comparing these two parse
trees, we can see that the detected zero pronouns
contribute to better division of clauses and improved
parsing performance, which in turn leads to im-
proved Chinese coreference resolution.
Detecting the presence of zero pronouns also
helps to improve local salience modeling, leading to
improved Chinese coreference resolution. Long sen-
tences containing multiple clauses occur more fre-
quently in Chinese compared to English. Further-
more, a coreference chain can span many sentences.
Zero pronouns can occur not only within one sen-
tence (e.g., the first and second zero pronouns of Ex-
ample (1)), but can also be scattered across multiple
sentences (e.g., the first and third zero pronouns of
Example (1)). The subjects in the second sentence
of Example (1) are omitted.4 Detection of zero pro-
nouns improves local salience modeling, and leads
to the correct identification of all the noun phrases
of the coreference chain in Example (1).
</bodyText>
<sectionHeader confidence="0.997999" genericHeader="method">
4 Zero Pronoun Detection
</sectionHeader>
<bodyText confidence="0.999859333333333">
Empty elements are those nodes in a parse tree that
do not have corresponding surface words or phrases.
Although empty elements exist in many languages
</bodyText>
<footnote confidence="0.9283265">
4In Chinese, pro-dropped subjects account for more than
36% of subjects in sentences (Kim, 2000).
</footnote>
<bodyText confidence="0.999990137931035">
and serve different purposes, they are particularly
important for some languages, such as Chinese,
where subjects and objects are frequently dropped to
keep a discourse concise. Among empty elements,
type *pro*, namely zero pronoun, is either used for
dropped subjects or objects, which can be recovered
from the context (anaphoric), or it is of little interest
for the reader or listener to know (non-anaphoric). In
the Chinese Treebank, type *pro* constitutes about
20% (Yang and Xue, 2010), and more than 85% of
them are anaphoric (Kong and Zhou, 2010). Thus,
zero pronouns are very important in bridging the in-
formation gap in a Chinese text. In this section, we
will introduce our zero pronoun detector.
In Chinese, a zero pronoun always occurs just be-
fore a predicate phrase node (e.g., VP). In particular,
if the predicate phrase node occurs in a coordinate
structure or is modified by an adverbial node, we
only need to consider its parent. A simplified seman-
tic role labeling (SRL) framework (only including
predicate recognition, argument pruning, and argu-
ment identification) is adopted to identify the pred-
icate phrase subtree (Xue, 2008), i.e., the minimal
subtree governed by a predicate and all its argu-
ments.
We carry out zero pronoun detection for every
predicate phrase subtree in an iterative manner from
a parse tree, i.e., determining whether there is a
zero pronoun before the given predicate phrase sub-
</bodyText>
<page confidence="0.913512">
282
</page>
<equation confidence="0.940892714285714">
IP
�
��������� �� � �
� � � �
VP PU
�
�������� � � � � � � �
</equation>
<figure confidence="0.98464137037037">
VP
��� �
��
�
VV
I)-
VP
��
.A-q
NN
NP
VV
A
VP
�� � �
NP
�� ��
QP
��
CD
�
CLP
M
NP
NN
CA
VP
�
����� �� � �
�
NP
�� � �
DNP
��� �
��
QP
�� � �
VV
**1
DEG
0
NP
NN
to
ADVP
AD
�
QP
��
CD
�
CLP
M
PU
</figure>
<figureCaption confidence="0.999998">
Figure 1: The parse tree without considering zero pronouns
</figureCaption>
<bodyText confidence="0.999740466666667">
tree. Viewing the position before the given predi-
cate phrase subtree as a zero pronoun candidate, we
can perform zero pronoun detection using a machine
learning approach.
During training, if a zero pronoun candidate has
a counterpart in the same position in the annotated
training corpus (either anaphoric or non-anaphoric),
a positive example is generated. Otherwise, a nega-
tive example is generated. During testing, each zero
pronoun candidate is presented to the zero pronoun
detector to determine whether it is a zero pronoun.
The features that are employed to detect zero pro-
nouns mainly model the context of the clause itself,
the left and right siblings, and the path of the clause
to the root node. Table 6 lists the features in detail.
</bodyText>
<subsectionHeader confidence="0.816737">
4.1 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.9997818">
We evaluate our zero pronoun detector using gold
parse trees and automatic parse trees produced by
the Berkeley parser. The SVM-light toolkit with ra-
dial basis kernel and default learning parameters is
employed as our learning algorithm.
</bodyText>
<tableCaption confidence="0.923173">
Table 7 lists the results. From the results, we
</tableCaption>
<table confidence="0.991228">
R P F
GS 89.32 87.29 88.29
Auto 74.19 77.79 75.95
</table>
<tableCaption confidence="0.9985445">
Table 7: Performance of zero pronoun detection on the
test set using gold and automatic parse trees
</tableCaption>
<bodyText confidence="0.999959">
find that the performance of our zero pronoun detec-
tor drops about 12% in F-measure when using au-
tomatic parse trees, compared to using gold parse
trees. That is, the performance of zero pronoun de-
tection also depends on the performance of the syn-
tactic parser.
</bodyText>
<sectionHeader confidence="0.8839875" genericHeader="method">
5 Exploiting Zero Pronouns to Improve
Chinese Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9996965">
In this section, we will propose two methods, refin-
ing the syntactic parser and refining learning exam-
ple generation, to exploit zero pronouns to improve
Chinese coreference resolution.
</bodyText>
<page confidence="0.969735">
283
</page>
<equation confidence="0.883334461538461">
IP
������������������ � � �
PPP
� �
� � �
� � � � ��
� � � �
� � � � �
� � � �
VP
��
VP
�� � �
</equation>
<figure confidence="0.989275370967742">
IP
��� �
��
�
VP
��� �
��
NP
�� ��
QP
/\
CD
�
VV
I)-
NP
VV
A
CLP
M
NP
NN
CA
NP
EE
#
NN
.A-q
PU
VV
**1
#
M
�
IP
��� � � �
VP
�
����� �� � �
�
PU
NP
EE
ADVP
AD
�
NP
�� � �
DNP
��� �
��
DEG
0
NP
QP
�� � �
QP
��
NN
to
CD
CLP
</figure>
<figureCaption confidence="0.99989">
Figure 2: The parse tree with the detected zero pronouns
</figureCaption>
<subsectionHeader confidence="0.998666">
5.1 Refining the Syntactic Parser
</subsectionHeader>
<bodyText confidence="0.99999">
Similar to our preliminary experiments, we retrain
the Berkeley parser with explicit, automatically de-
tected zero pronouns in the training set and parse
the test set with explicit, automatically detected
zero pronouns using the retrained model. In both
anaphoricity determination and coreference resolu-
tion, the output results of the retrained parser are
employed to generate all features.
</bodyText>
<subsectionHeader confidence="0.999745">
5.2 Refining Learning Example Generation
</subsectionHeader>
<bodyText confidence="0.99998015625">
In order to model the salience of all entities, we re-
gard all zero pronouns as a special kind of NPs when
generating the learning examples. Considering the
modest performance of our anaphoricity determina-
tion module, we do not determine the anaphoricity
of zero pronouns. Instead, in the coreference res-
olution stage, all zero pronouns will be considered
during learning example generation (including both
training and test example generation).
For example, consider a coreference chain A1-
A2-Z0-A3-A4 containing one zero pronoun found
in an annotated training document. A1, A2, A3,
and A4 are traditional entity mentions, and Z0 is a
zero pronoun. During training, pairs of mentions in
the chain that are immediately adjacent (i.e., A1-A2,
A2-Z0, Z0-A3, and A3-A4) are used to generate the
positive training examples. Among them, two ex-
amples (i.e., A2-Z0 and Z0-A3) are associated with
a zero pronoun, which can act as both an anaphor
and an antecedent. For each positive pair, e.g., Z0-
A3, we find any noun phrase and zero pronoun oc-
curring between the anaphor A3 and the antecedent
Z0, and pair each of them with A3 to form a nega-
tive example. Similarly, test examples can be gen-
erated except that only the preceding mentions and
zero pronouns in the current and previous two sen-
tences will be paired with an anaphor.
Incorporating zero pronouns models salience of
all entities more accurately. The ratio of positive to
negative examples is also less skewed as a result of
considering zero pronouns – the ratio changes from
1:7.9 to 1:6.8 after considering zero pronouns.
</bodyText>
<subsectionHeader confidence="0.997253">
5.3 Reprocessing
</subsectionHeader>
<bodyText confidence="0.997891">
Although in the OntoNotes corpus, dropped subjects
and objects (i.e., zero pronouns) are considered dur-
ing coreference resolution for Chinese, they are not
</bodyText>
<page confidence="0.997038">
284
</page>
<note confidence="0.997307923076923">
Feature Description
ClauseClass Whether the given clause is a terminal clause or non-terminal clause.
LeftSibling Whether the given clause has a sibling immediately to its left.
LeftSiblingNP Whether the left siblings of the given clause contain an NP.
RightSibling Whether the given clause has a sibling immediately to its right.
RightSiblingVP Whether the right siblings of the given clause contain a VP.
ParentIP/VP Whether the syntactic category of the immediate parent of the given clause is an
IP or VP.
RootPath Whether the path from the given clause to the root of the parse tree contains
an NP or VP or CP. This feature models how the given clause is syntactically
connected to the sentence as a whole, reflecting its function within the sentence.
ClauseType The given clause is an independent clause, a subordinate clause, or others.
Has-Arg0/Arg1 Whether the given clause has an agent or patient argument.
</note>
<tableCaption confidence="0.997957">
Table 6: Features employed to detect zero pronouns
</tableCaption>
<bodyText confidence="0.999872">
used in the CoNLL-2012 shared task (i.e., in the
gold evaluation keys, all the links formed by zero
pronouns are removed).
As described in Subsection 5.2, during training
and testing, all links associated with zero pronouns
will be considered in our coreference resolution sys-
tem. That is, we do not distinguish zero pronoun res-
olution from traditional coreference resolution, and
only view zero pronouns as special pronouns. After
generating all the links, zero pronouns are included
in coreference chains. For every coreference chain,
all zero pronouns will be removed before evaluation.
</bodyText>
<subsectionHeader confidence="0.999248">
5.4 Experimental Results and Analysis
</subsectionHeader>
<bodyText confidence="0.99995125">
For fair comparison, all our experiments in this sub-
section have been conducted using the same experi-
mental settings as our baseline system. When com-
pared to our baseline system, all improvements are
statistically significant (p &lt; 0.005).
Table 8 lists the coreference resolution perfor-
mance incorporating automatically detected zero
pronouns. The results show that:
</bodyText>
<listItem confidence="0.946989555555556">
• Using automatically detected zero pronouns
achieves better performance under all experi-
mental settings. In particular, using automatic
mentions, performance improves by 3.31%,
1.42%, and 2.34% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively. Using gold mention boundaries, au-
tomatic zero pronouns contribute 1.82% in av-
erage F-measure. Using gold mentions, the
</listItem>
<table confidence="0.9999446875">
R P F
Mention Detection 71.09 69.58 70.33
MUC 55.06 64.91 59.58
AM BCUBED 76.04 80.38 78.15
CEAF 53.98 49.19 51.47
Average 63.07
Mention Detection 82.44 70.10 75.77
MUC 75.58 69.89 72.62
GMB BCUBED 76.35 87.27 81.45
CEAF 65.17 52.31 58.04
Average 70.70
Mention Detection 84.31 100.00 91.49
MUC 80.83 86.27 83.46
GM BCUBED 74.18 92.74 82.43
CEAF 69.91 62.29 65.88
Average 77.26
</table>
<tableCaption confidence="0.9961095">
Table 8: Performance of our Chinese coreference resolu-
tion system incorporating zero pronouns
</tableCaption>
<page confidence="0.996936">
285
</page>
<bodyText confidence="0.925652">
contribution of zero pronouns is only 0.24% in
average F-measure. This is because employing
either gold mention boundaries or gold men-
tions improves parsing performance.
• Our system incorporating zero pronouns out-
performs the three best systems in the CoNLL-
2012 shared task when using automatic men-
tions or gold mention boundaries. Using gold
mentions, our average F-measure is slightly
lower than that of Chen and Ng (2012).5
Table 9 presents the contribution of our two meth-
ods of exploiting zero pronouns and the impact of
gold-standard zero pronouns. We conclude that:
• Both the refined parser and refined example
generation improve performance. While the
refined parser improves the recall of mention
detection and coreference resolution, refined
example generation contributes more to preci-
sion. Combining these two methods further im-
proves coreference resolution.
• There is a performance gap of 6.01%, 4.08%,
and 3.19% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively, between the coreference resolution
system with gold-standard zero pronouns and
without zero pronouns. This suggests the use-
fulness of zero pronoun detection in Chinese
coreference resolution.
</bodyText>
<listItem confidence="0.87890175">
• Our proposed methods incorporating automatic
zero pronouns reduce the performance gap by
about half. This shows the effectiveness of our
proposed methods.
</listItem>
<subsectionHeader confidence="0.827671">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999940375">
Although the evaluation of the CoNLL-2012 shared
task does not consider zero pronouns, we also eval-
uate the performance of zero pronoun resolution on
the development data set (i.e., extracting all the re-
solved coreference links containing zero pronouns,
acting as anaphor or antecedent, to conduct the eval-
uation independently). The results show that, for
the correct anaphoric zero pronouns, the precision
</bodyText>
<footnote confidence="0.836154">
5Statistical significance testing cannot be conducted since
their output files are not released.
</footnote>
<bodyText confidence="0.99993475">
of our system is 94.76%. So viewing zero pronouns
as a special kind of NP, zero pronouns can bridge
salience and contribute to coreference resolution. In
Example (1), the zero pronouns occurring in the sec-
ond sentence help to bridge the coreferential relation
between the mention “A i^itVI/this plan” in the
last sentence and the mention “— i^* ifVI/a re-
construction plan” in the first sentence.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999916173913044">
In the last decade, both manual rule-based ap-
proaches (Lee et al., 2011) and statistical ap-
proaches (Soon et al., 2001; Ng and Cardie, 2002;
Fernandes et al., 2012) have been proposed for
coreference resolution. Besides frequently used syn-
tactic and semantic features, more linguistic features
are exploited in recent work (Ponzetto and Strube,
2006; Ng, 2007; Versley, 2007). There is less re-
search on Chinese coreference resolution compared
to English.
Although zero pronouns are prevalent in Chinese,
there is relatively little work on this topic. For Chi-
nese zero pronoun resolution, representative work
includes Converse (2006), Zhao and Ng (2007), and
Kong and Zhou (2010).
For the use of zero pronouns, Chung and Gildea
(2010) applied some extracted patterns to recover
two types of empty elements (*PRO* and *pro*).
Although the performance is still not satisfactory
(e.g., 63.0 and 44.0 in F-measure for *PRO* and
*pro* respectively), it nevertheless improves ma-
chine translation performance by 0.96 in BLEU
score.
</bodyText>
<sectionHeader confidence="0.99819" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999905666666667">
In this paper, we focus on exploiting one of the key
characteristics of Chinese text, zero pronouns, to im-
prove Chinese coreference resolution. In particu-
lar, a simplified semantic role labeling framework
is proposed to detect zero pronouns effectively, and
two effective methods are employed to incorporate
zero pronouns into Chinese coreference resolution.
Experiments on the CoNLL-2012 shared task show
the effectiveness of our proposed approach. To the
best of our knowledge, this is the first attempt at in-
corporating zero pronouns into Chinese coreference
resolution.
</bodyText>
<page confidence="0.993234">
286
</page>
<table confidence="0.999851142857143">
R MD F R MUC F R BCUBED F R CEAF F Avg
P P P P
Baseline 65.26 67.20 66.22 51.64 61.82 56.27 73.40 80.38 76.73 53.16 45.66 49.13 60.71
+RP 72.01 66.24 69.00 55.02 61.47 58.07 77.83 78.97 78.40 50.40 49.81 50.10 62.19
+REG 65.92 70.02 67.91 49.98 66.27 56.98 73.64 83.45 78.24 51.12 47.44 49.21 61.48
+AZPs 71.09 69.58 70.33 55.06 64.91 59.58 76.04 80.38 78.15 53.98 49.19 51.47 63.07
+GZPs 72.18 70.59 71.38 58.61 66.45 62.28 78.79 82.94 80.81 54.12 50.63 52.32 65.14
</table>
<tableCaption confidence="0.9495605">
Table 9: Contributions of the two methods of incorporating zero pronouns and the impact of gold zero pronouns
(RP: refining parser using auto zero pronouns, REG: refining example generation using auto zero pronouns, AZPs:
combining both RP and REG using auto zero pronouns, and GZPs: combining both RP and REG using gold zero
pronouns)
</tableCaption>
<sectionHeader confidence="0.998082" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984955">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.998722" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875594202899">
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Joint Confer-
ence on EMNLP and CoNLL – Shared Task, pages 49–
55.
Chen Chen and Vincent Ng. 2012. Combining the best of
two worlds: A hybrid approach to multilingual coref-
erence resolution. In Proceedings of the Joint Con-
ference on EMNLP and CoNLL – Shared Task, pages
56–63.
Tagyoung Chung and Daniel Gildea. 2010. Effects of
empty categories on machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 636–645.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Eraldo Rezende Fernandes, Cicero Nogueira dos Santos,
and Ruy Luiz Milidi´u. 2012. Latent structure percep-
tron with feature induction for unrestricted coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL – Shared Task, pages 41–48.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Sch¨olkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing. MIT-Press.
Young-Joo Kim. 2000. Subject/object drop in the acqui-
sition of Korean: A cross-linguistic comparison. Jour-
nal of East Asian Linguistics, 9:325–351.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882–891.
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009.
Employing the centering theory in pronoun resolution
from the semantic perspective. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 987–996.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28–34.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104–111.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 536–543.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433–440.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference of the
NAACL, pages 192–199.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
</reference>
<page confidence="0.962201">
287
</page>
<reference confidence="0.993293357142857">
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 496–505.
Bonnie Lynn Webber. 1978. A Formal Approach to Dis-
course Anaphora. Garland Press.
Nianwen Xue. 2008. Labeling Chinese predicates
with semantic roles. Computational Linguistics,
34(2):225–255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
Recovering empty categories in the Chinese Treebank.
In Coling 2010: Posters, pages 1382–1390.
Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,
Liping Ge, Zengjian Liu, Meng Liao, and Xianbo Si.
2012. A mixed deterministic model for coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL – Shared Task, pages 76–82.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 541–550.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78–83.
</reference>
<page confidence="0.997112">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.437219">
<title confidence="0.999902">Exploiting Zero Pronouns to Improve Chinese Coreference Resolution</title>
<author confidence="0.981453">Fang</author>
<affiliation confidence="0.911546">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.965234">Singapore</address>
<email confidence="0.990439">dcskf@nus.edu.sg</email>
<author confidence="0.982469">Hwee Tou</author>
<affiliation confidence="0.911515">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939476">Singapore</address>
<email confidence="0.990764">nght@comp.nus.edu.sg</email>
<abstract confidence="0.995231466666667">Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven multilingual coreference resolution using resolver stacking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task,</booktitle>
<pages>49--55</pages>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven multilingual coreference resolution using resolver stacking. In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task, pages 49– 55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Combining the best of two worlds: A hybrid approach to multilingual coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task,</booktitle>
<pages>56--63</pages>
<contexts>
<context position="11179" citStr="Chen and Ng (2012)" startWordPosition="1712" endWordPosition="1715">the parser. Using gold mention boundaries can improve the parser performance. Thus our coreference resolution system can benefit much from using gold mention boundaries (especially the recall). • Employing gold mentions further boosts our system significantly. In comparison with using gold mention boundaries, the performance improvement is attributed more to an increase in precision. In comparison with the three best systems of CoNLL-2012 in the Chinese closed track (shown in Table 5), considering average F-measure, we find that using automatic mentions, our system is only inferior to that of Chen and Ng (2012); using gold mention boundaries, our system achieves the best performance; and using gold mentions, our system is only a little worse than that of Chen and Ng (2012). 3 Motivation In order to analyze the impact of zero pronouns on Chinese coreference resolution, we first use the released OntoNotes v5.0 data (i.e., the training and development portions of the CoNLL-2012 shared task) in a corpus study. Statistics show that anaphoric zero pronouns account for 10.7% of the mentions in coreference chains in the training data, while in the development data, the proportion is 11.3%. The experimental </context>
<context position="13563" citStr="Chen and Ng, 2012" startWordPosition="2090" endWordPosition="2093">gions, and bring some more attractions. Now we wait for approval of the government before implementing this plan again. It is expected that work can start next year.) Without considering zero pronouns, the parse tree of the second sentence output by the Berkeley parser is shown in Figure 1. Prior to parsing, using our zero pronoun detector to be introduced in Section 4, the presence of zero pronouns (denoted by #) can be detected. Figure 2 3In this paper, zero pronouns are denoted by “#” and mentions in the same coreference chain are shown in bold for all examples. 281 MD MUC BCUBED CEAF Avg (Chen and Ng, 2012) 71.64 62.21 73.55 50.97 62.24 AM (Yuan et al., 2012) 68.15 60.33 72.90 48.83 60.69 (Bj¨orkelund and Farkas, 2012) 66.37 58.61 73.10 48.19 59.97 Our baseline system (without ZPs) 66.22 56.27 76.73 49.13 60.71 Our refined system (with auto ZPs) 70.33 59.58 78.15 51.47 63.07 (Chen and Ng, 2012) 80.45 71.43 77.04 57.17 68.55 GMB (Yuan et al., 2012) 74.02 66.44 75.02 51.81 64.42 (Bj¨orkelund and Farkas, 2012) 71.02 63.56 74.52 50.20 62.76 Our baseline system (without ZPs) 75.29 70.84 81.04 54.75 68.88 Our refined system (with auto ZPs) 75.77 72.62 81.45 58.04 70.70 (Chen and Ng, 2012) 91.73 83.77 </context>
<context position="25213" citStr="Chen and Ng (2012)" startWordPosition="4065" endWordPosition="4068">0 91.49 MUC 80.83 86.27 83.46 GM BCUBED 74.18 92.74 82.43 CEAF 69.91 62.29 65.88 Average 77.26 Table 8: Performance of our Chinese coreference resolution system incorporating zero pronouns 285 contribution of zero pronouns is only 0.24% in average F-measure. This is because employing either gold mention boundaries or gold mentions improves parsing performance. • Our system incorporating zero pronouns outperforms the three best systems in the CoNLL2012 shared task when using automatic mentions or gold mention boundaries. Using gold mentions, our average F-measure is slightly lower than that of Chen and Ng (2012).5 Table 9 presents the contribution of our two methods of exploiting zero pronouns and the impact of gold-standard zero pronouns. We conclude that: • Both the refined parser and refined example generation improve performance. While the refined parser improves the recall of mention detection and coreference resolution, refined example generation contributes more to precision. Combining these two methods further improves coreference resolution. • There is a performance gap of 6.01%, 4.08%, and 3.19% in F-measure on the MUC, BCUBED, and CEAF evaluation metric, respectively, between the coreferen</context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen Chen and Vincent Ng. 2012. Combining the best of two worlds: A hybrid approach to multilingual coreference resolution. In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of empty categories on machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>636--645</pages>
<contexts>
<context position="27804" citStr="Chung and Gildea (2010)" startWordPosition="4464" endWordPosition="4467">l., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to detect zero pronouns effectively, and two effective methods are employed to </context>
</contexts>
<marker>Chung, Gildea, 2010</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2010. Effects of empty categories on machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Converse</author>
</authors>
<title>Pronominal Anaphora Resolution in Chinese.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27703" citStr="Converse (2006)" startWordPosition="4447" endWordPosition="4448">e, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling </context>
</contexts>
<marker>Converse, 2006</marker>
<rawString>Susan Converse. 2006. Pronominal Anaphora Resolution in Chinese. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="11179" citStr="(2012)" startWordPosition="1715" endWordPosition="1715">Using gold mention boundaries can improve the parser performance. Thus our coreference resolution system can benefit much from using gold mention boundaries (especially the recall). • Employing gold mentions further boosts our system significantly. In comparison with using gold mention boundaries, the performance improvement is attributed more to an increase in precision. In comparison with the three best systems of CoNLL-2012 in the Chinese closed track (shown in Table 5), considering average F-measure, we find that using automatic mentions, our system is only inferior to that of Chen and Ng (2012); using gold mention boundaries, our system achieves the best performance; and using gold mentions, our system is only a little worse than that of Chen and Ng (2012). 3 Motivation In order to analyze the impact of zero pronouns on Chinese coreference resolution, we first use the released OntoNotes v5.0 data (i.e., the training and development portions of the CoNLL-2012 shared task) in a corpus study. Statistics show that anaphoric zero pronouns account for 10.7% of the mentions in coreference chains in the training data, while in the development data, the proportion is 11.3%. The experimental </context>
<context position="25213" citStr="(2012)" startWordPosition="4068" endWordPosition="4068">80.83 86.27 83.46 GM BCUBED 74.18 92.74 82.43 CEAF 69.91 62.29 65.88 Average 77.26 Table 8: Performance of our Chinese coreference resolution system incorporating zero pronouns 285 contribution of zero pronouns is only 0.24% in average F-measure. This is because employing either gold mention boundaries or gold mentions improves parsing performance. • Our system incorporating zero pronouns outperforms the three best systems in the CoNLL2012 shared task when using automatic mentions or gold mention boundaries. Using gold mentions, our average F-measure is slightly lower than that of Chen and Ng (2012).5 Table 9 presents the contribution of our two methods of exploiting zero pronouns and the impact of gold-standard zero pronouns. We conclude that: • Both the refined parser and refined example generation improve performance. While the refined parser improves the recall of mention detection and coreference resolution, refined example generation contributes more to precision. Combining these two methods further improves coreference resolution. • There is a performance gap of 6.01%, 4.08%, and 3.19% in F-measure on the MUC, BCUBED, and CEAF evaluation metric, respectively, between the coreferen</context>
</contexts>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Learning. MIT-Press.</booktitle>
<editor>In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<contexts>
<context position="6592" citStr="Joachims, 1999" startWordPosition="1004" endWordPosition="1005">F Mention Detection 65.26 67.20 66.22 MUC 51.64 61.82 56.27 AM BCUBED 73.40 80.38 76.73 CEAF 53.16 45.66 49.13 Average 60.71 Mention Detection 82.01 69.58 75.29 MUC 76.21 66.18 70.84 GMB BCUBED 76.15 86.59 81.04 CEAF 59.75 50.52 54.75 Average 68.88 Mention Detection 79.80 100.00 88.77 MUC 80.86 85.48 83.11 GM BCUBED 73.66 91.94 81.79 CEAF 67.54 64.87 66.18 Average 77.02 Table 4: Performance of our Chinese coreference resolution system on the CoNLL-2012 test set 2.3 Results and Analysis All experiments in this section are conducted on the CoNLL-2012 shared task data set. The SVM-light toolkit (Joachims, 1999) with radial basis kernel and default learning parameters is employed in both anaphoricity determination and coreference resolution. Table 3 reports the performance of anaphoricity determination on the CoNLL-2012 test set using gold-standard parse trees (GS) and automatic parse trees (Auto). All performance figures in this paper are given in percentages. The results show that using both gold parse trees and automatic parse trees, our anaphoricity determination system achieves higher precision than recall. In comparison with using gold parse trees, precision decreases by about 9% and recall 11%</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods: Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Joo Kim</author>
</authors>
<title>Subject/object drop in the acquisition of Korean: A cross-linguistic comparison.</title>
<date>2000</date>
<journal>Journal of East Asian Linguistics,</journal>
<pages>9--325</pages>
<contexts>
<context position="15897" citStr="Kim, 2000" startWordPosition="2470" endWordPosition="2471">can also be scattered across multiple sentences (e.g., the first and third zero pronouns of Example (1)). The subjects in the second sentence of Example (1) are omitted.4 Detection of zero pronouns improves local salience modeling, and leads to the correct identification of all the noun phrases of the coreference chain in Example (1). 4 Zero Pronoun Detection Empty elements are those nodes in a parse tree that do not have corresponding surface words or phrases. Although empty elements exist in many languages 4In Chinese, pro-dropped subjects account for more than 36% of subjects in sentences (Kim, 2000). and serve different purposes, they are particularly important for some languages, such as Chinese, where subjects and objects are frequently dropped to keep a discourse concise. Among empty elements, type *pro*, namely zero pronoun, is either used for dropped subjects or objects, which can be recovered from the context (anaphoric), or it is of little interest for the reader or listener to know (non-anaphoric). In the Chinese Treebank, type *pro* constitutes about 20% (Yang and Xue, 2010), and more than 85% of them are anaphoric (Kong and Zhou, 2010). Thus, zero pronouns are very important in</context>
</contexts>
<marker>Kim, 2000</marker>
<rawString>Young-Joo Kim. 2000. Subject/object drop in the acquisition of Korean: A cross-linguistic comparison. Journal of East Asian Linguistics, 9:325–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for Chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<contexts>
<context position="16454" citStr="Kong and Zhou, 2010" startWordPosition="2557" endWordPosition="2560"> account for more than 36% of subjects in sentences (Kim, 2000). and serve different purposes, they are particularly important for some languages, such as Chinese, where subjects and objects are frequently dropped to keep a discourse concise. Among empty elements, type *pro*, namely zero pronoun, is either used for dropped subjects or objects, which can be recovered from the context (anaphoric), or it is of little interest for the reader or listener to know (non-anaphoric). In the Chinese Treebank, type *pro* constitutes about 20% (Yang and Xue, 2010), and more than 85% of them are anaphoric (Kong and Zhou, 2010). Thus, zero pronouns are very important in bridging the information gap in a Chinese text. In this section, we will introduce our zero pronoun detector. In Chinese, a zero pronoun always occurs just before a predicate phrase node (e.g., VP). In particular, if the predicate phrase node occurs in a coordinate structure or is modified by an adverbial node, we only need to consider its parent. A simplified semantic role labeling (SRL) framework (only including predicate recognition, argument pruning, and argument identification) is adopted to identify the predicate phrase subtree (Xue, 2008), i.e</context>
<context position="27749" citStr="Kong and Zhou (2010)" startWordPosition="4454" endWordPosition="4457"> et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to detect zero pronouns </context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and Guodong Zhou. 2010. A tree kernelbased unified framework for Chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882–891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Employing the centering theory in pronoun resolution from the semantic perspective.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>987--996</pages>
<contexts>
<context position="5843" citStr="Kong et al. (2009)" startWordPosition="879" endWordPosition="882">sation and web texts (i.e., a large proportion of personal pronouns and the organization of a text into several parts1) and preparing for dealing with zero pronouns, we add some features shown in Table 2.2 1A text in the CoNLL-2012 data set is broken down into different “parts”. 2AN denotes anaphor, CA denotes antecedent candidate, IP denotes a simple clause, and CP denotes a clause headed by a complementizer. For the feature ANPronounRanking, the relative ranking of a given pronoun is based on its semantic role and surface position, and we assign the highest rank to zero pronouns, similar to Kong et al. (2009). R P F GS 76.32 87.14 81.37 Auto 64.87 78.42 71.00 Table 3: Performance of anaphoricity determination on the CoNLL-2012 test set R P F Mention Detection 65.26 67.20 66.22 MUC 51.64 61.82 56.27 AM BCUBED 73.40 80.38 76.73 CEAF 53.16 45.66 49.13 Average 60.71 Mention Detection 82.01 69.58 75.29 MUC 76.21 66.18 70.84 GMB BCUBED 76.15 86.59 81.04 CEAF 59.75 50.52 54.75 Average 68.88 Mention Detection 79.80 100.00 88.77 MUC 80.86 85.48 83.11 GM BCUBED 73.66 91.94 81.79 CEAF 67.54 64.87 66.18 Average 77.02 Table 4: Performance of our Chinese coreference resolution system on the CoNLL-2012 test set </context>
</contexts>
<marker>Kong, Zhou, Zhu, 2009</marker>
<rawString>Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009. Employing the centering theory in pronoun resolution from the semantic perspective. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 987–996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="27143" citStr="Lee et al., 2011" startWordPosition="4361" endWordPosition="4364">aphoric zero pronouns, the precision 5Statistical significance testing cannot be conducted since their output files are not released. of our system is 94.76%. So viewing zero pronouns as a special kind of NP, zero pronouns can bridge salience and contribute to coreference resolution. In Example (1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1379" citStr="Ng and Cardie, 2002" startWordPosition="193" endWordPosition="196"> for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution. 1 Introduction As one of the most important tasks in discourse analysis, coreference resolution aims to link a given mention (i.e., entity or event) to its co-referring expression in a text and has been a focus of research in natural language processing (NLP) for decades. Over the last decade, various machine learning techniques have been applied to coreference resolution and have performed reasonably well (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012). Current techniques rely primarily on surface level features such as string match, syntactic features such as apposition, and shallow semantic features such as number, gender, semantic class, etc. Despite similarities between Chinese and English, there are differences that have a significant impact on coreference resolution. In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns (ZPs), to improve Chinese coreference resolution. In particular, a simplified semantic role labeling (SRL) framework is proposed to identify Chines</context>
<context position="27210" citStr="Ng and Cardie, 2002" startWordPosition="4373" endWordPosition="4376">sting cannot be conducted since their output files are not released. of our system is 94.76%. So viewing zero pronouns as a special kind of NP, zero pronouns can bridge salience and contribute to coreference resolution. In Example (1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) appli</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Semantic class induction and coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>536--543</pages>
<contexts>
<context position="27430" citStr="Ng, 2007" startWordPosition="4407" endWordPosition="4408">(1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>Vincent Ng. 2007. Semantic class induction and coreference resolution. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 536–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="12211" citStr="Petrov et al., 2006" startWordPosition="1877" endWordPosition="1880">cs show that anaphoric zero pronouns account for 10.7% of the mentions in coreference chains in the training data, while in the development data, the proportion is 11.3%. The experimental results of our Chinese coreference resolution system (i.e., the baseline) show that using both gold mention boundaries and gold mentions significantly improves system performance, especially for recall, largely due to improved parser performance. We then analyze the impact of zero pronouns on Chinese syntactic parsing. As a preliminary exploration, we integrate Chinese zero pronouns into the Berkeley parser (Petrov et al., 2006), experimenting with gold-standard or automatically determined zero pronouns kept or stripped off (using gold-standard word segmentation provided in the CoNLL-2012 data). The results indicate that given gold-standard zero pronouns, parsing performance improves by 1.8% in F-measure. Using automatically determined zero pronouns by our zero pronoun detector to be introduced in Section 4, parsing performance also improves by 1.4% in F-measure. In order to illustrate the impact of zero pronouns on parsing performance, consider the following example:3 Example (1): 4ITZAì4—*���M� #�����*��,#����›� ;,</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="27420" citStr="Ponzetto and Strube, 2006" startWordPosition="4403" endWordPosition="4406">nce resolution. In Example (1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it ne</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the NAACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1358" citStr="Soon et al., 2001" startWordPosition="189" endWordPosition="192">ploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution. 1 Introduction As one of the most important tasks in discourse analysis, coreference resolution aims to link a given mention (i.e., entity or event) to its co-referring expression in a text and has been a focus of research in natural language processing (NLP) for decades. Over the last decade, various machine learning techniques have been applied to coreference resolution and have performed reasonably well (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012). Current techniques rely primarily on surface level features such as string match, syntactic features such as apposition, and shallow semantic features such as number, gender, semantic class, etc. Despite similarities between Chinese and English, there are differences that have a significant impact on coreference resolution. In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns (ZPs), to improve Chinese coreference resolution. In particular, a simplified semantic role labeling (SRL) framework is propos</context>
<context position="5174" citStr="Soon et al. (2001)" startWordPosition="767" endWordPosition="770">s, e.g., those which contain (1) measure words such as ‘-*/one year’ and ‘-Oji/one time’; (2) named entities whose categories are PERCENT, MONEY, QUANTITY, and CARDINAL; (3) interrogative pronouns such as ‘ &apos;f+A/what’ and ‘ê?/where’. After pruning, we employ a learning-based method to train an independent classifier to determine whether the remaining mentions are anaphoric. Table 1 lists all the features employed in our anaphoricity determination system. 2.2 Coreference Resolution Our Chinese coreference resolution system adopts the same learning-based model and the same set of 12 features as Soon et al. (2001). Considering the special characteristics of conversation and web texts (i.e., a large proportion of personal pronouns and the organization of a text into several parts1) and preparing for dealing with zero pronouns, we add some features shown in Table 2.2 1A text in the CoNLL-2012 data set is broken down into different “parts”. 2AN denotes anaphor, CA denotes antecedent candidate, IP denotes a simple clause, and CP denotes a clause headed by a complementizer. For the feature ANPronounRanking, the relative ranking of a given pronoun is based on its semantic role and surface position, and we as</context>
<context position="27189" citStr="Soon et al., 2001" startWordPosition="4369" endWordPosition="4372">cal significance testing cannot be conducted since their output files are not released. of our system is 94.76%. So viewing zero pronouns as a special kind of NP, zero pronouns can bridge salience and contribute to coreference resolution. In Example (1), the zero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung an</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
</authors>
<title>Antecedent selection techniques for high-recall coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>496--505</pages>
<contexts>
<context position="27446" citStr="Versley, 2007" startWordPosition="4409" endWordPosition="4410">ero pronouns occurring in the second sentence help to bridge the coreferential relation between the mention “A i^itVI/this plan” in the last sentence and the mention “— i^* ifVI/a reconstruction plan” in the first sentence. 6 Related Work In the last decade, both manual rule-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machin</context>
</contexts>
<marker>Versley, 2007</marker>
<rawString>Yannick Versley. 2007. Antecedent selection techniques for high-recall coreference resolution. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 496–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Lynn Webber</author>
</authors>
<title>A Formal Approach to Discourse Anaphora.</title>
<date>1978</date>
<publisher>Garland Press.</publisher>
<contexts>
<context position="3007" citStr="Webber (1978)" startWordPosition="442" endWordPosition="443">ution The rest of this paper is organized as follows. Section 2 describes our baseline Chinese coreference resolution system. Section 3 motivates how the detection of zero pronouns can improve Chinese coreference resolution, using an illustrating example. Section 4 presents our approach to detect zero pronouns. Section 5 proposes two methods to exploit zero pronouns to improve Chinese coreference resolution, based on a corpus study and preliminary experiments. Section 6 briefly outlines the related work. Finally, we conclude our work in Section 7. 2 Chinese Coreference Resolution According to Webber (1978), coreference resolution can be decomposed into two complementary subtasks: (1) anaphoricity determination: deciding whether a given noun phrase (NP) is anaphoric or not; and (2) anaphora resolution: linking together multiple mentions of a given entity in the world. Our Chinese coreference resolution system also contains these two components. Using the training data set of CoNLL-2012 shared task, we first train an anaphoricity classifier to determine whether 278 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 278–288, Seattle, Washington, USA, 18-2</context>
</contexts>
<marker>Webber, 1978</marker>
<rawString>Bonnie Lynn Webber. 1978. A Formal Approach to Discourse Anaphora. Garland Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling Chinese predicates with semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="17049" citStr="Xue, 2008" startWordPosition="2656" endWordPosition="2657">and Zhou, 2010). Thus, zero pronouns are very important in bridging the information gap in a Chinese text. In this section, we will introduce our zero pronoun detector. In Chinese, a zero pronoun always occurs just before a predicate phrase node (e.g., VP). In particular, if the predicate phrase node occurs in a coordinate structure or is modified by an adverbial node, we only need to consider its parent. A simplified semantic role labeling (SRL) framework (only including predicate recognition, argument pruning, and argument identification) is adopted to identify the predicate phrase subtree (Xue, 2008), i.e., the minimal subtree governed by a predicate and all its arguments. We carry out zero pronoun detection for every predicate phrase subtree in an iterative manner from a parse tree, i.e., determining whether there is a zero pronoun before the given predicate phrase sub282 IP � ��������� �� � � � � � � VP PU � �������� � � � � � � � VP ��� � �� � VV I)- VP �� .A-q NN NP VV A VP �� � � NP �� �� QP �� CD � CLP M NP NN CA VP � ����� �� � � � NP �� � � DNP ��� � �� QP �� � � VV **1 DEG 0 NP NN to ADVP AD � QP �� CD � CLP M PU Figure 1: The parse tree without considering zero pronouns tree. Vi</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling Chinese predicates with semantic roles. Computational Linguistics, 34(2):225–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqin Yang</author>
<author>Nianwen Xue</author>
</authors>
<title>Chasing the ghost: Recovering empty categories in the Chinese Treebank. In Coling 2010: Posters,</title>
<date>2010</date>
<pages>1382--1390</pages>
<contexts>
<context position="16391" citStr="Yang and Xue, 2010" startWordPosition="2545" endWordPosition="2548">ents exist in many languages 4In Chinese, pro-dropped subjects account for more than 36% of subjects in sentences (Kim, 2000). and serve different purposes, they are particularly important for some languages, such as Chinese, where subjects and objects are frequently dropped to keep a discourse concise. Among empty elements, type *pro*, namely zero pronoun, is either used for dropped subjects or objects, which can be recovered from the context (anaphoric), or it is of little interest for the reader or listener to know (non-anaphoric). In the Chinese Treebank, type *pro* constitutes about 20% (Yang and Xue, 2010), and more than 85% of them are anaphoric (Kong and Zhou, 2010). Thus, zero pronouns are very important in bridging the information gap in a Chinese text. In this section, we will introduce our zero pronoun detector. In Chinese, a zero pronoun always occurs just before a predicate phrase node (e.g., VP). In particular, if the predicate phrase node occurs in a coordinate structure or is modified by an adverbial node, we only need to consider its parent. A simplified semantic role labeling (SRL) framework (only including predicate recognition, argument pruning, and argument identification) is ad</context>
</contexts>
<marker>Yang, Xue, 2010</marker>
<rawString>Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost: Recovering empty categories in the Chinese Treebank. In Coling 2010: Posters, pages 1382–1390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Yuan</author>
<author>Qingcai Chen</author>
<author>Yang Xiang</author>
<author>Xiaolong Wang</author>
<author>Liping Ge</author>
<author>Zengjian Liu</author>
<author>Meng Liao</author>
<author>Xianbo Si</author>
</authors>
<title>A mixed deterministic model for coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task,</booktitle>
<pages>76--82</pages>
<contexts>
<context position="13616" citStr="Yuan et al., 2012" startWordPosition="2100" endWordPosition="2103">or approval of the government before implementing this plan again. It is expected that work can start next year.) Without considering zero pronouns, the parse tree of the second sentence output by the Berkeley parser is shown in Figure 1. Prior to parsing, using our zero pronoun detector to be introduced in Section 4, the presence of zero pronouns (denoted by #) can be detected. Figure 2 3In this paper, zero pronouns are denoted by “#” and mentions in the same coreference chain are shown in bold for all examples. 281 MD MUC BCUBED CEAF Avg (Chen and Ng, 2012) 71.64 62.21 73.55 50.97 62.24 AM (Yuan et al., 2012) 68.15 60.33 72.90 48.83 60.69 (Bj¨orkelund and Farkas, 2012) 66.37 58.61 73.10 48.19 59.97 Our baseline system (without ZPs) 66.22 56.27 76.73 49.13 60.71 Our refined system (with auto ZPs) 70.33 59.58 78.15 51.47 63.07 (Chen and Ng, 2012) 80.45 71.43 77.04 57.17 68.55 GMB (Yuan et al., 2012) 74.02 66.44 75.02 51.81 64.42 (Bj¨orkelund and Farkas, 2012) 71.02 63.56 74.52 50.20 62.76 Our baseline system (without ZPs) 75.29 70.84 81.04 54.75 68.88 Our refined system (with auto ZPs) 75.77 72.62 81.45 58.04 70.70 (Chen and Ng, 2012) 91.73 83.77 81.15 68.38 77.77 GM (Yuan et al., 2012) 89.95 82.79 </context>
</contexts>
<marker>Yuan, Chen, Xiang, Wang, Ge, Liu, Liao, Si, 2012</marker>
<rawString>Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang, Liping Ge, Zengjian Liu, Meng Liao, and Xianbo Si. 2012. A mixed deterministic model for coreference resolution. In Proceedings of the Joint Conference on EMNLP and CoNLL – Shared Task, pages 76–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="27723" citStr="Zhao and Ng (2007)" startWordPosition="4449" endWordPosition="4452">le-based approaches (Lee et al., 2011) and statistical approaches (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012) have been proposed for coreference resolution. Besides frequently used syntactic and semantic features, more linguistic features are exploited in recent work (Ponzetto and Strube, 2006; Ng, 2007; Versley, 2007). There is less research on Chinese coreference resolution compared to English. Although zero pronouns are prevalent in Chinese, there is relatively little work on this topic. For Chinese zero pronoun resolution, representative work includes Converse (2006), Zhao and Ng (2007), and Kong and Zhou (2010). For the use of zero pronouns, Chung and Gildea (2010) applied some extracted patterns to recover two types of empty elements (*PRO* and *pro*). Although the performance is still not satisfactory (e.g., 63.0 and 44.0 in F-measure for *PRO* and *pro* respectively), it nevertheless improves machine translation performance by 0.96 in BLEU score. 7 Conclusion In this paper, we focus on exploiting one of the key characteristics of Chinese text, zero pronouns, to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is propose</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It Makes Sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="8629" citStr="Zhong and Ng, 2010" startWordPosition="1312" endWordPosition="1315">an appositive relation. NestIn Whether another NP is nested in the current mention. NestOut Whether the current mention is nested in another NP. FirstNP Whether the current mention is the first NP of the sentence. FrontDistance The number of words between the current mention and the nearest previous clause. BackDistance The number of words between the current mention and the nearest following clause. WordSense Whether the current mention and another phrase in the previous context have the same word sense. Word sense annotation is provided in the CoNLL-2012 data set, based on the IMS software (Zhong and Ng, 2010). Table 1: Features employed in our anaphoricity determination system Feature Description AN/CAPronounType Whether the anaphor or the antecedent candidate is a zero pronoun, first person, second person, third person, neutral pronoun, or others. In our coreference resolution system, a zero pronoun is viewed as a kind of special pronoun. AN/CAGrammaticalRole Whether the anaphor or the antecedent candidate is a subject, object, or others. AN/CAOwnerClauseType Whether the anaphor or the antecedent candidate is in a matrix clause, an independent clause, a subordinate clause, or none of the above. A</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
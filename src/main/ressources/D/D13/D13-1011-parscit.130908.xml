<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005961">
<title confidence="0.989728">
Learning to Freestyle: Hip Hop Challenge-Response Induction via
Transduction Rule Segmentation
</title>
<author confidence="0.95529">
Dekai WU Karteek ADDANKi Markus SAERs Meriem BELOUCiF
</author>
<affiliation confidence="0.947301">
Human Language Technology Center
Department of Computer Science
HKUST, Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.996253">
{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk
</email>
<sectionHeader confidence="0.994759" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889">
We present a novel model, FREESTYLE, that
learns to improvise rhyming and fluent re-
sponses upon being challenged with a line of
hip hop lyrics, by combining both bottom-
up token based rule induction and top-down
rule segmentation strategies to learn a stochas-
tic transduction grammar that simultaneously
learns both phrasing and rhyming associations.
In this attack on the woefully under-explored
natural language genre of music lyrics, we
exploit a strictly unsupervised transduction
grammar induction approach. Our task is par-
ticularly ambitious in that no use of any a pri-
ori linguistic or phonetic information is al-
lowed, even though the domain of hip hop
lyrics is particularly noisy and unstructured.
We evaluate the performance of the learned
model against a model learned only using
the more conventional bottom-up token based
rule induction, and demonstrate the superi-
ority of our combined token based and rule
segmentation induction method toward gen-
erating higher quality improvised responses,
measured on fluency and rhyming criteria as
judged by human evaluators. To highlight
some of the inherent challenges in adapting
other algorithms to this novel task, we also
compare the quality ofthe responses generated
by our model to those generated by an out-of-
the-box phrase based SMT system. We tackle
the challenge of selecting appropriate training
data for our task via a dedicated rhyme scheme
detection module, which is also acquired via
unsupervised learning and report improved
quality of the generated responses. Finally,
we report results with Maghrebi French hip
hop lyrics indicating that our model performs
surprisingly well with no special adaptation to
other languages.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896911764706">
The genre of lyrics in music has been severely under-
studied from the perspective of computational lin-
guistics despite being a form of language that has
perhaps had the most impact across almost all human
cultures. With the motivation of spurring further re-
search in this genre, we apply stochastic transduc-
tion grammar induction algorithms to address some
of the modeling issues in song lyrics. An ideal start-
ing point for this investigation is hip hop, a genre
that emphasizes rapping, spoken or chanted rhyming
lyrics against strong beats or simple melodies. Hip
hop lyrics, in contrast to poetry and other genres of
music, present a significant number of challenges for
learning as it lacks well-defined structure in terms of
rhyme scheme, meter, or overall meaning making it
an interesting genre to bring to light some of the less
studied modeling issues.
The domain of hip hop lyrics is particularly un-
structured when compared to classical poetry, a do-
main on which statistical methods have been applied
in the past. Hip hop lyrics are unstructured in the
sense that a very high degree of variation is permit-
ted in the meter of the lyrics, and large amounts of
colloquial vocabulary and slang from the subculture
are employed. The variance in the permitted me-
ter makes it hard to make any assumptions about
the stress patterns of verses in order to identify the
rhyming words used when generating output. The
broad range of unorthodox vocabulary used in hip
hop make it difficult to use off-the-shelf NLP tools
for doing phonological and/or morphological analy-
sis. These problems are further exacerbated by dif-
ferences in intonation of the same word and lack of
robust transcription (Liberman, 2010).
</bodyText>
<page confidence="0.980736">
102
</page>
<note confidence="0.73364">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 102–112,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963391304348">
We argue that stochastic transduction grammars,1
given their success in the area of machine transla-
tion and efficient unsupervised learning algorithms,
are ideal for capturing the structural relationship be-
tween lyrics. Hence, our FREESTYLE system mod-
els the problem of improvising a rhyming response
given any hip hop lyric challenge as transducing
a challenge line into a rhyming response. We
use a stochastic transduction grammar induced in
a completely unsupervised fashion using a combi-
nation of token based rule induction and segment-
ing (Saers et al., 2013) as the underlying model to
fully-automatically learn a challenge-response sys-
tem and compare its performance against a simpler
token based transduction grammar model. Both our
models are completely unsupervised and use no prior
phonetic or linguistic knowledge whatsoever despite
the highly unstructured and noisy domain.
We believe that the challenge-response system
based on an interpolated combination of token based
rule induction and rule segmenting transduction
grammars will generate more fluent and rhyming re-
sponses compared to one based on token based trans-
duction grammars models. This is based on the ob-
servation that token based transduction grammars
suffer from a lack of fluency; a consequence of the
degree of expressivity they permit. Therefore, as a
principal part of our investigation we compare the
quality of responses generated using a combination
of token based rule induction and top-down rule seg-
menting transduction grammars to those generated
by pure token based transduction grammars.
We also hypothesize that in order to generate flu-
ent and rhyming responses, it is not sufficient to train
the transduction grammars on all adjacent lines of a
hip hop verse. Therefore, we propose a data selec-
tion scheme using a rhyme scheme detector acquired
through unsupervised learning to generate the train-
ing data for the challenge-response systems. The
rhyme scheme detector segments each verse of a hip
hop song into stanzas and identifies the lines in each
stanza that rhyme with each other which are then
added as training instances. We demonstrate the su-
periority of our training data selection method by
comparing the quality of the responses generated by
the models trained on data selected with and without
</bodyText>
<footnote confidence="0.527597">
1Also known in SMT as “synchronous grammars”.
</footnote>
<bodyText confidence="0.998845738095238">
using the rhyme scheme detector.
Unlike conventional spoken and written language,
disfluencies and backing vocals2 occur very fre-
quently in the domain of hip hop lyrics which af-
fect the performance of NLP models designed for
processing well-formed sentences. We propose two
strategies to mitigate the effect of disfluencies on our
model performance and compare their efficacy using
human evaluations. Finally, in order to illustrate the
challenges faced by other NLP algorithms, we con-
trast the performance of our model against a conven-
tional, widely used phrase-based SMT model.
A brief terminological note: “stanza” and “verse”
are frequently confused and sometimes conflated.
Worse yet, their usage for song lyrics is often con-
tradictory to that for poetry. To avoid ambiguity
we consistently follow these technical definitions for
segments in decreasing size of granularity:
verse a large unit of a song’s lyrics. A song typi-
cally contains several verses interspersed with
choruses. In the present work, we do not differ-
entiate choruses from verses. In song lyrics, a
verse is most commonly represented as a sepa-
rate paragraph.
stanza a segment within a verse which has a me-
ter and rhyme scheme. Stanzas often consist of
2, 3, or 4 lines, but stanzas of more lines are
also common. Particularly in hip hop, a single
verse often contains many stanzas with differ-
ent rhyme schemes and meters.
line a segment within a stanza consisting of a single
line. In poetry, strictly speaking this would be
called a “verse”, which however conflicts with
the conventional use of “verse” in song lyrics.
In Section 2, we discuss some of the previous
work that applies statistical NLP methods to less
conventional domains and problems. We describe
our experimental conditions in Section 3. We com-
pare the performance of token and segment based
transduction grammar models in Section 4. We com-
pare our data selection schemes and disfluency han-
dling strategies in Sections 5 and 6. Finally, in
</bodyText>
<footnote confidence="0.96148">
2Particularly the repetitive chants, exclamations, and inter-
jections in hip hop “hype man” style backing vocals.
</footnote>
<page confidence="0.999246">
103
</page>
<bodyText confidence="0.997834">
Section 7 we describe some preliminary results ob-
tained using our approach on improvising hip hop
responses in French and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999989848484849">
Although a few attempts have been made to apply
statistical NLP learning methods to unconventional
domains, FREESTYLE is among the first to tackle the
genre of hip hop lyrics (Addanki and Wu, 2013; Wu
et al., 2013a,b). Our preliminary work suggested the
need for further research to identify models that cap-
ture the correct generalizations to be able to gener-
ate fluent and rhyming responses. As a step towards
this direction, we contrast the performance of inter-
polated bottom-up token based rule induction and
top-down segmenting transduction grammar models
and token based transduction grammar models. We
briefly describe some of the past work in statistical
NLP on unconventional domains below.
Most of the past work either uses some form of
prior linguistic knowledge or enforces harsher con-
straints such as set number of words in a line, or a set
meter which are warranted by more structured do-
mains such as poetry. However, in hip hop lyrics it
is hard to make any linguistic or structural assump-
tions. For example, words such as sho, flo, holla
which frequently appear in the lyrics are not part of
any standard lexicon and hip hop does not require a
set number of syllables in a line, unlike poems. Also,
surprising and unlikely rhymes in hip hop are fre-
quently achieved via intonation and assonance, mak-
ing it hard to apply prior phonological constraints.
A phrase based SMT system was trained to “trans-
late” the first line of a Chinese couplet or duilian
into the second by Jiang and Zhou (2008). The most
suitable next line was selected by applying linguistic
constraints to the n best output of the SMT system.
However in contrast to Chinese couplets, which ad-
here to strict rules requiring, for example, an identi-
cal number of characters in each line and one-to-one
correspondence in their metrical length, the domain
of hip hop lyrics is far more unstructured and there
exists no clear constraint that would ensure fluent
and rhyming responses to hip hop challenge lyrics.
Barbieri et al. (2012) use controlled Markov pro-
cesses to semi-automatically generate lyrics that sat-
isfy the structural constraints of rhyme and meter.
Tamil lyrics were automatically generated given a
melody using conditional random fields by A. et al.
(2009). The lyrics were represented as a sequence
of labels using the KNM system where K, N and M
represented the long vowels, short vowels and con-
sonants respectively.
Genzel et al. (2010) used SMT in conjunction
with stress patterns and rhymes found in a pronun-
ciation dictionary to produce translations of poems.
Although many constraints were applied in translat-
ing full verses of poems, it was challenging to sat-
isfy all the constraints. Stress patterns were assigned
to words given the meter of a line in Shakespeare’s
sonnets by Greene et al. (2010), which were then
combined with a language model to generate poems.
Sonderegger (2011) attempted to infer the pronun-
ciation of words in old English by identifying the
rhyming patterns using graph theory. However, their
heuristic of clustering words with similar IPA end-
ings resulted in large clusters of false positives such
as bloom and numb. A language-independent gener-
ative model for stanzas in poetry was proposed by
Reddy and Knight (2011) via which they could dis-
cover rhyme schemes in French and English poetry.
</bodyText>
<sectionHeader confidence="0.995731" genericHeader="method">
3 Experimental conditions
</sectionHeader>
<bodyText confidence="0.999964777777778">
Before introducing our FREESTYLE models, we first
detail our experimental assumptions and the evalua-
tion scheme under which the responses generated by
different models are compared against one another.
We describe our training data as well as a phrase-
based SMT (PBSMT) contrastive baseline. We also
define the evaluation scheme used to compare the re-
sponses of different systems on criteria of fluency
and rhyming.
</bodyText>
<subsectionHeader confidence="0.999768">
3.1 Training data
</subsectionHeader>
<bodyText confidence="0.9999872">
We used freely available user generated hip hop
lyrics on the Internet to provide training data for our
experiments. We collected approximately 52,000
English hip hop song lyrics amounting to approxi-
mately 800Mb of raw HTML content. The data was
cleaned by stripping HTML tags, metadata and nor-
malized for special characters and case differences.
The processed corpus contained 22 million tokens
with 260,000 verses and 2.7 million lines of hip hop
lyrics. As human evaluation using expert hip hop
</bodyText>
<page confidence="0.995069">
104
</page>
<bodyText confidence="0.9998705">
listeners is expensive, a small subset of 85 lines was
chosen as the test set to provide challenges for com-
paring the quality of responses generated by different
systems.
</bodyText>
<subsectionHeader confidence="0.999725">
3.2 Evaluation scheme
</subsectionHeader>
<bodyText confidence="0.999990538461538">
The performance of various FREESTYLE versions
was evaluated on the task of generating a improvised
fluent and rhyming response given a single line of a
hip hop verse as a challenge. The output of all the
systems on the test set was given to three indepen-
dent frequent hip hop listeners for manual evalua-
tion. They were asked to evaluate the system out-
puts according to fluency and the degree of rhyming.
They were free to choose the tune to make the lyrics
rhyme as the beats of the song were not used in the
training data. Each evaluator was asked to score the
response of each system on the criterion of fluency
and rhyming as being good, acceptable or bad.
</bodyText>
<subsectionHeader confidence="0.998049">
3.3 Phrase-based SMT baseline
</subsectionHeader>
<bodyText confidence="0.999940352941177">
In order to evaluate the performance of an out-of-
the-box phrase-based SMT (PBSMT) system toward
this novel task of generating rhyming and fluent re-
sponses, a standard Moses baseline (Koehn et al.,
2007) was also trained in order to compare its per-
formance with our transduction grammar induction
model. A 4-gram language model which was trained
on the entire training corpus using SRILM (Stolcke,
2002) was used to generate responses in conjunction
with the phrase-based translation model. As no au-
tomatic quality evaluation metrics exist for hip hop
responses analogous to BLEU for SMT, the model
weights cannot be tuned in conventional ways such
as MERT (Och, 2003). Instead, a slightly higher than
typical language model weight was empirically cho-
sen using a small development set to produce fluent
outputs.
</bodyText>
<sectionHeader confidence="0.8687505" genericHeader="method">
4 Interpolated segmenting model vs. token
based model
</sectionHeader>
<bodyText confidence="0.999974458333333">
We compare the performance of transduction gram-
mars induced via interpolated token based and rule
segmenting (ISTG) versus token based transduction
grammars (TG) on the task of generating a rhyming
and fluent response to hip hop challenges. We use
the framework of stochastic transduction grammars,
specifically bracketing ITGs (inversion transduction
grammars) (Wu, 1997), as our translation model for
“transducing” any given challenge into a rhyming
and fluent response. Our choice is motivated by
the significant amount of empirical evidence for the
representational capacity of transduction grammars
across a spectrum of natural language tasks such as
textual entailment (Wu, 2006), mining parallel sen-
tences (Wu and Fung, 2005) and machine translation
(Zens and Ney, 2003). Further, existence of effi-
cient learning algorithms (Saers et al., 2012; Saers
and Wu, 2011) that make no language specific as-
sumptions, make inversion transduction grammars a
suitable framework for our modeling needs. Exam-
ples of lexical transduction rules can be seen in Ta-
bles 3 and 5. In addition, the grammar also includes
structural transduction rules for the straight case
A → [A A] and also the inverted case A → &lt;A A&gt;.
</bodyText>
<subsectionHeader confidence="0.988878">
4.1 Token based vs. segmental ITGs
</subsectionHeader>
<bodyText confidence="0.999988793103448">
The degenerate case of ITGs are token based ITGs
wherein each translation rule contains at most one
token in input and output languages. Efficient induc-
tion algorithms with polynomial run time exist for to-
ken based ITGs and the expressivity they permit has
been empirically determined to capture most of the
word alignments that occur across natural languages.
The parameters of the token based ITGs can be es-
timated using expectation maximization through an
efficient dynamic programming algorithm in con-
junction with beam pruning (Saers and Wu, 2011).
In contrast to token based ITGs, each rule in a seg-
mental ITG grammar can contain more than one to-
ken in both input and output languages. In machine
translation applications, segmental models produce
translations that are more fluent as they can capture
lexical knowledge at a phrasal level. However, only
a handful of purely unsupervised algorithms exist
for learning segmental ITGs under matched training
and testing assumptions. Most other approaches in
SMT use a variety of ad hoc heuristics for extracting
segments from token alignments, justified purely by
short term improvements in automatic MT evalua-
tion metrics such as BLEU (Papineni et al., 2002)
which cannot be transferred to our current task. In-
stead, we use a completely unsupervised learning al-
gorithm for segmental ITGs that stays strictly within
the transduction grammar optimization framework
for both training and testing as proposed in Saers
</bodyText>
<page confidence="0.996089">
105
</page>
<bodyText confidence="0.9994745625">
et al. (2013).
Saers et al. (2013) induce a phrasal inversion
transduction grammar via interpolating the bottom-
up rule chunking approach proposed in Saers et al.
(2012) with a top-down rule segmenting approach
driven by a minimum description length objective
function (Solomonoff, 1959; Rissanen, 1983) that
trades off the maximum likelihood against model
size. Saers et al. (2013) report improvements in
BLEU score (Papineni et al., 2002) on their transla-
tion task. In our current approach instead of using a
bottom-up rule chunking approach we use a simpler
token based grammar instead. Given two grammars
(Ga and Gb) and an interpolation parameter α the
probability function of the interpolated grammar is
given by:
</bodyText>
<equation confidence="0.797514">
pa+b (r) = αpa (r) + (1 − α)pb (r)
</equation>
<bodyText confidence="0.999970636363636">
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. The
pseudocode for the top-down rule segmenting algo-
rithm is shown in 1. The algorithm uses the methods
collect_biaffixes, eval_dl, sort_by_delta and
make_segmentations. These methods collect all the
biaffixes in an ITG, evaluate the difference in de-
scription length, sort candidates by these differences,
and commit to a given set of candidates, respectively.
The suitable interpolation parameter is chosen em-
pirically based on the responses generated on a small
development set.
We compare the performance of inducing a token
based ITG versus inducing a segmental ITG using in-
terpolated bottom-up token based rule induction and
top-down rule segmentation. To highlight some of
the inherent challenges in adapting other algorithms
to this novel task, we also compare the quality of the
responses generated by our model to those generated
by an off-the-shelf phrase based SMT system.
</bodyText>
<subsectionHeader confidence="0.993856">
4.2 Decoding heuristics
</subsectionHeader>
<bodyText confidence="0.977630857142857">
We use our in-house ITG decoder implemented ac-
cording to the algorithm mentioned in Wu (1996)
for the generating responses to challenges by decod-
ing with the trained transduction grammars. The de-
coder uses a CKY-style parsing algorithm (Cocke,
Algorithm 1 Iterative rule segmenting learning
driven by minimum description length.
</bodyText>
<listItem confidence="0.996062764705882">
1: 4b ▷ The ITG being induced
2: repeat
3: Ssum +— 0
4: bs +— collect_biaffixes(4b)
5: bS +— []
6: for all b E bs do
7: S +— eval_dl(b, 4b)
8: if S &lt; 0 then
9: bS +— [bS, (b, S)]
10: sort_by_delta(bS)
11: for all (b, S) E bS do
12: S′ +— eval_dl(b, 4b)
13: if S′ &lt; 0 then
14: 4b +— make_segmentations(b, 4b)
15: Ssum +— Ssum + S′
16: until Ssum &gt; 0
17: return 4b
</listItem>
<bodyText confidence="0.998703904761905">
1969) with cube pruning (Chiang, 2007). The de-
coder builds an efficient hypergraph structure which
is then scored using the induced grammar. The
trained transduction grammar model was decoded
using the 4-gram language model and the model
weights determined as described in 3.3.
In our decoding algorithm, we restrict the reorder-
ing to only be monotonic as we want to produce out-
put that follows the same rhyming order of the chal-
lenge. Interleaved rhyming order is harder to evalu-
ate without the larger context of the song and we do
not address that problem in our current model. We
also penalize singleton rules to produce responses of
similar length as successive lines in a stanza are typ-
ically of similar length. Finally, we add a penalty to
reflexive translation rules that map the same surface
form to itself such as A → yo/yo. We obtain these
rules with a high probability due to the presence of
sentence pairs where both the input and output are
identical strings as many stanzas in our data contain
repeated chorus lines.
</bodyText>
<subsectionHeader confidence="0.661665">
4.3 Results: Rule segmentation improves
responses
</subsectionHeader>
<bodyText confidence="0.992978">
Results in Table 1 indicate that the ISTG outperforms
the TG model towards the task of generating fluent
and rhyming responses. On the criterion of fluency,
</bodyText>
<page confidence="0.99815">
106
</page>
<tableCaption confidence="0.9531095">
Table 1: Percentage of &gt;good and &gt;acceptable (i.e., either good or acceptable) responses on fluency and rhyming
criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS,
TG+RS, ISTG+RS are models trained on rhyme scheme based corpus selection strategy. Disfluency correction strategy
was used in all cases.
</tableCaption>
<table confidence="0.998391142857143">
model fluency (≥good) fluency (&gt;acceptable) rhyming (≥good) rhyming (&gt;acceptable)
PBSMT 3.14% 4.70% 1.57% 4.31%
TG 21.18% 54.51% 23.53% 39.21%
ISTG 26.27% 57.64% 27.45% 48.23%
PBSMT+RS 30.59% 43.53% 1.96% 9.02%
TG+RS 34.12% 60.39% 20.00% 42.74%
ISTG+RS 30.98% 61.18% 30.98% 53.72%
</table>
<tableCaption confidence="0.958376">
Table 2: Transduction rules learned by ISTG model.
</tableCaption>
<table confidence="0.9823075">
transduction grammar rule log prob.
A → long/wrong -11.6747
A → rhyme/time -11.6604
A → felt bad/couldn&apos;t see what i really had -11.3196
A → matter what you say/leaving anyway -11.8792
A → arhythamatic/this rhythm is sick -12.3492
</table>
<bodyText confidence="0.996470230769231">
the ISTG model produces a significantly higher frac-
tion of sentences rated good (26.27% vs. 21.18%)
and &gt;acceptable (57.64% vs. 54.51%). Higher frac-
tion of responses generated by the ISTG model are
rated as good (27.45% vs. 23.53%) and &gt;acceptable
(57.64% vs. 54.51%) compared to the TG model.
Both TG and ISTG model perform significantly bet-
ter than the PBSMT baseline. Upon inspecting the
learned rules, we noticed that the ISTG models cap-
ture rhyming correspondences both at the token and
segmental levels. Table 2 shows some examples
of the transduction rules learned by ISTG grammar
trained using rhyme scheme detection.
</bodyText>
<sectionHeader confidence="0.8982715" genericHeader="method">
5 Data selection via rhyme scheme
detection vs. adjacent lines
</sectionHeader>
<bodyText confidence="0.999985090909091">
We now compare two data selection approaches
for generating the training data for transduction
grammar induction via a rhyme scheme detection
module and choosing all adjacent lines in a verse.
We also briefly describe the training of the rhyme
scheme detection module and determine the efficacy
of our data selection scheme by training the ISTG
model, TG model and the PBSMT baseline on train-
ing data generated with and without employing the
rhyme scheme detection module. As the rule seg-
menting approach was intended to improve the flu-
ency as opposed to the rhyming nature of the re-
sponses, we only train the rule segmenting model
on the randomly chosen subset of all adjacent lines
in the verse. Further, adding adjacent lines as the
training data to the segmenting model maintains the
context of the responses generated thereby produc-
ing higher quality responses. The segmental trans-
duction grammar model was combined with the to-
ken based transduction grammar model trained on
data selected with and without using rhyme scheme
detection model.
</bodyText>
<subsectionHeader confidence="0.993921">
5.1 Rhyme scheme detection
</subsectionHeader>
<bodyText confidence="0.9999821">
Although our approach adapts a transduction gram-
mar induction model toward the problem of generat-
ing fluent and rhyming hip hop responses, it would
be undesirable to train the model directly on all the
successive lines of the verses—as done by Jiang and
Zhou (2008)—due to variance in hip hop rhyming
patterns. For example, adding successive lines of a
stanza which follows ABAB rhyme scheme as train-
ing instances to the transduction grammar causes in-
correct rhyme correspondences to be learned. The
fact that a verse (which is usually represented as
a separate paragraph) may contain multiple stanzas
of varying length and rhyme schemes worsens this
problem. Adding all possible pairs of lines in a verse
as training examples not only introduces a lot of
noise but also explodes the size of the training data
due to the large size of the verse.
We employ a rhyme scheme detection model (Ad-
danki and Wu, 2013) in order to select training in-
stances that are likely to rhyme. Lines belonging to
</bodyText>
<page confidence="0.997742">
107
</page>
<bodyText confidence="0.999992607142857">
the same stanza and marked as rhyming according
to the rhyme scheme detection model are added to
the training corpus. We believe that this data selec-
tion scheme will improve the rhyming associations
learned during the transduction grammar induction
thereby biasing the model towards producing fluent
and rhyming output.
The rhyme scheme detection model proposes a
HMM based generative model for a verse of hip hop
lyrics similar to Reddy and Knight (2011). However,
owing to the lack of well-defined verse structure in
hip hop, a number of hidden states corresponding to
stanzas of varying length are used to automatically
obtain a soft-segmentation of the verse. Each state
in the HMM corresponds to a stanza with a particu-
lar rhyme scheme such as AA, ABAB, AAAA while
the emissions correspond to the final words in the
stanza. We restrict the maximum length of a stanza
to be four to maintain a tractable number of states
and further only use states to represent stanzas whose
rhyme schemes could not be partitioned into smaller
schemes without losing a rhyme correspondence.
The parameters of the HMM are estimated using
the EM algorithm (Devijer, 1985) using the corpus
generated by taking the final word of each line in the
hip hop lyrics. The lines from each stanza that rhyme
with each other according to the Viterbi parse using
the trained model are added as training instances for
transduction grammar induction. As the source and
target languages are identical, each selected pair gen-
erates two training instances: a challenge-response
and a response-challenge pair.
The training data for the rhyme scheme detector
was obtained by extracting the end-of-line tokens
from each verse. However, upon data inspection we
noticed that shorter lines in hip hop stanzas are typi-
cally joined with a comma and represented as a sin-
gle line of text and hence all the tokens before the
commas were also added to the training corpus. We
obtained a corpus containing 4.2 million tokens cor-
responding to potential rhyming candidates compris-
ing of around 153,000 unique token types.
We evaluated the performance of our rhyme
scheme detector on the task of correctly labeling a
given verse with rhyme schemes. As our model is
completely unsupervised, we chose a random sam-
ple of 75 verses from our training data as our test set.
Two native English speakers who were frequent hip
hop listeners were asked to partition the verse into
stanzas and assign them with a gold standard rhyme
scheme. Precision and recall were aggregated for the
Viterbi parse of each verse against this gold standard
and f-score was calculated. The rhyme scheme de-
tection module employed in our data selection ob-
tained a precision of 35.81% and a recall of 57.25%,
giving an f-score of 44.06%.
</bodyText>
<subsectionHeader confidence="0.946585">
5.2 Training data selection via rhyme scheme
detection
</subsectionHeader>
<bodyText confidence="0.9999605">
We obtained around 600,000 training instances upon
extracting a training corpus using rhyme scheme de-
tection module as described in Section 5.1. We
added those lines that were adjacent and labeled as
rhyming by the rhyme scheme detector as training in-
stances resulting in a training corpus of size 200,000.
</bodyText>
<subsectionHeader confidence="0.998256">
5.3 Training data selection via adjacent lines
</subsectionHeader>
<bodyText confidence="0.999941111111111">
Considering all adjacent lines in a verse resulted in
a corpus with over 5 million training instances. In
order to ensure fair comparison of models trained
on data selected using rhyme scheme detection, we
randomly chose 200,000 training instances from the
generated corpus. The training corpus thus gener-
ated shared around 15% of training instances with
the corpus generated through our proposed data se-
lection scheme.
</bodyText>
<subsectionHeader confidence="0.871981">
5.4 Results: Rhyme scheme detection helps
</subsectionHeader>
<bodyText confidence="0.999921222222222">
Results in Table 1 indicate that using the rhyme
scheme detector for training data selection helps
produce significantly more fluent responses com-
pared to using adjacent lines. A possible explana-
tion for this could be that adding all adjacent lines as
training instances introduces a lot of noise into the
model which hurts the fluency of the responses gen-
erated. Also, the cumulative fraction of sentences
that were labeled good or ≥acceptable on the crite-
rion of rhyming is larger when rhyme scheme detec-
tion was used to generate the training data (although
the TG model trained on the corpus generated us-
ing adjacent lines produces a higher percentage of
rhyming responses that were rated good). Given the
significantly higher rate of response fluency when
using rhyme scheme detection, we argue that using
rhyme scheme detector for data selection is benefi-
cial.
</bodyText>
<page confidence="0.999481">
108
</page>
<tableCaption confidence="0.999818">
Table 3: English hip hop challenge-response examples.
</tableCaption>
<table confidence="0.78256945">
challenge man i die to see em all thun i just don&apos;t care
TG+RS in the sky and me the in polla and the you there
TG and the ride the me the and white the i the air
PBSMT+RS man i live to see em all i just don&apos;t care
challenge did a twelve year bid in the streets and held it down
TG+RS to the girls here kid the and to the thought the now
TG to the p&apos;s here did the a the i was the the
PBSMT+RS did a year in the streets and it down
challenge oh i believe in yesterday
TG+RS can you see the day
TG now you see the way
PBSMT+RS oh i believe in tomorrow
challenge what would i do
TG+RS just me and you
TG and you and you
PBSMT+RS what would you do
challenge cause you ain&apos;t going home till the early morn
TG+RS and the you this alone i i gotta on
TG and i you my on the a home we
PBSMT+RS cause you and your friends aint nothing but
</table>
<bodyText confidence="0.999891631578947">
It is also interesting to note from Table 1 that
ISTG+RS performs better than TG+RS indicating
that transduction grammar induced via interpolating
token based grammar and rule segmenting produces
better responses than token based transduction gram-
mar on both data selection schemes. Although the
average fraction of responses rated good on fluency
are slightly lower for ISTG+RS compared to TG+RS
(34.12% vs. 30.98%), the fraction of responses rated
&gt;acceptable are higher (61.18% vs. 57.64%). It is
important to note that the fraction of sentences rated
good and &gt;acceptable on rhyming are much larger
for ISTG+RS model. Although the fluency of the
responses generated by PBSMT+RS drastically im-
proves compared to PBSMT it still lags behind the
TG+RS and ISTG+RS models on both fluency and
rhyming. The results in Table 1 confirm our hypoth-
esis that off-the-shelf SMT systems are not guaran-
teed to be effective on our novel task.
</bodyText>
<subsectionHeader confidence="0.984771">
5.5 Challenge-response examples
</subsectionHeader>
<bodyText confidence="0.9998872">
Table 3 shows some of the challenges and the cor-
responding responses of PBSMT+RS, TG+RS and
TG model. While PBSMT+RS and TG+RS mod-
els generate responses reflecting a high degree of
fluency, the output of the TG contains a lot of ar-
ticles. It is interesting to note that TG+RS produces
responses comparable to PBSMT+RS despite being
a token based transduction grammar. However, PB-
SMT tends to produce responses that are too simi-
lar to the challenge. Moreover, TG models produce
responses that indeed rhyme better (shown in bold-
face). In fact, TG tries to rhyme words not only at the
end but also in middle of the lines, as our transduc-
tion grammar model captures structural associations
more effectively than the phrase-based model.
</bodyText>
<sectionHeader confidence="0.9721575" genericHeader="method">
6 Disfluency handling via disfluency
correction and filtering
</sectionHeader>
<bodyText confidence="0.9998252">
In this section, we compare the effect of two dis-
fluency mitigating strategies on the quality of the re-
sponses generated by the PBSMT baseline and token
based transduction grammar model with and without
using rhyme scheme detection.
</bodyText>
<subsectionHeader confidence="0.998737">
6.1 Correction vs. filtering
</subsectionHeader>
<bodyText confidence="0.999996166666667">
Error analysis of our initial runs showed a dis-
turbingly high proportion of responses generated by
our system that contained disfluencies with succes-
sive repetitions of words such as the and I. Upon in-
spection of data we noticed that the training lyrics
actually did contain such disfluencies and backing
vocal lines, amounting to 10% of our training data.
We therefore compared two alternative strategies to
tackle this problem. The first strategy involved fil-
tering out all lines from our training corpus which
contained such disfluencies. In the second strategy,
we implemented a disfluency detection and correc-
tion algorithm (for example, the the the, which fre-
quently occurred in the training corpus, was cor-
rected to simply the). The PBSMT baseline and the
TG model were trained on both the filtered and cor-
rected versions of the training corpus and the quality
of the responses were compared.
</bodyText>
<subsectionHeader confidence="0.999054">
6.2 Results: Disfluency correction helps
</subsectionHeader>
<bodyText confidence="0.999978153846154">
The results in Table 4 indicate that the disfluency
correction strategy outperforms the filtering strategy
for both TG and TG+RS models. For the model
TG+RS, disfluency correction generated 34.12%
good responses in terms of fluency, while the filter-
ing strategy produced only 28.63% good responses.
Similarly for the model TG, disfluency correction
produced 21.8% of responses with good fluency and
the filtering strategy produced only 17.25%. Dis-
fluency correction strategy produces higher fraction
of responses with &gt;acceptable fluency compared to
the filtering strategy for both TG and TG+RS mod-
els. This result is not surprising, as harshly pruning
</bodyText>
<page confidence="0.999516">
109
</page>
<tableCaption confidence="0.9870765">
Table 4: Effect of the disfluency correction strategies on fluency of the responses generated for the TG induction
models vs PBSMT baselines using both rhyme scheme detection and adjacent lines as the corpus selection method.
</tableCaption>
<table confidence="0.997309444444444">
model+disfluency strat. fluency (good) fluency (&gt;acceptable) rhyming (good) rhyming (&gt;acceptable)
PBSMT+filtering 4.3% 13.72% 3.53% 7.06%
PBSMT+correction 3.14% 4.70% 1.57% 4.31%
PBSMT+RS+filtering 31.76% 43.91% 12.15% 21.17%
PBSMT+RS+correction 30.59% 43.53% 1.96% 9.02%
TG+filtering 17.25% 46.27% 18.04% 33.33%
TG+correction 21.18% 54.51% 23.53% 39.21%
TG+RS+filtering 28.63% 56.86% 14.90% 34.51%
TG+RS+correction 34.12% 60.39% 20.00% 42.74%
</table>
<bodyText confidence="0.996932888888889">
the training corpus causes useful word association
information necessary for rhyming to be lost. Sur-
prisingly, for both PBSMT and PBSMT+RS models,
the disfluency correction has a negative effect on the
fluency level of the response but still falls behind TG
and TG+RS models. As disfluency correction yields
more fluent responses for TG and TG+RS models,
the results for ISTG and ISTG+RS models in Table
1 were obtained using disfluency correction strategy.
</bodyText>
<sectionHeader confidence="0.950519" genericHeader="method">
7 Maghrebi French hip hop
</sectionHeader>
<bodyText confidence="0.99997675">
We have begun to apply FREESTYLE to rap in lan-
guages other than English, taking advantage of
the language independence and linguistics-light ap-
proach of our unsupervised transduction grammar
induction methods. With no special adaption our
transduction grammar based model performs sur-
prisingly well, even with significantly smaller train-
ing data size and noisier data. These results across
different languages are encouraging as they can be
used to discover truly language independence as-
sumptions. We briefly describe our initial experi-
ments on Maghrebi French hip hop lyrics below.
</bodyText>
<subsectionHeader confidence="0.906981">
7.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999874">
We collected freely available French hip hop lyrics
of approximately 1300 songs. About 85% of the
songs were by Maghrebi French artists of Alge-
rian, Moroccan, or Tunisian cultural backgrounds,
while the remaining were by artists from the rest
of Francophonie. As the large majority of songs
are in Maghrebi French, the lyrics are sometimes
interspersed with romanized Arabic such as “De la
traversée du désert au bon couscous de Yéma” (Yéma
means My mother). Some songs also contain Berber
phrases, for instance “a yemmi ino, a thizizwith”
(which means my son, a bee). Furthermore, some
songs also contained English phrases in the style of
gangster rap such as “T&apos;es game over, game over... Le
son de Chicken wings”. As mentioned earlier, it is
complexity like this which dissuaded us from mak-
ing language specific assumptions in our model.
We extracted the end-of-line words and obtained
a corpus containing 120,000 tokens corresponding
to potential rhyming candidates with around 29,000
unique token types which was used as the training
data for the rhyme scheme detector module. For the
transduction grammar induction, the training data
contained about 47,000 sentence pairs selected us-
ing rhyme scheme detection.
</bodyText>
<sectionHeader confidence="0.662517" genericHeader="evaluation">
7.2 Results
</sectionHeader>
<bodyText confidence="0.999969176470588">
After human evaluation by native French speak-
ers and frequent hip hop listeners, our transduction
grammar based model generates about 9.2% and
14.5% of the responses that are rated good by the
human evaluators on the criterion of fluency and
rhyming respectively. About 30.2% and 38% of
the responses are rated as ≥acceptable. These num-
bers are encouraging given the noisy lyrics and much
smaller amount of training data. Some examples of
the challenge-response pairs and learned transduc-
tion rules in French are shown in Tables 5 and 6.
From Table 5, we can see that responses gener-
ated by the system rhyme with the challenges. The
first response is fluent and not only perfectly rhymes
with the challenge but also semantically valid. In the
second example, the model realizes a less common
AABA rhyme scheme through the response. The re-
</bodyText>
<page confidence="0.999021">
110
</page>
<tableCaption confidence="0.998954">
Table 5: French hip hop challenge-response examples.
</tableCaption>
<table confidence="0.9920632">
challenge Si je me trompe
response faut que je raconte
challenge Un jour je suis un livre
response et ce que je de vivre
challenge Pacha mama ils ne voient pas ta souffrance
response Combat ni leur de voulait de la décadence
challenge la palestine n&apos;etait pas une terre sans peuple.
response le darfour d&apos;autre de la guerre on est
challenge Une banlieue qui meut
response les yeux et
</table>
<tableCaption confidence="0.968797">
Table 6: Transduction rules for Maghrebi French hip hop.
</tableCaption>
<table confidence="0.878792666666667">
transduction grammar rule log prob.
A → terre/la guerre -9.4837
A → haine/peine -9.77056
A → mal/pays natal -10.6877
A → je frissonne/mi corazon -11.0931
A → gratteurs/rappeurs -11.7306
</table>
<bodyText confidence="0.999942545454546">
sponse in the third example, exhibits strong rhyming
with the challenge and both the challenge and the
response contain words like souffrance, combat and
décadence which are related. Similarly in the fourth
example, the challenge and response also contain se-
mantically related tokens which also rhyme. These
examples illustrate that our transduction grammar
formalism coupled with our rhyme scheme detection
module does capture the necessary correspondences
between lines of hip hop lyrics without assuming any
language specific resources.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999998416666667">
We presented a new machine learning approach for
improvising hip hop responses to challenge lyrics
by inducing stochastic transduction grammars, and
demonstrated that inducing the transduction rules by
interpolating bottom-up token based rule induction
and rule segmentation strategies outperforms a token
based baseline. We compared the performance of
our FREESTyLE model against a widely used off-the-
shelf phrase-based SMT model, showing that PB-
SMT falls short in tackling the noisy and highly un-
structured domain of hip hop lyrics. We showed that
the quality of responses improves when the training
data for the transduction grammar induction is se-
lected using a rhyme scheme detector. Several do-
main related oddities such as disfluencies and back-
ing vocals have been identified and some strategies
for alleviating their effects have been compared. We
also reported results on Maghrebi French hip hop
lyrics which indicate that our model works surpris-
ingly well with no special adaptation for languages
other than English. In the future, we plan to inves-
tigate alternative training data selection techniques,
disfluency handling strategies, search heuristics, and
novel transduction grammar induction models.
</bodyText>
<sectionHeader confidence="0.996416" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998880545454545">
This material is based upon work supported in part by
the Hong Kong Research Grants Council (RGC) research
grants GRF620811, GRF621008, GRF612806; by the
Defense Advanced Research Projects Agency (DARPA)
under BOLT contract no. HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-
0023; and by the European Union under the FP7 grant
agreement no. 287658. Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the RGC, EU, or DARPA.
</bodyText>
<sectionHeader confidence="0.99855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994225535714286">
Ananth Ramakrishnan A., Sankar KUPPAN, and
Lalitha Devi SOBHA. “Automatic generation of
Tamil lyrics for melodies.” Workshop on Computa-
tional Approaches to Linguistic Creativity (CALC-09).
2009.
Karteek ADDANKI and Dekai WU. “Unsupervised rhyme
scheme identification in hip hop lyrics using hidden
Markov models.” 1st International Conference on Sta-
tistical Language and Speech Processing (SLSP 2013).
2013.
Gabriele BARBIERI, François PACHET, Pierre ROy, and
Mirko DEGLI ESPOSTI. “Markov constraints for gen-
erating lyrics with style.” 20th European Conference
on Artificial Intelligence, (ECAI 2012). 2012.
David CHIANG. “Hierarchical phrase-based translation.”
Computational Linguistics, 33(2), 2007.
John COCKE. Programming languages and their compil-
ers: Preliminary notes. Courant Institute of Mathemat-
ical Sciences, New York University, 1969.
P.A. DEVIJER. “Baum’s forward-backward algorithm re-
visited.” Pattern Recognition Letters, 3(6), 1985.
D. GENZEL, J. USZKOREIT, and F. OCH. “Poetic statisti-
cal machine translation: rhyme and meter.” 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010). Association for Computa-
tional Linguistics, 2010.
E. GREENE, T. BODRUMLU, and K. KNIGHT. “Auto-
matic analysis of rhythmic poetry with applications
</reference>
<page confidence="0.985691">
111
</page>
<reference confidence="0.99967142">
to generation and translation.” 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010). Association for Computational Lin-
guistics, 2010.
Long JIANG and Ming ZHOu. “Generating Chinese
couplets using a statistical MT approach.” 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008). 2008.
Philipp KOEHN, Hieu HOANG, Alexandra BIRCH,
Chris CALLISON-BuRCH, Marcello FEDERICO, Nicola
BERTOLDI, Brooke COwAN, Wade SHEN, Christine
MORAN, Richard ZENS, Chris DYER, Ondrej BOJAR,
Alexandra CONSTANTIN, and Evan HERBST. “Moses:
Open source toolkit for statistical machine translation.”
Interactive Poster and Demonstration Sessions of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2007). June 2007.
Mark LIBERMAN. “Rap scholarship, rap meter, and the an-
thology of mondegreens.” http://languagelog.ldc.
upenn.edu/nll/?p=2824, December 2010. Accessed:
2013-06-30.
Franz Josef OCH. “Minimum error rate training in sta-
tistical machine translation.” 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
2003). July 2003.
Kishore PAPINENI, Salim ROuKOS, Todd WARD, and
Wei-Jing ZHu. “BLEU: a method for automatic evalu-
ation of machine translation.” 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02). July 2002.
S. REDDY and K. KNIGHT. “Unsupervised discovery of
rhyme schemes.” 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL HLT 2011), vol. 2. Association for
Computational Linguistics, 2011.
Jorma RISSANEN. “A universal prior for integers and es-
timation by minimum description length.” The Annals
of Statistics, 11(2), June 1983.
Markus SAERS, Karteek ADDANKI, and Dekai Wu. “From
finite-state to inversion transductions: Toward un-
supervised bilingual grammar induction.” 24th In-
ternational Conference on Computational Linguistics
(COLING 2012). December 2012.
Markus SAERS, Karteek ADDANKI, and Dekai Wu.
“Combining top-down and bottom-up search for un-
supervised induction of transduction grammars.” Sev-
enth Workshop on Syntax, Semantics and Structure in
Statistical Translation (SSST-7). June 2013.
Markus SAERS and Dekai Wu. “Reestimation of reified
rules in semiring parsing and biparsing.” Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-5). Association for Computational
Linguistics, June 2011.
Ray J. SOLOMONOFF. “A new method for discov-
ering the grammars of phrase structure languages.”
International Federation for Information Processing
Congress (IFIP). 1959.
M. SONDEREGGER. “Applications of graph theory to an
English rhyming corpus.” Computer Speech &amp; Lan-
guage, 25(3), 2011.
Andreas STOLCKE. “SRILM – an extensible language
modeling toolkit.” 7th International Conference on
Spoken Language Processing (ICSLP2002 - INTER-
SPEECH 2002). September 2002.
Dekai Wu. “A polynomial-time algorithm for statisti-
cal machine translation.” 34th Annual Meeting of the
Association for Computational Linguistics (ACL96).
1996.
Dekai Wu. “Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora.” Computa-
tional Linguistics, 23(3), 1997.
Dekai Wu. “Textual entailment recognition using inver-
sion transduction grammars.” Joaquin QuINONERO-
CANDELA, Ido DAGAN, Bernardo MAGNINI, and Flo-
rence D’ALCHE BuC (eds.), Machine Learning Chal-
lenges, Evaluating Predictive Uncertainty, Visual Ob-
ject Classification and Recognizing Textual Entail-
ment, First PASCAL Machine Learning Challenges
Workshop (MLCW 2005), vol. 3944 of Lecture Notes
in Computer Science. Springer, 2006.
Dekai Wu, Karteek ADDANKI, and Markus SAERS.
“FREESTYLE: A challenge-response system for hip
hop lyrics via unsupervised induction of stochastic
transduction grammars.” 14th Annual Conference of
the International Speech Communication Association
(Interspeech 2013). 2013a.
Dekai Wu, Karteek ADDANKI, and Markus SAERS.
“Modeling hip hop challenge-response lyrics as ma-
chine translation.” 14th Machine Translation Summit
(MT Summit XIV). 2013b.
Dekai Wu and Pascale FuNG. “Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora.” Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2005). Springer, 2005.
Richard ZENS and Hermann NEY. “A comparative study
on reordering constraints in statistical machine trans-
lation.” 41st Annual Meeting of the Association for
Computational Linguistics (ACL-2003). Association
for Computational Linguistics, 2003.
</reference>
<page confidence="0.998294">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.812468">
<title confidence="0.981307333333333">Learning to Freestyle: Hip Hop Challenge-Response Induction via Transduction Rule Segmentation Dekai WU Karteek ADDANKi Markus SAERs Meriem</title>
<author confidence="0.948634">Human Language Technology</author>
<affiliation confidence="0.998234">Department of Computer</affiliation>
<address confidence="0.910553">HKUST, Clear Water Bay, Hong Kong</address>
<email confidence="0.992953">{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk</email>
<abstract confidence="0.999728625">present a novel model, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. In this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. Our task is parambitious in that no use of any prior phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality ofthe responses generated by our model to those generated by an out-ofthe-box phrase based SMT system. We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. Finally, we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ananth Ramakrishnan A</author>
<author>Sankar KUPPAN</author>
<author>Lalitha Devi SOBHA</author>
</authors>
<title>Automatic generation of Tamil lyrics for melodies.”</title>
<date>2009</date>
<booktitle>Workshop on Computational Approaches to Linguistic Creativity (CALC-09).</booktitle>
<marker>A, KUPPAN, SOBHA, 2009</marker>
<rawString>Ananth Ramakrishnan A., Sankar KUPPAN, and Lalitha Devi SOBHA. “Automatic generation of Tamil lyrics for melodies.” Workshop on Computational Approaches to Linguistic Creativity (CALC-09). 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karteek ADDANKI</author>
<author>Dekai WU</author>
</authors>
<title>Unsupervised rhyme scheme identification in hip hop lyrics using hidden</title>
<date>2013</date>
<booktitle>Markov models.” 1st International Conference on Statistical Language and Speech Processing (SLSP</booktitle>
<marker>ADDANKI, WU, 2013</marker>
<rawString>Karteek ADDANKI and Dekai WU. “Unsupervised rhyme scheme identification in hip hop lyrics using hidden Markov models.” 1st International Conference on Statistical Language and Speech Processing (SLSP 2013). 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele BARBIERI</author>
<author>François PACHET</author>
<author>Pierre ROy</author>
<author>Mirko DEGLI ESPOSTI</author>
</authors>
<title>Markov constraints for generating lyrics with style.”</title>
<date>2012</date>
<booktitle>20th European Conference on Artificial Intelligence,</booktitle>
<location>ECAI</location>
<marker>BARBIERI, PACHET, ROy, ESPOSTI, 2012</marker>
<rawString>Gabriele BARBIERI, François PACHET, Pierre ROy, and Mirko DEGLI ESPOSTI. “Markov constraints for generating lyrics with style.” 20th European Conference on Artificial Intelligence, (ECAI 2012). 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David CHIANG</author>
</authors>
<title>Hierarchical phrase-based translation.”</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>CHIANG, 2007</marker>
<rawString>David CHIANG. “Hierarchical phrase-based translation.” Computational Linguistics, 33(2), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John COCKE</author>
</authors>
<title>Programming languages and their compilers: Preliminary notes.</title>
<date>1969</date>
<booktitle>Courant Institute of Mathematical Sciences,</booktitle>
<location>New York University,</location>
<marker>COCKE, 1969</marker>
<rawString>John COCKE. Programming languages and their compilers: Preliminary notes. Courant Institute of Mathematical Sciences, New York University, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A DEVIJER</author>
</authors>
<title>Baum’s forward-backward algorithm revisited.”</title>
<date>1985</date>
<journal>Pattern Recognition Letters,</journal>
<volume>3</volume>
<issue>6</issue>
<marker>DEVIJER, 1985</marker>
<rawString>P.A. DEVIJER. “Baum’s forward-backward algorithm revisited.” Pattern Recognition Letters, 3(6), 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D GENZEL</author>
<author>J USZKOREIT</author>
<author>F OCH</author>
</authors>
<title>Poetic statistical machine translation: rhyme and meter.”</title>
<date>2010</date>
<booktitle>2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010). Association for Computational Linguistics,</booktitle>
<marker>GENZEL, USZKOREIT, OCH, 2010</marker>
<rawString>D. GENZEL, J. USZKOREIT, and F. OCH. “Poetic statistical machine translation: rhyme and meter.” 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010). Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E GREENE</author>
<author>T BODRUMLU</author>
<author>K KNIGHT</author>
</authors>
<title>Automatic analysis of rhythmic poetry with applications to generation and translation.”</title>
<date>2010</date>
<booktitle>2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010). Association for Computational Linguistics,</booktitle>
<marker>GREENE, BODRUMLU, KNIGHT, 2010</marker>
<rawString>E. GREENE, T. BODRUMLU, and K. KNIGHT. “Automatic analysis of rhythmic poetry with applications to generation and translation.” 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010). Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long JIANG</author>
<author>Ming ZHOu</author>
</authors>
<title>Generating Chinese couplets using a statistical MT</title>
<date>2008</date>
<booktitle>approach.” 22nd International Conference on Computational Linguistics (COLING</booktitle>
<marker>JIANG, ZHOu, 2008</marker>
<rawString>Long JIANG and Ming ZHOu. “Generating Chinese couplets using a statistical MT approach.” 22nd International Conference on Computational Linguistics (COLING 2008). 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>“Moses</author>
</authors>
<title>Open source toolkit for statistical machine translation.”</title>
<date>2007</date>
<booktitle>Interactive Poster and Demonstration Sessions of the 45th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<marker>“Moses, 2007</marker>
<rawString>Philipp KOEHN, Hieu HOANG, Alexandra BIRCH, Chris CALLISON-BuRCH, Marcello FEDERICO, Nicola BERTOLDI, Brooke COwAN, Wade SHEN, Christine MORAN, Richard ZENS, Chris DYER, Ondrej BOJAR, Alexandra CONSTANTIN, and Evan HERBST. “Moses: Open source toolkit for statistical machine translation.” Interactive Poster and Demonstration Sessions of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007). June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark LIBERMAN</author>
</authors>
<title>Rap scholarship, rap meter, and the anthology of mondegreens.” http://languagelog.ldc.</title>
<date>2010</date>
<journal>Accessed:</journal>
<tech>upenn.edu/nll/?p=2824,</tech>
<pages>2013--06</pages>
<marker>LIBERMAN, 2010</marker>
<rawString>Mark LIBERMAN. “Rap scholarship, rap meter, and the anthology of mondegreens.” http://languagelog.ldc. upenn.edu/nll/?p=2824, December 2010. Accessed: 2013-06-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef OCH</author>
</authors>
<title>Minimum error rate training in statistical machine translation.”</title>
<date>2003</date>
<booktitle>41st Annual Meeting of the Association for Computational Linguistics (ACL2003).</booktitle>
<marker>OCH, 2003</marker>
<rawString>Franz Josef OCH. “Minimum error rate training in statistical machine translation.” 41st Annual Meeting of the Association for Computational Linguistics (ACL2003). July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore PAPINENI</author>
<author>Salim ROuKOS</author>
<author>Todd WARD</author>
<author>Wei-Jing ZHu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.”</title>
<date>2002</date>
<booktitle>40th Annual Meeting of the Association for Computational Linguistics (ACL02).</booktitle>
<marker>PAPINENI, ROuKOS, WARD, ZHu, 2002</marker>
<rawString>Kishore PAPINENI, Salim ROuKOS, Todd WARD, and Wei-Jing ZHu. “BLEU: a method for automatic evaluation of machine translation.” 40th Annual Meeting of the Association for Computational Linguistics (ACL02). July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S REDDY</author>
<author>K KNIGHT</author>
</authors>
<title>Unsupervised discovery of rhyme schemes.”</title>
<date>2011</date>
<booktitle>49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011),</booktitle>
<volume>2</volume>
<marker>REDDY, KNIGHT, 2011</marker>
<rawString>S. REDDY and K. KNIGHT. “Unsupervised discovery of rhyme schemes.” 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011), vol. 2. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma RISSANEN</author>
</authors>
<title>A universal prior for integers and estimation by minimum description length.”</title>
<date>1983</date>
<journal>The Annals of Statistics,</journal>
<volume>11</volume>
<issue>2</issue>
<marker>RISSANEN, 1983</marker>
<rawString>Jorma RISSANEN. “A universal prior for integers and estimation by minimum description length.” The Annals of Statistics, 11(2), June 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus SAERS</author>
<author>Karteek ADDANKI</author>
<author>Dekai Wu</author>
</authors>
<title>From finite-state to inversion transductions: Toward unsupervised bilingual grammar induction.”</title>
<date>2012</date>
<booktitle>24th International Conference on Computational Linguistics (COLING</booktitle>
<marker>SAERS, ADDANKI, Wu, 2012</marker>
<rawString>Markus SAERS, Karteek ADDANKI, and Dekai Wu. “From finite-state to inversion transductions: Toward unsupervised bilingual grammar induction.” 24th International Conference on Computational Linguistics (COLING 2012). December 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus SAERS</author>
<author>Karteek ADDANKI</author>
<author>Dekai Wu</author>
</authors>
<title>Combining top-down and bottom-up search for unsupervised induction of transduction grammars.”</title>
<date>2013</date>
<booktitle>Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-7).</booktitle>
<marker>SAERS, ADDANKI, Wu, 2013</marker>
<rawString>Markus SAERS, Karteek ADDANKI, and Dekai Wu. “Combining top-down and bottom-up search for unsupervised induction of transduction grammars.” Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-7). June 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus SAERS</author>
<author>Dekai Wu</author>
</authors>
<title>Reestimation of reified rules in semiring parsing and biparsing.”</title>
<date>2011</date>
<booktitle>Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-5). Association for Computational Linguistics,</booktitle>
<marker>SAERS, Wu, 2011</marker>
<rawString>Markus SAERS and Dekai Wu. “Reestimation of reified rules in semiring parsing and biparsing.” Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-5). Association for Computational Linguistics, June 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray J SOLOMONOFF</author>
</authors>
<title>A new method for discovering the grammars of phrase structure languages.” International Federation for Information Processing Congress (IFIP).</title>
<date>1959</date>
<marker>SOLOMONOFF, 1959</marker>
<rawString>Ray J. SOLOMONOFF. “A new method for discovering the grammars of phrase structure languages.” International Federation for Information Processing Congress (IFIP). 1959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M SONDEREGGER</author>
</authors>
<title>Applications of graph theory to an English rhyming corpus.”</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>SONDEREGGER, 2011</marker>
<rawString>M. SONDEREGGER. “Applications of graph theory to an English rhyming corpus.” Computer Speech &amp; Language, 25(3), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas STOLCKE</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.”</title>
<date>2002</date>
<booktitle>7th International Conference on Spoken Language Processing (ICSLP2002 - INTERSPEECH</booktitle>
<marker>STOLCKE, 2002</marker>
<rawString>Andreas STOLCKE. “SRILM – an extensible language modeling toolkit.” 7th International Conference on Spoken Language Processing (ICSLP2002 - INTERSPEECH 2002). September 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.”</title>
<date>1996</date>
<booktitle>34th Annual Meeting of the Association for Computational Linguistics (ACL96).</booktitle>
<contexts>
<context position="19214" citStr="Wu (1996)" startWordPosition="3081" endWordPosition="3082">meter is chosen empirically based on the responses generated on a small development set. We compare the performance of inducing a token based ITG versus inducing a segmental ITG using interpolated bottom-up token based rule induction and top-down rule segmentation. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an off-the-shelf phrase based SMT system. 4.2 Decoding heuristics We use our in-house ITG decoder implemented according to the algorithm mentioned in Wu (1996) for the generating responses to challenges by decoding with the trained transduction grammars. The decoder uses a CKY-style parsing algorithm (Cocke, Algorithm 1 Iterative rule segmenting learning driven by minimum description length. 1: 4b ▷ The ITG being induced 2: repeat 3: Ssum +— 0 4: bs +— collect_biaffixes(4b) 5: bS +— [] 6: for all b E bs do 7: S +— eval_dl(b, 4b) 8: if S &lt; 0 then 9: bS +— [bS, (b, S)] 10: sort_by_delta(bS) 11: for all (b, S) E bS do 12: S′ +— eval_dl(b, 4b) 13: if S′ &lt; 0 then 14: 4b +— make_segmentations(b, 4b) 15: Ssum +— Ssum + S′ 16: until Ssum &gt; 0 17: return 4b 1</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. “A polynomial-time algorithm for statistical machine translation.” 34th Annual Meeting of the Association for Computational Linguistics (ACL96). 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.”</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="14918" citStr="Wu, 1997" startWordPosition="2391" endWordPosition="2392">ventional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent outputs. 4 Interpolated segmenting model vs. token based model We compare the performance of transduction grammars induced via interpolated token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framewor</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. “Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.” Computational Linguistics, 23(3), 1997.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Textual entailment recognition using inversion transduction grammars.”</title>
<date>2006</date>
<booktitle>Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop (MLCW 2005),</booktitle>
<volume>3944</volume>
<editor>Joaquin QuINONEROCANDELA, Ido DAGAN, Bernardo MAGNINI, and Florence D’ALCHE BuC (eds.),</editor>
<publisher>Springer,</publisher>
<contexts>
<context position="15231" citStr="Wu, 2006" startWordPosition="2437" endWordPosition="2438"> token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A&gt;. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based</context>
</contexts>
<marker>Wu, 2006</marker>
<rawString>Dekai Wu. “Textual entailment recognition using inversion transduction grammars.” Joaquin QuINONEROCANDELA, Ido DAGAN, Bernardo MAGNINI, and Florence D’ALCHE BuC (eds.), Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop (MLCW 2005), vol. 3944 of Lecture Notes in Computer Science. Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Karteek ADDANKI</author>
<author>Markus SAERS</author>
</authors>
<title>FREESTYLE: A challenge-response system for hip hop lyrics via unsupervised induction of stochastic transduction</title>
<date>2013</date>
<booktitle>grammars.” 14th Annual Conference of the International Speech Communication Association (Interspeech</booktitle>
<contexts>
<context position="8731" citStr="Wu et al., 2013" startWordPosition="1373" endWordPosition="1376">dels in Section 4. We compare our data selection schemes and disfluency handling strategies in Sections 5 and 6. Finally, in 2Particularly the repetitive chants, exclamations, and interjections in hip hop “hype man” style backing vocals. 103 Section 7 we describe some preliminary results obtained using our approach on improvising hip hop responses in French and conclude in Section 8. 2 Related work Although a few attempts have been made to apply statistical NLP learning methods to unconventional domains, FREESTYLE is among the first to tackle the genre of hip hop lyrics (Addanki and Wu, 2013; Wu et al., 2013a,b). Our preliminary work suggested the need for further research to identify models that capture the correct generalizations to be able to generate fluent and rhyming responses. As a step towards this direction, we contrast the performance of interpolated bottom-up token based rule induction and top-down segmenting transduction grammar models and token based transduction grammar models. We briefly describe some of the past work in statistical NLP on unconventional domains below. Most of the past work either uses some form of prior linguistic knowledge or enforces harsher constraints such as </context>
</contexts>
<marker>Wu, ADDANKI, SAERS, 2013</marker>
<rawString>Dekai Wu, Karteek ADDANKI, and Markus SAERS. “FREESTYLE: A challenge-response system for hip hop lyrics via unsupervised induction of stochastic transduction grammars.” 14th Annual Conference of the International Speech Communication Association (Interspeech 2013). 2013a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekai Wu</author>
<author>Karteek ADDANKI</author>
<author>Markus SAERS</author>
</authors>
<title>Modeling hip hop challenge-response lyrics as machine translation.”</title>
<booktitle>14th Machine Translation Summit (MT Summit XIV). 2013b.</booktitle>
<marker>Wu, ADDANKI, SAERS, </marker>
<rawString>Dekai Wu, Karteek ADDANKI, and Markus SAERS. “Modeling hip hop challenge-response lyrics as machine translation.” 14th Machine Translation Summit (MT Summit XIV). 2013b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale FuNG</author>
</authors>
<title>Inversion transduction grammar constraints for mining parallel sentences from quasi-comparable corpora.”</title>
<date>2005</date>
<booktitle>Second International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<publisher>Springer,</publisher>
<marker>Wu, FuNG, 2005</marker>
<rawString>Dekai Wu and Pascale FuNG. “Inversion transduction grammar constraints for mining parallel sentences from quasi-comparable corpora.” Second International Joint Conference on Natural Language Processing (IJCNLP 2005). Springer, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard ZENS</author>
<author>Hermann NEY</author>
</authors>
<title>A comparative study on reordering constraints</title>
<date>2003</date>
<booktitle>in statistical machine translation.” 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003). Association for Computational Linguistics,</booktitle>
<marker>ZENS, NEY, 2003</marker>
<rawString>Richard ZENS and Hermann NEY. “A comparative study on reordering constraints in statistical machine translation.” 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003). Association for Computational Linguistics, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.999411">
Cross-Lingual Discriminative Learning of Sequence Models
with Posterior Regularization
</title>
<author confidence="0.910861">
Kuzman Ganchev
</author>
<affiliation confidence="0.862143">
Google Research
</affiliation>
<address confidence="0.970845">
76 9th Avenue
New York, NY 10011
</address>
<email confidence="0.998587">
kuzman@google.com
</email>
<sectionHeader confidence="0.994781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999393133333333">
We present a framework for cross-lingual
transfer of sequence information from a
resource-rich source language to a resource-
impoverished target language that incorporates
soft constraints via posterior regularization. To
this end, we use automatically word aligned
bitext between the source and target language
pair, and learn a discriminative conditional ran-
dom field model on the target side. Our poste-
rior regularization constraints are derived from
simple intuitions about the task at hand and
from cross-lingual alignment information. We
show improvements over strong baselines for
two tasks: part-of-speech tagging and named-
entity segmentation.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99968052631579">
Supervised systems for NLP tasks are available for
a handful of languages. These systems achieve high
accuracy for many applications; a variety of robust
algorithms to train them from labeled data have been
developed. Here, we focus on learning sequence mod-
els for the languages that lack annotated resources.
For a given resource-poor target language of inter-
est, we assume that parallel data with a resource-rich
source language exists. With the help of this bitext
and a supervised system in the source language, we
infer constraints over the label distribution in the tar-
get language, and train a discriminative model using
posterior regularization (Ganchev et al., 2010).
Cross-lingual learning of structured prediction
models via parallel data has been applied for several
natural language processing problems, including part-
of-speech (POS) tagging (Yarowsky and Ngai, 2001),
syntactic parsing (Hwa et al., 2005) and named-entity
recognition (Kim et al., 2012). These methods are
</bodyText>
<note confidence="0.77109975">
Dipanjan Das
Google Research
76 9th Avenue
New York, NY 10011
</note>
<email confidence="0.968151">
dipanjand@google.com
</email>
<bodyText confidence="0.999320162162162">
useful in several ways. First, they help in fast proto-
typing of natural language systems for new languages
that do not boast human annotations. Second, the
output of such systems could be used to bootstrap
more extensive human annotation projects (Vlachos,
2006). Finally, they are significantly more accurate
than purely unsupervised systems (McDonald et al.,
2011; Das and Petrov, 2011).
Recently, T¨ackstr¨om et al. (2013) presented a tech-
nique for coupling token constraints derived from pro-
jected cross-lingual information and type constraints
derived from noisy tag dictionaries to learn POS tag-
gers. Although this technique resulted in state-of-
the-art weakly supervised taggers, the authors used a
heuristic to combine the aforementioned two sources
of constraints: the dictionary constraints pruned the
tagger’s search space, and the intersected token-level
projections were treated as hard observations. On
the other hand, Ganchev et al. (2009) presented a
framework for learning weakly-supervised systems
(in their case, dependency parsers) that incorporated
alignment-based information too, but used the cross-
lingual information only as soft constraints, via poste-
rior regularization. The advantage of this framework
lay in the fact that the projections were only trusted
to a certain degree, determined by a strength hyper-
parameter, which unfortunately the authors did not
have an elegant way to tune. In this paper, we ex-
ploit the better aspects of these two lines of work:
first, we extend the framework of T¨ackstr¨om et al.
by treating the alignment-based projections only as
soft constraints (see §3.4); second, we choose the
constraint strength by utilizing the tag ambiguity of
tokens for a given resource-poor language (see §6.1).
Other than validating our framework on part-of-
speech tagging, we experiment on named-entity seg-
mentation in a cross-lingual framework. For this
</bodyText>
<page confidence="0.936034">
1996
</page>
<note confidence="0.731036">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1996–2006,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997775">
task, we present a novel method to perform high-
precision phrase-level entity transfer (§5.2.2); we
also provide ways to balance precision and recall
with posterior regularization (§6.2) by incorporating
intuitive soft constraints during learning. We mea-
sure performance on standard benchmark datasets for
both of these tasks, and report improvements over
state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.996371" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999888333333333">
Cross-lingual projection methods can be classified
by their use of two very broad ideas. The first idea
utilizes parallel data to create full or partial annota-
tions in the low-resource language and trains from
this data. This was popularized by Yarowsky and
Ngai (2001) who applied this to POS tagging and
shallow parsing. It was later applied to parsing (Hwa
et al., 2005) and named entity recognition (Kim et
al., 2012). The second idea, first proposed by Ze-
man and Resnik (2008) and applied more broadly
by McDonald et al. (2011), is to train a model on
a resource-rich language and apply it to a resource-
poor language directly. The disparity between the
languages is mitigated by the choice of features. In
addition to cross-lingual projection, purely unsuper-
vised methods have been explored but with limited
success (Christodoulopoulos et al., 2010). Here, we
resort to cross-lingual projection and incorporate the
first idea; we also follow Li et al. (2012) and use
Wiktionary to further constrain the POS tagging task.
Our learning setup is similar to that of Ganchev et
al. (2009), who also use posterior regularization but
focus on dependency parsing alone. Our work differs
with respect to the tasks, the learning algorithm and
also in that we use corpus-wide constraints, while
Ganchev et al. use one constraint per sentence. For
the part-of-speech tagging task, our approach is sim-
ilar to that of T¨ackstr¨om et al. (2013), who use an
almost identical learning setup but only make use of
hard constraints. By relaxing these constraints, we
allow the model to identify and ignore inconsistently
labeled parts of sentences, and achieve better results
using identical training and test data.
</bodyText>
<sectionHeader confidence="0.99489" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.96981075">
We give an overview of our approach, and present the
details of our model used for cross-lingual learning.
Algorithm 1 Cross-Lingual Learning with Posterior
Regularization
</bodyText>
<listItem confidence="0.861845444444444">
Require: Parallel source and target language data
De and Df, source language model (M)e, task-
specific target language constraints C.
Ensure: Of, a set of target language parameters.
1: D_&amp;quot;f +— word-align-bitext(De, Df )
2: De label-supervised(De)
3: Df project-and-filter-labels(De`-&apos;f, De)
4: Of learn-posterior-constrained(Df,C)
5: Return Of
</listItem>
<subsectionHeader confidence="0.99738">
3.1 General Overview
</subsectionHeader>
<bodyText confidence="0.99998795">
The general overview of our framework is provided
in Algorithm 1. The process of learning parame-
ters for a target language for a given task involves
four subtasks. First, we run word alignment over a
large corpus of parallel data between the resource-
rich source language and the resource-impoverished
target language (see §4.3). In the second step, we
use a supervised model to label the source side of
the parallel data (see §5.1.1 and §5.2.1). The third
step involves a task-specific word-alignment filter-
ing step; this step involves heuristics for which we
use cues from prior state-of-the-art (Das and Petrov,
2011; T¨ackstr¨om et al., 2013, see §5.1.2) and also
introduce some novel ones for the NE segmentation
problem (see §5.2.2). In the fourth step, we train a
linear chain conditional random field (Lafferty et al.,
2001, CRF henceforth) using posterior regularization.
In the next subsection, we turn to a brief summary of
this final step of estimating parameters of a discrimi-
native model with posterior regularization.
</bodyText>
<subsectionHeader confidence="0.999991">
3.2 Learning with Posterior Regularization
</subsectionHeader>
<bodyText confidence="0.977296181818182">
In this work, we utilize discriminative CRF mod-
els, and use posterior regularization (PR) to optimize
their parameters. As a framework, posterior regular-
ization is described in detail by Ganchev et al. (2010).
However in our work, we adopt a different optimiza-
tion technique; in what follows, we summarize the
optimization algorithm in the context of CRF models.
Let x be an input sentence with a set of possible
labelings Y(x) and let y E Y(x) be a particular la-
beling for sentence x. We use bold capital letters
X = {xi ... xnJ and Y = {yi ... ynJ to denote
</bodyText>
<page confidence="0.972388">
1997
</page>
<bodyText confidence="0.974793">
a corpus of sentences and labelings for the corpus
respectively. A CRF models the probability distri-
bution over possible labels for a sentence pθ(y|x)
as:
</bodyText>
<equation confidence="0.995191">
pθ(y  |x) ∝ exp(θ · f(x,y)) (1)
</equation>
<bodyText confidence="0.9974328">
where θ are the model parameters and f(.) is a fea-
ture function. The model examines sentences in iso-
lation, and the probability of a particular labeling for
a corpus is defined as a product over the individual
sentences:
</bodyText>
<equation confidence="0.9957395">
pθ(Y  |X) = � pθ(y  |x). (2)
(x,y)∈(X,Y)
</equation>
<bodyText confidence="0.905626666666667">
Traditionally, CRF models have been trained to op-
timize the regularized log-likelihood of the training
data
</bodyText>
<equation confidence="0.994696">
max L(θ) = max
θ θ log(pθ(Y  |X)) − γ ||θ ||(3)
</equation>
<bodyText confidence="0.9997252">
In our setting, we do not have a fully labeled cor-
pus, but we have constraints on the distribution of
labels. For example, we may know that a particular
token could be labeled only by a label inventory li-
censed by a dictionary, or that a labeling projected
from a source language is usually (but not always)
correct. We define these constraints in terms of fea-
ture expectations. Let q(Y) be a distribution over all
possible labelings of our corpus Y(X). Let Q be a
set of distributions defined by:
</bodyText>
<equation confidence="0.996583">
Q = {q(Y) : Eq[φ(X, Y)] ≤ b}, (4)
</equation>
<bodyText confidence="0.999766205882353">
where φ is a constraint feature function and b is a vec-
tor of non-negative values that serve as upper bounds
to the expectations of every constraint feature. The
vector b is used to encode our prior knowledge about
desirable distributions q(Y). Note that the constraint
features φ are not related to the model features f. The
model features, together with the model parameters θ
define the CRF model; the model features need to be
computed at inference time for prediction. By con-
trast, the constraint features and their corresponding
constraint values are used to define our training ob-
jective function (and are only used during learning).
The PR objective with no labeled data is defined with
respect to Q as:
where KL(Q||p) = minq∈Q KL(q||p) is the
KL-divergence (Kullback and Leibler, 1951)
from a set to a point. Note that as we add more
constraints, Q becomes a smaller set. In the
limit, Q = {q(Y) : q( ˆY) = 1} contains just one
distribution concentrated on a single labeling ˆY.
In this limit, posterior regularization degenerates
into the convex log-likelihood objective normally
used for supervised data JQ(θ) = L(θ). However,
in the general case, the PR objective JQ is not
necessarily convex. Prior work, including that of
Ganchev et al. propose an algorithm similar to
Expectation-Maximization (Dempster et al., 1977,
EM henceforth) to optimize JQ, but we follow Liang
et al. (2009) in using a schochastic update-based
algorithm described below.
Note: To make it easier to reason about constraint
values b, we scale constraint features φ(X, Y) to lie
in [0, 1] by computing maxY φ(X, Y) for the corpus
to which φ is applied.
</bodyText>
<subsectionHeader confidence="0.998604">
3.3 Optimization
</subsectionHeader>
<bodyText confidence="0.999892142857143">
The optimization procedure proposed by
Ganchev et al. is similar to the EM algorithm,
and computes the minimization minq∈Q KL(q||p)
at each step, using its dual form; this minimization is
convex, so there is no duality gap. They show that
the optimal primal variables q∗(Y) are related to the
optimal dual variables λ∗ by:
</bodyText>
<equation confidence="0.9977245">
q∗(Y) = pθ(Y|X)e−λ*·φ(X,Y) .(6)
Z(λ∗)
</equation>
<bodyText confidence="0.646749">
where Z(λ∗) is the normalizer. The dual problem is
given by:
</bodyText>
<equation confidence="0.958765">
max
λ≥0 −b · λ − log Z(λ). (7)
</equation>
<bodyText confidence="0.943068">
Substituting Eq. 7 into the objective in Eq. 5, we get
the saddle-point problem:
</bodyText>
<equation confidence="0.9872555">
pθ(Y|X)e−λ*·φ(X,Y)
− γ ||θ ||. (8)
</equation>
<bodyText confidence="0.998145666666667">
To optimize the above objective function, we need to
compute partial derivatives with respect to both θ and
λ. First, to compute the partial derivatives of Eq. 8
</bodyText>
<figure confidence="0.753035545454545">
PR: max
θ
JQ(θ) =
max
−KL(Qkpθ(Y  |X)) − γ ||θ ||(5)
θ
�
b · λ + log
Y
max min
θ λ≥0
</figure>
<page confidence="0.959871">
1998
</page>
<bodyText confidence="0.999383571428571">
with respect to θ, we need to find expectations of the
model features f given the current distribution pθ and
the constraint distribution q. To perform tractable
inference, a linear-chain CRF model assumes that
the feature function factorizes according to smaller
parts; in particular the factorization uses the follow-
ing structure:
</bodyText>
<equation confidence="0.982063">
f(x, y) = � f(x, yi, yi−1) (9)
i
</equation>
<bodyText confidence="0.9999942">
where i ranges over the tokens in the sentence. This
factorization allows us to efficiently compute expec-
tations over the labels yi and label-pairs (yi, yi+1).
To compute the partial gradient of Eq. 8 with respect
to λ, we need to find the expectations of the con-
straint features φ. In order to be tractable here too,
we ensure that φ also factorize according to the same
structure as f. Therefore, the gradient computation
w.r.t. λ turns out to be straightforward.
For all the experiments in this paper, we optimize
Eq. 8 using stochastic projected gradient. For each
training sentence, we compute the gradient of θ and
λ with respect to Eq. 8, take a gradient step in each
one, and truncate the negative entries in λ to zero.
We use a step size of 1 for all experiments.1
</bodyText>
<subsectionHeader confidence="0.997436">
3.4 Relationship with T¨ackstr¨om et al. (2013)
</subsectionHeader>
<bodyText confidence="0.995224242424243">
In this subsection, we focus briefly on the relationship
between this work and the work of T¨ackstr¨om et al.
(2013), who focused on constrained learning of POS
taggers. T¨ackstr¨om et al. define constrained lattices
and train by optimizing marginal conditional log-
likelihood. In our notation, they define their objective
as:
pθ(Y|X) − γIIθII (10)
where �Y(X) are the constrained lattices of label se-
quences that agree with both a dictionary and cross-
lingually projected POS tags for each sentence of
the training corpus. Let us define a constraint fea-
ture φ(X, Y) which counts the number of tags in Y
which are outside the constraint set Y(X) and require
φ(X, Y) &lt; 0. Note that,
arg min KL(q||pθ(Y|X)) s.t. φ(X,Y) &lt; 0
q
1Note that we did not implement regularization of B in the
stochastic optimizer, hence our PR objective (Eq. 8) was unregu-
larized; however, the baseline models use 22 regularization.
gives the same distribution as Eq. 10. Given this
equivalence, it is easy to see that the gradient of
Eq. 5 with respect to θ is the same as that of Eq. 10.
By using such constrained lattices, T¨ackstr¨om et al.
avoid maintaining a parameter for the constraint, but
lose the ability to relax the constraint value and al-
low some probability mass outside the pruned lat-
tice. Their paper also differs from ours in that they
use L-BFGS (Liu and Nocedal, 1989), while we use
an online optimization procedure. Since the objec-
tives are non-convex, the two optimization techniques
could lead to different local optima even when the
constraint is not relaxed (b = 0).
</bodyText>
<sectionHeader confidence="0.966231" genericHeader="method">
4 Tasks and Data
</sectionHeader>
<bodyText confidence="0.999961833333333">
In this section, we focus on the nature of the two tasks
that we attempt to solve, describe the source language
datasets we use to train our supervised models for
transfer, the target language datasets on which we
evaluate our models and the parallel data we use for
cross-lingual transfer.
</bodyText>
<subsectionHeader confidence="0.998122">
4.1 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999995545454545">
First, we focus on the task of part-of-speech tagging.
Following previous work on cross-lingual POS tag-
ging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013),
we adopt the POS tags of Petrov et al. (2012), ver-
sion 1.03;2 we use the October 2012 version of Wik-
tionary3 as our tag dictionary.
After pruning the search space with the dictionary,
we place soft constraints derived by projecting POS
tags across word alignments. The alignments are fil-
tered for confidence (see §5.1.2), but we also filter
any projected tags that are not licensed by the dictio-
nary. The example in Figure 1 illustrates why this
dictionary filtering step is important. Consider the
English-Spanish phrase pair from Figure 1, which
we observed in our training data. Our supervised tag-
ger correctly tags Asian with the ADJ tag as shown
in the figure. Asian is aligned to the Spanish word
Asia, which should be tagged NOUN. Because the
Spanish Wiktionary only allows the NOUN tag for
Asia, we do not project the ADJ tag from the English
word Asian. By contrast, we do project the NOUN
tag from the English word sponges to the Spanish
</bodyText>
<footnote confidence="0.997625">
2http://code.google.com/p/universal-pos-tags
3http://meta.wikimedia.org/wiki/Wiktionary
</footnote>
<figure confidence="0.559967166666667">
�
max log
θ
YEY(X)
1999
ADP ADJ NOUN
</figure>
<figureCaption confidence="0.9985146">
Figure 1: An English (top) – Spanish (bottom) phrase pair
from our parallel data. The correct POS tags and NER
annotations are shown for the English phrase. Word align-
ments are shown as links between English and Spanish
words.
</figureCaption>
<bodyText confidence="0.996233875">
word esponjas because this tag is in our dictionary
for the latter word.
For all our POS experiments, we evaluate on sev-
enteen target languages. Fifteen of these languages
were part of the experiments conducted by T¨ackstr¨om
et al. (2013); we add Arabic and Hungarian to the set.
The first column of Table 1 lists all seventeen lan-
guages using their two-letter abbreviation codes from
the ISO 639-1 standard. The evaluation datasets cor-
respond to the test sets from the CoNLL shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al., 2007). For French we use the treebank
of Abeill´e et al. (2003). English serves as our source
language and we use the Penn Treebank (Marcus et
al., 1993, with tags mapped to the universal tags) to
train our supervised source-side model.
</bodyText>
<subsectionHeader confidence="0.947619">
4.2 Named-Entity Segmentation
</subsectionHeader>
<bodyText confidence="0.999755964285714">
Second, we investigate the task of named-entity seg-
mentation. The goal of this task is to identify the
boundaries of named-entities for a given language
without classifying them by type. This is the un-
labeled version of named-entity recognition, and is
more amenable to cross-lingual supervision. To un-
derstand why that is, consider again the example
from Figure 1. The English supervised NE tagger
correctly identifies Asian as a named entity of type
MISC (miscellaneous). The word-alignments sug-
gest we should transfer this annotation to the Spanish
word Asia which is also an entity. However, this
should be labeled LOC (location) according to the
CoNLL annotation guidelines (Tjong Kim Sang and
De Meulder, 2003). Because syntactic variations
of this kind are common, it makes cross-lingual de-
tection of NE boundaries as well as types hard.4 In
this paper, we focus on named-entity segmentation
alone, consider the full NER task out of scope. We
use English as a source language and train a super-
vised English named-entity tagger with the labels in
place, using the CoNLL 2003 shared task data (Tjong
Kim Sang and De Meulder, 2003). We project the
spans using the maximal-span heuristic (Yarowsky
and Ngai, 2001). We project into Dutch, German and
Spanish and evaluate on the standard CoNLL 2002
and 2003 shared task data sets (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003).
</bodyText>
<subsectionHeader confidence="0.998473">
4.3 Parallel Data
</subsectionHeader>
<bodyText confidence="0.999979">
For both tasks we use parallel data gathered automat-
ically from the web using the method of Uszkoreit
et al. (2010), as well as data from Europarl (Koehn,
2005) and the UN parallel corpus (UN, 2006), for
languages covered by the latter two corpora. The
parallel sentences are word aligned with the aligner
of DeNero and Macherey (2011). The size of the
parallel corpus is larger than we need for our tasks,
so we follow T¨ackstr¨om et al. (2013) in sampling
500k tokens for POS tagging and 10k sentences for
named-entity segmentation (see §5.1.2 and §5.2.2).
</bodyText>
<sectionHeader confidence="0.99864" genericHeader="method">
5 Experimental Details
</sectionHeader>
<bodyText confidence="0.999976666666667">
In this section, we provide details about task-
specific implementations of the supervised source-
side model and the word-alignment filtering tech-
niques (steps 2 and 3 in Algorithm 1 respectively);
we also briefly describe the setup of the cross-lingual
experiments for each task.
</bodyText>
<subsectionHeader confidence="0.991411">
5.1 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.9999134">
We first focus on the experimental setup for the POS
tagging task. When describing feature sets we refer to
features conjoined with just a single tag as emission
features and with consecutive tag pairs as transition
features.
</bodyText>
<footnote confidence="0.9826526">
4We tried using English and German gazetteers from the
CoNLL 2002 and 2003 shared tasks as a label dictionary similar
to the way we use Wiktionary for POS tagging. This did not work
well because the CoNLL gazetteers do not have good coverage
on our parallel datasets, which we use for training.
</footnote>
<table confidence="0.386499666666667">
of [ Asian ] sponges
MISC
de las esponjas de Asia
</table>
<page confidence="0.706621">
2000
</page>
<subsubsectionHeader confidence="0.683459">
5.1.1 Supervised Source-Side Model
</subsubsectionHeader>
<bodyText confidence="0.999978526315789">
We tag the English side of our parallel data with
a supervised first-order linear-chain CRF POS tag-
ger. We use standard features for tagging. Our emis-
sion features are a bias feature, the current word,
its suffixes up to length 3, its capitalization shape,
whether it contains a hyphen, digit or punctuation
and its cluster identity. Our transition features are a
bias feature and the cluster identities of each word
in the transition. For the cluster-based features, we
use monolingual word clusters induced with the ex-
change algorithm of Uszkoreit and Brants (2008),
which implements the same objective as Brown et al.
(1992); these clusters have shown improvements for
sequence labeling tasks (Turian et al., 2010; T¨ack-
str¨om et al., 2012). We set the number of clusters to
256 for both the source side tagger and all the other
languages. On Section 23 of the WSJ section of the
Penn Treebank, the source side tagger achieves an
accuracy of 96.2%.
</bodyText>
<subsubsectionHeader confidence="0.919484">
5.1.2 Word Alignment Filtering
</subsubsectionHeader>
<bodyText confidence="0.999396142857143">
Following T¨ackstr¨om et al. (2013), we tag the En-
glish side of our parallel data using the source-side
POS tagger, intersect the word alignments and filter
alignments with confidence below 0.95. We sam-
ple 500,000 tokens of target side sentences for each
language, and use this as training data for learning
weakly-supervised taggers.
</bodyText>
<subsubsectionHeader confidence="0.546614">
5.1.3 Setup for Cross-Lingual Experiments
</subsubsectionHeader>
<bodyText confidence="0.999877">
Following T¨ackstr¨om et al. (2013) we use a re-
duced feature set for the cross-lingual models. The
emission features are the same as the supervised
model but without the punctuation feature,5 and we
use only the bias transition feature. Because this
limits the ability of the model to use context, we
also experiment with an extended feature set that
has transition features for the clusters of each word
in the transition, and their suffixes up to length 3.
We refer to the extended-feature models as “BASE+”
and “PR+” to distinguish them from the models with
fewer features, labeled “BASE” and “PR”.
We train BASE and BASE+ using L-BFGS with
an E2 regularization weight of 1 for 100 iterations to
reproduce the setup used by T¨ackstr¨om et al. (2013).
</bodyText>
<footnote confidence="0.757471">
5The dictionary licenses punctuations, only by the ‘.’ tag.
</footnote>
<bodyText confidence="0.999911428571429">
We have only one constraint feature in our poste-
rior regularization models that fires for the unpruned
projected tags on words xi. This feature controls
how often our model trusts a projected tag; we ex-
plain how its strength is chosen in §6.1. The PR and
PR+ models are trained using the stochastic gradient
method described in §3.3.
</bodyText>
<subsectionHeader confidence="0.995229">
5.2 Named-Entity Segmentation
</subsectionHeader>
<bodyText confidence="0.9990925">
In this subsection, we turn to the experimental details
of the named-entity segmentation system.
</bodyText>
<subsectionHeader confidence="0.803394">
5.2.1 Supervised Source-Side Model
</subsectionHeader>
<bodyText confidence="0.999951894736842">
To train our supervised source-side NER model,
we implemented a linear-chain first order CRF model.
Our feature set was inspired by the model of Kazama
and Torisawa (2007, §6.1); we used all the local fea-
tures from their model except the gazetteer features,
and added cluster emission features for offsets in the
range [-2, 2] and transition features for offsets in the
range [-1, 1] as well as a sentence-start feature. We
use automatic POS tags for all the experiments.
We use a BIO encoding of the four NER labels
(PER, LOC, ORG and MISC). We also experi-
mented with omitting the NE labels from the tagger,
still with a BIO encoding for segments, but the results
were worse on average than what we report in Table 2.
We train the source-side model on the CoNLL 2003
English training set with log-loss using L-BFGS for
100 iterations with E2 regularization weight of 0.1.
The model gets 90.9% and 87.5% labeled F1 on the
CoNLL development and test sets respectively.6
</bodyText>
<subsubsectionHeader confidence="0.503229">
5.2.2 Word-Alignment Filtering
</subsubsectionHeader>
<bodyText confidence="0.9998242">
Projecting named entities across languages can
be error prone for several reasons. Mistakes intro-
duced by the automatic word aligner is one of them.
Word alignment errors are particularly problematic
for entity mentions because of the garbage collector
effect (Brown et al., 1993); due to differences in the
word order between languages, a few alignment er-
rors can result in many errors in the other language.
Additionally, entities can occur on just one side of
the bitext.7 Another source of error is the automatic
</bodyText>
<footnote confidence="0.9995835">
6These performance values would place us among the top
three competitors of the CoNLL 2003 shared task.
7For example, “It’s all Greek to me.” in one language and “I
don’t understand it.” in another.
</footnote>
<page confidence="0.995417">
2001
</page>
<bodyText confidence="0.999975">
labeling on the source side, which is inaccurate if the
parallel corpus is out of domain. To mitigate these
errors, we aggressively filter the training data for this
task. We discard sentence pairs where more than
30% of the source language tokens are unaligned,
where any source entities are unaligned or where
any source entities are more than 4 tokens long. We
also compute a confidence score over entity anno-
tations as the minimum posterior over the tags that
comprise the entity and discard sentence pairs that
have an entity with confidence below 0.9. Finally,
we discard any sentences that contain no projected
entities. These filtering steps allow us to keep 7.4%,
9.7% and 10.4% of the aligned sentence pairs for Ger-
man, Spanish and Dutch, respectively, resulting in
very high-precision named-entity projections (see Ta-
ble 2). For comparison, we also perform experiments
without this filtering step.
</bodyText>
<subsubsectionHeader confidence="0.402957">
5.2.3 Setup for Cross-Lingual Experiments
</subsubsectionHeader>
<bodyText confidence="0.999995714285714">
We use a CRF with the same feature set and BIO
encoding for the cross-lingual models as the source-
side NER model. We compare our approach (“PR”
in Table 2) to a baseline (“BASE” in Table 2) which
treats the projected annotations as fully observed.
The PR model treats the projected NE spans of a
sentence as observed, and allows all labels on the
remaining tokens. Since the “O” tag is never seen, an
unconstrained model would learn to never predict it.
We add two features that fire when the current word
is tagged “O”: a bias feature and a feature that fires
when the automatic POS tag is a proper noun. We set
up 2 so the desired expectations are at least 0.98 and
at most 0.1 for these constraint features respectively.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999469666666667">
In this section, we turn to our experimental results;
first, we focus on POS tagging and then turn to the
NE segmentation task.
</bodyText>
<subsectionHeader confidence="0.997692">
6.1 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999409571428571">
Constraint Strength: As discussed in §4.1, it is
important to filter out projected annotations not li-
censed by Wiktionary. Thus, the quality of weakly-
supervised POS taggers learned from projections
is closely correlated with the coverage of the Wik-
tionary. To quantify the effect of Wiktionary cover-
age, we counted the expected number of possible tags
</bodyText>
<figure confidence="0.994719">
1
0.9
0.8
0.7
0.1 0.2 0.3 0.4
1/TpT
</figure>
<figureCaption confidence="0.996127">
Figure 2: Correlation between optimal constraint value b
and dictionary pruning efficiency. Each blue square is a
language, the green line is a linear approximation of the
data.
</figureCaption>
<bodyText confidence="0.999817393939394">
per token (TpT) for our unlabeled corpora. Specif-
ically, for each token, we counted the number of
tags licensed by the dictionary, or all tags for word
forms not in the dictionary. For each language, we
also ran our system with constraint strengths in 10.7,
0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.98, 1.001, and com-
puted the optimal constraint strength from this set.
We found that the best constraint strength is closely
correlated with the average number of tags available
for each token. Figure 2 shows the best constraint
strength as a function of the inverse of the number of
unpruned tags per token. As observed in the figure,
the relationship between the optimal strength and
1/TpT is roughly linear. Figure 2 also shows a linear
approximation to the data plotted. When applying
this technique to a new language, we would not be
able to estimate the optimal constraint strength, but
we could use the linear approximation and knowl-
edge of 1/TpT to estimate it. For our experiments
below, we perform this estimation for each language
using the linear approximation computed from the
remaining languages.
Results: The results for our part-of-speech tagging
experiments are in Table 1. We compare our results
to BASE, which corresponds to reruns of the best
model of T¨ackstr¨om et al. (2013, Column 9 of Ta-
ble 2), and closely aligns with the numbers reported
by the authors. We see in Table 1 that for both fea-
ture sets (i.e., with and without the ‘+’ extension),
our estimated constraint strength is usually better
than using a constraint strength of 1. The results
in the column labeled PR are better than BASE for
12 out of 17 languages, and the results for PR+ are
</bodyText>
<figure confidence="0.303821">
optimal b
</figure>
<page confidence="0.846701">
2002
</page>
<table confidence="0.9998583">
BASE BASE+ PR PR+
ar 37.84 44.96 * 49.04* 50.10*
bg 88.04 87.93 88.02 88.42*
cs 79.67 80.01 * 80.20* 80.68*
da 88.14 87.92 88.24* 87.90
de 90.32 89.97 90.41* 90.29
el 90.03 89.03 90.63* 90.24*
es 86.99 86.81 87.20* 87.21*
fr 87.07 87.53* 87.44* 87.48*
hu 82.05 82.05 82.14* 83.13*
it 89.48 89.89 * 89.52 89.72*
ja 80.63 78.54 80.02 79.68
nl 85.89 85.77 85.59 85.98*
pt 90.93 91.60 * 91.48* 91.56*
sl 82.46 82.08 83.16* 83.49*
sv 89.06 88.72 89.25* 88.77
tr 64.39 65.74 * 63.88 66.47*
zh 73.98 72.82 74.51* 68.43
Avg 81.59 81.85 * 82.40* 82.33*
-zh-ar 85.01 84.91 85.15* 85.40*
</table>
<tableCaption confidence="0.998579">
Table 1: POS tagging results. BASE represents the best
</tableCaption>
<bodyText confidence="0.993523016393443">
model of T¨ackstr¨om et al. (2013). PR is a system with
the same features but with relaxed constraints. BASE+
and PR+ add additional model features (see §5.2.3). * in-
dicates improvements over the previous state of the art
(BASE), and bold values indicate the best score for a lan-
guage. “Avg” indicates averaged results for all 17 lan-
guages, while “-zh-ar” shows averaged results without
Chinese and Arabic.
better than BASE+ for 13 out of 17 languages. Ad-
ditionally, adding features does not tend to help the
baseline model to a large extent (the wins are for
6 languages), but does tend to help the PR model
(for 11 languages); however, there is a large drop in
performance for Chinese.
Error Analysis: Here, we analyze the nature of
improvements that the PR models get. For the lan-
guages where PR results in large improvements, it
stems from the ability to allow the sentential con-
text to sometimes override the tag projected via the
parallel data. For example, the Czech word se can
either be a reflexive pronoun (such as ourselves in
English) or translate to the preposition with. The
pronominal sense comprises about 95% of occur-
rences in the Czech annotations, but it would not
appear in an English translation. For example, the
phrase “podivali jsme se” translates to “we looked”,
and the word jsme would typically be aligned to we;
se, which serves as a reflexive pronoun here, remains
unaligned. Consequently, in our data, over 7000 oc-
currences of se appear, but only 17 instances have a
tag projection that is not filtered by Wiktionary. Since
the remaining are tagged with the preposition tag, the
hard-constrained baseline always tags se as a prepo-
sition. By contrast, the soft-constrained PR model
predicts the pronominal sense in cases where the con-
text is most indicative of a pronoun – 38% of the time.
It still mistags many of the pronominal cases where
the contextual evidence is not strong enough. We get
very similar behavior with the Hungarian word hogy
which can translate to the conjunction that (as in “I
see that you are here”) or the adverb how.
We found that the drastic drop in performance for
Chinese under the PR+ model is due to the possessive
marker “的” which serves exclusively as a particle
in the test data. Wiktionary also allows the noun and
adverb tags. The adverbial use is actually a different
token (的确 H really, truly) containing the same
character. Because the cross-lingual training data is
based on machine-learned alignments, 99.4% of the
training examples of 的 have no annotations, and only
0.6% have the particle annotation projected from the
English ’s possessive marker. If we remove the noun
and adverb senses from the Wiktionary performance
of PR+ improves to 72.87%. Alternatively, we could
add another constraint to prefer closed-class words
over open-class words when both are licensed by the
dictionary. When we add such a constraint to Chinese
with a constraint value of 0.95, we recover most of the
loss (68.43 -* 72.94); however, we do not report this
specific change to the Chinese experimental setup in
Table 1 to maintain generality.
</bodyText>
<subsectionHeader confidence="0.998359">
6.2 Named-Entity Segmentation
</subsectionHeader>
<bodyText confidence="0.999879272727273">
Results: Table 2 shows the results for the named en-
tity segmentation experiments. First, we observe that
the word alignment filtering step (§5.2.2) improves
results for all three languages by significant margins,
for both the BASE and PR models. Both with and
without filtering, we observe that the baseline mod-
els are very strongly biased towards precision. The
filtering step tends to help with recall more than pre-
cison for both models. By having a soft constraint via
PR and allowing some segmentations to fall outside
of the transferred one, we get an increase in recall,
</bodyText>
<page confidence="0.912823">
2003
</page>
<table confidence="0.999970809523809">
No Filtering Filtering (§5.2.2)
Lang Metric BASE PR BASE PR
Prec 74.29 73.85 75.36 76.47
de Recall 41.69 54.50 54.71 64.61
F1 53.41 62.71 63.39 70.04
Prec 74.53 62.10 82.50 70.22
es Recall 56.39 78.33 67.27 81.10
F1 64.20 69.28 74.11 75.27
Prec 81.90 75.12 86.39 76.09
nl Recall 50.54 76.11 65.45 79.11
F1 62.51 75.61 74.47 77.57
Above: dev, below: test
Prec 73.23 71.67 69.90 70.94
de Recall 39.70 51.81 52.52 61.42
F1 51.49 60.14 59.97 65.84
Prec 75.38 65.40 83.50 73.68
es Recall 56.00 80.30 67.55 83.31
F1 64.26 72.09 74.68 78.20
Prec 79.45 73.55 86.01 77.05
nl Recall 47.45 75.37 65.16 80.11
F1 59.42 74.45 74.14 78.55
</table>
<tableCaption confidence="0.9211725">
Table 2: Result for the named-entity segmentation exper-
iments. The highest score in each category is shown in
bold. Note that “No Filtering” still discards sentences with
no projected entities.
</tableCaption>
<bodyText confidence="0.999969306122449">
and in turn an improved F1 score. On average the
PR model improves F-score by 3.6% on the develop-
ment set and 4.6% on the test set over the baseline
(when filtering is used). Note that because we focus
on named entity segmentation, our results are not
directly comparable to those of T¨ackstr¨om (2012),
who train a de-lexicalized named entity recognizer
on one language and apply it to other languages.
Error Analysis: In order to get a sense for the types
of errors made by the baseline which are corrected
by the PR model, we collected statistics about the
most frequent errors in the segments extracted by
the baseline and by our model. We divided the er-
rors into missing segments, extraneous segments and
overlapping segments.
From Table 2, it is clear that the most common
errors for the baseline models are missing entities.
From our analysis of the CoNLL development data,
we found that the entities that occur with little context
(such as the location and publisher of an item) at the
onset of news articles are most frequently missed. For
German, dpa (Deutsche Presse-Agentur) and Reuter
are the two most common missing segmentations;
the Spanish counterparts are Gobierno (Government)
and Barcelona, while for Dutch they are De Morgen
and Brussel. While filtering parallel sentences and
using a soft constraint both increase recall, even our
strongest model does not get enough information to
predict these entities, and they continue to be major
sources of error. By contrast, the names mentioned
in context are the ones that are most frequently added
to the analysis when PR is used. In a sense this
is desirable, since a machine-learned named-entity
segmentation system is most useful for the long tail
of entity mentions.
If we filter the training data and use the PR model
to further increase recall, precision errors tend to
become relatively more frequent (this trend is ob-
servable in Table 2). For German, the most frequent
precision error is Mark referring to the Deutsche
Mark. For Spanish, the most frequent precision er-
rors are due to boundary errors. The Spanish an-
notation guidelines include enclosing quotes as part
of the entity name, and failing to include them ac-
counts for just under 1% of the precision errors of
the PR system that uses filtering. The second most
frequent error is failing to segment Inter de Mil´an.
The model segments out either Inter or Mil´an or both
by themselves depending on context.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999940647058824">
In this paper, we presented a framework for cross-
lingual transfer of sequence information from a
resource-rich source language to a resource-poor tar-
get language. Our framework incorporates soft con-
straints while training with projected information
via posterior regularization. We presented the effi-
cacy of our framework on two very useful natural
language tasks: POS tagging and named-entity seg-
mentation. The soft constraints used in our work
model intuitions about a given task. For the POS
tagging problem, we designed constraints that also
incorporate projected token-level information, and
presented a principled method for choosing the extent
to which this information should be trusted within
the PR framework. This approach generalizes the
state of the art in cross-lingual projection work in
the context of POS tagging, and improves upon it.
</bodyText>
<page confidence="0.987046">
2004
</page>
<bodyText confidence="0.99924725">
Across seventeen languages, our models outperform
the previous state of the art by an average of 0.8%
(greater than 4% error reduction), and outperforms
it on twelve out of seventeen languages. For named-
entity segmentation, our model results in 3.6% and
4.6% absolute improvements in F1-score on our de-
velopment and test sets respectively, when averaged
across three languages.
</bodyText>
<sectionHeader confidence="0.996874" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999940333333333">
We would like to thank Ryan McDonald, Fernando
Pereira, Slav Petrov and Oscar T¨ackstr¨om for numer-
ous discussions on this topic and providing detailed
feedback on early drafts of this paper. We are also
grateful to the four anonymous reviewers for their
valuable comments.
</bodyText>
<sectionHeader confidence="0.998316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913011904762">
Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel.
2003. Building a Treebank for French. In A. Abeill´e,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the Work-
shop on Human Language Technology.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1–38.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal ofMachine Learn-
ing Research, 11:2001–2049.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311–325.
Jun’ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012.
Multilingual named entity recognition using parallel
data and metadata from wikipedia. In Proceedings of
ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit.
Solomon Kullback and Richard A. Leibler. 1951. On
information and sufficiency. Annals of Mathematical
Statistics, 22:49–86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential families.
In Proceedings of ICML.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313–330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
</reference>
<page confidence="0.807105">
2005
</page>
<reference confidence="0.999576105263158">
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. Trans-
actions of the Association for Computational Linguis-
tics, 1:1–12.
Oscar T¨ackstr¨om. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recogni-
tion. In Proceedings of the NAACL-HLT Workshop on
the Induction of Linguistic Structure.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Andreas Vlachos. 2006. Active annotation. Proceedings
of EACL.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP Workshop: NLP for Less Privi-
leged Languages.
</reference>
<page confidence="0.992033">
2006
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365294">
<title confidence="0.987439">Cross-Lingual Discriminative Learning of Sequence with Posterior Regularization</title>
<author confidence="0.669109">Kuzman</author>
<affiliation confidence="0.52164">Google</affiliation>
<address confidence="0.8049565">76 9th New York, NY</address>
<email confidence="0.999551">kuzman@google.com</email>
<abstract confidence="0.9917950625">We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Lionel Cl´ement</author>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Building a Treebank for French. In</title>
<date>2003</date>
<booktitle>Treebanks: Building and Using Parsed Corpora, chapter 10.</booktitle>
<editor>A. Abeill´e, editor,</editor>
<publisher>Kluwer.</publisher>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French. In A. Abeill´e, editor, Treebanks: Building and Using Parsed Corpora, chapter 10. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="20806" citStr="Brown et al. (1992)" startWordPosition="3401" endWordPosition="3404">he English side of our parallel data with a supervised first-order linear-chain CRF POS tagger. We use standard features for tagging. Our emission features are a bias feature, the current word, its suffixes up to length 3, its capitalization shape, whether it contains a hyphen, digit or punctuation and its cluster identity. Our transition features are a bias feature and the cluster identities of each word in the transition. For the cluster-based features, we use monolingual word clusters induced with the exchange algorithm of Uszkoreit and Brants (2008), which implements the same objective as Brown et al. (1992); these clusters have shown improvements for sequence labeling tasks (Turian et al., 2010; T¨ackstr¨om et al., 2012). We set the number of clusters to 256 for both the source side tagger and all the other languages. On Section 23 of the WSJ section of the Penn Treebank, the source side tagger achieves an accuracy of 96.2%. 5.1.2 Word Alignment Filtering Following T¨ackstr¨om et al. (2013), we tag the English side of our parallel data using the source-side POS tagger, intersect the word alignments and filter alignments with confidence below 0.95. We sample 500,000 tokens of target side sentence</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Meredith J Goldsmith</author>
<author>Jan Hajic</author>
<author>Robert L Mercer</author>
<author>Surya Mohanty</author>
</authors>
<title>But dictionaries are data too.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="24129" citStr="Brown et al., 1993" startWordPosition="3951" endWordPosition="3954">lts were worse on average than what we report in Table 2. We train the source-side model on the CoNLL 2003 English training set with log-loss using L-BFGS for 100 iterations with E2 regularization weight of 0.1. The model gets 90.9% and 87.5% labeled F1 on the CoNLL development and test sets respectively.6 5.2.2 Word-Alignment Filtering Projecting named entities across languages can be error prone for several reasons. Mistakes introduced by the automatic word aligner is one of them. Word alignment errors are particularly problematic for entity mentions because of the garbage collector effect (Brown et al., 1993); due to differences in the word order between languages, a few alignment errors can result in many errors in the other language. Additionally, entities can occur on just one side of the bitext.7 Another source of error is the automatic 6These performance values would place us among the top three competitors of the CoNLL 2003 shared task. 7For example, “It’s all Greek to me.” in one language and “I don’t understand it.” in another. 2001 labeling on the source side, which is inaccurate if the parallel corpus is out of domain. To mitigate these errors, we aggressively filter the training data fo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Goldsmith, Hajic, Mercer, Mohanty, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Meredith J. Goldsmith, Jan Hajic, Robert L. Mercer, and Surya Mohanty. 1993. But dictionaries are data too. In Proceedings of the Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="16988" citStr="Buchholz and Marsi, 2006" startWordPosition="2769" endWordPosition="2772"> English phrase. Word alignments are shown as links between English and Spanish words. word esponjas because this tag is in our dictionary for the latter word. For all our POS experiments, we evaluate on seventeen target languages. Fifteen of these languages were part of the experiments conducted by T¨ackstr¨om et al. (2013); we add Arabic and Hungarian to the set. The first column of Table 1 lists all seventeen languages using their two-letter abbreviation codes from the ISO 639-1 standard. The evaluation datasets correspond to the test sets from the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). For French we use the treebank of Abeill´e et al. (2003). English serves as our source language and we use the Penn Treebank (Marcus et al., 1993, with tags mapped to the universal tags) to train our supervised source-side model. 4.2 Named-Entity Segmentation Second, we investigate the task of named-entity segmentation. The goal of this task is to identify the boundaries of named-entities for a given language without classifying them by type. This is the unlabeled version of named-entity recognition, and is more amenable to cross-lingual supervision. To understand why th</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5263" citStr="Christodoulopoulos et al., 2010" startWordPosition="782" endWordPosition="785">his was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al. (2009), who also use posterior regularization but focus on dependency parsing alone. Our work differs with respect to the tasks, the learning algorithm and also in that we use corpus-wide constraints, while Ganchev et al. use one constraint per sentence. For the part-of-speech tagging task, our approach is similar to that of T¨ackstr¨om et al. (2013), who use an almost </context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised partof-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="2301" citStr="Das and Petrov, 2011" startWordPosition="334" endWordPosition="337">eech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a framework for learning</context>
<context position="7276" citStr="Das and Petrov, 2011" startWordPosition="1097" endWordPosition="1100">w The general overview of our framework is provided in Algorithm 1. The process of learning parameters for a target language for a given task involves four subtasks. First, we run word alignment over a large corpus of parallel data between the resourcerich source language and the resource-impoverished target language (see §4.3). In the second step, we use a supervised model to label the source side of the parallel data (see §5.1.1 and §5.2.1). The third step involves a task-specific word-alignment filtering step; this step involves heuristics for which we use cues from prior state-of-the-art (Das and Petrov, 2011; T¨ackstr¨om et al., 2013, see §5.1.2) and also introduce some novel ones for the NE segmentation problem (see §5.2.2). In the fourth step, we train a linear chain conditional random field (Lafferty et al., 2001, CRF henceforth) using posterior regularization. In the next subsection, we turn to a brief summary of this final step of estimating parameters of a discriminative model with posterior regularization. 3.2 Learning with Posterior Regularization In this work, we utilize discriminative CRF models, and use posterior regularization (PR) to optimize their parameters. As a framework, posteri</context>
<context position="15122" citStr="Das and Petrov, 2011" startWordPosition="2459" endWordPosition="2462"> Since the objectives are non-convex, the two optimization techniques could lead to different local optima even when the constraint is not relaxed (b = 0). 4 Tasks and Data In this section, we focus on the nature of the two tasks that we attempt to solve, describe the source language datasets we use to train our supervised models for transfer, the target language datasets on which we evaluate our models and the parallel data we use for cross-lingual transfer. 4.1 Part-of-Speech Tagging First, we focus on the task of part-of-speech tagging. Following previous work on cross-lingual POS tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), we adopt the POS tags of Petrov et al. (2012), version 1.03;2 we use the October 2012 version of Wiktionary3 as our tag dictionary. After pruning the search space with the dictionary, we place soft constraints derived by projecting POS tags across word alignments. The alignments are filtered for confidence (see §5.1.2), but we also filter any projected tags that are not licensed by the dictionary. The example in Figure 1 illustrates why this dictionary filtering step is important. Consider the English-Spanish phrase pair from Figure 1, which we observed in our trai</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised partof-speech tagging with bilingual graph-based projections. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="10761" citStr="Dempster et al., 1977" startWordPosition="1698" endWordPosition="1701">s: where KL(Q||p) = minq∈Q KL(q||p) is the KL-divergence (Kullback and Leibler, 1951) from a set to a point. Note that as we add more constraints, Q becomes a smaller set. In the limit, Q = {q(Y) : q( ˆY) = 1} contains just one distribution concentrated on a single labeling ˆY. In this limit, posterior regularization degenerates into the convex log-likelihood objective normally used for supervised data JQ(θ) = L(θ). However, in the general case, the PR objective JQ is not necessarily convex. Prior work, including that of Ganchev et al. propose an algorithm similar to Expectation-Maximization (Dempster et al., 1977, EM henceforth) to optimize JQ, but we follow Liang et al. (2009) in using a schochastic update-based algorithm described below. Note: To make it easier to reason about constraint values b, we scale constraint features φ(X, Y) to lie in [0, 1] by computing maxY φ(X, Y) for the corpus to which φ is applied. 3.3 Optimization The optimization procedure proposed by Ganchev et al. is similar to the EM algorithm, and computes the minimization minq∈Q KL(q||p) at each step, using its dual form; this minimization is convex, so there is no duality gap. They show that the optimal primal variables q∗(Y) </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Klaus Macherey</author>
</authors>
<title>Model-based aligner combination using dual decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="19016" citStr="DeNero and Macherey (2011)" startWordPosition="3106" endWordPosition="3109">e Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly describe the setup of the cross-lingual experiments for each task. 5.1 Part-of-Speech Tagging We first focus on the experimental setup for the</context>
</contexts>
<marker>DeNero, Macherey, 2011</marker>
<rawString>John DeNero and Klaus Macherey. 2011. Model-based aligner combination using dual decomposition. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2866" citStr="Ganchev et al. (2009)" startWordPosition="415" endWordPosition="418">ed systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a framework for learning weakly-supervised systems (in their case, dependency parsers) that incorporated alignment-based information too, but used the crosslingual information only as soft constraints, via posterior regularization. The advantage of this framework lay in the fact that the projections were only trusted to a certain degree, determined by a strength hyperparameter, which unfortunately the authors did not have an elegant way to tune. In this paper, we exploit the better aspects of these two lines of work: first, we extend the framework of T¨ackstr¨om et al. by treating t</context>
<context position="5497" citStr="Ganchev et al. (2009)" startWordPosition="822" endWordPosition="825"> Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al. (2009), who also use posterior regularization but focus on dependency parsing alone. Our work differs with respect to the tasks, the learning algorithm and also in that we use corpus-wide constraints, while Ganchev et al. use one constraint per sentence. For the part-of-speech tagging task, our approach is similar to that of T¨ackstr¨om et al. (2013), who use an almost identical learning setup but only make use of hard constraints. By relaxing these constraints, we allow the model to identify and ignore inconsistently labeled parts of sentences, and achieve better results using identical training an</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>11--2001</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal ofMachine Learning Research, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>03</issue>
<contexts>
<context position="1762" citStr="Hwa et al., 2005" startWordPosition="252" endWordPosition="255">sources. For a given resource-poor target language of interest, we assume that parallel data with a resource-rich source language exists. With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique f</context>
<context position="4782" citStr="Hwa et al., 2005" startWordPosition="704" endWordPosition="707">h posterior regularization (§6.2) by incorporating intuitive soft constraints during learning. We measure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use W</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(03):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with nonlocal features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23019" citStr="Kazama and Torisawa (2007" startWordPosition="3764" endWordPosition="3767">r posterior regularization models that fires for the unpruned projected tags on words xi. This feature controls how often our model trusts a projected tag; we explain how its strength is chosen in §6.1. The PR and PR+ models are trained using the stochastic gradient method described in §3.3. 5.2 Named-Entity Segmentation In this subsection, we turn to the experimental details of the named-entity segmentation system. 5.2.1 Supervised Source-Side Model To train our supervised source-side NER model, we implemented a linear-chain first order CRF model. Our feature set was inspired by the model of Kazama and Torisawa (2007, §6.1); we used all the local features from their model except the gazetteer features, and added cluster emission features for offsets in the range [-2, 2] and transition features for offsets in the range [-1, 1] as well as a sentence-start feature. We use automatic POS tags for all the experiments. We use a BIO encoding of the four NER labels (PER, LOC, ORG and MISC). We also experimented with omitting the NE labels from the tagger, still with a BIO encoding for segments, but the results were worse on average than what we report in Table 2. We train the source-side model on the CoNLL 2003 En</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. A new perceptron algorithm for sequence labeling with nonlocal features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungchul Kim</author>
<author>Kristina Toutanova</author>
<author>Hwanjo Yu</author>
</authors>
<title>Multilingual named entity recognition using parallel data and metadata from wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1810" citStr="Kim et al., 2012" startWordPosition="259" endWordPosition="262">ge of interest, we assume that parallel data with a resource-rich source language exists. With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from proje</context>
<context position="4830" citStr="Kim et al., 2012" startWordPosition="712" endWordPosition="715">ng intuitive soft constraints during learning. We measure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging t</context>
</contexts>
<marker>Kim, Toutanova, Yu, 2012</marker>
<rawString>Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from wikipedia. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="18841" citStr="Koehn, 2005" startWordPosition="3079" endWordPosition="3080">ish as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>22--49</pages>
<contexts>
<context position="10225" citStr="Kullback and Leibler, 1951" startWordPosition="1609" endWordPosition="1612">r b is used to encode our prior knowledge about desirable distributions q(Y). Note that the constraint features φ are not related to the model features f. The model features, together with the model parameters θ define the CRF model; the model features need to be computed at inference time for prediction. By contrast, the constraint features and their corresponding constraint values are used to define our training objective function (and are only used during learning). The PR objective with no labeled data is defined with respect to Q as: where KL(Q||p) = minq∈Q KL(q||p) is the KL-divergence (Kullback and Leibler, 1951) from a set to a point. Note that as we add more constraints, Q becomes a smaller set. In the limit, Q = {q(Y) : q( ˆY) = 1} contains just one distribution concentrated on a single labeling ˆY. In this limit, posterior regularization degenerates into the convex log-likelihood objective normally used for supervised data JQ(θ) = L(θ). However, in the general case, the PR objective JQ is not necessarily convex. Prior work, including that of Ganchev et al. propose an algorithm similar to Expectation-Maximization (Dempster et al., 1977, EM henceforth) to optimize JQ, but we follow Liang et al. (200</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A. Leibler. 1951. On information and sufficiency. Annals of Mathematical Statistics, 22:49–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="7488" citStr="Lafferty et al., 2001" startWordPosition="1132" endWordPosition="1135">pus of parallel data between the resourcerich source language and the resource-impoverished target language (see §4.3). In the second step, we use a supervised model to label the source side of the parallel data (see §5.1.1 and §5.2.1). The third step involves a task-specific word-alignment filtering step; this step involves heuristics for which we use cues from prior state-of-the-art (Das and Petrov, 2011; T¨ackstr¨om et al., 2013, see §5.1.2) and also introduce some novel ones for the NE segmentation problem (see §5.2.2). In the fourth step, we train a linear chain conditional random field (Lafferty et al., 2001, CRF henceforth) using posterior regularization. In the next subsection, we turn to a brief summary of this final step of estimating parameters of a discriminative model with posterior regularization. 3.2 Learning with Posterior Regularization In this work, we utilize discriminative CRF models, and use posterior regularization (PR) to optimize their parameters. As a framework, posterior regularization is described in detail by Ganchev et al. (2010). However in our work, we adopt a different optimization technique; in what follows, we summarize the optimization algorithm in the context of CRF </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10827" citStr="Liang et al. (2009)" startWordPosition="1710" endWordPosition="1713">nd Leibler, 1951) from a set to a point. Note that as we add more constraints, Q becomes a smaller set. In the limit, Q = {q(Y) : q( ˆY) = 1} contains just one distribution concentrated on a single labeling ˆY. In this limit, posterior regularization degenerates into the convex log-likelihood objective normally used for supervised data JQ(θ) = L(θ). However, in the general case, the PR objective JQ is not necessarily convex. Prior work, including that of Ganchev et al. propose an algorithm similar to Expectation-Maximization (Dempster et al., 1977, EM henceforth) to optimize JQ, but we follow Liang et al. (2009) in using a schochastic update-based algorithm described below. Note: To make it easier to reason about constraint values b, we scale constraint features φ(X, Y) to lie in [0, 1] by computing maxY φ(X, Y) for the corpus to which φ is applied. 3.3 Optimization The optimization procedure proposed by Ganchev et al. is similar to the EM algorithm, and computes the minimization minq∈Q KL(q||p) at each step, using its dual form; this minimization is convex, so there is no duality gap. They show that the optimal primal variables q∗(Y) are related to the optimal dual variables λ∗ by: q∗(Y) = pθ(Y|X)e−</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning from measurements in exponential families. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="14454" citStr="Liu and Nocedal, 1989" startWordPosition="2348" endWordPosition="2351">d not implement regularization of B in the stochastic optimizer, hence our PR objective (Eq. 8) was unregularized; however, the baseline models use 22 regularization. gives the same distribution as Eq. 10. Given this equivalence, it is easy to see that the gradient of Eq. 5 with respect to θ is the same as that of Eq. 10. By using such constrained lattices, T¨ackstr¨om et al. avoid maintaining a parameter for the constraint, but lose the ability to relax the constraint value and allow some probability mass outside the pruned lattice. Their paper also differs from ours in that they use L-BFGS (Liu and Nocedal, 1989), while we use an online optimization procedure. Since the objectives are non-convex, the two optimization techniques could lead to different local optima even when the constraint is not relaxed (b = 0). 4 Tasks and Data In this section, we focus on the nature of the two tasks that we attempt to solve, describe the source language datasets we use to train our supervised models for transfer, the target language datasets on which we evaluate our models and the parallel data we use for cross-lingual transfer. 4.1 Part-of-Speech Tagging First, we focus on the task of part-of-speech tagging. Follow</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="17156" citStr="Marcus et al., 1993" startWordPosition="2800" endWordPosition="2803">OS experiments, we evaluate on seventeen target languages. Fifteen of these languages were part of the experiments conducted by T¨ackstr¨om et al. (2013); we add Arabic and Hungarian to the set. The first column of Table 1 lists all seventeen languages using their two-letter abbreviation codes from the ISO 639-1 standard. The evaluation datasets correspond to the test sets from the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). For French we use the treebank of Abeill´e et al. (2003). English serves as our source language and we use the Penn Treebank (Marcus et al., 1993, with tags mapped to the universal tags) to train our supervised source-side model. 4.2 Named-Entity Segmentation Second, we investigate the task of named-entity segmentation. The goal of this task is to identify the boundaries of named-entities for a given language without classifying them by type. This is the unlabeled version of named-entity recognition, and is more amenable to cross-lingual supervision. To understand why that is, consider again the example from Figure 1. The English supervised NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-al</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2278" citStr="McDonald et al., 2011" startWordPosition="330" endWordPosition="333">ms, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-level projections were treated as hard observations. On the other hand, Ganchev et al. (2009) presented a</context>
<context position="4941" citStr="McDonald et al. (2011)" startWordPosition="732" endWordPosition="735">oth of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al. (2009), who also use posterior regularization but </context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="15196" citStr="Petrov et al. (2012)" startWordPosition="2473" endWordPosition="2476">d lead to different local optima even when the constraint is not relaxed (b = 0). 4 Tasks and Data In this section, we focus on the nature of the two tasks that we attempt to solve, describe the source language datasets we use to train our supervised models for transfer, the target language datasets on which we evaluate our models and the parallel data we use for cross-lingual transfer. 4.1 Part-of-Speech Tagging First, we focus on the task of part-of-speech tagging. Following previous work on cross-lingual POS tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), we adopt the POS tags of Petrov et al. (2012), version 1.03;2 we use the October 2012 version of Wiktionary3 as our tag dictionary. After pruning the search space with the dictionary, we place soft constraints derived by projecting POS tags across word alignments. The alignments are filtered for confidence (see §5.1.2), but we also filter any projected tags that are not licensed by the dictionary. The example in Figure 1 illustrates why this dictionary filtering step is important. Consider the English-Spanish phrase pair from Figure 1, which we observed in our training data. Our supervised tagger correctly tags Asian with the ADJ tag as </context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
</authors>
<title>Nudging the envelope of direct transfer methods for multilingual named entity recognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.</booktitle>
<marker>T¨ackstr¨om, 2012</marker>
<rawString>Oscar T¨ackstr¨om. 2012. Nudging the envelope of direct transfer methods for multilingual named entity recognition. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="18623" citStr="Sang, 2002" startWordPosition="3040" endWordPosition="3041">ions of this kind are common, it makes cross-lingual detection of NE boundaries as well as types hard.4 In this paper, we focus on named-entity segmentation alone, consider the full NER task out of scope. We use English as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20895" citStr="Turian et al., 2010" startWordPosition="3414" endWordPosition="3417">agger. We use standard features for tagging. Our emission features are a bias feature, the current word, its suffixes up to length 3, its capitalization shape, whether it contains a hyphen, digit or punctuation and its cluster identity. Our transition features are a bias feature and the cluster identities of each word in the transition. For the cluster-based features, we use monolingual word clusters induced with the exchange algorithm of Uszkoreit and Brants (2008), which implements the same objective as Brown et al. (1992); these clusters have shown improvements for sequence labeling tasks (Turian et al., 2010; T¨ackstr¨om et al., 2012). We set the number of clusters to 256 for both the source side tagger and all the other languages. On Section 23 of the WSJ section of the Penn Treebank, the source side tagger achieves an accuracy of 96.2%. 5.1.2 Word Alignment Filtering Following T¨ackstr¨om et al. (2013), we tag the English side of our parallel data using the source-side POS tagger, intersect the word alignments and filter alignments with confidence below 0.95. We sample 500,000 tokens of target side sentences for each language, and use this as training data for learning weakly-supervised taggers</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UN</author>
</authors>
<date>2006</date>
<note>ODS UN parallel corpus.</note>
<contexts>
<context position="18879" citStr="UN, 2006" startWordPosition="3086" endWordPosition="3087">rvised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly descr</context>
</contexts>
<marker>UN, 2006</marker>
<rawString>UN. 2006. ODS UN parallel corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="20746" citStr="Uszkoreit and Brants (2008)" startWordPosition="3391" endWordPosition="3394">as esponjas de Asia 2000 5.1.1 Supervised Source-Side Model We tag the English side of our parallel data with a supervised first-order linear-chain CRF POS tagger. We use standard features for tagging. Our emission features are a bias feature, the current word, its suffixes up to length 3, its capitalization shape, whether it contains a hyphen, digit or punctuation and its cluster identity. Our transition features are a bias feature and the cluster identities of each word in the transition. For the cluster-based features, we use monolingual word clusters induced with the exchange algorithm of Uszkoreit and Brants (2008), which implements the same objective as Brown et al. (1992); these clusters have shown improvements for sequence labeling tasks (Turian et al., 2010; T¨ackstr¨om et al., 2012). We set the number of clusters to 256 for both the source side tagger and all the other languages. On Section 23 of the WSJ section of the Penn Treebank, the source side tagger achieves an accuracy of 96.2%. 5.1.2 Word Alignment Filtering Following T¨ackstr¨om et al. (2013), we tag the English side of our parallel data using the source-side POS tagger, intersect the word alignments and filter alignments with confidence </context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay Ponte</author>
<author>Ashok Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="18796" citStr="Uszkoreit et al. (2010)" startWordPosition="3069" endWordPosition="3072">ne, consider the full NER task out of scope. We use English as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks, so we follow T¨ackstr¨om et al. (2013) in sampling 500k tokens for POS tagging and 10k sentences for named-entity segmentation (see §5.1.2 and §5.2.2). 5 Experimental Details In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filt</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
</authors>
<title>Active annotation.</title>
<date>2006</date>
<booktitle>Proceedings of EACL.</booktitle>
<contexts>
<context position="2175" citStr="Vlachos, 2006" startWordPosition="318" endWordPosition="319">diction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om et al. (2013) presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers. Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger’s search space, and the intersected token-le</context>
</contexts>
<marker>Vlachos, 2006</marker>
<rawString>Andreas Vlachos. 2006. Active annotation. Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="1724" citStr="Yarowsky and Ngai, 2001" startWordPosition="246" endWordPosition="249">dels for the languages that lack annotated resources. For a given resource-poor target language of interest, we assume that parallel data with a resource-rich source language exists. With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (Ganchev et al., 2010). Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging (Yarowsky and Ngai, 2001), syntactic parsing (Hwa et al., 2005) and named-entity recognition (Kim et al., 2012). These methods are Dipanjan Das Google Research 76 9th Avenue New York, NY 10011 dipanjand@google.com useful in several ways. First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations. Second, the output of such systems could be used to bootstrap more extensive human annotation projects (Vlachos, 2006). Finally, they are significantly more accurate than purely unsupervised systems (McDonald et al., 2011; Das and Petrov, 2011). Recently, T¨ackstr¨om</context>
<context position="4678" citStr="Yarowsky and Ngai (2001)" startWordPosition="685" endWordPosition="688">m highprecision phrase-level entity transfer (§5.2.2); we also provide ways to balance precision and recall with posterior regularization (§6.2) by incorporating intuitive soft constraints during learning. We measure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we reso</context>
<context position="18487" citStr="Yarowsky and Ngai, 2001" startWordPosition="3014" endWordPosition="3017">er, this should be labeled LOC (location) according to the CoNLL annotation guidelines (Tjong Kim Sang and De Meulder, 2003). Because syntactic variations of this kind are common, it makes cross-lingual detection of NE boundaries as well as types hard.4 In this paper, we focus on named-entity segmentation alone, consider the full NER task out of scope. We use English as a source language and train a supervised English named-entity tagger with the labels in place, using the CoNLL 2003 shared task data (Tjong Kim Sang and De Meulder, 2003). We project the spans using the maximal-span heuristic (Yarowsky and Ngai, 2001). We project into Dutch, German and Spanish and evaluate on the standard CoNLL 2002 and 2003 shared task data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). 4.3 Parallel Data For both tasks we use parallel data gathered automatically from the web using the method of Uszkoreit et al. (2010), as well as data from Europarl (Koehn, 2005) and the UN parallel corpus (UN, 2006), for languages covered by the latter two corpora. The parallel sentences are word aligned with the aligner of DeNero and Macherey (2011). The size of the parallel corpus is larger than we need for our tasks,</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP Workshop: NLP for Less Privileged Languages.</booktitle>
<contexts>
<context position="4890" citStr="Zeman and Resnik (2008)" startWordPosition="722" endWordPosition="726">ure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines. 2 Prior Work Cross-lingual projection methods can be classified by their use of two very broad ideas. The first idea utilizes parallel data to create full or partial annotations in the low-resource language and trains from this data. This was popularized by Yarowsky and Ngai (2001) who applied this to POS tagging and shallow parsing. It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012). The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by McDonald et al. (2011), is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features. In addition to cross-lingual projection, purely unsupervised methods have been explored but with limited success (Christodoulopoulos et al., 2010). Here, we resort to cross-lingual projection and incorporate the first idea; we also follow Li et al. (2012) and use Wiktionary to further constrain the POS tagging task. Our learning setup is similar to that of Ganchev et al.</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In Proceedings of IJCNLP Workshop: NLP for Less Privileged Languages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
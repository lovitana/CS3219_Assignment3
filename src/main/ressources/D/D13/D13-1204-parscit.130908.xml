<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994613">
Breaking Out of Local Optima with Count Transforms
and Model Recombination: A Study in Grammar Induction
</title>
<author confidence="0.831955">
Valentin I. Spitkovsky Hiyan Alshawi Daniel Jurafsky
</author>
<email confidence="0.986563">
valentin@cs.stanford.edu hiyan@google.com jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.99725" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845653846154">
Many statistical learning problems in NLP call
for local model search methods. But accu-
racy tends to suffer with current techniques,
which often explore either too narrowly or too
broadly: hill-climbers can get stuck in local
optima, whereas samplers may be inefficient.
We propose to arrange individual local opti-
mizers into organized networks. Our building
blocks are operators of two types: (i) trans-
form, which suggests new places to search, via
non-random restarts from already-found local
optima; and (ii) join, which merges candidate
solutions to find better optima. Experiments
on grammar induction show that pursuing dif-
ferent transforms (e.g., discarding parts of a
learned model or ignoring portions of train-
ing data) results in improvements. Groups of
locally-optimal solutions can be further per-
turbed jointly, by constructing mixtures. Us-
ing these tools, we designed several modu-
lar dependency grammar induction networks
of increasing complexity. Our complete sys-
tem achieves 48.6% accuracy (directed depen-
dency macro-average over all 19 languages in
the 2006/7 CoNLL data) — more than 5%
higher than the previous state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965625">
Statistical methods for grammar induction often boil
down to solving non-convex optimization problems.
Early work attempted to locally maximize the likeli-
hood of a corpus, using EM to estimate probabilities
of dependency arcs between word bigrams (Paskin
2001a; 2001b). That parsing model has since been
extended to make unsupervised learning more feasi-
ble (Klein and Manning, 2004; Headden et al., 2009;
Spitkovsky et al., 2012b). But even the latest tech-
niques can be quite error-prone and sensitive to ini-
tialization, because of approximate, local search.
In theory, global optima can be found by enumer-
ating all parse forests that derive a corpus, though
this is usually prohibitively expensive in practice. A
preferable brute force approach is sampling, as in
Markov-chain Monte Carlo (MCMC) and random
restarts (Hu et al., 1994), which hit exact solutions
eventually. Restarts can be giant steps in a parameter
space that undo all previous work. At the other ex-
treme, MCMC may cling to a neighborhood, reject-
ing most proposed moves that would escape a local
attractor. Sampling methods thus take unbounded
time to solve a problem (and can’t certify optimal-
ity) but are useful for finding approximate solutions
to grammar induction (Cohn et al., 2011; Mareˇcek
and ˇZabokrtsk´y, 2011; Naseem and Barzilay, 2011).
We propose an alternative (deterministic) search
heuristic that combines local optimization via EM
with non-random restarts. Its new starting places are
informed by previously found solutions, unlike con-
ventional restarts, but may not resemble their prede-
cessors, unlike typical MCMC moves. We show that
one good way to construct such steps in a parame-
ter space is by forgetting some aspects of a learned
model. Another is by merging promising solutions,
since even simple interpolation (Jelinek and Mercer,
1980) of local optima may be superior to all of the
originals. Informed restarts can make it possible to
explore a combinatorial search space more rapidly
and thoroughly than with traditional methods alone.
</bodyText>
<sectionHeader confidence="0.988258" genericHeader="general terms">
2 Abstract Operators
</sectionHeader>
<bodyText confidence="0.9997624">
Let C be a collection of counts — the sufficient
statistics from which a candidate solution to an
optimization problem could be computed, e.g., by
smoothing and normalizing to yield probabilities.
The counts may be fractional and solutions could
take the form of multinomial distributions. A local
optimizer L will convert C into C∗ = LD(C) — an
updated collection of counts, resulting in a proba-
bilistic model that is no less (and hopefully more)
consistent with a data set D than the original C:
</bodyText>
<equation confidence="0.753702">
(1)
C∗
C LD
</equation>
<page confidence="0.854097">
1983
</page>
<note confidence="0.722857">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1983–1995,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.992983">
Unless C* is a global optimum, we should be able
to make further improvements. But if L is idempo-
tent (and ran to convergence) then L(L(C)) = L(C).
Given only C and LD, the single-node optimization
network above would be the minimal search pattern
worth considering. However, if we had another opti-
mizer L′ — or a fresh starting point C′ — then more
complicated networks could become useful.
</bodyText>
<subsectionHeader confidence="0.917316">
2.1 Transforms (Unary)
</subsectionHeader>
<bodyText confidence="0.999941076923077">
New starts could be chosen by perturbing an existing
solution, as in MCMC, or independently of previous
results, as in random restarts. We focus on interme-
diate changes to C, without injecting randomness.
All of our transforms involve selective forgetting
or filtering. For example, if the probabilistic model
that is being estimated decomposes into independent
constituents (e.g., several multinomials) then a sub-
set of them can be reset to uniform distributions, by
discarding associated counts from C. In text classifi-
cation, this could correspond to eliminating frequent
or rare tokens from bags-of-words. We use circular
shapes to represent such model ablation operators:
</bodyText>
<sectionHeader confidence="0.386602" genericHeader="keywords">
C
</sectionHeader>
<bodyText confidence="0.999974333333333">
An orthogonal approach might separate out vari-
ous counts in C by their provenance. For instance,
if D consisted of several heterogeneous data sources,
then the counts from some of them could be ignored:
a classifier might be estimated from just news text.
We will use squares to represent data-set filtering:
</bodyText>
<equation confidence="0.7893425">
(3)
C
</equation>
<bodyText confidence="0.999382714285714">
Finally, if C represents a mixture of possible inter-
pretations over D — e.g., because it captures the out-
put of a “soft” EM algorithm — contributions from
less likely, noisier completions could also be sup-
pressed (and their weights redistributed to the more
likely ones), as in “hard” EM. Diamonds will repre-
sent plain (single) steps of Viterbi training:
</bodyText>
<sectionHeader confidence="0.864221" genericHeader="introduction">
C
</sectionHeader>
<subsectionHeader confidence="0.998082">
2.2 Joins (Binary)
</subsectionHeader>
<bodyText confidence="0.999946111111111">
Starting from different initializers, say C1 and C2,
it may be possible for L to arrive at distinct local
optima, C*1 =� C2*. The better of the two solutions,
according to likelihood LD of D, could then be se-
lected — as is standard practice when sampling.
Our joining technique could do better than either
C*1 or C*2, by entertaining also a third possibility,
which combines the two candidates. We construct
a mixture model by adding together all counts from
C*1 and C*2 into C+ = C*1 + C*2. Original initializers
C1, C2 will, this way, have equal pull on the merged
model,1 regardless of nominal size (because C*1, C*2
will have converged using a shared training set, D).
We return the best of C*1, C*2 and C*+ = L(C+). This
approach may uncover more (and never returns less)
likely solutions than choosing among C*1, C*2 alone:
We will use a short-hand notation to represent the
combiner network diagrammed above, less clutter:
</bodyText>
<equation confidence="0.293636">
(6)
</equation>
<sectionHeader confidence="0.971155" genericHeader="method">
3 The Task and Methodology
</sectionHeader>
<bodyText confidence="0.9997751">
We apply transform and join paradigms to grammar
induction, an important problem of computational
linguistics that involves notoriously difficult objec-
tives (Pereira and Schabes, 1992; de Marcken, 1995;
Gimpel and Smith, 2012, inter alia). The goal is to
induce grammars capable of parsing unseen text. In-
put, in both training and testing, is a sequence of to-
kens labeled as: (i) a lexical item and its category,
(w,c,,,); (ii) a punctuation mark; or (iii) a sentence
boundary. Output is unlabeled dependency trees.
</bodyText>
<subsectionHeader confidence="0.999325">
3.1 Models and Data
</subsectionHeader>
<bodyText confidence="0.999880142857143">
We constrain all parse structures to be projective, via
dependency-and-boundary grammars (Spitkovsky et
al., 2012a; 2012b): DBMs 0–3 are head-outward
generative parsing models (Alshawi, 1996) that dis-
tinguish complete sentences from incomplete frag-
ments in a corpus D: Dcomp comprises inputs ending
with punctuation; Dfrag = D − Dcomp is everything
</bodyText>
<footnote confidence="0.930379">
1If desired, a scaling factor could be used to bias C+ towards
either C∗1 or C∗2, for example based on their likelihood ratio.
</footnote>
<equation confidence="0.876376">
LD
C*1 + C*2 = C+
LD
C*1 = L(C1)
</equation>
<figure confidence="0.7772865">
(5)
+
LD
arg MAX GD
C*2 = L(C2)
C1
C2
(2)
(4)
LD
C1
C2
</figure>
<page confidence="0.992672">
1984
</page>
<bodyText confidence="0.999703256410256">
else. The “complete” subset is further partitioned
into simple sentences, Dsimp ⊆ Dcomp, with no inter-
nal punctuation, and others, which may be complex.
As an example, consider the beginning of an arti-
cle from (simple) Wikipedia: (i) Linguistics (ii) Lin-
guistics (sometimes called philology) is the science
that studies language. (iii) Scientists who study lan-
guage are called linguists. Since the title does not
end with punctuation, it would be relegated to Dfrag.
But two complete sentences would be in Dcomp, with
the last also filed under Dsimp, as it has only a trail-
ing punctuation mark. Spitkovsky et al. suggested
two curriculum learning strategies: (i) one in which
induction begins with clean, simple data, Dsimp, and
a basic model, DBM-1 (2012b); and (ii) an alterna-
tive bootstrapping approach: starting with still more,
simpler data — namely, short inter-punctuation frag-
ments up to length l = 15, Dsplit ⊇ Dsimp — and a
bare-bones model, DBM-0 (2012a). In our example,
Dsplit would hold five text snippets: (i) Linguistics;
(ii) Linguistics; (iii) sometimes called philology;
(iv) is the science that studies language; and (v) Sci-
entists who study language are called linguists.
Only the last piece of text would still be considered
complete, isolating its contribution to sentence root
and boundary word distributions from those of in-
complete fragments. The sparse model, DBM-0, as-
sumes a uniform distribution for roots of incomplete
inputs and reduces conditioning contexts of stopping
probabilities, which works well with split data. We
will exploit both DBM-0 and the full DBM,2 draw-
ing also on split, simple and raw views of input text.
All experiments prior to final multi-lingual eval-
uation will use the Penn English Treebank’s Wall
Street Journal (WSJ) portion (Marcus et al., 1993) as
the underlying tokenized and sentence-broken cor-
pus D. Instead of gold parts-of-speech, we plugged
in 200 context-sensitive unsupervised tags, from
Spitkovsky et al. (2011c),3 for the word categories.
</bodyText>
<subsectionHeader confidence="0.999911">
3.2 Smoothing and Lexicalization
</subsectionHeader>
<bodyText confidence="0.9424925">
All unlexicalized instances of DBMs will be esti-
mated with “add one” (a.k.a. Laplace) smoothing,
</bodyText>
<footnote confidence="0.8852008">
2We use the short-hand DBM to refer to DBM-3, which is
equivalent to DBM-2 if D has no internally-punctuated sen-
tences (D=Dsplit), and DBM-1 if all inputs also have trailing
punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0.
3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2
</footnote>
<bodyText confidence="0.999753166666667">
using only the word category c,,, to represent a token.
Fully-lexicalized grammars (L-DBM) are left un-
smoothed, and represent each token as both a word
and its category, i.e., the whole pair (w, c,,,). To eval-
uate a lexicalized parsing model, we will always ob-
tain a delexicalized-and-smoothed instance first.
</bodyText>
<subsectionHeader confidence="0.999455">
3.3 Optimization and Viterbi Decoding
</subsectionHeader>
<bodyText confidence="0.9999955">
We use “early-switching lateen” EM (Spitkovsky et
al., 2011a, §2.4) to train unlexicalized models, alter-
nating between the objectives of ordinary (soft) and
hard EM algorithms, until neither can improve its
own objective without harming the other’s. This ap-
proach does not require tuning termination thresh-
olds, allowing optimizers to run to numerical con-
vergence if necessary, and handles only our shorter
inputs (l &lt; 15), starting with soft EM (L = SL, for
“soft lateen”). Lexicalized models will cover full
data (l &lt; 45) and employ “early-stopping lateen”
EM (2011a, §2.3), re-estimating via hard EM until
soft EM’s objective suffers. Alternating EMs would
be expensive here, since updates take (at least) O(l3)
time, and hard EM’s objective (L = H) is the one
better suited to long inputs (Spitkovsky et al., 2010).
Our decoders always force an inter-punctuation
fragment to derive itself (Spitkovsky et al., 2011b,
§2.2).4 In evaluation, such (loose) constraints may
help attach sometimes and philology to called (and
the science... to is). In training, stronger (strict)
constraints also disallow attachment of fragments’
heads by non-heads, to connect Linguistics, called
and is (assuming each piece got parsed correctly).
</bodyText>
<subsectionHeader confidence="0.998622">
3.4 Final Evaluation and Metrics
</subsectionHeader>
<bodyText confidence="0.99992375">
Evaluation is against held-out CoNLL shared task
data (Buchholz and Marsi, 2006; Nivre et al., 2007),
spanning 19 languages. We compute performance
as directed dependency accuracies (DDA), fractions
of correct unlabeled arcs in parsed output (an extrin-
sic metric).5 For most WSJ experiments we include
also sentence and parse tree cross-entropies (soft and
hard EMs’ intrinsic metrics), in bits per token (bpt).
</bodyText>
<footnote confidence="0.9899865">
4But these constraints do not impact training with shorter
inputs, since there is no internal punctuation in Dsplit or Dsimp.
5We converted gold labeled constituents in WSJ to unlabeled
reference dependencies using deterministic “head-percolation”
rules (Collins, 1999); sentence root symbols, though not punc-
tuation arcs, contribute to scores, as is standard (Paskin, 2001b).
</footnote>
<page confidence="0.996895">
1985
</page>
<sectionHeader confidence="0.995537" genericHeader="method">
4 Concrete Operators
</sectionHeader>
<bodyText confidence="0.999944875">
We will now instantiate the operators sketched out
in §2 specifically for the grammar induction task.
Throughout, we repeatedly employ single steps of
Viterbi training to transfer information between sub-
networks in a model-independent way: when a mod-
ule’s output is a set of (Viterbi) parse trees, it neces-
sarily contains sufficient information required to es-
timate an arbitrarily-factored model down-stream.6
</bodyText>
<subsectionHeader confidence="0.991331">
4.1 Transform #1: A Simple Filter
</subsectionHeader>
<bodyText confidence="0.99996">
Given a model that was estimated from (and there-
fore parses) a data set D, the simple filter (F) at-
tempts to extract a cleaner model, based on the sim-
pler complete sentences of Dsi.p. It is implemented
as a single (unlexicalized) step of Viterbi training:
The idea here is to focus on sentences that are not
too complicated yet grammatical. This punctuation-
sensitive heuristic may steer a learner towards easy
but representative training text and, we showed, aids
grammar induction (Spitkovsky et al., 2012b, §7.1).
</bodyText>
<subsectionHeader confidence="0.977969">
4.2 Transform #2: A Symmetrizer
</subsectionHeader>
<bodyText confidence="0.9999843">
The symmetrizer (S) reduces input models to sets of
word association scores. It blurs all details of in-
duced parses in a data set D, except the number of
times each (ordered) word pair participates in a de-
pendency relation. We implemented symmetrization
also as a single unlexicalized Viterbi training step,
but now with proposed parse trees’ scores, for a sen-
tence in D, proportional to a product over non-root
dependency arcs of one plus how often the left and
right tokens (are expected to) appear connected:
</bodyText>
<equation confidence="0.804842">
(8)
</equation>
<bodyText confidence="0.986080125">
The idea behind the symmetrizer is to glean infor-
mation from skeleton parses. Grammar inducers can
sometimes make good progress in resolving undi-
rected parse structures despite being wrong about
the polarities of most arcs (Spitkovsky et al., 2009,
Figure 3: Uninformed). Symmetrization offers an
extra chance to make heads or tails of syntactic rela-
tions, after learning which words tend to go together.
</bodyText>
<footnote confidence="0.810546">
6A related approach — initializing EM training with an
M-step — was advocated by Klein and Manning (2004, §3).
</footnote>
<bodyText confidence="0.99961952631579">
At each instance where a word ® attaches O on
(say) the right, our implementation attributes half its
weight to the intended construction, O&apos;z�, reserving
the other half for the symmetric structure, O attach-
ing ® to its left: Vz�. For the desired effect, these
aggregated counts are left unnormalized, while all
other counts (of word fertilities and sentence roots)
get discarded. To see why we don’t turn word attach-
ment scores into probabilities, consider sentences
�a O and © z�. The fact that O co-occurs with O
introduces an asymmetry into z�’s relation with c�:
?( O  |c�) = 1 differs from ?( © |z�) = 1/2. Normal-
izing might force the interpretation Co (and also
g&apos;z�), not because there is evidence in the data, but
as a side-effect of a model’s head-driven nature (i.e.,
factored with dependents conditioned on heads). Al-
ways branching right would be a mistake, however,
for example if O is a noun, since either of O or �c
could be a determiner, with the other a verb.
</bodyText>
<subsectionHeader confidence="0.987305">
4.3 Join: A Combiner
</subsectionHeader>
<bodyText confidence="0.999990272727273">
The combiner must admit arbitrary inputs, includ-
ing models not estimated from D, unlike the trans-
forms. Consequently, as a preliminary step, we con-
vert each input Ci into parse trees of D, with counts
C′i, via Viterbi-decoding with a smoothed, unlexical-
ized version of the corresponding incoming model.
Actual combination is then performed in a more pre-
cise (unsmoothed) fashion: C∗i are the (lexicalized)
solutions starting from C′i; and C∗� is initialized with
their sum, Ei C∗i . Counts of the lexicalized model
with lowest cross-entropy on D become the output:7
</bodyText>
<sectionHeader confidence="0.980602" genericHeader="method">
5 Basic Networks
</sectionHeader>
<bodyText confidence="0.999986">
We are ready to propose a non-trivial subnetwork for
grammar induction, based on the transform and join
operators, which we will reuse in larger networks.
</bodyText>
<subsectionHeader confidence="0.866017">
5.1 Fork/Join (FJ)
</subsectionHeader>
<bodyText confidence="0.99993275">
Given a model that parses a base data set D0, the
fork/join subnetwork will output an adaptation of
that model for D. It could facilitate a grammar in-
duction process, e.g., by advancing it from smaller
</bodyText>
<footnote confidence="0.664151">
7In our diagrams, lexicalized modules are shaded black.
</footnote>
<figure confidence="0.661488333333333">
C,
C2
LD
C F (7)
C S
(9)
</figure>
<page confidence="0.903619">
1986
</page>
<bodyText confidence="0.9936706">
to larger — or possibly more complex — data sets.
We first fork off two variations of the incoming
model based on D0: (i) a filtered view, which fo-
cuses on cleaner, simpler data (transform #1); and
(ii) a symmetrized view that backs off to word asso-
ciations (transform #2). Next is grammar induction
over D. We optimize a full DBM instance starting
from the first fork, and bootstrap a reduced DBMo
from the second. Finally, the two new induced sets
of parse trees, for D, are merged (lexicalized join):
</bodyText>
<equation confidence="0.986649">
(10)
C1
C′
1
HL·DBM
D
C′
2
SLDBM0
D
</equation>
<bodyText confidence="0.985978533333333">
The idea here is to prepare for two scenarios: an
incoming grammar that is either good or bad for D.
If the model is good, DBM should be able to hang
on to it and make improvements. But if it is bad,
DBM could get stuck fitting noise, whereas DBMo
might be more likely to ramp up to a good alterna-
tive. Since we can’t know ahead of time which is the
true case, we pursue both optimization paths simul-
taneously and let a combiner later decide for us.
Note that the forks start (and end) optimizing with
soft EM. This is because soft EM integrates previ-
ously unseen tokens into new grammars better than
hard EM, as evidenced by our failed attempt to re-
produce the “baby steps” strategy with Viterbi train-
ing (Spitkovsky et al., 2010, Figure 4). A combiner
then executes hard EM, and since outputs of trans-
forms are trees, the end-to-end process is a chain of
lateen alternations that starts and ends with hard EM.
We will use a “grammar inductor” to represent
subnetworks that transition from Dlsplit to Dl+1 , by
split
taking transformed parse trees of inter-punctuation
fragments up to length l (base data set, D0) to ini-
tialize training over fragments up to length l + 1:
The FJ network instantiates a grammar inductor
with l = 14, thus training on inter-punctuation frag-
ments up to length 15, as in previous work, starting
from an empty set of counts, C = 0. Smoothing
causes initial parse trees to be chosen uniformly at
random, as suggested by Cohen and Smith (2010):
</bodyText>
<figure confidence="0.5256475">
15
�
</figure>
<subsectionHeader confidence="0.954469">
5.2 Iterated Fork/Join (IFJ)
</subsectionHeader>
<bodyText confidence="0.9978485">
Our second network daisy-chains grammar induc-
tors, starting from the single-word inter-punctuation
fragments in D1 split, then retraining on D2split, and so
forth, until finally stopping at D15 it, as before:
</bodyText>
<subsectionHeader confidence="0.571957">
spl
</subsectionHeader>
<bodyText confidence="0.99999119047619">
We diagrammed this system as not taking an input,
since the first inductor’s output is fully determined
by unique parse trees of single-token strings. This
iterative approach to optimization is akin to deter-
ministic annealing (Rose, 1998), and is patterned af-
ter “baby steps” (Spitkovsky et al., 2009, §4.2).
Unlike the basic FJ, where symmetrization was a
no-op (since there were no counts in C = 0), IFJ
makes use of symmetrizers — e.g., in the third in-
ductor, whose input is based on strings with up to
two tokens. Although it should be easy to learn
words that go together from very short fragments,
extracting correct polarities of their relations could
be a challenge: to a large extent, outputs of early in-
ductors may be artifacts of how our generative mod-
els factor (see §4.2) or how ties are broken in opti-
mization (Spitkovsky et al., 2012a, Appendix B). We
therefore expect symmetrization to be crucial in ear-
lier stages but to weaken any high quality grammars,
nearer the end; it will be up to combiners to handle
such phase transitions correctly (or gracefully).
</bodyText>
<subsectionHeader confidence="0.99568">
5.3 Grounded Iterated Fork/Join (GIFJ)
</subsectionHeader>
<bodyText confidence="0.999948714285714">
So far, our networks have been either purely itera-
tive (IFJ) or static (FJ). These two approaches can
also be combined, by injecting FJ’s solutions into
IFJ’s more dynamic stream. Our new transition sub-
network will join outputs of grammar inductors that
either (i) continue a previous solution (as in IFJ); or
(ii) start over from scratch (“grounding” to an FJ):
</bodyText>
<equation confidence="0.8200105">
HL·DBM
l+1 Dl+� Cl+1
</equation>
<bodyText confidence="0.896928333333333">
split
The full GIFJ network can then be obtained by un-
rolling the above template from l = 14 back to one.
</bodyText>
<figure confidence="0.981568090909091">
C
F
Do
S
SLDBM
D
C2
l+1
C
(11)
(12)
1 2 14 15
(13)
l+1
Cl
�
(14)
1987
15 WSJ15
WSJsplit sim
Instance Label
0=C
</figure>
<equation confidence="0.9873035">
SL(S(C)) = C2
SL(F(C)) = C1
H(C2) = C2
H(C&apos;) = Cl
C1 + C�2 = C+
H(C+) = C+
</equation>
<table confidence="0.985937">
Model hsents htrees DDA hsents htrees DDA TA
DBM 6.54 6.75 83.7 6.05 6.21 85.1 42.7
— 8.76 10.46 21.4 8.58 10.52 20.7 3.9
DBM0 6.18 6.39 57.0 5.90 6.11 57.5 10.4
DBM 5.89 5.99 62.2 5.79 5.90 60.9 12.0
L-DBM 7.28 7.30 59.2 6.87 6.88 58.6 10.4
L-DBM 7.07 7.08 62.3 6.72 6.73 60.8 12.0
L-DBM 7.20 7.27 64.0 6.82 6.88 62.5 12.3
L-DBM 7.02 7.04 64.2 6.64 6.65 62.7 12.8
L-DBM 6.95 6.96 70.5 6.55 6.56 68.2 14.9
L-DBM 6.91 6.92 71.4 6.52 6.52 69.2 15.6
L-DBM 6.83 6.83 72.3 6.41 6.41 70.2 17.9
L-DBM 6.92 6.93 71.9 6.53 6.53 69.8 16.7
L-DBM 6.83 6.83 72.9 6.41 6.41 70.6 18.0
Description
Supervised (MLE of WSJ45)
Random Projective Parses
B 1 Unlexicalized
A j Baselines
Baseline
Combination
Fork/Join
Iterated Fork/Join (IFJ)
Grounded Iterated Fork/Join
Grammar Transformer (GT)
IFJ 1 w/Iterated
GT 1 Combiners
I
</table>
<tableCaption confidence="0.987999">
Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments
up to length 15 (WSJ15
</tableCaption>
<bodyText confidence="0.732733">
split) and its subset of simple, complete sentences (WSJ15
simp, with exact tree accuracies — TA).
</bodyText>
<sectionHeader confidence="0.969972" genericHeader="method">
6 Performance of Basic Networks
</sectionHeader>
<bodyText confidence="0.9999205">
We compared our three networks’ performance on
their final training sets, WSJ15
split (see Table 1, which
also tabulates results for a cleaner subset, WSJ15
simp).
The first network starts from C = ∅, helping us es-
tablish several straw-man baselines. Its empty ini-
tializer corresponds to guessing (projective) parse
trees uniformly at random, which has 21.4% accu-
racy and sentence string cross-entropy of 8.76bpt.
</bodyText>
<subsectionHeader confidence="0.73597">
6.1 Fork/Join (FJ)
</subsectionHeader>
<bodyText confidence="0.9999742">
FJ’s symmetrizer yields random parses of WSJ14
split,
which initialize training of DBM0. This baseline (B)
lowers cross-entropy to 6.18bpt and scores 57.0%.
FJ’s filter starts from parse trees of WSJ14
simp only, and
trains up a full DBM. This choice makes a stronger
baseline (A), with 5.89bpt cross-entropy, at 62.2%.
The join operator uses counts from A and B, C1
and C2, to obtain parse trees whose own counts C′1
and C′2 initialize lexicalized training. From each C′i,
an optimizer arrives at C∗i . Grammars corresponding
to these counts have higher cross-entropies, because
of vastly larger vocabularies, but also better accura-
cies: 59.2 and 62.3%. Their mixture C+ is a simple
sum of counts in C∗1 and C∗2: it is not expected to be
an improvement but happens to be a good move, re-
sulting in a grammar with higher accuracy (64.0%),
though not better Viterbi cross-entropy (7.27 falls
between 7.08 and 7.30bpt) than both sources. The
combiner’s third alternative, a locally optimal C∗+, is
then obtained by re-optimizing from C+. This so-
lution performs slightly better (64.2%) and will be
the local optimum returned by FJ’s join operator, be-
cause it attains the lowest cross-entropy (7.04bpt).
</bodyText>
<subsectionHeader confidence="0.990521">
6.2 Iterated Fork/Join (IFJ)
</subsectionHeader>
<bodyText confidence="0.999989461538461">
IFJ’s iterative approach results in an improvement:
70.5% accuracy and 6.96bpt cross-entropy. To test
how much of this performance could be obtained by
a simpler iterated network, we experimented with
ablated systems that don’t fork or join, i.e., our clas-
sic “baby steps” schema (chaining together 15 op-
timizers), using both DBM and DBM0, with and
without a transform in-between. However, all such
“linear” networks scored well below 50%. We con-
clude from these results that an ability to branch out
into different promising regions of a solution space,
and to merge solutions of varying quality into better
models, are important properties of FJ subnetworks.
</bodyText>
<subsectionHeader confidence="0.99334">
6.3 Grounded Iterated Fork/Join (GIFJ)
</subsectionHeader>
<bodyText confidence="0.99993775">
Grounding improves GIFJ’s performance further, to
71.4% accuracy and 6.92bpt cross-entropy. This re-
sult shows that fresh perspectives from optimizers
that start over can make search efforts more fruitful.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="method">
7 Enhanced Subnetworks
</sectionHeader>
<bodyText confidence="0.984866">
Modularity and abstraction allow for compact repre-
sentations of complex systems. Another key benefit
is that individual components can be understood and
improved in isolation, as we will demonstrate next.
</bodyText>
<page confidence="0.877865">
1988
</page>
<figure confidence="0.994956">
SDBM
D�
simp
0
l+1 �
�
�
</figure>
<subsectionHeader confidence="0.910618">
7.1 An Iterative Combiner (IC)
</subsectionHeader>
<bodyText confidence="0.9999377">
Our basic combiner introduced a third option, C∗+,
into a pool of candidate solutions, {C∗1, C∗2 }. This
new entry may not be a simple mixture of the orig-
inals, because of non-linear effects from applying L
to C∗1 + C∗2, but could most likely still be improved.
Rather than stop at C∗+, when it is better than both
originals, we could recombine it with a next best so-
lution, continuing until no further improvement is
made. Iterating can’t harm a given combiner’s cross-
entropy (e.g., it lowers FJ’s from 7.04 to 7.00bpt),
and its advantages can be realized more fully in the
larger networks (albeit without any end-to-end guar-
antees): upgrading all 15 combiners in IFJ would
improve performance (slightly) more than ground-
ing (71.5 vs. 71.4%), and lower cross-entropy (from
6.96 to 6.93bpt). But this approach is still a bit timid.
A more greedy way is to proceed so long as C∗+
is not worse than both predecessors. We shall now
state our most general iterative combiner (IC) algo-
rithm: Start with a solution pool p = {C∗i }ni=1. Next,
construct p′ by adding C∗+ = L(Eni=1 C∗i ) to p and re-
moving the worst of n + 1 candidates in the new set.
Finally, if p = p′, return the best of the solutions in p;
otherwise, repeat from p := p′. At n = 2, one could
think of taking L(C∗1 + C∗2) as performing a kind of
bisection search in some (strange) space. With these
new and improved combiners, the IFJ network per-
forms better: 71.9% (up from 70.5 — see Table 1),
lowering cross-entropy (down from 6.96 to 6.93bpt).
We propose a distinguished notation for the ICs:
</bodyText>
<equation confidence="0.509277">
(15)
</equation>
<subsectionHeader confidence="0.990773">
7.2 A Grammar Transformer (GT)
</subsectionHeader>
<bodyText confidence="0.9997875">
The levels of our systems’ performance at grammar
induction thus far suggest that the space of possible
networks (say, with up to k components) may itself
be worth exploring more thoroughly. We leave this
exercise to future work, ending with two relatively
straight-forward extensions for grounded systems.
Our static bootstrapping mechanism (“ground” of
GIFJ) can be improved by pretraining with simple
sentences first — as in the curriculum for learning
DBM-1 (Spitkovsky et al., 2012b, §7.1), but now
with a variable length cut-off l (much lower than the
original 45) — instead of starting from ∅ directly:
</bodyText>
<equation confidence="0.980025">
(16)
l
</equation>
<bodyText confidence="0.9999585">
The output of this subnetwork can then be refined,
by reconciling it with a previous dynamic solution.
We perform a mini-join of a new ground’s counts
with Cl, using the filter transform (single steps of
lexicalized Viterbi training on clean, simple data),
ahead of the main join (over more training data):
</bodyText>
<equation confidence="0.9751595">
(17)
l
</equation>
<bodyText confidence="0.99984775">
This template can be unrolled, as before, to obtain
our last network (GT), which achieves 72.9% accu-
racy and 6.83bpt cross-entropy (slightly less accu-
rate with basic combiners, at 72.3% — see Table 1).
</bodyText>
<sectionHeader confidence="0.759955" genericHeader="method">
8 Full Training and System Combination
</sectionHeader>
<bodyText confidence="0.702104">
All systems that we described so far stop training at
</bodyText>
<equation confidence="0.35136">
D15
</equation>
<bodyText confidence="0.977859652173913">
split. We will use a two-stage adaptor network to
transition their grammars to a full data set, D45:
The first stage exposes grammar inducers to longer
inputs (inter-punctuation fragments with up to 45
tokens); the second stage, at last, reassembles text
snippets into actual sentences (also up to l = 45).8
After full training, our IFJ and GT systems parse
Section 23 of WSJ at 62.7 and 63.4% accuracy, bet-
ter than the previous state-of-the-art (61.2% — see
Table 2). To test the generalized IC algorithm, we
merged our implementations of these three strong
grammar induction pipelines into a combined sys-
tem (CS). It scored highest: 64.4%.
The quality of bracketings corresponding to (non-
trivial) spans derived by heads of our dependency
structures is competitive with the state-of-the-art in
unsupervised constituent parsing. On the WSJ sen-
tences up to length 40 in Section 23, CS attains sim-
ilar F1-measure (54.2 vs. 54.6, with higher recall) to
8Note that smoothing in the final (unlexicalized) Viterbi step
masks the fact that model parts that could not be properly es-
timated in the first stage (e.g., probabilities of punctuation-
crossing arcs) are being initialized to uniform multinomials.
</bodyText>
<figure confidence="0.9976368">
HL·DBM
D45
HL·DBM
D45
split
C
(18)
#3
(IFJ) #2
(GT) #1
HL·DBM
D45
CS
(19)
*
C1
C2
HL•DBM
Cl F D&apos;1+PlitCl+1
l+1
</figure>
<page confidence="0.976539">
1989
</page>
<table confidence="0.998588692307692">
System DDA (@10)
(Gimpel and Smith, 2012) 53.1 (64.3)
(Gillenwater et al., 2010) 53.3 (64.3)
(Bisk and Hockenmaier, 2012) 53.3 (71.5)
(Blunsom and Cohn, 2010) 55.7 (67.7)
(Tu and Honavar, 2012) 57.0 (71.4)
(Spitkovsky et al., 2011b) 58.4 (71.4)
(Spitkovsky et al., 2011c) 59.1 (71.4)
#3 (Spitkovsky et al., 2012a) 61.2 (71.4)
#2 w/Full TrainingIFJ 62.7 (70.3)
#1 ( GT 63.4 (70.3)
#1 + #2 + #3 System Combination CS 64.4 (72.0)
Supervised DBM (also with loose decoding) 76.3 (85.4)
</table>
<tableCaption confidence="0.85174725">
Table 2: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences and up to length ten) for
recent systems, our full networks (IFJ and GT), and three-
way combination (CS) with the previous state-of-the-art.
</tableCaption>
<bodyText confidence="0.882875">
PRLG (Ponvert et al., 2011), which is the strongest
system of which we are aware (see Table 3).9
</bodyText>
<sectionHeader confidence="0.992383" genericHeader="method">
9 Multi-Lingual Evaluation
</sectionHeader>
<bodyText confidence="0.959339821428571">
Last, we checked how our algorithms generalize out-
side English WSJ, by testing in 23 more set-ups: all
2006/7 CoNLL test sets (Buchholz and Marsi, 2006;
Nivre et al., 2007), spanning 19 languages. Most re-
cent work evaluates against this multi-lingual data,
with the unrealistic assumption of part-of-speech
tags. But since inducing high quality word clusters
for many languages would be beyond the scope of
our paper, here we too plugged in gold tags for word
categories (instead of unsupervised tags, as in §3–8).
We compared to the two strongest systems we
knew:10 MZ (Mareˇcek and ˇZabokrtsk´y, 2012) and
SAJ (Spitkovsky et al., 2012b), which report average
accuracies of 40.0 and 42.9% for CoNLL data (see
Table 4). Our fully-trained IFJ and GT systems score
40.0 and 47.6%. As before, combining these net-
works with our own implementation of the best pre-
vious state-of-the-art system (SAJ) yields a further
improvement, increasing final accuracy to 48.6%.
9These numbers differ from Ponvert et al.’s (2011, Table 6)
for the full Section 23 because we restricted their eval-ps.py
script to a maximum length of 40 words, in our evaluation, to
match other previous work: Golland et al.’s (2012, Figure 1) for
CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest.
10During review, another strong system (Mareˇcek and Straka,
2013, scoring 48.7%) of possible interest to the reader came out,
exploiting prior knowledge of stopping probabilities (estimated
from large POS-tagged corpora, via reducibility principles).
</bodyText>
<table confidence="0.998794076923077">
System F1
Binary-Branching Upper Bound 85.7
Left-Branching Baseline 12.0
CCM (Klein and Manning, 2002) 33.7
Right-Branching Baseline 40.7
F-CCM (Huang et al., 2012) 45.1
HMM (Ponvert et al., 2011) 46.3
LLCCM (Golland et al., 2012) 47.6 P R
CCL (Seginer, 2007) 52.8 54.6 51.1
PRLG (Ponvert et al., 2011) 54.6 60.4 49.8
CS System Combination 54.2 55.6 52.8
Supervised DBM Skyline 59.3 65.7 54.1
Dependency-Based Upper Bound 87.2 100 77.3
</table>
<tableCaption confidence="0.982387">
Table 3: Harmonic mean (F1) of precision (P) and re-
call (R) for unlabeled constituent bracketings on Section
23 of WSJ (sentences up to length 40) for our combined
system (CS), recent state-of-the-art and the baselines.
</tableCaption>
<sectionHeader confidence="0.996281" genericHeader="method">
10 Discussion
</sectionHeader>
<bodyText confidence="0.9999098">
CoNLL training sets were intended for comparing
supervised systems, and aren’t all suitable for unsu-
pervised learning: 12 languages have under 10,000
sentences (with Arabic, Basque, Danish, Greek, Ital-
ian, Slovenian, Spanish and Turkish particularly
small), compared to WSJ’s nearly 50,000. In some
treebanks sentences are very short (e.g., Chinese and
Japanese, which appear to have been split on punc-
tuation), and in others extremely long (e.g., Arabic).
Even gold tags aren’t always helpful, as their num-
ber is rarely ideal for grammar induction (e.g., 42 vs.
200 for English). These factors contribute to high
variances of our (and previous) results (see Table 4).
Nevertheless, if we look at the more stable aver-
age accuracies, we see a positive trend as we move
from a simpler fully-trained system (IFJ, 40.0%),
to a more complex system (GT, 47.6%), to system
combination (CS, 48.6%). Grounding seems to be
more important for the CoNLL sets, possibly be-
cause of data sparsity or availability of gold tags.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="related work">
11 Related Work
</sectionHeader>
<bodyText confidence="0.999789857142857">
The surest way to avoid local optima is to craft
an objective that doesn’t have them. For example,
Wang et al. (2008) demonstrated a convex train-
ing method for semi-supervised dependency pars-
ing; Lashkari and Golland (2008) introduced a con-
vex reformulation of likelihood functions for clus-
tering tasks; and Corlett and Penn (2010) designed
</bodyText>
<page confidence="0.987438">
1990
</page>
<table confidence="0.994879615384615">
Directed Dependency Accuracies (DDA) (@10)
MZ SAJ IFJ GT CS
26.5 10.9 33.3 8.3 9.3 (30.2)
27.9 44.9 26.1 25.6 26.8 (45.6)
26.8 33.3 23.5 24.2 24.4 (32.8)
46.0 65.2 35.8 64.2 63.4 (69.1)
47.0 62.1 65.0 68.4 68.0 (79.2)
— 63.2 56.0 55.8 58.4 (60.8)
— 57.0 49.0 48.6 52.5 (56.0)
49.5 55.1 44.5 43.9 44.0 (52.3)
48.0 54.2 42.9 24.5 34.3 (51.1)
38.6 22.2 37.8 17.1 21.4 (29.8)
44.2 46.6 40.8 51.3 48.0 (48.7)
49.2 29.6 39.3 57.6 58.2 (75.0)
44.8 39.1 34.1 54.5 56.2 (71.2)
20.2 26.9 23.7 45.0 45.4 (52.2)
51.8 58.2 24.8 52.9 58.3 (67.6)
43.3 40.7 56.8 31.1 34.9 (44.9)
50.8 22.7 32.6 63.7 63.0 (68.9)
50.6 72.4 38.0 72.7 74.5 (81.1)
18.1 35.2 42.1 50.8 50.9 (57.3)
51.9 28.2 57.0 61.7 61.4 (73.2)
48.2 50.7 46.6 48.6 49.7 (62.1)
— 34.4 28.0 32.9 29.2 (33.2)
15.7 44.8 42.1 41.7 37.9 (42.4)
Average: 40.0 42.9 40.0 47.6 48.6 (57.8)
</table>
<tableCaption confidence="0.7951518">
Table 4: Blind evaluation on 2006/7 CoNLL test sets (all
sentences) for our full networks (IFJ and GT), previous
state-of-the-art systems of Spitkovsky et al. (2012b) and
Mareˇcek and ˇZabokrtsk´y (2012), and three-way combi-
nation with SAJ (CS, including results up to length ten).
</tableCaption>
<bodyText confidence="0.999974205882353">
a search algorithm for encoding decipherment prob-
lems that guarantees to quickly converge on optimal
solutions. Convexity can be ideal for comparative
analyses, by eliminating dependence on initial con-
ditions. But for many NLP tasks, including grammar
induction, the most relevant known objective func-
tions are still riddled with local optima. Renewed ef-
forts to find exact solutions (Eisner, 2012; Gormley
and Eisner, 2013) may be a good fit for the smaller
and simpler, earlier stages of our iterative networks.
Multi-start methods (Solis and Wets, 1981) can
recover certain global extrema almost surely (i.e.,
with probability approaching one). Moreover, ran-
dom restarts via uniform probability measures can
be optimal, in a worst-case-analysis sense, with par-
allel processing sometimes leading to exponential
speed-ups (Hu et al., 1994). This approach is rarely
emphasized in NLP literature. For instance, Moore
and Quirk (2008) demonstrated consistent, substan-
tial gains from random restarts in statistical machine
translation (but also suggested better and faster re-
placements — see below); Ravi and Knight (2009,
§5, Figure 8) found random restarts for EM to be
crucial in parts-of-speech disambiguation. However,
other reviews are few and generally negative (Kim
and Mooney, 2010; Martin-Brualla et al., 2010).
Iterated local search methods (Hoos and St¨utzle,
2004; Johnson et al., 1988, inter alia) escape lo-
cal basins of attraction by perturbing candidate so-
lutions, without undoing all previous work. “Large-
step” moves can come from jittering (Hinton and
Roweis, 2003), dithering (Price et al., 2005, Ch. 2)
or smoothing (Bhargava and Kondrak, 2009). Non-
improving “sideways” moves offer substantial help
with hard satisfiability problems (Selman et al.,
1992); and injecting non-random noise (Selman et
al., 1994), by introducing “uphill” moves via mix-
tures of random walks and greedy search strate-
gies, does better than random noise alone or simu-
lated annealing (Kirkpatrick et al., 1983). In NLP,
Moore and Quirk’s (2008) random walks from pre-
vious local optima were faster than uniform sam-
pling and also increased BLEU scores; Elsner and
Schudy (2009) showed that local search can outper-
form greedy solutions for document clustering and
chat disentanglement tasks; and Mei et al. (2001)
incorporated tabu search (Glover, 1989; Glover and
Laguna, 1993, Ch. 3) into HMM training for ASR.
Genetic algorithms are a fusion of what’s best in
local search and multi-start methods (Houck et al.,
1996), exploiting a problem’s structure to combine
valid parts of any partial solutions (Holland, 1975;
Goldberg, 1989). Evolutionary heuristics proved
useful in the induction of phonotactics (Belz, 1998),
text planning (Mellish et al., 1998), factored mod-
eling of morphologically-rich languages (Duh and
Kirchhoff, 2004) and plot induction for story gener-
ation (McIntyre and Lapata, 2010). Multi-objective
genetic algorithms (Fonseca and Fleming, 1993) can
handle problems with equally important but con-
flicting criteria (Stadler, 1988), using Pareto-optimal
ensembles. They are especially well-suited to lan-
guage, which evolves under pressures from compet-
ing (e.g., speaker, listener and learner) constraints,
and have been used to model configurations of vow-
els and tone systems (Ke et al., 2003). Our transform
and join mechanisms also exhibit some features of
genetic search, and make use of competing objec-
</bodyText>
<figure confidence="0.994831625">
CoNLL Data
Arabic 2006
’7
Basque ’7
Bulgarian ’7
Catalan ’7
Chinese ’6
’7
Czech ’6
’7
Danish ’6
Dutch ’6
English ’7
German ’6
Greek ’6
Hungarian ’7
Italian ’7
Japanese ’6
Portuguese ’6
Slovenian ’6
Spanish ’6
Swedish ’6
Turkish ’6
’7
</figure>
<page confidence="0.986338">
1991
</page>
<bodyText confidence="0.999686">
tives: good sets of parse trees must make sense both
lexicalized and with word categories, to rich and im-
poverished models of grammar, and for both long,
complex sentences and short, simple text fragments.
This selection of text filters is a specialized case
of more general “data perturbation” techniques —
even cycling over randomly chosen mini-batches
that partition a data set helps avoid some local op-
tima (Liang and Klein, 2009). Elidan et al. (2002)
suggested how example-reweighing could cause “in-
formed” changes, rather than arbitrary damage, to
a hypothesis. Their (adversarial) training scheme
guided learning toward improved generalizations,
robust against input fluctuations. Language learn-
ing has a rich history of reweighing data via (co-
operative) “starting small” strategies (Elman, 1993),
beginning from simpler or more certain cases. This
family of techniques has met with success in semi-
supervised named entity classification (Collins and
Singer, 1999; Yarowsky, 1995),11 parts-of-speech
induction (Clark, 2000; 2003), and language model-
ing (Krueger and Dayan, 2009; Bengio et al., 2009),
in addition to unsupervised parsing (Spitkovsky et
al., 2009; Tu and Honavar, 2011; Cohn et al., 2011).
</bodyText>
<sectionHeader confidence="0.784322" genericHeader="conclusions">
12 Conclusion
</sectionHeader>
<bodyText confidence="0.999877315789474">
We proposed several simple algorithms for combin-
ing grammars and showed their usefulness in merg-
ing the outputs of iterative and static grammar in-
duction systems. Unlike conventional system com-
bination methods, e.g., in machine translation (Xiao
et al., 2010), ours do not require incoming mod-
els to be of similar quality to make improvements.
We exploited these properties of the combiners to
reconcile grammars induced by different views of
data (Blum and Mitchell, 1998). One such view re-
tains just the simple sentences, making it easier to
recognize root words. Another splits text into many
inter-punctuation fragments, helping learn word as-
sociations. The induced dependency trees can them-
selves also be viewed not only as directed structures
but also as skeleton parses, facilitating the recovery
of correct polarities for unlabeled dependency arcs.
By reusing templates, as in dynamic Bayesian
network (DBN) frameworks (Koller and Friedman,
</bodyText>
<footnote confidence="0.6822405">
11The so-called Yarowsky-cautious modification of the orig-
inal algorithm for unsupervised word-sense disambiguation.
</footnote>
<bodyText confidence="0.999916347826087">
2009, §6.2.2), we managed to specify relatively
“deep” learning architectures without sacrificing
(too much) clarity or simplicity. On a still more
speculative note, we see two (admittedly, tenuous)
connections to human cognition. First, the benefits
of not normalizing probabilities, when symmetriz-
ing, might be related to human language process-
ing through the base-rate fallacy (Bar-Hillel, 1980;
Kahneman and Tversky, 1982) and the availability
heuristic (Chapman, 1967; Tversky and Kahneman,
1973), since people are notoriously bad at probabil-
ity (Attneave, 1953; Kahneman and Tversky, 1972;
Kahneman and Tversky, 1973). And second, inter-
mittent “unlearning” — though perhaps not of the
kind that takes place inside of our transforms —
is an adaptation that can be essential to cognitive
development in general, as evidenced by neuronal
pruning in mammals (Craik and Bialystok, 2006;
Low and Cheng, 2006). “Forgetful EM” strategies
that reset subsets of parameters may thus, possibly,
be no less relevant to unsupervised learning than is
“partial EM,” which only suppresses updates, other
EM variants (Neal and Hinton, 1999), or “dropout
training” (Hinton et al., 2012; Wang and Manning,
2013), which is important in supervised settings.
Future parsing models, in grammar induction,
may benefit by modeling head-dependent relations
separately from direction. As frequently employed
in tasks like semantic role labeling (Carreras and
M`arquez, 2005) and relation extraction (Sun et al.,
2011), it may be easier to first establish existence,
before trying to understand its nature. Other key
next steps may include exploring more intelligent
ways of combining systems (Surdeanu and Man-
ning, 2010; Petrov, 2010) and automating the op-
erator discovery process. Furthermore, we are opti-
mistic that both count transforms and model recom-
bination could be usefully incorporated into sam-
pling methods: although symmetrized models may
have higher cross-entropies, hence prone to rejection
in vanilla MCMC, they could work well as seeds
in multi-chain designs; existing algorithms, such as
MCMCMC (Geyer, 1991), which switch contents
of adjacent chains running at different temperatures,
may also benefit from introducing the option to com-
bine solutions, in addition to just swapping them.
</bodyText>
<page confidence="0.995216">
1992
</page>
<sectionHeader confidence="0.999208" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998436">
We thank Yun-Hsuan Sung, for early-stage discussions
on ways of extending “baby steps,” Elias Ponvert, for
sharing all of the relevant experimental results and eval-
uation scripts from his work with Jason Baldridge and
Katrin Erk, and the anonymous reviewers, for their
helpful comments on the draft version of this paper.
Funded, in part, by Defense Advanced Research Projects
Agency (DARPA) Deep Exploration and Filtering of
Text (DEFT) Program, under Air Force Research Lab-
oratory (AFRL) prime contract no. FA8750-13-2-0040.
Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. Once again, the
first author thanks Moofus.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999565946236559">
H. Alshawi. 1996. Head automata for speech translation. In
ICSLP.
F. Attneave. 1953. Psychological probability as a function of
experienced frequency. Experimental Psychology, 46.
M. Bar-Hillel. 1980. The base-rate fallacy in probability judg-
ments. Acta Psychologica, 44.
A. Belz. 1998. Discovering phonotactic finite-state automata
by genetic search. In COLIIG-ACL.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
A. Bhargava and G. Kondrak. 2009. Multiple word alignment
with profile hidden Markov models. In IAACL-HLT:Stu-
dent Research and Doctoral Consortium.
Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar
induction with combinatory categorial grammars. In AAAI.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree
substitution grammars for dependency parsing. In EMILP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoILL.
X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. In CoILL.
L. J. Chapman. 1967. Illusory correlation in observational re-
port. Verbal Learning and Verbal Behavior, 6.
A. Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In CoILL-LLL.
A. Clark. 2003. Combining distributional and morphological
information for part of speech induction. In EACL.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. JMLR.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In EMILP.
M. Collins. 1999. Head-Driven Statistical Models for Iatural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
E. Corlett and G. Penn. 2010. An exact A∗ method for deci-
phering letter-substitution ciphers. In ACL.
F. I. M. Craik and E. Bialystok. 2006. Cognition through the
lifespan: mechanisms of change. TREIDS in Cognitive Sci-
ences, 10.
C. de Marcken. 1995. Lexical heads, phrase structure and the
induction of grammar. In WVLC.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In COLIIG.
J. Eisner. 2012. Grammar induction: Beyond local search. In
ICGI.
G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002.
Data perturbation for escaping local maxima in learning. In
AAAI.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
M. Elsner and W. Schudy. 2009. Bounding and comparing
methods for correlation clustering beyond ILP. In IAACL-
HLT: Integer Linear Programming for ILP.
C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for
multiobjective optimization: Formulation, discussion and
generalization. In ICGA.
C. J. Geyer. 1991. Markov chain Monte Carlo maximum like-
lihood. In Interface Symposium.
J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2012. Concavity and initialization
for unsupervised dependency parsing. In IAACL-HLT.
F. Glover and M. Laguna. 1993. Tabu search. In C. R.
Reeves, editor, Modern Heuristic Techniques for Combina-
torial Problems. Blackwell Scientific Publications.
F. Glover. 1989. Tabu search — Part I. ORSA Journal on
Computing, 1.
D. E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization &amp; Machine Learning. Addison-Wesley.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature-
rich constituent context model for grammar induction. In
EMILP-CoILL.
M. R. Gormley and J. Eisner. 2013. Nonconvex global opti-
mization for latent-variable models. In ACL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In IAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor embed-
ding. In IIPS.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. In ArXiv.
J. H. Holland. 1975. Adaptation in Iatural and Artificial Sys-
tems: An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence. University of Michigan
Press.
H. H. Hoos and T. St¨utzle. 2004. Stochastic Local Search:
Foundations and Applications. Morgan Kaufmann.
</reference>
<page confidence="0.626545">
1993
</page>
<reference confidence="0.999963543859649">
C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison
of genetic algorithms, random restart, and two-opt switching
for solving large location-allocation problems. Computers
&amp; Operations Research, 23.
X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random
restarts in global optimization. Technical report, GT.
Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved con-
stituent context model with features. In PACLIC.
F. Jelinek and R. L. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pattern
Recognition in Practice.
D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988.
How easy is local search? Journal of Computer and System
Sciences, 37.
D. Kahneman and A. Tversky. 1972. Subjective probability: A
judgment of representativeness. Cognitive Psychology, 3.
D. Kahneman and A. Tversky. 1973. On the psychology of
prediction. Psychological Review, 80.
D. Kahneman and A. Tversky. 1982. Evidential impact of base
rates. In D. Kahneman, P. Slovic, and A. Tversky, editors,
Judgment under uncertainty: Heuristics and biases. Cam-
bridge University Press.
J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization mod-
els of sound systems using genetic algorithms. Computa-
tional Linguistics, 29.
J. Kim and R. J. Mooney. 2010. Generative alignment and
semantic parsing for learning from ambiguous supervision.
In COLIIG.
S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Opti-
mization by simulated annealing. Science, 220.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphical
Models: Principles and Techniques. MIT Press.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
D. Lashkari and P. Golland. 2008. Convex clustering with
exemplar-based models. In IIPS.
P. Liang and D. Klein. 2009. Online EM for unsupervised
models. In IAACL-HLT.
L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essen-
tial step underlying the developmental plasticity of neuronal
connections. Royal Society of London Philosophical Trans-
actions Series B, 361.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Mareˇcek and M. Straka. 2013. Stop-probability estimates
computed on a large corpus improve unsupervised depen-
dency parsing. In ACL.
D. Mareˇcek and Z. ˇZabokrtsk´y. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. Mareˇcek and Z. ˇZabokrtsk´y. 2012. Exploiting reducibility
in unsupervised dependency parsing. In EMILP-CoILL.
R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo-
Arnuncio, and M. Ciaramita. 2010. Instance sense induction
from attribute sets. In COLIIG.
N. McIntyre and M. Lapata. 2010. Plot induction and evolu-
tionary search for story generation. In ACL.
X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Op-
timization of HMM by the tabu search algorithm. In RO-
CLIIG.
C. Mellish, A. Knott, J. Oberlander, and M. O’Donnell. 1998.
Experiments using stochastic search for text planning. In
IILG.
R. C. Moore and C. Quirk. 2008. Random restarts in min-
imum error rate training for statistical machine translation.
In COLIIG.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
R. M. Neal and G. E. Hinton. 1999. A view of the EM al-
gorithm that justifies incremental, sparse, and other variants.
In M. I. Jordan, editor, Learning in Graphical Models. MIT
Press.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMILP-CoILL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In IIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
S. Petrov. 2010. Products of random latent variable grammars.
In IAACL-HLT.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differ-
ential Evolution: A Practical Approach to Global Optimiza-
tion. Springer.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In ACL-IJCILP.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression and related optmization
problems. Proceedings of the IEEE, 86.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
B. Selman, H. Levesque, and D. Mitchell. 1992. A new method
for solving hard satisfiability problems. In AAAI.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies
for improving local search. In AAAI.
F. J. Solis and R. J.-B. Wets. 1981. Minimization by random
search techniques. Mathematics of Operations Research, 6.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How “Less is More” in unsupervised dependency
parsing. In GRLL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning.
2010. Viterbi training improves unsupervised dependency
parsing. In CoILL.
</reference>
<page confidence="0.896796">
1994
</page>
<reference confidence="0.99967902631579">
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models. In ICGI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three
dependency-and-boundary models for grammar induction.
In EMNLP-CoNLL.
W. Stadler, editor. 1988. Multicriteria Optimization in Engi-
neering and in the Sciences. Plenum Press.
A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised
relation extraction with large-scale word clustering. In ACL.
M. Surdeanu and C. D. Manning. 2010. Ensemble models for
dependency parsing: Cheap and good? In NAACL-HLT.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
K. Tu and V. Honavar. 2012. Unambiguity regularization
for unsupervised learning of probabilistic grammars. In
EMNLP-CoNLL.
A. Tversky and D. Kahneman. 1973. Availability: A heuristic
for judging frequency and probability. Cognitive Psychol-
ogy, 5.
S. I. Wang and C. D. Manning. 2013. Fast dropout training. In
ICML.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised convex training for dependency parsing. In HLT-
ACL.
T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based
system combination for machine translation. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In ACL.
</reference>
<page confidence="0.992918">
1995
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958031">
<title confidence="0.9988165">Breaking Out of Local Optima with Count and Model Recombination: A Study in Grammar Induction</title>
<author confidence="0.999951">Valentin I Spitkovsky Hiyan Alshawi Daniel Jurafsky</author>
<email confidence="0.987666">valentin@cs.stanford.eduhiyan@google.comjurafsky@stanford.edu</email>
<abstract confidence="0.998883481481481">Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks. Our building are operators of two types: (i) transwhich suggests new places to search, via non-random restarts from already-found local and (ii) which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="7634" citStr="Alshawi, 1996" startWordPosition="1208" endWordPosition="1209">t involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w,c,,,); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inputs ending with punctuation; Dfrag = D − Dcomp is everything 1If desired, a scaling factor could be used to bias C+ towards either C∗1 or C∗2, for example based on their likelihood ratio. LD C*1 + C*2 = C+ LD C*1 = L(C1) (5) + LD arg MAX GD C*2 = L(C2) C1 C2 (2) (4) LD C1 C2 1984 else. The “complete” subset is further partitioned into simple sentences, Dsimp ⊆ Dcomp, with no internal punctuation, and others, which may be complex. As an example, consider the beginning of an article from (simple) Wik</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Attneave</author>
</authors>
<title>Psychological probability as a function of experienced frequency.</title>
<date>1953</date>
<journal>Experimental Psychology,</journal>
<volume>46</volume>
<contexts>
<context position="41062" citStr="Attneave, 1953" startWordPosition="6695" endWordPosition="6696">thm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al.</context>
</contexts>
<marker>Attneave, 1953</marker>
<rawString>F. Attneave. 1953. Psychological probability as a function of experienced frequency. Experimental Psychology, 46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bar-Hillel</author>
</authors>
<title>The base-rate fallacy in probability judgments.</title>
<date>1980</date>
<journal>Acta Psychologica,</journal>
<volume>44</volume>
<contexts>
<context position="40893" citStr="Bar-Hillel, 1980" startWordPosition="6671" endWordPosition="6672">ncy arcs. By reusing templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no le</context>
</contexts>
<marker>Bar-Hillel, 1980</marker>
<rawString>M. Bar-Hillel. 1980. The base-rate fallacy in probability judgments. Acta Psychologica, 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Discovering phonotactic finite-state automata by genetic search.</title>
<date>1998</date>
<booktitle>In COLIIG-ACL.</booktitle>
<contexts>
<context position="37257" citStr="Belz, 1998" startWordPosition="6123" endWordPosition="6124">form sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our</context>
</contexts>
<marker>Belz, 1998</marker>
<rawString>A. Belz. 1998. Discovering phonotactic finite-state automata by genetic search. In COLIIG-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="39310" citStr="Bengio et al., 2009" startWordPosition="6435" endWordPosition="6438">ing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One s</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhargava</author>
<author>G Kondrak</author>
</authors>
<title>Multiple word alignment with profile hidden Markov models.</title>
<date>2009</date>
<booktitle>In IAACL-HLT:Student Research and Doctoral Consortium.</booktitle>
<contexts>
<context position="36211" citStr="Bhargava and Kondrak, 2009" startWordPosition="5959" endWordPosition="5962">ut also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat dise</context>
</contexts>
<marker>Bhargava, Kondrak, 2009</marker>
<rawString>A. Bhargava and G. Kondrak. 2009. Multiple word alignment with profile hidden Markov models. In IAACL-HLT:Student Research and Doctoral Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bisk</author>
<author>J Hockenmaier</author>
</authors>
<title>Simple robust grammar induction with combinatory categorial grammars.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="29170" citStr="Bisk and Hockenmaier, 2012" startWordPosition="4817" endWordPosition="4820">constituent parsing. On the WSJ sentences up to length 40 in Section 23, CS attains similar F1-measure (54.2 vs. 54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et a</context>
</contexts>
<marker>Bisk, Hockenmaier, 2012</marker>
<rawString>Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar induction with combinatory categorial grammars. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training. In</title>
<date>1998</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="39903" citStr="Blum and Mitchell, 1998" startWordPosition="6529" endWordPosition="6532">yan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier to recognize root words. Another splits text into many inter-punctuation fragments, helping learn word associations. The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs. By reusing templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT. P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="12124" citStr="Buchholz and Marsi, 2006" startWordPosition="1922" endWordPosition="1925">e, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric).5 For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs’ intrinsic metrics), in bits per token (bpt). 4But these constraints do not impact training with shorter inputs, since there is no internal punctuation in Dsplit or Dsimp. 5We converted gold labeled constituents in WSJ to unlabeled reference dependencies using deterministic “head-percolation” rules (Collins, 199</context>
<context position="30027" citStr="Buchholz and Marsi, 2006" startWordPosition="4962" endWordPosition="4965">70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Last, we checked how our algorithms generalize outside English WSJ, by testing in 23 more set-ups: all 2006/7 CoNLL test sets (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. Most recent work evaluates against this multi-lingual data, with the unrealistic assumption of part-of-speech tags. But since inducing high quality word clusters for many languages would be beyond the scope of our paper, here we too plugged in gold tags for word categories (instead of unsupervised tags, as in §3–8). We compared to the two strongest systems we knew:10 MZ (Mareˇcek and ˇZabokrtsk´y, 2012) and SAJ (Spitkovsky et al., 2012b), which report average accuracies of 40.0 and 42.9% for CoNLL data (see Table 4). Our fully-trained IFJ and GT sy</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoILL.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL2005 shared task: Semantic role labeling. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J Chapman</author>
</authors>
<title>Illusory correlation in observational report.</title>
<date>1967</date>
<journal>Verbal Learning and Verbal Behavior,</journal>
<volume>6</volume>
<contexts>
<context position="40968" citStr="Chapman, 1967" startWordPosition="6681" endWordPosition="6682">s (Koller and Friedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppr</context>
</contexts>
<marker>Chapman, 1967</marker>
<rawString>L. J. Chapman. 1967. Illusory correlation in observational report. Verbal Learning and Verbal Behavior, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In CoILL-LLL.</booktitle>
<contexts>
<context position="39233" citStr="Clark, 2000" startWordPosition="6424" endWordPosition="6425">and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In CoILL-LLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18936" citStr="Cohen and Smith (2010)" startWordPosition="3080" endWordPosition="3083">chain of lateen alternations that starts and ends with hard EM. We will use a “grammar inductor” to represent subnetworks that transition from Dlsplit to Dl+1 , by split taking transformed parse trees of inter-punctuation fragments up to length l (base data set, D0) to initialize training over fragments up to length l + 1: The FJ network instantiates a grammar inductor with l = 14, thus training on inter-punctuation fragments up to length 15, as in previous work, starting from an empty set of counts, C = 0. Smoothing causes initial parse trees to be chosen uniformly at random, as suggested by Cohen and Smith (2010): 15 � 5.2 Iterated Fork/Join (IFJ) Our second network daisy-chains grammar inductors, starting from the single-word inter-punctuation fragments in D1 split, then retraining on D2split, and so forth, until finally stopping at D15 it, as before: spl We diagrammed this system as not taking an input, since the first inductor’s output is fully determined by unique parse trees of single-token strings. This iterative approach to optimization is akin to deterministic annealing (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al., 2009, §4.2). Unlike the basic FJ, where symmetrization </context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>P Blunsom</author>
<author>S Goldwater</author>
</authors>
<title>Inducing treesubstitution grammars.</title>
<date>2011</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="2647" citStr="Cohn et al., 2011" startWordPosition="398" endWordPosition="401">ive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most proposed moves that would escape a local attractor. Sampling methods thus take unbounded time to solve a problem (and can’t certify optimality) but are useful for finding approximate solutions to grammar induction (Cohn et al., 2011; Mareˇcek and ˇZabokrtsk´y, 2011; Naseem and Barzilay, 2011). We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random restarts. Its new starting places are informed by previously found solutions, unlike conventional restarts, but may not resemble their predecessors, unlike typical MCMC moves. We show that one good way to construct such steps in a parameter space is by forgetting some aspects of a learned model. Another is by merging promising solutions, since even simple interpolation (Jelinek and Mercer, 1980) of local optima may be </context>
<context position="39414" citStr="Cohn et al., 2011" startWordPosition="6452" endWordPosition="6455">ining scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier to recognize root words. Another splits tex</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2011</marker>
<rawString>T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing treesubstitution grammars. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification. In</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="39174" citStr="Collins and Singer, 1999" startWordPosition="6416" endWordPosition="6419">-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. </context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In EMILP. M. Collins. 1999. Head-Driven Statistical Models for Iatural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Corlett</author>
<author>G Penn</author>
</authors>
<title>An exact A∗ method for deciphering letter-substitution ciphers.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="33427" citStr="Corlett and Penn (2010)" startWordPosition="5507" endWordPosition="5510">see a positive trend as we move from a simpler fully-trained system (IFJ, 40.0%), to a more complex system (GT, 47.6%), to system combination (CS, 48.6%). Grounding seems to be more important for the CoNLL sets, possibly because of data sparsity or availability of gold tags. 11 Related Work The surest way to avoid local optima is to craft an objective that doesn’t have them. For example, Wang et al. (2008) demonstrated a convex training method for semi-supervised dependency parsing; Lashkari and Golland (2008) introduced a convex reformulation of likelihood functions for clustering tasks; and Corlett and Penn (2010) designed 1990 Directed Dependency Accuracies (DDA) (@10) MZ SAJ IFJ GT CS 26.5 10.9 33.3 8.3 9.3 (30.2) 27.9 44.9 26.1 25.6 26.8 (45.6) 26.8 33.3 23.5 24.2 24.4 (32.8) 46.0 65.2 35.8 64.2 63.4 (69.1) 47.0 62.1 65.0 68.4 68.0 (79.2) — 63.2 56.0 55.8 58.4 (60.8) — 57.0 49.0 48.6 52.5 (56.0) 49.5 55.1 44.5 43.9 44.0 (52.3) 48.0 54.2 42.9 24.5 34.3 (51.1) 38.6 22.2 37.8 17.1 21.4 (29.8) 44.2 46.6 40.8 51.3 48.0 (48.7) 49.2 29.6 39.3 57.6 58.2 (75.0) 44.8 39.1 34.1 54.5 56.2 (71.2) 20.2 26.9 23.7 45.0 45.4 (52.2) 51.8 58.2 24.8 52.9 58.3 (67.6) 43.3 40.7 56.8 31.1 34.9 (44.9) 50.8 22.7 32.6 63.7 6</context>
</contexts>
<marker>Corlett, Penn, 2010</marker>
<rawString>E. Corlett and G. Penn. 2010. An exact A∗ method for deciphering letter-substitution ciphers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F I M Craik</author>
<author>E Bialystok</author>
</authors>
<title>Cognition through the lifespan: mechanisms of change.</title>
<date>2006</date>
<journal>TREIDS in Cognitive Sciences,</journal>
<volume>10</volume>
<contexts>
<context position="41382" citStr="Craik and Bialystok, 2006" startWordPosition="6744" endWordPosition="6747">malizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et a</context>
</contexts>
<marker>Craik, Bialystok, 2006</marker>
<rawString>F. I. M. Craik and E. Bialystok. 2006. Cognition through the lifespan: mechanisms of change. TREIDS in Cognitive Sciences, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Marcken</author>
</authors>
<title>Lexical heads, phrase structure and the induction of grammar.</title>
<date>1995</date>
<booktitle>In WVLC.</booktitle>
<marker>de Marcken, 1995</marker>
<rawString>C. de Marcken. 1995. Lexical heads, phrase structure and the induction of grammar. In WVLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic learning of language model structure.</title>
<date>2004</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="37374" citStr="Duh and Kirchhoff, 2004" startWordPosition="6137" endWordPosition="6140">erform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objecCoNLL Dat</context>
</contexts>
<marker>Duh, Kirchhoff, 2004</marker>
<rawString>K. Duh and K. Kirchhoff. 2004. Automatic learning of language model structure. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Grammar induction: Beyond local search.</title>
<date>2012</date>
<booktitle>In ICGI.</booktitle>
<contexts>
<context position="34947" citStr="Eisner, 2012" startWordPosition="5770" endWordPosition="5771"> for our full networks (IFJ and GT), previous state-of-the-art systems of Spitkovsky et al. (2012b) and Mareˇcek and ˇZabokrtsk´y (2012), and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts </context>
</contexts>
<marker>Eisner, 2012</marker>
<rawString>J. Eisner. 2012. Grammar induction: Beyond local search. In ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Elidan</author>
<author>M Ninio</author>
<author>N Friedman</author>
<author>D Schuurmans</author>
</authors>
<title>Data perturbation for escaping local maxima in learning.</title>
<date>2002</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="38660" citStr="Elidan et al. (2002)" startWordPosition="6345" endWordPosition="6348">zech ’6 ’7 Danish ’6 Dutch ’6 English ’7 German ’6 Greek ’6 Hungarian ’7 Italian ’7 Japanese ’6 Portuguese ’6 Slovenian ’6 Spanish ’6 Swedish ’6 Turkish ’6 ’7 1991 tives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments. This selection of text filters is a specialized case of more general “data perturbation” techniques — even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language model</context>
</contexts>
<marker>Elidan, Ninio, Friedman, Schuurmans, 2002</marker>
<rawString>G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002. Data perturbation for escaping local maxima in learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Learning and development in neural networks: The importance of starting small.</title>
<date>1993</date>
<journal>Cognition,</journal>
<volume>48</volume>
<contexts>
<context position="39008" citStr="Elman, 1993" startWordPosition="6393" endWordPosition="6394">xt fragments. This selection of text filters is a specialized case of more general “data perturbation” techniques — even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventi</context>
</contexts>
<marker>Elman, 1993</marker>
<rawString>J. L. Elman. 1993. Learning and development in neural networks: The importance of starting small. Cognition, 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>W Schudy</author>
</authors>
<title>Bounding and comparing methods for correlation clustering beyond ILP.</title>
<date>2009</date>
<booktitle>In IAACLHLT: Integer Linear Programming for ILP.</booktitle>
<contexts>
<context position="36716" citStr="Elsner and Schudy (2009)" startWordPosition="6039" endWordPosition="6042">om jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling o</context>
</contexts>
<marker>Elsner, Schudy, 2009</marker>
<rawString>M. Elsner and W. Schudy. 2009. Bounding and comparing methods for correlation clustering beyond ILP. In IAACLHLT: Integer Linear Programming for ILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Fonseca</author>
<author>P J Fleming</author>
</authors>
<title>Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization.</title>
<date>1993</date>
<booktitle>In ICGA.</booktitle>
<contexts>
<context position="37506" citStr="Fonseca and Fleming, 1993" startWordPosition="6155" endWordPosition="6158">over, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objecCoNLL Data Arabic 2006 ’7 Basque ’7 Bulgarian ’7 Catalan ’7 Chinese ’6 ’7 Czech ’6 ’7 Danish ’6 Dutch ’6 English ’7 German ’6 Greek ’6 Hungar</context>
</contexts>
<marker>Fonseca, Fleming, 1993</marker>
<rawString>C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization. In ICGA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Geyer</author>
</authors>
<title>Markov chain Monte Carlo maximum likelihood.</title>
<date>1991</date>
<booktitle>In Interface Symposium.</booktitle>
<contexts>
<context position="42598" citStr="Geyer, 1991" startWordPosition="6927" endWordPosition="6928">2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them. 1992 Acknowledgments We thank Yun-Hsuan Sung, for early-stage discussions on ways of extending “baby steps,” Elias Ponvert, for sharing all of the relevant experimental results and evaluation scripts from his work with Jason Baldridge and Katrin Erk, and the anonymous reviewers, for their helpful comments on the draft version of this paper. Funded, in part, by Defense Advanced Research Projects Agency (DARPA) Deep Ex</context>
</contexts>
<marker>Geyer, 1991</marker>
<rawString>C. J. Geyer. 1991. Markov chain Monte Carlo maximum likelihood. In Interface Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Posterior sparsity in unsupervised dependency parsing.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Gillenwater, Ganchev, Grac¸a, Pereira, Taskar, 2010</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, F. Pereira, and B. Taskar. 2010. Posterior sparsity in unsupervised dependency parsing. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Concavity and initialization for unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="7132" citStr="Gimpel and Smith, 2012" startWordPosition="1127" endWordPosition="1130">ed model,1 regardless of nominal size (because C*1, C*2 will have converged using a shared training set, D). We return the best of C*1, C*2 and C*+ = L(C+). This approach may uncover more (and never returns less) likely solutions than choosing among C*1, C*2 alone: We will use a short-hand notation to represent the combiner network diagrammed above, less clutter: (6) 3 The Task and Methodology We apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w,c,,,); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inpu</context>
<context position="29090" citStr="Gimpel and Smith, 2012" startWordPosition="4805" endWordPosition="4808">endency structures is competitive with the state-of-the-art in unsupervised constituent parsing. On the WSJ sentences up to length 40 in Section 23, CS attains similar F1-measure (54.2 vs. 54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and </context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N. A. Smith. 2012. Concavity and initialization for unsupervised dependency parsing. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Glover</author>
<author>M Laguna</author>
</authors>
<title>Tabu search.</title>
<date>1993</date>
<booktitle>Modern Heuristic Techniques for Combinatorial Problems.</booktitle>
<editor>In C. R. Reeves, editor,</editor>
<publisher>Blackwell Scientific Publications.</publisher>
<contexts>
<context position="36915" citStr="Glover and Laguna, 1993" startWordPosition="6069" endWordPosition="6072">roblems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can hand</context>
</contexts>
<marker>Glover, Laguna, 1993</marker>
<rawString>F. Glover and M. Laguna. 1993. Tabu search. In C. R. Reeves, editor, Modern Heuristic Techniques for Combinatorial Problems. Blackwell Scientific Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Glover</author>
</authors>
<date>1989</date>
<journal>Tabu search — Part I. ORSA Journal on Computing,</journal>
<volume>1</volume>
<contexts>
<context position="36890" citStr="Glover, 1989" startWordPosition="6067" endWordPosition="6068">tisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca an</context>
</contexts>
<marker>Glover, 1989</marker>
<rawString>F. Glover. 1989. Tabu search — Part I. ORSA Journal on Computing, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Goldberg</author>
</authors>
<date>1989</date>
<booktitle>Genetic Algorithms in Search, Optimization &amp; Machine Learning.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="37172" citStr="Goldberg, 1989" startWordPosition="6112" endWordPosition="6113">LP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and hav</context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>D. E. Goldberg. 1989. Genetic Algorithms in Search, Optimization &amp; Machine Learning. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Golland</author>
<author>J DeNero</author>
<author>J Uszkoreit</author>
</authors>
<title>A featurerich constituent context model for grammar induction.</title>
<date>2012</date>
<booktitle>In EMILP-CoILL.</booktitle>
<contexts>
<context position="31628" citStr="Golland et al., 2012" startWordPosition="5214" endWordPosition="5217">valuation, to match other previous work: Golland et al.’s (2012, Figure 1) for CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest. 10During review, another strong system (Mareˇcek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles). System F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) 47.6 P R CCL (Seginer, 2007) 52.8 54.6 51.1 PRLG (Ponvert et al., 2011) 54.6 60.4 49.8 CS System Combination 54.2 55.6 52.8 Supervised DBM Skyline 59.3 65.7 54.1 Dependency-Based Upper Bound 87.2 100 77.3 Table 3: Harmonic mean (F1) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines. 10 Discussion CoNLL training sets were intended for comparing supervised systems, and aren’t all suitable for unsupervised learning: 12 languages have under 10,000 sentences </context>
</contexts>
<marker>Golland, DeNero, Uszkoreit, 2012</marker>
<rawString>D. Golland, J. DeNero, and J. Uszkoreit. 2012. A featurerich constituent context model for grammar induction. In EMILP-CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Gormley</author>
<author>J Eisner</author>
</authors>
<title>Nonconvex global optimization for latent-variable models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="34974" citStr="Gormley and Eisner, 2013" startWordPosition="5772" endWordPosition="5775">networks (IFJ and GT), previous state-of-the-art systems of Spitkovsky et al. (2012b) and Mareˇcek and ˇZabokrtsk´y (2012), and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine tran</context>
</contexts>
<marker>Gormley, Eisner, 2013</marker>
<rawString>M. R. Gormley and J. Eisner. 2013. Nonconvex global optimization for latent-variable models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="1795" citStr="Headden et al., 2009" startWordPosition="259" endWordPosition="262">sing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborh</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>S Roweis</author>
</authors>
<title>Stochastic neighbor embedding.</title>
<date>2003</date>
<booktitle>In IIPS.</booktitle>
<contexts>
<context position="36130" citStr="Hinton and Roweis, 2003" startWordPosition="5946" endWordPosition="5949">, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that l</context>
</contexts>
<marker>Hinton, Roweis, 2003</marker>
<rawString>G. Hinton and S. Roweis. 2003. Stochastic neighbor embedding. In IIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>N Srivastava</author>
<author>A Krizhevsky</author>
<author>I Sutskever</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors.</title>
<date>2012</date>
<booktitle>In ArXiv.</booktitle>
<contexts>
<context position="41668" citStr="Hinton et al., 2012" startWordPosition="6788" endWordPosition="6791">ttneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore,</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. In ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Holland</author>
</authors>
<title>Adaptation in Iatural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence.</title>
<date>1975</date>
<publisher>University of Michigan Press.</publisher>
<contexts>
<context position="37155" citStr="Holland, 1975" startWordPosition="6110" endWordPosition="6111">l., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) con</context>
</contexts>
<marker>Holland, 1975</marker>
<rawString>J. H. Holland. 1975. Adaptation in Iatural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. University of Michigan Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Hoos</author>
<author>T St¨utzle</author>
</authors>
<title>Stochastic Local Search: Foundations and Applications.</title>
<date>2004</date>
<publisher>Morgan Kaufmann.</publisher>
<marker>Hoos, St¨utzle, 2004</marker>
<rawString>H. H. Hoos and T. St¨utzle. 2004. Stochastic Local Search: Foundations and Applications. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Houck</author>
<author>J A Joines</author>
<author>M G Kay</author>
</authors>
<title>Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems.</title>
<date>1996</date>
<journal>Computers &amp; Operations Research,</journal>
<volume>23</volume>
<contexts>
<context position="37058" citStr="Houck et al., 1996" startWordPosition="6094" endWordPosition="6097"> greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to</context>
</contexts>
<marker>Houck, Joines, Kay, 1996</marker>
<rawString>C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison of genetic algorithms, random restart, and two-opt switching for solving large location-allocation problems. Computers &amp; Operations Research, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Hu</author>
<author>R Shonkwiler</author>
<author>M C Spruill</author>
</authors>
<title>Random restarts in global optimization.</title>
<date>1994</date>
<tech>Technical report, GT.</tech>
<contexts>
<context position="2226" citStr="Hu et al., 1994" startWordPosition="328" endWordPosition="331">cy arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most proposed moves that would escape a local attractor. Sampling methods thus take unbounded time to solve a problem (and can’t certify optimality) but are useful for finding approximate solutions to grammar induction (Cohn et al., 2011; Mareˇcek and ˇZabokrtsk´y, 2011; Naseem and Barzilay, 2011). We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random r</context>
<context position="35390" citStr="Hu et al., 1994" startWordPosition="5834" endWordPosition="5837">LP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attracti</context>
</contexts>
<marker>Hu, Shonkwiler, Spruill, 1994</marker>
<rawString>X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random restarts in global optimization. Technical report, GT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Huang</author>
<author>M Zhang</author>
<author>C L Tan</author>
</authors>
<title>Improved constituent context model with features.</title>
<date>2012</date>
<booktitle>In PACLIC.</booktitle>
<contexts>
<context position="31562" citStr="Huang et al., 2012" startWordPosition="5202" endWordPosition="5205">heir eval-ps.py script to a maximum length of 40 words, in our evaluation, to match other previous work: Golland et al.’s (2012, Figure 1) for CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest. 10During review, another strong system (Mareˇcek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles). System F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) 47.6 P R CCL (Seginer, 2007) 52.8 54.6 51.1 PRLG (Ponvert et al., 2011) 54.6 60.4 49.8 CS System Combination 54.2 55.6 52.8 Supervised DBM Skyline 59.3 65.7 54.1 Dependency-Based Upper Bound 87.2 100 77.3 Table 3: Harmonic mean (F1) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines. 10 Discussion CoNLL training sets were intended for comparing supervised systems, and aren’t all suitable fo</context>
</contexts>
<marker>Huang, Zhang, Tan, 2012</marker>
<rawString>Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved constituent context model with features. In PACLIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice.</title>
<date>1980</date>
<contexts>
<context position="3223" citStr="Jelinek and Mercer, 1980" startWordPosition="485" endWordPosition="488">solutions to grammar induction (Cohn et al., 2011; Mareˇcek and ˇZabokrtsk´y, 2011; Naseem and Barzilay, 2011). We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random restarts. Its new starting places are informed by previously found solutions, unlike conventional restarts, but may not resemble their predecessors, unlike typical MCMC moves. We show that one good way to construct such steps in a parameter space is by forgetting some aspects of a learned model. Another is by merging promising solutions, since even simple interpolation (Jelinek and Mercer, 1980) of local optima may be superior to all of the originals. Informed restarts can make it possible to explore a combinatorial search space more rapidly and thoroughly than with traditional methods alone. 2 Abstract Operators Let C be a collection of counts — the sufficient statistics from which a candidate solution to an optimization problem could be computed, e.g., by smoothing and normalizing to yield probabilities. The counts may be fractional and solutions could take the form of multinomial distributions. A local optimizer L will convert C into C∗ = LD(C) — an updated collection of counts, r</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>F. Jelinek and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Johnson</author>
<author>C H Papadimitriou</author>
<author>M Yannakakis</author>
</authors>
<title>How easy is local search?</title>
<date>1988</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>37</volume>
<contexts>
<context position="35945" citStr="Johnson et al., 1988" startWordPosition="5917" endWordPosition="5920">sing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al.,</context>
</contexts>
<marker>Johnson, Papadimitriou, Yannakakis, 1988</marker>
<rawString>D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988. How easy is local search? Journal of Computer and System Sciences, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kahneman</author>
<author>A Tversky</author>
</authors>
<title>Subjective probability: A judgment of representativeness.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<contexts>
<context position="41090" citStr="Kahneman and Tversky, 1972" startWordPosition="6697" endWordPosition="6700">ised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 20</context>
</contexts>
<marker>Kahneman, Tversky, 1972</marker>
<rawString>D. Kahneman and A. Tversky. 1972. Subjective probability: A judgment of representativeness. Cognitive Psychology, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kahneman</author>
<author>A Tversky</author>
</authors>
<title>On the psychology of prediction.</title>
<date>1973</date>
<journal>Psychological Review,</journal>
<volume>80</volume>
<contexts>
<context position="41119" citStr="Kahneman and Tversky, 1973" startWordPosition="6701" endWordPosition="6704">on. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in su</context>
</contexts>
<marker>Kahneman, Tversky, 1973</marker>
<rawString>D. Kahneman and A. Tversky. 1973. On the psychology of prediction. Psychological Review, 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kahneman</author>
<author>A Tversky</author>
</authors>
<title>Evidential impact of base rates.</title>
<date>1982</date>
<editor>In D. Kahneman, P. Slovic, and A. Tversky, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="40922" citStr="Kahneman and Tversky, 1982" startWordPosition="6673" endWordPosition="6676">ng templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised l</context>
</contexts>
<marker>Kahneman, Tversky, 1982</marker>
<rawString>D. Kahneman and A. Tversky. 1982. Evidential impact of base rates. In D. Kahneman, P. Slovic, and A. Tversky, editors, Judgment under uncertainty: Heuristics and biases. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ke</author>
<author>M Ogura</author>
<author>W S-Y Wang</author>
</authors>
<title>Optimization models of sound systems using genetic algorithms.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="37852" citStr="Ke et al., 2003" startWordPosition="6207" endWordPosition="6210">otactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objecCoNLL Data Arabic 2006 ’7 Basque ’7 Bulgarian ’7 Catalan ’7 Chinese ’6 ’7 Czech ’6 ’7 Danish ’6 Dutch ’6 English ’7 German ’6 Greek ’6 Hungarian ’7 Italian ’7 Japanese ’6 Portuguese ’6 Slovenian ’6 Spanish ’6 Swedish ’6 Turkish ’6 ’7 1991 tives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments. This selection of text filters is a specia</context>
</contexts>
<marker>Ke, Ogura, Wang, 2003</marker>
<rawString>J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization models of sound systems using genetic algorithms. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>R J Mooney</author>
</authors>
<title>Generative alignment and semantic parsing for learning from ambiguous supervision.</title>
<date>2010</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="35837" citStr="Kim and Mooney, 2010" startWordPosition="5901" endWordPosition="5904">starts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks a</context>
</contexts>
<marker>Kim, Mooney, 2010</marker>
<rawString>J. Kim and R. J. Mooney. 2010. Generative alignment and semantic parsing for learning from ambiguous supervision. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kirkpatrick</author>
<author>C D Gelatt</author>
<author>M P Vecchi</author>
</authors>
<title>Optimization by simulated annealing.</title>
<date>1983</date>
<journal>Science,</journal>
<pages>220</pages>
<contexts>
<context position="36551" citStr="Kirkpatrick et al., 1983" startWordPosition="6011" endWordPosition="6014">ohnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, </context>
</contexts>
<marker>Kirkpatrick, Gelatt, Vecchi, 1983</marker>
<rawString>S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A generative constituentcontext model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31500" citStr="Klein and Manning, 2002" startWordPosition="5193" endWordPosition="5196">.’s (2011, Table 6) for the full Section 23 because we restricted their eval-ps.py script to a maximum length of 40 words, in our evaluation, to match other previous work: Golland et al.’s (2012, Figure 1) for CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest. 10During review, another strong system (Mareˇcek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles). System F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) 47.6 P R CCL (Seginer, 2007) 52.8 54.6 51.1 PRLG (Ponvert et al., 2011) 54.6 60.4 49.8 CS System Combination 54.2 55.6 52.8 Supervised DBM Skyline 59.3 65.7 54.1 Dependency-Based Upper Bound 87.2 100 77.3 Table 3: Harmonic mean (F1) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines. 10 Discussion CoNLL training sets were intende</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. A generative constituentcontext model for improved grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1773" citStr="Klein and Manning, 2004" startWordPosition="255" endWordPosition="258">uction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC ma</context>
<context position="14877" citStr="Klein and Manning (2004" startWordPosition="2357" endWordPosition="2360"> non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected: (8) The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009, Figure 3: Uninformed). Symmetrization offers an extra chance to make heads or tails of syntactic relations, after learning which words tend to go together. 6A related approach — initializing EM training with an M-step — was advocated by Klein and Manning (2004, §3). At each instance where a word ® attaches O on (say) the right, our implementation attributes half its weight to the intended construction, O&apos;z�, reserving the other half for the symmetric structure, O attaching ® to its left: Vz�. For the desired effect, these aggregated counts are left unnormalized, while all other counts (of word fertilities and sentence roots) get discarded. To see why we don’t turn word attachment scores into probabilities, consider sentences �a O and © z�. The fact that O co-occurs with O introduces an asymmetry into z�’s relation with c�: ?( O |c�) = 1 differs fro</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Krueger</author>
<author>P Dayan</author>
</authors>
<title>Flexible shaping: How learning in small steps helps.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>110</volume>
<contexts>
<context position="39288" citStr="Krueger and Dayan, 2009" startWordPosition="6431" endWordPosition="6434">ested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and </context>
</contexts>
<marker>Krueger, Dayan, 2009</marker>
<rawString>K. A. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lashkari</author>
<author>P Golland</author>
</authors>
<title>Convex clustering with exemplar-based models.</title>
<date>2008</date>
<booktitle>In IIPS.</booktitle>
<contexts>
<context position="33319" citStr="Lashkari and Golland (2008)" startWordPosition="5490" endWordPosition="5493">of our (and previous) results (see Table 4). Nevertheless, if we look at the more stable average accuracies, we see a positive trend as we move from a simpler fully-trained system (IFJ, 40.0%), to a more complex system (GT, 47.6%), to system combination (CS, 48.6%). Grounding seems to be more important for the CoNLL sets, possibly because of data sparsity or availability of gold tags. 11 Related Work The surest way to avoid local optima is to craft an objective that doesn’t have them. For example, Wang et al. (2008) demonstrated a convex training method for semi-supervised dependency parsing; Lashkari and Golland (2008) introduced a convex reformulation of likelihood functions for clustering tasks; and Corlett and Penn (2010) designed 1990 Directed Dependency Accuracies (DDA) (@10) MZ SAJ IFJ GT CS 26.5 10.9 33.3 8.3 9.3 (30.2) 27.9 44.9 26.1 25.6 26.8 (45.6) 26.8 33.3 23.5 24.2 24.4 (32.8) 46.0 65.2 35.8 64.2 63.4 (69.1) 47.0 62.1 65.0 68.4 68.0 (79.2) — 63.2 56.0 55.8 58.4 (60.8) — 57.0 49.0 48.6 52.5 (56.0) 49.5 55.1 44.5 43.9 44.0 (52.3) 48.0 54.2 42.9 24.5 34.3 (51.1) 38.6 22.2 37.8 17.1 21.4 (29.8) 44.2 46.6 40.8 51.3 48.0 (48.7) 49.2 29.6 39.3 57.6 58.2 (75.0) 44.8 39.1 34.1 54.5 56.2 (71.2) 20.2 26.9</context>
</contexts>
<marker>Lashkari, Golland, 2008</marker>
<rawString>D. Lashkari and P. Golland. 2008. Convex clustering with exemplar-based models. In IIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="38638" citStr="Liang and Klein, 2009" startWordPosition="6341" endWordPosition="6344">talan ’7 Chinese ’6 ’7 Czech ’6 ’7 Danish ’6 Dutch ’6 English ’7 German ’6 Greek ’6 Hungarian ’7 Italian ’7 Japanese ’6 Portuguese ’6 Slovenian ’6 Spanish ’6 Swedish ’6 Turkish ’6 ’7 1991 tives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments. This selection of text filters is a specialized case of more general “data perturbation” techniques — even cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 200</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Low</author>
<author>H-J Cheng</author>
</authors>
<title>Axon pruning: an essential step underlying the developmental plasticity of neuronal connections.</title>
<date>2006</date>
<journal>Royal Society of London Philosophical Transactions Series B,</journal>
<pages>361</pages>
<contexts>
<context position="41404" citStr="Low and Cheng, 2006" startWordPosition="6748" endWordPosition="6751">n symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be e</context>
</contexts>
<marker>Low, Cheng, 2006</marker>
<rawString>L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essential step underlying the developmental plasticity of neuronal connections. Royal Society of London Philosophical Transactions Series B, 361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="9816" citStr="Marcus et al., 1993" startWordPosition="1572" endWordPosition="1575">nguists. Only the last piece of text would still be considered complete, isolating its contribution to sentence root and boundary word distributions from those of incomplete fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0. 3http://nlp.stanford.edu/pubs/gold</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mareˇcek</author>
<author>M Straka</author>
</authors>
<title>Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Mareˇcek, Straka, 2013</marker>
<rawString>D. Mareˇcek and M. Straka. 2013. Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mareˇcek</author>
<author>Z ˇZabokrtsk´y</author>
</authors>
<title>Gibbs sampling with treeness constraint in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In ROBUS.</booktitle>
<marker>Mareˇcek, ˇZabokrtsk´y, 2011</marker>
<rawString>D. Mareˇcek and Z. ˇZabokrtsk´y. 2011. Gibbs sampling with treeness constraint in unsupervised dependency parsing. In ROBUS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mareˇcek</author>
<author>Z ˇZabokrtsk´y</author>
</authors>
<title>Exploiting reducibility in unsupervised dependency parsing.</title>
<date>2012</date>
<booktitle>In EMILP-CoILL.</booktitle>
<marker>Mareˇcek, ˇZabokrtsk´y, 2012</marker>
<rawString>D. Mareˇcek and Z. ˇZabokrtsk´y. 2012. Exploiting reducibility in unsupervised dependency parsing. In EMILP-CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Martin-Brualla</author>
<author>E Alfonseca</author>
<author>M Pasca</author>
<author>K Hall</author>
<author>E RobledoArnuncio</author>
<author>M Ciaramita</author>
</authors>
<title>Instance sense induction from attribute sets.</title>
<date>2010</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="35867" citStr="Martin-Brualla et al., 2010" startWordPosition="5905" endWordPosition="5908">bability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, d</context>
</contexts>
<marker>Martin-Brualla, Alfonseca, Pasca, Hall, RobledoArnuncio, Ciaramita, 2010</marker>
<rawString>R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. RobledoArnuncio, and M. Ciaramita. 2010. Instance sense induction from attribute sets. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N McIntyre</author>
<author>M Lapata</author>
</authors>
<title>Plot induction and evolutionary search for story generation.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="37442" citStr="McIntyre and Lapata, 2010" startWordPosition="6148" endWordPosition="6151">lement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-objective genetic algorithms (Fonseca and Fleming, 1993) can handle problems with equally important but conflicting criteria (Stadler, 1988), using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (Ke et al., 2003). Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objecCoNLL Data Arabic 2006 ’7 Basque ’7 Bulgarian ’7 Catalan ’7 Chinese ’6 ’7 Cze</context>
</contexts>
<marker>McIntyre, Lapata, 2010</marker>
<rawString>N. McIntyre and M. Lapata. 2010. Plot induction and evolutionary search for story generation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-d Mei</author>
<author>S-h Sun</author>
<author>J-s Pan</author>
<author>T-Y Chen</author>
</authors>
<title>Optimization of HMM by the tabu search algorithm.</title>
<date>2001</date>
<booktitle>In ROCLIIG.</booktitle>
<contexts>
<context position="36851" citStr="Mei et al. (2001)" startWordPosition="6060" endWordPosition="6063">s” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a fusion of what’s best in local search and multi-start methods (Houck et al., 1996), exploiting a problem’s structure to combine valid parts of any partial solutions (Holland, 1975; Goldberg, 1989). Evolutionary heuristics proved useful in the induction of phonotactics (Belz, 1998), text planning (Mellish et al., 1998), factored modeling of morphologically-rich languages (Duh and Kirchhoff, 2004) and plot induction for story generation (McIntyre and Lapata, 2010). Multi-o</context>
</contexts>
<marker>Mei, Sun, Pan, Chen, 2001</marker>
<rawString>X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Optimization of HMM by the tabu search algorithm. In ROCLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>A Knott</author>
<author>J Oberlander</author>
<author>M O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In IILG.</booktitle>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>C. Mellish, A. Knott, J. Oberlander, and M. O’Donnell. 1998. Experiments using stochastic search for text planning. In IILG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>C Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="35482" citStr="Moore and Quirk (2008)" startWordPosition="5848" endWordPosition="5851"> still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves c</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>R. C. Moore and C. Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>R Barzilay</author>
</authors>
<title>Using semantic cues to learn syntax.</title>
<date>2011</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="2708" citStr="Naseem and Barzilay, 2011" startWordPosition="406" endWordPosition="409">xpensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most proposed moves that would escape a local attractor. Sampling methods thus take unbounded time to solve a problem (and can’t certify optimality) but are useful for finding approximate solutions to grammar induction (Cohn et al., 2011; Mareˇcek and ˇZabokrtsk´y, 2011; Naseem and Barzilay, 2011). We propose an alternative (deterministic) search heuristic that combines local optimization via EM with non-random restarts. Its new starting places are informed by previously found solutions, unlike conventional restarts, but may not resemble their predecessors, unlike typical MCMC moves. We show that one good way to construct such steps in a parameter space is by forgetting some aspects of a learned model. Another is by merging promising solutions, since even simple interpolation (Jelinek and Mercer, 1980) of local optima may be superior to all of the originals. Informed restarts can make </context>
</contexts>
<marker>Naseem, Barzilay, 2011</marker>
<rawString>T. Naseem and R. Barzilay. 2011. Using semantic cues to learn syntax. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
<author>G E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In M. I. Jordan, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="41624" citStr="Neal and Hinton, 1999" startWordPosition="6781" endWordPosition="6784">ce people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating </context>
</contexts>
<marker>Neal, Hinton, 1999</marker>
<rawString>R. M. Neal and G. E. Hinton. 1999. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In EMILP-CoILL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In EMILP-CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Cubic-time parsing and learning algorithms for grammatical bigram models.</title>
<date>2001</date>
<tech>Technical report, UCB.</tech>
<contexts>
<context position="1651" citStr="Paskin 2001" startWordPosition="238" endWordPosition="239">urbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutio</context>
<context position="12830" citStr="Paskin, 2001" startWordPosition="2027" endWordPosition="2028">curacies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric).5 For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs’ intrinsic metrics), in bits per token (bpt). 4But these constraints do not impact training with shorter inputs, since there is no internal punctuation in Dsplit or Dsimp. 5We converted gold labeled constituents in WSJ to unlabeled reference dependencies using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols, though not punctuation arcs, contribute to scores, as is standard (Paskin, 2001b). 1985 4 Concrete Operators We will now instantiate the operators sketched out in §2 specifically for the grammar induction task. Throughout, we repeatedly employ single steps of Viterbi training to transfer information between subnetworks in a model-independent way: when a module’s output is a set of (Viterbi) parse trees, it necessarily contains sufficient information required to estimate an arbitrarily-factored model down-stream.6 4.1 Transform #1: A Simple Filter Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F) attempts to extract a cleaner</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M. A. Paskin. 2001a. Cubic-time parsing and learning algorithms for grammatical bigram models. Technical report, UCB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Grammatical bigrams. In</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1651" citStr="Paskin 2001" startWordPosition="238" endWordPosition="239">urbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutio</context>
<context position="12830" citStr="Paskin, 2001" startWordPosition="2027" endWordPosition="2028">curacies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric).5 For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs’ intrinsic metrics), in bits per token (bpt). 4But these constraints do not impact training with shorter inputs, since there is no internal punctuation in Dsplit or Dsimp. 5We converted gold labeled constituents in WSJ to unlabeled reference dependencies using deterministic “head-percolation” rules (Collins, 1999); sentence root symbols, though not punctuation arcs, contribute to scores, as is standard (Paskin, 2001b). 1985 4 Concrete Operators We will now instantiate the operators sketched out in §2 specifically for the grammar induction task. Throughout, we repeatedly employ single steps of Viterbi training to transfer information between subnetworks in a model-independent way: when a module’s output is a set of (Viterbi) parse trees, it necessarily contains sufficient information required to estimate an arbitrarily-factored model down-stream.6 4.1 Transform #1: A Simple Filter Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F) attempts to extract a cleaner</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M. A. Paskin. 2001b. Grammatical bigrams. In IIPS. F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="42208" citStr="Petrov, 2010" startWordPosition="6869" endWordPosition="6870">iants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer, 1991), which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them. 1992 Acknowledgments We thank </context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>S. Petrov. 2010. Products of random latent variable grammars. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ponvert</author>
<author>J Baldridge</author>
<author>K Erk</author>
</authors>
<title>Simple unsupervised grammar induction from raw text with cascaded finite state models.</title>
<date>2011</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="29779" citStr="Ponvert et al., 2011" startWordPosition="4920" endWordPosition="4923">maier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Last, we checked how our algorithms generalize outside English WSJ, by testing in 23 more set-ups: all 2006/7 CoNLL test sets (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. Most recent work evaluates against this multi-lingual data, with the unrealistic assumption of part-of-speech tags. But since inducing high quality word clusters for many languages would be beyond the scope of our paper, here we too plugged in gold tags for word categories (instead of unsupervised tags, a</context>
<context position="31594" citStr="Ponvert et al., 2011" startWordPosition="5208" endWordPosition="5211">ximum length of 40 words, in our evaluation, to match other previous work: Golland et al.’s (2012, Figure 1) for CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest. 10During review, another strong system (Mareˇcek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles). System F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) 47.6 P R CCL (Seginer, 2007) 52.8 54.6 51.1 PRLG (Ponvert et al., 2011) 54.6 60.4 49.8 CS System Combination 54.2 55.6 52.8 Supervised DBM Skyline 59.3 65.7 54.1 Dependency-Based Upper Bound 87.2 100 77.3 Table 3: Harmonic mean (F1) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines. 10 Discussion CoNLL training sets were intended for comparing supervised systems, and aren’t all suitable for unsupervised learning: 12 lang</context>
</contexts>
<marker>Ponvert, Baldridge, Erk, 2011</marker>
<rawString>E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded finite state models. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K V Price</author>
<author>R M Storn</author>
<author>J A Lampinen</author>
</authors>
<title>Differential Evolution: A Practical Approach to Global Optimization.</title>
<date>2005</date>
<publisher>Springer.</publisher>
<contexts>
<context position="36161" citStr="Price et al., 2005" startWordPosition="5951" endWordPosition="5954">rts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform gree</context>
</contexts>
<marker>Price, Storn, Lampinen, 2005</marker>
<rawString>K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differential Evolution: A Practical Approach to Global Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In ACL-IJCILP.</booktitle>
<contexts>
<context position="35668" citStr="Ravi and Knight (2009" startWordPosition="5875" endWordPosition="5878">rative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help wi</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In ACL-IJCILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression and related optmization problems.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>86</pages>
<contexts>
<context position="19423" citStr="Rose, 1998" startWordPosition="3157" endWordPosition="3158">counts, C = 0. Smoothing causes initial parse trees to be chosen uniformly at random, as suggested by Cohen and Smith (2010): 15 � 5.2 Iterated Fork/Join (IFJ) Our second network daisy-chains grammar inductors, starting from the single-word inter-punctuation fragments in D1 split, then retraining on D2split, and so forth, until finally stopping at D15 it, as before: spl We diagrammed this system as not taking an input, since the first inductor’s output is fully determined by unique parse trees of single-token strings. This iterative approach to optimization is akin to deterministic annealing (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al., 2009, §4.2). Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = 0), IFJ makes use of symmetrizers — e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see §4.2) or how ties are broken in optimization (Spitkovsky et</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression and related optmization problems. Proceedings of the IEEE, 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31657" citStr="Seginer, 2007" startWordPosition="5222" endWordPosition="5223">ork: Golland et al.’s (2012, Figure 1) for CCM and LLCCM; Huang et al.’s (2012, Table 2) for the rest. 10During review, another strong system (Mareˇcek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles). System F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (Klein and Manning, 2002) 33.7 Right-Branching Baseline 40.7 F-CCM (Huang et al., 2012) 45.1 HMM (Ponvert et al., 2011) 46.3 LLCCM (Golland et al., 2012) 47.6 P R CCL (Seginer, 2007) 52.8 54.6 51.1 PRLG (Ponvert et al., 2011) 54.6 60.4 49.8 CS System Combination 54.2 55.6 52.8 Supervised DBM Skyline 59.3 65.7 54.1 Dependency-Based Upper Bound 87.2 100 77.3 Table 3: Harmonic mean (F1) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines. 10 Discussion CoNLL training sets were intended for comparing supervised systems, and aren’t all suitable for unsupervised learning: 12 languages have under 10,000 sentences (with Arabic, Basque, Danish,</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Fast unsupervised incremental parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Selman</author>
<author>H Levesque</author>
<author>D Mitchell</author>
</authors>
<title>A new method for solving hard satisfiability problems.</title>
<date>1992</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="36321" citStr="Selman et al., 1992" startWordPosition="5974" endWordPosition="5977">arts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. </context>
</contexts>
<marker>Selman, Levesque, Mitchell, 1992</marker>
<rawString>B. Selman, H. Levesque, and D. Mitchell. 1992. A new method for solving hard satisfiability problems. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Selman</author>
<author>H A Kautz</author>
<author>B Cohen</author>
</authors>
<title>Noise strategies for improving local search.</title>
<date>1994</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="36375" citStr="Selman et al., 1994" startWordPosition="5982" endWordPosition="5985">ation. However, other reviews are few and generally negative (Kim and Mooney, 2010; Martin-Brualla et al., 2010). Iterated local search methods (Hoos and St¨utzle, 2004; Johnson et al., 1988, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. “Largestep” moves can come from jittering (Hinton and Roweis, 2003), dithering (Price et al., 2005, Ch. 2) or smoothing (Bhargava and Kondrak, 2009). Nonimproving “sideways” moves offer substantial help with hard satisfiability problems (Selman et al., 1992); and injecting non-random noise (Selman et al., 1994), by introducing “uphill” moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing (Kirkpatrick et al., 1983). In NLP, Moore and Quirk’s (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and Mei et al. (2001) incorporated tabu search (Glover, 1989; Glover and Laguna, 1993, Ch. 3) into HMM training for ASR. Genetic algorithms are a</context>
</contexts>
<marker>Selman, Kautz, Cohen, 1994</marker>
<rawString>B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies for improving local search. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Solis</author>
<author>R J-B Wets</author>
</authors>
<title>Minimization by random search techniques.</title>
<date>1981</date>
<journal>Mathematics of Operations Research,</journal>
<volume>6</volume>
<contexts>
<context position="35106" citStr="Solis and Wets, 1981" startWordPosition="5794" endWordPosition="5797">y combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier stages of our iterative networks. Multi-start methods (Solis and Wets, 1981) can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (Hu et al., 1994). This approach is rarely emphasized in NLP literature. For instance, Moore and Quirk (2008) demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements — see below); Ravi and Knight (2009, §5, Figure 8) found random restarts </context>
</contexts>
<marker>Solis, Wets, 1981</marker>
<rawString>F. J. Solis and R. J.-B. Wets. 1981. Minimization by random search techniques. Mathematics of Operations Research, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Baby Steps: How “Less is More” in unsupervised dependency parsing.</title>
<date>2009</date>
<booktitle>In GRLL.</booktitle>
<contexts>
<context position="14615" citStr="Spitkovsky et al., 2009" startWordPosition="2314" endWordPosition="2317">the number of times each (ordered) word pair participates in a dependency relation. We implemented symmetrization also as a single unlexicalized Viterbi training step, but now with proposed parse trees’ scores, for a sentence in D, proportional to a product over non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected: (8) The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009, Figure 3: Uninformed). Symmetrization offers an extra chance to make heads or tails of syntactic relations, after learning which words tend to go together. 6A related approach — initializing EM training with an M-step — was advocated by Klein and Manning (2004, §3). At each instance where a word ® attaches O on (say) the right, our implementation attributes half its weight to the intended construction, O&apos;z�, reserving the other half for the symmetric structure, O attaching ® to its left: Vz�. For the desired effect, these aggregated counts are left unnormalized, while all other counts (of wo</context>
<context position="19485" citStr="Spitkovsky et al., 2009" startWordPosition="3166" endWordPosition="3169">s to be chosen uniformly at random, as suggested by Cohen and Smith (2010): 15 � 5.2 Iterated Fork/Join (IFJ) Our second network daisy-chains grammar inductors, starting from the single-word inter-punctuation fragments in D1 split, then retraining on D2split, and so forth, until finally stopping at D15 it, as before: spl We diagrammed this system as not taking an input, since the first inductor’s output is fully determined by unique parse trees of single-token strings. This iterative approach to optimization is akin to deterministic annealing (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al., 2009, §4.2). Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = 0), IFJ makes use of symmetrizers — e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see §4.2) or how ties are broken in optimization (Spitkovsky et al., 2012a, Appendix B). We therefore expect symmetrization t</context>
<context position="39372" citStr="Spitkovsky et al., 2009" startWordPosition="6444" endWordPosition="6447">amage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier t</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2009</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How “Less is More” in unsupervised dependency parsing. In GRLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="11600" citStr="Spitkovsky et al., 2010" startWordPosition="1847" endWordPosition="1850"> improve its own objective without harming the other’s. This approach does not require tuning termination thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l &lt; 15), starting with soft EM (L = SL, for “soft lateen”). Lexicalized models will cover full data (l &lt; 45) and employ “early-stopping lateen” EM (2011a, §2.3), re-estimating via hard EM until soft EM’s objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l3) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as dire</context>
<context position="18196" citStr="Spitkovsky et al., 2010" startWordPosition="2950" endWordPosition="2953"> model is good, DBM should be able to hang on to it and make improvements. But if it is bad, DBM could get stuck fitting noise, whereas DBMo might be more likely to ramp up to a good alternative. Since we can’t know ahead of time which is the true case, we pursue both optimization paths simultaneously and let a combiner later decide for us. Note that the forks start (and end) optimizing with soft EM. This is because soft EM integrates previously unseen tokens into new grammars better than hard EM, as evidenced by our failed attempt to reproduce the “baby steps” strategy with Viterbi training (Spitkovsky et al., 2010, Figure 4). A combiner then executes hard EM, and since outputs of transforms are trees, the end-to-end process is a chain of lateen alternations that starts and ends with hard EM. We will use a “grammar inductor” to represent subnetworks that transition from Dlsplit to Dl+1 , by split taking transformed parse trees of inter-punctuation fragments up to length l (base data set, D0) to initialize training over fragments up to length l + 1: The FJ network instantiates a grammar inductor with l = 14, thus training on inter-punctuation fragments up to length 15, as in previous work, starting from </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010. Viterbi training improves unsupervised dependency parsing. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9991" citStr="Spitkovsky et al. (2011" startWordPosition="1597" endWordPosition="1600">e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0. 3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 using only the word category c,,, to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word and i</context>
<context position="11699" citStr="Spitkovsky et al., 2011" startWordPosition="1861" endWordPosition="1864">ation thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l &lt; 15), starting with soft EM (L = SL, for “soft lateen”). Lexicalized models will cover full data (l &lt; 45) and employ “early-stopping lateen” EM (2011a, §2.3), re-estimating via hard EM until soft EM’s objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l3) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsi</context>
<context position="29279" citStr="Spitkovsky et al., 2011" startWordPosition="4835" endWordPosition="4838">54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Las</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="9991" citStr="Spitkovsky et al. (2011" startWordPosition="1597" endWordPosition="1600">e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0. 3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 using only the word category c,,, to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word and i</context>
<context position="11699" citStr="Spitkovsky et al., 2011" startWordPosition="1861" endWordPosition="1864">ation thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l &lt; 15), starting with soft EM (L = SL, for “soft lateen”). Lexicalized models will cover full data (l &lt; 45) and employ “early-stopping lateen” EM (2011a, §2.3), re-estimating via hard EM until soft EM’s objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l3) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsi</context>
<context position="29279" citStr="Spitkovsky et al., 2011" startWordPosition="4835" endWordPosition="4838">54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Las</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>A X Chang</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold partof-speech tags.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9991" citStr="Spitkovsky et al. (2011" startWordPosition="1597" endWordPosition="1600">e fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM,2 drawing also on split, simple and raw views of input text. All experiments prior to final multi-lingual evaluation will use the Penn English Treebank’s Wall Street Journal (WSJ) portion (Marcus et al., 1993) as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c),3 for the word categories. 3.2 Smoothing and Lexicalization All unlexicalized instances of DBMs will be estimated with “add one” (a.k.a. Laplace) smoothing, 2We use the short-hand DBM to refer to DBM-3, which is equivalent to DBM-2 if D has no internally-punctuated sentences (D=Dsplit), and DBM-1 if all inputs also have trailing punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0. 3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2 using only the word category c,,, to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word and i</context>
<context position="11699" citStr="Spitkovsky et al., 2011" startWordPosition="1861" endWordPosition="1864">ation thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l &lt; 15), starting with soft EM (L = SL, for “soft lateen”). Lexicalized models will cover full data (l &lt; 45) and employ “early-stopping lateen” EM (2011a, §2.3), re-estimating via hard EM until soft EM’s objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l3) time, and hard EM’s objective (L = H) is the one better suited to long inputs (Spitkovsky et al., 2010). Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2).4 In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments’ heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly). 3.4 Final Evaluation and Metrics Evaluation is against held-out CoNLL shared task data (Buchholz and Marsi, 2006; Nivre et al., 2007), spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsi</context>
<context position="29279" citStr="Spitkovsky et al., 2011" startWordPosition="4835" endWordPosition="4838">54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Las</context>
</contexts>
<marker>Spitkovsky, Chang, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky. 2011c. Unsupervised dependency parsing without gold partof-speech tags. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models.</title>
<date>2012</date>
<booktitle>In ICGI.</booktitle>
<contexts>
<context position="1820" citStr="Spitkovsky et al., 2012" startWordPosition="263" endWordPosition="266">omplete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most propo</context>
<context position="7556" citStr="Spitkovsky et al., 2012" startWordPosition="1196" endWordPosition="1199">n paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w,c,,,); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inputs ending with punctuation; Dfrag = D − Dcomp is everything 1If desired, a scaling factor could be used to bias C+ towards either C∗1 or C∗2, for example based on their likelihood ratio. LD C*1 + C*2 = C+ LD C*1 = L(C1) (5) + LD arg MAX GD C*2 = L(C2) C1 C2 (2) (4) LD C1 C2 1984 else. The “complete” subset is further partitioned into simple sentences, Dsimp ⊆ Dcomp, with no internal punctuation, and others, which may be </context>
<context position="13810" citStr="Spitkovsky et al., 2012" startWordPosition="2181" endWordPosition="2184">cient information required to estimate an arbitrarily-factored model down-stream.6 4.1 Transform #1: A Simple Filter Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F) attempts to extract a cleaner model, based on the simpler complete sentences of Dsi.p. It is implemented as a single (unlexicalized) step of Viterbi training: The idea here is to focus on sentences that are not too complicated yet grammatical. This punctuationsensitive heuristic may steer a learner towards easy but representative training text and, we showed, aids grammar induction (Spitkovsky et al., 2012b, §7.1). 4.2 Transform #2: A Symmetrizer The symmetrizer (S) reduces input models to sets of word association scores. It blurs all details of induced parses in a data set D, except the number of times each (ordered) word pair participates in a dependency relation. We implemented symmetrization also as a single unlexicalized Viterbi training step, but now with proposed parse trees’ scores, for a sentence in D, proportional to a product over non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected: (8) The idea behind the symmetrizer is to glea</context>
<context position="20033" citStr="Spitkovsky et al., 2012" startWordPosition="3266" endWordPosition="3269"> (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al., 2009, §4.2). Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = 0), IFJ makes use of symmetrizers — e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see §4.2) or how ties are broken in optimization (Spitkovsky et al., 2012a, Appendix B). We therefore expect symmetrization to be crucial in earlier stages but to weaken any high quality grammars, nearer the end; it will be up to combiners to handle such phase transitions correctly (or gracefully). 5.3 Grounded Iterated Fork/Join (GIFJ) So far, our networks have been either purely iterative (IFJ) or static (FJ). These two approaches can also be combined, by injecting FJ’s solutions into IFJ’s more dynamic stream. Our new transition subnetwork will join outputs of grammar inductors that either (i) continue a previous solution (as in IFJ); or (ii) start over from scr</context>
<context position="26991" citStr="Spitkovsky et al., 2012" startWordPosition="4451" endWordPosition="4454">lowering cross-entropy (down from 6.96 to 6.93bpt). We propose a distinguished notation for the ICs: (15) 7.2 A Grammar Transformer (GT) The levels of our systems’ performance at grammar induction thus far suggest that the space of possible networks (say, with up to k components) may itself be worth exploring more thoroughly. We leave this exercise to future work, ending with two relatively straight-forward extensions for grounded systems. Our static bootstrapping mechanism (“ground” of GIFJ) can be improved by pretraining with simple sentences first — as in the curriculum for learning DBM-1 (Spitkovsky et al., 2012b, §7.1), but now with a variable length cut-off l (much lower than the original 45) — instead of starting from ∅ directly: (16) l The output of this subnetwork can then be refined, by reconciling it with a previous dynamic solution. We perform a mini-join of a new ground’s counts with Cl, using the filter transform (single steps of lexicalized Viterbi training on clean, simple data), ahead of the main join (over more training data): (17) l This template can be unrolled, as before, to obtain our last network (GT), which achieves 72.9% accuracy and 6.83bpt cross-entropy (slightly less accurate </context>
<context position="29360" citStr="Spitkovsky et al., 2012" startWordPosition="4848" endWordPosition="4851">terbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Last, we checked how our algorithms generalize outside English WSJ, by testing in 23</context>
<context position="34432" citStr="Spitkovsky et al. (2012" startWordPosition="5690" endWordPosition="5693">9.8) 44.2 46.6 40.8 51.3 48.0 (48.7) 49.2 29.6 39.3 57.6 58.2 (75.0) 44.8 39.1 34.1 54.5 56.2 (71.2) 20.2 26.9 23.7 45.0 45.4 (52.2) 51.8 58.2 24.8 52.9 58.3 (67.6) 43.3 40.7 56.8 31.1 34.9 (44.9) 50.8 22.7 32.6 63.7 63.0 (68.9) 50.6 72.4 38.0 72.7 74.5 (81.1) 18.1 35.2 42.1 50.8 50.9 (57.3) 51.9 28.2 57.0 61.7 61.4 (73.2) 48.2 50.7 46.6 48.6 49.7 (62.1) — 34.4 28.0 32.9 29.2 (33.2) 15.7 44.8 42.1 41.7 37.9 (42.4) Average: 40.0 42.9 40.0 47.6 48.6 (57.8) Table 4: Blind evaluation on 2006/7 CoNLL test sets (all sentences) for our full networks (IFJ and GT), previous state-of-the-art systems of Spitkovsky et al. (2012b) and Mareˇcek and ˇZabokrtsk´y (2012), and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier st</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models. In ICGI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Three dependency-and-boundary models for grammar induction.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1820" citStr="Spitkovsky et al., 2012" startWordPosition="263" endWordPosition="266">omplete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 1 Introduction Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams (Paskin 2001a; 2001b). That parsing model has since been extended to make unsupervised learning more feasible (Klein and Manning, 2004; Headden et al., 2009; Spitkovsky et al., 2012b). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search. In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A preferable brute force approach is sampling, as in Markov-chain Monte Carlo (MCMC) and random restarts (Hu et al., 1994), which hit exact solutions eventually. Restarts can be giant steps in a parameter space that undo all previous work. At the other extreme, MCMC may cling to a neighborhood, rejecting most propo</context>
<context position="7556" citStr="Spitkovsky et al., 2012" startWordPosition="1196" endWordPosition="1199">n paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (Pereira and Schabes, 1992; de Marcken, 1995; Gimpel and Smith, 2012, inter alia). The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w,c,,,); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees. 3.1 Models and Data We constrain all parse structures to be projective, via dependency-and-boundary grammars (Spitkovsky et al., 2012a; 2012b): DBMs 0–3 are head-outward generative parsing models (Alshawi, 1996) that distinguish complete sentences from incomplete fragments in a corpus D: Dcomp comprises inputs ending with punctuation; Dfrag = D − Dcomp is everything 1If desired, a scaling factor could be used to bias C+ towards either C∗1 or C∗2, for example based on their likelihood ratio. LD C*1 + C*2 = C+ LD C*1 = L(C1) (5) + LD arg MAX GD C*2 = L(C2) C1 C2 (2) (4) LD C1 C2 1984 else. The “complete” subset is further partitioned into simple sentences, Dsimp ⊆ Dcomp, with no internal punctuation, and others, which may be </context>
<context position="13810" citStr="Spitkovsky et al., 2012" startWordPosition="2181" endWordPosition="2184">cient information required to estimate an arbitrarily-factored model down-stream.6 4.1 Transform #1: A Simple Filter Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F) attempts to extract a cleaner model, based on the simpler complete sentences of Dsi.p. It is implemented as a single (unlexicalized) step of Viterbi training: The idea here is to focus on sentences that are not too complicated yet grammatical. This punctuationsensitive heuristic may steer a learner towards easy but representative training text and, we showed, aids grammar induction (Spitkovsky et al., 2012b, §7.1). 4.2 Transform #2: A Symmetrizer The symmetrizer (S) reduces input models to sets of word association scores. It blurs all details of induced parses in a data set D, except the number of times each (ordered) word pair participates in a dependency relation. We implemented symmetrization also as a single unlexicalized Viterbi training step, but now with proposed parse trees’ scores, for a sentence in D, proportional to a product over non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected: (8) The idea behind the symmetrizer is to glea</context>
<context position="20033" citStr="Spitkovsky et al., 2012" startWordPosition="3266" endWordPosition="3269"> (Rose, 1998), and is patterned after “baby steps” (Spitkovsky et al., 2009, §4.2). Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = 0), IFJ makes use of symmetrizers — e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors may be artifacts of how our generative models factor (see §4.2) or how ties are broken in optimization (Spitkovsky et al., 2012a, Appendix B). We therefore expect symmetrization to be crucial in earlier stages but to weaken any high quality grammars, nearer the end; it will be up to combiners to handle such phase transitions correctly (or gracefully). 5.3 Grounded Iterated Fork/Join (GIFJ) So far, our networks have been either purely iterative (IFJ) or static (FJ). These two approaches can also be combined, by injecting FJ’s solutions into IFJ’s more dynamic stream. Our new transition subnetwork will join outputs of grammar inductors that either (i) continue a previous solution (as in IFJ); or (ii) start over from scr</context>
<context position="26991" citStr="Spitkovsky et al., 2012" startWordPosition="4451" endWordPosition="4454">lowering cross-entropy (down from 6.96 to 6.93bpt). We propose a distinguished notation for the ICs: (15) 7.2 A Grammar Transformer (GT) The levels of our systems’ performance at grammar induction thus far suggest that the space of possible networks (say, with up to k components) may itself be worth exploring more thoroughly. We leave this exercise to future work, ending with two relatively straight-forward extensions for grounded systems. Our static bootstrapping mechanism (“ground” of GIFJ) can be improved by pretraining with simple sentences first — as in the curriculum for learning DBM-1 (Spitkovsky et al., 2012b, §7.1), but now with a variable length cut-off l (much lower than the original 45) — instead of starting from ∅ directly: (16) l The output of this subnetwork can then be refined, by reconciling it with a previous dynamic solution. We perform a mini-join of a new ground’s counts with Cl, using the filter transform (single steps of lexicalized Viterbi training on clean, simple data), ahead of the main join (over more training data): (17) l This template can be unrolled, as before, to obtain our last network (GT), which achieves 72.9% accuracy and 6.83bpt cross-entropy (slightly less accurate </context>
<context position="29360" citStr="Spitkovsky et al., 2012" startWordPosition="4848" endWordPosition="4851">terbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Table 3).9 9 Multi-Lingual Evaluation Last, we checked how our algorithms generalize outside English WSJ, by testing in 23</context>
<context position="34432" citStr="Spitkovsky et al. (2012" startWordPosition="5690" endWordPosition="5693">9.8) 44.2 46.6 40.8 51.3 48.0 (48.7) 49.2 29.6 39.3 57.6 58.2 (75.0) 44.8 39.1 34.1 54.5 56.2 (71.2) 20.2 26.9 23.7 45.0 45.4 (52.2) 51.8 58.2 24.8 52.9 58.3 (67.6) 43.3 40.7 56.8 31.1 34.9 (44.9) 50.8 22.7 32.6 63.7 63.0 (68.9) 50.6 72.4 38.0 72.7 74.5 (81.1) 18.1 35.2 42.1 50.8 50.9 (57.3) 51.9 28.2 57.0 61.7 61.4 (73.2) 48.2 50.7 46.6 48.6 49.7 (62.1) — 34.4 28.0 32.9 29.2 (33.2) 15.7 44.8 42.1 41.7 37.9 (42.4) Average: 40.0 42.9 40.0 47.6 48.6 (57.8) Table 4: Blind evaluation on 2006/7 CoNLL test sets (all sentences) for our full networks (IFJ and GT), previous state-of-the-art systems of Spitkovsky et al. (2012b) and Mareˇcek and ˇZabokrtsk´y (2012), and three-way combination with SAJ (CS, including results up to length ten). a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions (Eisner, 2012; Gormley and Eisner, 2013) may be a good fit for the smaller and simpler, earlier st</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three dependency-and-boundary models for grammar induction. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Stadler</author>
<author>editor</author>
</authors>
<date>1988</date>
<booktitle>Multicriteria Optimization in Engineering and in the Sciences.</booktitle>
<publisher>Plenum Press.</publisher>
<marker>Stadler, editor, 1988</marker>
<rawString>W. Stadler, editor. 1988. Multicriteria Optimization in Engineering and in the Sciences. Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sun</author>
<author>R Grishman</author>
<author>S Sekine</author>
</authors>
<title>Semi-supervised relation extraction with large-scale word clustering.</title>
<date>2011</date>
<booktitle>In ACL. M. Surdeanu</booktitle>
<contexts>
<context position="41991" citStr="Sun et al., 2011" startWordPosition="6833" endWordPosition="6836">ok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC (Geyer</context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised relation extraction with large-scale word clustering. In ACL. M. Surdeanu and C. D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tu</author>
<author>V Honavar</author>
</authors>
<title>On the utility of curricula in unsupervised learning of probabilistic grammars.</title>
<date>2011</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="39394" citStr="Tu and Honavar, 2011" startWordPosition="6448" endWordPosition="6451">heir (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier to recognize root words</context>
</contexts>
<marker>Tu, Honavar, 2011</marker>
<rawString>K. Tu and V. Honavar. 2011. On the utility of curricula in unsupervised learning of probabilistic grammars. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tu</author>
<author>V Honavar</author>
</authors>
<title>Unambiguity regularization for unsupervised learning of probabilistic grammars.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="29242" citStr="Tu and Honavar, 2012" startWordPosition="4829" endWordPosition="4832">tains similar F1-measure (54.2 vs. 54.6, with higher recall) to 8Note that smoothing in the final (unlexicalized) Viterbi step masks the fact that model parts that could not be properly estimated in the first stage (e.g., probabilities of punctuationcrossing arcs) are being initialized to uniform multinomials. HL·DBM D45 HL·DBM D45 split C (18) #3 (IFJ) #2 (GT) #1 HL·DBM D45 CS (19) * C1 C2 HL•DBM Cl F D&apos;1+PlitCl+1 l+1 1989 System DDA (@10) (Gimpel and Smith, 2012) 53.1 (64.3) (Gillenwater et al., 2010) 53.3 (64.3) (Bisk and Hockenmaier, 2012) 53.3 (71.5) (Blunsom and Cohn, 2010) 55.7 (67.7) (Tu and Honavar, 2012) 57.0 (71.4) (Spitkovsky et al., 2011b) 58.4 (71.4) (Spitkovsky et al., 2011c) 59.1 (71.4) #3 (Spitkovsky et al., 2012a) 61.2 (71.4) #2 w/Full TrainingIFJ 62.7 (70.3) #1 ( GT 63.4 (70.3) #1 + #2 + #3 System Combination CS 64.4 (72.0) Supervised DBM (also with loose decoding) 76.3 (85.4) Table 2: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art. PRLG (Ponvert et al., 2011), which is the strongest system of which we are aware (see Tabl</context>
</contexts>
<marker>Tu, Honavar, 2012</marker>
<rawString>K. Tu and V. Honavar. 2012. Unambiguity regularization for unsupervised learning of probabilistic grammars. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tversky</author>
<author>D Kahneman</author>
</authors>
<title>Availability: A heuristic for judging frequency and probability.</title>
<date>1973</date>
<journal>Cognitive Psychology,</journal>
<volume>5</volume>
<contexts>
<context position="40997" citStr="Tversky and Kahneman, 1973" startWordPosition="6683" endWordPosition="6686">riedman, 11The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively “deep” learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy (Bar-Hillel, 1980; Kahneman and Tversky, 1982) and the availability heuristic (Chapman, 1967; Tversky and Kahneman, 1973), since people are notoriously bad at probability (Attneave, 1953; Kahneman and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM varia</context>
</contexts>
<marker>Tversky, Kahneman, 1973</marker>
<rawString>A. Tversky and D. Kahneman. 1973. Availability: A heuristic for judging frequency and probability. Cognitive Psychology, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S I Wang</author>
<author>C D Manning</author>
</authors>
<title>Fast dropout training.</title>
<date>2013</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="41693" citStr="Wang and Manning, 2013" startWordPosition="6792" endWordPosition="6795">an and Tversky, 1972; Kahneman and Tversky, 1973). And second, intermittent “unlearning” — though perhaps not of the kind that takes place inside of our transforms — is an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals (Craik and Bialystok, 2006; Low and Cheng, 2006). “Forgetful EM” strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is “partial EM,” which only suppresses updates, other EM variants (Neal and Hinton, 1999), or “dropout training” (Hinton et al., 2012; Wang and Manning, 2013), which is important in supervised settings. Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling (Carreras and M`arquez, 2005) and relation extraction (Sun et al., 2011), it may be easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems (Surdeanu and Manning, 2010; Petrov, 2010) and automating the operator discovery process. Furthermore, we are optimistic that b</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>S. I. Wang and C. D. Manning. 2013. Fast dropout training. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q I Wang</author>
<author>D Schuurmans</author>
<author>D Lin</author>
</authors>
<title>Semisupervised convex training for dependency parsing.</title>
<date>2008</date>
<booktitle>In HLTACL.</booktitle>
<contexts>
<context position="33213" citStr="Wang et al. (2008)" startWordPosition="5475" endWordPosition="5478">for grammar induction (e.g., 42 vs. 200 for English). These factors contribute to high variances of our (and previous) results (see Table 4). Nevertheless, if we look at the more stable average accuracies, we see a positive trend as we move from a simpler fully-trained system (IFJ, 40.0%), to a more complex system (GT, 47.6%), to system combination (CS, 48.6%). Grounding seems to be more important for the CoNLL sets, possibly because of data sparsity or availability of gold tags. 11 Related Work The surest way to avoid local optima is to craft an objective that doesn’t have them. For example, Wang et al. (2008) demonstrated a convex training method for semi-supervised dependency parsing; Lashkari and Golland (2008) introduced a convex reformulation of likelihood functions for clustering tasks; and Corlett and Penn (2010) designed 1990 Directed Dependency Accuracies (DDA) (@10) MZ SAJ IFJ GT CS 26.5 10.9 33.3 8.3 9.3 (30.2) 27.9 44.9 26.1 25.6 26.8 (45.6) 26.8 33.3 23.5 24.2 24.4 (32.8) 46.0 65.2 35.8 64.2 63.4 (69.1) 47.0 62.1 65.0 68.4 68.0 (79.2) — 63.2 56.0 55.8 58.4 (60.8) — 57.0 49.0 48.6 52.5 (56.0) 49.5 55.1 44.5 43.9 44.0 (52.3) 48.0 54.2 42.9 24.5 34.3 (51.1) 38.6 22.2 37.8 17.1 21.4 (29.8)</context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2008</marker>
<rawString>Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semisupervised convex training for dependency parsing. In HLTACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Xiao</author>
<author>J Zhu</author>
<author>M Zhu</author>
<author>H Wang</author>
</authors>
<title>Boosting-based system combination for machine translation.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="39689" citStr="Xiao et al., 2010" startWordPosition="6494" endWordPosition="6497">techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data (Blum and Mitchell, 1998). One such view retains just the simple sentences, making it easier to recognize root words. Another splits text into many inter-punctuation fragments, helping learn word associations. The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs. By</context>
</contexts>
<marker>Xiao, Zhu, Zhu, Wang, 2010</marker>
<rawString>T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based system combination for machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="39191" citStr="Yarowsky, 1995" startWordPosition="6420" endWordPosition="6421">data set helps avoid some local optima (Liang and Klein, 2009). Elidan et al. (2002) suggested how example-reweighing could cause “informed” changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) “starting small” strategies (Elman, 1993), beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification (Collins and Singer, 1999; Yarowsky, 1995),11 parts-of-speech induction (Clark, 2000; 2003), and language modeling (Krueger and Dayan, 2009; Bengio et al., 2009), in addition to unsupervised parsing (Spitkovsky et al., 2009; Tu and Honavar, 2011; Cohn et al., 2011). 12 Conclusion We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (Xiao et al., 2010), ours do not require incoming models to be of similar quality to make improvements. We exploited thes</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
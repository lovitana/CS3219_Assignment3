<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002725">
<title confidence="0.994078">
Bilingual Word Embeddings for Phrase-Based Machine Translation
</title>
<author confidence="0.998762">
Will Y. Zou†, Richard Socher, Daniel Cer, Christopher D. Manning
</author>
<affiliation confidence="0.888712">
Department of Electrical Engineering† and Computer Science Department
Stanford University, Stanford, CA 94305, USA
</affiliation>
<email confidence="0.996725">
{wzou,danielcer,manning}@stanford.edu, richard@socher.org
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999553769230769">
We introduce bilingual word embeddings: se-
mantic embeddings associated across two lan-
guages in the context of neural language mod-
els. We propose a method to learn bilingual
embeddings from a large unlabeled corpus,
while utilizing MT word alignments to con-
strain translational equivalence. The new em-
beddings significantly out-perform baselines
in word semantic similarity. A single semantic
similarity feature induced with bilingual em-
beddings adds near half a BLEU point to the
results of NIST08 Chinese-English machine
translation task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846784313726">
It is difficult to recognize and quantify semantic sim-
ilarities across languages. The Fr-En phrase-pair
{‘un cas de force majeure’, ‘case of absolute neces-
sity’}, Zh-En phrase pair {‘依然故我’,‘persist in a
stubborn manner’} are similar in semantics. If co-
occurrences of exact word combinations are rare in
the training parallel text, it can be difficult for classi-
cal statistical MT methods to identify this similarity,
or produce a reasonable translation given the source
phrase.
We introduce an unsupervised neural model
to learn bilingual semantic embedding for words
across two languages. As an extension to their
monolingual counter-part (Turian et al., 2010;
Huang et al., 2012; Bengio et al., 2003), bilin-
gual embeddings capture not only semantic infor-
mation of monolingual words, but also semantic re-
lationships across different languages. This prop-
erty allows them to define semantic similarity met-
rics across phrase-pairs, making them perfect fea-
tures for machine translation.
To learn bilingual embeddings, we use a new ob-
jective function which embodies both monolingual
semantics and bilingual translation equivalence. The
latter utilizes word alignments, a natural sub-task
in the machine translation pipeline. Through large-
scale curriculum training (Bengio et al., 2009), we
obtain bilingual distributed representations which
lie in the same feature space. Embeddings of di-
rect translations overlap, and semantic relationships
across bilingual embeddings were further improved
through unsupervised learning on a large unlabeled
corpus.
Consequently, we produce for the research com-
munity a first set of Mandarin Chinese word embed-
dings with 100,000 words trained on the Chinese
Gigaword corpus. We evaluate these embedding
on Chinese word semantic similarity from SemEval-
2012 (Jin and Wu, 2012). The embeddings sig-
nificantly out-perform prior work and pruned tf-idf
base-lines. In addition, the learned embeddings
give rise to 0.11 F1 improvement in Named Entity
Recognition on the OntoNotes dataset (Hovy et al.,
2006) with a neural network model.
We apply the bilingual embeddings in an end-to-
end phrase-based MT system by computing seman-
tic similarities between phrase pairs. On NIST08
Chinese-English translation task, we obtain an im-
provement of 0.48 BLEU from a competitive base-
line (30.01 BLEU to 30.49 BLEU) with the Stanford
Phrasal MT system.
</bodyText>
<page confidence="0.886001">
1393
</page>
<bodyText confidence="0.3234525">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.707816" genericHeader="method">
2 Review of prior work
</sectionHeader>
<bodyText confidence="0.998908764705882">
Distributed word representations are useful in NLP
applications such as information retrieval (Pas¸ca et
al., 2006; Manning et al., 2008), search query ex-
pansions (Jones et al., 2006), or representing se-
mantics of words (Reisinger et al., 2010). A num-
ber of methods have been explored to train and ap-
ply word embeddings using continuous models for
language. Collobert et al. (2008) learn embed-
dings in an unsupervised manner through a con-
trastive estimation technique. Mnih and Hinton (
2008), Morin and Bengio ( 2005) proposed efficient
hierarchical continuous-space models. To system-
atically compare embeddings, Turian et al. (2010)
evaluated improvements they bring to state-of-the-
art NLP benchmarks. Huang et al. (2012) intro-
duced global document context and multiple word
prototypes. Recently, morphology is explored to
learn better word representations through Recursive
Neural Networks (Luong et al., 2013).
Bilingual word representations have been ex-
plored with hand-designed vector space mod-
els (Peirsman and Pad´o , 2010; Sumita, 2000),
and with unsupervised algorithms such as LDA and
LSA (Boyd-Graber and Resnik, 2010; Tam et al.,
2007; Zhao and Xing, 2006). Only recently have
continuous space models been applied to machine
translation (Le et al., 2012). Despite growing in-
terest in these models, little work has been done
along the same lines to train bilingual distributioned
word represenations to improve machine translation.
In this paper, we learn bilingual word embeddings
which achieve competitive performance on seman-
tic word similarity, and apply them in a practical
phrase-based MT system.
</bodyText>
<sectionHeader confidence="0.998525" genericHeader="method">
3 Algorithm and methods
</sectionHeader>
<subsectionHeader confidence="0.999873">
3.1 Unsupervised training with global context
</subsectionHeader>
<bodyText confidence="0.999838">
Our method starts with embedding learning formu-
lations in Collobert et al. (2008). Given a context
window c in a document d, the optimization mini-
mizes the following Context Objective for a word w
in the vocabulary:
</bodyText>
<equation confidence="0.998744333333333">
�IC = max(0,1 − f(cw, d) + f(cwr, d))
wrEVR
(1)
</equation>
<bodyText confidence="0.9987888">
Here f is a function defined by a neural network.
w1 is a word chosen in a random subset VR of the
vocabulary, and cwr is the context window contain-
ing word w&apos;. This unsupervised objective func-
tion contrasts the score between when the correct
word is placed in context with when a random word
is placed in the same context. We incorporate the
global context information as in Huang et al. (2012),
shown to improve performance of word embed-
dings.
</bodyText>
<subsectionHeader confidence="0.999952">
3.2 Bilingual initialization and training
</subsectionHeader>
<bodyText confidence="0.999971307692308">
In the joint semantic space of words across two lan-
guages, the Chinese word ‘政府’ is expected to be
close to its English translation ‘government’. At the
same time, when two words are not direct transla-
tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep
pond), their semantic proximity could be correctly
quantified.
We describe in the next sub-sections the methods
to intialize and train bilingual embeddings. These
methods ensure that bilingual embeddings retain
their translational equivalence while their distribu-
tional semantics are improved during online training
with a monolingual corpus.
</bodyText>
<subsectionHeader confidence="0.846509">
3.2.1 Initialization by MT alignments
</subsectionHeader>
<bodyText confidence="0.999983642857143">
First, we use MT Alignment counts as weighting
to initialize Chinese word embeddings. In our ex-
periments, we use MT word alignments extracted
with the Berkeley Aligner (Liang et al., 2006) 1.
Specifically, we use the following equation to com-
pute starting word embeddings:
In this equation, S is the number of possible tar-
get language words that are aligned with the source
word. Cts denotes the number of times when word t
in the target and word s in the source are aligned in
the training parallel text; Ct denotes the total num-
ber of counts of word t that appeared in the target
language. Finally, Laplace smoothing is applied to
this weighting function.
</bodyText>
<footnote confidence="0.818552">
1On NIST08 Zh-En training data and data from GALE MT
evaluation in the past 5 years
</footnote>
<equation confidence="0.9968024">
S
Wt-init = E
s=1
Cts + 1
Ct + S Ws (2)
</equation>
<page confidence="0.925996">
1394
</page>
<bodyText confidence="0.999917">
Single-prototype English embeddings by Huang
et al. (2012) are used to initialize Chinese em-
beddings. The initialization readily provides a set
(Align-Init) of benchmark embeddings in experi-
ments (Section 4), and ensures translation equiva-
lence in the embeddings at start of training.
</bodyText>
<subsectionHeader confidence="0.751223">
3.2.2 Bilingual training
</subsectionHeader>
<bodyText confidence="0.997524769230769">
Using the alignment counts, we form alignment
matrices Aen→zh and Azh→en. For Aen→zh, each
row corresponds to a Chinese word, and each col-
umn an English word. An element azo is first as-
signed the counts of when the ith Chinese word is
aligned with the jth English word in parallel text.
After assignments, each row is normalized such that
it sums to one. The matrix Azh→en is defined sim-
ilarly. Denote the set of Chinese word embeddings
as Vzh, with each row a word embedding, and the
set of English word embeddings as Ven. With the
two alignment matrices, we define the Translation
Equivalence Objective:
</bodyText>
<equation confidence="0.9986515">
JTEO-en→zh = IIVzh − Aen→zhVenII2 (3)
JTEO-zh→en = IIVen − Azh→enVzh112 (4)
</equation>
<bodyText confidence="0.9289564375">
We optimize for a combined objective during train-
ing. For the Chinese embeddings we optimize for:
JCO-zh + AJTEO-en→zh (5)
For the English embeddings we optimize for:
JCO-en + AJTEO-zh→en (6)
During bilingual training, we chose the value of A
such that convergence is achieved for both JCO and
JTEO. A small validation set of word similarities
from (Jin and Wu, 2012) is used to ensure the em-
beddings have reasonable semantics. 2
In the next sections, ‘bilingual trained’ embed-
dings refer to those initialized with MT alignments
and trained with the objective defined by Equa-
tion 5. ‘Monolingual trained’ embeddings refer to
those intialized by alignment but trained without
JTEO-en→zh.
</bodyText>
<footnote confidence="0.712861">
2In our experiments, A = 50.
</footnote>
<subsectionHeader confidence="0.997234">
3.3 Curriculum training
</subsectionHeader>
<bodyText confidence="0.9999056">
We train 100k-vocabulary word embeddings using
curriculum training (Turian et al., 2010) with Equa-
tion 5. For each curriculum, we sort the vocabu-
lary by frequency and segment the vocabulary by a
band-size taken from {5k, 10k, 25k, 50k}. Separate
bands of the vocabulary are trained in parallel using
minibatch L-BFGS on the Chinese Gigaword cor-
pus 3. We train 100,000 iterations for each curricu-
lum, and the entire 100k vocabulary is trained for
500,000 iterations. The process takes approximately
19 days on a eight-core machine. We show visual-
ization of learned embeddings overlaid with English
in Figure 1. The two-dimensional vectors for this vi-
sualization is obtained with t-SNE (van der Maaten
and Hinton, 2008). To make the figure comprehen-
sible, subsets of Chinese words are provided with
reference translations in boxes with green borders.
Words across the two languages are positioned by
the semantic relationships implied by their embed-
dings.
</bodyText>
<figureCaption confidence="0.80855725">
Figure 1: Overlaid bilingual embeddings: English words
are plotted in yellow boxes, and Chinese words in green;
reference translations to English are provided in boxes
with green borders directly below the original word.
</figureCaption>
<sectionHeader confidence="0.999707" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999669">
4.1 Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.9989915">
We evaluate the Mandarin Chinese embeddings with
the semantic similarity test-set provided by the or-
</bodyText>
<footnote confidence="0.966544">
3Fifth Edition. LDC catelog number LDC2011T13. We only
exclude cna cmn, the Traditional Chinese segment of the cor-
pus.
</footnote>
<page confidence="0.992433">
1395
</page>
<tableCaption confidence="0.999886">
Table 1: Results on Chinese Semantic Similarity
</tableCaption>
<table confidence="0.9998097">
Method Sp. Corr. K. Tau
(×100) (×100)
Prior work (Jin and Wu, 2012) 5.0
Tf-idf
Naive tf-idf 41.5 28.7
Pruned tf-idf 46.7 32.3
Word Embeddings
Align-Init 52.9 37.6
Mono-trained 59.3 42.1
Biling-trained 60.8 43.3
</table>
<tableCaption confidence="0.8677802">
ganizers of SemEval-2012 Task 4. This test-set con-
tains 297 Chinese word pairs with similarity scores
estimated by humans.
The results for semantic similarity are shown in
Table 1. We show two evaluation metrics: Spear-
</tableCaption>
<bodyText confidence="0.986236882352941">
man Correlation and Kendall’s Tau. For both, bilin-
gual embeddings trained with the combined objec-
tive defined by Equation 5 perform best. For pruned
tf-idf, we follow Reisinger et al. (2010; Huang et
al. (2012) and count word co-occurrences in a 10-
word window. We use the best results from a
range of pruning and feature thresholds to compare
against our method. The bilingual and monolingual
trained embeddings4 out-perform pruned tf-idf by
14.1 and 12.6 Spearman Correlation (×100), respec-
tively. Further, they out-perform embeddings initial-
ized from alignment by 7.9 and 6.4. Both our tf-idf
implementation and the word embeddings have sig-
nificantly higher Kendall’s Tau value compared to
Prior work (Jin and Wu, 2012). We verified Tau cal-
culations with original submissions provided by the
authors.
</bodyText>
<subsectionHeader confidence="0.993652">
4.2 Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.999964">
We perform NER experiments on OntoNotes (v4.0)
(Hovy et al., 2006) to validate the quality of the
Chinese word embeddings. Our experimental set-
up is the same as Wang et al. (2013). With em-
beddings, we build a naive feed-forward neural net-
work (Collobert et al., 2008) with 2000 hidden neu-
rons and a sliding window of five words. This naive
setting, without sequence modeling or sophisticated
</bodyText>
<footnote confidence="0.650384666666667">
4Due to variations caused by online minibatch L-BFGS, we
take embeddings from five random points out of last 105 mini-
batch iterations, and average their semantic similarity results.
</footnote>
<tableCaption confidence="0.993064">
Table 2: Results on Named Entity Recognition
</tableCaption>
<table confidence="0.9998985">
Embeddings Prec. Rec. F1 Improve
Align-Init 0.34 0.52 0.41
Mono-trained 0.54 0.62 0.58 0.17
Biling-trained 0.48 0.55 0.52 0.11
</table>
<tableCaption confidence="0.969764">
Table 3: Vector Matching Alignment AER (lower is bet-
ter)
</tableCaption>
<table confidence="0.997408">
Embeddings Prec. Rec. AER
Mono-trained 0.27 0.32 0.71
Biling-trained 0.37 0.45 0.59
</table>
<bodyText confidence="0.9756628">
join optimization, is not competitive with state-of-
the-art (Wang et al., 2013). Table 2 shows that the
bilingual embeddings obtains 0.11 F1 improvement,
lagging monolingual, but significantly better than
Align-Init (as in Section3.2.1) on the NER task.
</bodyText>
<subsectionHeader confidence="0.975523">
4.3 Vector matching alignment
</subsectionHeader>
<bodyText confidence="0.999988428571429">
Translation equivalence of the bilingual embeddings
is evaluated by naive word alignment to match word
embeddings by cosine distance.5 The Alignment Er-
ror Rates (AER) reported in Table 3 suggest that
bilingual training using Equation 5 produces embed-
dings with better translation equivalence compared
to those produced by monolingual training.
</bodyText>
<subsectionHeader confidence="0.997843">
4.4 Phrase-based machine translation
</subsectionHeader>
<bodyText confidence="0.999539">
Our experiments are performed using the Stan-
ford Phrasal phrase-based machine translation sys-
tem (Cer et al., 2010). In addition to NIST08 train-
ing data, we perform phrase extraction, filtering
and phrase table learning with additional data from
GALE MT evaluations in the past 5 years. In turn,
our baseline is established at 30.01 BLEU and rea-
sonably competitive relative to NIST08 results. We
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the decoder.
In the phrase-based MT system, we add one fea-
ture to bilingual phrase-pairs. For each phrase, the
word embeddings are averaged to obtain a feature
vector. If a word is not found in the vocabulary, we
disregard and assume it is not in the phrase; if no
word is found in a phrase, a zero vector is assigned
</bodyText>
<footnote confidence="0.9815515">
5This is evaluated on 10,000 randomly selected sentence
pairs from the MT training set.
</footnote>
<page confidence="0.995128">
1396
</page>
<tableCaption confidence="0.997479">
Table 4: NIST08 Chinese-English translation BLEU
</tableCaption>
<table confidence="0.996187571428571">
Method BLEU
Our baseline 30.01
Embeddings
Random-Init Mono-trained 30.09
Align-Init 30.31
Mono-trained 30.40
Biling-trained 30.49
</table>
<bodyText confidence="0.993713625">
to it. We then compute the cosine distance between
the feature vectors of a phrase pair to form a seman-
tic similarity feature for the decoder.
Results on NIST08 Chinese-English translation
task are reported in Table 46. An increase of
0.48 BLEU is obtained with semantic similarity
with bilingual embeddings. The increase is modest,
just surpassing a reference standard deviation 0.29
BLEU Cer et al. (2010)7 evaluated on a similar sys-
tem. We intend to publish further analysis on statis-
tical significance of this result as an appendix. From
these suggestive evidence in the MT results, random
initialized monolingual trained embeddings add lit-
tle gains to the baseline. Bilingual initialization and
training seem to be offering relatively more consis-
tent gains by introducing translational equivalence.
</bodyText>
<sectionHeader confidence="0.994531" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999927733333333">
In this paper, we introduce bilingual word embed-
dings through initialization and optimization con-
straint using MT alignments The embeddings are
learned through curriculum training on the Chinese
Gigaword corpus. We show good performance on
Chinese semantic similarity with bilingual trained
embeddings. When used to compute semantic simi-
larity of phrase pairs, bilingual embeddings improve
NIST08 end-to-end machine translation results by
just below half a BLEU point. This implies that se-
mantic embeddings are useful features for improv-
ing MT systems. Further, our results offer sugges-
tive evidence that bilingual word embeddings act as
high-quality semantic features and embody bilingual
translation equivalence across languages.
</bodyText>
<footnote confidence="0.996331">
6We report case-insensitive BLEU
7With 4-gram BLEU metric from Table 4
</footnote>
<sectionHeader confidence="0.967452" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866222222222">
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
or the US government. We thank John Bauer and
Thang Luong for helpful discussions.
</bodyText>
<sectionHeader confidence="0.99128" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999024078947368">
A. Klementiev, I. Titov and B. Bhattarai. 2012. Induc-
ing Crosslingual Distributed Representation of Words.
COLING.
Y. Bengio, J. Louradour, R. Collobert and J. Weston.
2009. Curriculum Learning. ICML.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research.
Y. Bengio and Y. LeCunn. 2007. Scaling learning algo-
rithms towards AI. Large-Scale Kernal Machines.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent dirichlet allocation. EMNLP.
D. Cer, M. Galley, D. Jurafsky and C. Manning. 2010.
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Ar-
bitrary Model Features. In Proceedings of the North
American Association of Computational Linguistics -
Demo Session (NAACL-10).
D. Cer, C. Manning and D. Jurafsky. 2010. The Best
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. NAACL.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. ICML.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proceedings of the Fourth Workshop on
Statistical Machine Translation.
M. Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Man-
ning. 2008. NIST Open Machine Translation 2008
Evaluation: Stanford University’s System Description.
Unpublished working notes of the 2008 NIST Open
Machine Translation Evaluation Workshop.
S. Green, S. Wang, D. Cer and C. Manning. 2013. Fast
and adaptive online training of feature-rich translation
models. ACL.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N.
Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath
</reference>
<page confidence="0.93599">
1397
</page>
<reference confidence="0.996656057471264">
and B. Kingsbury. 2012. Deep Neural Networks for
Acoustic Modeling in Speech Recognition. IEEE Sig-
nal Processing Magazine.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.
Weischedel. 2006. OntoNotes: the 90% solution.
NAACL-HLT.
E. H. Huang, R. Socher, C. D. Manning and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. ACL.
P. Jin and Y. Wu. 2012. SemEval-2012 Task 4: Eval-
uating Chinese Word Similarity. Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation. Association for Computational Linguistics.
R. Jones. 2006. Generating query substitutions. In Pro-
ceedings of the 15th international conference on World
Wide Web.
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation. HLT.
H. Le, A. Allauzen and F. Yvon 2012. Continuous space
translation models with neural networks. NAACL.
P. Liang, B. Taskar and D. Klein. 2006. Alignment by
agreement. NAACL.
M. Luong, R. Socher and C. Manning. 2013. Better
word representations with recursive neural networks
for morphology. CONLL.
L. van der Maaten and G. Hinton. 2008. Visualizing data
using t-SNE. Journal of Machine Learning Research.
A. Maas and R. E. Daly and P. T. Pham and D. Huang and
A. Y. Ng and C. Potts. 2011. Learning word vectors
for sentiment analysis. ACL.
C. Manning and P. Raghavan and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press, New York, NY, USA.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky and S.
Khudanpur. 2010. Recurrent neural network based
language model. INTERSPEECH.
T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Ef-
ficient Estimation of Word Representations in Vector
Space. arXiv:1301.3781v1.
A. Mnih and G. Hinton. 2008. A scalable hierarchical
distributed language model. NIPS.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. AISTATS.
F. Och. 2003. Minimum error rate training in statistical
machine translation. ACL.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. ACL.
Y. Peirsman and S. Pad´o. 2010. Cross-lingual induction
of selectional preferences with bilingual vector spaces.
ACL.
J. Reisinger and R. J. Mooney. 2010. Multi-prototype
vector-space models of word meaning. NAACL.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Comput. Surv., 34:1-47, March.
R. Socher, J. Pennington, E. Huang, A. Y. Ng and
C. D. Manning. 2011. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
EMNLP.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
NIPS.
E. Sumita. 2000. Lexical transfer using a vector-space
model. ACL.
Y. Tam, I. Lane and T. Schultz. 2007. Bilingual-LSA
based LM adaptation for spoken language translation.
ACL.
S. Tellex and B. Katz and J. Lin and A. Fernandes and
G. Marton. 2003. Quantitative evaluation of passage
retrieval algorithms for question answering. In Pro-
ceedings of the 26th Annual International ACM SIGIR
Conference on Search and Development in Informa-
tion Retrieval, pages 41-47. ACM Press.
J. Turian and L. Ratinov and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. ACL.
M. Wang, W. Che and C. D. Manning. 2013. Joint Word
Alignment and Bilingual Named Entity Recognition
Using Dual Decomposition. ACL.
K. Yamada and K. Knight. 2001. A Syntax-based Statis-
tical Translation Model. ACL.
B. Zhao and E. P. Xing 2006. BiTAM: Bilingual topic
AdMixture Models for word alignment. ACL.
</reference>
<page confidence="0.99401">
1398
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732396">
<title confidence="0.999811">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
<author confidence="0.998747">Y Richard Socher</author>
<author confidence="0.998747">Daniel Cer</author>
<author confidence="0.998747">D Christopher</author>
<affiliation confidence="0.8753215">of Electrical Computer Science Stanford University, Stanford, CA 94305,</affiliation>
<email confidence="0.98843">richard@socher.org</email>
<abstract confidence="0.999220214285714">We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>I Titov</author>
<author>B Bhattarai</author>
</authors>
<title>Inducing Crosslingual Distributed Representation of Words.</title>
<date>2012</date>
<publisher>COLING.</publisher>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>A. Klementiev, I. Titov and B. Bhattarai. 2012. Inducing Crosslingual Distributed Representation of Words. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<date>2009</date>
<booktitle>Curriculum Learning. ICML.</booktitle>
<contexts>
<context position="2146" citStr="Bengio et al., 2009" startWordPosition="304" endWordPosition="307"> Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This property allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation. To learn bilingual embeddings, we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence. The latter utilizes word alignments, a natural sub-task in the machine translation pipeline. Through largescale curriculum training (Bengio et al., 2009), we obtain bilingual distributed representations which lie in the same feature space. Embeddings of direct translations overlap, and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus. Consequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Y. Bengio, J. Louradour, R. Collobert and J. Weston. 2009. Curriculum Learning. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="1567" citStr="Bengio et al., 2003" startWordPosition="221" endWordPosition="224">The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This property allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation. To learn bilingual embeddings, we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence. The latter utilizes word alignments, a natural sub-task in the machine translation pipeline. Through largescale curriculum training (Bengio et al., 2009), we obtain bilingual</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>Y LeCunn</author>
</authors>
<title>Scaling learning algorithms towards AI. Large-Scale Kernal Machines.</title>
<date>2007</date>
<marker>Bengio, LeCunn, 2007</marker>
<rawString>Y. Bengio and Y. LeCunn. 2007. Scaling learning algorithms towards AI. Large-Scale Kernal Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>P Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: multilingual supervised latent dirichlet allocation.</title>
<date>2010</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="4586" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="671" endWordPosition="674">nd Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 Algorithm and methods 3.1 Unsupervised training with global context Our method starts with embedding learning fo</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment analysis across languages: multilingual supervised latent dirichlet allocation. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>Phrasal: A Toolkit for Statistical Machine Translation with Facilities for Extraction and Incorporation of Arbitrary Model Features.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics -Demo Session (NAACL-10).</booktitle>
<contexts>
<context position="13459" citStr="Cer et al., 2010" startWordPosition="2102" endWordPosition="2105">onolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task. 4.3 Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; i</context>
<context position="14796" citStr="Cer et al. (2010)" startWordPosition="2320" endWordPosition="2323">from the MT training set. 1396 Table 4: NIST08 Chinese-English translation BLEU Method BLEU Our baseline 30.01 Embeddings Random-Init Mono-trained 30.09 Align-Init 30.31 Mono-trained 30.40 Biling-trained 30.49 to it. We then compute the cosine distance between the feature vectors of a phrase pair to form a semantic similarity feature for the decoder. Results on NIST08 Chinese-English translation task are reported in Table 46. An increase of 0.48 BLEU is obtained with semantic similarity with bilingual embeddings. The increase is modest, just surpassing a reference standard deviation 0.29 BLEU Cer et al. (2010)7 evaluated on a similar system. We intend to publish further analysis on statistical significance of this result as an appendix. From these suggestive evidence in the MT results, random initialized monolingual trained embeddings add little gains to the baseline. Bilingual initialization and training seem to be offering relatively more consistent gains by introducing translational equivalence. 5 Conclusion In this paper, we introduce bilingual word embeddings through initialization and optimization constraint using MT alignments The embeddings are learned through curriculum training on the Chi</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>D. Cer, M. Galley, D. Jurafsky and C. Manning. 2010. Phrasal: A Toolkit for Statistical Machine Translation with Facilities for Extraction and Incorporation of Arbitrary Model Features. In Proceedings of the North American Association of Computational Linguistics -Demo Session (NAACL-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>C Manning</author>
<author>D Jurafsky</author>
</authors>
<title>The Best Lexical Metric for Phrase-Based Statistical MT System Optimization.</title>
<date>2010</date>
<publisher>NAACL.</publisher>
<contexts>
<context position="13459" citStr="Cer et al., 2010" startWordPosition="2102" endWordPosition="2105">onolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task. 4.3 Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; i</context>
<context position="14796" citStr="Cer et al. (2010)" startWordPosition="2320" endWordPosition="2323">from the MT training set. 1396 Table 4: NIST08 Chinese-English translation BLEU Method BLEU Our baseline 30.01 Embeddings Random-Init Mono-trained 30.09 Align-Init 30.31 Mono-trained 30.40 Biling-trained 30.49 to it. We then compute the cosine distance between the feature vectors of a phrase pair to form a semantic similarity feature for the decoder. Results on NIST08 Chinese-English translation task are reported in Table 46. An increase of 0.48 BLEU is obtained with semantic similarity with bilingual embeddings. The increase is modest, just surpassing a reference standard deviation 0.29 BLEU Cer et al. (2010)7 evaluated on a similar system. We intend to publish further analysis on statistical significance of this result as an appendix. From these suggestive evidence in the MT results, random initialized monolingual trained embeddings add little gains to the baseline. Bilingual initialization and training seem to be offering relatively more consistent gains by introducing translational equivalence. 5 Conclusion In this paper, we introduce bilingual word embeddings through initialization and optimization constraint using MT alignments The embeddings are learned through curriculum training on the Chi</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>D. Cer, C. Manning and D. Jurafsky. 2010. The Best Lexical Metric for Phrase-Based Statistical MT System Optimization. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<publisher>ICML.</publisher>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<marker>Foster, Kuhn, 2009</marker>
<rawString>G. Foster and R. Kuhn. 2009. Stabilizing minimum error rate training. Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>P Chang</author>
<author>D Cer</author>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Evaluation: Stanford University’s System Description. Unpublished working notes of the</title>
<date>2008</date>
<journal>NIST Open Machine Translation</journal>
<marker>Galley, Chang, Cer, Finkel, Manning, 2008</marker>
<rawString>M. Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Manning. 2008. NIST Open Machine Translation 2008 Evaluation: Stanford University’s System Description. Unpublished working notes of the 2008 NIST Open Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>S Wang</author>
<author>D Cer</author>
<author>C Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models.</title>
<date>2013</date>
<publisher>ACL.</publisher>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>S. Green, S. Wang, D. Cer and C. Manning. 2013. Fast and adaptive online training of feature-rich translation models. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>L Deng</author>
<author>D Yu</author>
<author>G Dahl</author>
<author>A Mohamed</author>
<author>N Jaitly</author>
<author>A Senior</author>
<author>V Vanhoucke</author>
<author>P Nguyen</author>
<author>T Sainath</author>
<author>B Kingsbury</author>
</authors>
<title>Deep Neural Networks for Acoustic Modeling in Speech Recognition.</title>
<date>2012</date>
<journal>IEEE Signal Processing Magazine.</journal>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, Kingsbury, 2012</marker>
<rawString>G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath and B. Kingsbury. 2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition. IEEE Signal Processing Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<publisher>NAACL-HLT.</publisher>
<contexts>
<context position="2895" citStr="Hovy et al., 2006" startWordPosition="415" endWordPosition="418">nd semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus. Consequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Review of prior work Distributed word representations are u</context>
<context position="11859" citStr="Hovy et al., 2006" startWordPosition="1857" endWordPosition="1860">ults from a range of pruning and feature thresholds to compare against our method. The bilingual and monolingual trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 Named Entity Recognition We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings. Our experimental setup is the same as Wang et al. (2013). With embeddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words. This naive setting, without sequence modeling or sophisticated 4Due to variations caused by online minibatch L-BFGS, we take embeddings from five random points out of last 105 minibatch iterations, and average their semantic similarity results. Table 2: Results on Named Entity Recognition Embeddings Prec. Rec. F1 Improve Align-Init 0.3</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. Weischedel. 2006. OntoNotes: the 90% solution. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Huang</author>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1545" citStr="Huang et al., 2012" startWordPosition="217" endWordPosition="220">s across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This property allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation. To learn bilingual embeddings, we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence. The latter utilizes word alignments, a natural sub-task in the machine translation pipeline. Through largescale curriculum training (Bengio et al., 2009</context>
<context position="4180" citStr="Huang et al. (2012)" startWordPosition="612" endWordPosition="615"> al., 2006; Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger et al., 2010). A number of methods have been explored to train and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work </context>
<context position="5801" citStr="Huang et al. (2012)" startWordPosition="877" endWordPosition="880">g formulations in Collobert et al. (2008). Given a context window c in a document d, the optimization minimizes the following Context Objective for a word w in the vocabulary: �IC = max(0,1 − f(cw, d) + f(cwr, d)) wrEVR (1) Here f is a function defined by a neural network. w1 is a word chosen in a random subset VR of the vocabulary, and cwr is the context window containing word w&apos;. This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context. We incorporate the global context information as in Huang et al. (2012), shown to improve performance of word embeddings. 3.2 Bilingual initialization and training In the joint semantic space of words across two languages, the Chinese word ‘政府’ is expected to be close to its English translation ‘government’. At the same time, when two words are not direct translations, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified. We describe in the next sub-sections the methods to intialize and train bilingual embeddings. These methods ensure that bilingual embeddings retain their translational equivalence while their d</context>
<context position="7371" citStr="Huang et al. (2012)" startWordPosition="1139" endWordPosition="1142"> equation to compute starting word embeddings: In this equation, S is the number of possible target language words that are aligned with the source word. Cts denotes the number of times when word t in the target and word s in the source are aligned in the training parallel text; Ct denotes the total number of counts of word t that appeared in the target language. Finally, Laplace smoothing is applied to this weighting function. 1On NIST08 Zh-En training data and data from GALE MT evaluation in the past 5 years S Wt-init = E s=1 Cts + 1 Ct + S Ws (2) 1394 Single-prototype English embeddings by Huang et al. (2012) are used to initialize Chinese embeddings. The initialization readily provides a set (Align-Init) of benchmark embeddings in experiments (Section 4), and ensures translation equivalence in the embeddings at start of training. 3.2.2 Bilingual training Using the alignment counts, we form alignment matrices Aen→zh and Azh→en. For Aen→zh, each row corresponds to a Chinese word, and each column an English word. An element azo is first assigned the counts of when the ith Chinese word is aligned with the jth English word in parallel text. After assignments, each row is normalized such that it sums t</context>
<context position="11169" citStr="Huang et al. (2012)" startWordPosition="1750" endWordPosition="1753">Tau (×100) (×100) Prior work (Jin and Wu, 2012) 5.0 Tf-idf Naive tf-idf 41.5 28.7 Pruned tf-idf 46.7 32.3 Word Embeddings Align-Init 52.9 37.6 Mono-trained 59.3 42.1 Biling-trained 60.8 43.3 ganizers of SemEval-2012 Task 4. This test-set contains 297 Chinese word pairs with similarity scores estimated by humans. The results for semantic similarity are shown in Table 1. We show two evaluation metrics: Spearman Correlation and Kendall’s Tau. For both, bilingual embeddings trained with the combined objective defined by Equation 5 perform best. For pruned tf-idf, we follow Reisinger et al. (2010; Huang et al. (2012) and count word co-occurrences in a 10- word window. We use the best results from a range of pruning and feature thresholds to compare against our method. The bilingual and monolingual trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 N</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>E. H. Huang, R. Socher, C. D. Manning and A. Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jin</author>
<author>Y Wu</author>
</authors>
<title>SemEval-2012 Task 4: Evaluating Chinese Word Similarity.</title>
<date>2012</date>
<booktitle>Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2670" citStr="Jin and Wu, 2012" startWordPosition="381" endWordPosition="384">he machine translation pipeline. Through largescale curriculum training (Bengio et al., 2009), we obtain bilingual distributed representations which lie in the same feature space. Embeddings of direct translations overlap, and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus. Consequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference o</context>
<context position="8672" citStr="Jin and Wu, 2012" startWordPosition="1356" endWordPosition="1359">ings as Vzh, with each row a word embedding, and the set of English word embeddings as Ven. With the two alignment matrices, we define the Translation Equivalence Objective: JTEO-en→zh = IIVzh − Aen→zhVenII2 (3) JTEO-zh→en = IIVen − Azh→enVzh112 (4) We optimize for a combined objective during training. For the Chinese embeddings we optimize for: JCO-zh + AJTEO-en→zh (5) For the English embeddings we optimize for: JCO-en + AJTEO-zh→en (6) During bilingual training, we chose the value of A such that convergence is achieved for both JCO and JTEO. A small validation set of word similarities from (Jin and Wu, 2012) is used to ensure the embeddings have reasonable semantics. 2 In the next sections, ‘bilingual trained’ embeddings refer to those initialized with MT alignments and trained with the objective defined by Equation 5. ‘Monolingual trained’ embeddings refer to those intialized by alignment but trained without JTEO-en→zh. 2In our experiments, A = 50. 3.3 Curriculum training We train 100k-vocabulary word embeddings using curriculum training (Turian et al., 2010) with Equation 5. For each curriculum, we sort the vocabulary by frequency and segment the vocabulary by a band-size taken from {5k, 10k, 2</context>
<context position="10597" citStr="Jin and Wu, 2012" startWordPosition="1659" endWordPosition="1662"> their embeddings. Figure 1: Overlaid bilingual embeddings: English words are plotted in yellow boxes, and Chinese words in green; reference translations to English are provided in boxes with green borders directly below the original word. 4 Experiments 4.1 Semantic Similarity We evaluate the Mandarin Chinese embeddings with the semantic similarity test-set provided by the or3Fifth Edition. LDC catelog number LDC2011T13. We only exclude cna cmn, the Traditional Chinese segment of the corpus. 1395 Table 1: Results on Chinese Semantic Similarity Method Sp. Corr. K. Tau (×100) (×100) Prior work (Jin and Wu, 2012) 5.0 Tf-idf Naive tf-idf 41.5 28.7 Pruned tf-idf 46.7 32.3 Word Embeddings Align-Init 52.9 37.6 Mono-trained 59.3 42.1 Biling-trained 60.8 43.3 ganizers of SemEval-2012 Task 4. This test-set contains 297 Chinese word pairs with similarity scores estimated by humans. The results for semantic similarity are shown in Table 1. We show two evaluation metrics: Spearman Correlation and Kendall’s Tau. For both, bilingual embeddings trained with the combined objective defined by Equation 5 perform best. For pruned tf-idf, we follow Reisinger et al. (2010; Huang et al. (2012) and count word co-occurrenc</context>
</contexts>
<marker>Jin, Wu, 2012</marker>
<rawString>P. Jin and Y. Wu. 2012. SemEval-2012 Task 4: Evaluating Chinese Word Similarity. Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web.</booktitle>
<marker>Jones, 2006</marker>
<rawString>R. Jones. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<publisher>HLT.</publisher>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och and D. Marcu. 2003. Statistical Phrase-Based Translation. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Le</author>
<author>A Allauzen</author>
<author>F Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<publisher>NAACL.</publisher>
<contexts>
<context position="4724" citStr="Le et al., 2012" startWordPosition="694" endWordPosition="697">ements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 Algorithm and methods 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window c in a document d, the optimization minimizes the following Context Objectiv</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>H. Le, A. Allauzen and F. Yvon 2012. Continuous space translation models with neural networks. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<publisher>NAACL.</publisher>
<contexts>
<context position="6714" citStr="Liang et al., 2006" startWordPosition="1017" endWordPosition="1020">tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified. We describe in the next sub-sections the methods to intialize and train bilingual embeddings. These methods ensure that bilingual embeddings retain their translational equivalence while their distributional semantics are improved during online training with a monolingual corpus. 3.2.1 Initialization by MT alignments First, we use MT Alignment counts as weighting to initialize Chinese word embeddings. In our experiments, we use MT word alignments extracted with the Berkeley Aligner (Liang et al., 2006) 1. Specifically, we use the following equation to compute starting word embeddings: In this equation, S is the number of possible target language words that are aligned with the source word. Cts denotes the number of times when word t in the target and word s in the source are aligned in the training parallel text; Ct denotes the total number of counts of word t that appeared in the target language. Finally, Laplace smoothing is applied to this weighting function. 1On NIST08 Zh-En training data and data from GALE MT evaluation in the past 5 years S Wt-init = E s=1 Cts + 1 Ct + S Ws (2) 1394 S</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar and D. Klein. 2006. Alignment by agreement. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Luong</author>
<author>R Socher</author>
<author>C Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<publisher>CONLL.</publisher>
<contexts>
<context position="4370" citStr="Luong et al., 2013" startWordPosition="638" endWordPosition="641">and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve compe</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>M. Luong, R. Socher and C. Manning. 2013. Better word representations with recursive neural networks for morphology. CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Maaten</author>
<author>G Hinton</author>
</authors>
<title>Visualizing data using t-SNE.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research.</journal>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>L. van der Maaten and G. Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maas</author>
<author>R E Daly</author>
<author>P T Pham</author>
<author>D Huang</author>
<author>A Y Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<publisher>ACL.</publisher>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. Maas and R. E. Daly and P. T. Pham and D. Huang and A. Y. Ng and C. Potts. 2011. Learning word vectors for sentiment analysis. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>P Raghavan</author>
<author>H Schtze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3594" citStr="Manning et al., 2008" startWordPosition="520" endWordPosition="523">d phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Review of prior work Distributed word representations are useful in NLP applications such as information retrieval (Pas¸ca et al., 2006; Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger et al., 2010). A number of methods have been explored to train and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced gl</context>
</contexts>
<marker>Manning, Raghavan, Schtze, 2008</marker>
<rawString>C. Manning and P. Raghavan and H. Schtze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafiat</author>
<author>L Burget</author>
<author>J Cernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<publisher>INTERSPEECH.</publisher>
<marker>Mikolov, Karafiat, Burget, Cernocky, Khudanpur, 2010</marker>
<rawString>T. Mikolov, M. Karafiat, L. Burget, J. Cernocky and S. Khudanpur. 2010. Recurrent neural network based language model. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<publisher>NIPS.</publisher>
<marker>Mnih, Hinton, 2008</marker>
<rawString>A. Mnih and G. Hinton. 2008. A scalable hierarchical distributed language model. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<publisher>AISTATS.</publisher>
<marker>Morin, Bengio, 2005</marker>
<rawString>F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="13790" citStr="Och, 2003" startWordPosition="2158" endWordPosition="2159">g using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 5This is evaluated on 10,000 randomly selected sentence pairs from the MT training set. 1396 Table 4: NIST08 Chinese-English translation BLEU Method BLEU Our baseline 30.01 Embeddings Random-Init Mono-trained 30.09 Align-Init 30.31 Mono-trained 30.40 Biling-trained 30.49 t</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: fact extraction in the fast lane.</title>
<date>2006</date>
<publisher>ACL.</publisher>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits and A. Jain. 2006. Names and similarities on the web: fact extraction in the fast lane. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Peirsman</author>
<author>S Pad´o</author>
</authors>
<title>Cross-lingual induction of selectional preferences with bilingual vector spaces.</title>
<date>2010</date>
<publisher>ACL.</publisher>
<marker>Peirsman, Pad´o, 2010</marker>
<rawString>Y. Peirsman and S. Pad´o. 2010. Cross-lingual induction of selectional preferences with bilingual vector spaces. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>R J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<publisher>NAACL.</publisher>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>J. Reisinger and R. J. Mooney. 2010. Multi-prototype vector-space models of word meaning. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Comput. Surv.,</journal>
<pages>34--1</pages>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Comput. Surv., 34:1-47, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<publisher>EMNLP.</publisher>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. Huang, A. Y. Ng and C. D. Manning. 2011. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<publisher>NIPS.</publisher>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
</authors>
<title>Lexical transfer using a vector-space model.</title>
<date>2000</date>
<publisher>ACL.</publisher>
<contexts>
<context position="4502" citStr="Sumita, 2000" startWordPosition="660" endWordPosition="661"> a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 Algorithm and methods 3.1 Un</context>
</contexts>
<marker>Sumita, 2000</marker>
<rawString>E. Sumita. 2000. Lexical transfer using a vector-space model. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tam</author>
<author>I Lane</author>
<author>T Schultz</author>
</authors>
<title>Bilingual-LSA based LM adaptation for spoken language translation.</title>
<date>2007</date>
<publisher>ACL.</publisher>
<contexts>
<context position="4604" citStr="Tam et al., 2007" startWordPosition="675" endWordPosition="678">icient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 Algorithm and methods 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Coll</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Y. Tam, I. Lane and T. Schultz. 2007. Bilingual-LSA based LM adaptation for spoken language translation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Search and Development in Information Retrieval,</booktitle>
<pages>41--47</pages>
<publisher>ACM Press.</publisher>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>S. Tellex and B. Katz and J. Lin and A. Fernandes and G. Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th Annual International ACM SIGIR Conference on Search and Development in Information Retrieval, pages 41-47. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1525" citStr="Turian et al., 2010" startWordPosition="213" endWordPosition="216"> semantic similarities across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This property allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation. To learn bilingual embeddings, we use a new objective function which embodies both monolingual semantics and bilingual translation equivalence. The latter utilizes word alignments, a natural sub-task in the machine translation pipeline. Through largescale curriculum training </context>
<context position="4091" citStr="Turian et al. (2010)" startWordPosition="599" endWordPosition="602">rd representations are useful in NLP applications such as information retrieval (Pas¸ca et al., 2006; Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger et al., 2010). A number of methods have been explored to train and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to mach</context>
<context position="9133" citStr="Turian et al., 2010" startWordPosition="1426" endWordPosition="1429"> training, we chose the value of A such that convergence is achieved for both JCO and JTEO. A small validation set of word similarities from (Jin and Wu, 2012) is used to ensure the embeddings have reasonable semantics. 2 In the next sections, ‘bilingual trained’ embeddings refer to those initialized with MT alignments and trained with the objective defined by Equation 5. ‘Monolingual trained’ embeddings refer to those intialized by alignment but trained without JTEO-en→zh. 2In our experiments, A = 50. 3.3 Curriculum training We train 100k-vocabulary word embeddings using curriculum training (Turian et al., 2010) with Equation 5. For each curriculum, we sort the vocabulary by frequency and segment the vocabulary by a band-size taken from {5k, 10k, 25k, 50k}. Separate bands of the vocabulary are trained in parallel using minibatch L-BFGS on the Chinese Gigaword corpus 3. We train 100,000 iterations for each curriculum, and the entire 100k vocabulary is trained for 500,000 iterations. The process takes approximately 19 days on a eight-core machine. We show visualization of learned embeddings overlaid with English in Figure 1. The two-dimensional vectors for this visualization is obtained with t-SNE (van</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian and L. Ratinov and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>W Che</author>
<author>C D Manning</author>
</authors>
<title>Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition.</title>
<date>2013</date>
<publisher>ACL.</publisher>
<contexts>
<context position="11972" citStr="Wang et al. (2013)" startWordPosition="1878" endWordPosition="1881">trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 Named Entity Recognition We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings. Our experimental setup is the same as Wang et al. (2013). With embeddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words. This naive setting, without sequence modeling or sophisticated 4Due to variations caused by online minibatch L-BFGS, we take embeddings from five random points out of last 105 minibatch iterations, and average their semantic similarity results. Table 2: Results on Named Entity Recognition Embeddings Prec. Rec. F1 Improve Align-Init 0.34 0.52 0.41 Mono-trained 0.54 0.62 0.58 0.17 Biling-trained 0.48 0.55 0.52 0.11 Table 3: Vector Matching Alignmen</context>
</contexts>
<marker>Wang, Che, Manning, 2013</marker>
<rawString>M. Wang, W. Che and C. D. Manning. 2013. Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A Syntax-based Statistical Translation Model.</title>
<date>2001</date>
<publisher>ACL.</publisher>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A Syntax-based Statistical Translation Model. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>BiTAM: Bilingual topic AdMixture Models for word alignment.</title>
<date>2006</date>
<publisher>ACL.</publisher>
<contexts>
<context position="4626" citStr="Zhao and Xing, 2006" startWordPosition="679" endWordPosition="682">l continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 Algorithm and methods 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). G</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>B. Zhao and E. P. Xing 2006. BiTAM: Bilingual topic AdMixture Models for word alignment. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
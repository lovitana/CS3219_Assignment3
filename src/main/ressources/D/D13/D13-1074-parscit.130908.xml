<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000152">
<title confidence="0.974586">
Joint Learning and Inference for Grammatical Error Correction
</title>
<author confidence="0.989564">
Alla Rozovskaya and Dan Roth
</author>
<affiliation confidence="0.993112">
Cognitive Computation Group
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.9418815">
201 N. Goodwin Avenue
Urbana, IL 61801
</address>
<email confidence="0.999794">
{rozovska,danr}@illinois.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830791666667">
State-of-the-art systems for grammatical er-
ror correction are based on a collection of
independently-trained models for specific er-
rors. Such models ignore linguistic interac-
tions at the sentence level and thus do poorly
on mistakes that involve grammatical depen-
dencies among several words. In this paper,
we identify linguistic structures with interact-
ing grammatical properties and propose to ad-
dress such dependencies via joint inference
and joint learning.
We show that it is possible to identify interac-
tions well enough to facilitate a joint approach
and, consequently, that joint methods correct
incoherent predictions that independently-
trained classifiers tend to produce. Further-
more, because the joint learning model con-
siders interacting phenomena during training,
it is able to identify mistakes that require mak-
ing multiple changes simultaneously and that
standard approaches miss. Overall, our model
significantly outperforms the Illinois system
that placed first in the CoNLL-2013 shared
task on grammatical error correction.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991926">
There has recently been a lot of work addressing er-
rors made by English as a Second Language (ESL)
learners. In the past two years, three competitions
devoted to grammatical error correction for non-
native writers took place: HOO-2011 (Dale and Kil-
garriff, 2011), HOO-2012 (Dale et al., 2012), and
the CoNLL-2013 shared task (Ng et al., 2013).
Nowadays *phone/phones *has/have many
functionalities, *included/including *0/a
camera and *Al/a Wi-Fi receiver.
</bodyText>
<figureCaption confidence="0.999691">
Figure 1: Examples of representative ESL errors.
</figureCaption>
<bodyText confidence="0.999813275862069">
Most of the work in the area of ESL error cor-
rection has addressed the task by building statistical
models that specialize in correcting a specific type
of a mistake. Figure 1 illustrates several types of
errors common among non-native speakers of En-
glish: article, subject-verb agreement, noun num-
ber, and verb form. A significant proportion of re-
search has focused on correcting mistakes in article
and preposition usage (Izumi et al., 2003; Han et
al., 2006; Felice and Pulman, 2008; Gamon et al.,
2008; Tetreault and Chodorow, 2008; Gamon, 2010;
Rozovskaya and Roth, 2010b). Several studies also
consider verb-related and noun-related errors (Lee
and Seneff, 2008; Gamon et al., 2008; Dahlmeier
and Ng, 2012). The predictions made by individual
models are then applied independently (Rozovskaya
et al., 2011) or pipelined (Dahlmeier and Ng, 2012).
The standard approach of training individual clas-
sifiers considers each word independently and thus
assumes that there are no interactions between er-
rors and between grammatical phenomena. But an
ESL writer may make multiple mistakes in a single
sentence and these result in misleading local cues
given to individual classifiers. In the example shown
in Figure 1, the agreement error on the verb “have”
interacts with the noun number error: a correction
system that takes into account the context may in-
fer, because of the word “phone”, that the verb num-
ber is correct. For this reason, a system that consid-
</bodyText>
<page confidence="0.967522">
791
</page>
<note confidence="0.7395405">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791–802,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.961837235294118">
ers noun and agreement errors separately will fail to
identify and correct the interacting errors shown in
Fig. 1. Furthermore, it may also produce inconsis-
tent predictions.
Even though it is quite clear that grammatical er-
rors interact, for various conceptual and technical
reasons, this issue has not been addressed in a sig-
nificant way in the literature. We believe that the
reasons for that are three-fold: (1) Data: until very
recently we did not have data that jointly annotates
sufficiently many errors of interacting phenomena
(see Sec. 2). (2) Conceptual: Correcting errors in
interacting linguistic phenomena requires that one
identifies those phenomena and, more importantly,
can recognize reliably the interacting components
(e.g., given a verb, identify the subject to enable en-
forcing agreement). The perception has been that
this cannot be done reliably (Sec. 4). (3) Technical:
The NLP community has started to better understand
joint learning and inference and apply it to various
phenomena (Roth and Yih, 2004; Punyakanok et al.,
2008; Martins et al., 2011; Clarke and Lapata, 2007;
Sutton and McCallum, 2007) (Sec. 5).
In this paper we present, for the first time, a suc-
cessful approach to jointly resolving grammatical er-
rors. Specifically:
• We identify two pairs of interacting phenomena,
subject-verb and article-NPhead agreements; we
show how to reliably identify these pairs in noisy
ESL data, thereby facilitating the joint correction of
these phenomena.
• We propose two joint approaches: (1) a joint infer-
ence approach implemented on top of individually
learned models using an integer linear programming
formulation (ILP, (Roth and Yih, 2004)), and (2) a
model that jointly learns each pair of these phenom-
ena. We show that each of these methods has its ad-
vantages, and that both solve the two challenges out-
lined above: the joint models exclude inconsistent
predictions that violate linguistic constraints. The
joint learning model exhibits superior performance,
as it is also able to overcome the problem of the
noisy context encountered by the individual mod-
els and to identify errors in contexts, where multiple
changes need to be applied at the same time.
We show that our joint models produce state-of-
the-art performance and, in particular, significantly
outperform the University of Illinois system that
placed first in the CoNLL-2013 shared task, increas-
ing the F1 score by 2 and 4 points in different evalu-
ation settings.
</bodyText>
<sectionHeader confidence="0.895285" genericHeader="introduction">
2 Task Description and Motivation
</sectionHeader>
<bodyText confidence="0.999988727272727">
To illustrate the utility of jointly addressing interact-
ing grammatical phenomena, we consider the cor-
pus of the CoNLL-2013 shared task on grammatical
error correction (Ng et al., 2013), which we found
to be particularly well-suited for addressing interac-
tions between grammatical phenomena. The task fo-
cuses on the following five common mistakes made
by ESL writers: article, preposition, noun number,
subject-verb agreement, and verb form, and we ad-
dress two interactions: article-NPhead and subject-
verb.
The training data for the task is from the NUCLE
corpus (Dahlmeier et al., 2013), an error-tagged col-
lection of essays written by non-native learners of
English. The test data is an additional set of essays
by learners from the same linguistic background.
The training and the test data contain 1.2M and 29K
words, respectively. Although the corpus contains
errors of other types, the task focuses on five types
of errors. Table 1 shows the number of mistakes1 of
each type and the error rates, i.e. the percentage of
erroneous words by error type.
</bodyText>
<table confidence="0.995237285714286">
Error Number of errors and error rate
Training Test
Article 6658 (2.4%) 690 (10.0%)
Prep. 2404 (2.0%) 311 (10.7%)
Noun 3779 (1.6%) 396 (6.0%)
Verb Agr. 1527(2.0%) 124 (5.2%)
Verb Form 1453 (0.8%) 122 (2.5%)
</table>
<tableCaption confidence="0.999477">
Table 1: Number of annotated errors in the CoNLL-
</tableCaption>
<bodyText confidence="0.810835125">
2013 shared task. Percentage denotes the error rates, i.e.
the number of erroneous instances with respect to the to-
tal number of relevant instances in the data. For example,
10.7% of prepositions in the test data are used incorrectly.
The numbers in the revised data set are slightly higher.
We note that the CoNLL-2013 data set is the first
annotated collection that makes a study like ours
feasible. The presence of a common test set that
</bodyText>
<footnote confidence="0.955397">
1System performance in the shared task is evaluated on data
with and without additional revisions added based on the input
from participants. The number of mistakes in the revised test
data is slightly higher.
</footnote>
<page confidence="0.996997">
792
</page>
<bodyText confidence="0.999965307692308">
contains a good number of interacting errors – ar-
ticle, noun, and verb agreement mistakes – makes
the data set well-suited for studying which approach
works best for addressing interacting phenomena.
The HOO-2011 shared task collection (Dale and
Kilgarriff, 2011) contains a very small number of
noun and agreement errors (41 and 11 in test, re-
spectively), while the HOO-2012 competition (Dale
et al., 2012) only addresses article and preposition
mistakes. Indeed, in parallel to the work presented
here, Wu and Ng (2013) attempted the ILP-based
approach of Roth and Yih (2004) in this domain.
They were not able to show any improvement, for
two reasons. First, the HOO-2011 data set which
they used does not contain a good number of errors
in interacting structures. Second, and most impor-
tantly, they applied constraints in an indiscriminate
manner. In contrast, we show how to identify the
interacting structures’ components in a reliable way,
and this plays a key role in the joint modeling im-
provements.
Lack of data hindered other earlier efforts for
error correction beyond individual language phe-
nomena. Brockett et al. (2006) applied machine-
translation techniques to correct noun number errors
on mass nouns and article usage but their application
was restricted to a small set of constructions. Park
and Levy (2011) proposed a language-modeling ap-
proach to whole sentence error correction but their
model is not competitive with individually trained
models. Finally, Dahlmeier and Ng (2012) proposed
a decoder model, focusing on four types of errors
in the data set of the HOO-2011 competition (Dale
and Kilgarriff, 2011). The decoder optimized the se-
quence in which individual classifiers were to be ap-
plied to the sentence. However, because the decoder
still corrected mistakes in a pipeline fashion, one at
a time, it is unlikely that it could deal with cases that
require simultaneous changes.
</bodyText>
<sectionHeader confidence="0.985811" genericHeader="method">
3 The University of Illinois System
</sectionHeader>
<bodyText confidence="0.998210076923077">
Below, we briefly describe the University of Illinois
system (henceforth Illinois; in the overview paper of
the shared task the system is referred to as UI) that
achieved the best result in the CoNLL-2013 shared
task and which we use as our baseline model. For
a complete description, we refer the reader to Ro-
zovskaya et al. (2013).
The Illinois system implements five machine-
learning independently-trained classifiers that fol-
low the popular approach to ESL error correction
borrowed from the context-sensitive spelling correc-
tion task (Golding and Roth, 1999; Carlson et al.,
2001). A confusion set is defined that specifies a
list of confusable words. Each occurrence of a con-
fusable word in text is represented as a vector of
features derived from a context window around the
target. The problem is cast as a multi-class classi-
fication task and a classifier is trained on native or
learner data. At prediction time, the model selects
the most likely candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions. The arti-
cle confusion set is as follows: {a,the,0}2. The con-
fusion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants (Table 2).
</bodyText>
<table confidence="0.9929705">
“Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear technol-
ogy.”
Error type Confusion set
Noun {factor, factors}
Verb Agr. {contribute, contributes}
Verb Form {included, including, includes, include}
</table>
<tableCaption confidence="0.9206985">
Table 2: Confusion sets for noun number, agreement,
and form classifiers.
</tableCaption>
<bodyText confidence="0.998695823529412">
The article classifier is a discriminative model
that draws on the state-of-the-art approach described
in Rozovskaya et al. (2012). The model makes use
of the Averaged Perceptron algorithm (Freund and
Schapire, 1996) and is trained on the training data of
the shared task with rich features.
The other models are trained on native English
data, the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Naive
Bayes (NB) algorithm. All models use word n-gram
features derived from the 4-word window around the
target word. In the preposition model, priors for
preposition preferences are learned from the shared
task training data (Rozovskaya and Roth, 2011).
20 denotes noun-phrase-initial contexts where an article is
likely to have been omitted. The variants “a” and “an” are con-
flated and are restored later.
</bodyText>
<page confidence="0.995382">
793
</page>
<table confidence="0.98536525">
Example Predictions made by the Illinois system
“They believe that such situation must be avoided.” such situation —* such a situations
“Nevertheless , electric cars is still regarded as a great trial innovation.” cars is —* car are
“Every students have appointments with the head of the department.” No change
</table>
<tableCaption confidence="0.999684">
Table 3: Examples of predictions of the Illinois system that combines independently-trained models.
</tableCaption>
<bodyText confidence="0.9987353">
The words that are selected as input to classifiers
are called candidates. Article and preposition can-
didates are identified with a closed list of words;
noun-phrase-initial contexts for the article classifier
are determined using a shallow parser3 (Punyakanok
and Roth, 2001). Candidates for the noun, agree-
ment, and form classifiers are identified with a part-
of-speech tagger4, e.g. noun candidates are words
that are tagged as NN or NNS. Table 4 shows the
total number of candidates for each classifier.
</bodyText>
<table confidence="0.9945875">
Classifier
Art. P N Agr. F
Train 254K 103K 240K 75K 175K
Test 6K 2.5K 2.6K 2.4K 4.8K
</table>
<tableCaption confidence="0.992954">
Table 4: Number of candidate words by classifier type
in training and test data.
</tableCaption>
<sectionHeader confidence="0.995096" genericHeader="method">
4 Interacting Mistakes
</sectionHeader>
<bodyText confidence="0.999972714285714">
The approach of addressing each type of mistake in-
dividually is problematic when multiple phenomena
interact. Consider the examples in Table 3 and the
predictions made by the Illinois system. In the first
and second sentences, there are two possible ways
to correct the structures “such situation” and “cars
is”. In the former, either the article or the noun num-
ber should be changed; in the latter, either the noun
number or the verb agreement marker5. In these ex-
amples, each of the independently-trained classifiers
identifies the problem because each system makes a
decision using the second error as part of its contex-
tual cues, and thus the individual systems produce
inconsistent predictions.
</bodyText>
<footnote confidence="0.89853">
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
4http://cogcomp.cs.illinois.edu/page/
software view/POS
5Both of these solutions will result in grammatical output
and the specific choice between the two depends on the wider
essay context.
</footnote>
<bodyText confidence="0.999992857142857">
The second type of interaction concerns cases that
require correcting more than one word at a time:
the last example in Table 3 requires making changes
both to the verb and the subject. Since each of the in-
dependent classifiers (for nouns and for verb agree-
ment) takes into account the other word as part of
its features, they both infer that the verb number is
correct and that the grammatical subject “student”
should be plural.
We refer to the words whose grammatical prop-
erties interact as structures. The independently-
trained classifiers tend to fail to provide valid cor-
rections in contexts where it is important to consider
both words of the structure.
</bodyText>
<subsectionHeader confidence="0.917945">
4.1 Structures for Joint Modeling
</subsectionHeader>
<bodyText confidence="0.999939434782609">
We address two linguistic structures that are relevant
for the grammatical phenomena considered: article-
NPhead and subject-verb. In the article-NPhead
structures, the interaction is between the head of
the noun phrase (NP) and the article that refers to
the NP (first example in Table 3). In particular,
the model should take into account that the article
“a” cannot be combined with a noun in plural form.
For subject-verb agreement, the subject and the verb
should agree in number.
We now need to identify all pairs of candidates
that form the relevant structures. Article-NPhead
structures are pairs of words, such that the first word
is a candidate of type article, while the second word
is a noun candidate. Given an article candidate, the
head of its NP is determined using the POS infor-
mation (this information is obtained from the article
feature vector because the NP head is a feature used
by the article system)6. Subject-verb structures are
pairs of noun-agreement candidates. Given a verb,
its subject is identified with a dependency parser
(Marneffe et al., 2006).
To evaluate the accuracy of subject and NP head
</bodyText>
<footnote confidence="0.666102">
6Some heads are not identified or belong to a different part
of speech.
</footnote>
<page confidence="0.994502">
794
</page>
<bodyText confidence="0.999791092592593">
predictions, a random sample of 500 structures of
each type from the training data was examined by
a human annotator with formal training in Linguis-
tics. The human annotations were then compared
against the automatic predictions. The results of
the evaluation for subject-verb and article-NPhead
structures are shown in Tables 5 and 6, respectively.
Although the overall accuracy is above 90% for both
structures, the accuracy varies by the distance be-
tween the structure components and drops signifi-
cantly as the distance increases. For article-NPhead
structures, distance indicates the position of the NP
head with respect to the article, e.g. distance of 1
means that the head immediately follows the arti-
cle. For subject-verb structures, distance is shown
with respect to the verb: a distance of -1 means that
the subject immediately precedes the verb. Although
in most cases the subject is located to the left of
the verb, in some constructions, such as existential
clauses and inversions, it occurs after the verb.
Based on the accuracy results for identifying the
structure components, we select those structures
where the components are reliably identified. For
article-NPhead, valid structures are those where the
distance is at most three words. For subject-verb, we
consider as valid those structures where the identi-
fied subject is located within two words to the left or
three words to the right of the verb.
The valid structures are selected as input to the
joint model (Sec. 5). The joint learning model con-
siders only those valid structures whose components
are adjacent. In adjacent structures the NP head im-
mediately follows the article, and the verb immedi-
ately follows the subject. Joint inference is not re-
stricted to adjacent structures.
The last column of Table 5 shows that valid
subject-verb structures account for 67.5% of all
verbs whose subjects are common nouns (51.7% are
cases where the words are adjacent). Verbs whose
subjects are common nouns account for 57.8% of all
verbs that have subjects (verbs with different types
of subjects, most of which are personal pronouns,
are not considered here, since these subjects are not
part of the noun classifier).
Valid article-NPhead structures account for
98.0% of all articles whose NP heads are common
nouns (47.5% of those are adjacent structures), as
shown in the last column of Table 6. 71.0% of arti-
cles in the training data belong to an NP whose head
is a common noun; NPs whose heads belong to dif-
ferent parts of speech are not considered.
Note also that because a noun may belong both to
an article-NPhead and a subject-verb structure, the
structures contain an overlap.
</bodyText>
<table confidence="0.999774166666667">
Distance Accuracy % of all subj. Cumul.
predictions
-1 97.6% 51.7% 51.7%
1,2,3 100.0% 8.9% 60.6%
-2 88.2% 6.9% 67.5%
Other 80.8% 32.5% 100.0%
</table>
<tableCaption confidence="0.792213333333333">
Table 5: Accuracy of subject identification on a random
sample of subject-verb structures from the training data.
The overall accuracy is 91.52%. For each distance, the follow-
ing are shown: accuracy based on comparison with human eval-
uation; the percentage of all predictions that have this distance;
the cumulative percentage.
</tableCaption>
<table confidence="0.9999405">
Distance Accuracy % of all head Cumul.
predictions
1 94.8% 47.5% 47.5%
2 94.4% 44.0% 91.5%
3 92.3% 6.5% 98.0%
Other 89.1% 2.0% 100%
</table>
<tableCaption confidence="0.898652666666667">
Table 6: Accuracy of NP head identification on a random
sample of article-NPhead structures from training data. The
overall accuracy is 94.45%. For each distance, the following are
shown: accuracy based on comparison with human evaluation;
the percentage of all predictions that have this distance; the cu-
mulative percentage.
</tableCaption>
<sectionHeader confidence="0.998081" genericHeader="method">
5 The Joint Model
</sectionHeader>
<bodyText confidence="0.975421571428571">
In this section, we present the joint inference and
the joint learning approaches. In the joint inference
approach, we use the independently-learned models
from the Illinois system, and the interacting target
words identified earlier are considered only at infer-
ence stage. In the joint learning method, we jointly
learn a model for the interacting phenomena.
The label space in the joint models corresponds
to sequences of labels from the confusion sets of
the individual classifiers: {a − sing, a − pl, the −
sing, the − pl, 0 − sing, 0 − pl} and {sing −
sing, sing−pl, pl−sing, pl−pl} for article-NPhead
and subject-verb structures, respectively7. Invalid
7“sing” and “pl” refer to the grammatical number of noun
</bodyText>
<page confidence="0.992959">
795
</page>
<bodyText confidence="0.999866666666667">
structures, such as pl-sing are excluded via hard con-
straints (when we run joint inference) or via implicit
soft constraints (when we use joint learning).
</bodyText>
<subsectionHeader confidence="0.98078">
5.1 Joint Inference
</subsectionHeader>
<bodyText confidence="0.99935409375">
In the individual model approach, decisions are
made for each word independently, ignoring the in-
teractions among linguistic phenomena. The pur-
pose of joint inference is to include linguistic (i.e.
structural) knowledge, such as “plural nouns do not
take an indefinite article”, and “agreement consis-
tency between the verb and the subject that controls
it”. This knowledge should be useful for resolving
inconsistencies produced by individual classifiers.
The inference approach we develop in this paper
follows the one proposed by Roth and Yih (2004)
of training individual models and combining them
at decision time via joint inference. The advantage
of this method is that it allows us to build upon
any existing independently-learned models that pro-
vide a distribution over their outcome, and produce
a coherent global output that respects our declarative
constraints. We formulate our component inference
problems as integer linear program (ILP) instances
as in Roth and Yih (2004).
The inference takes as input the individual clas-
sifiers’ confidence scores for each prediction, along
with a list of constraints. The output is the optimal
solution that maximizes the linear sum of the confi-
dence scores, subject to the constraints that encode
the interactions. The joint model thus selects a hy-
pothesis that both obtains the best score according
to the individual models and satisfies the constraints
that reflect the interactions among the grammatical
phenomena at the level of linguistic structures, as
defined in Sec. 4.
Inference The joint inference is enforced at the
level of structures, and each structure corresponds
to one ILP instance. All structures consist of two or
three words: when an article-NPhead structure and
a subject-verb structure include the same noun, the
structure input to the ILP consists of an article-noun-
and verb agreement candidates. The candidates themselves are
the surface forms of specific words that realize these grammat-
ical properties. Note that a subject in subject-verb structures is
always third person, since all subjects in subject-verb structures
are common nouns; other subjects, including pronouns, are ex-
cluded. Thus the agreement distinction is singular vs. plural.
verb triple. We formulate the inference problem as
follows: Given a structure s that consists of n words,
let wi correspond to the ith word in the structure. Let
h denote a hypothesis from the hypothesis space H
for s, and score(wi, h, li) denote the score assigned
by the appropriate error-specific model to wi under
h for label l from the confusion set of word wi. We
denote by ew,l the Boolean variable that indicates
whether the prediction on word w is assigned the
value l (ew,l = 1) or not (ew,l = 0).
We assume that each independent classifier re-
turns a score that corresponds to the likelihood of
word wi under h being labeled li. The softmax func-
tion (Bishop, 1995) is used to convert raw activation
scores to conditional probabilities for the discrimi-
native article model. The NB scores are also normal-
ized and correspond to probabilities. Then the infer-
ence task is solved by maximizing the overall score
of a candidate assignment of labels l to words w (this
set of feasible assignments is denoted H here) sub-
ject to the constraints C for the structure s:
</bodyText>
<equation confidence="0.9995048">
h = arg max score(h) =
hEH
n
= arg max score(wi, h, li)ewi,li
hEH i=1
</equation>
<bodyText confidence="0.937649333333333">
subject to C(s)
Constraints In the 10, 11 linear programming for-
mulation described above, we can encode linguis-
tic constraints that reflect the interactions among the
linguistic phenomena. The inference enforces the
following structural and linguistic constraints:
</bodyText>
<listItem confidence="0.984735666666667">
1. The indefinite article “a” cannot refer to an NP headed by
a plural noun.
2. Subject and verb must agree in number.
</listItem>
<bodyText confidence="0.999516333333333">
In addition, we encode “legitimacy” constraints, that
make sure that each w is assigned a single label. All
constraints are encoded as hard constraints.
</bodyText>
<subsectionHeader confidence="0.998688">
5.2 Joint Learning
</subsectionHeader>
<bodyText confidence="0.999775666666667">
We now describe how we learn the subject-verb and
article-NPhead structures jointly. The joint model is
implemented as a NB classifier and is trained in the
same way as the independent models on the Google
corpus with word n-gram features. Unlike the inde-
pendent models, where the target corresponds to one
</bodyText>
<page confidence="0.99671">
796
</page>
<table confidence="0.99951">
System Adjacent structures All distances
F1 (Orig.) F1(Revised) F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14 31.20 42.14
Na¨ıveVerb 31.19 42.20 31.13 42.16
Na¨ıveNoun 31.03 41.87 30.91 41.70
This paper joint systems Joint Inference (adjacent) Joint Inference (all distances)
F1 (Orig.) F1(Revised) F1 (Orig.) F1 (Revised)
Subject-verb 31.90 42.94 31.97 42.86
Article-NPhead 31.63 42.48 31.79 42.59
Subject-verb + article-NPhead 32.35 43.16 32.51 43.19
</table>
<tableCaption confidence="0.988993571428571">
Table 7: Joint Inference Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Adjacent denotes a setting, where the joint inference is applied to structures with consecutive components (article-NPhead or
subject-verb). All distances denotes a setting, where the constraints are applied to all valid structures, as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Na¨ıveVerb and Na¨ıveNoun denote heuristics, where a
verb or subject are changed to ensure agreement. All improvements over the Illinois system are statistically significant (McNemar’s
test, p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.999927">
word, here the target corresponds to two words that
are part of the structure and the label space of the
model is modified accordingly. Since we use fea-
tures that can be computed from the small windows
in the Google corpus, the joint learning model han-
dles only adjacent structures (Sec. 4.1). Because the
target consists of two words and the Google corpus
contains counts for n-grams of length at most five,
the features are collected in the three word window
around the target.8
Unlike with the joint inference, here we do not
explicitly encode linguistic constraints. One reason
for this is that the NP head and subject predictions
are not 100% accurate, so input structures will have
noise. However, the joint model learns these con-
straints through the evidence seen in training.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.8690908125">
In this section, we describe our experimental setup
and evaluate the performance of the joint approach.
In the joint approach, the joint components pre-
sented in Sec. 5 handle the interacting structures de-
scribed in Sec. 4. The individual classifiers of the
Illinois system make predictions for the remaining
words. The research question addressed by the ex-
periments is the following: Given independently-
trained systems for different types of errors, can we
improve the performance by considering the phe-
8Also note that when the article is 0, the surface form of
the structure corresponds to the NP head alone; this does not
present a problem because in the NB model the context counts
are normalized with the prior counts.
nomena that interact jointly? To address this, we
report the results in the following settings:
</bodyText>
<listItem confidence="0.910324692307692">
1. Joint Inference: we compare the Illinois sys-
tem that is a collection of individually-trained mod-
els that are applied independently with a model
that uses joint inference encoded as declarative con-
straints in the ILP formulation and show that using
joint inference results in a strong performance gain.
2. Joint Learning: we compare the Illinois system
with a model that incorporates jointly-trained com-
ponents for the two linguistic structures that we de-
scribed in Sec. 4. We show that joint training pro-
duces an even stronger gain in performance com-
pared to the Illinois model.
2. Joint Learning and Inference: we apply joint in-
</listItem>
<bodyText confidence="0.982534444444444">
ference to the output of the joint learning system to
account for dependencies not covered by the joint
learning model.
We report F1 performance scored using the offi-
cial scorer from the shared task (Dahlmeier and Ng,
2012). The task reports two types of evaluation: on
the original gold data and on gold data with addi-
tional corrections. We refer to the results as Origi-
nal and Revised.
</bodyText>
<subsectionHeader confidence="0.993146">
6.1 Joint Inference Results
</subsectionHeader>
<bodyText confidence="0.999886714285714">
Table 7 shows the results of applying joint infer-
ence to the Illinois system. Both the article-NPhead
and the subject-verb constraints improve the perfor-
mance. The results for the joint inference are shown
in two settings, adjacent and all structures, so that
later we can compare joint inference with the joint
learning model that handles only adjacent structures.
</bodyText>
<page confidence="0.994176">
797
</page>
<table confidence="0.99988825">
Illinois system Illinois- NBArticle
F1(Orig.) F1(Revised) F1(Orig.) F1(Revised)
Illinois 31.20 42.14 31.71 41.38
This paper joint systems Joint Learning (adjacent) Joint Learning (adjacent)
F1(Orig.) F1(Revised) F1 (Orig.) F1 (Revised)
Subject-verb 32.64* 43.37* 33.09* 42.78*
Article-NPhead 33.89* 42.57* 33.16* 41.51
Subject-verb + article-NPhead 35.12* 43.73* 34.41* 42.76*
</table>
<tableCaption confidence="0.601677">
Table 8: Joint Learning Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.
Illinois-NBArticle denotes the Illinois system, where the discriminative article model is replaced with a NB classifier. Adjacent
denotes a setting, where the structure components are consecutive (article-NPhead or subject-verb), as described in Sec. 4.1.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system. In all cases, the candidates that are not part of the
structures are handled by the respective components of the Illinois system. Statistically significant improvements (McNemar’s test,
p &lt; 0.01) over the Illinois system are marked with an asterisk (*).
</tableCaption>
<bodyText confidence="0.999985909090909">
It is also interesting to note that the key improvement
comes from considering structures whose compo-
nents are adjacent. This is not surprising given that
the accuracy for subject and NP head identification
drops as the distance increases.
For subject-verb constraints, we also implement
a naive approach that looks for contradictions and
changes either the verb or the subject if they do not
satisfy the number agreement. These two heuris-
tics are denoted as Na¨&apos;veVerb and Na¨&apos;veNoun. The
heuristics differ from the joint inference in that they
enforce agreement by always changing either the
noun (Na¨&apos;veNoun) or the verb (Na¨&apos;veVerb), while
the joint inference does this using the scores pro-
duced by the independent models. In other words,
the key is the objective function, while the compo-
nents of the objective function are the same in the
heuristics and the joint inference. The results in Ta-
ble 7 show that simply enforcing agreement does not
work well and that the ILP formulation is indeed ef-
fective and improves over the independently-trained
models in all cases.
Recall that valid structures include only those
whose components can be identified in a reliable
way (Sec. 4.1). To evaluate the impact of that filter-
ing, we perform two experiments with subject-verb
structures (long-distance dependencies are more
common in those constructions than in the article-
NPhead structures): first, we apply joint inference
to all subject-verb structures. We obtain F1 scores of
31.61 and 42.28, on original and revised gold data,
respectively, which is significantly worse than the
results on subject-verb structures in Table 7 (31.97
and 42.86, respectively) and only slightly better than
the baseline performance of the Illinois system. Fur-
thermore, when we apply joint inference to those
structures which were excluded by filtering in Sec.
4.1, we find that the performance degrades com-
pared to the Illinois system (30.85 and 41.58). These
results demonstrate that the joint inference improve-
ments are due to structures whose components can
be identified with high accuracy and that it is essen-
tial to identify these structures; bad structures, on the
other hand, hurt performance.
</bodyText>
<subsectionHeader confidence="0.999748">
6.2 Joint Learning Results
</subsectionHeader>
<bodyText confidence="0.999985666666667">
Now we show experimental results of the joint learn-
ing (Table 8). Note that the joint learning component
considers only those structures where the words are
adjacent. Because the Illinois system presented in
Sec. 3 makes use of a discriminative article model,
while the joint model uses NB, we also show results,
where the article model is replaced by a NB classi-
fier trained on the Google corpus. In all cases, joint
learning demonstrates a strong performance gain.
</bodyText>
<subsectionHeader confidence="0.999967">
6.3 Joint Learning and Inference Results
</subsectionHeader>
<bodyText confidence="0.999537909090909">
Finally, we apply joint inference to the output of the
joint learning system in Sec. 6.2. Table 9 shows
the results of the Illinois model, the model that ap-
plies joint inference and joint learning separately,
and both. Even though the joint learning performs
better than the joint inference, the joint learning
covers only adjacent structures. Furthermore, joint
learning does not address overlapping structures of
triples that consist of article, subject, and verb (6%
of all structures). Joint inference allows us to ensure
consistent predictions in cases not addressed by the
</bodyText>
<page confidence="0.992993">
798
</page>
<table confidence="0.99521275">
Example Illinois system JL and JI
“Moreover, the increased technologies help people to overcome No change technology helps
different natural disasters.
“At that time,... there are surveillances in everyone’s heart and there are* surveillance* there is surveillance
criminals are more difficult to hide.”
“In such situation, individuals will lose their basic privacy.” such a* situations* such a situation
“In supermarket monitor is needed because we have to track No change monitors are
thieves.”
</table>
<tableCaption confidence="0.974255333333333">
Table 10: Examples of mistakes that are corrected by the joint model but not by the Illinois model. Illinois denotes the result
obtained by the top CoNLL-2013 shared task system from the University of Illinois. JL and JI stand for joint learning and joint
inference, respectively. Inconsistent predictions are starred.
</tableCaption>
<table confidence="0.9998968">
F1 (Orig.) F1 (Revised)
Illinois 31.20 42.14
Joint Inference 32.51 43.19
Joint Learning 35.12 43.73
Joint Learn. + Inf. 35.21 43.74
</table>
<tableCaption confidence="0.83689">
Table 9: Joint Learning and Inference. All results are on the
CoNLL-2013 test data using the original and revised gold anno-
tations. Results of the joint models that include the joint infer-
</tableCaption>
<bodyText confidence="0.984391642857143">
ence component are shown for structures of all distances. Illi-
nois denotes the result obtained by the top CoNLL-2013 shared
task system. All joint systems demonstrate a statistically sig-
nificant improvement over the Illinois system; joint learning
improvements are also statistically significant compared to the
joint inference results (McNemar’s test, p &lt; 0.01).
joint learning model. Indeed, we can get a small im-
provement by adding joint inference on top of the
joint learning on original annotations. Since the re-
vised corrections are based on the participants’ input
and are most likely biased towards system predic-
tions for corrections missed by the original annota-
tors (Ng et al., 2013), it is more difficult to show
improvement on revised data.
</bodyText>
<sectionHeader confidence="0.98596" genericHeader="discussions">
7 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.999919380952381">
In the previous section, we evaluated the proposed
joint inference and joint learning models that han-
dle interacting grammatical phenomena. We showed
that the joint models produce significant improve-
ments over the highest-scoring CoNLL-2013 shared
task system that consists of independently-trained
classifiers: the joint approaches increase the F1
score by 4 F1 points on the original gold data and
almost 2 points on the revised data (Table 9).
These results are interesting from the point of
view of developing a practical error correction sys-
tem. However, recall that the errors in the interact-
ing structures are only a subset of mistakes in the
CoNLL-2013 data set but the evaluation in Sec. 6 is
performed with respect to all of these errors. From
a scientific point of view, it is interesting to evalu-
ate the impact of the joint models more precisely by
considering the improvements on the relevant struc-
tures only. Table 11 shows how much the joint learn-
ing approach improves on the subset of relevant mis-
takes.
</bodyText>
<table confidence="0.9929498">
Structure Illinois Performance
(F1)
Joint Learning
Subject-verb 39.64 52.25
Article-NPhead 30.65 35.90
</table>
<tableCaption confidence="0.767848">
Table 11: Evaluation of the joint learning performance on
the subset of the data containing interacting errors. All re-
</tableCaption>
<bodyText confidence="0.994644761904762">
sults are on the CoNLL-2013 test data using the original anno-
tations. Illinois denotes the result obtained by the top CoNLL-
2013 shared task system. All improvements are statistically sig-
nificant over the Illinois system (McNemar’s test, p &lt; 0.01).
Error Analysis To better understand where the joint
models have an advantage over the independently-
trained classifiers, we analyze the output produced
by each of the approaches. In Table 10 we show
examples of mistakes that the model that uses joint
learning and inference is able to identify correctly,
along with the original predictions made by the Illi-
nois system.
Joint Inference vs. Joint Learning We wish
to stress that the joint approaches do not simply
perform better but also make coherent decisions
by disallowing illegitimate outputs. The joint in-
ference approach does this by enforcing linguis-
tic constraints on the output. The joint learning
model, while not explicitly encoding these con-
straints, learns them from the distribution of the
training data.
</bodyText>
<page confidence="0.994505">
799
</page>
<bodyText confidence="0.99988805">
Joint inference is a less expensive model, since it
uses the scores produced by the individual classifiers
and thus does not require additional training. Joint
learning, on the other hand, is superior to joint infer-
ence, since it is better at modeling interactions where
multiple errors occur simultaneously – it eliminates
the noisy context present when learning the inde-
pendent classifiers. Consider the first example from
Table 10, where both the noun and the agreement
classifiers receive noisy input: the verb “help” and
the noun “technologies” act as part of input features
for the noun and agreement classifiers, respectively.
The noisy features prevent both modules from iden-
tifying the two errors.
Finally, an important distinction of the joint learn-
ing method is that it considers all possible output se-
quences in training, and thus it is able to better iden-
tify errors that require multiple changes, such as the
last example in Table 10, where the Illinois system
proposes no changes.
</bodyText>
<subsectionHeader confidence="0.713419">
7.1 Error Correction: Challenges
</subsectionHeader>
<bodyText confidence="0.999983069767442">
We finalize our discussion with a few comments on
the challenges of the error correction task.
Task Difficulty As shown in Table 1 in Sec. 2, only
a small percentage of words have mistakes, while
over 90% (about 98% in training) are used correctly.
The low error rates are the key reason the error cor-
rection task is so difficult: it is quite challenging for
a system to improve over a writer that already per-
forms at the level of over 90%. Indeed, very few
NLP tasks already have systems that perform at that
level, even when the data is not as noisy as the ESL
data.
Evaluation Metrics In the CoNLL-2013 competi-
tion, as well as the competitions alluded to earlier,
systems were compared on F1 performance, and,
consequently, this is the metric we optimize in this
paper. Practical error correction systems, however,
should be tuned to minimize recall to guarantee that
the overall quality of the text does not go down. In-
deed, the error sparsity makes it very challenging to
identify mistakes accurately, and no system in the
shared task achieves a precision over 50%. How-
ever, once the precision drops below 50%, the sys-
tem introduces more mistakes than it identifies.
Clearly, optimizing the F1 measure does not en-
sure that the quality of the text improves as a re-
sult of running the system. Thus, it can be argued
that the F1 measure is not the right measure for er-
ror correction. A different evaluation metric based
on the accuracy of the data before and after running
the system was proposed in Rozovskaya and Roth
(2010c). When optimizing for this metric, the noun
module, for instance, at recall point 20%, achieves
a precision of 63.93%. This translates into accuracy
of 94.46%, while the baseline on noun errors in the
test data (i.e. the accuracy of the data before running
the system) is 94.0% (Table 1). This means that the
system improves the quality of the data.
Annotation Lastly, we believe that it is important
to provide alternative corrections, as the agreement
on what constitutes a mistake even among native
English speakers can be quite low (Madnani et al.,
2011).
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999991">
This work presented the first successful study that
jointly corrects grammatical mistakes. We ad-
dressed two pairs of interacting phenomena and
showed that it is possible to reliably identify their
components, thereby facilitating the joint approach.
We described two joint methods: a joint in-
ference approach implemented via ILP and a
joint learning model. The joint inference en-
forces constraints using the scores produced by the
independently-trained models. The joint learning
model learns the interacting phenomena as struc-
tures. The joint methods produce a significant im-
provement over a state-of-the-art system that com-
bines independently-trained models and, impor-
tantly, produce linguistically legitimate output.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999503538461539">
The authors thank Peter Chew, Jennifer Cole, Mark Sam-
mons, and the anonymous reviewers for their helpful
feedback. The authors thank Josh Gioja for the code
that performs phonetic disambiguation of the indefinite
article. This material is based on research sponsored
by DARPA under agreement number FA8750-13-2-0008.
The U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and con-
clusions contained herein are those of the authors and
should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or im-
plied, of DARPA or the U.S. Government.
</bodyText>
<page confidence="0.992721">
800
</page>
<sectionHeader confidence="0.996367" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940462264151">
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions.
Oxford University Press.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia, PA.
C. Brockett, D. B. William, and M. Gamon. 2006.
Correcting ESL errors using phrasal SMT techniques.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 249–256, Sydney, Australia, July. Association
for Computational Linguistics.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
J. Clarke and M. Lapata. 2007. Modelling compression
with discourse constraints. In Proceedings of the 2007
Joint Conference of EMNLP-CoNLL.
D. Dahlmeier and H.T Ng. 2012. A beam-search de-
coder for grammatical error correction. In EMNLP-
CoNLL, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL HLT
2013 Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, Atlanta, Georgia,
June. Association for Computational Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A re-
port on the preposition and determiner error correction
shared task. In Proc. of the NAACL HLT 2012 Seventh
Workshop on Innovative Use of NLP for Building Edu-
cational Applications, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
R. De Felice and S. Pulman. 2008. A classifier-based ap-
proach to preposition and determiner error correction
in L2 English. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 169–176, Manchester, UK, August.
Y. Freund and R. E. Schapire. 1996. Experiments with
a new boosting algorithm. In Proc. 13th International
Conference on Machine Learning.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings of
IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing. In NAACL, pages 163–171,
Los Angeles, California, June.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction. Ma-
chine Learning.
N. Han, M. Chodorow, and C. Leacock. 2006. Detecting
errors in English article usage by non-native speakers.
Journal of Natural Language Engineering, 12(2):115–
129.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners’ English spoken data. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
145–148, Sapporo, Japan, July.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174–182, Columbus, Ohio,
June. Association for Computational Linguistics.
N. Madnani, M. Chodorow, J. Tetreault, and A. Ro-
zovskaya. 2011. They can help: Using crowdsourcing
to improve the evaluation of grammatical error detec-
tion systems. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 508–513, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
M. Marneffe, B. MacCartney, and Ch. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In LREC.
A. Martins, Noah N. Smith, M. Figueiredo, and P. Aguiar.
2011. Dual decomposition with many overlapping
components. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 238–249, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task on
grammatical error correction. In Proc. of the Sev-
enteenth Conference on Computational Natural Lan-
guage Learning. Association for Computational Lin-
guistics.
A. Park and R. Levy. 2011. Automated whole sentence
grammar correction using a noisy channel model. In
ACL, Portland, Oregon, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, CoNLL.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
</reference>
<page confidence="0.97839">
801
</page>
<reference confidence="0.999427885714286">
NAACL Workshop on Innovative Use of NLP for Build-
ing Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training paradigms
for correcting errors in grammar and usage. In
NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the HOO 2012 shared task on error cor-
rection.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system in
the CoNLL-2013 shared task. In CoNLL Shared Task.
C. Sutton and A. McCallum. 2007. Piecewise pseudo-
likelihood for efficient training of conditional random
fields. In Zoubin Ghahramani, editor, ICML.
J. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
865–872, Manchester, UK, August.
Y. Wu and H.T. Ng. 2013. Grammatical error correction
using integer linear programming. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
1456–1465, Sofia, Bulgaria, August. Association for
Computational Linguistics.
</reference>
<page confidence="0.998218">
802
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.825397">
<title confidence="0.998999">Joint Learning and Inference for Grammatical Error Correction</title>
<author confidence="0.997292">Alla Rozovskaya</author>
<author confidence="0.997292">Dan</author>
<affiliation confidence="0.998618">Cognitive Computation University of Illinois at</affiliation>
<address confidence="0.929497">201 N. Goodwin Urbana, IL</address>
<abstract confidence="0.998044">State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors. Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words. In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning. We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce. Furthermore, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="23683" citStr="Bishop, 1995" startWordPosition="3770" endWordPosition="3771">iven a structure s that consists of n words, let wi correspond to the ith word in the structure. Let h denote a hypothesis from the hypothesis space H for s, and score(wi, h, li) denote the score assigned by the appropriate error-specific model to wi under h for label l from the confusion set of word wi. We denote by ew,l the Boolean variable that indicates whether the prediction on word w is assigned the value l (ew,l = 1) or not (ew,l = 0). We assume that each independent classifier returns a score that corresponds to the likelihood of word wi under h being labeled li. The softmax function (Bishop, 1995) is used to convert raw activation scores to conditional probabilities for the discriminative article model. The NB scores are also normalized and correspond to probabilities. Then the inference task is solved by maximizing the overall score of a candidate assignment of labels l to words w (this set of feasible assignments is denoted H here) subject to the constraints C for the structure s: h = arg max score(h) = hEH n = arg max score(wi, h, li)ewi,li hEH i=1 subject to C(s) Constraints In the 10, 11 linear programming formulation described above, we can encode linguistic constraints that refl</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>C. Bishop. 1995. Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11912" citStr="Brants and Franz, 2006" startWordPosition="1868" endWordPosition="1871">pe Confusion set Noun {factor, factors} Verb Agr. {contribute, contributes} Verb Form {included, including, includes, include} Table 2: Confusion sets for noun number, agreement, and form classifiers. The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 20 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. 793 Example Predictions made by the Illinois system “They believe that such situation must be avoided.” such situation —* such a situations “Nevertheless , electric cars is still r</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>D B William</author>
<author>M Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="9056" citStr="Brockett et al. (2006)" startWordPosition="1421" endWordPosition="1424">Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence</context>
</contexts>
<marker>Brockett, William, Gamon, 2006</marker>
<rawString>C. Brockett, D. B. William, and M. Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 249–256, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling up context sensitive text correction.</title>
<date>2001</date>
<booktitle>In IAAI.</booktitle>
<contexts>
<context position="10450" citStr="Carlson et al., 2001" startWordPosition="1643" endWordPosition="1646"> 3 The University of Illinois System Below, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five machinelearning independently-trained classifiers that follow the popular approach to ESL error correction borrowed from the context-sensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined that specifies a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model selects the most likely candidate from the confusion set. The confusion set for prepositions includes the top 12 most frequent English prepositions. The article confusion set is as follows: {a,the,0}2. The confusion sets for noun, agreement, and form mo</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up context sensitive text correction. In IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4561" citStr="Clarke and Lapata, 2007" startWordPosition="690" endWordPosition="693">nnotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), and (2) a model that join</context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>J. Clarke and M. Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>A beam-search decoder for grammatical error correction.</title>
<date>2012</date>
<booktitle>In EMNLPCoNLL, Jeju Island, Korea,</booktitle>
<contexts>
<context position="2513" citStr="Dahlmeier and Ng, 2012" startWordPosition="371" endWordPosition="374">tatistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts with the noun number error: a correction system</context>
<context position="9422" citStr="Dahlmeier and Ng (2012)" startWordPosition="1476" endWordPosition="1479">to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University of Illinois System Below, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI)</context>
<context position="28735" citStr="Dahlmeier and Ng, 2012" startWordPosition="4585" endWordPosition="4588">nd show that using joint inference results in a strong performance gain. 2. Joint Learning: we compare the Illinois system with a model that incorporates jointly-trained components for the two linguistic structures that we described in Sec. 4. We show that joint training produces an even stronger gain in performance compared to the Illinois model. 2. Joint Learning and Inference: we apply joint inference to the output of the joint learning system to account for dependencies not covered by the joint learning model. We report F1 performance scored using the official scorer from the shared task (Dahlmeier and Ng, 2012). The task reports two types of evaluation: on the original gold data and on gold data with additional corrections. We refer to the results as Original and Revised. 6.1 Joint Inference Results Table 7 shows the results of applying joint inference to the Illinois system. Both the article-NPhead and the subject-verb constraints improve the performance. The results for the joint inference are shown in two settings, adjacent and all structures, so that later we can compare joint inference with the joint learning model that handles only adjacent structures. 797 Illinois system Illinois- NBArticle F</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>D. Dahlmeier and H.T Ng. 2012. A beam-search decoder for grammatical error correction. In EMNLPCoNLL, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
<author>S M Wu</author>
</authors>
<title>Building a large annotated corpus of learner english: The nus corpus of learner english.</title>
<date>2013</date>
<booktitle>In Proc. of the NAACL HLT 2013 Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="6543" citStr="Dahlmeier et al., 2013" startWordPosition="1004" endWordPosition="1007">. 2 Task Description and Motivation To illustrate the utility of jointly addressing interacting grammatical phenomena, we consider the corpus of the CoNLL-2013 shared task on grammatical error correction (Ng et al., 2013), which we found to be particularly well-suited for addressing interactions between grammatical phenomena. The task focuses on the following five common mistakes made by ESL writers: article, preposition, noun number, subject-verb agreement, and verb form, and we address two interactions: article-NPhead and subjectverb. The training data for the task is from the NUCLE corpus (Dahlmeier et al., 2013), an error-tagged collection of essays written by non-native learners of English. The test data is an additional set of essays by learners from the same linguistic background. The training and the test data contain 1.2M and 29K words, respectively. Although the corpus contains errors of other types, the task focuses on five types of errors. Table 1 shows the number of mistakes1 of each type and the error rates, i.e. the percentage of erroneous words by error type. Error Number of errors and error rate Training Test Article 6658 (2.4%) 690 (10.0%) Prep. 2404 (2.0%) 311 (10.7%) Noun 3779 (1.6%) </context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proc. of the NAACL HLT 2013 Eighth Workshop on Innovative Use of NLP for Building Educational Applications, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="1557" citStr="Dale and Kilgarriff, 2011" startWordPosition="222" endWordPosition="226">e, because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Nowadays *phone/phones *has/have many functionalities, *included/including *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research h</context>
<context position="8185" citStr="Dale and Kilgarriff, 2011" startWordPosition="1278" endWordPosition="1281">We note that the CoNLL-2013 data set is the first annotated collection that makes a study like ours feasible. The presence of a common test set that 1System performance in the shared task is evaluated on data with and without additional revisions added based on the input from participants. The number of mistakes in the revised test data is slightly higher. 792 contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast</context>
<context position="9553" citStr="Dale and Kilgarriff, 2011" startWordPosition="1498" endWordPosition="1501">ts. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University of Illinois System Below, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>R. Dale and A. Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>I Anisimoff</author>
<author>G Narroway</author>
</authors>
<title>A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="1587" citStr="Dale et al., 2012" startWordPosition="228" endWordPosition="231">nsiders interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss. Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction. 1 Introduction There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners. In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). Nowadays *phone/phones *has/have many functionalities, *included/including *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mista</context>
<context position="8329" citStr="Dale et al., 2012" startWordPosition="1303" endWordPosition="1306">stem performance in the shared task is evaluated on data with and without additional revisions added based on the input from participants. The number of mistakes in the revised test data is slightly higher. 792 contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>R. Dale, I. Anisimoff, and G. Narroway. 2012. A report on the preposition and determiner error correction shared task. In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>A classifier-based approach to preposition and determiner error correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>169--176</pages>
<location>Manchester, UK,</location>
<marker>De Felice, Pulman, 2008</marker>
<rawString>R. De Felice and S. Pulman. 2008. A classifier-based approach to preposition and determiner error correction in L2 English. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 169–176, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proc. 13th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="11706" citStr="Freund and Schapire, 1996" startWordPosition="1833" endWordPosition="1836">d and include its morphological variants (Table 2). “Hence, the environmental *factor/factors also *contributes/contribute to various difficulties, *included/including problems in nuclear technology.” Error type Confusion set Noun {factor, factors} Verb Agr. {contribute, contributes} Verb Form {included, including, includes, include} Table 2: Confusion sets for noun number, agreement, and form classifiers. The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 20 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflate</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting algorithm. In Proc. 13th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="2306" citStr="Gamon et al., 2008" startWordPosition="341" endWordPosition="344">ties, *included/including *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentenc</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan, D. Belenko, and L. Vanderwende. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing.</title>
<date>2010</date>
<booktitle>In NAACL,</booktitle>
<pages>163--171</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="2349" citStr="Gamon, 2010" startWordPosition="349" endWordPosition="350">-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing. In NAACL, pages 163–171, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="10427" citStr="Golding and Roth, 1999" startWordPosition="1639" endWordPosition="1642">re simultaneous changes. 3 The University of Illinois System Below, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five machinelearning independently-trained classifiers that follow the popular approach to ESL error correction borrowed from the context-sensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined that specifies a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model selects the most likely candidate from the confusion set. The confusion set for prepositions includes the top 12 most frequent English prepositions. The article confusion set is as follows: {a,the,0}2. The confusion sets for noun,</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>129</pages>
<contexts>
<context position="2261" citStr="Han et al., 2006" startWordPosition="333" endWordPosition="336">ys *phone/phones *has/have many functionalities, *included/including *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer m</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Journal of Natural Language Engineering, 12(2):115– 129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<title>Automatic error detection in the Japanese learners’ English spoken data.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--148</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2243" citStr="Izumi et al., 2003" startWordPosition="329" endWordPosition="332">t al., 2013). Nowadays *phone/phones *has/have many functionalities, *included/including *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. B</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 145–148, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Correcting misuse of verb forms.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>174--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2468" citStr="Lee and Seneff, 2008" startWordPosition="363" endWordPosition="366">ction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts wi</context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>J. Lee and S. Seneff. 2008. Correcting misuse of verb forms. In ACL, pages 174–182, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>A Rozovskaya</author>
</authors>
<title>They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>508--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="40991" citStr="Madnani et al., 2011" startWordPosition="6549" endWordPosition="6552"> running the system was proposed in Rozovskaya and Roth (2010c). When optimizing for this metric, the noun module, for instance, at recall point 20%, achieves a precision of 63.93%. This translates into accuracy of 94.46%, while the baseline on noun errors in the test data (i.e. the accuracy of the data before running the system) is 94.0% (Table 1). This means that the system improves the quality of the data. Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (Madnani et al., 2011). 8 Conclusion This work presented the first successful study that jointly corrects grammatical mistakes. We addressed two pairs of interacting phenomena and showed that it is possible to reliably identify their components, thereby facilitating the joint approach. We described two joint methods: a joint inference approach implemented via ILP and a joint learning model. The joint inference enforces constraints using the scores produced by the independently-trained models. The joint learning model learns the interacting phenomena as structures. The joint methods produce a significant improvement</context>
</contexts>
<marker>Madnani, Chodorow, Tetreault, Rozovskaya, 2011</marker>
<rawString>N. Madnani, M. Chodorow, J. Tetreault, and A. Rozovskaya. 2011. They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 508–513, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>Manning, 2006</marker>
<rawString>M. Marneffe, B. MacCartney, and Ch. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martins</author>
<author>Noah N Smith</author>
<author>M Figueiredo</author>
<author>P Aguiar</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>238--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="4536" citStr="Martins et al., 2011" startWordPosition="686" endWordPosition="689">ve data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), </context>
</contexts>
<marker>Martins, Smith, Figueiredo, Aguiar, 2011</marker>
<rawString>A. Martins, Noah N. Smith, M. Figueiredo, and P. Aguiar. 2011. Dual decomposition with many overlapping components. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 238–249, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<title>The conll-2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proc. of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<marker>Hadiwinoto, Tetreault, 2013</marker>
<rawString>H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and J. Tetreault. 2013. The conll-2013 shared task on grammatical error correction. In Proc. of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Park</author>
<author>R Levy</author>
</authors>
<title>Automated whole sentence grammar correction using a noisy channel model.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="9247" citStr="Park and Levy (2011)" startWordPosition="1451" endWordPosition="1454">not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to correct noun number errors on mass nouns and article usage but their application was restricted to a small set of constructions. Park and Levy (2011) proposed a language-modeling approach to whole sentence error correction but their model is not competitive with individually trained models. Finally, Dahlmeier and Ng (2012) proposed a decoder model, focusing on four types of errors in the data set of the HOO-2011 competition (Dale and Kilgarriff, 2011). The decoder optimized the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University </context>
</contexts>
<marker>Park, Levy, 2011</marker>
<rawString>A. Park and R. Levy. 2011. Automated whole sentence grammar correction using a noisy channel model. In ACL, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="13023" citStr="Punyakanok and Roth, 2001" startWordPosition="2039" endWordPosition="2042"> that such situation must be avoided.” such situation —* such a situations “Nevertheless , electric cars is still regarded as a great trial innovation.” cars is —* car are “Every students have appointments with the head of the department.” No change Table 3: Examples of predictions of the Illinois system that combines independently-trained models. The words that are selected as input to classifiers are called candidates. Article and preposition candidates are identified with a closed list of words; noun-phrase-initial contexts for the article classifier are determined using a shallow parser3 (Punyakanok and Roth, 2001). Candidates for the noun, agreement, and form classifiers are identified with a partof-speech tagger4, e.g. noun candidates are words that are tagged as NN or NNS. Table 4 shows the total number of candidates for each classifier. Classifier Art. P N Agr. F Train 254K 103K 240K 75K 175K Test 6K 2.5K 2.6K 2.4K 4.8K Table 4: Number of candidate words by classifier type in training and test data. 4 Interacting Mistakes The approach of addressing each type of mistake individually is problematic when multiple phenomena interact. Consider the examples in Table 3 and the predictions made by the Illin</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="4514" citStr="Punyakanok et al., 2008" startWordPosition="682" endWordPosition="685">ry recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng</booktitle>
<editor>and Ellen Riloff, editors, CoNLL.</editor>
<contexts>
<context position="4489" citStr="Roth and Yih, 2004" startWordPosition="678" endWordPosition="681">: (1) Data: until very recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear progra</context>
<context position="8499" citStr="Roth and Yih (2004)" startWordPosition="1330" endWordPosition="1333">revised test data is slightly higher. 792 contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett et al. (2006) applied machinetranslation techniques to c</context>
<context position="21340" citStr="Roth and Yih (2004)" startWordPosition="3387" endWordPosition="3390"> soft constraints (when we use joint learning). 5.1 Joint Inference In the individual model approach, decisions are made for each word independently, ignoring the interactions among linguistic phenomena. The purpose of joint inference is to include linguistic (i.e. structural) knowledge, such as “plural nouns do not take an indefinite article”, and “agreement consistency between the verb and the subject that controls it”. This knowledge should be useful for resolving inconsistencies produced by individual classifiers. The inference approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at decision time via joint inference. The advantage of this method is that it allows us to build upon any existing independently-learned models that provide a distribution over their outcome, and produce a coherent global output that respects our declarative constraints. We formulate our component inference problems as integer linear program (ILP) instances as in Roth and Yih (2004). The inference takes as input the individual classifiers’ confidence scores for each prediction, along with a list of constraints. The output is the optimal solutio</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Hwee Tou Ng and Ellen Riloff, editors, CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Annotating ESL errors: Challenges and rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="2376" citStr="Rozovskaya and Roth, 2010" startWordPosition="351" endWordPosition="354"> Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi</context>
<context position="40431" citStr="Rozovskaya and Roth (2010" startWordPosition="6456" endWordPosition="6459"> does not go down. Indeed, the error sparsity makes it very challenging to identify mistakes accurately, and no system in the shared task achieves a precision over 50%. However, once the precision drops below 50%, the system introduces more mistakes than it identifies. Clearly, optimizing the F1 measure does not ensure that the quality of the text improves as a result of running the system. Thus, it can be argued that the F1 measure is not the right measure for error correction. A different evaluation metric based on the accuracy of the data before and after running the system was proposed in Rozovskaya and Roth (2010c). When optimizing for this metric, the noun module, for instance, at recall point 20%, achieves a precision of 63.93%. This translates into accuracy of 94.46%, while the baseline on noun errors in the test data (i.e. the accuracy of the data before running the system) is 94.0% (Table 1). This means that the system improves the quality of the data. Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (Madnani et al., 2011). 8 Conclusion This work presented the f</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010a. Annotating ESL errors: Challenges and rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2376" citStr="Rozovskaya and Roth, 2010" startWordPosition="351" endWordPosition="354"> Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi</context>
<context position="40431" citStr="Rozovskaya and Roth (2010" startWordPosition="6456" endWordPosition="6459"> does not go down. Indeed, the error sparsity makes it very challenging to identify mistakes accurately, and no system in the shared task achieves a precision over 50%. However, once the precision drops below 50%, the system introduces more mistakes than it identifies. Clearly, optimizing the F1 measure does not ensure that the quality of the text improves as a result of running the system. Thus, it can be argued that the F1 measure is not the right measure for error correction. A different evaluation metric based on the accuracy of the data before and after running the system was proposed in Rozovskaya and Roth (2010c). When optimizing for this metric, the noun module, for instance, at recall point 20%, achieves a precision of 63.93%. This translates into accuracy of 94.46%, while the baseline on noun errors in the test data (i.e. the accuracy of the data before running the system) is 94.0% (Table 1). This means that the system improves the quality of the data. Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (Madnani et al., 2011). 8 Conclusion This work presented the f</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010b. Generating confusion sets for context-sensitive error correction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2376" citStr="Rozovskaya and Roth, 2010" startWordPosition="351" endWordPosition="354"> Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classi</context>
<context position="40431" citStr="Rozovskaya and Roth (2010" startWordPosition="6456" endWordPosition="6459"> does not go down. Indeed, the error sparsity makes it very challenging to identify mistakes accurately, and no system in the shared task achieves a precision over 50%. However, once the precision drops below 50%, the system introduces more mistakes than it identifies. Clearly, optimizing the F1 measure does not ensure that the quality of the text improves as a result of running the system. Thus, it can be argued that the F1 measure is not the right measure for error correction. A different evaluation metric based on the accuracy of the data before and after running the system was proposed in Rozovskaya and Roth (2010c). When optimizing for this metric, the noun module, for instance, at recall point 20%, achieves a precision of 63.93%. This translates into accuracy of 94.46%, while the baseline on noun errors in the test data (i.e. the accuracy of the data before running the system) is 94.0% (Table 1). This means that the system improves the quality of the data. Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (Madnani et al., 2011). 8 Conclusion This work presented the f</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010c. Training paradigms for correcting errors in grammar and usage. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12177" citStr="Rozovskaya and Roth, 2011" startWordPosition="1909" endWordPosition="1912">n the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 20 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. 793 Example Predictions made by the Illinois system “They believe that such situation must be avoided.” such situation —* such a situations “Nevertheless , electric cars is still regarded as a great trial innovation.” cars is —* car are “Every students have appointments with the head of the department.” No change Table 3: Examples of predictions of the Illinois system that combines independently-trained models. The words that are selected as</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>M Sammons</author>
<author>J Gioja</author>
<author>D Roth</author>
</authors>
<date>2011</date>
<institution>University of Illinois</institution>
<note>system in HOO text correction shared task.</note>
<contexts>
<context position="2613" citStr="Rozovskaya et al., 2011" startWordPosition="385" endWordPosition="388">several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers. In the example shown in Figure 1, the agreement error on the verb “have” interacts with the noun number error: a correction system that takes into account the context may infer, because of the word “phone”, that the verb number is</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth. 2011. University of Illinois system in HOO text correction shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<title>shared task on error correction.</title>
<date>2012</date>
<booktitle>The UI system in the HOO</booktitle>
<contexts>
<context position="11620" citStr="Rozovskaya et al. (2012)" startWordPosition="1820" endWordPosition="1823">2. The confusion sets for noun, agreement, and form modules depend on the target word and include its morphological variants (Table 2). “Hence, the environmental *factor/factors also *contributes/contribute to various difficulties, *included/including problems in nuclear technology.” Error type Confusion set Noun {factor, factors} Verb Agr. {contribute, contributes} Verb Form {included, including, includes, include} Table 2: Confusion sets for noun number, agreement, and form classifiers. The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The other models are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word n-gram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). 20 denotes noun-phrase-initial contexts w</context>
</contexts>
<marker>Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>A. Rozovskaya, M. Sammons, and D. Roth. 2012. The UI system in the HOO 2012 shared task on error correction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>K-W Chang</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<date>2013</date>
<booktitle>The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</booktitle>
<contexts>
<context position="10198" citStr="Rozovskaya et al. (2013)" startWordPosition="1607" endWordPosition="1611">ed the sequence in which individual classifiers were to be applied to the sentence. However, because the decoder still corrected mistakes in a pipeline fashion, one at a time, it is unlikely that it could deal with cases that require simultaneous changes. 3 The University of Illinois System Below, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five machinelearning independently-trained classifiers that follow the popular approach to ESL error correction borrowed from the context-sensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined that specifies a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model s</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>A. Rozovskaya, K.-W. Chang, M. Sammons, and D. Roth. 2013. The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Piecewise pseudolikelihood for efficient training of conditional random fields.</title>
<date>2007</date>
<editor>In Zoubin Ghahramani, editor, ICML.</editor>
<contexts>
<context position="4589" citStr="Sutton and McCallum, 2007" startWordPosition="694" endWordPosition="697">y errors of interacting phenomena (see Sec. 2). (2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement). The perception has been that this cannot be done reliably (Sec. 4). (3) Technical: The NLP community has started to better understand joint learning and inference and apply it to various phenomena (Roth and Yih, 2004; Punyakanok et al., 2008; Martins et al., 2011; Clarke and Lapata, 2007; Sutton and McCallum, 2007) (Sec. 5). In this paper we present, for the first time, a successful approach to jointly resolving grammatical errors. Specifically: • We identify two pairs of interacting phenomena, subject-verb and article-NPhead agreements; we show how to reliably identify these pairs in noisy ESL data, thereby facilitating the joint correction of these phenomena. • We propose two joint approaches: (1) a joint inference approach implemented on top of individually learned models using an integer linear programming formulation (ILP, (Roth and Yih, 2004)), and (2) a model that jointly learns each pair of thes</context>
</contexts>
<marker>Sutton, McCallum, 2007</marker>
<rawString>C. Sutton and A. McCallum. 2007. Piecewise pseudolikelihood for efficient training of conditional random fields. In Zoubin Ghahramani, editor, ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>865--872</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2336" citStr="Tetreault and Chodorow, 2008" startWordPosition="345" endWordPosition="348">uding *0/a camera and *Al/a Wi-Fi receiver. Figure 1: Examples of representative ESL errors. Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake. Figure 1 illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form. A significant proportion of research has focused on correcting mistakes in article and preposition usage (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008; Gamon, 2010; Rozovskaya and Roth, 2010b). Several studies also consider verb-related and noun-related errors (Lee and Seneff, 2008; Gamon et al., 2008; Dahlmeier and Ng, 2012). The predictions made by individual models are then applied independently (Rozovskaya et al., 2011) or pipelined (Dahlmeier and Ng, 2012). The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena. But an ESL writer may make multiple mistakes in a single sentence and these result in misleadi</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction using integer linear programming.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1456--1465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8443" citStr="Wu and Ng (2013)" startWordPosition="1321" endWordPosition="1324">put from participants. The number of mistakes in the revised test data is slightly higher. 792 contains a good number of interacting errors – article, noun, and verb agreement mistakes – makes the data set well-suited for studying which approach works best for addressing interacting phenomena. The HOO-2011 shared task collection (Dale and Kilgarriff, 2011) contains a very small number of noun and agreement errors (41 and 11 in test, respectively), while the HOO-2012 competition (Dale et al., 2012) only addresses article and preposition mistakes. Indeed, in parallel to the work presented here, Wu and Ng (2013) attempted the ILP-based approach of Roth and Yih (2004) in this domain. They were not able to show any improvement, for two reasons. First, the HOO-2011 data set which they used does not contain a good number of errors in interacting structures. Second, and most importantly, they applied constraints in an indiscriminate manner. In contrast, we show how to identify the interacting structures’ components in a reliable way, and this plays a key role in the joint modeling improvements. Lack of data hindered other earlier efforts for error correction beyond individual language phenomena. Brockett </context>
</contexts>
<marker>Wu, Ng, 2013</marker>
<rawString>Y. Wu and H.T. Ng. 2013. Grammatical error correction using integer linear programming. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1456–1465, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
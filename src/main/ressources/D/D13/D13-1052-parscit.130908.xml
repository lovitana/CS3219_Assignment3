<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.8878755">
Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and
Forest-to-String Decoding.
</title>
<author confidence="0.949822">
Martin ˇCmejrektt
</author>
<affiliation confidence="0.970389">
†IBM Prague Research Lab
</affiliation>
<address confidence="0.8623455">
V Parku 2294/4
Prague, Czech Republic, 148 00
</address>
<email confidence="0.987767">
martin.cmejrek@us.ibm.com
</email>
<note confidence="0.3609515">
Haitao Mit and Bowen Zhou$
$IBM T. J. Watson Research Center
</note>
<address confidence="0.7198205">
1101 Kitchawan Rd
Yorktown Heights, NY 10598
</address>
<email confidence="0.997288">
{hmi,zhou}@us.ibm.com
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999339857142857">
Machine translation benefits from system
combination. We propose flexible interaction
of hypergraphs as a novel technique combin-
ing different translation models within one de-
coder. We introduce features controlling the
interactions between the two systems and ex-
plore three interaction schemes of hiero and
forest-to-string models—specification, gener-
alization, and interchange. The experiments
are carried out on large training data with
strong baselines utilizing rich sets of dense
and sparse features. All three schemes signif-
icantly improve results of any single system
on four testsets. We find that specification—a
more constrained scheme that almost entirely
uses forest-to-string rules, but optionally uses
hiero rules for shorter spans—comes out as
the strongest, yielding improvement up to 0.9
(T -B )/2 points. We also provide a de-
tailed experimental and qualitative analysis of
the results.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995581977777778">
Recent years have witnessed the success of var-
ious statistical machine translation (SMT) mod-
els using different levels of linguistic knowledge–
phrase (Koehn et al., 2003), hiero (Chiang, 2005),
and syntax-based (Liu et al., 2006; Galley et al.,
2006). System combination became a promising
way of building up synergy from different SMT sys-
tems and their specific merits.
Numerous efforts that have been proposed in this
field recently can be broadly divided into two cat-
.M. Cˇ and H. M. contributed equally to this work.
egories: Offline system combination (Rosti et al.,
2007; He et al., 2008; Watanabe and Sumita, 2011;
Denero et al., 2010) aims at producing consensus
translations from the outputs of multiple individ-
ual systems. Those outputs usually contain k-best
lists of translations, which only explore a small por-
tion of the entire search space of each system. This
issue is well addressed in joint decoding (Liu et
al., 2009), or online system combination, showing
comparable improvements to the offline combina-
tion methods. Rather than finding consensus trans-
lations from the outputs of individual systems, joint
decoding works with different grammars at the de-
coding time. Although limited to individual systems
sharing the same search paradigm (e.g. left-to-right
or bottom-up), joint decoding offers many poten-
tial advatages: search through a larger space, bet-
ter efficiency, features designed once for all subsys-
tems, potential cross-system features, online sharing
of partial hypotheses, and many others.
Different approaches have different strengths in
general–hiero rules are believed to provide reliable
lexical coverage, while tree-to-string rules are good
at non-local reorderings. Different contexts present
different challenges–noun phrases usually follow
the adjacency principle, while verb phrases require
more challenging reorderings. In this work, we study
different schemes of interaction between translation
models, reflecting their specific strengths at differ-
ent (syntactic) contexts. We make five new contribu-
tions:
First, we propose a framework for joint decod-
ing by means of flexible combination of trans-
lation hypergraphs, allowing for detailed con-
</bodyText>
<page confidence="0.976982">
545
</page>
<note confidence="0.733283">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 545–555,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999657074074074">
trol of interactions between the different sys-
tems using soft constraints (Section 3).
Second, we study three interaction schemes–
special cases of joint decoding: generalization,
specification, and interchange (Section 3.3).
Third, instead of using a tree-to-string system,
we use a much stronger forest-to-string sys-
tem with fuzzy match of nonterminal categories
(Section 2.1).
Fourth, we train strong systems on a large-
scale data set, and test all methods on four test
sets. Experimental results (Section 6) show that
our new approach brings improvement of up to
0.9 points in terms of (T − B )/2 over the
best single system.
Fifth, we conduct a comprehensive experimen-
tal analysis, and find that joint decoding actu-
ally prefers tree-to-string rules in both shorter
and longer spans. (Section 6.3).
The paper is organized as follows: We briefly re-
view the individual models in Section 2, describe
the method of joint decoding using three alternative
interaction schemes in Section 3, describe the fea-
tures controlling the interactions and fuzzy match in
Section 4, review the related work in Section 5, and
finally, describe our experiments and give detailed
discussion of the results in Section 6.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="introduction">
2 Individual Models
</sectionHeader>
<bodyText confidence="0.922277727272727">
Our individual models are two state-of-the-art sys-
tems: a hiero model (Chiang, 2005), and a forest-to-
string model (Mi et al., 2008; Mi and Huang, 2008).
We will use the following example from Chinese
to English to explain both individual and joint de-
coding algorithms throughout this paper.
tˇaol`un h`ui zˇenmey`ang
discussion/NN will/VV how/VV
discuss/VV meeting/NN
There are several possible meanings based on the
different POS tagging sequences:
</bodyText>
<listItem confidence="0.9327598">
1: NN VV VV: How is the discussion going?
2: VV NN VV: Discuss about the meeting.
3: NN NN VV: How was the discussion meeting?
4: VV VV VV: Discuss what will happen.
id rule
</listItem>
<equation confidence="0.950847214285714">
r1 VV(tˇaol`un) discuss
r2 NP(tˇaol`un) the discussion
r3 NP(h`ui) the meeting
r4 VP(zˇenmey`ang) how
r� VP(zˇenmey`ang) about
4
r5 IP(x1:NP x2:VP) x2 x1
r6 IP(x1:VV x2:IP) x1 x2
r7 IP(x1:NP VP(VV(h`ui) x2:VP)) x2 is x1 going
r11 X(x1:X zˇenmey`ang) how was x1
r12 X(zˇenmey`ang) what
r13 X(tˇaol`un h`ui) the discussion meeting
r14 X(h`ui x1:X) x1 will happen
r15 S(x1:S x2:X) x1 x2
</equation>
<tableCaption confidence="0.8515735">
Table 1: Translation rules. Tree-to-string (r1–r7), hiero
(r11–r14), vanilla glue (r15).
</tableCaption>
<equation confidence="0.910961">
x2:VP x2 is x1 going
h`ui
</equation>
<figureCaption confidence="0.997193">
Figure 1: Tree-to-string rule r7.
</figureCaption>
<bodyText confidence="0.954095666666667">
Table 1 shows translation rules that can generate
all four translations. We will use those rules in the
following sections.
</bodyText>
<subsectionHeader confidence="0.997172">
2.1 Forest-to-string
</subsectionHeader>
<bodyText confidence="0.9999298125">
Forest-to-string translation (Mi et al., 2008) is a lin-
guistic syntax-based system, which significantly im-
proves the translation quality of the tree-to-string
model (Liu et al., 2006; Huang et al., 2006) by using
a packed parse forest as the input instead of a single
parse tree.
Figure 1 shows a tree-to-string translation
rule (Huang et al., 2006), which is a tuple
(lhs(r), rhs(r), ql(r)), where lhs(r) is the source-side
tree fragment, whose internal nodes are labeled by
nonterminal symbols (like NP and VP), and whose
frontier nodes are labeled by source-language words
(like “h`ui”) or variables from a set X = {x1, x2, ...1;
rhs(r) is the target-side string expressed in target-
language words (like “going”) and variables; and
ql(r) is a mapping from X to nonterminals. Each
</bodyText>
<equation confidence="0.743578333333333">
IP
x1:NP VP
VV
</equation>
<page confidence="0.893224">
546
</page>
<figure confidence="0.983737979591837">
4 Rh 4
VV1,2
NP1, 2
h`ui
NP1, 2
h`ui
VP2, 3
zˇenmey`ang
VP2, 3
zˇenmey`ang
VV1,2
IP0, 3
(a)
IP1, 3
VP1, 3
NP0,1
VV0, 1
tˇaol`un
NP0,1
VV0,1
tˇaol`un
e6
IP0, 3
VP1, 3
IP1, 3
e5
e7
Rt (b)
�
X1, 3
(b&apos;) X0,2
tˇaol`un h`ui
zˇenmey`ang
NP1, 2
tˇaol`un
h`ui
VP2, 3 X2,3
zˇenmey`ang
� (c)
X0, 2
IP0, 3 X0, 3
IP1, 3 X1, 3
VP1, 3
X0, 3
e11
e14
X2,3
VV1,2
VV0,1 NP0,1
</figure>
<figureCaption confidence="0.819019888888889">
Figure 2: Parse and translation hypergraphs. (a) The parse forest of the example sentence. Solid hyperedges denote
the 1-best parse. (b) The corresponding translation forest Ft after applying the tree-to-string translation rule set Rt.
Target lexical content is not shown. Each translation hyperedge (e.g. e7) has the same index as the corresponding rule
(r7). Gray nodes (e.g. VP1,3) became inaccessible due to the insufficient rule coverage. (b&apos;) The translation forest Fh
after applying the hierarchical rule set Rh to the input sentence. (c) The combined translation forest HI obtained by
superimposing b and b&apos;. The nodes within each solid box share the same span. See Figure 3 for an example of the
internal structure of a box. The forest-to-string system can produce the translation 1 (dashed derivation: r2, r4 and r7)
and 2 (solid derivation: r1, r3, r&apos;4, r5, and r6). Hierarchical rules generate the translation 3 (r11 and r13). The translation
4 is available by using joint decoding at X1, 3 -� IP1, 3 with the derivation: r1, r6, r12, and r14.
</figureCaption>
<bodyText confidence="0.949447666666667">
variable xi E X occurs exactly once in lhs(r) and
exactly once in rhs(r). Take the rule r7 in Figure 1
for example, we have:
</bodyText>
<equation confidence="0.999916333333333">
lhs(r7) = IP(x1:NP VP(VV(h`ui) x2:VP)),
rhs(r7) = x2 is x1 going,
Or7) = 1x1 H NP, x2 H VP}.
</equation>
<bodyText confidence="0.999945">
Typically, a forest-to-string system performs
translation in two steps (shown in Figure 2): pars-
ing and decoding. In the parsing step, we convert the
source language input into a parse forest (a). In the
decoding step, we first convert the parse forest into a
translation forest Ft in (b) by using the fast pattern-
matching technique (Zhang et al., 2009). For exam-
ple, we pattern-match the rule r7 rooted at IP0, 3, in
such a way that x1 spans NP0,1 and x2 spans VP2, 3,
and add a translation hyperedge e7 in (b). Then the
decoder searches for the best derivation on the trans-
lation forest and outputs the target string.
</bodyText>
<subsectionHeader confidence="0.996341">
2.2 Hiero
</subsectionHeader>
<bodyText confidence="0.9998302">
Hiero (hierarchical phrase-based) model (Chiang,
2005) acquires rules of synchronous context-free
grammars (SCFGs) from word-aligned parallel data,
and uses plain sequences of words as the input, with-
out any syntactic information.
</bodyText>
<page confidence="0.957742">
547
</page>
<figure confidence="0.73974">
IP&apos;1, 3 X&apos;1, 3
IP1, 3 X1, 3
scheme interaction edges in supernode
IP&apos;1, 3 X&apos;1, 3
Interchange
IP1,3 X1,3
</figure>
<figureCaption confidence="0.995407666666667">
Figure 3: Three interaction schemes for joint decoding.
Details of the interaction supernode for span (1, 3) shown
in Figure 2 (c). Soft constraints control the transitions.
</figureCaption>
<bodyText confidence="0.999936823529412">
SCFG can be formalized as a set of tuples
(lhs(r), rhs(r), 0(r)), where lhs(r) is the source-side
one-level CFG, whose root is X or S, and whose
frontier nodes are labeled by source-language words
(like “h`ui”) or variables from a set X = 1x1, x2, ...1;
rhs(r) is the target-side string expressed in target-
language words (like “going”) and variables; and
0(r) is a mapping from X to nonterminals. Table 1
shows examples of hiero rules r11–r15.
Although different on source side, hiero decod-
ing can be formalized equally as forest-to-string de-
coding: First, pattern-match the input sentence into
a translation forest Fh. For example, since the rule
r11 matches “zˇenmey`ang” such that x1 spans the first
two words, add a hyperedge e11 in Figure 2 (b&apos;).
Then search for the best derivation over the trans-
lation forest.
</bodyText>
<sectionHeader confidence="0.987081" genericHeader="method">
3 Joint Decoding
</sectionHeader>
<bodyText confidence="0.999943615384615">
The goal of joint decoding is to let different MT
models collaborate within the framework of a single
decoder. This can be done by combining translation
hypergraphs of the different models at the decod-
ing time, so that online sharing of partial hypotheses
overcomes weaknesses and boosts strengths of the
systems combined.
As both forest-to-string and hiero produce trans-
lation forests that share the same hypergraph struc-
ture, we first formalize the hypergraph, then we in-
troduce an algorithm to combine different hyper-
graphs, and finally we describe three joint decoding
schemes over the merged hypergraph.
</bodyText>
<subsectionHeader confidence="0.99758">
3.1 Hypergraphs
</subsectionHeader>
<bodyText confidence="0.99813008">
More formally, a hypergraph H is a pair (V, E),
where V is the set of nodes, and E the set of hyper-
edges. For a given sentence w1:l = w1 ... wl, each
node v E V is in the form of Yi, j, where Y is a
nonterminal in the context-free grammar1 and i, j,
0 &lt; i &lt; j &lt; l, are string positions in the sentence
w1:l, which denote the recognition of nonterminal
Y spanning the substring from positions i through j
(that is, wi+1 ... wj). Each hyperedge e E E is a tuple
(tails(e), head(e), target(e)), where head(e) E V is
the consequent node in the deductive step, tails(e) E
V* is the list of antecedent nodes, and target(e) is
a list of rhs(r) for rules r such that each rule r has
the same lhs(r) pattern-matched at the node head(e).
For example, the hyperedge e7 in Figure 2 (b) is
e7 = ((NP0, 1, VP2, 3), IP0, 3, (x2 is x1 going)),
where we can infer the mapping to be
1x1 H NP0, 1, x2 H VP2, 3 1.
We also denote BS(v) to be the set of incoming
hyperedges of node v, which represent the different
ways of deriving v. For example, BS(IP0,3) is a set
of e7 and e6.
There is also a distinguished root node TOP in
each hypergraph, denoting the goal item in transla-
tion, which is simply TOP0,l.
</bodyText>
<subsectionHeader confidence="0.999899">
3.2 Combining Hypergraphs
</subsectionHeader>
<bodyText confidence="0.999507875">
We enable interaction between translation hyper-
graphs, such as hiero Fh = (Vh, Eh) and forest-to-
string Ft = (Vt, Et), on nodes covering the same
span (e.g. IP1, 3 and X1, 3 in Figure 2 (c) grouped in
a box). We call such groups interaction supernodes
and show a detailed example of a supernode for span
(1, 3) in Figure 3.
The combination runs in four steps:
</bodyText>
<footnote confidence="0.68112">
1In this paper, nonterminal labels X and S denote hiero
derivations, other labels are tree-to-string labels.
</footnote>
<figure confidence="0.8766665">
Generalization
IP&apos;1, 3 X&apos;1, 3
IP1,3 X1,3
Specification
IP&apos;1, 3 X&apos;1, 3
IP1,3 X1,3
</figure>
<page confidence="0.932535">
548
</page>
<listItem confidence="0.982182769230769">
1. For each node v = Yi, j, v E Vh U Vt, we create
a new interaction node v&apos; = Y&apos;i, j with empty
BS (v&apos;). For example, we create two nodes,
IP&apos;1, 3 and X&apos;1, 3, at the top of Figure 3.
2. For each hyperedge e E BS(v), v E Vt U Vh,
we replace each v in tails(e) with v&apos;. For exam-
ple, e7 becomes ((NP&apos;0,1, VP&apos;2, 3), IP0, 3, (x2 is
x1 going)).
3. All the nodes and hyperedges form the merged
hypergraph Fm, such as in Figure 2 (c).
4. Insert interaction hyperedges connecting nodes
within each interaction supernode to make Fm
connected again.
</listItem>
<bodyText confidence="0.945109">
In the following subsection we present details of in-
teractions and introduce three alternative schemes.
</bodyText>
<subsectionHeader confidence="0.997842">
3.3 Three Schemes of Joint Decoding
</subsectionHeader>
<bodyText confidence="0.999990958333333">
Interaction hyperedges within each supernode allow
the decoder either to stay within the same system
(e.g. in hiero using X1, 3 —� X&apos;1, 3 in Figure 3), or to
switch to the other (e.g. to forest-to-string using X1, 3
� IP&apos;1,3).
For example, translation 4 can be produced as
follows: The source string “zˇenmey`ang” is trans-
lated by the phrase rule r12. The hiero hyperedge
e14 combines it with the translation of “h`ui”, reach-
ing the hiero node X1, 3. Using the interaction edge
X1,3 —� IP&apos;1, 3 will switch into the tree-to-string
model, so that the translation can be completed with
the tree-to-string edge e6 that connects it with a par-
tial tree-to string translation of “tˇaol`un” done by r1.
In order to achieve more precise control over the
interaction between tree-to-string and hiero deriva-
tions, we propose the following three basic inter-
action schemes: generalization, specification, in-
terchange. The schemes control the interaction be-
tween hiero and tree-to-string models by means of
soft constraints. Some schemes may even restrict
certain types of transitions. The schemes are de-
picted in Figure 3 and their details are discussed in
the following three subsections.
</bodyText>
<subsectionHeader confidence="0.817082">
3.3.1 Specification
</subsectionHeader>
<bodyText confidence="0.999986727272727">
The specification decoding scheme reflects the in-
tuition of using hiero rules to translate shorter spans
and tree-to-string rules to reorder higher-level sen-
tence structures. In other words, the scheme allows
one-way switching from the hiero general nontermi-
nal into the more specific nonterminal of a tree-to-
string rule. Transitions in reverse directions are not
allowed. This is achieved by inserting specification
interaction hyperedges e leading from hiero nodes
Xi, j or Si, j into all tree-to-string interaction nodes
Y&apos;i, j within the same supernode.
</bodyText>
<subsectionHeader confidence="0.745866">
3.3.2 Generalization
</subsectionHeader>
<bodyText confidence="0.999991466666667">
In some translation domains, hiero outperforms
tree-to-string systems, as was shown in experiments
in Section 6. While local hiero or tree-to-string re-
orderings perform well, long distance reorderings
proposed by tree-to-string may be too risky (e.g. due
to parsing errors), so that monotone concatenation
of long sequences2 is the more reliable strategy. The
generalization decoding scheme, complementary to
the specification, is motivated by the idea of incorpo-
rating reliable tree-to-string translations for some se-
quences into a strong hiero translation system. This
is achieved by inserting generalization interaction
hyperedges e leading from tree-to-string nodes Yi, j
nodes into general hiero interaction nodes X&apos;i, j and
S&apos;i, j within the same supernode.
</bodyText>
<sectionHeader confidence="0.557795" genericHeader="method">
3.3.3 Interchange
</sectionHeader>
<bodyText confidence="0.999981857142857">
The interchange decoding scheme is a union of
the two previous approaches. Any derivation can
freely combine hiero and tree-to-string productions.
Both specification and generalization interaction
hyperedges are inserted leading from all hiero and
tree-to-string nodes Xi, j, Si, j, and Yi, j into all inter-
action nodes X&apos;i, j, S&apos;i, j, and Y&apos;i, j.
</bodyText>
<subsectionHeader confidence="0.986143">
3.4 Fuzzy match
</subsectionHeader>
<bodyText confidence="0.999948777777778">
The translation rule set cannot usually cover all
hyperedges in the parse forest, thus some nodes
become inaccessible in the translation forest (e.g.
VP1, 3 in Figure 2). However, in the parse forest, as
opposed to a 1-best tree, we can find other nodes
spanning the same sequence wi:j (e.g. node IP1,3).
In order to re-enable inaccessible nodes and to in-
crease the variability of the translation forest, we
allow reaching them from the other tree-to-string
</bodyText>
<footnote confidence="0.971336">
2Monotone glue is the only possibility for very long spans
exceeding the hiero maxParse treshold.
</footnote>
<page confidence="0.996388">
549
</page>
<bodyText confidence="0.9999409">
nodes within the same interaction node. This can
be achieved by adding fuzzy hyperedges between
every tree-to-string state Yi, j and a differently la-
beled tree-to-string interaction state Z0i, j. For exam-
ple, in the span (0,1), we have a fuzzy hyperedge
VV0,1 → NP00,1.
While interaction hyperedges combine different
translation models, fuzzy hyperedges combine dif-
ferent derivations within the same (tree-to-string)
model.
</bodyText>
<sectionHeader confidence="0.998337" genericHeader="method">
4 Interaction Features
</sectionHeader>
<bodyText confidence="0.996584615384615">
Our baseline systems use the log-linear framework
to estimate the probability P(D) of a derivation D
from features φi and their weights λi as P(D) ∝
exp (Zi λiφi). Similarly as Chiang et al. (2009), our
systems use tens of dense (e.g. language models,
translation probabilities) and thousands of sparse
(e.g. lexical, fertility) features.
The features related to the joint decoding experi-
ments are the costs for specification, generalization,
interchange, and the fuzzy match. Let Lt be the set
of the labels used by the source language parser and
Lh = {S,X} be the labels used by hiero.
The generalization feature
</bodyText>
<equation confidence="0.73765575">
φY→Z = |{e; e ∈ D, ∃i, j tails(e) = {Yi, j} (1)
∧head(e) = Z0i, j}|
is the total number of generalization hyperedges in
D going from tree-to-string states Y ∈ Lt to hiero
states Z0 ∈ Lh.
The specification feature
φZ→Y = |{e; e ∈ D, ∃i, j tails(e) = {Zi, j} (2)
∧head(e) = Y0i, j}|
</equation>
<bodyText confidence="0.990945571428571">
is the total number of specification hyperedges in D
going from hiero states Z ∈ Lh to tree-to-string states
Y0 ∈ Lt.
The interchange feature is implemented by en-
abling the generalization and specification features
at the same time for both tuning and testing.
The fuzzy match feature
</bodyText>
<equation confidence="0.998638">
φU→W = |{e; e ∈ D, ∃i, j tails(e) = {Ui, j} (3)
∧head(e) = W0i, j}|
</equation>
<bodyText confidence="0.999811333333333">
is the total number of fuzzy match hyperedges in D
going from tree-to-tree states U ∈ Lt to tree-to-string
states W0 ∈ Lt. 3
We use MIRA to obtain weights for the new fea-
tures by tuning on the development set. The num-
ber of new parameters to tune can be estimated as
|Lh |× |Lt |for generalization and specification, and
2 × |Lh |× |Lt |for interchange. For the fuzzy match
of tree-to-string nonterminals we have |Lt |× |Lt |pa-
rameters organized as a sparse matrix, since we only
consider combinations on nonterminal labels that
cooccur in the data.4
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999655">
From the previous explorations of online translation
model combination, we see the work of Liu et al.
(2009) proposing an unconstrained combination of
hiero and tree-to-string models as a special configu-
ration of our framework, and we also replicate it.
Denero et al. (2010) combine translation mod-
els even with different search paradigms. Their ap-
proach is different, since their component systems
do not interact at decoding time, instead, each of
them provides its weighted translation forest first,
the forests are then combined to infer a new com-
bination model.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.984747416666667">
In this section we describe the setup, present results,
and analyze the experiments. Finally, we propose fu-
ture directions of research.
3Here we allow U = W, which can be viewed in such a way
that exact match is a special case of fuzzy match.
4We also carried out an alternative experiment with only
three fuzzy match features estimated from the training data
parse forest by Naive Bayes by observing all spans in the train-
ing data, accumulating counts Cs(U) and Cs(U,W) of nonter-
minals (or pairs of nonterminals) heading the same span s. The
first two features (one for each direction) are based on condi-
tional probabilities:
</bodyText>
<equation confidence="0.979672111111111">
Z�
s∈spans Cs (U, W)
φ(U|W) = −log Z . (4)
s∈spans Cs(W)
The third feature is based on joint probability:
Z�
s∈spans Cs(U, W)
Z . (5)
s∈spans,A,B∈Lt Cs(A, B)
</equation>
<bodyText confidence="0.9968275">
The average performance drops by 0.1 (T -B )/2 points,
compared to the interchange eperiment.
</bodyText>
<equation confidence="0.990009">
φ(U,W) = − log
</equation>
<page confidence="0.989148">
550
</page>
<table confidence="0.9997134">
System GALE-web P1R6-web MT08 news MT08 web Avg.
(T-B)/2
B (T-B)/2 B (T-B)/2 B (T-B)/2 B (T-B)/2
T2S 32.6 11.6 16.9 23.5 37.7 7.8 28.1 14.5 14.4
Single Hiero 33.7 10.2 17.0 23.1 39.2 6.3 28.8 13.7 13.3
F2S 34.0 10.3 17.3 23.2 39.6 6.3 29.2 13.6 13.4
Liu:09 34.1 9.7 17.0 23.0 38.8 6.7 29.0 13.2 13.2
Joint Gen. 34.4 9.7 17.8 22.6 40.0 6.1 29.6 13.1 12.9
Spe. 35.1 9.4 18.1 22.2 40.2 5.8 29.6 12.9 12.6
Int. 34.9 9.4 17.9 22.3 40.0 6.2 29.6 12.9 12.7
</table>
<tableCaption confidence="0.99797">
Table 2: All results of single and joint decoding systems.
</tableCaption>
<subsectionHeader confidence="0.989565">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999732605263158">
The training corpus consists of 16 million sen-
tence pairs available within the DARPA BOLT
Chinese-English task. The corpus includes a mix
of newswire, broadcast news, webblog and comes
from various sources such as LDC, HK Law, HK
Hansard and UN data. The Chinese text is seg-
mented with a segmenter trained on CTB data using
conditional random fields (CRF). Language models
are trained on the English side of the parallel cor-
pus, and on monolingual corpora, such as Gigaword
(LDC2011T07) and Google News, altogether com-
prising around 10 billion words.
We use a modified version of the Berkeley parser
(Petrov and Klein, 2007) to obtain a parse forest
for each training sentence, then we prune it with
the marginal probability-based inside-outside algo-
rithm to contain only 3n CFG nodes, where n is the
sentence length. Finally, we apply the forest-based
GHKM algorithm (Mi and Huang, 2008; Galley et
al., 2004) to extract tree-to-string translation rules
from forest-string pairs.
In the decoding step, we prune the input hyper-
graphs to 10n nodes before we use fast pattern-
matching (Zhang et al., 2009) to convert the parse
forest into the translation forest.
We tune on 1275 sentences, each with 4 refer-
ences, from the LDC2010E30 corpus, initially re-
leased under the DARPA GALE program.
All MT experiments are optimized with
MIRA (Crammer et al., 2006) to maximize
(T -B )/2.
We test on four different test sets: GALE-web test
set from LDC2010E30 corpus (1239 sentences, 4
references), P1R6-web test set from LDC2012E124
corpus (1124 sentences, 1 reference), NIST MT08
newswire portion (691 sentences, 4 references), and
NIST MT08 web portion (666 sentences, 4 refer-
ences).
</bodyText>
<subsectionHeader confidence="0.810041">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.9968325">
Table 2 shows all results of single and joint decoding
systems. The B score of the single hiero baseline
is 39.2 on MT08-news, showing that it is a strong
system. The single F2S baseline achieves compara-
ble scores on all four test sets.
Then, for reference, we present results of joint Hi-
ero and T2S decoding, which is, to our knowledge, a
strong and competitive reimplementaion of the work
described by Liu et al. (2009). Finally, we present re-
sults of joint decoding of hiero and F2S in three in-
teraction schemes: generalization, specification, and
interchange.
All three combination schemes significantly im-
prove results of any single system on all four test-
sets. On average and measured in (T -B )/2,
our systems improve the best single system by 0.4
(generalization), 0.7 (specification), and 0.6 (inter-
change).
The specification comes out as the strongest inter-
action scheme, beating the second interchange on 2
testsets by 0.1 and 0.4 (T -B )/2 points and on 3
testsets by 0.2 B points.
</bodyText>
<subsectionHeader confidence="0.99983">
6.3 Discussion of Results
</subsectionHeader>
<bodyText confidence="0.9999385">
Interpretations of model behavior with thousands of
parameters that may possibly overlap and interfere
should be always attempted with caution. In this sec-
tion we highlight some interesting observations, ac-
</bodyText>
<page confidence="0.993469">
551
</page>
<table confidence="0.999937875">
Specification Generalization Interchange
X → ∗ ∗ → X
X → ∗ ∗ → X
VP 0.069 QP 0.057 VV 0.062 NN 0.048
IP 0.059 PP 0.054 VP 0.044 PP 0.041
VV 0.053 NN 0.048 NN 0.034 CP 0.035
NR 0.032 DP 0.044 QP 0.025 LCP 0.035
ADVP 0.025 NR 0.034 ADVP 0.022 DEG 0.031
QP 0.023 DNP 0.032 LCP 0.021 DP 0.028
CC 0.017 NP 0.030 NP 0.018 DEC 0.027
DVP 0.017 LC 0.025 P 0.017 QP 0.027
NP 0.017 DEC 0.023 IP 0.016 LC 0.021
P 0.012 DEG 0.023 NR 0.016 NP 0.019
... ... ... ... ... ... ... ...
CS -0.005 VV -0.010 VSB -0.004 FLR -0.006
CP -0.007 PRN -0.011 PN -0.004 DVP -0.009
AD -0.011 PN -0.013 PU -0.004 BA -0.010
VRD -0.012 BA -0.015 M -0.007 JJ -0.011
PU -0.028 VP -0.015 VRD -0.014 AS -0.014
ADJP -0.028 VRD -0.028 DNP -0.023 VRD -0.017
DNP -0.045 JJ -0.035 ADJP -0.039 ADVP -0.021
PP -0.064 VC -0.037 PP -0.058 PN -0.033
PRN -0.069 DFL -0.054 DP -0.070 DFL -0.038
DP -0.092 PU -0.073 PRN -0.080 PU -0.103
</table>
<tableCaption confidence="0.993654">
Table 3: Examples of specification, generalization, and interchange weights. POS tags in italics.
</tableCaption>
<table confidence="0.997965733333333">
ADVP
CLP
ADJP
FLR
DFL
DP
VCP
VSB
VRD
VCD
QP
NP
DVP
DNP
LCP
PP
VP
CP
PRN
IP
FRAG
#Interactions Generalization Inter. gen.
F2S → glue 5557 4202
F2S → hiero 695 1178
total gen. 6252 5380
Specification Inter. spec.
phrase → F2S 2763 2235
glue → F2S 946 841
hiero → F2S 683 839
total spec. 4392 3915
</table>
<tableCaption confidence="0.944134">
Table 5: Rule interactions on GALE-web test set.
</tableCaption>
<figure confidence="0.97658655">
18
17
16
15
14
13
Average span length
12
11
10
9
8
7
6
5
4
3
2
1
0
</figure>
<figureCaption confidence="0.998067">
Figure 4: Average span length for selected syntactic la-
bels on GALE-web test set.
</figureCaption>
<bodyText confidence="0.999667875">
companying them with our subjective judgements
and speculations.
Table 3 shows the specification and generalization
features tuned for the three combination schemes,
then sorted by their weights AX→Y or AY→X. Features
shown at the top of the table are very expensive (the
system tries to avoid them), while inexpensive fea-
tures are at the bottom (the system is encouraged to
use them).
The most expensive interactions for the specifi-
cation belong to constituents (IP, VP) that usually
occur higher in a syntactic tree (see Figure 4 for av-
erage span lengths of selected syntactic labels), and
often require non-local reorderings. This indicates
that the decoder is discouraged from switching from
hiero into F2S derivation at these higher-level spans.
</bodyText>
<page confidence="0.992036">
552
</page>
<table confidence="0.999430333333333">
rule type Generalization Specification Interchange
F2S 18,807 58% 19,399 70% 18,400 61%
Hiero 3,730 12% 2,330 8% 3,133 10%
Glue 7,367 23% 571 2% 4,714 16%
Phrase 2,274 7% 5,484 20% 3,868 13%
total 32,178 27,784 30,115
</table>
<tableCaption confidence="0.99981">
Table 4: Rule counts on GALE-web test set.
</tableCaption>
<figure confidence="0.980953444444444">
Generalization
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70
Span length
Specification
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70
Span length
Interchange
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70
Span length
</figure>
<figureCaption confidence="0.999624">
Figure 5: Rule distributions on GALE-web test set.
</figureCaption>
<bodyText confidence="0.999986710526316">
The third most expensive feature belongs to a
part-of-speech tag—the preterminal VV. We may
hypothesize that it shows the importance of lexical
information for the precision of reordering typically
carried out within (parent) VP nodes, and/or the im-
portance of POS information for succesful disam-
biguation of word senses in translation. Ideally, the
system can use a VP rule with a lexicalized VV. Less
preferably, the VV part has to be translated by an-
other T2S rule (losing the lexical constraint). In the
worst case, the system has to use a hiero hypothe-
sis to translate the VV part (losing the syntactic con-
straint), risking imprecise translation, since the hiero
rule is not constrained to senses corresponding to the
source POS VV. Again, the high penalty discourages
from using the hiero derivation in this context.
On the other hand, the bottom of the table shows
labels that encourage using hiero–DP, PP, DNP,
ADJP, etc.–shorter phrases that tend to be monotone
and less ambiguous.
Similar interpretations seem plausible when ex-
amining the generalization experiment. Expensive
features related to preterminals (NR, NN, CD) may
suggest two alternative principles: First, using F2S
rules for thes POS categories and then switching to
hiero is discouraged, since these contexts are more
reliably handled by hiero due to better lexical cover-
age and common adjacency in nominal categories.
Second, since there is only one attempt to switch
from F2S derivation to hiero, letting F2S complete
even larger spans (and maybe switching to hiero
later) is favorable.
The tail of generalization feature weights is more
difficult to interpret. The discount on VP encourages
decoder to use F2S for entire verb phrases before
switching to hiero, on the other hand, other verb-
related preterminals occupy the tail as well, hurrying
into early switching from F2S to hiero.
</bodyText>
<figure confidence="0.999066366666667">
Number of rules
10^4
10^3
10^2
10^1
10^0
F2S
Hiero
Glue
Phrase
Number of rules
10^4
10^3
10^2
10^1
10^0
F2S
Hiero
Glue
Phrase
Number of rules
10^4
10^3
10^2
10^1
10^0
F2S
Hiero
Glue
Phrase
</figure>
<page confidence="0.997522">
553
</page>
<bodyText confidence="0.999980065573771">
Finally, the feature weights tuned for the in-
terchange experiment are divided into two sub-
columns. Both generalization and specification
weights show similar trends as in the previous two
interaction schemes, although blurred (VP and IP
descending from the absolute top). Since transitions
in both ways are allowed, the search space is big-
ger and the system may behave differently. It is even
possible for a path in the hypergraph to zigzag be-
tween F2S and hiero nodes to collect interaction dis-
counts, “diluting” the syntactic homogeneity of the
hypothesis.
Figure 5 and Tables 4 and 5 show rule distribu-
tions, total rule counts, and numbers of interactions
of different types for the three interaction schemes
on the GALE-web test set. The scope of phrase rules
is limited to 6 words. The scope of hiero rules is lim-
ited to 20 words by the commonly used maxParse
parameter, leaving longer spans to the glue rule.
The trends of F2S and glue rules show the most
obvious difference. In the generalization, F2S rules
translate spans of up to 50 words. Glue rules pre-
vail on spans longer then 7 words. The specification
is reversed, pushing the longest scope of hiero and
glue rules down to 40 words, completing the longest
sentences entirely with F2S. The interchange comes
out as a mixture of the previous two trends.
All three schemes prefer using F2S rules at
shorter spans, to the contrary of our original assump-
tion of phrasal and hiero rules being stronger on lo-
cal contexts in general. Here we may refer again
to the specification feature weights for preterminals
VV, NR, CC and P in Table 3 and to our previously
stated hypothesis about the importance of preserving
lexical and syntactic context.
Hiero rules usage on longer spans drops fastest
for specification, slowest for generalization, and in
between for interchange.
It is also interesting to notice the trends on very
short spans (2–4 words) shown by rule distributions
and reflected in numbers of interaction types. While
specification often transitions from a single phrase
rule directly into F2S, the interchange has relatively
higher counts of hiero rules, another sign of the hiero
and F2S interaction.
Synthesizing from several sources of indications
is difficult, however, we arrive at the conclusion that
joint decoding of hiero and F2S significantly im-
proves the performance. While the single systems
show similar performance, their roles are not bal-
anced in joint decoding. It seems that the role of hi-
ero consists in enabling F2S in most contexts.
We have focused on three special cases of inter-
action. We see a great potential in further studies
of other schemes, allowing more flexible interaction
than simple specification, but still more constrained
than the interchange. It seems also promising to re-
fine the interaction modeling with features taking
into account more information than a single syntac-
tic label, and to explore additional ways of parame-
ter estimation.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999996">
We have proposed flexible interaction of hyper-
graphs as a novel technique combining hiero
and forest-to-string translation models within one
decoder. We have explored three basic interac-
tion schemes—specification, generalization, and
interchange—and described soft constraints control-
ling the interactions. We have carried out experi-
ments on large training data and with strong base-
lines. Of the three schemes, the specification shows
the highest gains, achieving improvements from 0.5
to 0.9 (T -B )/2 points over the best single sys-
tem. We have conducted a detailed analysis of each
system output based on different indications of inter-
actions, discussed possible interpretations of results,
and finally offered our conclusion and proposed fu-
ture lines of research.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996">
We thank Jiˇr´ı Havelka for proofreading and help-
ful suggestions. We would like to acknowledge the
support of DARPA under Grant HR0011-12-C-0015
for funding part of this work. The views, opinions,
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995378">
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of HLT-NAACL, pages 218–226.
</reference>
<page confidence="0.986975">
554
</page>
<reference confidence="0.998077293103448">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
John Denero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In In Proceedings NAACL-HLT, pages 975–983.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT-NAACL, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961–968, Sydney, Aus-
tralia, July.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-HMM-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of EMNLP,
pages 98–107, October.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66–73.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL, pages 127–133.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609–
616.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings ofACL-IJCNLP, pages 576–584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206–214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT, pages
192–199.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404–411.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
ACL, pages 312–319, Prague, Czech Republic, June.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
translation system combination by confusion forest. In
Proceedings of ACL 2011, pages 1249–1257.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim
Tan. 2009. Fast translation rule matching for syntax-
based statistical machine translation. In Proceedings
of EMNLP, pages 1037–1045, Singapore, August.
</reference>
<page confidence="0.998476">
555
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.066294">
<title confidence="0.428031">and Hypergraph Interactions for Joint Hierarchical</title>
<affiliation confidence="0.79902">Prague Research Parku</affiliation>
<address confidence="0.925755">Prague, Czech Republic, 148</address>
<email confidence="0.997152">martin.cmejrek@us.ibm.com</email>
<author confidence="0.912748">T J Watson Research</author>
<address confidence="0.3835025">1101 Kitchawan Yorktown Heights, NY</address>
<abstract confidence="0.999592545454545">Machine translation benefits from system We propose interaction hypergraphs a novel technique combintranslation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—specification, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 -B points. We also provide a detailed experimental and qualitative analysis of the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="17788" citStr="Chiang et al. (2009)" startWordPosition="2911" endWordPosition="2914">teraction node. This can be achieved by adding fuzzy hyperedges between every tree-to-string state Yi, j and a differently labeled tree-to-string interaction state Z0i, j. For example, in the span (0,1), we have a fuzzy hyperedge VV0,1 → NP00,1. While interaction hyperedges combine different translation models, fuzzy hyperedges combine different derivations within the same (tree-to-string) model. 4 Interaction Features Our baseline systems use the log-linear framework to estimate the probability P(D) of a derivation D from features φi and their weights λi as P(D) ∝ exp (Zi λiφi). Similarly as Chiang et al. (2009), our systems use tens of dense (e.g. language models, translation probabilities) and thousands of sparse (e.g. lexical, fertility) features. The features related to the joint decoding experiments are the costs for specification, generalization, interchange, and the fuzzy match. Let Lt be the set of the labels used by the source language parser and Lh = {S,X} be the labels used by hiero. The generalization feature φY→Z = |{e; e ∈ D, ∃i, j tails(e) = {Yi, j} (1) ∧head(e) = Z0i, j}| is the total number of generalization hyperedges in D going from tree-to-string states Y ∈ Lt to hiero states Z0 ∈</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of HLT-NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1465" citStr="Chiang, 2005" startWordPosition="206" endWordPosition="207">tures. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations,</context>
<context position="4965" citStr="Chiang, 2005" startWordPosition="736" endWordPosition="737">d that joint decoding actually prefers tree-to-string rules in both shorter and longer spans. (Section 6.3). The paper is organized as follows: We briefly review the individual models in Section 2, describe the method of joint decoding using three alternative interaction schemes in Section 3, describe the features controlling the interactions and fuzzy match in Section 4, review the related work in Section 5, and finally, describe our experiments and give detailed discussion of the results in Section 6. 2 Individual Models Our individual models are two state-of-the-art systems: a hiero model (Chiang, 2005), and a forest-tostring model (Mi et al., 2008; Mi and Huang, 2008). We will use the following example from Chinese to English to explain both individual and joint decoding algorithms throughout this paper. tˇaol`un h`ui zˇenmey`ang discussion/NN will/VV how/VV discuss/VV meeting/NN There are several possible meanings based on the different POS tagging sequences: 1: NN VV VV: How is the discussion going? 2: VV NN VV: Discuss about the meeting. 3: NN NN VV: How was the discussion meeting? 4: VV VV VV: Discuss what will happen. id rule r1 VV(tˇaol`un) discuss r2 NP(tˇaol`un) the discussion r3 NP</context>
<context position="9299" citStr="Chiang, 2005" startWordPosition="1482" endWordPosition="1483">(shown in Figure 2): parsing and decoding. In the parsing step, we convert the source language input into a parse forest (a). In the decoding step, we first convert the parse forest into a translation forest Ft in (b) by using the fast patternmatching technique (Zhang et al., 2009). For example, we pattern-match the rule r7 rooted at IP0, 3, in such a way that x1 spans NP0,1 and x2 spans VP2, 3, and add a translation hyperedge e7 in (b). Then the decoder searches for the best derivation on the translation forest and outputs the target string. 2.2 Hiero Hiero (hierarchical phrase-based) model (Chiang, 2005) acquires rules of synchronous context-free grammars (SCFGs) from word-aligned parallel data, and uses plain sequences of words as the input, without any syntactic information. 547 IP&apos;1, 3 X&apos;1, 3 IP1, 3 X1, 3 scheme interaction edges in supernode IP&apos;1, 3 X&apos;1, 3 Interchange IP1,3 X1,3 Figure 3: Three interaction schemes for joint decoding. Details of the interaction supernode for span (1, 3) shown in Figure 2 (c). Soft constraints control the transitions. SCFG can be formalized as a set of tuples (lhs(r), rhs(r), 0(r)), where lhs(r) is the source-side one-level CFG, whose root is X or S, and wh</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="22755" citStr="Crammer et al., 2006" startWordPosition="3780" endWordPosition="3783">inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs. In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching (Zhang et al., 2009) to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program. All MT experiments are optimized with MIRA (Crammer et al., 2006) to maximize (T -B )/2. We test on four different test sets: GALE-web test set from LDC2010E30 corpus (1239 sentences, 4 references), P1R6-web test set from LDC2012E124 corpus (1124 sentences, 1 reference), NIST MT08 newswire portion (691 sentences, 4 references), and NIST MT08 web portion (666 sentences, 4 references). 6.2 Results Table 2 shows all results of single and joint decoding systems. The B score of the single hiero baseline is 39.2 on MT08-news, showing that it is a strong system. The single F2S baseline achieves comparable scores on all four test sets. Then, for reference, we prese</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Denero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation. In</title>
<date>2010</date>
<booktitle>In Proceedings NAACL-HLT,</booktitle>
<pages>975--983</pages>
<contexts>
<context position="1915" citStr="Denero et al., 2010" startWordPosition="280" endWordPosition="283">ssed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. Although limited to individual systems sharing the same </context>
<context position="19680" citStr="Denero et al. (2010)" startWordPosition="3248" endWordPosition="3251"> of new parameters to tune can be estimated as |Lh |× |Lt |for generalization and specification, and 2 × |Lh |× |Lt |for interchange. For the fuzzy match of tree-to-string nonterminals we have |Lt |× |Lt |parameters organized as a sparse matrix, since we only consider combinations on nonterminal labels that cooccur in the data.4 5 Related Work From the previous explorations of online translation model combination, we see the work of Liu et al. (2009) proposing an unconstrained combination of hiero and tree-to-string models as a special configuration of our framework, and we also replicate it. Denero et al. (2010) combine translation models even with different search paradigms. Their approach is different, since their component systems do not interact at decoding time, instead, each of them provides its weighted translation forest first, the forests are then combined to infer a new combination model. 6 Experiment In this section we describe the setup, present results, and analyze the experiments. Finally, we propose future directions of research. 3Here we allow U = W, which can be viewed in such a way that exact match is a special case of fuzzy match. 4We also carried out an alternative experiment with</context>
</contexts>
<marker>Denero, Kumar, Chelba, Och, 2010</marker>
<rawString>John Denero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In In Proceedings NAACL-HLT, pages 975–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="22312" citStr="Galley et al., 2004" startWordPosition="3707" endWordPosition="3710">egmenter trained on CTB data using conditional random fields (CRF). Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword (LDC2011T07) and Google News, altogether comprising around 10 billion words. We use a modified version of the Berkeley parser (Petrov and Klein, 2007) to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs. In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching (Zhang et al., 2009) to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program. All MT experiments are optimized with MIRA (Crammer et al., 2006) to maximize (T -B )/2. We test on four different test sets: GALE-web test set from LDC2010E30 corpus (1239 sentences, 4 references), P1R6-web test set from </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1523" citStr="Galley et al., 2006" startWordPosition="214" endWordPosition="217">lts of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search s</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="1866" citStr="He et al., 2008" startWordPosition="272" endWordPosition="275">ults. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. Althoug</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of EMNLP, pages 98–107, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="6386" citStr="Huang et al., 2006" startWordPosition="962" endWordPosition="965">12 X(zˇenmey`ang) what r13 X(tˇaol`un h`ui) the discussion meeting r14 X(h`ui x1:X) x1 will happen r15 S(x1:S x2:X) x1 x2 Table 1: Translation rules. Tree-to-string (r1–r7), hiero (r11–r14), vanilla glue (r15). x2:VP x2 is x1 going h`ui Figure 1: Tree-to-string rule r7. Table 1 shows translation rules that can generate all four translations. We will use those rules in the following sections. 2.1 Forest-to-string Forest-to-string translation (Mi et al., 2008) is a linguistic syntax-based system, which significantly improves the translation quality of the tree-to-string model (Liu et al., 2006; Huang et al., 2006) by using a packed parse forest as the input instead of a single parse tree. Figure 1 shows a tree-to-string translation rule (Huang et al., 2006), which is a tuple (lhs(r), rhs(r), ql(r)), where lhs(r) is the source-side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source-language words (like “h`ui”) or variables from a set X = {x1, x2, ...1; rhs(r) is the target-side string expressed in targetlanguage words (like “going”) and variables; and ql(r) is a mapping from X to nonterminals. Each IP x1:NP VP VV 546 4 </context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1443" citStr="Koehn et al., 2003" startWordPosition="201" endWordPosition="204">sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1501" citStr="Liu et al., 2006" startWordPosition="210" endWordPosition="213">antly improve results of any single system on four testsets. We find that specification—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (T -B )/2 points. We also provide a detailed experimental and qualitative analysis of the results. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion </context>
<context position="6365" citStr="Liu et al., 2006" startWordPosition="958" endWordPosition="961">`ang) how was x1 r12 X(zˇenmey`ang) what r13 X(tˇaol`un h`ui) the discussion meeting r14 X(h`ui x1:X) x1 will happen r15 S(x1:S x2:X) x1 x2 Table 1: Translation rules. Tree-to-string (r1–r7), hiero (r11–r14), vanilla glue (r15). x2:VP x2 is x1 going h`ui Figure 1: Tree-to-string rule r7. Table 1 shows translation rules that can generate all four translations. We will use those rules in the following sections. 2.1 Forest-to-string Forest-to-string translation (Mi et al., 2008) is a linguistic syntax-based system, which significantly improves the translation quality of the tree-to-string model (Liu et al., 2006; Huang et al., 2006) by using a packed parse forest as the input instead of a single parse tree. Figure 1 shows a tree-to-string translation rule (Huang et al., 2006), which is a tuple (lhs(r), rhs(r), ql(r)), where lhs(r) is the source-side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source-language words (like “h`ui”) or variables from a set X = {x1, x2, ...1; rhs(r) is the target-side string expressed in targetlanguage words (like “going”) and variables; and ql(r) is a mapping from X to nonterminals. Each </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP,</booktitle>
<pages>576--584</pages>
<contexts>
<context position="2209" citStr="Liu et al., 2009" startWordPosition="328" endWordPosition="331">om different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. Although limited to individual systems sharing the same search paradigm (e.g. left-to-right or bottom-up), joint decoding offers many potential advatages: search through a larger space, better efficiency, features designed once for all subsystems, potential cross-system features, online sharing of partial hypotheses, and many others. Different appr</context>
<context position="19514" citStr="Liu et al. (2009)" startWordPosition="3222" endWordPosition="3225">from tree-to-tree states U ∈ Lt to tree-to-string states W0 ∈ Lt. 3 We use MIRA to obtain weights for the new features by tuning on the development set. The number of new parameters to tune can be estimated as |Lh |× |Lt |for generalization and specification, and 2 × |Lh |× |Lt |for interchange. For the fuzzy match of tree-to-string nonterminals we have |Lt |× |Lt |parameters organized as a sparse matrix, since we only consider combinations on nonterminal labels that cooccur in the data.4 5 Related Work From the previous explorations of online translation model combination, we see the work of Liu et al. (2009) proposing an unconstrained combination of hiero and tree-to-string models as a special configuration of our framework, and we also replicate it. Denero et al. (2010) combine translation models even with different search paradigms. Their approach is different, since their component systems do not interact at decoding time, instead, each of them provides its weighted translation forest first, the forests are then combined to infer a new combination model. 6 Experiment In this section we describe the setup, present results, and analyze the experiments. Finally, we propose future directions of re</context>
<context position="23510" citStr="Liu et al. (2009)" startWordPosition="3907" endWordPosition="3910">-web test set from LDC2012E124 corpus (1124 sentences, 1 reference), NIST MT08 newswire portion (691 sentences, 4 references), and NIST MT08 web portion (666 sentences, 4 references). 6.2 Results Table 2 shows all results of single and joint decoding systems. The B score of the single hiero baseline is 39.2 on MT08-news, showing that it is a strong system. The single F2S baseline achieves comparable scores on all four test sets. Then, for reference, we present results of joint Hiero and T2S decoding, which is, to our knowledge, a strong and competitive reimplementaion of the work described by Liu et al. (2009). Finally, we present results of joint decoding of hiero and F2S in three interaction schemes: generalization, specification, and interchange. All three combination schemes significantly improve results of any single system on all four testsets. On average and measured in (T -B )/2, our systems improve the best single system by 0.4 (generalization), 0.7 (specification), and 0.6 (interchange). The specification comes out as the strongest interaction scheme, beating the second interchange on 2 testsets by 0.1 and 0.4 (T -B )/2 points and on 3 testsets by 0.2 B points. 6.3 Discussion of Results I</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint decoding with multiple translation models. In Proceedings ofACL-IJCNLP, pages 576–584, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="5032" citStr="Mi and Huang, 2008" startWordPosition="747" endWordPosition="750">n both shorter and longer spans. (Section 6.3). The paper is organized as follows: We briefly review the individual models in Section 2, describe the method of joint decoding using three alternative interaction schemes in Section 3, describe the features controlling the interactions and fuzzy match in Section 4, review the related work in Section 5, and finally, describe our experiments and give detailed discussion of the results in Section 6. 2 Individual Models Our individual models are two state-of-the-art systems: a hiero model (Chiang, 2005), and a forest-tostring model (Mi et al., 2008; Mi and Huang, 2008). We will use the following example from Chinese to English to explain both individual and joint decoding algorithms throughout this paper. tˇaol`un h`ui zˇenmey`ang discussion/NN will/VV how/VV discuss/VV meeting/NN There are several possible meanings based on the different POS tagging sequences: 1: NN VV VV: How is the discussion going? 2: VV NN VV: Discuss about the meeting. 3: NN NN VV: How was the discussion meeting? 4: VV VV VV: Discuss what will happen. id rule r1 VV(tˇaol`un) discuss r2 NP(tˇaol`un) the discussion r3 NP(h`ui) the meeting r4 VP(zˇenmey`ang) how r� VP(zˇenmey`ang) about </context>
<context position="22290" citStr="Mi and Huang, 2008" startWordPosition="3703" endWordPosition="3706">s segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword (LDC2011T07) and Google News, altogether comprising around 10 billion words. We use a modified version of the Berkeley parser (Petrov and Klein, 2007) to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs. In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching (Zhang et al., 2009) to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program. All MT experiments are optimized with MIRA (Crammer et al., 2006) to maximize (T -B )/2. We test on four different test sets: GALE-web test set from LDC2010E30 corpus (1239 sentences, 4 references), P</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL: HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="5011" citStr="Mi et al., 2008" startWordPosition="743" endWordPosition="746">to-string rules in both shorter and longer spans. (Section 6.3). The paper is organized as follows: We briefly review the individual models in Section 2, describe the method of joint decoding using three alternative interaction schemes in Section 3, describe the features controlling the interactions and fuzzy match in Section 4, review the related work in Section 5, and finally, describe our experiments and give detailed discussion of the results in Section 6. 2 Individual Models Our individual models are two state-of-the-art systems: a hiero model (Chiang, 2005), and a forest-tostring model (Mi et al., 2008; Mi and Huang, 2008). We will use the following example from Chinese to English to explain both individual and joint decoding algorithms throughout this paper. tˇaol`un h`ui zˇenmey`ang discussion/NN will/VV how/VV discuss/VV meeting/NN There are several possible meanings based on the different POS tagging sequences: 1: NN VV VV: How is the discussion going? 2: VV NN VV: Discuss about the meeting. 3: NN NN VV: How was the discussion meeting? 4: VV VV VV: Discuss what will happen. id rule r1 VV(tˇaol`un) discuss r2 NP(tˇaol`un) the discussion r3 NP(h`ui) the meeting r4 VP(zˇenmey`ang) how r� V</context>
<context position="6229" citStr="Mi et al., 2008" startWordPosition="937" endWordPosition="940">zˇenmey`ang) about 4 r5 IP(x1:NP x2:VP) x2 x1 r6 IP(x1:VV x2:IP) x1 x2 r7 IP(x1:NP VP(VV(h`ui) x2:VP)) x2 is x1 going r11 X(x1:X zˇenmey`ang) how was x1 r12 X(zˇenmey`ang) what r13 X(tˇaol`un h`ui) the discussion meeting r14 X(h`ui x1:X) x1 will happen r15 S(x1:S x2:X) x1 x2 Table 1: Translation rules. Tree-to-string (r1–r7), hiero (r11–r14), vanilla glue (r15). x2:VP x2 is x1 going h`ui Figure 1: Tree-to-string rule r7. Table 1 shows translation rules that can generate all four translations. We will use those rules in the following sections. 2.1 Forest-to-string Forest-to-string translation (Mi et al., 2008) is a linguistic syntax-based system, which significantly improves the translation quality of the tree-to-string model (Liu et al., 2006; Huang et al., 2006) by using a packed parse forest as the input instead of a single parse tree. Figure 1 shows a tree-to-string translation rule (Huang et al., 2006), which is a tuple (lhs(r), rhs(r), ql(r)), where lhs(r) is the source-side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source-language words (like “h`ui”) or variables from a set X = {x1, x2, ...1; rhs(r) is the</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL: HLT, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="22027" citStr="Petrov and Klein, 2007" startWordPosition="3660" endWordPosition="3663">he training corpus consists of 16 million sentence pairs available within the DARPA BOLT Chinese-English task. The corpus includes a mix of newswire, broadcast news, webblog and comes from various sources such as LDC, HK Law, HK Hansard and UN data. The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models are trained on the English side of the parallel corpus, and on monolingual corpora, such as Gigaword (LDC2011T07) and Google News, altogether comprising around 10 billion words. We use a modified version of the Berkeley parser (Petrov and Klein, 2007) to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs. In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching (Zhang et al., 2009) to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC20</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>312--319</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1849" citStr="Rosti et al., 2007" startWordPosition="268" endWordPosition="271"> analysis of the results. 1 Introduction Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decod</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of ACL, pages 312–319, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Machine translation system combination by confusion forest.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>1249--1257</pages>
<contexts>
<context position="1893" citStr="Watanabe and Sumita, 2011" startWordPosition="276" endWordPosition="279">ion Recent years have witnessed the success of various statistical machine translation (SMT) models using different levels of linguistic knowledge– phrase (Koehn et al., 2003), hiero (Chiang, 2005), and syntax-based (Liu et al., 2006; Galley et al., 2006). System combination became a promising way of building up synergy from different SMT systems and their specific merits. Numerous efforts that have been proposed in this field recently can be broadly divided into two cat.M. Cˇ and H. M. contributed equally to this work. egories: Offline system combination (Rosti et al., 2007; He et al., 2008; Watanabe and Sumita, 2011; Denero et al., 2010) aims at producing consensus translations from the outputs of multiple individual systems. Those outputs usually contain k-best lists of translations, which only explore a small portion of the entire search space of each system. This issue is well addressed in joint decoding (Liu et al., 2009), or online system combination, showing comparable improvements to the offline combination methods. Rather than finding consensus translations from the outputs of individual systems, joint decoding works with different grammars at the decoding time. Although limited to individual sys</context>
</contexts>
<marker>Watanabe, Sumita, 2011</marker>
<rawString>Taro Watanabe and Eiichiro Sumita. 2011. Machine translation system combination by confusion forest. In Proceedings of ACL 2011, pages 1249–1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>Fast translation rule matching for syntaxbased statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1037--1045</pages>
<location>Singapore,</location>
<contexts>
<context position="8968" citStr="Zhang et al., 2009" startWordPosition="1421" endWordPosition="1424">1, 3 with the derivation: r1, r6, r12, and r14. variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r). Take the rule r7 in Figure 1 for example, we have: lhs(r7) = IP(x1:NP VP(VV(h`ui) x2:VP)), rhs(r7) = x2 is x1 going, Or7) = 1x1 H NP, x2 H VP}. Typically, a forest-to-string system performs translation in two steps (shown in Figure 2): parsing and decoding. In the parsing step, we convert the source language input into a parse forest (a). In the decoding step, we first convert the parse forest into a translation forest Ft in (b) by using the fast patternmatching technique (Zhang et al., 2009). For example, we pattern-match the rule r7 rooted at IP0, 3, in such a way that x1 spans NP0,1 and x2 spans VP2, 3, and add a translation hyperedge e7 in (b). Then the decoder searches for the best derivation on the translation forest and outputs the target string. 2.2 Hiero Hiero (hierarchical phrase-based) model (Chiang, 2005) acquires rules of synchronous context-free grammars (SCFGs) from word-aligned parallel data, and uses plain sequences of words as the input, without any syntactic information. 547 IP&apos;1, 3 X&apos;1, 3 IP1, 3 X1, 3 scheme interaction edges in supernode IP&apos;1, 3 X&apos;1, 3 Interch</context>
<context position="22504" citStr="Zhang et al., 2009" startWordPosition="3738" endWordPosition="3741">T07) and Google News, altogether comprising around 10 billion words. We use a modified version of the Berkeley parser (Petrov and Klein, 2007) to obtain a parse forest for each training sentence, then we prune it with the marginal probability-based inside-outside algorithm to contain only 3n CFG nodes, where n is the sentence length. Finally, we apply the forest-based GHKM algorithm (Mi and Huang, 2008; Galley et al., 2004) to extract tree-to-string translation rules from forest-string pairs. In the decoding step, we prune the input hypergraphs to 10n nodes before we use fast patternmatching (Zhang et al., 2009) to convert the parse forest into the translation forest. We tune on 1275 sentences, each with 4 references, from the LDC2010E30 corpus, initially released under the DARPA GALE program. All MT experiments are optimized with MIRA (Crammer et al., 2006) to maximize (T -B )/2. We test on four different test sets: GALE-web test set from LDC2010E30 corpus (1239 sentences, 4 references), P1R6-web test set from LDC2012E124 corpus (1124 sentences, 1 reference), NIST MT08 newswire portion (691 sentences, 4 references), and NIST MT08 web portion (666 sentences, 4 references). 6.2 Results Table 2 shows a</context>
</contexts>
<marker>Zhang, Zhang, Li, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 2009. Fast translation rule matching for syntaxbased statistical machine translation. In Proceedings of EMNLP, pages 1037–1045, Singapore, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000072">
<title confidence="0.980929">
Semi-Markov Phrase-based Monolingual Alignment
</title>
<author confidence="0.921558">
Xuchen Yao and Benjamin Van Durme
</author>
<affiliation confidence="0.891232">
Johns Hopkins University
</affiliation>
<address confidence="0.524063">
Baltimore, MD, USA
</address>
<author confidence="0.988471">
Chris Callison-Burch* Peter Clark
</author>
<affiliation confidence="0.8689875">
University of Pennsylvania Allen Institute for Artificial Intelligence
Philadelphia, PA, USA Seattle, WA, USA
</affiliation>
<sectionHeader confidence="0.992966" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870615384616">
We introduce a novel discriminative model for
phrase-based monolingual alignment using a
semi-Markov CRF. Our model achieves state-
of-the-art alignment accuracy on two phrase-
based alignment datasets (RTE and para-
phrase), while doing significantly better than
other strong baselines in both non-identical
alignment and phrase-only alignment. Addi-
tional experiments highlight the potential ben-
efit of our alignment model to RTE, para-
phrase identification and question answering,
where even a naive application of our model’s
alignment score approaches the state of the art.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964584037735849">
Various NLP tasks can be treated as an alignment
problem: machine translation (aligning words in one
language with words in another language), ques-
tion answering (aligning question words with the an-
swer phrase), textual entailment recognition (align-
ing premise with hypothesis), paraphrase detection
(aligning semantically equivalent words), etc. Even
though most of these tasks involve only a single lan-
guage, alignment research has primarily focused on
the bilingual setting (i.e., machine translation) rather
than monolingual. Moreover, most work has con-
sidered token-based approaches over phrase-based.1
Here we seek to address this imbalance by proposing
better phrase-based models for monolingual word
alignment.
*Performed while faculty at Johns Hopkins University.
1In this paper we use the term token-based alignment for
one-to-one alignment and phrase-based for non one-to-one
alignment, and word alignment in general for both.
Most token-based alignment models can extrin-
sically handle phrase-based alignment to some ex-
tent. For instance, in the case of NYC align-
ing to New York City, the single source word
NYC may align three times separately to the tar-
get words: NYCHNew, NYCHYork, NYCHCity.
Or in the case of identical alignment, New York
City aligning to New York City is simply
NewHNew, YorkHYork, CityHCity. How-
ever, it is not as clear how to token-align New York
(as a city) with New York City. The problem is
more prominent when aligning phrasal paraphrases
ormultiword expressions, such as pass away and
kick the bucket. This suggests an intrinsi-
cally phrase-based alignment model.
The token aligner jacana-align (Yao et al., 2013a)
has achieved state-of-the-art result on the task of
monolingual alignment, based on previous work of
Blunsom and Cohn (2006). It employs a Conditional
Random Field (Lafferty et al., 2001) to align tokens
from the source sentence to tokens in the target sen-
tence, by treating source tokens as “observation” and
target tokens as “hidden states”. However, it is not
designed to handle phrase-based alignment, largely
due to the Markov nature of the underlying model:
a state can only span one token each time, making
it unable to align multiple consecutive tokens (i.e. a
phrase). We extend this model by introducing semi-
Markov states for phrase-based alignment: a state
can instead span multiple consecutive time steps,
thus aligning phrases on the source side. Also, we
merge phrases on the target side to phrasal states,
allowing the model to align phrases on the target
side as well. We evaluate the resulting semi-Markov
</bodyText>
<page confidence="0.957374">
590
</page>
<note confidence="0.733556">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590–600,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998232">
CRF model on the task of phrase-based alignment,
and then show a basic application in the NLP tasks
of recognizing textual entailment, paraphrase iden-
tification, and question answering sentence ranking.
The final phrase-based aligner is open-source.2
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999769897435897">
Most work in monolingual alignment employs de-
pendency tree/graph matching algorithms, includ-
ing tree edit distance (Punyakanok et al., 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Yao et al., 2013b), Particle Swarm Optimiza-
tion (Mehdad, 2009), linear regression/classification
models (Chambers et al., 2007; Wang and Manning,
2010), and min-cut (Roth and Frank, 2012). These
works inherently only support token-based align-
ment, with phrase-like alignment achieved by first
merging tokens to phrases as a preprocessing step.
The MANLI aligner (MacCartney et al., 2008)
and its derivations (Thadani and McKeown, 2011;
Thadani et al., 2012) are the first known phrase-
based aligners specifically designed for aligning En-
glish sentence pairs. It applies discriminative per-
ceptron learning with various features and handles
phrase-based alignment of arbitrary phrase lengths.
MANLI suffers from slow decoding time due to its
large search space. This was optimized by Thadani
and McKeown (2011) through Integer Linear Pro-
gramming (ILP), where benefiting from modern ILP
solvers they showed an order-of-magnitude speedup
in decoding. Also, various syntactic constraints can
be easily added, significantly improving exact align-
ment match rate for whole sentence pairs. Besides
the common application of textual entailment and
question answering, monolingual alignment has also
been applied in the field of text generation (Barzilay
and Lee, 2003; Pang et al., 2003).
Word alignment has been more explored in ma-
chine translation. The IBM models (Brown et al.,
1993) allow many-to-one alignment and are essen-
tially asymmetric. Phrase-based MT historically
relied on heuristics (Koehn, 2010) to merge two
sets of word alignment in opposite directions to
yield phrasal alignment. Later, researchers explored
non-heuristic phrase-based methods. Among them,
Marcu and Wong (2002) described a joint proba-
</bodyText>
<footnote confidence="0.813181">
2http://code.google.com/p/jacana/
</footnote>
<bodyText confidence="0.999797">
bility model that generates both the source and tar-
get sentences simultaneously. All possible pairs of
phrases in both sentences are enumerated and then
pruned with statistical evidence. Deng and Byrne
(2008) explored token-to-phrase alignment based
on HMM models (Vogel et al., 1996) by explic-
itly modeling the token-to-phrase probability and
phrase lengths. However, the token-to-phrase align-
ment is only in one direction: each target state still
only spans one source word, and thus alignment on
the source side is limited to tokens. Andr´es-Ferrer
and Juan (2009) extended the HMM-based method
to Hidden Semi-Markov Models (HSMM) (Osten-
dorf et al., 1996), allowing phrasal alignments on
the source side. Finally, Bansal et al. (2011) unified
the HSMM models with the alignment by agreement
framework (Liang et al., 2006), achieving phrasal
alignment that agreed in both directions.
Despite successful usage of generative semi-
Markov models in bilingual alignment, this has not
been followed with models in discriminative mono-
lingual alignment. Essentially monolingual align-
ment would benefit more from discriminative mod-
els with various feature extractions (just like those
defined in MANLI) than generative models without
any predefined feature (just like how they were used
in bilingual alignment). To combine the strengths of
both semi-Markov models and discriminative train-
ing, we propose to use the semi-Markov Conditional
Random Field (Sarawagi and Cohen, 2004), which
was first used in information extraction to tag con-
tinuous segments of input sequences and outper-
formed conventional CRFs in the task of named en-
tity recognition. We describe this model in the fol-
lowing section.
</bodyText>
<sectionHeader confidence="0.998198" genericHeader="method">
3 The Alignment Model
</sectionHeader>
<bodyText confidence="0.998913857142857">
Our objective is to define a model that supports
phrase-based alignment of arbitrary phrase length.
In this section we first describe a regular CRF
model that supports one-to-one token-based align-
ment (Blunsom and Cohn, 2006; Yao et al., 2013a),
then extend it to phrase-based alignment with the
semi-Markov model.
</bodyText>
<page confidence="0.996442">
591
</page>
<subsectionHeader confidence="0.87217">
3.1 Token-based Model
</subsectionHeader>
<bodyText confidence="0.999611818181818">
Given a source sentence s of length M, and a target
sentence t of length N, the alignment from s to t is a
sequence of target word indices a, where aic[1,M] E
[0, N]. We specify that when ai = 0, source word si
is aligned to a NULL state, i.e., deleted. This models
a many-to-one alignment from source to target: mul-
tiple source words can be aligned to the same target
word, but not vice versa. One-to-many alignment
can be obtained by running the aligner in the other
direction. The probability of alignment sequence a
conditioned on both s and t is then:
</bodyText>
<equation confidence="0.9499295">
exp(Ei,k λkfk(ai−1, ai, s, t))
Z(s, t)
</equation>
<bodyText confidence="0.998917285714286">
This assumes a first-order Conditional Random
Field (Lafferty et al., 2001). Since the word align-
ment task is evaluated over F1, instead of directly
optimizing it, we choose a much easier objective
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denomi-
nator:
</bodyText>
<equation confidence="0.970139">
Z(s, t) = � �exp( λkfk(ai−1, ai, s, t)
a i,k
+cost(ay, ˆa))
</equation>
<bodyText confidence="0.9998953">
where ay is the true alignments. cost(ay, ˆa) can
be viewed as special “features” that encourage de-
coding to be consistent with true labels. It is only
computed during training in the denominator be-
cause in the numerator cost(ay, ay) = 0. Ham-
ming cost is used in practice without learning the
weights (i.e., uniform weights). The more inconsis-
tence there is between ay and ˆa, the more penalized
is the decoding sequence aˆ through the cost func-
tion.
</bodyText>
<subsectionHeader confidence="0.967581">
3.2 Phrase-based Model
</subsectionHeader>
<bodyText confidence="0.998280142857143">
The token-based model supports 1 : 1 alignment.
We first extend it in the direction of ls : 1, where
a target state spans ls words on the source side (ls
source words align to 1 target word). Then we ex-
tend it in the direction of 1 : lt, where lt is the tar-
get phrase length a source word aligns to (1 source
word aligns to lt target words). The final combined
</bodyText>
<figureCaption confidence="0.83823">
Figure 1: A semi-Markov phrase-based model
</figureCaption>
<bodyText confidence="0.884546538461539">
example and the desired Viterbi decoding path.
Shaded horizontal circles represent the source
sentence (Shops are closed up for now
until March) and hollow vertical circles repre-
sent the hidden states with state IDs for the target
sentence (Shops are temporarily closed
down). State 0, a NULL state, is designated for dele-
tion. One state (e.g. state 3 and 15) can span multi-
ple consecutive source words (a semi-Markov prop-
erty) for aligning phrases on the source side. States
with an ID larger than the target sentence length
indicate “phrasal states” (states 6-15 in this exam-
ple), where consecutive target tokens are merged for
aligning phrases on the target side. Combining the
semi-Markov property and phrasal states yields for
instance, a 2×2 alignment between closed up in
the source and closed down in the target.
model supports ls : lt alignment. Throughout this
section we use Figure 1 as an illustrative example,
which shows phrasal alignment between the source
sentence: (Shops are closed up for now
until March) and the target sentence: (Shops
are temporarily closed down).
1 : 1 alignment is a special case of ls : 1 align-
ment where the target side state spans ls = 1 source
word, i.e., at each time step i, the source side word
</bodyText>
<figure confidence="0.995722476190476">
closed-
down 15
closed
Shops
temp.
shops
-are
down
NULL
...-...
are
0
2
3
4
5
6
7..14
1
shopsShops are closed up for now until March
p(a  |s, t) =
</figure>
<page confidence="0.987557">
592
</page>
<bodyText confidence="0.961687551724138">
si aligns to one state ai and the next aligned state
ai+1 only depends on the current state ai. This is
the Markovian property of the CRF. When ls &gt; 1,
during the time frame [i, i + ls), all source words
[ai, ai+ls) share the same state ai. Or in other words,
the state ai “spans” the following ls time steps. The
Markovian property still holds “outside” the time
frame ls, i.e., ai+ls still only depends on ai, the pre-
vious state ls time steps ago. But “within” the time
frame ls, the Markovian property does not hold any
more: [ai,..., ai+ls−1] are essentially the same state
ai. This is the semi-Markov property. States can be
distinguished by this property into two types: semi-
Markovian states and Markovian states.
We have generalized the regular CRF to a semi-
Markov CRF. Now we define it by generalizing the
feature function:
exp(Ei,k,ls Akfk(ai−ls, ai, s, t))
Z(s, t)
At time i, the k-th feature function fk mainly
extracts features from the pair of source words
(si−ls, ..., si] and target word tai (still with a spe-
cial case that ai = 0 marks for deletion). Inference
is still Viterbi-like: except for the fact during maxi-
mization, the Viterbi algorithm not only checks the
previous one time step, but all ls time steps. Sup-
pose the allowed maximal source phrase length is
Ls, define Vi(a  |s, t) as the highest score along the
decoding path until time i ending with state a:
</bodyText>
<equation confidence="0.972874">
Vi(a  |s, t) = max
a1,a2,...ai_1
</equation>
<bodyText confidence="0.825642">
then the recursive maximization is:
with factor:
</bodyText>
<equation confidence="0.965049">
�q&apos;i(a�,a,ls,s,t) = Akfk(a�i−ls , ai, s, t )
k
</equation>
<bodyText confidence="0.999791355555556">
and the best alignment a can be obtained by back-
tracking the last state aM from VM(aM  |s, t).
Training a semi-Markov CRF is very similar to
the inference, except for replacing maximization
with summation. The forward-backward algorithm
should also be used to dynamically compute the nor-
malization function Z(s, t). Compared to regular
CRFs, a semi-Markov CRF has a decoding time
complexity of O(LsMN2), a constant factor Ls
(usually 3 or 4) slower.
To extend from 1 : 1 alignment to 1 : lt alignment
with one source word aligning to lt target words,
we simply explode the state space by Lt times with
Lt the maximal allowed target phrase length. Thus
the states can be represented as an N x Lt ma-
trix. The state at (j, lt) represents the target phrase
[tj, ..., tj+lt). In this paper we distinguish states by
three types: NULL state (j = 0, lt = 0), token state
(lt = 1) and phrasal state (lt &gt; 1).
To efficiently store and compute these states, we
linearize the two dimensional matrix with a linear
function mapping uniquely between the state ID and
the target phrase offset/span. Suppose the target
phrase tj of length ltj E [1, Lt] holds a position
ptj E [1, N], and the source word si is aligned to
this state (ptj, ltj), a tuple for (position, span). Then
state ID asi is computed as:
Assume in Figure 1, Lt = 2, then the state ID for
the phrasal state (5, 2) closed-down with ptj = 5
for the position of word down and ltj = 2 for the
span of 2 words (looking “backward” from the word
down) is: 5 + (5 − 1) x 2 + 2 = 15.
Similarly, given a state id asi, the original target
phrase position and length can be recovered through
integer division and modulation. Thus during decod-
ing, if one output state is 15, we would know that it
uniquely comes from the phrasal state (5,2), repre-
senting the target phrase closed down.
This two dimensional definition of state space ex-
pands the number of states from 1 + N to 1 +
LtN. Thus the decoding complexity becomes
O(M(LtN)2) = O(L2tMN2) with a usual value
of 3 or 4 for Lt.
Now we have defined separately the ls : 1 model
and the 1 : lt model. We can simply merge them to
</bodyText>
<equation confidence="0.99596">
p(a  |s, t) =
p(a1, a2, ... , ai = a  |s, t)
Vi(a  |s, t) = max
a�
max [Vi−ls(a�  |s, t)
ls=1...Ls
+XFi(a�, a, ls, s, t)]
asi (ptj , ltj) = ptj ltj = 1
N + (ptj − 1) x Lt + ltj 1 &lt; ltj &lt; Lt
</equation>
<page confidence="0.978728">
593
</page>
<bodyText confidence="0.999918666666667">
have an ls : lt alignment model. The semi-Markov
property makes it possible for any target states to
align phrases on the source side, while the two di-
mensional state mapping makes it possible for any
source words to align phrases on the target side. For
instance, in Figure 1, the phrasal state a15 repre-
sents the two-word phrase closed down on the
target side, while still spanning for two words on the
source side, allowing a 2 × 2 alignment. State a15 is
phrasal, and at source word position 3 and 4 (span-
ning closed up) it is semi-Markovian. The final
decoding complexity is O(LsL2tMN2), a factor of
30 ∼ 60 times slower than the token-based model
(with a typical value of 3 or 4 for Ls and Lt).
In the following we describe features.
</bodyText>
<subsectionHeader confidence="0.994043">
3.3 Feature Design
</subsectionHeader>
<bodyText confidence="0.99994055">
We reused features in the original token-based
model based on string similarity, POS tags, position,
WordNet, distortion and context. Then we used an
additional chunker to mark phrase boundaries only
for feature extraction:
Chunking Features are binary indicators of
whether the phrase types of two phrases match.
Also, we added indicators for mappings between
source phrase types and target phrase types, such as
“vp2np”, meaning that a verb phrase in the source is
mapped to a noun phrase in the target.
Moreover, we introduced the following lexical
features:
PPDB Features (Ganitkevitch et al., 2013) in-
clude various similarity scores derived from a para-
phrase database with 73 million phrasal and 8 mil-
lion lexical paraphrases. Various paraphrase condi-
tional probability was employed. For instance, for
the ADJP/VP phrase pair capable of and able
to, there are the following minus-log probabilities:
</bodyText>
<equation confidence="0.986673">
p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0
p(e1|e2) = 1.3,p(e2|lhs) = 6.7,p(e2|e1) = 2.8
p(e1|e2, lhs) = 0.6,p(e2|e1, lhs) = 2.3
</equation>
<bodyText confidence="0.980062953488372">
where e1/e2 are the phrase pair, and lhs is the
left hand side syntactic non-terminal symbol. We
did not use the syntactic part (e.g., the NP of
NNS ↔ the NNS of NP) of PPDB as we did not
make the assumption that the input sentence pairs
were well-formed (and newswire-like) English, or
even of a language with a parser available. Also, for
phrasal alignments, we ruled out those paraphrases
spanning multiple syntactic structures, or of differ-
ent syntactic structures (indicated as [X] in PPDB),
for instance, and crazy ↔ , mad.
Semantic Relatedness Feature is a single scaled
number in [0, 1] from the best performing system
(Han et al., 2013) of the *Sem 2013 Semantic Tex-
tual Similarity (STS) task. We included this fea-
ture mainly to deal with cases where “related” words
cannot be well measured by either paraphrases or
distributional similarities. For instance, in one align-
ment dataset annotators aligned married with
wife. Adding a few other words as comparison, the
Han et al. (2013) system gives the following similar-
ity scores:
married/wife: 0.85
married/husband: 0.84
married/child: 0.10
married/stone: 0.01
Name Phylogeny Feature (Andrews et al., 2012)
is a similarity feature with a string transducer to
model how one name evolves to another. Examples
below show how similar is the name Bill associ-
ated with other names in log probability:
Bill/Bill: -0.8
Bill/Billy: -5.2
Bill/William: -13.6
Bill/Mary: -18.6
Finally, one decision we made during feature
design was not to use any parsing-based features,
with a permissive assumption that the input might
not be well-formed English, or even not complete
sentences (such as fragmented snippets from web
search). The “deepest” linguistic processing stays at
the level of tagging and chunking, making the model
more easily extendable to other languages.
</bodyText>
<subsectionHeader confidence="0.946283">
3.4 Feature Value
</subsectionHeader>
<bodyText confidence="0.9998975">
In this phrase-based model, the width of a state span
over the source words depends on the competition
between features fired on the phrases as a whole vs.
the consecutive but individual tokens. We found it
critical to assign feature values “fairly” among to-
kens and phrases to make sure that semi-Markov
states and phrasal states fire up often enough for
phrasal alignments.
</bodyText>
<page confidence="0.987001">
594
</page>
<table confidence="0.99955">
train test length %align.
MSR06 800 800 29/11 36%
Edinburgh++ 715 305 22/22 78%
</table>
<tableCaption confidence="0.998215">
Table 1: Statistics of the two manually aligned cor-
</tableCaption>
<bodyText confidence="0.946035137931034">
pora, divided into training and test in sentence pairs.
The length column shows average lengths of source
and target sentences in a pair. %align. is the per-
centage of aligned tokens.
To illustrate this in a simplified way, take
closed up↔closed down in Figure 1, and as-
sume the only feature is the normalized number of
matching tokens in the pair. Then this feature firing
on the following pairs would have values (the nor-
malization factor is the maximal phrase length):
closed↔closed 1.0
closed up↔closed 0.5
closed up↔up 0.5
closed up↔closed down 0.5
...↔... ...
The desired alignment closed up↔closed
down would not have survived the state com-
petition due to its weak feature value. In this
case the model would simply prefer a token align-
ment closed↔closed and up↔... (probably
NULL).
Thus we upweighted feature values by the max-
imum source or target phrase length to encour-
age phrasal alignments, in this case closed up
↔closed down:1.0. Then this alignment would
have a better chance to be picked out with additional
features, such as with the PPDB and Semantic Relat-
edness Features, which are also upweighted by max-
imum phrase lengths.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.997017">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999827">
There are two annotated datasets for training and
testing. MSR063 (Brockett, 2007) has annotated
alignments on the 2006 PASCAL RTE2 develop-
ment and test corpora, with 1600 pairs in total.
</bodyText>
<footnote confidence="0.986928">
3http://www.cs.biu.ac.il/˜nlp/files/RTE_
2006_Aligned.zip
</footnote>
<table confidence="0.996749">
1x1 1x2 1x3 2x2 2x3 3x3 more
MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8
EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1
</table>
<tableCaption confidence="0.984363">
Table 2: Percentage of various alignment sizes
</tableCaption>
<bodyText confidence="0.998544972222222">
(undirectional, e.g., 1x2 and 2x1 are merged) af-
ter synthesizing phrasal alignment from token align-
ment in the training portion of two corpora.
Semantically equivalent words and phrases in the
premise and hypothesis sentences are aligned in a
manner analogous to alignments in statistical ma-
chine translation. This dataset is asymmetric: on
average the premises contain 29 words and the hy-
potheses 11 words. Edinburgh++4 (Thadani et al.,
2012) is a revised version of the Edinburgh para-
phrase corpus(Cohn et al., 2008) with sentences
from the following resources: 1. the Multiple-
Translation Chinese corpus; 2. Jules Verne’s novel
Twenty Thousand Leagues Under the Sea. 3. the
Microsoft Research paraphrase corpus (Dolan et al.,
2004). The corpus is more balanced and symmetric:
the source and target sentences are both 22 words
long on average. Table 1 shows some statistics.
Both corpora contain mostly token-based align-
ment. For MSR06, MacCartney et al. (2008) showed
that setting the allowable phrase size to be greater
than one only increased F1 by 0.2%. For Ed-
inburgh++, the annotation guideline5 explicitly in-
structs to “prefer smaller alignments whenever pos-
sible”. Statistics shows that single token alignment
counts 96% and 95% of total alignments in these two
corpora separately. With such a heavy imbalance to-
wards only token-based alignment, a phrase-based
aligner would learn feature weights that award token
alignments more than phrasal alignments.
Thus we synthesized phrasal alignments from
continuous monotonic token alignments in these two
corpora. We first ran the OpenNLP chunker through
the corpora. Then for each phrase pair, if each token
in the source phrase is aligned to a token in the tar-
get phrase in a monotonic way, and vice versa, we
</bodyText>
<footnote confidence="0.9933005">
4http://www.ling.ohio-state.edu/˜scott/
#edinburgh-plusplus
5http://staffwww.dcs.shef.ac.uk/people/
T.Cohn/paraphrase_guidelines.pdf
</footnote>
<page confidence="0.9985">
595
</page>
<bodyText confidence="0.999927444444444">
merge these alignments to form one single phrasal
alignment.6 Table 2 lists the percentage of vari-
ous alignment sizes after the merge. Two obser-
vations can be made: first, the portion of phrasal
alignments increases to 10% — 20% after merging;
second, allowing a maximal phrase length of 3 cov-
ers 98% — 99% of total alignments, thus a phrase
length larger than 3 would be a bad trade-off for cov-
erage vs speed.
</bodyText>
<subsectionHeader confidence="0.996745">
4.2 Baselines and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.988576560000001">
MacCartney et al. (2008) and Yao et al. (2013a)
showed that the traditional MT bilingual aligner
GIZA++ (Och and Ney, 2003) presented weak re-
sults on the task of monolingual alignment. Thus
we instead used four other strong baselines:
Meteor (Denkowski and Lavie, 2011): a sys-
tem for evaluating machine translation by aligning
MT output with reference sentences. It is designed
for the task of monolingual alignment and supports
phrasal alignment. We used version 1.4 and default
weights to optimize by maximum accuracy.
MANLI-constraint (Thadani and McKeown,
2011): a re-implemented MANLI system with ILP-
powered decoding for speed and hard syntactic con-
straints to boost exact match rate, with reported
numbers on MSR06.
MANLI-joint (Thadani et al., 2012): an im-
proved version of MANLI-constraint that not only
models phrasal alignments, but also alignments be-
tween dependency arcs, with reported numbers on
the original Edinburgh paraphrase corpus.
jacana-token (Yao et al., 2013a): a token-
based aligner with state-of-the-art performance on
MSR06.
Note that the jacana-token aligner is open-source,
so we were able to re-train it with exactly the
same feature set used by our phrase-based model.
This allows a fair comparison of model performance
(token-based vs. phrase-based). The MANLI* sys-
tems are not available, thus we only reported their
numbers from published papers.
The standard evaluation metrics for alignments
are precision (P), recall (R), F1, and exact matching
6a few examples: two Atlanta-based
companies two Atlanta companies, the
UK+-+the UK, the 17-year-old+-+the teenager,
was held+-+was held.
rate (E) based on either tokens (two tokens are con-
sidered aligned iff they are aligned) or phrases (two
tokens are considered aligned iff they are contained
within phrases that are aligned). Following Thadani
et al. (2012), we only report the results based on
token alignments (which allows a partial credit if
their containing phrases are not aligned), even for
the phrase-based alignment task. The reasoning is
that if a phrase-based aligner is already doing bet-
ter than a token aligner in terms of token alignment
scores, then the difference in terms of phrase align-
ment scores will be even larger. Thus showing the
superiority of token alignment scores is sufficient.
</bodyText>
<subsectionHeader confidence="0.991477">
4.3 Implementation and Training
</subsectionHeader>
<bodyText confidence="0.999955">
The elements in the phrase-based model: dynamic
state indices, semi-Markov and phrasal states, are
not typically found in standard CRF implementa-
tions. Thus we implemented the phrase-based model
in the Scala programming language, which is fully
interoperable with Java, using one semi-Markov
CRF package7 as a reference. We used the L2 reg-
ularizer and LBFGS for optimization. OpenNLP8
provided the POS tagger and chunker and JWNL9
interfaced with WordNet (Fellbaum, 1998).
</bodyText>
<subsectionHeader confidence="0.56882">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999802529411765">
Table 3 gives scores (in bigger fonts) of different
aligners on MSR06 and Edinburgh++ and their cor-
responding phrasal versions. Overall, the token-
based aligner did the best on the original corpora, in
which single token alignment counts more than 95%
of total alignment. The phrase-based aligner did
slightly worse. We think the main reason was that it
output more phrasal alignment, which in turn harms
scores in token-based evaluation (for instance, if the
gold alignment is NewHNew, YorkHYork, then
the phrasal alignment of New YorkHNew York
would only have half the precision because it inher-
ently also aligns New in the source with York in
the target.). Further investigation showed that on the
Edinburgh++ corpus, over-generated phrase-based
alignment, when evaluated under just token align-
ment, contributed hurting about 1.1% of overall F1,
</bodyText>
<footnote confidence="0.999115">
7http://crf.sf.net
8http://opennlp.apache.org/
9http://jwordnet.sf.net/
</footnote>
<page confidence="0.998355">
596
</page>
<bodyText confidence="0.998172541666667">
a gap that would make the phrase aligner (85.9%)
outperform the token aligner (86.4%).
On the phrasal alignment corpora (represented by
MSR06P and EDB++P in Table 3), the phrase-based
aligner did significantly better. Note that the over-
all F1 and exact match rate are still much lower
than those scores obtained from the original corpora,
suggesting that the phrasal corpora present a much
harder task. Furthermore, as a more “fair” com-
parison between the two aligners, we synthesized
phrasal alignments from the output of the token-
based aligner, just as how the phrased-based corpora
were prepared, then evaluated its performance again.
Still, on the EDB++P corpus, the token aligner was
about 1.6% (current difference is 69.1% vs. 72.8%)
worse than the phrase-based aligner.
Also, we want to emphasize that since the token-
based aligner and the phrase-based aligner shared
exactly the same features and lexical resources, the
performance boost of the phrase-based aligner on
the phrasal corpora results from a better model de-
sign: it is the semi-Markov property and phrasal
states making the phrase-based aligner better.
To further investigate the performance of aligners
with respect to different types of alignment, we di-
vided the scores into those for identical alignments
(such as NewHNew) and non-identical alignments
(such as wifeHspouse), indicated by the sub-
scripts i and n in Table 3. In terms of identical
alignment, most aligners were able to score more
than 90%, but for non-identical alignment there was
noticeable decrease. Still, on the phrasal alignment
corpora, the phrase-based model has a much larger
recall score for non-identical alignment than others.
We also divided scores with respect to token-only
alignment and phrase-only alignment. Due to space
limit, we only show results on synthesized Edin-
burgh++, in Table 4. Meteor and the token aligner
inherently have either very limited or no support for
phrasal alignment, thus they had very low scores
on phrase-only alignment. We then ran the align-
ers in two directions and merged the results with the
“union” MT heuristic to get better phrase support.
But that still did not bring F1p’s up to over 5%.
The phrase-based aligner baseline Meteor did
worse than our aligners. We think there are two rea-
sons: First, Meteor was not trained on these corpora.
Second, Meteor only does strict word, stem, syn-
</bodyText>
<table confidence="0.999773666666667">
P% R% F1% E%
System Ri/Rn F1i/F1n
Pi/Pn
82.5 81.2 81.9 15.0
Meteor
89.9/39.9 97.3/24.6 93.5/30.5
MANLI-cons. 89.5 86.2 87.8 33.0
93.6 83.5 88.3 32.1
token
96.6/77.7 96.9/35.6 96.8/48.8
phrase 92.1 82.8 86.8 29.1
95.7/65.0 95.9/34.7 95.8/45.2
82.5 68.3 74.7
Meteor
7.3
89.9/40.1 97.3/8.8 93.5/14.5
92.9 66.1 77.2
token 13.5
95.5/77.5 94.3/11.1 94.9/19.5
83.5 77.0 80.1
phrase 14.3
94.9/55.5 94.2/48.1 94.5/51.5
88.3 80.5 84.2
Meteor
12.7
94.0/61.4 97.8/24.1 95.9/34.7
MANLI-jnt* 76.6 83.8 79.2 12.2
91.3 82.0 86.4
token
15.0
96.4/63.9 97.4/36.4 96.9/46.4
phrase 90.4 81.9 85.9 13.7
96.0/57.4 97.8/38.3 96.9/46.0
88.4 60.6 71.9
Meteor 2.9
94.0/61.9 97.0/6.5 95.5/11.7
90.7 55.8 69.1
token 2.3
96.2/58.6 91.3/7.1 93.7/12.7
82.3 65.3 72.8
phrase 1.6
95.6/60.4 93.1/34.3 94.4/43.8
</table>
<tableCaption confidence="0.99975">
Table 3: Results on original (mostly token) and phrasal
</tableCaption>
<bodyText confidence="0.953570083333333">
(P) alignment corpora, where (x%) indicates how much
alignment is identical alignment, such as NewHNew. E%
stands for exact (perfect) match rate. Subscript i stands
for corresponding scores for “identical” alignment and n
for “non-identical”. *: scores of MANLI-joint were for
the original Edinburgh corpus instead of Edinburgh++
(with hand corrections) so it is not a direct comparison.
onym and paraphrase matching but does not use any
string similarity measures; this can be supported by
the large difference between, for instance, F1z and
F1�. In general Meteor did well on identical align-
ment, but not so well on non-identical alignment.
</bodyText>
<sectionHeader confidence="0.993214" genericHeader="method">
5 Applications
</sectionHeader>
<bodyText confidence="0.985234">
Natural language alignment can be applied to vari-
ous NLP tasks. While how to most effectively apply
</bodyText>
<equation confidence="0.979581">
EDB++P (51.7%)
MSR06 (78.6%)
MSR06P (59.0%)
EDB++ (75.2%)
</equation>
<page confidence="0.989933">
597
</page>
<table confidence="0.983271222222222">
System P% R% F1% E%
Pt/Pp Rt/Rp F1t/F1p
88.4 60.6 71.9
Meteor 2.9
59.5/14.9 90.6/1.1 71.8/2.0
90.7 55.8 69.1
token 2.3
59.4/21.4 85.5/0.9 70.1/1.7
phrase 82.3 65.3 72.8 1.6
73.3/48.0 73.5/44.2 73.4/46.0
system A% P% R%
de Marneffe et al. (2006) 60.5 61.8 60.2
MacCartney and Manning (2008) 64.3 65.5 63.9
Heilman and Smith (2010) 62.8 61.9 71.2
the token aligner 59.1 61.2 55.4
our phrasal aligner 57.6 57.2 68.8
(a) Recognizing Textual Entailment
EDB++P
</table>
<tableCaption confidence="0.900543666666667">
Table 4: Same results on the phrasal Edinburgh++ cor-
pus but with scores divided by token-only alignment
(subscript t) and phrase-only alignment (subscript p).
</tableCaption>
<bodyText confidence="0.990464848484848">
it is another topic, we simply show in this section us-
ing just alignment scores in binary prediction prob-
lems. Specifically, we pick the tasks of recognizing
textual entailment (RTE), paraphrase identification
(PP), and question answering sentence ranking (QA)
described in Heilman and Smith (2010):
RTE: predicting whether a hypothesis can be in-
ferred from the premise, with training data from
RTE-1/2 and RTE-3 dev, and test from RTE-3 test.
PP: predicting whether two sentences are para-
phrases, with training and test data from the MSR
Paraphrase Corpus (Dolan et al., 2004).
QA: predicting whether a sentence contains the
answer to the question, with training data from
TREC-8 to TREC-12 and test data from TREC-13.
For each aligned pair, we can compute a normal-
ized decoding score. Following MacCartney et al.
(2008), we select a threshold score and predict true
if the decoding score is above this threshold. For the
tasks of RTE and PP, we tuned this threshold w.r.t
the maximal accuracy on the training set, then re-
ported performance on the test set. For the task of
QA, since the evaluation methods in Mean Average
Precision and Mean Reciprocal Rank only need a
ranked list of answer sentences, and the scores on
the test set are sufficient to provide the ranking, we
did not tune anything on training but instead directly
ran the aligner on the test set. All three tasks shared
the same aligner model trained on the superset of
MSR06 and Edinburgh++. Results are reported in
Table 5. We could not report on Meteor as Meteor
does not explicitly output alignment scores.
We did not expect the aligners to beat any of the
</bodyText>
<table confidence="0.9916428">
system A% P% R%
Wan et al. (2006) 75.6 77 90
Das and Smith (2009) 73.9 74.9 91.3
Heilman and Smith (2010) 73.2 75.7 87.8
the token aligner 70.0 72.6 88.1
our phrasal aligner 68.1 68.6 95.8
(b) Paraphrase Identification
system MAP MRR
Cui et al. (2005) 0.4271 0.5259
Wang et al. (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Yao et al. (2013b) 0.6307 0.7477
the token aligner 0.5982 0.6582
our phrasal aligner 0.6165 0.7333
(c) Question Answering Sentence Ranking
</table>
<tableCaption confidence="0.692346666666667">
Table 5: Results (Accuracy, Precision, Recall, Mean
Average Precision, Mean Reciprocal Rank) on the
tasks of RTE, PP and QA.
</tableCaption>
<bodyText confidence="0.999583875">
state-of-the-art result since no sophisticated models
were additionally used but only the alignment score.
Still, the aligners showed competitive performance.
It still follows the pattern from the alignment exper-
iment that the phrasal aligner had higher recall and
lower precision than the token aligner in the task of
RTE and PP. In the QA task, the phrasal aligner per-
formed better than all systems except for the top one.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999516">
We have introduced a phrase-to-phrase alignment
model based on semi-Markov Conditional Random
Fields. The combination of semi-Markov states and
phrasal states makes phrasal alignment on both the
source and target sides possible. The final phrase-
</bodyText>
<page confidence="0.993928">
598
</page>
<bodyText confidence="0.9999658">
based aligner performed the best on two phrasal
alignment corpora and showed its potential usage
in three NLP tasks. Future work includes aligning
discontinuous (gappy) phrases and integrating align-
ment more closely in NLP applications.
</bodyText>
<sectionHeader confidence="0.954503" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9987205">
We thank Vulcan Inc. for funding this work. We also
thank Jason Smith, Travis Wolfe, Frank Ferraro and
the three anonymous reviewers for their comments
and suggestion.
</bodyText>
<sectionHeader confidence="0.998647" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999424184782609">
Jes´us Andr´es-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Procedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: a generative model of string
variation. In Proceedings of EMNLP 2012.
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL, pages
16–23.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL2006, pages 65–72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263–311.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165–170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597–614, December.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ’05, pages 400–407, New York, NY,
USA. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468–476, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D Manning. 2006. Learning to distinguish valid
textual entailments. In Second Pascal RTE Challenge
Workshop.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. Audio, Speech, and Language Processing, IEEE
Transactions on, 16(3):494–507.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, Stroudsburg, PA, USA.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages 758–
764.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin CRFs: training log-linear models with cost
functions. In NAACL 2010, pages 733–736.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. UMBC-EBIQUITY-
CORE: Semantic Textual Similarity Systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011–1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17–20.
</reference>
<page confidence="0.986037">
599
</page>
<reference confidence="0.999894142857143">
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of ACL 2008,
pages 521–528.
Bill MacCartney, Michel Galley, and Christopher D Man-
ning. 2008. A phrase-based alignment model for nat-
ural language inference. In Proceedings of EMNLP,
pages 802–811.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP-2002, pages 133–139.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289–292.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19–51.
Mari Ostendorf, Vassilios V Digalakis, and Owen A Kim-
ball. 1996. From HMM’s to segment models: a uni-
fied view of stochastic modeling for speech recogni-
tion. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360–378.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL, pages 102–109.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning
predicates across monolingual comparable texts using
graph-based clustering. In Proceedings of EMNLP-
CoNLL, pages 171–182, Jeju Island, Korea, July.
Sarawagi Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems, 17:1185–1192.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings ofACL short.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A joint phrasal and dependency model for paraphrase
alignment. In Proceedings of COLING 2012: Posters,
pages 1229–1238, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics - Volume 2, COLING ’96, pages
836–841.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris.
2006. Using dependency-based features to take the
“para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING, pages 1164–1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL, pages 22–32, Prague, Czech Repub-
lic, June.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013a. A Lightweight and
High Performance Monolingual Word Aligner. In
Proceedings of ACL 2013 short, Sofia, Bulgaria.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013b. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
</reference>
<page confidence="0.99675">
600
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828273">
<title confidence="0.999184">Semi-Markov Phrase-based Monolingual Alignment</title>
<author confidence="0.999901">Yao Van_Durme</author>
<affiliation confidence="0.999875">Johns Hopkins University</affiliation>
<address confidence="0.921424">Baltimore, MD, USA Clark</address>
<affiliation confidence="0.998276">University of Pennsylvania Allen Institute for Artificial Intelligence</affiliation>
<address confidence="0.998288">Philadelphia, PA, USA Seattle, WA, USA</address>
<abstract confidence="0.999038071428571">We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jes´us Andr´es-Ferrer</author>
<author>Alfons Juan</author>
</authors>
<title>A phrasebased hidden semi-markov approach to machine translation.</title>
<date>2009</date>
<booktitle>In Procedings of European Association for Machine Translation (EAMT),</booktitle>
<location>Barcelona, Spain,</location>
<marker>Andr´es-Ferrer, Juan, 2009</marker>
<rawString>Jes´us Andr´es-Ferrer and Alfons Juan. 2009. A phrasebased hidden semi-markov approach to machine translation. In Procedings of European Association for Machine Translation (EAMT), Barcelona, Spain, May. European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Andrews</author>
<author>Jason Eisner</author>
<author>Mark Dredze</author>
</authors>
<title>Name phylogeny: a generative model of string variation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="17910" citStr="Andrews et al., 2012" startWordPosition="2978" endWordPosition="2981">edness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves to another. Examples below show how similar is the name Bill associated with other names in log probability: Bill/Bill: -0.8 Bill/Billy: -5.2 Bill/William: -13.6 Bill/Mary: -18.6 Finally, one decision we made during feature design was not to use any parsing-based features, with a permissive assumption that the input might not be well-formed English, or even not complete sentences (such as fragmented snippets from web search). The “deepest” linguistic processing stays at the level of tagging and chunking, making the</context>
</contexts>
<marker>Andrews, Eisner, Dredze, 2012</marker>
<rawString>Nicholas Andrews, Jason Eisner, and Mark Dredze. 2012. Name phylogeny: a generative model of string variation. In Proceedings of EMNLP 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Chris Quirk</author>
<author>Robert Moore</author>
</authors>
<title>Gappy phrasal alignment by agreement.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, Oregon,</location>
<contexts>
<context position="6583" citStr="Bansal et al. (2011)" startWordPosition="974" endWordPosition="977">h sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both </context>
</contexts>
<marker>Bansal, Quirk, Moore, 2011</marker>
<rawString>Mohit Bansal, Chris Quirk, and Robert Moore. 2011. Gappy phrasal alignment by agreement. In Proceedings of ACL, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="5360" citStr="Barzilay and Lee, 2003" startWordPosition="793" endWordPosition="796">les phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in </context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings of NAACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL2006,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2642" citStr="Blunsom and Cohn (2006)" startWordPosition="386" endWordPosition="389"> to the target words: NYCHNew, NYCHYork, NYCHCity. Or in the case of identical alignment, New York City aligning to New York City is simply NewHNew, YorkHYork, CityHCity. However, it is not as clear how to token-align New York (as a city) with New York City. The problem is more prominent when aligning phrasal paraphrases ormultiword expressions, such as pass away and kick the bucket. This suggests an intrinsically phrase-based alignment model. The token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as “observation” and target tokens as “hidden states”. However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase). We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus a</context>
<context position="7782" citStr="Blunsom and Cohn, 2006" startWordPosition="1157" endWordPosition="1160"> the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Conditional Random Field (Sarawagi and Cohen, 2004), which was first used in information extraction to tag continuous segments of input sequences and outperformed conventional CRFs in the task of named entity recognition. We describe this model in the following section. 3 The Alignment Model Our objective is to define a model that supports phrase-based alignment of arbitrary phrase length. In this section we first describe a regular CRF model that supports one-to-one token-based alignment (Blunsom and Cohn, 2006; Yao et al., 2013a), then extend it to phrase-based alignment with the semi-Markov model. 591 3.1 Token-based Model Given a source sentence s of length M, and a target sentence t of length N, the alignment from s to t is a sequence of target word indices a, where aic[1,M] E [0, N]. We specify that when ai = 0, source word si is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target: multiple source words can be aligned to the same target word, but not vice versa. One-to-many alignment can be obtained by running the aligner in the other direction. The</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Proceedings of ACL2006, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="20348" citStr="Brockett, 2007" startWordPosition="3380" endWordPosition="3381">ompetition due to its weak feature value. In this case the model would simply prefer a token alignment closed↔closed and up↔... (probably NULL). Thus we upweighted feature values by the maximum source or target phrase length to encourage phrasal alignments, in this case closed up ↔closed down:1.0. Then this alignment would have a better chance to be picked out with additional features, such as with the PPDB and Semantic Relatedness Features, which are also upweighted by maximum phrase lengths. 4 Experiment 4.1 Data Preparation There are two annotated datasets for training and testing. MSR063 (Brockett, 2007) has annotated alignments on the 2006 PASCAL RTE2 development and test corpora, with 1600 pairs in total. 3http://www.cs.biu.ac.il/˜nlp/files/RTE_ 2006_Aligned.zip 1x1 1x2 1x3 2x2 2x3 3x3 more MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8 EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1 Table 2: Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora. Semantically equivalent words and phrases in the premise and hypothesis sentences are aligned in a manner analogous to alignments in statistical m</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 corpus. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5479" citStr="Brown et al., 1993" startWordPosition="814" endWordPosition="817">e. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Cer</author>
<author>Trond Grenager</author>
<author>David Hall</author>
<author>Chloe Kiddon</author>
<author>Bill MacCartney</author>
<author>MarieCatherine de Marneffe</author>
<author>Daniel Ramage</author>
<author>Eric Yeh</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>165--170</pages>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, MarieCatherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 165–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing corpora for the development and evaluation of paraphrase systems.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="21176" citStr="Cohn et al., 2008" startWordPosition="3510" endWordPosition="3513">3 5.7 0.0 1.9 0.8 EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1 Table 2: Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora. Semantically equivalent words and phrases in the premise and hypothesis sentences are aligned in a manner analogous to alignments in statistical machine translation. This dataset is asymmetric: on average the premises contain 29 words and the hypotheses 11 words. Edinburgh++4 (Thadani et al., 2012) is a revised version of the Edinburgh paraphrase corpus(Cohn et al., 2008) with sentences from the following resources: 1. the MultipleTranslation Chinese corpus; 2. Jules Verne’s novel Twenty Thousand Leagues Under the Sea. 3. the Microsoft Research paraphrase corpus (Dolan et al., 2004). The corpus is more balanced and symmetric: the source and target sentences are both 22 words long on average. Table 1 shows some statistics. Both corpora contain mostly token-based alignment. For MSR06, MacCartney et al. (2008) showed that setting the allowable phrase size to be greater than one only increased F1 by 0.2%. For Edinburgh++, the annotation guideline5 explicitly instr</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing corpora for the development and evaluation of paraphrase systems. Computational Linguistics, 34(4):597–614, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>TatSeng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05,</booktitle>
<pages>400--407</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="33259" citStr="Cui et al. (2005)" startWordPosition="5418" endWordPosition="5421">e ranking, we did not tune anything on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on the superset of MSR06 and Edinburgh++. Results are reported in Table 5. We could not report on Meteor as Meteor does not explicitly output alignment scores. We did not expect the aligners to beat any of the system A% P% R% Wan et al. (2006) 75.6 77 90 Das and Smith (2009) 73.9 74.9 91.3 Heilman and Smith (2010) 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 (b) Paraphrase Identification system MAP MRR Cui et al. (2005) 0.4271 0.5259 Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Yao et al. (2013b) 0.6307 0.7477 the token aligner 0.5982 0.6582 our phrasal aligner 0.6165 0.7333 (c) Question Answering Sentence Ranking Table 5: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA. state-of-the-art result since no sophisticated models were additionally used but only the alignment score. Still, the aligners showed competitive performance. It still follows the pattern from the alignment experiment that the phrasal aligner had hi</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and TatSeng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>468--476</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="33073" citStr="Das and Smith (2009)" startWordPosition="5386" endWordPosition="5389"> QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked list of answer sentences, and the scores on the test set are sufficient to provide the ranking, we did not tune anything on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on the superset of MSR06 and Edinburgh++. Results are reported in Table 5. We could not report on Meteor as Meteor does not explicitly output alignment scores. We did not expect the aligners to beat any of the system A% P% R% Wan et al. (2006) 75.6 77 90 Das and Smith (2009) 73.9 74.9 91.3 Heilman and Smith (2010) 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 (b) Paraphrase Identification system MAP MRR Cui et al. (2005) 0.4271 0.5259 Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Yao et al. (2013b) 0.6307 0.7477 the token aligner 0.5982 0.6582 our phrasal aligner 0.6165 0.7333 (c) Question Answering Sentence Ranking Table 5: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA. state-of-the-art result since no sophisticated models were addi</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 468–476, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Second Pascal RTE Challenge Workshop.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D Manning. 2006. Learning to distinguish valid textual entailments. In Second Pascal RTE Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation. Audio, Speech, and Language Processing,</title>
<date>2008</date>
<journal>IEEE Transactions on,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="6054" citStr="Deng and Byrne (2008)" startWordPosition="893" endWordPosition="896">anslation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Lia</context>
</contexts>
<marker>Deng, Byrne, 2008</marker>
<rawString>Yonggang Deng and William Byrne. 2008. HMM word and phrase alignment for statistical machine translation. Audio, Speech, and Language Processing, IEEE Transactions on, 16(3):494–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="23277" citStr="Denkowski and Lavie, 2011" startWordPosition="3836" endWordPosition="3839">of various alignment sizes after the merge. Two observations can be made: first, the portion of phrasal alignments increases to 10% — 20% after merging; second, allowing a maximal phrase length of 3 covers 98% — 99% of total alignments, thus a phrase length larger than 3 would be a bad trade-off for coverage vs speed. 4.2 Baselines and Evaluation Metrics MacCartney et al. (2008) and Yao et al. (2013a) showed that the traditional MT bilingual aligner GIZA++ (Och and Ney, 2003) presented weak results on the task of monolingual alignment. Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences. It is designed for the task of monolingual alignment and supports phrasal alignment. We used version 1.4 and default weights to optimize by maximum accuracy. MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding for speed and hard syntactic constraints to boost exact match rate, with reported numbers on MSR06. MANLI-joint (Thadani et al., 2012): an improved version of MANLI-constraint that not only models phrasal alignments, but also alignments between de</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21391" citStr="Dolan et al., 2004" startWordPosition="3542" endWordPosition="3545">ining portion of two corpora. Semantically equivalent words and phrases in the premise and hypothesis sentences are aligned in a manner analogous to alignments in statistical machine translation. This dataset is asymmetric: on average the premises contain 29 words and the hypotheses 11 words. Edinburgh++4 (Thadani et al., 2012) is a revised version of the Edinburgh paraphrase corpus(Cohn et al., 2008) with sentences from the following resources: 1. the MultipleTranslation Chinese corpus; 2. Jules Verne’s novel Twenty Thousand Leagues Under the Sea. 3. the Microsoft Research paraphrase corpus (Dolan et al., 2004). The corpus is more balanced and symmetric: the source and target sentences are both 22 words long on average. Table 1 shows some statistics. Both corpora contain mostly token-based alignment. For MSR06, MacCartney et al. (2008) showed that setting the allowable phrase size to be greater than one only increased F1 by 0.2%. For Edinburgh++, the annotation guideline5 explicitly instructs to “prefer smaller alignments whenever possible”. Statistics shows that single token alignment counts 96% and 95% of total alignments in these two corpora separately. With such a heavy imbalance towards only to</context>
<context position="31956" citStr="Dolan et al., 2004" startWordPosition="5186" endWordPosition="5189"> and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting whether a hypothesis can be inferred from the premise, with training data from RTE-1/2 and RTE-3 dev, and test from RTE-3 test. PP: predicting whether two sentences are paraphrases, with training and test data from the MSR Paraphrase Corpus (Dolan et al., 2004). QA: predicting whether a sentence contains the answer to the question, with training data from TREC-8 to TREC-12 and test data from TREC-13. For each aligned pair, we can compute a normalized decoding score. Following MacCartney et al. (2008), we select a threshold score and predict true if the decoding score is above this threshold. For the tasks of RTE and PP, we tuned this threshold w.r.t the maximal accuracy on the training set, then reported performance on the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of COLING, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<contexts>
<context position="25801" citStr="Fellbaum, 1998" startWordPosition="4225" endWordPosition="4226">ase alignment scores will be even larger. Thus showing the superiority of token alignment scores is sufficient. 4.3 Implementation and Training The elements in the phrase-based model: dynamic state indices, semi-Markov and phrasal states, are not typically found in standard CRF implementations. Thus we implemented the phrase-based model in the Scala programming language, which is fully interoperable with Java, using one semi-Markov CRF package7 as a reference. We used the L2 regularizer and LBFGS for optimization. OpenNLP8 provided the POS tagger and chunker and JWNL9 interfaced with WordNet (Fellbaum, 1998). 4.4 Results Table 3 gives scores (in bigger fonts) of different aligners on MSR06 and Edinburgh++ and their corresponding phrasal versions. Overall, the tokenbased aligner did the best on the original corpora, in which single token alignment counts more than 95% of total alignment. The phrase-based aligner did slightly worse. We think the main reason was that it output more phrasal alignment, which in turn harms scores in token-based evaluation (for instance, if the gold alignment is NewHNew, YorkHYork, then the phrasal alignment of New YorkHNew York would only have half the precision becaus</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The Paraphrase Database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>758--764</pages>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In Proceedings of NAACL-HLT, pages 758– 764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmaxmargin CRFs: training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In NAACL 2010,</booktitle>
<pages>733--736</pages>
<contexts>
<context position="8717" citStr="Gimpel and Smith, 2010" startWordPosition="1323" endWordPosition="1326"> source word si is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target: multiple source words can be aligned to the same target word, but not vice versa. One-to-many alignment can be obtained by running the aligner in the other direction. The probability of alignment sequence a conditioned on both s and t is then: exp(Ei,k λkfk(ai−1, ai, s, t)) Z(s, t) This assumes a first-order Conditional Random Field (Lafferty et al., 2001). Since the word alignment task is evaluated over F1, instead of directly optimizing it, we choose a much easier objective (Gimpel and Smith, 2010) and add a cost function to the normalizing function Z(s, t) in the denominator: Z(s, t) = � �exp( λkfk(ai−1, ai, s, t) a i,k +cost(ay, ˆa)) where ay is the true alignments. cost(ay, ˆa) can be viewed as special “features” that encourage decoding to be consistent with true labels. It is only computed during training in the denominator because in the numerator cost(ay, ay) = 0. Hamming cost is used in practice without learning the weights (i.e., uniform weights). The more inconsistence there is between ay and ˆa, the more penalized is the decoding sequence aˆ through the cost function. 3.2 Phra</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010. Softmaxmargin CRFs: training log-linear models with cost functions. In NAACL 2010, pages 733–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC-EBIQUITYCORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="17390" citStr="Han et al., 2013" startWordPosition="2899" endWordPosition="2902">air, and lhs is the left hand side syntactic non-terminal symbol. We did not use the syntactic part (e.g., the NP of NNS ↔ the NNS of NP) of PPDB as we did not make the assumption that the input sentence pairs were well-formed (and newswire-like) English, or even of a language with a parser available. Also, for phrasal alignments, we ruled out those paraphrases spanning multiple syntactic structures, or of different syntactic structures (indicated as [X] in PPDB), for instance, and crazy ↔ , mad. Semantic Relatedness Feature is a single scaled number in [0, 1] from the best performing system (Han et al., 2013) of the *Sem 2013 Semantic Textual Similarity (STS) task. We included this feature mainly to deal with cases where “related” words cannot be well measured by either paraphrases or distributional similarities. For instance, in one alignment dataset annotators aligned married with wife. Adding a few other words as comparison, the Han et al. (2013) system gives the following similarity scores: married/wife: 0.85 married/husband: 0.84 married/child: 0.10 married/stone: 0.01 Name Phylogeny Feature (Andrews et al., 2012) is a similarity feature with a string transducer to model how one name evolves </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC-EBIQUITYCORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<pages>1011--1019</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="4102" citStr="Heilman and Smith, 2010" startWordPosition="610" endWordPosition="613"> on Empirical Methods in Natural Language Processing, pages 590–600, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron lea</context>
<context position="31094" citStr="Heilman and Smith (2010)" startWordPosition="5050" endWordPosition="5053">instance, F1z and F1�. In general Meteor did well on identical alignment, but not so well on non-identical alignment. 5 Applications Natural language alignment can be applied to various NLP tasks. While how to most effectively apply EDB++P (51.7%) MSR06 (78.6%) MSR06P (59.0%) EDB++ (75.2%) 597 System P% R% F1% E% Pt/Pp Rt/Rp F1t/F1p 88.4 60.6 71.9 Meteor 2.9 59.5/14.9 90.6/1.1 71.8/2.0 90.7 55.8 69.1 token 2.3 59.4/21.4 85.5/0.9 70.1/1.7 phrase 82.3 65.3 72.8 1.6 73.3/48.0 73.5/44.2 73.4/46.0 system A% P% R% de Marneffe et al. (2006) 60.5 61.8 60.2 MacCartney and Manning (2008) 64.3 65.5 63.9 Heilman and Smith (2010) 62.8 61.9 71.2 the token aligner 59.1 61.2 55.4 our phrasal aligner 57.6 57.2 68.8 (a) Recognizing Textual Entailment EDB++P Table 4: Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting</context>
<context position="33113" citStr="Heilman and Smith (2010)" startWordPosition="5393" endWordPosition="5396"> Mean Average Precision and Mean Reciprocal Rank only need a ranked list of answer sentences, and the scores on the test set are sufficient to provide the ranking, we did not tune anything on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on the superset of MSR06 and Edinburgh++. Results are reported in Table 5. We could not report on Meteor as Meteor does not explicitly output alignment scores. We did not expect the aligners to beat any of the system A% P% R% Wan et al. (2006) 75.6 77 90 Das and Smith (2009) 73.9 74.9 91.3 Heilman and Smith (2010) 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 (b) Paraphrase Identification system MAP MRR Cui et al. (2005) 0.4271 0.5259 Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Yao et al. (2013b) 0.6307 0.7477 the token aligner 0.5982 0.6582 our phrasal aligner 0.6165 0.7333 (c) Question Answering Sentence Ranking Table 5: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA. state-of-the-art result since no sophisticated models were additionally used but only the alignment sco</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proceedings of NAACL 2010, pages 1011–1019, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5603" citStr="Koehn, 2010" startWordPosition="832" endWordPosition="833">ers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase length</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges on RTE,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="4077" citStr="Kouylekov and Magnini, 2005" startWordPosition="606" endWordPosition="609">edings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590–600, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies disc</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In PASCAL Challenges on RTE, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2705" citStr="Lafferty et al., 2001" startWordPosition="396" endWordPosition="399">e of identical alignment, New York City aligning to New York City is simply NewHNew, YorkHYork, CityHCity. However, it is not as clear how to token-align New York (as a city) with New York City. The problem is more prominent when aligning phrasal paraphrases ormultiword expressions, such as pass away and kick the bucket. This suggests an intrinsically phrase-based alignment model. The token aligner jacana-align (Yao et al., 2013a) has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of Blunsom and Cohn (2006). It employs a Conditional Random Field (Lafferty et al., 2001) to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as “observation” and target tokens as “hidden states”. However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase). We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus aligning phrases on the source side. Also, we merge phrases on t</context>
<context position="8570" citStr="Lafferty et al., 2001" startWordPosition="1298" endWordPosition="1301">et sentence t of length N, the alignment from s to t is a sequence of target word indices a, where aic[1,M] E [0, N]. We specify that when ai = 0, source word si is aligned to a NULL state, i.e., deleted. This models a many-to-one alignment from source to target: multiple source words can be aligned to the same target word, but not vice versa. One-to-many alignment can be obtained by running the aligner in the other direction. The probability of alignment sequence a conditioned on both s and t is then: exp(Ei,k λkfk(ai−1, ai, s, t)) Z(s, t) This assumes a first-order Conditional Random Field (Lafferty et al., 2001). Since the word alignment task is evaluated over F1, instead of directly optimizing it, we choose a much easier objective (Gimpel and Smith, 2010) and add a cost function to the normalizing function Z(s, t) in the denominator: Z(s, t) = � �exp( λkfk(ai−1, ai, s, t) a i,k +cost(ay, ˆa)) where ay is the true alignments. cost(ay, ˆa) can be viewed as special “features” that encourage decoding to be consistent with true labels. It is only computed during training in the denominator because in the numerator cost(ay, ay) = 0. Hamming cost is used in practice without learning the weights (i.e., unif</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6670" citStr="Liang et al., 2006" startWordPosition="988" endWordPosition="991">08) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Condi</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>521--528</pages>
<contexts>
<context position="31054" citStr="MacCartney and Manning (2008)" startWordPosition="5043" endWordPosition="5046">pported by the large difference between, for instance, F1z and F1�. In general Meteor did well on identical alignment, but not so well on non-identical alignment. 5 Applications Natural language alignment can be applied to various NLP tasks. While how to most effectively apply EDB++P (51.7%) MSR06 (78.6%) MSR06P (59.0%) EDB++ (75.2%) 597 System P% R% F1% E% Pt/Pp Rt/Rp F1t/F1p 88.4 60.6 71.9 Meteor 2.9 59.5/14.9 90.6/1.1 71.8/2.0 90.7 55.8 69.1 token 2.3 59.4/21.4 85.5/0.9 70.1/1.7 phrase 82.3 65.3 72.8 1.6 73.3/48.0 73.5/44.2 73.4/46.0 system A% P% R% de Marneffe et al. (2006) 60.5 61.8 60.2 MacCartney and Manning (2008) 64.3 65.5 63.9 Heilman and Smith (2010) 62.8 61.9 71.2 the token aligner 59.1 61.2 55.4 our phrasal aligner 57.6 57.2 68.8 (a) Recognizing Textual Entailment EDB++P Table 4: Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment (subscript p). it is another topic, we simply show in this section using just alignment scores in binary prediction problems. Specifically, we pick the tasks of recognizing textual entailment (RTE), paraphrase identification (PP), and question answering sentence ranking (QA) described in H</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>Bill MacCartney and Christopher D Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of ACL 2008, pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>802--811</pages>
<contexts>
<context position="4491" citStr="MacCartney et al., 2008" startWordPosition="666" endWordPosition="669">d aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can b</context>
<context position="21620" citStr="MacCartney et al. (2008)" startWordPosition="3579" endWordPosition="3582">on average the premises contain 29 words and the hypotheses 11 words. Edinburgh++4 (Thadani et al., 2012) is a revised version of the Edinburgh paraphrase corpus(Cohn et al., 2008) with sentences from the following resources: 1. the MultipleTranslation Chinese corpus; 2. Jules Verne’s novel Twenty Thousand Leagues Under the Sea. 3. the Microsoft Research paraphrase corpus (Dolan et al., 2004). The corpus is more balanced and symmetric: the source and target sentences are both 22 words long on average. Table 1 shows some statistics. Both corpora contain mostly token-based alignment. For MSR06, MacCartney et al. (2008) showed that setting the allowable phrase size to be greater than one only increased F1 by 0.2%. For Edinburgh++, the annotation guideline5 explicitly instructs to “prefer smaller alignments whenever possible”. Statistics shows that single token alignment counts 96% and 95% of total alignments in these two corpora separately. With such a heavy imbalance towards only token-based alignment, a phrase-based aligner would learn feature weights that award token alignments more than phrasal alignments. Thus we synthesized phrasal alignments from continuous monotonic token alignments in these two corp</context>
<context position="23032" citStr="MacCartney et al. (2008)" startWordPosition="3796" endWordPosition="3799">vice versa, we 4http://www.ling.ohio-state.edu/˜scott/ #edinburgh-plusplus 5http://staffwww.dcs.shef.ac.uk/people/ T.Cohn/paraphrase_guidelines.pdf 595 merge these alignments to form one single phrasal alignment.6 Table 2 lists the percentage of various alignment sizes after the merge. Two observations can be made: first, the portion of phrasal alignments increases to 10% — 20% after merging; second, allowing a maximal phrase length of 3 covers 98% — 99% of total alignments, thus a phrase length larger than 3 would be a bad trade-off for coverage vs speed. 4.2 Baselines and Evaluation Metrics MacCartney et al. (2008) and Yao et al. (2013a) showed that the traditional MT bilingual aligner GIZA++ (Och and Ney, 2003) presented weak results on the task of monolingual alignment. Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences. It is designed for the task of monolingual alignment and supports phrasal alignment. We used version 1.4 and default weights to optimize by maximum accuracy. MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding fo</context>
<context position="32200" citStr="MacCartney et al. (2008)" startWordPosition="5226" endWordPosition="5229">entification (PP), and question answering sentence ranking (QA) described in Heilman and Smith (2010): RTE: predicting whether a hypothesis can be inferred from the premise, with training data from RTE-1/2 and RTE-3 dev, and test from RTE-3 test. PP: predicting whether two sentences are paraphrases, with training and test data from the MSR Paraphrase Corpus (Dolan et al., 2004). QA: predicting whether a sentence contains the answer to the question, with training data from TREC-8 to TREC-12 and test data from TREC-13. For each aligned pair, we can compute a normalized decoding score. Following MacCartney et al. (2008), we select a threshold score and predict true if the decoding score is above this threshold. For the tasks of RTE and PP, we tuned this threshold w.r.t the maximal accuracy on the training set, then reported performance on the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked list of answer sentences, and the scores on the test set are sufficient to provide the ranking, we did not tune anything on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP, pages 802–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="5788" citStr="Marcu and Wong (2002)" startWordPosition="856" endWordPosition="859"> sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of EMNLP-2002, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
</authors>
<title>Automatic cost estimation for tree edit distance using particle swarm optimization.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>289--292</pages>
<contexts>
<context position="4166" citStr="Mehdad, 2009" startWordPosition="622" endWordPosition="623">le, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment o</context>
</contexts>
<marker>Mehdad, 2009</marker>
<rawString>Yashar Mehdad. 2009. Automatic cost estimation for tree edit distance using particle swarm optimization. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 289–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="23131" citStr="Och and Ney, 2003" startWordPosition="3813" endWordPosition="3816">k/people/ T.Cohn/paraphrase_guidelines.pdf 595 merge these alignments to form one single phrasal alignment.6 Table 2 lists the percentage of various alignment sizes after the merge. Two observations can be made: first, the portion of phrasal alignments increases to 10% — 20% after merging; second, allowing a maximal phrase length of 3 covers 98% — 99% of total alignments, thus a phrase length larger than 3 would be a bad trade-off for coverage vs speed. 4.2 Baselines and Evaluation Metrics MacCartney et al. (2008) and Yao et al. (2013a) showed that the traditional MT bilingual aligner GIZA++ (Och and Ney, 2003) presented weak results on the task of monolingual alignment. Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences. It is designed for the task of monolingual alignment and supports phrasal alignment. We used version 1.4 and default weights to optimize by maximum accuracy. MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding for speed and hard syntactic constraints to boost exact match rate, with reported numbers on MSR06. M</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Vassilios V Digalakis</author>
<author>Owen A Kimball</author>
</authors>
<title>From HMM’s to segment models: a unified view of stochastic modeling for speech recognition.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>5</issue>
<contexts>
<context position="6504" citStr="Ostendorf et al., 1996" startWordPosition="961" endWordPosition="965">e source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just lik</context>
</contexts>
<marker>Ostendorf, Digalakis, Kimball, 1996</marker>
<rawString>Mari Ostendorf, Vassilios V Digalakis, and Owen A Kimball. 1996. From HMM’s to segment models: a unified view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio Processing, 4(5):360–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="5380" citStr="Pang et al., 2003" startWordPosition="797" endWordPosition="800">nt of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for whole sentence pairs. Besides the common application of textual entailment and question answering, monolingual alignment has also been applied in the field of text generation (Barzilay and Lee, 2003; Pang et al., 2003). Word alignment has been more explored in machine translation. The IBM models (Brown et al., 1993) allow many-to-one alignment and are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are e</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of NAACL, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen T Yih</author>
</authors>
<title>Mapping Dependencies Trees: An Application to Question Answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th International Symposium on Artificial Intelligence and Mathematics,</booktitle>
<location>Fort Lauderdale, Florida.</location>
<contexts>
<context position="4048" citStr="Punyakanok et al., 2004" startWordPosition="602" endWordPosition="605">ing semi-Markov 590 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590–600, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English se</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004. Mapping Dependencies Trees: An Application to Question Answering. In Proceedings of the 8th International Symposium on Artificial Intelligence and Mathematics, Fort Lauderdale, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicates across monolingual comparable texts using graph-based clustering.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>171--182</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="4291" citStr="Roth and Frank, 2012" startWordPosition="637" endWordPosition="640">rase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadan</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012. Aligning predicates across monolingual comparable texts using graph-based clustering. In Proceedings of EMNLPCoNLL, pages 171–182, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarawagi Sarawagi</author>
<author>William Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>17--1185</pages>
<contexts>
<context position="7316" citStr="Sarawagi and Cohen, 2004" startWordPosition="1081" endWordPosition="1084">lignment that agreed in both directions. Despite successful usage of generative semiMarkov models in bilingual alignment, this has not been followed with models in discriminative monolingual alignment. Essentially monolingual alignment would benefit more from discriminative models with various feature extractions (just like those defined in MANLI) than generative models without any predefined feature (just like how they were used in bilingual alignment). To combine the strengths of both semi-Markov models and discriminative training, we propose to use the semi-Markov Conditional Random Field (Sarawagi and Cohen, 2004), which was first used in information extraction to tag continuous segments of input sequences and outperformed conventional CRFs in the task of named entity recognition. We describe this model in the following section. 3 The Alignment Model Our objective is to define a model that supports phrase-based alignment of arbitrary phrase length. In this section we first describe a regular CRF model that supports one-to-one token-based alignment (Blunsom and Cohn, 2006; Yao et al., 2013a), then extend it to phrase-based alignment with the semi-Markov model. 591 3.1 Token-based Model Given a source se</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sarawagi Sarawagi and William Cohen. 2004. Semimarkov conditional random fields for information extraction. Advances in Neural Information Processing Systems, 17:1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>Optimal and syntactically-informed decoding for monolingual phrase-based alignment.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL short.</booktitle>
<contexts>
<context position="4538" citStr="Thadani and McKeown, 2011" startWordPosition="673" endWordPosition="676"> work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact a</context>
<context position="23573" citStr="Thadani and McKeown, 2011" startWordPosition="3880" endWordPosition="3883"> for coverage vs speed. 4.2 Baselines and Evaluation Metrics MacCartney et al. (2008) and Yao et al. (2013a) showed that the traditional MT bilingual aligner GIZA++ (Och and Ney, 2003) presented weak results on the task of monolingual alignment. Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences. It is designed for the task of monolingual alignment and supports phrasal alignment. We used version 1.4 and default weights to optimize by maximum accuracy. MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding for speed and hard syntactic constraints to boost exact match rate, with reported numbers on MSR06. MANLI-joint (Thadani et al., 2012): an improved version of MANLI-constraint that not only models phrasal alignments, but also alignments between dependency arcs, with reported numbers on the original Edinburgh paraphrase corpus. jacana-token (Yao et al., 2013a): a tokenbased aligner with state-of-the-art performance on MSR06. Note that the jacana-token aligner is open-source, so we were able to re-train it with exactly the same feature set</context>
</contexts>
<marker>Thadani, McKeown, 2011</marker>
<rawString>Kapil Thadani and Kathleen McKeown. 2011. Optimal and syntactically-informed decoding for monolingual phrase-based alignment. In Proceedings ofACL short.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Scott Martin</author>
<author>Michael White</author>
</authors>
<title>A joint phrasal and dependency model for paraphrase alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1229--1238</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="4561" citStr="Thadani et al., 2012" startWordPosition="677" endWordPosition="680">ent employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search space. This was optimized by Thadani and McKeown (2011) through Integer Linear Programming (ILP), where benefiting from modern ILP solvers they showed an order-of-magnitude speedup in decoding. Also, various syntactic constraints can be easily added, significantly improving exact alignment match rate for</context>
<context position="21101" citStr="Thadani et al., 2012" startWordPosition="3497" endWordPosition="3500">nlp/files/RTE_ 2006_Aligned.zip 1x1 1x2 1x3 2x2 2x3 3x3 more MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8 EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1 Table 2: Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora. Semantically equivalent words and phrases in the premise and hypothesis sentences are aligned in a manner analogous to alignments in statistical machine translation. This dataset is asymmetric: on average the premises contain 29 words and the hypotheses 11 words. Edinburgh++4 (Thadani et al., 2012) is a revised version of the Edinburgh paraphrase corpus(Cohn et al., 2008) with sentences from the following resources: 1. the MultipleTranslation Chinese corpus; 2. Jules Verne’s novel Twenty Thousand Leagues Under the Sea. 3. the Microsoft Research paraphrase corpus (Dolan et al., 2004). The corpus is more balanced and symmetric: the source and target sentences are both 22 words long on average. Table 1 shows some statistics. Both corpora contain mostly token-based alignment. For MSR06, MacCartney et al. (2008) showed that setting the allowable phrase size to be greater than one only increa</context>
<context position="23764" citStr="Thadani et al., 2012" startWordPosition="3910" endWordPosition="3913">eak results on the task of monolingual alignment. Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences. It is designed for the task of monolingual alignment and supports phrasal alignment. We used version 1.4 and default weights to optimize by maximum accuracy. MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding for speed and hard syntactic constraints to boost exact match rate, with reported numbers on MSR06. MANLI-joint (Thadani et al., 2012): an improved version of MANLI-constraint that not only models phrasal alignments, but also alignments between dependency arcs, with reported numbers on the original Edinburgh paraphrase corpus. jacana-token (Yao et al., 2013a): a tokenbased aligner with state-of-the-art performance on MSR06. Note that the jacana-token aligner is open-source, so we were able to re-train it with exactly the same feature set used by our phrase-based model. This allows a fair comparison of model performance (token-based vs. phrase-based). The MANLI* systems are not available, thus we only reported their numbers f</context>
</contexts>
<marker>Thadani, Martin, White, 2012</marker>
<rawString>Kapil Thadani, Scott Martin, and Michael White. 2012. A joint phrasal and dependency model for paraphrase alignment. In Proceedings of COLING 2012: Posters, pages 1229–1238, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics - Volume 2, COLING ’96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="6130" citStr="Vogel et al., 1996" startWordPosition="904" endWordPosition="907"> are essentially asymmetric. Phrase-based MT historically relied on heuristics (Koehn, 2010) to merge two sets of word alignment in opposite directions to yield phrasal alignment. Later, researchers explored non-heuristic phrase-based methods. Among them, Marcu and Wong (2002) described a joint proba2http://code.google.com/p/jacana/ bility model that generates both the source and target sentences simultaneously. All possible pairs of phrases in both sentences are enumerated and then pruned with statistical evidence. Deng and Byrne (2008) explored token-to-phrase alignment based on HMM models (Vogel et al., 1996) by explicitly modeling the token-to-phrase probability and phrase lengths. However, the token-to-phrase alignment is only in one direction: each target state still only spans one source word, and thus alignment on the source side is limited to tokens. Andr´es-Ferrer and Juan (2009) extended the HMM-based method to Hidden Semi-Markov Models (HSMM) (Ostendorf et al., 1996), allowing phrasal alignments on the source side. Finally, Bansal et al. (2011) unified the HSMM models with the alignment by agreement framework (Liang et al., 2006), achieving phrasal alignment that agreed in both directions</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics - Volume 2, COLING ’96, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the “para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop.</booktitle>
<contexts>
<context position="33041" citStr="Wan et al. (2006)" startWordPosition="5379" endWordPosition="5382">the test set. For the task of QA, since the evaluation methods in Mean Average Precision and Mean Reciprocal Rank only need a ranked list of answer sentences, and the scores on the test set are sufficient to provide the ranking, we did not tune anything on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on the superset of MSR06 and Edinburgh++. Results are reported in Table 5. We could not report on Meteor as Meteor does not explicitly output alignment scores. We did not expect the aligners to beat any of the system A% P% R% Wan et al. (2006) 75.6 77 90 Das and Smith (2009) 73.9 74.9 91.3 Heilman and Smith (2010) 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 (b) Paraphrase Identification system MAP MRR Cui et al. (2005) 0.4271 0.5259 Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Yao et al. (2013b) 0.6307 0.7477 the token aligner 0.5982 0.6582 our phrasal aligner 0.6165 0.7333 (c) Question Answering Sentence Ranking Table 5: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA. state-of-the-art result since n</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the “para-farce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1164--1172</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4255" citStr="Wang and Manning, 2010" startWordPosition="631" endWordPosition="634">inguistics CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking. The final phrase-based aligner is open-source.2 2 Related Work Most work in monolingual alignment employs dependency tree/graph matching algorithms, including tree edit distance (Punyakanok et al., 2004; Kouylekov and Magnini, 2005; Heilman and Smith, 2010; Yao et al., 2013b), Particle Swarm Optimization (Mehdad, 2009), linear regression/classification models (Chambers et al., 2007; Wang and Manning, 2010), and min-cut (Roth and Frank, 2012). These works inherently only support token-based alignment, with phrase-like alignment achieved by first merging tokens to phrases as a preprocessing step. The MANLI aligner (MacCartney et al., 2008) and its derivations (Thadani and McKeown, 2011; Thadani et al., 2012) are the first known phrasebased aligners specifically designed for aligning English sentence pairs. It applies discriminative perceptron learning with various features and handles phrase-based alignment of arbitrary phrase lengths. MANLI suffers from slow decoding time due to its large search</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of COLING, pages 1164–1172, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy Model? A QuasiSynchronous Grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>22--32</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="33292" citStr="Wang et al. (2007)" startWordPosition="5424" endWordPosition="5427">ing on training but instead directly ran the aligner on the test set. All three tasks shared the same aligner model trained on the superset of MSR06 and Edinburgh++. Results are reported in Table 5. We could not report on Meteor as Meteor does not explicitly output alignment scores. We did not expect the aligners to beat any of the system A% P% R% Wan et al. (2006) 75.6 77 90 Das and Smith (2009) 73.9 74.9 91.3 Heilman and Smith (2010) 73.2 75.7 87.8 the token aligner 70.0 72.6 88.1 our phrasal aligner 68.1 68.6 95.8 (b) Paraphrase Identification system MAP MRR Cui et al. (2005) 0.4271 0.5259 Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Yao et al. (2013b) 0.6307 0.7477 the token aligner 0.5982 0.6582 our phrasal aligner 0.6165 0.7333 (c) Question Answering Sentence Ranking Table 5: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA. state-of-the-art result since no sophisticated models were additionally used but only the alignment score. Still, the aligners showed competitive performance. It still follows the pattern from the alignment experiment that the phrasal aligner had higher recall and lower precision t</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A QuasiSynchronous Grammar for QA. In Proceedings of EMNLP-CoNLL, pages 22–32, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>A Lightweight and High Performance Monolingual Word Aligner.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013 short,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013a. A Lightweight and High Performance Monolingual Word Aligner. In Proceedings of ACL 2013 short, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Peter Clark</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Answer Extraction as Sequence Tagging with Tree Edit Distance.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Yao, Van Durme, Clark, Callison-Burch, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013b. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of NAACL 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
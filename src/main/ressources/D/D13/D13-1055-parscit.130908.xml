<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004195">
<title confidence="0.985277">
Automatically Classifying Edit Categories in Wikipedia Revisions
</title>
<author confidence="0.959663">
Johannes Daxenberger† and Iryna Gurevych†�
</author>
<affiliation confidence="0.96960575">
t Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technische Universit¨at Darmstadt
t Information Center for Education
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.951449">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.994691" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845818181818">
In this paper, we analyze a novel set of fea-
tures for the task of automatic edit category
classification. Edit category classification as-
signs categories such as spelling error correc-
tion, paraphrase or vandalism to edits in a doc-
ument. Our features are based on differences
between two versions of a document includ-
ing meta data, textual and language properties
and markup. In a supervised machine learning
experiment, we achieve a micro-averaged F1
score of .62 on a corpus of edits from the En-
glish Wikipedia. In this corpus, each edit has
been multi-labeled according to a 21-category
taxonomy. A model trained on the same
data achieves state-of-the-art performance on
the related task of fluency edit classification.
We apply pattern mining to automatically la-
beled edits in the revision histories of different
Wikipedia articles. Our results suggest that
high-quality articles show a higher degree of
homogeneity with respect to their collabora-
tion patterns as compared to random articles.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838">
Due to its ever-evolving and collaboratively built
content, Wikipedia has been the subject of many
NLP studies. While the number of newly created
articles in the online encyclopedia declined in the
last few years (Suh et al., 2009), the number of edits
in existing articles is rather stable.1 It is reasonable
to assume that the latter will not change in the near
</bodyText>
<footnote confidence="0.87035">
1http://stats.wikimedia.org/EN/
TablesDatabaseEdits.htm
</footnote>
<bodyText confidence="0.99982865625">
future. One of the major reasons for the popular-
ity of Wikipedia is its up-to-dateness (Keegan et al.,
2013), which in turn requires constant editing activ-
ity. Wikipedia’s revision history stores all changes
made to any page in the encyclopedia in separate
revisions. Previous studies have exploited revision
history data in tasks such as preposition error cor-
rection (Cahill et al., 2013), spelling error correc-
tion (Zesch, 2012) or paraphrasing (Max and Wis-
niewski, 2010). However, they all use different ap-
proaches to extract the information needed for their
task. Ferschke et al. (2013) outline several appli-
cations benefiting from revision history data. They
argue for a unified approach to extract and classify
edits from revision histories based on a predefined
edit category taxonomy.
In this work, we show how the extraction and
automatic multi-label classification of any edit in
Wikipedia can be handled with a single approach.
Therefore, we use the 21-category edit classification
taxonomy developed in previous work (Daxenberger
and Gurevych, 2012). This taxonomy enables a fine-
grained analysis of edit activity in revision histories.
We present the results from an automatic classifica-
tion experiment, based on an annotated corpus of ed-
its in the English Wikipedia. Additional information
necessary to reproduce our results, including word
lists and training, development and test data, is re-
leased online.2 To the best of our knowledge, this
is the first approach allowing to classify each single
edit in Wikipedia into one or more of 21 different
edit categories using a supervised machine learning
</bodyText>
<footnote confidence="0.9615535">
2http://www.ukp.tu-darmstadt.de/data/
edit-classification
</footnote>
<page confidence="0.840128">
578
</page>
<note confidence="0.747161">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 578–589,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998936358208955">
approach.
We define our task as edit category classification.
An edit is a coherent, local change which modifies a
document and which can be related to certain meta
data (e.g. its author, time stamp etc.). In edit ca-
tegory classification, we aim to detect all n edits
ekv_1,v with 0 &lt; k &lt; n in adjacent versions rv_1, rv
of a document (we refer to the older revision as rv_1
and to the newer as rv) and assign each of them
to one or more edit categories. There exist at least
two main applications of edit category classification:
First, a fine-grained classification of edits in collab-
oratively created documents such as Wikipedia ar-
ticles, scientific papers or research proposals, would
help us to better understand the collaborative writing
process. This includes answers to questions about
the kind of contribution of individual authors (Who
has added substantial contents?, Who has improved
stylistic issues?) and about the kind of collabora-
tion which characterizes different articles (Liu and
Ram, 2011). Second, automatic classification of ed-
its generates huge amounts of training data for the
above mentioned NLP systems.
Edit category classification is related to the bet-
ter known task of document pair classification.
In document pair classification, a pair of docu-
ments has to be assigned to one or more categories
(e.g. paraphrase/non-paraphrase, plagiarism/non-
plagiarism). Here, the document may be a very short
text, such as a sentence or a single word. Appli-
cations of document pair classification include pla-
giarism detection (Potthast et al., 2012), paraphrase
detection (Madnani et al., 2012) or text similarity
detection (B¨ar et al., 2012). In edit category clas-
sification, we also have two documents. However,
these documents are different versions of the same
text. This scenario implies certain characteristics for
a well-designed feature set as we will demonstrate in
this study.
The main contributions of this paper are: First,
we introduce a novel feature set for edit category
classification. Second, we evaluate the performance
of this feature set on different tasks within a cor-
pus of Wikipedia edits. We propose the new task of
edit category classification and show that our model
is able to classify edits from a 21-category taxon-
omy. Furthermore, our model achieves state-of-the-
art performance in a fluency edit classification task
(Bronner and Monz, 2012). Third, we analyze col-
laboration patterns based on edit categories on two
subsets of Wikipedia articles, namely featured and
non-featured articles. We detect correlations be-
tween collaboration patterns and high-quality arti-
cles. This is demonstrated by the fact that featured
articles have a higher degree of homogeneity with
respect to their collaboration patterns as compared
to random articles.
The rest of this paper is structured as follows. In
Section 2, we motivate our experiments based on
previous work. Section 3 explains our training data
and the features we use for the machine learning ex-
periments. In Section 4, we present and discuss the
results of our experiments. We also demonstrate an
application of our classifier model in Section 5 by
mining frequent collaboration patterns in the revi-
sion histories of different articles. Finally, we draw
a conclusion in Section 6.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999815115384615">
Wikipedia is a huge data source for generating train-
ing data for edit category classification, as all pre-
vious versions of each page in the encyclopedia
are stored in its revision history. Unsurprisingly,
the number of studies extracting certain kinds of
Wikipedia edits keeps growing. Most of these use
manually defined rules or filters find the right kind
of edits. Among the latter, there are NLP applica-
tions such as the detection of lexical errors (Nelken
and Yamangil, 2008), spelling error correction (Max
and Wisniewski, 2010; Zesch, 2012), preposition er-
ror correction (Cahill et al., 2013), sentence com-
pression (Nelken and Yamangil, 2008; Yamangil
and Nelken, 2008), summarization (Nelken and Ya-
mangil, 2008), simplification (Yatskar et al., 2010;
Woodsend and Lapata, 2011), paraphrasing (Max
and Wisniewski, 2010; Dutrey et al., 2011), tex-
tual entailment (Zanzotto and Pennacchiotti, 2010;
Cabrio et al., 2012), information retrieval (Aji et al.,
2010; Nunes et al., 2011) and bias detection (Re-
casens et al., 2013).
Bronner and Monz (2012) define features for the
supervised classification of factual and fluency edits.
Their features are calculated both on character- and
word-level. Furthermore, they use features based
on POS tags, named entities, acronyms, and a lan-
</bodyText>
<page confidence="0.998754">
579
</page>
<figureCaption confidence="0.999479">
Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool.
</figureCaption>
<bodyText confidence="0.998621285714286">
guage model (word n-grams). In their experiments,
character-level features and named entity features
show the highest improvement over the baseline.
Vandalism detection in Wikipedia has mostly
been defined as a binary machine learning task,
where the goal is to classify a pair of adjacent re-
visions as vandalized or not-vandalized based on
edit category features. In Adler et al. (2011), the
authors group these features into meta data (au-
thor, comment and time stamp of a revision), rep-
utation (author and article reputation), textual (lan-
guage independent, i.e. token- and character-based)
and language features (language dependent, mostly
dictionary-based). They carry out cross-validation
experiments on the PAN-WVC-10 corpus (Potthast
and Holfeld, 2011). Classifiers based on reputa-
tion and text performed best. Adler et al. (2011)
use Random Forests as classifier (Breiman, 2001)
in their experiments. This classifier was also used
in the vandalism detection study of Javanmardi et al.
(2011) where it outperformed the classifiers based
on Logistic Regression and Naive Bayes.
Different to the approach of Bronner and Monz
(2012) and previous vandalism classification stud-
ies, we built a model which accounts for multi-
labeling and a fine-grained edit category system.
Our feature set builds upon existing work while
adding a substantial number of new features.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999478">
3.1 Wikipedia Edit Category Corpus
</subsectionHeader>
<bodyText confidence="0.999876627906977">
For our experiments, we used the freely avail-
able Wikipedia Edit Category Corpus (WPEC) com-
piled in previous work (Daxenberger and Gurevych,
2012). In this corpus, each pair of adjacent revisions
is segmented into one or more edits. This enables
an accurate picture of the editing process, as an au-
thor may perform several independent edits in the
same revision. Furthermore, edits are multi-labeled,
i.e. each edit is assigned one or more categories.
This is important for a precise description of major
edits, e.g. when an entire new paragraph including
text, references and markup is added. There are four
basic types of edits, namely Insertions, Deletions,
Modifications and Relocations. These are calculated
via a line-based diff comparison on the source text
(including wiki markup). As previously suggested
(Daxenberger and Gurevych, 2012), inside modified
lines, only the span of text which has actually been
changed is marked as edit (either Insertion, Dele-
tion or Modification), not the entire line. We ex-
tracted the data which is not contained in WPEC
(meta data and plain text of r„_1 and r„) using the
Java Wikipedia Library (JWPL) with the Revision
Toolkit (Ferschke et al., 2011).
In Daxenberger and Gurevych (2012), we divide
the 21-category taxonomy into text-base (meaning-
changing edits), surface (non meaning-changing ed-
its) and Wikipedia policy (VANDALISM and RE-
VERT) edits. Among the text-base edits, we include
categories for templates, references (internal and ex-
ternal links), files and information, each of which
is further divided into an insertion (I), deletion (D)
and modification (M) category. Surface edits con-
sist of paraphrases, spelling and grammar correc-
tions, relocations and markup edits. The latter cate-
gory contains all edits which affect markup elements
that are not covered by any of the other categories
and is divided into insertions, deletions and modifi-
cations. This includes, for example, apostrophes in
&amp;quot;&apos;bold text&amp;quot;&apos;. We also suggested an OTHER category,
which is intended for edits which cannot be labeled
due to segmentation errors. Figure 1 shows an exam-
ple edit from WPEC, labeled with the REFERENCE-
</bodyText>
<page confidence="0.995447">
580
</page>
<table confidence="0.995168260869565">
Feature Value Explanation
Meta Data Author group user Wikimedia user group of the author
Author is registered* true Author is registered (otherwise: IP user)
Same author* false Authors of r„ and r„_1 are the same
Comment length* 0 Number of characters in the comment
Vulgarism in comment false Comment contains a word from in the vulgarism word list
Comment is auto-generated false Entire comment has been auto-generated
Auto-generated comment ratio 0 Auto-generated part of comment divided by length of the comment
Incorrect comment ratio 0 Out-of-dictionary word count divided by word count in the comment
Comment n-grams1 — Presence or absence of token n-grams in the comment
Is revert* false Comment contains a word from in the revert word list
Is minor false Revision has been marked as minor change
Time difference* 505 Time difference between r„_1 and r„ (in minutes)
Number of edits 1 Absolute number of edits in the (r„_1, r„)-pair
Textual Diff capitals* 0 Difference in the number of capitals
Diff digits* 0 Difference in the number of digits
Diff special characters* 2 Difference in the number of non-alphanumeric characters
Diff whitespace characters 1 Difference in the number of whitespace characters
Diff characters* 9 Difference in the number of characters
Diff tokens* 1 Difference in the number of whitespace-separated tokens
Diff repeated characters 0 Difference in the number of repeated characters
Diff repeated tokens 0 Difference in the number of repeated white-space separated tokens
Cosine similarity 0 Cosine similarity
Levenshtein distance* 9 Levenshtein distance
Optimal string alignment distance 9 Optimal string alignment distance (Damerau-Levenshtein distance)
Ratio diff to paragraph characters 0.02 Diff characters divided by the length of the edited paragraph
Ratio diff to revision characters 0.0005 Diff characters divided by the length of r„_1
Ratio diff to paragraph tokens 0.04 Diff tokens divided by the length of the edited paragraph
Ratio diff to revision tokens 0.0003 Diff tokens divided by the length of r„_1
Ratio old to new paragraph 0 Difference in the number of characters in the edited paragraph
Character n-grams1 p,o,e,t,r,y2 Presence or absence of n-grams of edited characters
Token n-grams1 poetry2 Presence or absence of n-grams of edited tokens
Simple edit type Insertion Modification, Insertion, Deletion or Relocation
Markup Diff number m 0 Difference in the number of m
Diff type m false Different types of m
Diff type context m true3 Different types of m within the immediate context of the edit
Is covered by m true3 Edit is covered by m in r„_1
Covers m false Edit covers m in r„_1
Language Diff spelling errors* 0 Difference in the number of out-of-dictionary words
Diff vulgar words* 0 Difference in the number of tokens contained in vandalism word list
Semantic similarity -1 Explicit Semantic Analysis with vector indexes from Wiktionary
Diff POS tags* false POS tag sets are symmetrically different
Diff type POS tags* 0 Number of distinct POS tags
1 N-gram features are represented as boolean features.
2 In this example, n = 1 (unigrams).
3 True if m corresponds to internal link, false otherwise.
</table>
<tableCaption confidence="0.989148333333333">
Table 1: List of edit category classification features with explanations. The values correspond to the the example edit
from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with
* have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012).
</tableCaption>
<page confidence="0.99591">
581
</page>
<bodyText confidence="0.9997783125">
M category. WPEC was created in a manual anno-
tation study with three annotators. The overall inter-
annotator agreement measured as Krippendorf’s α is
.67 (Daxenberger and Gurevych, 2012). The exper-
iments in this study are based on the gold standard
annotations in WPEC, which have been derived by
means of a majority vote for each edit.
WPEC consists of 981 revision pairs, segmented
into 1,995 edits. We define edit category classifica-
tion as a multi-label classification task. For the sake
of readability, in the following we will refer to an
edit ekv−1,v as ei, with ei E E, where 0 &lt; i &lt; 1995
and E is the set of all edits. An edit ei is the basic
classification unit in our task. Each ei has to be la-
beled with a set of categories y C C, where C is the
set of all edit categories, |C |= 21.
</bodyText>
<subsectionHeader confidence="0.881643">
3.2 Features for Edit Category Classification
</subsectionHeader>
<bodyText confidence="0.98089412">
We grouped our features into meta data, textual,
markup and language features. An overview and
explanation of all features can be found in Table 1.
The scheme we apply to group edit category clas-
sification features is similar to the system used by
Adler et al. (2011). We re-use some of the features
suggested by Adler et al. (2011), Javanmardi et al.
(2011) and Bronner and Monz (2012), as marked in
Table 1. Features are calculated on edited text spans.
We label the edited text span corresponding to ei in
rv−1 as tv−1 and the edited text span in rv as tv. In
edits which are insertions, we consider tv−1 to be
empty, while tv is considered empty for deletions.
For Relocations, tv−1 = tv.
Table 1 includes the value of each feature for the
example edit from Figure 1. This edit modifies the
link [[Dactyl|Dactylic]] by adding a speci-
fication to the target of that link. For spell-checking,
we use British and US-American English Jazzy dic-
tionaries.3 Markup elements are detected by the
Sweble Wikitext parser (Dohrn and Riehle, 2011).
Meta data features We consider the comment,
author, time stamp or any other flag (“minor
change”) of rv as meta data. The Wikimedia user
group4 of an author specifies the edit permissions
</bodyText>
<footnote confidence="0.98234075">
3http://sourceforge.net/projects/
jazzydicts
4http://meta.wikimedia.org/wiki/User_
classes
</footnote>
<bodyText confidence="0.999963936170213">
of this user (e.g. bot, administrator, blocked user).
We indicate whether the revision comments or parts
of it have been auto-generated. This happens when
a page is blanked, i.e. all of its content has been
deleted or replaced or when a new page or redirect is
created (denoted by the Comment is auto-generated
feature). Furthermore, edits within a specific sec-
tion of an article are automatically marked by adding
a prefix with the name of this section to the com-
ment of the revision (denoted by the Auto-generated
comment ratio feature). Meta data features have the
same value for all edits in a (rv−1, rv)-pair.
Textual features Textual features are calculated
based on a certain property of the changed text. In
a preprocessing step, any wiki markup inside tv−1
and tv is deleted. As for the example edit from Fig-
ure 1, tv−1 would correspond to an empty string and
tv would be represented as “ (poetry)”. The n-gram
feature spaces are composed of n-grams that are
present either in tv−1 but not tv, or vice verse. Char-
acter n-grams only contain English alphabet charac-
ters, token n-grams consist of words excluding spe-
cial characters.
Markup features As opposed to textual features,
wiki markup features account for the Wikimedia
specific markup elements. Markup features are cal-
culated based on the number and type of a markup
element m and the surrounding context of an edit.
Here, m can be a template, an external or internal
link, an image or any other element used to describe
markup including HTML tags. The type of m is
defined by the link target for internal and external
links and images, by the name of the template for
templates and by the wiki markup element name for
markup elements. Markup features are calculated on
text spans tv−1 and tv. Naturally, wiki markup is not
deleted beforehand. The edited text spans tv−1 and
tv may be located inside a markup element m (e.g. a
link or a template). In such cases, our diff algorithm
will not label the entire element m, but rather the
actually modified text. However, such an edit may
change the name of a template or the target of a link
(as in the example edit from Figure 1). We there-
fore include the immediate context sv−1 and sv of
each edit and compare the type of potential markup
elements m in sv−1 and sv. Here, sv (sv−1) is de-
fined as tv (tv−1) including all preceding and follow-
</bodyText>
<page confidence="0.980403">
582
</page>
<table confidence="0.99948875">
Revisions Edits Cardinality
Train 713 1,597 1.20
Test 89 229 1.24
Dev 89 169 1.21
</table>
<tableCaption confidence="0.985199666666667">
Table 2: Statistics of the training, test and development
set. Cardinality is the average number of edit categories
assigned to an edit.
</tableCaption>
<bodyText confidence="0.9907318">
ing characters in rv (rv_1) which are not separated
from tv (tv_1) by a boundary character (whitespace
or line break). The above described features model
what is actually edited in the text. A number of fea-
tures are calculated on tv_1 only. These features are
more likely to inform about where an edit is con-
ducted. They specify whether tv_1 covers (i.e. con-
tains) a certain wiki markup element and vice versa,
i.e. whether tv_1 is located inside a text span that
belongs to a markup element.
Language Language features are calculated on the
context sv_1 and sv of edits, any wiki markup is
deleted. For the Explicit Semantic Analysis, we use
Wiktionary (Zesch et al., 2008) and not Wikipedia
assuming that the former has a better coverage with
respect to different lexical classes. POS tagging was
carried out using the OpenNLP POS tagger.5 The
vandalism word list contains a hand-crafted set of
around 100 vandalism and spam words from various
places in the web.
</bodyText>
<subsectionHeader confidence="0.986809">
3.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999921769230769">
We extract features with the help of ClearTK (Ogren
et al., 2008). For the machine learning part, we use
Weka (Hall et al., 2009) with the Meka6 and Mu-
lan (Tsoumakas et al., 2010) extensions for multi-
label classification. We use DKPro Lab (Eckart de
Castilho et al., 2011) to test different parameter com-
binations. We randomly split the gold standard data
from WPEC into 80% training, 10% test and 10%
development set, as shown in Table 2.
Multi-label Classification We report the perfor-
mance of various machine learning algorithms. A
comprehensive overview of multi-label classifica-
tion algorithms and evaluation measures can be
</bodyText>
<footnote confidence="0.995434333333333">
5Maxent model for English, http://opennlp.
apache.org
6http://meka.sourceforge.net
</footnote>
<table confidence="0.999904111111111">
Threshold – – .10 .25 .33
Accuracy .09 .13 .50 .44 .53
Exact Match .06 .13 .35 .36 .44
Example F1 .09 .13 .55 .47 .56
Precision .10 .13 .54 .46 .56
Recall .10 .13 .61 .50 .60
Macro-F1 .10 .06 .49 .35 .51
Label Micro-F1 .10 .12 .59 .49 .62
Ranking One Error .90 .87 .42 .48 .34
</table>
<tableCaption confidence="0.978607">
Table 3: Overall classification results with 3 multi-label
classifiers and a C4.5 decision tree base classifier, as com-
pared to random and majority category baselines.
</tableCaption>
<bodyText confidence="0.999917848484849">
found in Madjarov et al. (2012). Multi-label classi-
fication problems are solved by either transforming
the multi-label classification task into one or more
single-label classification tasks (problem transfor-
mation method) or by adapting single-label clas-
sification algorithms (algorithm adaption method).
Several algorithms have been developed on top of
the former methods and use ensembles of such clas-
sifiers (ensemble methods). We applied the Bi-
nary Relevance approach (BR), a simple transfor-
mation method which converts the multi-label prob-
lem into |C |binary single-label problems, where |C|
is the number of categories. Hence, this method
trains a classifier for each category in the corpus
(one-against-all). It is the most straightforward ap-
proach when dealing with multi-labeled data. How-
ever, it does not consider possible relationships or
dependencies between categories. Therefore, we
tested two more sophisticated methods. Hierar-
chy of multi-label classifiers HOMER (Tsoumakas
et al., 2008) is a problem transformation method.
It accounts for possibly hierarchical relationships
among categories by dividing the overall category
set into a tree-like structure with nodes of small ca-
tegory sets of size k and leaves of single categories.
Subsequently, a multi-label classifier is applied to
each node in the tree. Random k-labelsets RAKEL
(Tsoumakas et al., 2011) is an ensemble method,
which randomly chooses l typically small subsets
with k categories from the overall set of catego-
ries. Subsequently, all k-labelsets which are found
in the multi-labeled data set are converted into new
categories in a single-labeled data set using the la-
</bodyText>
<page confidence="0.994545">
583
</page>
<bodyText confidence="0.998059047619048">
bel powerset transformation (Trohidis et al., 2008).
HOMER and BR are among the multi-label clas-
sifiers, which Madjarov et al. (2012) recommend
as benchmark methods. As underlying single-label
classification algorithm, we used a C4.5 decision
tree classifier (Quinlan, 1993), as decision tree clas-
sifiers yield state-of-the-art performance in the re-
lated work.
Multi-label Evaluation We denote the set of rel-
evant categories for each edit ei E E as yi E
C and the set of predicted categories as h(ei).
Evaluation measures for multi-label classification
systems are based on either bipartitions or rank-
ings. Among the former, we report example-
based (weighting each edit equally) and label-based
(weighting each edit category equally) measures.
The accuracy of a multi-label classifier is defined
Jac-
as 1 P|E  ||h(ei)nyi  |which corresponds to the E |i=1 |h(ei)vyi|
card similarity of h(ei) and yi averaged over all ed-
its. We report subset accuracy (exact match), cal-
</bodyText>
<equation confidence="0.9237715">
P|E|
culated as 1 i=1 I, with I = 1 if h(ei) =
|E|
yi and I = 0 otherwise. Example-based pre-
cision is defined as 1 P|E |l h(ei)nyi |, recall as
|E |i=1 |h(ei)|
1 |E ||h(ei)nyi |and F1 as 1 P|E |2x|h(ei)nyi|
|E I i=1 |yi |,|E |i=1 |h(ei)|+|yi|
</equation>
<bodyText confidence="0.999597608695652">
For the label-based measures, we report macro-
and micro-averaged F1 scores. As a ranking-based
measure, we report one error, which is defined as
1|E |ar max
|E |Pi=1 J[ g m
expr is true and JexprK = 0 otherwise. f(ei, c) de-
notes the rank of category c E C as predicted by
the classifier. The one error measure evaluates the
number of edits where the highest ranked category
in the predictions is not in the set of relevant cate-
gories. It becomes smaller when the performance of
the classifier increases.
Table 3 shows the overall classification scores.
We calculated a random baseline, which multi-labels
edits at random considering the label powerset fre-
quencies it has learned from the training data. Fur-
thermore, we calculated a majority category base-
line, which labels all edits with the most frequent
edit category in the training data. In Figure 2, we
list the results for each category, together with the
average pair-wise inter-rater agreement (F1 scores).
The F1 scores are calculated based on the study we
carried out in Daxenberger and Gurevych (2012).
</bodyText>
<subsectionHeader confidence="0.496767">
Parameters and Feature selection All parame-
</subsectionHeader>
<bodyText confidence="0.999922366666667">
ters have been adjusted on the development set us-
ing the RAKEL classifier, aiming to optimize accu-
racy. With respect to the n-gram features, we tested
values for n = 1, 2 and 3. For comment n-grams,
unigrams turned out to yield the best overall per-
formance, and bigrams for character and token n-
grams. The word and character n-gram spaces are
limited to the 500 most frequent items, the comment
n-gram space is limited to the 1,500 most frequent
items. To transform ranked output into bipartitions,
it is necessary to set a threshold. This threshold is
reported in Table 3 and has been optimized for each
classifier with respect to label cardinality (average
number of labels assigned to edits) on the develop-
ment set. Since most of the traditional feature se-
lection methods cannot be applied directly to multi-
labeled data, we used the label powerset approach to
transform the multi-labeled data into single-labeled
data and subsequently applied χ2. Feature reduc-
tion to the highest-ranked features clearly improved
the classifier performance on the development set.
We therefore limited the feature space to the 150
highest-ranked features in our experiments.
For the RAKEL classifier, we set l = 42 (twice
the size of the category set) and k = 3. In HOMER,
we used BR as transformation method, random dis-
tribution of categories to the children nodes and
k = 3. For all other classifier parameters, we used
the default settings as configured in Meka respective
Mulan.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.989956">
The classifiers significantly outperformed both base-
lines. RAKEL shows best performance for almost
all measures in Table 3. The simpler BR approach,
which assumes no dependencies between categories,
still outperforms HOMER.
We trained and tested the classifier with different
feature groups (see Table 1), to analyze the impor-
tance of single types of features. As shown in Fig-
ure 2, textual features had the highest impact on clas-
sification performance. On the opposite, language
features played a minor role in our experiments.
Among the highest ranked individual features for the
entire set of categories, we find textual (Levenshtein
distance, Simple edit type), markup (Diff number
f(ei, c)] E/ yiK, JexprK = 1 if
</bodyText>
<page confidence="0.945639">
584
</page>
<figure confidence="0.99861475">
F1 score
0.8
0.6
0.4
0.2
0
1
Human Classifier Textual Markup Meta Data Language
</figure>
<figureCaption confidence="0.993848">
Figure 2: F1 scores of RAKEL with C4.5 as base classifier for individual categories. We add human inter-annotator
agreement as average pair-wise F1 scores as well as F1 scores for classifiers trained and tested on single feature
groups, cf. Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M
and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set.
</figureCaption>
<bodyText confidence="0.999959872727273">
markup elements) and meta data (Number of edits)
features.
Bronner and Monz (2012) report an accuracy
of .88 for their best performing system on the bi-
nary classification task of distinguishing fluency and
factual edits. The best performing classifier in
their study was Random Forests (Breiman, 2001).
To compare our features with their approach, we
mapped the 21 edit categories from Daxenberger and
Gurevych (2012) to the binary category set (factual
vs. fluency) of Bronner and Monz (2012). Edits la-
beled as SPELLING/GRAMMAR, MARKUP, RELO-
CATION and PARAPHRASE are considered fluency
edits, the remaining categories factual edits. We re-
moved all edits labeled as OTHER, REVERT or VAN-
DALISM from WPEC. After applying the category
mapping, we deleted all edits which were labeled
with both the fluency and factual category. The lat-
ter may happen due to multi-labeling. This resulted
in 1,262 edits labeled as either fluency or factual.
On the 80% training split from Table 2, we trained
a Random Forests classifier with the optimized fea-
ture set and feature reduction as described in Sec-
tion 3.3. The number of trees was set to 100, with
unlimited depth. On the remaining data (test and
development split), we achieved an accuracy of .90.
Although we did not use the same data set as Bron-
ner and Monz (2012), this result suggests that our
feature set is suited for related tasks such as fluency
detection.
With respect to vandalism detection in Wikipedia,
state-of-the-art systems have a performance of
around .82 to .85 AUC-PR on the English Wikipedia
(Adler et al., 2011). We suspect that the low perfor-
mance of our system for Vandalism edits is mostly
due to a lower amount of training data, a higher skew
in the training and test data and the fact that we did
not include features which inform about future ac-
tions (e.g. whether a revision is reverted).
Error Analysis Sparseness is a major problem
for some of the 21 categories, as shown in Fig-
ure 2 by categories such as FILE-D, TEMPLATE-
D, MARKUP-M or PARAPHRASE which have only
very few examples in training, development and
test set. Categories with low inter-annotator agree-
ment in WPEC such as MARKUP-M, PARAPHRASE
or OTHER also yielded low classification accuracy.
We analyzed frequent errors of the classifier with
the help of a confusion matrix. PARAPHRASE ed-
its have been confused with INFORMATION-M by
the classifier. Furthermore, the classifier had prob-
lems to distinguish between VANDALISM and RE-
VERT as well as INFORMATION-I. Generally, modi-
fications as compared to insertions or deletions per-
form worse. All of the classifiers we tested, build
</bodyText>
<page confidence="0.996749">
585
</page>
<bodyText confidence="0.989930666666667">
their predictions by thresholding over a ranking, cf.
Table 3. This generates a source of errors, because
the classifier is not able to make a prediction, if it
does not have enough confidence for any of the cate-
gories. The imbalance of the data, because of the
high skew in the category distribution, is another
reason for classification errors. In ambiguous cases,
the classifier will be biased toward the category with
more examples in the training data.
</bodyText>
<sectionHeader confidence="0.591178" genericHeader="method">
5 A closer look at edit sequences: Mining
collaboration patterns
</sectionHeader>
<bodyText confidence="0.999954545454545">
An edit category classifier allows us to label en-
tire article revision histories. We applied the best-
performing model from Section 3.3 trained on the
entire WPEC to automatically classify all edits in the
Wikipedia Quality Assessment Corpus (WPQAC)
as presented in previous work (Daxenberger and
Gurevych, 2012). WPQAC consists of 10 fea-
tured and 10 non-featured articles7, with an over-
all number of 21,578 revisions (9,986 revisions
from featured articles and 11,592 from non-featured
articles), extracted from the April 2011 English
Wikipedia dump. The articles in WPQAC are care-
fully chosen to form comparable pairs of featured
and non-featured articles, which should reduce the
noise of external influences on edit activity such
as popularity or visibility. In Daxenberger and
Gurevych (2012), we have shown significant dif-
ferences in the edit category distribution of arti-
cles with featured status before and after the articles
were featured. We concluded that articles become
more stable after being featured, as shown by the
higher number of surface edits and lower number of
meaning-changing edits.
Different to our previous approach which is based
on the mere distribution of edit categories, in the
present study we include the chronological order of
edits and use a 10 times larger amount of data for our
experiments. We segmented all adjacent revisions
in WPQAC into edits, following the approach ex-
plained in Daxenberger and Gurevych (2012). Dur-
ing the classification process, we discarded revisions
where the classifier could not assign any of the 21
edit categories with a confidence higher than the
</bodyText>
<footnote confidence="0.9116815">
7http://en.wikipedia.org/wiki/Wikipedia:
FA
</footnote>
<bodyText confidence="0.999037421052632">
threshold, cf. Table 3. This resulted in 17,640 re-
maining revisions. We applied a sequential pattern
mining algorithm with time constraints (Hirate and
Yamana, 2006; Fournier-Viger et al., 2008) to the
data. The latter is based on the PrefixSpan algorithm
(Pei et al., 2004). Calculations have been carried out
within the open-source SPMF Java data mining plat-
form.8
We created one time-extended sequence database
for the 10 featured articles and one for the 10 non-
featured articles. The sequence databases consist
of one row per article. Each row is a chronologi-
cally ordered list of revisions. Each revision is rep-
resented by the itemset of all edit categories for all
edits in that revision (in alphabetical order).
The output of the algorithm are sequential pat-
terns with time constraints. To obtain meaningful
results, we constrained the output with the follow-
ing parameters:
</bodyText>
<listItem confidence="0.964738666666667">
• Minimum support: 1 (the patterns have to be
present in each article)
• Time interval allowed between two successive
itemsets in the patterns: 1 (patterns are ex-
tracted only from adjacent revisions)
• Minimum time interval between the first item-
</listItem>
<bodyText confidence="0.944956">
set and the last itemset in the patterns: 1 (the
length of the patterns is 2 or higher)
As this output reflects recurring sequences of ad-
jacent revisions labeled with edit categories, we re-
fer to it as collaboration patterns. With these pa-
rameters, the algorithm discovered 1,358 sequen-
tial patterns for featured articles and 968 for non-
featured articles. The number of shared patterns in
featured and non-featured articles is 427, this corre-
sponds to the number of frequent patterns in a se-
quence database which contains all 20 featured and
non-featured articles. The maximum length of pat-
terns which were found was 6 for featured articles,
and 5 for non-featured articles. These numbers show
that the defined collaboration patterns seem to have
discriminative power for different kinds of articles.
Featured articles can be characterized by a higher
</bodyText>
<footnote confidence="0.9763015">
8http://www.philippe-fournier-viger.
com/spmf
</footnote>
<page confidence="0.990879">
586
</page>
<table confidence="0.9981805">
1 INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 INFORMATION-I 5 INFORMATION-I
Featured 1 INFORMATION-D, INFORMATION-I 2 INFORMATION-I 3 INFORMATION-I 4 REFERENCE-I
1 TEMPLATE-D 2 REFERENCE-I
1 INFORMATION-I 2 INFORMATION-I, REFERENCE-I 3 INFORMATION-I 4 REFERENCE-I 5 MARKUP-I
1 MARKUP-I 2 REFERENCE-D 3 MARKUP-I
1 VANDALISM 2 REVERT
Non-
Featured
</table>
<tableCaption confidence="0.988982">
Table 4: Examples of collaboration patterns which have been found in either all featured or all non-featured articles of
WPQAC.
</tableCaption>
<bodyText confidence="0.999963285714286">
degree of homogeneity with respect to their collab-
orative patterns due to a higher number and length
of frequent sequential patterns in featured articles as
compared to non-featured articles.
In Table 4, we list some examples of collabora-
tion patterns with a minimum support of 1 which
we found in featured, but not non-featured arti-
cles, or vice verse. Unsurprisingly, patterns which
contain combinations of the most frequent catego-
ries (INFORMATION-I, REFERENCE-I), have a high
overall frequency. The diversity inside collaboration
patterns measured by the number of different edit
categories was higher in non-featured articles. For
example, the VANDALISM - REVERT pattern was
only found in non-featured articles. Patterns in fea-
tured articles tended to be more homogeneous, as
shown by the first pattern in Table 4, a repetition
of additions of information. We conclude that dis-
tinguished, high-quality articles, show a higher de-
gree of homogeneity as compared to a subset of non-
featured articles and the overall corpus.
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987735294117">
In this study, we evaluated a novel feature set
for building a model to automatically classify
Wikipedia edits. Using a freely available cor-
pus (Daxenberger and Gurevych, 2012), our model
achieved a micro-averaged F1 score of .62 classify-
ing edits within a range of 21 categories. Textual
features had the highest impact on classifier perfor-
mance, whereas language features play a minor role.
The same classifier model obtained state-of-the-art
performance on the related task of fluency edit clas-
sification. Applications which potentially benefit
from our work include the analysis of the writing
process in collaboratively created documents, such
as wikis or research papers. We have demonstrated
how our model can be used to detect collaboration
patterns in article revision histories. On a subset
of articles from the English Wikipedia, we found
that high-quality articles show a higher degree of
homogeneity in their collaborative patterns as com-
pared to random articles. Furthermore, automatic
edit category classification allows to generate huge
amounts of category-filtered training data for NLP
tasks, e.g. spelling and grammar correction or van-
dalism detection. With respect to future work, we
plan to include more resources, e.g. the PAN-WVC-
10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max
and Wisniewski, 2010) to increase the size of train-
ing data. A larger amount of labeled data would
certainly help to improve the classifier performance
for weak categories (e.g. VANDALISM and PARA-
PHRASE) and sparse categories (e.g. TEMPLATE-D,
MARKUP-M). Based on our trained classifier, anno-
tating more examples can be alleviated with the help
of active learning.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999675">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-Professorship
Program under grant No. I/82806, and by the
Hessian research excellence program “Landes-
Offensive zur Entwicklung Wissenschaftlich-
¨okonomischer Exzellenz” (LOEWE) as part of the
research center “Digital Humanities”. We thank the
anonymous reviewers for their valuable feedback.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9952634">
B Thomas Adler, Luca Alfaro, Santiago M Mola-
Velasco, Paolo Rosso, and Andrew G West. 2011.
Wikipedia Vandalism Detection: Combining Natural
Language, Metadata, and Reputation Features. In
Alexander Gelbukh, editor, Computational Linguistics
</reference>
<page confidence="0.98947">
587
</page>
<reference confidence="0.997982235849056">
and Intelligent Text Processing, Lecture Notes in Com-
puter Science, pages 277–288. Springer.
Ablimit Aji, Yu Wang, and Eugene Agichtein. 2010. Us-
ing the Past To Score the Present: Extending Term
Weighting Models Through Revision History Analy-
sis. ReCALL, pages 629–638.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation, held in conjunction
with the 1st Joint Conference on Lexical and Compu-
tational Semantics, pages 435–40, Montreal, Canada,
USA.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45(1):5–32.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories. In
European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 356–366, Avi-
gnon, France.
Elena Cabrio, Bernardo Magnini, and Angelina Ivanova.
2012. Extracting Context-Rich Entailment Rules from
Wikipedia Revision History. In Proceedings of the 3rd
Workshop on The People’s Web meets NLP, pages 34–
43, Jeju Island, Republic of Korea.
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 507–
517, Atlanta, GA, USA.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In Proceed-
ings of the 24th International Conference on Compu-
tational Linguistics, pages 711–726, Mumbai, India.
Hannes Dohrn and Dirk Riehle. 2011. Design and imple-
mentation of the Sweble Wikitext parser. In Proceed-
ings of the 7th International Symposium on Wikis and
Open Collaboration, pages 72–81, Mountain View,
CA, USA.
Camille Dutrey, Houda Bouamor, Delphine Bernhard,
and Aur´elien Max. 2011. Local modifications and
paraphrases in Wikipedia’s revision history. Proce-
samiento del Lenguaje Natural, 46:51–58.
Richard Eckart de Castilho, Iryna Gurevych, and
Richard Eckart de Castilho. 2011. A Lightweight
Framework for Reproducible Parameter Sweeping in
Information Retrieval. In Proceedings of the Work-
shop on Data Infrastructures for Supporting Informa-
tion Retrieval Evaluation, pages 7–10, Glasgow, UK.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Access-
ing Wikipedia’s Edit History. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
System Demonstrations, pages 97–102, Portland, OR,
USA.
Oliver Ferschke, Johannes Daxenberger, and Iryna
Gurevych. 2013. A Survey of NLP Methods and Re-
sources for Analyzing the Collaborative Writing Pro-
cess in Wikipedia. In Iryna Gurevych and Jungi Kim,
editors, The Peoples Web Meets NLP: Collaboratively
Constructed Language Resources, Theory and Appli-
cations of Natural Language Processing, chapter 5.
Springer.
Philippe Fournier-Viger, Roger Nkambou, and Engel-
bert Mephu Nguifo. 2008. A Knowledge Discovery
Framework for Learning Task Models from User Inter-
actions in Intelligent Tutoring Systems. In Alexander
Gelbukh and Eduardo F. Morales, editors, Proceedings
of the 7th Mexican International Conference on Artifi-
cial Intelligence, Lecture Notes in Computer Science,
pages 765–778. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10–18.
Yu Hirate and Hayato Yamana. 2006. Generalized Se-
quential Pattern Mining with Item Intervals. Journal
of Computers, 1(3):51–60.
Sara Javanmardi, David W. McDonald, and Cristina V.
Lopes. 2011. Vandalism Detection in Wikipedia: A
High-Performing, Feature-Rich Model and its Reduc-
tion Through Lasso. In Proceedings of the 7th Interna-
tional Symposium on Wikis and Open Collaboration,
pages 82–90, Mountain View, CA, USA.
Brian Keegan, Darren Gergle, and Noshir Contractor.
2013. Hot Off the Wiki: Structures and Dynamics
of Wikipedia’s Coverage of Breaking News Events.
American Behavioral Scientist, 57(5):595–622, May.
Jun Liu and Sudha Ram. 2011. Who does what: Col-
laboration patterns in the wikipedia and their impact
on article quality. ACM Trans. Management Inf. Syst.,
2(2):11.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Sa&amp;quot;so Dz&amp;quot;eroski. 2012. An extensive experimental
comparison of methods for multi-label learning. Pat-
tern Recognition, 45(9):3084–3104.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
</reference>
<page confidence="0.979971">
588
</page>
<reference confidence="0.999511819047619">
Language Technologies, pages 182–190, Montr´eal,
Canada.
Aur´elien Max and Guillaume Wisniewski. 2010. Mining
Naturally-occurring Corrections and Paraphrases from
Wikipedias Revision History. In Proceedings of the
7th Conference on International Language Resources
and Evaluation, Valletta, Malta.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedia’s Article Revision History for Training
Computational Linguistics Algorithms. In Proceed-
ings of the 1st AAAI Workshop on Wikipedia and Arti-
ficial Intelligence, pages 31–36, Chicago, IL, USA.
S´ergio Nunes, Cristina Ribeiro, and Gabriel David. 2011.
Term weighting based on document revision history.
Journal of the American Society for Information Sci-
ence and Technology, 62(12):2471–2478.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC), pages 32–38, Marrakech, Morocco.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal,
and Mei-Chun Hsu. 2004. Mining sequential patterns
by pattern-growth: the PrefixSpan approach. IEEE
Transactions on Knowledge and Data Engineering,
16(11):1424–1440.
Martin Potthast and Teresa Holfeld. 2011. Overview of
the 2nd International Competition on Wikipedia Van-
dalism Detection. In Notebook Papers of CLEF 2011
Labs and Workshops, Amsterdam, Netherlands.
Martin Potthast, Tim Gollub, Matthias Hagen, Johannes
Kiesel, Maximilian Michel, Arnd Oberl¨ander, Mar-
tin Tippmann, Alberto Barr´on-Cede˜no, Parth Gupta,
Paolo Rosso, and Benno Stein. 2012. Overview of
the 4th International Competition on Plagiarism De-
tection. In CLEF 2012 Evaluation Labs and Workshop
Working Notes Papers, Rome, Italy.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic Models for Analyz-
ing and Detecting Biased Language. In Proceedings of
the 51st Annual Meeting on Association for Computa-
tional Linguistics, pages 1650–1659, Sofia, Bulgaria.
Bongwon Suh, Gregorio Convertino, Ed H. Chi, and Pe-
ter Pirolli. 2009. The singularity is not near: slow-
ing growth of Wikipedia. In Proceedings of the 5th
International Symposium on Wikis and Open Collabo-
ration, Orlando, FL, USA.
Konstantinos Trohidis, Grigorios Tsoumakas, George
Kalliris, and Ioannis Vlahavas. 2008. Multi-label
classification of music into emotions. In 9th Inter-
national Conference on Music Information Retrieval,
pages 325–330, Philadelphia, PA, USA.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2008. Effective and Efficient Multilabel Classi-
fication in Domains with Large Number of Labels. In
Proceedings of the ECML/PKDD 2008 Workshop on
Mining Multidimensional Data, Antwerp, Belgium.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining and
Knowledge Discovery Handbook, chapter 34, pages
667–685. Springer.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2011. Random k-Labelsets for Multi-Label
Classification. IEEE Transactions on Knowledge and
Data Engineering, 23(7):1079–1089.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 409–420, Edinburgh, Scot-
land, UK.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sentence
Compression. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Short Papers,
pages 137–140, Columbus, OH, USA.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from Wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ’10, pages 365–368, Los Angeles, CA, USA.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the COLING-Workshop on The People’s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 28–36, Beijing, China.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic Re-
latedness. In Proceedings of the Twenty-Third AAAI
Conference on Artificial Intelligence, pages 861–866,
Chicago, IL, USA.
Torsten Zesch. 2012. Measuring Contextual Fitness Us-
ing Error Contexts Extracted from the Wikipedia Revi-
sion History. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 529–538, Avignon, France.
</reference>
<page confidence="0.998821">
589
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465858">
<title confidence="0.99786">Automatically Classifying Edit Categories in Wikipedia Revisions</title>
<author confidence="0.594098">Iryna</author>
<affiliation confidence="0.8987255">Knowledge Processing Lab Department of Computer Science, Technische Universit¨at Darmstadt Center for Education German Institute for Educational Research and Educational Information</affiliation>
<web confidence="0.984921">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.999410173913044">In this paper, we analyze a novel set of features for the task of automatic edit category classification. Edit category classification assigns categories such as spelling error correction, paraphrase or vandalism to edits in a document. Our features are based on differences between two versions of a document including meta data, textual and language properties and markup. In a supervised machine learning experiment, we achieve a micro-averaged F1 score of .62 on a corpus of edits from the English Wikipedia. In this corpus, each edit has been multi-labeled according to a 21-category taxonomy. A model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification. We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles. Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Thomas Adler</author>
<author>Luca Alfaro</author>
<author>Santiago M MolaVelasco</author>
<author>Paolo Rosso</author>
<author>Andrew G West</author>
</authors>
<title>Wikipedia Vandalism Detection: Combining Natural Language, Metadata, and Reputation Features.</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing, Lecture Notes in Computer Science,</booktitle>
<pages>277--288</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="8736" citStr="Adler et al. (2011)" startWordPosition="1340" endWordPosition="1343">d both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection in Wikipedia has mostly been defined as a binary machine learning task, where the goal is to classify a pair of adjacent revisions as vandalized or not-vandalized based on edit category features. In Adler et al. (2011), the authors group these features into meta data (author, comment and time stamp of a revision), reputation (author and article reputation), textual (language independent, i.e. token- and character-based) and language features (language dependent, mostly dictionary-based). They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011). Classifiers based on reputation and text performed best. Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments. This classifier was also used in the vandalism detection study of Javanmardi e</context>
<context position="15378" citStr="Adler et al. (2011)" startWordPosition="2384" endWordPosition="2387">1 Explicit Semantic Analysis with vector indexes from Wiktionary Diff POS tags* false POS tag sets are symmetrically different Diff type POS tags* 0 Number of distinct POS tags 1 N-gram features are represented as boolean features. 2 In this example, n = 1 (unigrams). 3 True if m corresponds to internal link, false otherwise. Table 1: List of edit category classification features with explanations. The values correspond to the the example edit from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with * have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012). 581 M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf’s α is .67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit. WPEC consists of 981 revision pairs, segmented into 1,995 edits. We define edit category classification as a multi-label classification task. For the sake of readability, in the following we will refer to </context>
<context position="16610" citStr="Adler et al. (2011)" startWordPosition="2610" endWordPosition="2613">,v as ei, with ei E E, where 0 &lt; i &lt; 1995 and E is the set of all edits. An edit ei is the basic classification unit in our task. Each ei has to be labeled with a set of categories y C C, where C is the set of all edit categories, |C |= 21. 3.2 Features for Edit Category Classification We grouped our features into meta data, textual, markup and language features. An overview and explanation of all features can be found in Table 1. The scheme we apply to group edit category classification features is similar to the system used by Adler et al. (2011). We re-use some of the features suggested by Adler et al. (2011), Javanmardi et al. (2011) and Bronner and Monz (2012), as marked in Table 1. Features are calculated on edited text spans. We label the edited text span corresponding to ei in rv−1 as tv−1 and the edited text span in rv as tv. In edits which are insertions, we consider tv−1 to be empty, while tv is considered empty for deletions. For Relocations, tv−1 = tv. Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a specification to the target of that link. For spell-checking, we use British and US-American English</context>
<context position="30601" citStr="Adler et al., 2011" startWordPosition="4937" endWordPosition="4940">g split from Table 2, we trained a Random Forests classifier with the optimized feature set and feature reduction as described in Section 3.3. The number of trees was set to 100, with unlimited depth. On the remaining data (test and development split), we achieved an accuracy of .90. Although we did not use the same data set as Bronner and Monz (2012), this result suggests that our feature set is suited for related tasks such as fluency detection. With respect to vandalism detection in Wikipedia, state-of-the-art systems have a performance of around .82 to .85 AUC-PR on the English Wikipedia (Adler et al., 2011). We suspect that the low performance of our system for Vandalism edits is mostly due to a lower amount of training data, a higher skew in the training and test data and the fact that we did not include features which inform about future actions (e.g. whether a revision is reverted). Error Analysis Sparseness is a major problem for some of the 21 categories, as shown in Figure 2 by categories such as FILE-D, TEMPLATED, MARKUP-M or PARAPHRASE which have only very few examples in training, development and test set. Categories with low inter-annotator agreement in WPEC such as MARKUP-M, PARAPHRAS</context>
</contexts>
<marker>Adler, Alfaro, MolaVelasco, Rosso, West, 2011</marker>
<rawString>B Thomas Adler, Luca Alfaro, Santiago M MolaVelasco, Paolo Rosso, and Andrew G West. 2011. Wikipedia Vandalism Detection: Combining Natural Language, Metadata, and Reputation Features. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, Lecture Notes in Computer Science, pages 277–288. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ablimit Aji</author>
<author>Yu Wang</author>
<author>Eugene Agichtein</author>
</authors>
<title>Using the Past To Score the Present: Extending Term Weighting Models Through Revision History Analysis. ReCALL,</title>
<date>2010</date>
<pages>629--638</pages>
<contexts>
<context position="7919" citStr="Aji et al., 2010" startWordPosition="1212" endWordPosition="1215">ight kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection i</context>
</contexts>
<marker>Aji, Wang, Agichtein, 2010</marker>
<rawString>Ablimit Aji, Yu Wang, and Eugene Agichtein. 2010. Using the Past To Score the Present: Extending Term Weighting Models Through Revision History Analysis. ReCALL, pages 629–638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--40</pages>
<location>Montreal, Canada, USA.</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, pages 435–40, Montreal, Canada, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="9235" citStr="Breiman, 2001" startWordPosition="1413" endWordPosition="1414"> pair of adjacent revisions as vandalized or not-vandalized based on edit category features. In Adler et al. (2011), the authors group these features into meta data (author, comment and time stamp of a revision), reputation (author and article reputation), textual (language independent, i.e. token- and character-based) and language features (language dependent, mostly dictionary-based). They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011). Classifiers based on reputation and text performed best. Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments. This classifier was also used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes. Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multilabeling and a fine-grained edit category system. Our feature set builds upon existing work while adding a substantial number of new features. 3 Experiments 3.1 Wikipedia Edit Category Corpus For our experiments, we used the freely available Wikipedia Edit Cate</context>
<context position="29328" citStr="Breiman, 2001" startWordPosition="4726" endWordPosition="4727">e F1 scores as well as F1 scores for classifiers trained and tested on single feature groups, cf. Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set. markup elements) and meta data (Number of edits) features. Bronner and Monz (2012) report an accuracy of .88 for their best performing system on the binary classification task of distinguishing fluency and factual edits. The best performing classifier in their study was Random Forests (Breiman, 2001). To compare our features with their approach, we mapped the 21 edit categories from Daxenberger and Gurevych (2012) to the binary category set (factual vs. fluency) of Bronner and Monz (2012). Edits labeled as SPELLING/GRAMMAR, MARKUP, RELOCATION and PARAPHRASE are considered fluency edits, the remaining categories factual edits. We removed all edits labeled as OTHER, REVERT or VANDALISM from WPEC. After applying the category mapping, we deleted all edits which were labeled with both the fluency and factual category. The latter may happen due to multi-labeling. This resulted in 1,262 edits la</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Machine Learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bronner</author>
<author>Christof Monz</author>
</authors>
<title>User Edits Classification Using Document Revision Histories.</title>
<date>2012</date>
<booktitle>In European Chapter of the Association for Computational Linguistics (EACL 2012),</booktitle>
<pages>356--366</pages>
<location>Avignon, France.</location>
<contexts>
<context position="6051" citStr="Bronner and Monz, 2012" startWordPosition="921" endWordPosition="924">ions of the same text. This scenario implies certain characteristics for a well-designed feature set as we will demonstrate in this study. The main contributions of this paper are: First, we introduce a novel feature set for edit category classification. Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits. We propose the new task of edit category classification and show that our model is able to classify edits from a 21-category taxonomy. Furthermore, our model achieves state-of-theart performance in a fluency edit classification task (Bronner and Monz, 2012). Third, we analyze collaboration patterns based on edit categories on two subsets of Wikipedia articles, namely featured and non-featured articles. We detect correlations between collaboration patterns and high-quality articles. This is demonstrated by the fact that featured articles have a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles. The rest of this paper is structured as follows. In Section 2, we motivate our experiments based on previous work. Section 3 explains our training data and the features we use for the machine learning </context>
<context position="8008" citStr="Bronner and Monz (2012)" startWordPosition="1228" endWordPosition="1231">tion of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection in Wikipedia has mostly been defined as a binary machine learning task, where the goal is </context>
<context position="9485" citStr="Bronner and Monz (2012)" startWordPosition="1450" endWordPosition="1453">le reputation), textual (language independent, i.e. token- and character-based) and language features (language dependent, mostly dictionary-based). They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011). Classifiers based on reputation and text performed best. Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments. This classifier was also used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes. Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multilabeling and a fine-grained edit category system. Our feature set builds upon existing work while adding a substantial number of new features. 3 Experiments 3.1 Wikipedia Edit Category Corpus For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012). In this corpus, each pair of adjacent revisions is segmented into one or more edits. This enables an accurate picture of the editing process, as an author may perform seve</context>
<context position="15431" citStr="Bronner and Monz (2012)" startWordPosition="2393" endWordPosition="2396">from Wiktionary Diff POS tags* false POS tag sets are symmetrically different Diff type POS tags* 0 Number of distinct POS tags 1 N-gram features are represented as boolean features. 2 In this example, n = 1 (unigrams). 3 True if m corresponds to internal link, false otherwise. Table 1: List of edit category classification features with explanations. The values correspond to the the example edit from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with * have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012). 581 M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf’s α is .67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit. WPEC consists of 981 revision pairs, segmented into 1,995 edits. We define edit category classification as a multi-label classification task. For the sake of readability, in the following we will refer to an edit ekv−1,v as ei, with ei E E, where 0 &lt; i &lt; 199</context>
<context position="16664" citStr="Bronner and Monz (2012)" startWordPosition="2619" endWordPosition="2622"> the set of all edits. An edit ei is the basic classification unit in our task. Each ei has to be labeled with a set of categories y C C, where C is the set of all edit categories, |C |= 21. 3.2 Features for Edit Category Classification We grouped our features into meta data, textual, markup and language features. An overview and explanation of all features can be found in Table 1. The scheme we apply to group edit category classification features is similar to the system used by Adler et al. (2011). We re-use some of the features suggested by Adler et al. (2011), Javanmardi et al. (2011) and Bronner and Monz (2012), as marked in Table 1. Features are calculated on edited text spans. We label the edited text span corresponding to ei in rv−1 as tv−1 and the edited text span in rv as tv. In edits which are insertions, we consider tv−1 to be empty, while tv is considered empty for deletions. For Relocations, tv−1 = tv. Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a specification to the target of that link. For spell-checking, we use British and US-American English Jazzy dictionaries.3 Markup elements are detected by </context>
<context position="29109" citStr="Bronner and Monz (2012)" startWordPosition="4690" endWordPosition="4693"> 584 F1 score 0.8 0.6 0.4 0.2 0 1 Human Classifier Textual Markup Meta Data Language Figure 2: F1 scores of RAKEL with C4.5 as base classifier for individual categories. We add human inter-annotator agreement as average pair-wise F1 scores as well as F1 scores for classifiers trained and tested on single feature groups, cf. Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set. markup elements) and meta data (Number of edits) features. Bronner and Monz (2012) report an accuracy of .88 for their best performing system on the binary classification task of distinguishing fluency and factual edits. The best performing classifier in their study was Random Forests (Breiman, 2001). To compare our features with their approach, we mapped the 21 edit categories from Daxenberger and Gurevych (2012) to the binary category set (factual vs. fluency) of Bronner and Monz (2012). Edits labeled as SPELLING/GRAMMAR, MARKUP, RELOCATION and PARAPHRASE are considered fluency edits, the remaining categories factual edits. We removed all edits labeled as OTHER, REVERT or</context>
<context position="30335" citStr="Bronner and Monz (2012)" startWordPosition="4894" endWordPosition="4898">NDALISM from WPEC. After applying the category mapping, we deleted all edits which were labeled with both the fluency and factual category. The latter may happen due to multi-labeling. This resulted in 1,262 edits labeled as either fluency or factual. On the 80% training split from Table 2, we trained a Random Forests classifier with the optimized feature set and feature reduction as described in Section 3.3. The number of trees was set to 100, with unlimited depth. On the remaining data (test and development split), we achieved an accuracy of .90. Although we did not use the same data set as Bronner and Monz (2012), this result suggests that our feature set is suited for related tasks such as fluency detection. With respect to vandalism detection in Wikipedia, state-of-the-art systems have a performance of around .82 to .85 AUC-PR on the English Wikipedia (Adler et al., 2011). We suspect that the low performance of our system for Vandalism edits is mostly due to a lower amount of training data, a higher skew in the training and test data and the fact that we did not include features which inform about future actions (e.g. whether a revision is reverted). Error Analysis Sparseness is a major problem for </context>
</contexts>
<marker>Bronner, Monz, 2012</marker>
<rawString>Amit Bronner and Christof Monz. 2012. User Edits Classification Using Document Revision Histories. In European Chapter of the Association for Computational Linguistics (EACL 2012), pages 356–366, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Bernardo Magnini</author>
<author>Angelina Ivanova</author>
</authors>
<title>Extracting Context-Rich Entailment Rules from Wikipedia Revision History.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on The People’s Web meets NLP,</booktitle>
<pages>pages</pages>
<location>Republic of</location>
<contexts>
<context position="7878" citStr="Cabrio et al., 2012" startWordPosition="1206" endWordPosition="1209"> manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement</context>
</contexts>
<marker>Cabrio, Magnini, Ivanova, 2012</marker>
<rawString>Elena Cabrio, Bernardo Magnini, and Angelina Ivanova. 2012. Extracting Context-Rich Entailment Rules from Wikipedia Revision History. In Proceedings of the 3rd Workshop on The People’s Web meets NLP, pages 34– 43, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Diane Napolitano</author>
</authors>
<title>Robust Systems for Preposition Error Correction Using Wikipedia Revisions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>507--517</pages>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="2177" citStr="Cahill et al., 2013" startWordPosition="321" endWordPosition="324">d in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near 1http://stats.wikimedia.org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classificat</context>
<context position="7554" citStr="Cahill et al., 2013" startWordPosition="1160" endWordPosition="1163">on 6. 2 Related Work Wikipedia is a huge data source for generating training data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. </context>
</contexts>
<marker>Cahill, Madnani, Tetreault, Napolitano, 2013</marker>
<rawString>Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane Napolitano. 2013. Robust Systems for Preposition Error Correction Using Wikipedia Revisions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 507– 517, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Iryna Gurevych</author>
</authors>
<title>A Corpus-Based Study of Edit Categories in Featured and Non-Featured Wikipedia Articles.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>711--726</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="2849" citStr="Daxenberger and Gurevych, 2012" startWordPosition="423" endWordPosition="426"> or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxonomy enables a finegrained analysis of edit activity in revision histories. We present the results from an automatic classification experiment, based on an annotated corpus of edits in the English Wikipedia. Additional information necessary to reproduce our results, including word lists and training, development and test data, is released online.2 To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning 2http://www.ukp.tu-darmstadt.de/data/ edit-cla</context>
<context position="9912" citStr="Daxenberger and Gurevych, 2012" startWordPosition="1516" endWordPosition="1519">o used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes. Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multilabeling and a fine-grained edit category system. Our feature set builds upon existing work while adding a substantial number of new features. 3 Experiments 3.1 Wikipedia Edit Category Corpus For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012). In this corpus, each pair of adjacent revisions is segmented into one or more edits. This enables an accurate picture of the editing process, as an author may perform several independent edits in the same revision. Furthermore, edits are multi-labeled, i.e. each edit is assigned one or more categories. This is important for a precise description of major edits, e.g. when an entire new paragraph including text, references and markup is added. There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations. These are calculated via a line-based diff comparison </context>
<context position="15622" citStr="Daxenberger and Gurevych, 2012" startWordPosition="2423" endWordPosition="2426">2 In this example, n = 1 (unigrams). 3 True if m corresponds to internal link, false otherwise. Table 1: List of edit category classification features with explanations. The values correspond to the the example edit from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with * have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012). 581 M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf’s α is .67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit. WPEC consists of 981 revision pairs, segmented into 1,995 edits. We define edit category classification as a multi-label classification task. For the sake of readability, in the following we will refer to an edit ekv−1,v as ei, with ei E E, where 0 &lt; i &lt; 1995 and E is the set of all edits. An edit ei is the basic classification unit in our task. Each ei has to be labeled with a set of categories y C C, where C is the set of all edit categories, </context>
<context position="26242" citStr="Daxenberger and Gurevych (2012)" startWordPosition="4215" endWordPosition="4218">es. It becomes smaller when the performance of the classifier increases. Table 3 shows the overall classification scores. We calculated a random baseline, which multi-labels edits at random considering the label powerset frequencies it has learned from the training data. Furthermore, we calculated a majority category baseline, which labels all edits with the most frequent edit category in the training data. In Figure 2, we list the results for each category, together with the average pair-wise inter-rater agreement (F1 scores). The F1 scores are calculated based on the study we carried out in Daxenberger and Gurevych (2012). Parameters and Feature selection All parameters have been adjusted on the development set using the RAKEL classifier, aiming to optimize accuracy. With respect to the n-gram features, we tested values for n = 1, 2 and 3. For comment n-grams, unigrams turned out to yield the best overall performance, and bigrams for character and token ngrams. The word and character n-gram spaces are limited to the 500 most frequent items, the comment n-gram space is limited to the 1,500 most frequent items. To transform ranked output into bipartitions, it is necessary to set a threshold. This threshold is re</context>
<context position="29444" citStr="Daxenberger and Gurevych (2012)" startWordPosition="4742" endWordPosition="4745">able 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M and TEMPLATE-M categories are omitted in this Figure, as they had no examples in the development or test set. markup elements) and meta data (Number of edits) features. Bronner and Monz (2012) report an accuracy of .88 for their best performing system on the binary classification task of distinguishing fluency and factual edits. The best performing classifier in their study was Random Forests (Breiman, 2001). To compare our features with their approach, we mapped the 21 edit categories from Daxenberger and Gurevych (2012) to the binary category set (factual vs. fluency) of Bronner and Monz (2012). Edits labeled as SPELLING/GRAMMAR, MARKUP, RELOCATION and PARAPHRASE are considered fluency edits, the remaining categories factual edits. We removed all edits labeled as OTHER, REVERT or VANDALISM from WPEC. After applying the category mapping, we deleted all edits which were labeled with both the fluency and factual category. The latter may happen due to multi-labeling. This resulted in 1,262 edits labeled as either fluency or factual. On the 80% training split from Table 2, we trained a Random Forests classifier w</context>
<context position="32479" citStr="Daxenberger and Gurevych, 2012" startWordPosition="5247" endWordPosition="5250"> the categories. The imbalance of the data, because of the high skew in the category distribution, is another reason for classification errors. In ambiguous cases, the classifier will be biased toward the category with more examples in the training data. 5 A closer look at edit sequences: Mining collaboration patterns An edit category classifier allows us to label entire article revision histories. We applied the bestperforming model from Section 3.3 trained on the entire WPEC to automatically classify all edits in the Wikipedia Quality Assessment Corpus (WPQAC) as presented in previous work (Daxenberger and Gurevych, 2012). WPQAC consists of 10 featured and 10 non-featured articles7, with an overall number of 21,578 revisions (9,986 revisions from featured articles and 11,592 from non-featured articles), extracted from the April 2011 English Wikipedia dump. The articles in WPQAC are carefully chosen to form comparable pairs of featured and non-featured articles, which should reduce the noise of external influences on edit activity such as popularity or visibility. In Daxenberger and Gurevych (2012), we have shown significant differences in the edit category distribution of articles with featured status before a</context>
<context position="37546" citStr="Daxenberger and Gurevych, 2012" startWordPosition="6037" endWordPosition="6040"> categories was higher in non-featured articles. For example, the VANDALISM - REVERT pattern was only found in non-featured articles. Patterns in featured articles tended to be more homogeneous, as shown by the first pattern in Table 4, a repetition of additions of information. We conclude that distinguished, high-quality articles, show a higher degree of homogeneity as compared to a subset of nonfeatured articles and the overall corpus. 6 Conclusion In this study, we evaluated a novel feature set for building a model to automatically classify Wikipedia edits. Using a freely available corpus (Daxenberger and Gurevych, 2012), our model achieved a micro-averaged F1 score of .62 classifying edits within a range of 21 categories. Textual features had the highest impact on classifier performance, whereas language features play a minor role. The same classifier model obtained state-of-the-art performance on the related task of fluency edit classification. Applications which potentially benefit from our work include the analysis of the writing process in collaboratively created documents, such as wikis or research papers. We have demonstrated how our model can be used to detect collaboration patterns in article revisio</context>
</contexts>
<marker>Daxenberger, Gurevych, 2012</marker>
<rawString>Johannes Daxenberger and Iryna Gurevych. 2012. A Corpus-Based Study of Edit Categories in Featured and Non-Featured Wikipedia Articles. In Proceedings of the 24th International Conference on Computational Linguistics, pages 711–726, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannes Dohrn</author>
<author>Dirk Riehle</author>
</authors>
<title>Design and implementation of the Sweble Wikitext parser.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th International Symposium on Wikis and Open Collaboration,</booktitle>
<pages>72--81</pages>
<location>Mountain View, CA, USA.</location>
<contexts>
<context position="17315" citStr="Dohrn and Riehle, 2011" startWordPosition="2732" endWordPosition="2735">ures are calculated on edited text spans. We label the edited text span corresponding to ei in rv−1 as tv−1 and the edited text span in rv as tv. In edits which are insertions, we consider tv−1 to be empty, while tv is considered empty for deletions. For Relocations, tv−1 = tv. Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a specification to the target of that link. For spell-checking, we use British and US-American English Jazzy dictionaries.3 Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011). Meta data features We consider the comment, author, time stamp or any other flag (“minor change”) of rv as meta data. The Wikimedia user group4 of an author specifies the edit permissions 3http://sourceforge.net/projects/ jazzydicts 4http://meta.wikimedia.org/wiki/User_ classes of this user (e.g. bot, administrator, blocked user). We indicate whether the revision comments or parts of it have been auto-generated. This happens when a page is blanked, i.e. all of its content has been deleted or replaced or when a new page or redirect is created (denoted by the Comment is auto-generated feature)</context>
</contexts>
<marker>Dohrn, Riehle, 2011</marker>
<rawString>Hannes Dohrn and Dirk Riehle. 2011. Design and implementation of the Sweble Wikitext parser. In Proceedings of the 7th International Symposium on Wikis and Open Collaboration, pages 72–81, Mountain View, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Dutrey</author>
<author>Houda Bouamor</author>
<author>Delphine Bernhard</author>
<author>Aur´elien Max</author>
</authors>
<title>Local modifications and paraphrases in Wikipedia’s revision history.</title>
<date>2011</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>46--51</pages>
<contexts>
<context position="7802" citStr="Dutrey et al., 2011" startWordPosition="1195" endWordPosition="1198">extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, cha</context>
</contexts>
<marker>Dutrey, Bouamor, Bernhard, Max, 2011</marker>
<rawString>Camille Dutrey, Houda Bouamor, Delphine Bernhard, and Aur´elien Max. 2011. Local modifications and paraphrases in Wikipedia’s revision history. Procesamiento del Lenguaje Natural, 46:51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
<author>Richard Eckart de Castilho</author>
</authors>
<title>A Lightweight Framework for Reproducible Parameter Sweeping in Information Retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Data Infrastructures for Supporting Information Retrieval Evaluation,</booktitle>
<pages>7--10</pages>
<location>Glasgow, UK.</location>
<marker>de Castilho, Gurevych, de Castilho, 2011</marker>
<rawString>Richard Eckart de Castilho, Iryna Gurevych, and Richard Eckart de Castilho. 2011. A Lightweight Framework for Reproducible Parameter Sweeping in Information Retrieval. In Proceedings of the Workshop on Data Infrastructures for Supporting Information Retrieval Evaluation, pages 7–10, Glasgow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations,</booktitle>
<pages>97--102</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="10960" citStr="Ferschke et al., 2011" startWordPosition="1685" endWordPosition="1688"> and markup is added. There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations. These are calculated via a line-based diff comparison on the source text (including wiki markup). As previously suggested (Daxenberger and Gurevych, 2012), inside modified lines, only the span of text which has actually been changed is marked as edit (either Insertion, Deletion or Modification), not the entire line. We extracted the data which is not contained in WPEC (meta data and plain text of r„_1 and r„) using the Java Wikipedia Library (JWPL) with the Revision Toolkit (Ferschke et al., 2011). In Daxenberger and Gurevych (2012), we divide the 21-category taxonomy into text-base (meaningchanging edits), surface (non meaning-changing edits) and Wikipedia policy (VANDALISM and REVERT) edits. Among the text-base edits, we include categories for templates, references (internal and external links), files and information, each of which is further divided into an insertion (I), deletion (D) and modification (M) category. Surface edits consist of paraphrases, spelling and grammar corrections, relocations and markup edits. The latter category contains all edits which affect markup elements </context>
</contexts>
<marker>Ferschke, Zesch, Gurevych, 2011</marker>
<rawString>Oliver Ferschke, Torsten Zesch, and Iryna Gurevych. 2011. Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations, pages 97–102, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Johannes Daxenberger</author>
<author>Iryna Gurevych</author>
</authors>
<title>A Survey of NLP Methods and Resources for Analyzing the Collaborative Writing Process in Wikipedia.</title>
<date>2013</date>
<booktitle>In Iryna Gurevych</booktitle>
<editor>and Jungi Kim, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2378" citStr="Ferschke et al. (2013)" startWordPosition="353" endWordPosition="356">org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxonomy enables a finegrained analysis of edit activity in revision histories. We present the results from an automatic c</context>
</contexts>
<marker>Ferschke, Daxenberger, Gurevych, 2013</marker>
<rawString>Oliver Ferschke, Johannes Daxenberger, and Iryna Gurevych. 2013. A Survey of NLP Methods and Resources for Analyzing the Collaborative Writing Process in Wikipedia. In Iryna Gurevych and Jungi Kim, editors, The Peoples Web Meets NLP: Collaboratively Constructed Language Resources, Theory and Applications of Natural Language Processing, chapter 5. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Fournier-Viger</author>
<author>Roger Nkambou</author>
<author>Engelbert Mephu Nguifo</author>
</authors>
<title>A Knowledge Discovery Framework for Learning Task Models from User Interactions in Intelligent Tutoring Systems.</title>
<date>2008</date>
<booktitle>In Alexander Gelbukh and</booktitle>
<pages>765--778</pages>
<editor>Eduardo F. Morales, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="34021" citStr="Fournier-Viger et al., 2008" startWordPosition="5484" endWordPosition="5487"> include the chronological order of edits and use a 10 times larger amount of data for our experiments. We segmented all adjacent revisions in WPQAC into edits, following the approach explained in Daxenberger and Gurevych (2012). During the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the 7http://en.wikipedia.org/wiki/Wikipedia: FA threshold, cf. Table 3. This resulted in 17,640 remaining revisions. We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data. The latter is based on the PrefixSpan algorithm (Pei et al., 2004). Calculations have been carried out within the open-source SPMF Java data mining platform.8 We created one time-extended sequence database for the 10 featured articles and one for the 10 nonfeatured articles. The sequence databases consist of one row per article. Each row is a chronologically ordered list of revisions. Each revision is represented by the itemset of all edit categories for all edits in that revision (in alphabetical order). The output of the algorithm are sequential patterns with time constraints. </context>
</contexts>
<marker>Fournier-Viger, Nkambou, Nguifo, 2008</marker>
<rawString>Philippe Fournier-Viger, Roger Nkambou, and Engelbert Mephu Nguifo. 2008. A Knowledge Discovery Framework for Learning Task Models from User Interactions in Intelligent Tutoring Systems. In Alexander Gelbukh and Eduardo F. Morales, editors, Proceedings of the 7th Mexican International Conference on Artificial Intelligence, Lecture Notes in Computer Science, pages 765–778. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="21289" citStr="Hall et al., 2009" startWordPosition="3414" endWordPosition="3417">ge Language features are calculated on the context sv_1 and sv of edits, any wiki markup is deleted. For the Explicit Semantic Analysis, we use Wiktionary (Zesch et al., 2008) and not Wikipedia assuming that the former has a better coverage with respect to different lexical classes. POS tagging was carried out using the OpenNLP POS tagger.5 The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web. 3.3 Experimental Setup We extract features with the help of ClearTK (Ogren et al., 2008). For the machine learning part, we use Weka (Hall et al., 2009) with the Meka6 and Mulan (Tsoumakas et al., 2010) extensions for multilabel classification. We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter combinations. We randomly split the gold standard data from WPEC into 80% training, 10% test and 10% development set, as shown in Table 2. Multi-label Classification We report the performance of various machine learning algorithms. A comprehensive overview of multi-label classification algorithms and evaluation measures can be 5Maxent model for English, http://opennlp. apache.org 6http://meka.sourceforge.net Threshold – – .1</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hirate</author>
<author>Hayato Yamana</author>
</authors>
<title>Generalized Sequential Pattern Mining with Item Intervals.</title>
<date>2006</date>
<journal>Journal of Computers,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="33991" citStr="Hirate and Yamana, 2006" startWordPosition="5480" endWordPosition="5483">, in the present study we include the chronological order of edits and use a 10 times larger amount of data for our experiments. We segmented all adjacent revisions in WPQAC into edits, following the approach explained in Daxenberger and Gurevych (2012). During the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the 7http://en.wikipedia.org/wiki/Wikipedia: FA threshold, cf. Table 3. This resulted in 17,640 remaining revisions. We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data. The latter is based on the PrefixSpan algorithm (Pei et al., 2004). Calculations have been carried out within the open-source SPMF Java data mining platform.8 We created one time-extended sequence database for the 10 featured articles and one for the 10 nonfeatured articles. The sequence databases consist of one row per article. Each row is a chronologically ordered list of revisions. Each revision is represented by the itemset of all edit categories for all edits in that revision (in alphabetical order). The output of the algorithm are sequential pa</context>
</contexts>
<marker>Hirate, Yamana, 2006</marker>
<rawString>Yu Hirate and Hayato Yamana. 2006. Generalized Sequential Pattern Mining with Item Intervals. Journal of Computers, 1(3):51–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Javanmardi</author>
<author>David W McDonald</author>
<author>Cristina V Lopes</author>
</authors>
<title>Vandalism Detection in Wikipedia: A High-Performing, Feature-Rich Model and its Reduction Through Lasso.</title>
<date>2011</date>
<booktitle>In Proceedings of the 7th International Symposium on Wikis and Open Collaboration,</booktitle>
<pages>82--90</pages>
<location>Mountain View, CA, USA.</location>
<contexts>
<context position="9348" citStr="Javanmardi et al. (2011)" startWordPosition="1429" endWordPosition="1432">t al. (2011), the authors group these features into meta data (author, comment and time stamp of a revision), reputation (author and article reputation), textual (language independent, i.e. token- and character-based) and language features (language dependent, mostly dictionary-based). They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011). Classifiers based on reputation and text performed best. Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments. This classifier was also used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes. Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multilabeling and a fine-grained edit category system. Our feature set builds upon existing work while adding a substantial number of new features. 3 Experiments 3.1 Wikipedia Edit Category Corpus For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012). In this corpus, each pair of adjac</context>
<context position="15404" citStr="Javanmardi et al. (2011)" startWordPosition="2388" endWordPosition="2391">nalysis with vector indexes from Wiktionary Diff POS tags* false POS tag sets are symmetrically different Diff type POS tags* 0 Number of distinct POS tags 1 N-gram features are represented as boolean features. 2 In this example, n = 1 (unigrams). 3 True if m corresponds to internal link, false otherwise. Table 1: List of edit category classification features with explanations. The values correspond to the the example edit from Figure 1. m may refer to internal link, external link, image, template or markup element. Features marked with * have previously been mentioned in Adler et al. (2011), Javanmardi et al. (2011) or Bronner and Monz (2012). 581 M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf’s α is .67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit. WPEC consists of 981 revision pairs, segmented into 1,995 edits. We define edit category classification as a multi-label classification task. For the sake of readability, in the following we will refer to an edit ekv−1,v as ei, wit</context>
<context position="16636" citStr="Javanmardi et al. (2011)" startWordPosition="2614" endWordPosition="2617">, where 0 &lt; i &lt; 1995 and E is the set of all edits. An edit ei is the basic classification unit in our task. Each ei has to be labeled with a set of categories y C C, where C is the set of all edit categories, |C |= 21. 3.2 Features for Edit Category Classification We grouped our features into meta data, textual, markup and language features. An overview and explanation of all features can be found in Table 1. The scheme we apply to group edit category classification features is similar to the system used by Adler et al. (2011). We re-use some of the features suggested by Adler et al. (2011), Javanmardi et al. (2011) and Bronner and Monz (2012), as marked in Table 1. Features are calculated on edited text spans. We label the edited text span corresponding to ei in rv−1 as tv−1 and the edited text span in rv as tv. In edits which are insertions, we consider tv−1 to be empty, while tv is considered empty for deletions. For Relocations, tv−1 = tv. Table 1 includes the value of each feature for the example edit from Figure 1. This edit modifies the link [[Dactyl|Dactylic]] by adding a specification to the target of that link. For spell-checking, we use British and US-American English Jazzy dictionaries.3 Mark</context>
</contexts>
<marker>Javanmardi, McDonald, Lopes, 2011</marker>
<rawString>Sara Javanmardi, David W. McDonald, and Cristina V. Lopes. 2011. Vandalism Detection in Wikipedia: A High-Performing, Feature-Rich Model and its Reduction Through Lasso. In Proceedings of the 7th International Symposium on Wikis and Open Collaboration, pages 82–90, Mountain View, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Keegan</author>
<author>Darren Gergle</author>
<author>Noshir Contractor</author>
</authors>
<title>Hot Off the Wiki: Structures and Dynamics of Wikipedia’s Coverage of Breaking News Events.</title>
<date>2013</date>
<journal>American Behavioral Scientist,</journal>
<volume>57</volume>
<issue>5</issue>
<contexts>
<context position="1896" citStr="Keegan et al., 2013" startWordPosition="278" endWordPosition="281">with respect to their collaboration patterns as compared to random articles. 1 Introduction Due to its ever-evolving and collaboratively built content, Wikipedia has been the subject of many NLP studies. While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near 1http://stats.wikimedia.org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and </context>
</contexts>
<marker>Keegan, Gergle, Contractor, 2013</marker>
<rawString>Brian Keegan, Darren Gergle, and Noshir Contractor. 2013. Hot Off the Wiki: Structures and Dynamics of Wikipedia’s Coverage of Breaking News Events. American Behavioral Scientist, 57(5):595–622, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Liu</author>
<author>Sudha Ram</author>
</authors>
<title>Who does what: Collaboration patterns in the wikipedia and their impact on article quality.</title>
<date>2011</date>
<journal>ACM Trans. Management Inf. Syst.,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="4672" citStr="Liu and Ram, 2011" startWordPosition="706" endWordPosition="709"> as rv) and assign each of them to one or more edit categories. There exist at least two main applications of edit category classification: First, a fine-grained classification of edits in collaboratively created documents such as Wikipedia articles, scientific papers or research proposals, would help us to better understand the collaborative writing process. This includes answers to questions about the kind of contribution of individual authors (Who has added substantial contents?, Who has improved stylistic issues?) and about the kind of collaboration which characterizes different articles (Liu and Ram, 2011). Second, automatic classification of edits generates huge amounts of training data for the above mentioned NLP systems. Edit category classification is related to the better known task of document pair classification. In document pair classification, a pair of documents has to be assigned to one or more categories (e.g. paraphrase/non-paraphrase, plagiarism/nonplagiarism). Here, the document may be a very short text, such as a sentence or a single word. Applications of document pair classification include plagiarism detection (Potthast et al., 2012), paraphrase detection (Madnani et al., 2012</context>
</contexts>
<marker>Liu, Ram, 2011</marker>
<rawString>Jun Liu and Sudha Ram. 2011. Who does what: Collaboration patterns in the wikipedia and their impact on article quality. ACM Trans. Management Inf. Syst., 2(2):11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gjorgji Madjarov</author>
<author>Dragi Kocev</author>
<author>Dejan Gjorgjevikj</author>
<author>Saso Dzeroski</author>
</authors>
<title>An extensive experimental comparison of methods for multi-label learning.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>9</issue>
<contexts>
<context position="22349" citStr="Madjarov et al. (2012)" startWordPosition="3590" endWordPosition="3593">label classification algorithms and evaluation measures can be 5Maxent model for English, http://opennlp. apache.org 6http://meka.sourceforge.net Threshold – – .10 .25 .33 Accuracy .09 .13 .50 .44 .53 Exact Match .06 .13 .35 .36 .44 Example F1 .09 .13 .55 .47 .56 Precision .10 .13 .54 .46 .56 Recall .10 .13 .61 .50 .60 Macro-F1 .10 .06 .49 .35 .51 Label Micro-F1 .10 .12 .59 .49 .62 Ranking One Error .90 .87 .42 .48 .34 Table 3: Overall classification results with 3 multi-label classifiers and a C4.5 decision tree base classifier, as compared to random and majority category baselines. found in Madjarov et al. (2012). Multi-label classification problems are solved by either transforming the multi-label classification task into one or more single-label classification tasks (problem transformation method) or by adapting single-label classification algorithms (algorithm adaption method). Several algorithms have been developed on top of the former methods and use ensembles of such classifiers (ensemble methods). We applied the Binary Relevance approach (BR), a simple transformation method which converts the multi-label problem into |C |binary single-label problems, where |C| is the number of categories. Hence</context>
<context position="24108" citStr="Madjarov et al. (2012)" startWordPosition="3853" endWordPosition="3856">ture with nodes of small category sets of size k and leaves of single categories. Subsequently, a multi-label classifier is applied to each node in the tree. Random k-labelsets RAKEL (Tsoumakas et al., 2011) is an ensemble method, which randomly chooses l typically small subsets with k categories from the overall set of categories. Subsequently, all k-labelsets which are found in the multi-labeled data set are converted into new categories in a single-labeled data set using the la583 bel powerset transformation (Trohidis et al., 2008). HOMER and BR are among the multi-label classifiers, which Madjarov et al. (2012) recommend as benchmark methods. As underlying single-label classification algorithm, we used a C4.5 decision tree classifier (Quinlan, 1993), as decision tree classifiers yield state-of-the-art performance in the related work. Multi-label Evaluation We denote the set of relevant categories for each edit ei E E as yi E C and the set of predicted categories as h(ei). Evaluation measures for multi-label classification systems are based on either bipartitions or rankings. Among the former, we report examplebased (weighting each edit equally) and label-based (weighting each edit category equally) </context>
</contexts>
<marker>Madjarov, Kocev, Gjorgjevikj, Dzeroski, 2012</marker>
<rawString>Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and Sa&amp;quot;so Dz&amp;quot;eroski. 2012. An extensive experimental comparison of methods for multi-label learning. Pattern Recognition, 45(9):3084–3104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5273" citStr="Madnani et al., 2012" startWordPosition="798" endWordPosition="801"> (Liu and Ram, 2011). Second, automatic classification of edits generates huge amounts of training data for the above mentioned NLP systems. Edit category classification is related to the better known task of document pair classification. In document pair classification, a pair of documents has to be assigned to one or more categories (e.g. paraphrase/non-paraphrase, plagiarism/nonplagiarism). Here, the document may be a very short text, such as a sentence or a single word. Applications of document pair classification include plagiarism detection (Potthast et al., 2012), paraphrase detection (Madnani et al., 2012) or text similarity detection (B¨ar et al., 2012). In edit category classification, we also have two documents. However, these documents are different versions of the same text. This scenario implies certain characteristics for a well-designed feature set as we will demonstrate in this study. The main contributions of this paper are: First, we introduce a novel feature set for edit category classification. Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits. We propose the new task of edit category classification and show that our model</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elien Max</author>
<author>Guillaume Wisniewski</author>
</authors>
<title>Mining Naturally-occurring Corrections and Paraphrases from Wikipedias Revision History.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation,</booktitle>
<location>Valletta, Malta.</location>
<contexts>
<context position="2261" citStr="Max and Wisniewski, 2010" startWordPosition="333" endWordPosition="337">icles is rather stable.1 It is reasonable to assume that the latter will not change in the near 1http://stats.wikimedia.org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (Daxenberger and Gurevych, 2012). This taxon</context>
<context position="7488" citStr="Max and Wisniewski, 2010" startWordPosition="1150" endWordPosition="1153">istories of different articles. Finally, we draw a conclusion in Section 6. 2 Related Work Wikipedia is a huge data source for generating training data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits.</context>
<context position="38692" citStr="Max and Wisniewski, 2010" startWordPosition="6210" endWordPosition="6213">d how our model can be used to detect collaboration patterns in article revision histories. On a subset of articles from the English Wikipedia, we found that high-quality articles show a higher degree of homogeneity in their collaborative patterns as compared to random articles. Furthermore, automatic edit category classification allows to generate huge amounts of category-filtered training data for NLP tasks, e.g. spelling and grammar correction or vandalism detection. With respect to future work, we plan to include more resources, e.g. the PAN-WVC10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max and Wisniewski, 2010) to increase the size of training data. A larger amount of labeled data would certainly help to improve the classifier performance for weak categories (e.g. VANDALISM and PARAPHRASE) and sparse categories (e.g. TEMPLATE-D, MARKUP-M). Based on our trained classifier, annotating more examples can be alleviated with the help of active learning. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Hessian research excellence program “LandesOffensive zur Entwicklung Wissenschaftlich¨okonomische</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>Aur´elien Max and Guillaume Wisniewski. 2010. Mining Naturally-occurring Corrections and Paraphrases from Wikipedias Revision History. In Proceedings of the 7th Conference on International Language Resources and Evaluation, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Elif Yamangil</author>
</authors>
<title>Mining Wikipedia’s Article Revision History for Training Computational Linguistics Algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st AAAI Workshop on Wikipedia and Artificial Intelligence,</booktitle>
<pages>31--36</pages>
<location>Chicago, IL, USA.</location>
<contexts>
<context position="7435" citStr="Nelken and Yamangil, 2008" startWordPosition="1143" endWordPosition="1146">ining frequent collaboration patterns in the revision histories of different articles. Finally, we draw a conclusion in Section 6. 2 Related Work Wikipedia is a huge data source for generating training data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the su</context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>Rani Nelken and Elif Yamangil. 2008. Mining Wikipedia’s Article Revision History for Training Computational Linguistics Algorithms. In Proceedings of the 1st AAAI Workshop on Wikipedia and Artificial Intelligence, pages 31–36, Chicago, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ergio Nunes</author>
<author>Cristina Ribeiro</author>
<author>Gabriel David</author>
</authors>
<title>Term weighting based on document revision history.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>12</issue>
<contexts>
<context position="7940" citStr="Nunes et al., 2011" startWordPosition="1216" endWordPosition="1219">. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection in Wikipedia has mostl</context>
</contexts>
<marker>Nunes, Ribeiro, David, 2011</marker>
<rawString>S´ergio Nunes, Cristina Ribeiro, and Gabriel David. 2011. Term weighting based on document revision history. Journal of the American Society for Information Science and Technology, 62(12):2471–2478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA toolkit for statistical natural language processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC),</booktitle>
<pages>32--38</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="21225" citStr="Ogren et al., 2008" startWordPosition="3402" endWordPosition="3405">cated inside a text span that belongs to a markup element. Language Language features are calculated on the context sv_1 and sv of edits, any wiki markup is deleted. For the Explicit Semantic Analysis, we use Wiktionary (Zesch et al., 2008) and not Wikipedia assuming that the former has a better coverage with respect to different lexical classes. POS tagging was carried out using the OpenNLP POS tagger.5 The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web. 3.3 Experimental Setup We extract features with the help of ClearTK (Ogren et al., 2008). For the machine learning part, we use Weka (Hall et al., 2009) with the Meka6 and Mulan (Tsoumakas et al., 2010) extensions for multilabel classification. We use DKPro Lab (Eckart de Castilho et al., 2011) to test different parameter combinations. We randomly split the gold standard data from WPEC into 80% training, 10% test and 10% development set, as shown in Table 2. Multi-label Classification We report the performance of various machine learning algorithms. A comprehensive overview of multi-label classification algorithms and evaluation measures can be 5Maxent model for English, http://o</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA toolkit for statistical natural language processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC), pages 32–38, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Pei</author>
<author>Jiawei Han</author>
<author>Behzad Mortazavi-Asl</author>
<author>Jianyong Wang</author>
<author>Helen Pinto</author>
<author>Qiming Chen</author>
<author>Umeshwar Dayal</author>
<author>Mei-Chun Hsu</author>
</authors>
<title>Mining sequential patterns by pattern-growth: the PrefixSpan approach.</title>
<date>2004</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>16</volume>
<issue>11</issue>
<contexts>
<context position="34101" citStr="Pei et al., 2004" startWordPosition="5499" endWordPosition="5502">experiments. We segmented all adjacent revisions in WPQAC into edits, following the approach explained in Daxenberger and Gurevych (2012). During the classification process, we discarded revisions where the classifier could not assign any of the 21 edit categories with a confidence higher than the 7http://en.wikipedia.org/wiki/Wikipedia: FA threshold, cf. Table 3. This resulted in 17,640 remaining revisions. We applied a sequential pattern mining algorithm with time constraints (Hirate and Yamana, 2006; Fournier-Viger et al., 2008) to the data. The latter is based on the PrefixSpan algorithm (Pei et al., 2004). Calculations have been carried out within the open-source SPMF Java data mining platform.8 We created one time-extended sequence database for the 10 featured articles and one for the 10 nonfeatured articles. The sequence databases consist of one row per article. Each row is a chronologically ordered list of revisions. Each revision is represented by the itemset of all edit categories for all edits in that revision (in alphabetical order). The output of the algorithm are sequential patterns with time constraints. To obtain meaningful results, we constrained the output with the following param</context>
</contexts>
<marker>Pei, Han, Mortazavi-Asl, Wang, Pinto, Chen, Dayal, Hsu, 2004</marker>
<rawString>Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Jianyong Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal, and Mei-Chun Hsu. 2004. Mining sequential patterns by pattern-growth: the PrefixSpan approach. IEEE Transactions on Knowledge and Data Engineering, 16(11):1424–1440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Teresa Holfeld</author>
</authors>
<title>Overview of the 2nd International Competition on Wikipedia Vandalism Detection.</title>
<date>2011</date>
<booktitle>In Notebook Papers of CLEF 2011 Labs and Workshops,</booktitle>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="9108" citStr="Potthast and Holfeld, 2011" startWordPosition="1391" endWordPosition="1394">er the baseline. Vandalism detection in Wikipedia has mostly been defined as a binary machine learning task, where the goal is to classify a pair of adjacent revisions as vandalized or not-vandalized based on edit category features. In Adler et al. (2011), the authors group these features into meta data (author, comment and time stamp of a revision), reputation (author and article reputation), textual (language independent, i.e. token- and character-based) and language features (language dependent, mostly dictionary-based). They carry out cross-validation experiments on the PAN-WVC-10 corpus (Potthast and Holfeld, 2011). Classifiers based on reputation and text performed best. Adler et al. (2011) use Random Forests as classifier (Breiman, 2001) in their experiments. This classifier was also used in the vandalism detection study of Javanmardi et al. (2011) where it outperformed the classifiers based on Logistic Regression and Naive Bayes. Different to the approach of Bronner and Monz (2012) and previous vandalism classification studies, we built a model which accounts for multilabeling and a fine-grained edit category system. Our feature set builds upon existing work while adding a substantial number of new f</context>
<context position="38653" citStr="Potthast and Holfeld, 2011" startWordPosition="6204" endWordPosition="6207">s or research papers. We have demonstrated how our model can be used to detect collaboration patterns in article revision histories. On a subset of articles from the English Wikipedia, we found that high-quality articles show a higher degree of homogeneity in their collaborative patterns as compared to random articles. Furthermore, automatic edit category classification allows to generate huge amounts of category-filtered training data for NLP tasks, e.g. spelling and grammar correction or vandalism detection. With respect to future work, we plan to include more resources, e.g. the PAN-WVC10 (Potthast and Holfeld, 2011) or WiCoPaCo (Max and Wisniewski, 2010) to increase the size of training data. A larger amount of labeled data would certainly help to improve the classifier performance for weak categories (e.g. VANDALISM and PARAPHRASE) and sparse categories (e.g. TEMPLATE-D, MARKUP-M). Based on our trained classifier, annotating more examples can be alleviated with the help of active learning. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Hessian research excellence program “LandesOffensive zur E</context>
</contexts>
<marker>Potthast, Holfeld, 2011</marker>
<rawString>Martin Potthast and Teresa Holfeld. 2011. Overview of the 2nd International Competition on Wikipedia Vandalism Detection. In Notebook Papers of CLEF 2011 Labs and Workshops, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Tim Gollub</author>
<author>Matthias Hagen</author>
<author>Johannes Kiesel</author>
<author>Maximilian Michel</author>
<author>Arnd Oberl¨ander</author>
<author>Martin Tippmann</author>
<author>Alberto Barr´on-Cede˜no</author>
<author>Parth Gupta</author>
<author>Paolo Rosso</author>
<author>Benno Stein</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 4th International Competition on Plagiarism Detection. In CLEF 2012 Evaluation Labs and Workshop Working Notes Papers,</booktitle>
<location>Rome, Italy.</location>
<marker>Potthast, Gollub, Hagen, Kiesel, Michel, Oberl¨ander, Tippmann, Barr´on-Cede˜no, Gupta, Rosso, Stein, 2012</marker>
<rawString>Martin Potthast, Tim Gollub, Matthias Hagen, Johannes Kiesel, Maximilian Michel, Arnd Oberl¨ander, Martin Tippmann, Alberto Barr´on-Cede˜no, Parth Gupta, Paolo Rosso, and Benno Stein. 2012. Overview of the 4th International Competition on Plagiarism Detection. In CLEF 2012 Evaluation Labs and Workshop Working Notes Papers, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="24249" citStr="Quinlan, 1993" startWordPosition="3873" endWordPosition="3874">he tree. Random k-labelsets RAKEL (Tsoumakas et al., 2011) is an ensemble method, which randomly chooses l typically small subsets with k categories from the overall set of categories. Subsequently, all k-labelsets which are found in the multi-labeled data set are converted into new categories in a single-labeled data set using the la583 bel powerset transformation (Trohidis et al., 2008). HOMER and BR are among the multi-label classifiers, which Madjarov et al. (2012) recommend as benchmark methods. As underlying single-label classification algorithm, we used a C4.5 decision tree classifier (Quinlan, 1993), as decision tree classifiers yield state-of-the-art performance in the related work. Multi-label Evaluation We denote the set of relevant categories for each edit ei E E as yi E C and the set of predicted categories as h(ei). Evaluation measures for multi-label classification systems are based on either bipartitions or rankings. Among the former, we report examplebased (weighting each edit equally) and label-based (weighting each edit category equally) measures. The accuracy of a multi-label classifier is defined Jacas 1 P|E ||h(ei)nyi |which corresponds to the E |i=1 |h(ei)vyi| card similar</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Dan Jurafsky</author>
</authors>
<title>Linguistic Models for Analyzing and Detecting Biased Language.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>1650--1659</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7983" citStr="Recasens et al., 2013" startWordPosition="1223" endWordPosition="1227">ations such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show the highest improvement over the baseline. Vandalism detection in Wikipedia has mostly been defined as a binary machine learning</context>
</contexts>
<marker>Recasens, Danescu-Niculescu-Mizil, Jurafsky, 2013</marker>
<rawString>Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for Analyzing and Detecting Biased Language. In Proceedings of the 51st Annual Meeting on Association for Computational Linguistics, pages 1650–1659, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bongwon Suh</author>
<author>Gregorio Convertino</author>
<author>Ed H Chi</author>
<author>Peter Pirolli</author>
</authors>
<title>The singularity is not near: slowing growth of Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th International Symposium on Wikis and Open Collaboration,</booktitle>
<location>Orlando, FL, USA.</location>
<contexts>
<context position="1599" citStr="Suh et al., 2009" startWordPosition="233" endWordPosition="236">e data achieves state-of-the-art performance on the related task of fluency edit classification. We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles. Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles. 1 Introduction Due to its ever-evolving and collaboratively built content, Wikipedia has been the subject of many NLP studies. While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near 1http://stats.wikimedia.org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error corre</context>
</contexts>
<marker>Suh, Convertino, Chi, Pirolli, 2009</marker>
<rawString>Bongwon Suh, Gregorio Convertino, Ed H. Chi, and Peter Pirolli. 2009. The singularity is not near: slowing growth of Wikipedia. In Proceedings of the 5th International Symposium on Wikis and Open Collaboration, Orlando, FL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos Trohidis</author>
</authors>
<title>Grigorios Tsoumakas, George Kalliris, and Ioannis Vlahavas.</title>
<date>2008</date>
<booktitle>In 9th International Conference on Music Information Retrieval,</booktitle>
<pages>325--330</pages>
<location>Philadelphia, PA, USA.</location>
<marker>Trohidis, 2008</marker>
<rawString>Konstantinos Trohidis, Grigorios Tsoumakas, George Kalliris, and Ioannis Vlahavas. 2008. Multi-label classification of music into emotions. In 9th International Conference on Music Information Retrieval, pages 325–330, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Ioannis Katakis</author>
<author>Ioannis Vlahavas</author>
</authors>
<title>Effective and Efficient Multilabel Classification in Domains with Large Number of Labels.</title>
<date>2008</date>
<booktitle>In Proceedings of the ECML/PKDD 2008 Workshop on Mining Multidimensional Data,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="23321" citStr="Tsoumakas et al., 2008" startWordPosition="3729" endWordPosition="3732"> ensembles of such classifiers (ensemble methods). We applied the Binary Relevance approach (BR), a simple transformation method which converts the multi-label problem into |C |binary single-label problems, where |C| is the number of categories. Hence, this method trains a classifier for each category in the corpus (one-against-all). It is the most straightforward approach when dealing with multi-labeled data. However, it does not consider possible relationships or dependencies between categories. Therefore, we tested two more sophisticated methods. Hierarchy of multi-label classifiers HOMER (Tsoumakas et al., 2008) is a problem transformation method. It accounts for possibly hierarchical relationships among categories by dividing the overall category set into a tree-like structure with nodes of small category sets of size k and leaves of single categories. Subsequently, a multi-label classifier is applied to each node in the tree. Random k-labelsets RAKEL (Tsoumakas et al., 2011) is an ensemble method, which randomly chooses l typically small subsets with k categories from the overall set of categories. Subsequently, all k-labelsets which are found in the multi-labeled data set are converted into new ca</context>
</contexts>
<marker>Tsoumakas, Katakis, Vlahavas, 2008</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2008. Effective and Efficient Multilabel Classification in Domains with Large Number of Labels. In Proceedings of the ECML/PKDD 2008 Workshop on Mining Multidimensional Data, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
</authors>
<title>Ioannis Katakis, and Ioannis Vlahavas.</title>
<date>2010</date>
<booktitle>In Oded Maimon and Lior Rokach, editors, Data Mining and Knowledge Discovery Handbook, chapter 34,</booktitle>
<pages>667--685</pages>
<publisher>Springer.</publisher>
<marker>Tsoumakas, 2010</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2010. Mining multi-label data. In Oded Maimon and Lior Rokach, editors, Data Mining and Knowledge Discovery Handbook, chapter 34, pages 667–685. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
</authors>
<title>Ioannis Katakis, and Ioannis Vlahavas.</title>
<date>2011</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>23</volume>
<issue>7</issue>
<marker>Tsoumakas, 2011</marker>
<rawString>Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. 2011. Random k-Labelsets for Multi-Label Classification. IEEE Transactions on Knowledge and Data Engineering, 23(7):1079–1089.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>409--420</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="7740" citStr="Woodsend and Lapata, 2011" startWordPosition="1186" endWordPosition="1189">ored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff pa</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 409–420, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Rani Nelken</author>
</authors>
<title>Mining Wikipedia Revision Histories for Improving Sentence Compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association</booktitle>
<pages>137--140</pages>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="7631" citStr="Yamangil and Nelken, 2008" startWordPosition="1171" endWordPosition="1174">ining data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, a</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>Elif Yamangil and Rani Nelken. 2008. Mining Wikipedia Revision Histories for Improving Sentence Compression. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Short Papers, pages 137–140, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>365--368</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="7712" citStr="Yatskar et al., 2010" startWordPosition="1182" endWordPosition="1185">he encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as disp</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 365–368, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from Wikipedia using co-training.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING-Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>28--36</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7856" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="1202" endWordPosition="1205">s keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their features are calculated both on character- and word-level. Furthermore, they use features based on POS tags, named entities, acronyms, and a lan579 Figure 1: An example edit from WPEC labeled with REFERENCE-M, as displayed by Wikimedia’s diff page tool. guage model (word n-grams). In their experiments, character-level features and named entity features show t</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>Fabio Massimo Zanzotto and Marco Pennacchiotti. 2010. Expanding textual entailment corpora from Wikipedia using co-training. In Proceedings of the COLING-Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 28–36, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using Wiktionary for Computing Semantic Relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,</booktitle>
<pages>861--866</pages>
<location>Chicago, IL, USA.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Using Wiktionary for Computing Semantic Relatedness. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, pages 861–866, Chicago, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
</authors>
<title>Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>529--538</pages>
<location>Avignon, France.</location>
<contexts>
<context position="2218" citStr="Zesch, 2012" startWordPosition="329" endWordPosition="330">umber of edits in existing articles is rather stable.1 It is reasonable to assume that the latter will not change in the near 1http://stats.wikimedia.org/EN/ TablesDatabaseEdits.htm future. One of the major reasons for the popularity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activity. Wikipedia’s revision history stores all changes made to any page in the encyclopedia in separate revisions. Previous studies have exploited revision history data in tasks such as preposition error correction (Cahill et al., 2013), spelling error correction (Zesch, 2012) or paraphrasing (Max and Wisniewski, 2010). However, they all use different approaches to extract the information needed for their task. Ferschke et al. (2013) outline several applications benefiting from revision history data. They argue for a unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy. In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach. Therefore, we use the 21-category edit classification taxonomy developed in previous work (</context>
<context position="7502" citStr="Zesch, 2012" startWordPosition="1154" endWordPosition="1155">cles. Finally, we draw a conclusion in Section 6. 2 Related Work Wikipedia is a huge data source for generating training data for edit category classification, as all previous versions of each page in the encyclopedia are stored in its revision history. Unsurprisingly, the number of studies extracting certain kinds of Wikipedia edits keeps growing. Most of these use manually defined rules or filters find the right kind of edits. Among the latter, there are NLP applications such as the detection of lexical errors (Nelken and Yamangil, 2008), spelling error correction (Max and Wisniewski, 2010; Zesch, 2012), preposition error correction (Cahill et al., 2013), sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008), summarization (Nelken and Yamangil, 2008), simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), paraphrasing (Max and Wisniewski, 2010; Dutrey et al., 2011), textual entailment (Zanzotto and Pennacchiotti, 2010; Cabrio et al., 2012), information retrieval (Aji et al., 2010; Nunes et al., 2011) and bias detection (Recasens et al., 2013). Bronner and Monz (2012) define features for the supervised classification of factual and fluency edits. Their feature</context>
</contexts>
<marker>Zesch, 2012</marker>
<rawString>Torsten Zesch. 2012. Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538, Avignon, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
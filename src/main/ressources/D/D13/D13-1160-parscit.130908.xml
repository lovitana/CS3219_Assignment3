<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000126">
<title confidence="0.979912">
Semantic Parsing on Freebase from Question-Answer Pairs
</title>
<author confidence="0.999051">
Jonathan Berant Andrew Chou Roy Frostig Percy Liang
</author>
<affiliation confidence="0.996534">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.999456">
{joberant,akchou}@stanford.edu {rf,pliang}@cs.stanford.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737">
In this paper, we train a semantic parser that
scales up to Freebase. Instead of relying on
annotated logical forms, which is especially
expensive to obtain at large scale, we learn
from question-answer pairs. The main chal-
lenge in this setting is narrowing down the
huge number of possible logical predicates for
a given question. We tackle this problem in
two ways: First, we build a coarse mapping
from phrases to predicates using a knowledge
base and a large text corpus. Second, we
use a bridging operation to generate additional
predicates based on neighboring predicates.
On the dataset of Cai and Yates (2013), despite
not having annotated logical forms, our sys-
tem outperforms their state-of-the-art parser.
Additionally, we collected a more realistic and
challenging dataset of question-answer pairs
and improves over a natural baseline.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999085846153846">
We focus on the problem of semantic parsing nat-
ural language utterances into logical forms that can
be executed to produce denotations. Traditional se-
mantic parsers (Zelle and Mooney, 1996; Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al., 2010) have two limitations: (i)
they require annotated logical forms as supervision,
and (ii) they operate in limited domains with a small
number of logical predicates. Recent developments
aim to lift these limitations, either by reducing the
amount of supervision (Clarke et al., 2010; Liang et
al., 2011; Goldwasser et al., 2011; Artzi and Zettle-
moyer, 2011) or by increasing the number of logical
</bodyText>
<affiliation confidence="0.873764">
Occidental College, Columbia University
</affiliation>
<figure confidence="0.978864571428571">
Execute on Database
Type.University fl Education.BarackObama
Type.University
alignment
BarackObama
alignment
Which college did Obama go to ?
</figure>
<figureCaption confidence="0.997697">
Figure 1: Our task is to map questions to answers via la-
</figureCaption>
<bodyText confidence="0.9857556">
tent logical forms. To narrow down the space of logical
predicates, we use a (i) coarse alignment based on Free-
base and a text corpus and (ii) a bridging operation that
generates predicates compatible with neighboring predi-
cates.
predicates (Cai and Yates, 2013). The goal of this
paper is to do both: learn a semantic parser with-
out annotated logical forms that scales to the large
number of predicates on Freebase.
At the lexical level, a major challenge in semantic
parsing is mapping natural language phrases (e.g.,
“attend”) to logical predicates (e.g., Education).
While limited-domain semantic parsers are able
to learn the lexicon from per-example supervision
(Kwiatkowski et al., 2011; Liang et al., 2011), at
large scale they have inadequate coverage (Cai and
Yates, 2013). Previous work on semantic parsing on
Freebase uses a combination of manual rules (Yahya
et al., 2012; Unger et al., 2012), distant supervision
(Krishnamurthy and Mitchell, 2012), and schema
</bodyText>
<figure confidence="0.79185">
bridging
Education
</figure>
<page confidence="0.860452">
1533
</page>
<note confidence="0.7331615">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999773435897436">
matching (Cai and Yates, 2013). We use a large
amount of web text and a knowledge base to build a
coarse alignment between phrases and predicates—
an approach similar in spirit to Cai and Yates (2013).
However, this alignment only allows us to gen-
erate a subset of the desired predicates. Aligning
light verbs (e.g., “go”) and prepositions is not very
informative due to polysemy, and rare predicates
(e.g., “cover price”) are difficult to cover even given
a large corpus. To improve coverage, we propose
a new bridging operation that generates predicates
based on adjacent predicates rather than on words.
At the compositional level, a semantic parser must
combine the predicates into a coherent logical form.
Previous work based on CCG requires manually
specifying combination rules (Krishnamurthy and
Mitchell, 2012) or inducing the rules from anno-
tated logical forms (Kwiatkowski et al., 2010; Cai
and Yates, 2013). We instead define a few simple
composition rules which over-generate and then use
model features to simulate soft rules and categories.
In particular, we use POS tag features and features
on the denotations of the predicted logical forms.
We experimented with two question answering
datasets on Freebase. First, on the dataset of Cai
and Yates (2013), we showed that our system out-
performs their state-of-the-art system 62% to 59%,
despite using no annotated logical forms. Second,
we collected a new realistic dataset of questions by
performing a breadth-first search using the Google
Suggest API; these questions are then answered by
Amazon Mechanical Turk workers. Although this
dataset is much more challenging and noisy, we are
still able to achieve 31.4% accuracy, a 4.5% ab-
solute improvement over a natural baseline. Both
datasets as well as the source code for SEMPRE, our
semantic parser, are publicly released and can be
downloaded from http://nlp.stanford.edu/
software/sempre/.
</bodyText>
<sectionHeader confidence="0.994635" genericHeader="introduction">
2 Setup
</sectionHeader>
<bodyText confidence="0.931024833333333">
Problem Statement Our task is as follows: Given
(i) a knowledge base K, and (ii) a training set of
question-answer pairs {(xi, yi)}Z1, output a se-
mantic parser that maps new questions x to answers
y via latent logical forms z and the knowledge base
K.
</bodyText>
<subsectionHeader confidence="0.97425">
2.1 Knowledge base
</subsectionHeader>
<bodyText confidence="0.984993625">
Let £ denote a set of entities (e.g., BarackObama),
and let P denote a set of properties (e.g.,
PlaceOfBirth). A knowledge base K is a set
of assertions (e1, p, e2) E £ x P x £ (e.g.,
(BarackObama, PlaceOfBirth, Honolulu)).
We use the Freebase knowledge base (Google,
2013), which has 41M non-numeric entities, 19K
properties, and 596M assertions.1
</bodyText>
<subsectionHeader confidence="0.994421">
2.2 Logical forms
</subsectionHeader>
<bodyText confidence="0.999920380952381">
To query the knowledge base, we use a logical lan-
guage called Lambda Dependency-Based Compo-
sitional Semantics (λ-DCS)—see Liang (2013) for
details. For the purposes of this paper, we use a re-
stricted subset called simple λ-DCS, which we will
define below for the sake of completeness.
The chief motivation of λ-DCS is to produce
logical forms that are simpler than lambda cal-
culus forms. For example, λx.1a.p1(x, a) n
lb.p2(a, b) n p3(b, e) is expressed compactly in
λ-DCS as p1.p2.p3.e. Like DCS (Liang et al.,
2011), λ-DCS makes existential quantification im-
plicit, thereby reducing the number of variables.
Variables are only used for anaphora and building
composite binary predicates; these do not appear in
simple λ-DCS.
Each logical form in simple λ-DCS is either a
unary (which denotes a subset of £) or a binary
(which denotes a subset of £ x £). The basic λ-
DCS logical forms z and their denotations JzKK are
defined recursively as follows:
</bodyText>
<listItem confidence="0.9848071">
• Unary base case: If e E £ is an entity (e.g.,
Seattle), then e is a unary logical form with
JzKK = {e}.
• Binary base case: If p E P is a property (e.g.,
PlaceOfBirth), then p is a binary logical form
with JpKK = {(e1, e2) : (e1, p, e2) E K}.2
• Join: If b is a binary and u is a unary, then b.u
(e.g., PlaceOfBirth.Seattle) is a unary de-
noting a join and project: Jb.uKK = {e1 E £ :
le2.(e1, e2) E JbKK n e2 E JuKK}.
</listItem>
<footnote confidence="0.9986508">
1In this paper, we condense Freebase names for readability
(/people/person becomes Person).
2Binaries can be also built out of lambda abstractions (e.g.,
Ax.Performance.Actor.x), but as these constructions are
not central to this paper, we defer to (Liang, 2013).
</footnote>
<page confidence="0.988628">
1534
</page>
<listItem confidence="0.982491285714286">
• Intersection: If u1 and u2 are both unaries,
then u1 fl u2 (e.g., Profession.Scientist fl
PlaceOfBirth.Seattle) denotes set intersec-
tion: Ju1 n u2K)C = Ju1K)C n Ju2K)C.
• Aggregation: If u is a unary, then count(u)
denotes the cardinality: Jcount(u)K)C =
I|JuK)C|I.
</listItem>
<bodyText confidence="0.995124818181818">
As a final example, “number of dramas star-
ring Tom Cruise” in lambda calculus would
be represented as count(Ax.Genre(x, Drama) ∧
ly.Performance(x, y) ∧ Actor(y, TomCruise));
in A-DCS, it is simply count(Genre.Drama fl
Performance.Actor.TomCruise).
It is useful to think of the knowledge base K as
a directed graph in which entities are nodes and
properties are labels on the edges. Then simple A-
DCS unary logical forms are tree-like graph patterns
which pick out a subset of the nodes.
</bodyText>
<subsectionHeader confidence="0.98938">
2.3 Framework
</subsectionHeader>
<bodyText confidence="0.9988855">
Given an utterance x, our semantic parser constructs
a distribution over possible derivations D(x). Each
derivation d E D(x) is a tree specifying the appli-
cation of a set of combination rules that culminates
in the logical form d.z at the root of the tree—see
Figure 2 for an example.
Composition Derivations are constructed recur-
sively based on (i) a lexicon mapping natural lan-
guage phrases to knowledge base predicates, and (ii)
a small set of composition rules.
More specifically, we build a set of derivations for
each span of the utterance. We first use the lexicon to
generate single-predicate derivations for any match-
ing span (e.g., “born” maps to PeopleBornHere).
Then, given any logical form z1 that has been con-
structed over the span [i1 : j1] and z2 over a non-
overlapping span [i2 : j2], we generate the following
logical forms over the enclosing span [min(i1, i2) :
max(j1, j2)]: intersection z1 n z2, join z1.z2, ag-
gregation z1(z2) (e.g., if z1 = count), or bridging
z1 n p.z2 for any property p E P (explained more in
Section 3.2).3
Note that the construction of derivations D(x)
allows us to skip any words, and in general heav-
</bodyText>
<footnote confidence="0.984425666666667">
3We also discard logical forms are incompatible according
to the Freebase types (e.g., Profession.Politician n
Type.City would be rejected).
</footnote>
<figure confidence="0.359576">
Type.LocationflPeopleBornHere.BarackObama
</figure>
<figureCaption confidence="0.99930525">
Figure 2: An example of a derivation d of the utterance
“Where was Obama born?” and its sub-derivations, each
labeled with composition rule (in blue) and logical form
(in red). The derivation d skips the words “was” and “?”.
</figureCaption>
<bodyText confidence="0.956222333333333">
ily over-generates. We instead rely on features and
learning to guide us away from the bad derivations.
Modeling Following Zettlemoyer and Collins
(2005) and Liang et al. (2011), we define a
discriminative log-linear model over derivations
d E D(x) given utterances x: po(d  |x) =
</bodyText>
<equation confidence="0.9922285">
exp{0(x,d)&gt;o}0 T , where O(x, d) is a feature
�d0∈D(a) exp{0(x,d) o}
</equation>
<bodyText confidence="0.997784285714286">
vector extracted from the utterance and the deriva-
tion, and 0 E Rb is the vector of parameters to
be learned. As our training data consists only of
question-answer pairs (xi, yi), we maximize the log-
likelihood of the correct answer (Jd.zK)C = yi), sum-
ming over the latent derivation d. Formally, our
training objective is
</bodyText>
<equation confidence="0.984336">
n
O(0) = log E po(d  |xi). (1)
i=1 dED(x):Qd.z➢K=yz
</equation>
<bodyText confidence="0.9999785">
Section 4 describes an approximation of this ob-
jective that we maximize to choose parameters 0.
</bodyText>
<sectionHeader confidence="0.994247" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999985083333333">
Our knowledge base has more than 19,000 proper-
ties, so a major challenge is generating a manage-
able set of predicates for an utterance. We propose
two strategies for doing this. First (Section 3.1),
we construct a lexicon that maps natural language
phrases to logical predicates by aligning a large text
corpus to Freebase, reminiscent of Cai and Yates
(2013). Second, we generate logical predicates com-
patible with neighboring predicates using the bridg-
ing operation (Section 3.2). Bridging is crucial when
aligning phrases is difficult or even impossible. The
derivations produced by combining these predicates
</bodyText>
<figure confidence="0.944939185185185">
intersection
Type.Location was PeopleBornHere.BarackObama ?
where BarackObama PeopleBornHere
lexicon lexicon
Obama born
lexicon
join
1535
grew up in[Person,Location]
born in[Person,Date]
married in[Person,Date]
born in[Person,Location]
DateOfBirth
PlaceOfBirth
Marriage.StartDate
PlacesLived.Location
(RandomPerson,Seattle)
F(r1) .F(r2)
(MichelleObama,Chicago)
(BarackObama,Honolulu)
C(r1, r2)
(BarackObama,Chicago)
Alignment features
log-phrase-count:log(15765)
log-predicate-count: log(9182)
log-intersection-count: log(6048)
KB-best-match: 0
</figure>
<figureCaption confidence="0.997222666666667">
Figure 3: We construct a bipartite graph over phrases R1
and predicates R2. Each edge (r1, r2) is associated with
alignment features.
</figureCaption>
<bodyText confidence="0.9999625">
are scored using features that capture lexical, syn-
tactic and semantic regularities (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.99829">
3.1 Alignment
</subsectionHeader>
<bodyText confidence="0.994614701492537">
We now discuss the construction of a lexicon L,
which is a mapping from natural language phrases
to logical predicates accompanied by a set of fea-
tures. Specifically, for a phrase w (e.g., “born in”),
L(w) is a set of entries (z, s), where z is a predicate
and s is the set of features. A lexicon is constructed
by alignment of a large text corpus to the knowledge
base (KB). Intuitively, a phrase and a predicate align
if they co-occur with many of the same entities.
Here is a summary of our alignment proce-
dure: We construct a set of typed4 phrases
R1 (e.g., “born in”[Person,Location]) and pred-
icates R2 (e.g., PlaceOfBirth). For each
r E R1 U R2, we create its extension
F(r), which is a set of co-occurring entity-
pairs (e.g., F(“born in”[Person,Location]) =
{(BarackObama, Honolulu), ... }. The lexicon is
generated based on the overlap F(r1) n F(r2), for
r1 E R1 and r2 E R2.
Typed phrases 15 million triples (e1, r, e2) (e.g.,
(“Obama”, “was also born in”, “August 1961”))
4Freebase associates each entity with a set of types using the
Type property.
were extracted from ClueWeb09 using the ReVerb
open IE system (Fader et al., 2011). Lin et al. (2012)
released a subset of these triples5 where they were
able to substitute the subject arguments with KB en-
tities. We downloaded their dataset and heuristically
replaced object arguments with KB entities by walk-
ing on the Freebase graph from subject KB entities
and performing simple string matching. In addition,
we normalized dates with SUTime (Chang and Man-
ning, 2012).
We lemmatize and normalize each text phrase
r E R1 and augment it with a type signature
[t1, t2] to deal with polysemy (“born in” could ei-
ther map to PlaceOfBirth or DateOfBirth). We
add an entity pair (e1, e2) to the extension of
F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1
(t2). For example, (BarackObama, 1961) is added
to F(“born in”[Person, Date]). We perform a simi-
lar procedure that uses a Hearst-like pattern (Hearst,
1992) to map phrases to unary predicates. If a
text phrase r E R1 matches the pattern “(is|was
a|the) x IN”, where IN is a preposition, then we
add e1 to F(x). For (Honolulu, “is a city in”,
Hawaii), we extract x = “city&amp;quot; and add Honolulu
to F(“city”). From the initial 15M triples, we ex-
tracted 55,081 typed binary phrases (9,456 untyped)
and 6,299 unary phrases.
Logical predicates Binary logical predicates con-
tain (i) all KB properties6 and (ii) concatenations of
two properties p1.p2 if the intermediate type repre-
sents an event (e.g., the married to relation is rep-
resented by Marriage.Spouse). For unary pred-
icates, we consider all logical forms Type.t and
Profession.t for all (abstract) entities t E £ (e.g.
Type.Book and Profession.Author). The types
of logical predicates considered during alignment is
restricted in this paper, but automatic induction of
more compositional logical predicates is an interest-
ing direction. Finally, we define the extension of a
logical predicate r E R2 to be its denotation, that is,
the corresponding set of entities or entity pairs.
Lexicon construction Given typed phrases R1,
logical predicates R2, and their extensions F, we
now generate the lexicon. It is useful to think of a
</bodyText>
<footnote confidence="0.994271">
5http://knowitall.cs.washington.edu/
linked_extractions/
6We filter properties from the domains user and base.
</footnote>
<page confidence="0.942834">
1536
</page>
<table confidence="0.99783815">
Category Description
Alignment Log of # entity pairs that occur with the
phrase r1 (|F(r1)|)
Log of # entity pairs that occur with the
logical predicate r2 (|F(r2)|)
Log of # entity pairs that occur with both
r1 and r2 (|F(r1) n F(r2)|)
Whether r2 is the best match for r1 (r2 =
arg max,. |F(r1) n F(r)|)
Lexicalized Conjunction of phrase w and predicate z
Text similarity Phrase r1 is equal/prefix/suffix of 82
Phrase overlap of r1 and 82
Bridging Log of # entity pairs that occur with bridg-
ing predicate b (|F(b)|)
Kind of bridging (# unaries involved)
The binary b injected
Composition # of intersect/join/bridging operations
POS tags in join/bridging and skipped
words
Size of denotation of logical form
</table>
<tableCaption confidence="0.992958">
Table 1: Full set of features. For the alignment and text sim-
</tableCaption>
<construct confidence="0.803093">
ilarity, r1 is a phrase, r2 is a predicate with Freebase name 82,
and b is a binary predicate with type signature (t1, t2).
</construct>
<bodyText confidence="0.980648481481482">
bipartite graph with left nodes R1 and right nodes
R2 (Figure 3). We add an edge (r1, r2) if (i) the
type signatures of r1 and r2 match7 and (ii) their ex-
tensions have non-empty overlap (F(r1) n F(r2) 7�
0). Our final graph contains 109K edges for binary
predicates and 294K edges for unary predicates.
Naturally, non-zero overlap by no means guaran-
tees that r1 should map to r2. In our noisy data,
even “born in” and Marriage.EndDate co-occur 4
times. Rather than thresholding based on some cri-
terion, we compute a set of features, which are used
by the model downstream in conjunction with other
sources of information.
We compute three types of features (Table 1).
Alignment features are unlexicalized and measure
association based on argument overlap. Lexicalized
features are standard conjunctions of the phrase w
and the logical form z. Text similarity features com-
pare the (untyped) phrase (e.g., “born”) to the Free-
base name of the logical predicate (e.g., “People
born here”): Given the phrase r1 and the Freebase
name S2 of the predicate r2, we compute string sim-
ilarity features such as whether r1 and S2 are equal,
7Each Freebase property has a designated type signa-
ture, which can be extended to composite predicates, e.g.,
sig(Marriage.StartDate) = (Person, Date).
as well as some other measures of token overlap.
</bodyText>
<subsectionHeader confidence="0.998901">
3.2 Bridging
</subsectionHeader>
<bodyText confidence="0.999964644444444">
While alignment can cover many predicates, it is un-
reliable for cases where the predicates are expressed
weakly or implicitly. For example, in “What govern-
ment does Chile have?”, the predicate is expressed
by the light verb have, in “What actors are in Top
Gun?”, it is expressed by a highly ambiguous prepo-
sition, and in “What is Italy money?” [sic], it is
omitted altogether. Since natural language doesn’t
offer much help here, let us turn elsewhere for guid-
ance. Recall that at this point our main goal is to
generate a manageable set of candidate logical forms
to be scored by the log-linear model.
In the first example, suppose the phrases “Chile”
and “government” are parsed as Chile and
Type.FormOfGovernment, respectively, and we hy-
pothesize a connecting binary. The two predicates
impose strong type constraints on that binary, so we
can afford to generate all the binary predicates that
type check (see Table 2). More formally, given two
unaries z1 and z2 with types t1 and t2, we generate a
logical form z1 n b.z2 for each binary b whose type
signature is (t1, t2). Figure 1 visualizes bridging of
the unaries Type.University and Obama.
Now consider the example “What is the
cover price of X-men?” Here, the binary
ComicBookCoverPrice is expressed explicitly, but
is not in our lexicon since the language use is rare.
To handle this, we allow bridging to generate a bi-
nary based on a single unary; in this case, based on
the unary X-Men (Table 2), we generate several bina-
ries including ComicBookCoverPrice. Generically,
given a unary z with type t, we construct a logical
form b.z for any predicate b with type (*, t).
Finally, consider the question “Who did
Tom Cruise marry in 2006?”. Suppose we
parse the phrase “Tom Cruise marry” into
Marriage.Spouse.TomCruise, or more explicitly,
Ax.le.Marriage(x, e) ∧ Spouse(e, TomCruise).
Here, the neo-Davidsonian event variable e is an
intermediate quantity, but needs to be further mod-
ified (in this case, by the temporal modifier 2006).
To handle this, we apply bridging to a unary and the
intermediate event (see Table 2). Generically, given
a logical form p1.p2.z&apos; where p2 has type (t1, *),
and a unary z with type t, bridging injects z and
</bodyText>
<page confidence="0.934917">
1537
</page>
<table confidence="0.9999125">
# Form 1 Form 2 Bridging
1 Type.FormOfGovernment Chile Type.FormOfGovernmentflGovernmentTypeOf.Chile
2 X-Men ComicBookCoverPriceOf.X-Men
3 Marriage.Spouse.TomCruise 2006 Marriage.(Spouse.TomCruise fl StartDate.2006)
</table>
<tableCaption confidence="0.998896">
Table 2: Three examples of the bridging operation. The bridging binary predicate b is in boldface.
</tableCaption>
<bodyText confidence="0.99775225">
constructs a logical form p1.(p2.z&apos; n b.z) for each
logical predicate b with type (t1, t).
In each of the three examples, bridging gener-
ates a binary predicate based on neighboring logi-
cal predicates rather than on explicit lexical material.
In a way, our bridging operation shares with bridg-
ing anaphora (Clark, 1975) the idea of establishing
a novel relation between distinct parts of a sentence.
Naturally, we need features to distinguish between
the generated predicates, or decide whether bridging
is even appropriate at all. Given a binary b, features
include the log of the predicate count log |F(b)|, in-
dicators for the kind of bridging, an indicator on the
binary b for injections (Table 1). In addition, we add
all text similarity features by comparing the Free-
base name of b with content words in the question.
</bodyText>
<subsectionHeader confidence="0.995597">
3.3 Composition
</subsectionHeader>
<bodyText confidence="0.999900392857143">
So far, we have mainly focused on the generation of
predicates. We now discuss three classes of features
pertaining to their composition.
Rule features Each derivation d is the result of ap-
plying some number of intersection, join, and bridg-
ing operations. To control this number, we define
indicator features on each of these counts. This is in
contrast to the norm of having a single feature whose
value is equal to the count, which can only repre-
sent one-sided preferences for having more or fewer
of a given operation. Indicator features stabilize the
model, preferring derivations with a well-balanced
inventory of operations.
Part-of-speech tag features To guide the compo-
sition of predicates, we use POS tags in two ways.
First, we introduce features indicating when a word
of a given POS tag is skipped, which could capture
the fact that skipping auxiliaries is generally accept-
able, while skipping proper nouns is not. Second,
we introduce features on the POS tags involved in a
composition, inspired by dependency parsing (Mc-
Donald et al., 2005). Specifically, when we combine
logical forms z1 and z2 via a join or bridging, we
include a feature on the POS tag of (the first word
spanned by) z1 conjoined with the POS tag corre-
sponding to z2. Rather than using head-modifier in-
formation from dependency trees (Branavan et al.,
2012; Krishnamurthy and Mitchell, 2012; Cai and
Yates, 2013; Poon, 2013), we can learn the appro-
priate relationships tailored for downstream accu-
racy. For example, the phrase “located” is aligned
to the predicate ContainedBy. POS features can de-
tect that if “located” precedes a noun phrase (“What
is located in Beijing?”), then the noun phrase is the
object of the predicate, and if it follows the noun
phrase (“Where is Beijing located?”), then it is in
subject position.
Note that our three operations (intersection, join,
and bridging) are quite permissive, and we rely on
features, which encode soft, overlapping rules. In
contrast, CCG-based methods (Kwiatkowski et al.,
2010; Kwiatkowski et al., 2011) encode the com-
bination preferences structurally in non-overlapping
rules; these could be emulated with features with
weights clamped to −oc.
Denotation features While it is clear that learning
from denotations rather than logical forms is a draw-
back since it provides less information, it is less ob-
vious that working with denotations actually gives
us additional information. Specifically, we include
four features indicating whether the denotation of
the predicted logical form has size 0, 1, 2, or at least
3. This feature encodes presupposition constraints
in a soft way: when people ask a question, usually
there is an answer and it is often unique. This allows
us to favor logical forms with this property.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999871">
We now evaluate our semantic parser empirically.
In Section 4.1, we compare our approach to Cai
and Yates (2013) on their recently released dataset
(henceforth, FREE917) and present results on a new
</bodyText>
<page confidence="0.967783">
1538
</page>
<bodyText confidence="0.995998789473684">
dataset that we collected (henceforth, WEBQUES-
TIONS). In Section 4.2, we provide detailed experi-
ments to provide additional insight on our system.
Setup We implemented a standard beam-based
bottom-up parser which stores the k-best derivations
for each span. We use k = 500 for all our experi-
ments on FREE917 and k = 200 on WEBQUES-
TIONS. The root beam yields the candidate set D(x)
and is used to approximate the sum in the objective
function 0(0) in (1). In experiments on WEBQUES-
TIONS, D(x) contained 197 derivations on average.
We write the approximate objective as O(0; 0) =
Ei log EdED(xi;B):Qd.z] lc=yi p(d I xi; 0) to explic-
itly show dependence on the parameters 0� used for
beam search. We optimize the objective by initial-
izing 00 to 0 and applying AdaGrad (stochastic gra-
dient ascent with per-feature adaptive step size con-
trol) (Duchi et al., 2010), so that 0t+1 is set based on
taking a stochastic approximation of aO(B;Bt) ��B=Bt.
</bodyText>
<equation confidence="0.776907">
aB
</equation>
<bodyText confidence="0.999941277777778">
We make six passes over the training examples.
We used POS tagging and named-entity recogni-
tion to restrict what phrases in the utterance could
be mapped by the lexicon. Entities must be named
entities, proper nouns or a sequence of at least two
tokens. Unaries must be a sequence of nouns, and
binaries must be either a content word, or a verb fol-
lowed by either a noun phrase or a particle. In addi-
tion, we used 17 hand-written rules to map question
words such as “where” and “how many” to logical
forms such as Type.Location and Count.
To compute denotations, we convert a logical
form z into a SPARQL query and execute it on our
copy of Freebase using the Virtuoso engine. On
WEBQUESTIONS, a full run over the training exam-
ples involves approximately 600,000 queries. For
evaluation, we predict the answer from the deriva-
tion with highest probability.
</bodyText>
<subsectionHeader confidence="0.9967125">
4.1 Main results
4.1.1 FREE917
</subsectionHeader>
<bodyText confidence="0.9999588">
Cai and Yates (2013) created a dataset consist-
ing of 917 questions involving 635 Freebase rela-
tions, annotated with lambda calculus forms. We
converted all 917 questions into simple A-DCS, ex-
ecuted them on Freebase and used the resulting an-
swers to train and evaluate. To map phrases to Free-
base entities we used the manually-created entity
lexicon used by Cai and Yates (2013), which con-
tains 1,100 entries. Because entity disambiguation
is a challenging problem in semantic parsing, the en-
tity lexicon simplifies the problem.
Following Cai and Yates (2013), we held out 30%
of the examples for the final test, and performed all
development on the remaining 70%. During devel-
opment, we split the data and used 512 examples
(80%) for training and the remaining 129 (20%) for
validation. All reported development numbers are
averaged across 3 random splits. We evaluated us-
ing accuracy, the fraction of examples where the pre-
dicted answer exactly matched the correct answer.
Our main empirical result is that our system,
which was trained only on question-answer pairs,
obtained 62% accuracy on the test set, outperform-
ing the 59% accuracy reported by Cai and Yates
(2013), who trained on full logical forms.
</bodyText>
<sectionHeader confidence="0.94023" genericHeader="evaluation">
4.1.2 WEBQUESTIONS
</sectionHeader>
<bodyText confidence="0.999942678571428">
Dataset collection Because FREE917 requires
logical forms, it is difficult to scale up due to the
required expertise of annotating logical forms. We
therefore created a new dataset, WEBQUESTIONS,
of question-answer pairs obtained from non-experts.
To collect this dataset, we used the Google Sug-
gest API to obtain questions that begin with a wh-
word and contain exactly one entity. We started with
the question “Where was Barack Obama born?”
and performed a breadth-first search over questions
(nodes), using the Google Suggest API supplying
the edges of the graph. Specifically, we queried the
question excluding the entity, the phrase before the
entity, or the phrase after it; each query generates 5
candidate questions, which are added to the queue.
We iterated until 1M questions were visited; a ran-
dom 100K were submitted to Amazon Mechanical
Turk (AMT).
The AMT task requested that workers answer the
question using only the Freebase page of the ques-
tions’ entity, or otherwise mark it as unanswerable
by Freebase. The answer was restricted to be one of
the possible entities, values, or list of entities on the
page. As this list was long, we allowed the user to
filter the list by typing. We paid the workers $0.03
per question. Out of 100K questions, 6,642 were
annotated identically by at least two AMT workers.
We again held out a 35% random subset of the
</bodyText>
<page confidence="0.985934">
1539
</page>
<table confidence="0.9948334">
Dataset # examples # word types
GeoQuery 880 279
ATIS 5,418 936
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
</table>
<tableCaption confidence="0.999242">
Table 3: Statistics on various semantic parsing datasets. Our
</tableCaption>
<bodyText confidence="0.979514425">
new dataset, WEBQUESTIONS, is much larger than FREE917
and much more lexically diverse than ATIS.
questions for the final test, and performed all devel-
opment on the remaining 65%, which was further
divided into an 80%–20% split for training and val-
idation. To map entities, we built a Lucene index
over the 41M Freebase entities.
Table 3 provides some statistics about the new
questions. One major difference in the datasets is
the distribution of questions: FREE917 starts from
Freebase properties and solicits questions about
these properties; these questions tend to be tai-
lored to the properties. WEBQUESTIONS starts from
questions completely independent of Freebase, and
therefore the questions tend to be more natural and
varied. For example, for the Freebase property
ComicGenre, FREE917 contains the question “What
genre is Doonesbury?”, while WEBQUESTIONS for
the property MusicGenre contains “What music did
Beethoven compose?”.
The number of word types in WEBQUESTIONS is
larger than in datasets such as ATIS and GeoQuery
(Table 3), making lexical mapping much more chal-
lenging. On the other hand, in terms of structural
complexity WEBQUESTIONS is simpler and many
questions contain a unary, a binary and an entity.
In some questions, the answer provided by AMT
workers is only roughly accurate, because workers
are restricted to selecting answers from the Freebase
page. For example, the answer given by workers to
the question “What is James Madison most famous
for?” is “President of the United States” rather than
“Authoring the Bill of Rights”.
Results AMT workers sometimes provide partial
answers, e.g., the answer to “What movies does Tay-
lor Lautner play in?” is a set of 17 entities, out
of which only 10 appear on the Freebase page. We
therefore allow partial credit and score an answer us-
ing the F1 measure, comparing the predicted set of
entities to the annotated set of entities.
</bodyText>
<table confidence="0.9969765">
System FREE917 WebQ.
ALIGNMENT 38.0 30.6
BRIDGING 66.9 21.2
ALIGNMENT+BRIDGING 71.3 32.9
</table>
<tableCaption confidence="0.992819">
Table 4: Accuracies on the development set under different
schemes of binary predicate generation. In ALIGNMENT, bi-
naries are generated only via the alignment lexicon. In BRIDG-
ING, binaries are generated through the bridging operation only.
ALIGNMENT+BRIDGING corresponds to the full system.
</tableCaption>
<bodyText confidence="0.999854588235294">
As a baseline, we omit from our system the main
contributions presented in this paper—that is, we
disallow bridging, and remove denotation and align-
ment features. The accuracy on the test set of this
system is 26.9%, whereas our full system obtains
31.4%, a significant improvement.
Note that the number of possible derivations for
questions in WEBQUESTIONS is quite large. In the
question “What kind of system of government does
the United States have?” the phrase “United States”
maps to 231 entities in our lexicon, the verb “have”
maps to 203 binaries, and the phrases “kind”, “sys-
tem”, and “government” all map to many different
unary and binary predicates. Parsing correctly in-
volves skipping some words, mapping other words
to predicates, while resolving many ambiguities in
the way that the various predicates can combine.
</bodyText>
<subsectionHeader confidence="0.999504">
4.2 Detailed analysis
</subsectionHeader>
<bodyText confidence="0.999984611111111">
We now delve deeper to explore the contributions of
the various components of our system. All ablation
results reported next were run on the development
set (over 3 random splits).
Generation of binary predicates Recall that our
system has two mechanisms for suggesting binaries:
from the alignment lexicon or via the bridging op-
eration. Table 4 shows accuracies when only one or
both is used. Interestingly, alignment alone is better
than bridging alone on WEBQUESTIONS, whereas
for FREE917, it is the opposite. The reason for this
is that FREE917 contains questions on rare pred-
icates. These are often missing from the lexicon,
but tend to have distinctive types and hence can be
predicted from neighboring predicates. In contrast,
WEBQUESTIONS contains questions that are com-
monly searched for and focuses on popular predi-
cates, therefore exhibiting larger lexical variation.
</bodyText>
<page confidence="0.969063">
1540
</page>
<table confidence="0.97615575">
System FREE917 WebQ.
FULL 71.3 32.9
-POS 70.5 28.9
-DENOTATION 58.6 28.0
</table>
<tableCaption confidence="0.9718875">
Table 5: Accuracies on the development set with features re-
moved. POS and DENOTATION refer to the POS tag and deno-
</tableCaption>
<table confidence="0.908136">
tation features from Section 3.3.
System FREE917 WebQ.
ALIGNMENT 71.3 32.9
LEXICALIZED 68.5 34.2
LEXICALIZED+ALIGNMENT 69.0 36.4
</table>
<tableCaption confidence="0.998410666666667">
Table 6: Accuracies on the development set using either
unlexicalized alignment features (ALIGNMENT) or lexicalized
features (LEXICALIZED).
</tableCaption>
<bodyText confidence="0.999758741935484">
For instance, when training without an align-
ment lexicon, the system errs on “When did Nathan
Smith die?”. Bridging suggests binaries that are
compatible with the common types Person and
Datetime, and the binary PlaceOfBirth is cho-
sen. On the other hand, without bridging, the sys-
tem errs on “In which comic book issue did Kitty
Pryde first appear?”, which refers to the rare pred-
icate ComicBookFirstAppearance. With bridging,
the parser can identify the correct binary, by linking
the types ComicBook and ComicBookCharacter. On
both datasets, best performance is achieved by com-
bining the two sources of information.
Overall, running on WEBQUESTIONS, the parser
constructs derivations that contain about 12,000 dis-
tinct binary predicates.
Feature variations Table 5 shows the results of
feature ablation studies. Accuracy drops when POS
tag features are omitted, e.g., in the question “What
number is Kevin Youkilis on the Boston Red Sox” the
parser happily skips the NNPs “Kevin Youkilis” and
returns the numbers of all players on the Boston Red
Sox. A significant loss is incurred without denota-
tion features, largely due to the parser returning log-
ical forms with empty denotations. For instance, the
question “How many people were at the 2006 FIFA
world cup final?” is answered with a logical form
containing the property PeopleInvolved rather than
SoccerMatchAttendance, resulting in an empty de-
notation.
Next we study the impact of lexicalized versus
</bodyText>
<figure confidence="0.626404">
0 iterations 1 iterations 2 iterations
</figure>
<figureCaption confidence="0.994781125">
Figure 4: Beam of candidate derivations D(x) for 50
WEBQUESTIONS examples. In each matrix, columns
correspond to examples and rows correspond to beam po-
sition (ranked by decreasing model score). Green cells
mark the positions of derivations with correct denota-
tions. Note that both the number of good derivations and
their positions improve as 0 is optimized.
Figure 5: Accuracy and oracle as beam size k increases.
</figureCaption>
<bodyText confidence="0.998498086956522">
unlexicalized features (Table 6). In the large WE-
BQUESTIONS dataset, lexicalized features helped,
and so we added those features to our model when
running on the test set. In FREE917 lexicalized fea-
tures result in overfitting due to the small number of
training examples. Thus, we ran our final parser on
the test set without lexicalized features.
Effect of beam size An intrinsic challenge in se-
mantic parsing is to handle the exponentially large
set of possible derivations. We rely heavily on the
k-best beam approximation in the parser keeping
good derivations that lead to the correct answer. Re-
call that the set of candidate derivations D(x) de-
pends on the parameters 0. In the initial stages of
learning, 0 is far from optimal, so good derivations
are likely to fall below the k-best cutoff of inter-
nal parser beams. As a result, D(x) contains few
derivations with the correct answer. Still, placing
these few derivations on the beam allows the train-
ing procedure to bootstrap 0 into a good solution.
Figure 4 illustrates this improvement in D(x) across
early training iterations.
Smaller choices of k yield a coarser approxima-
</bodyText>
<figure confidence="0.9997055">
0 100 200 300 400 500
(a) FREE917
oracle
accuracy
0 100 200
(b) WEBQUESTIONS
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
oracle
accuracy
</figure>
<page confidence="0.988457">
1541
</page>
<bodyText confidence="0.999940833333334">
tion in beam search. As we increase k (Figure 5), we
see a tapering improvement in accuracy. We also see
a widening gap between accuracy and oracle score,8
as including a good derivation in D(x) is made eas-
ier but the learning problem is made more difficult.
Error analysis The accuracy on WEBQUES-
TIONS is much lower than on FREE917. We an-
alyzed WEBQUESTIONS examples and found sev-
eral main causes of error: (i) Disambiguating en-
tities in WEBQUESTIONS is much harder because
the entity lexicon has 41M entities. For example,
given “Where did the battle of New Orleans start?”
the system identifies “New Orleans” as the target
entity rather than its surrounding noun phrase. Re-
call that all FREE917 experiments used a carefully
chosen entity lexicon. (ii) Bridging can often fail
when the question’s entity is compatible with many
binaries. For example, in “What did Charles Bab-
bage make?”, the system chooses a wrong binary
compatible with the type Person. (iii) The system
sometimes incorrectly draws verbs from subordinate
clauses. For example, in “Where did Walt Disney
live before he died?” it returns the place of death of
Walt Disney, ignoring the matrix verb live.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.985457666666667">
Our work intersects with two strands of work.
The first involves learning models of semantics
guided by denotations or interactions with the world.
Besides semantic parsing for querying databases
(Popescu et al., 2003; Clarke et al., 2010; Liang
et al., 2011), previous work has looked at inter-
preting natural language for performing program-
ming tasks (Kushman and Barzilay, 2013; Lei et
al., 2013), playing computer games (Branavan et al.,
2010; Branavan et al., 2011), following navigational
instructions (Chen, 2012; Artzi and Zettlemoyer,
2013), and interacting in the real world via percep-
tion (Matuszek et al., 2012; Tellex et al., 2011; Kr-
ishnamurthy and Kollar, 2013). Our system uses
denotations rather than logical forms as a training
signal, but also benefits from denotation features,
which becomes possible in the grounded setting.
The second body of work involves connecting
natural language and open-domain databases. Sev-
8Oracle score is the fraction of examples for which D(x)
contains any derivation with the correct denotation.
eral works perform relation extraction using dis-
tant supervision from a knowledge base (Riedel et
al., 2010; Carlson et al., 2010; Hoffmann et al.,
2011; Surdeanu et al., 2012). While similar in spirit
to our alignment procedure for building the lexi-
con, one difference is that relation extraction cares
about facts, aggregating over phrases, whereas a
lexicon concerns specific phrases, thus aggregating
over facts. On the question answering side, recent
methods have made progress in building semantic
parsers for the open domain, but still require a fair
amount of manual effort (Yahya et al., 2012; Unger
et al., 2012; Cai and Yates, 2013). Our system re-
duces the amount of supervision and has a more ex-
tensive evaluation on a new dataset.
Finally, although Freebase has thousands of prop-
erties, open information extraction (Banko et al.,
2007; Fader et al., 2011; Masaum et al., 2012)
and associated question answering systems (Fader
et al., 2013) work over an even larger open-ended
set of properties. The drawback of this regime is
that the noise and the difficulty in canonicaliza-
tion make it hard to perform reliable composition,
thereby nullifying one of the key benefits of se-
mantic parsing. An interesting midpoint involves
keeping the structured knowledge base but aug-
menting the predicates, for example using random
walks (Lao et al., 2011) or Markov logic (Zhang
et al., 2012). This would allow us to map atomic
words (e.g., “wife”) to composite predicates (e.g.,
Ax.Marriage.Spouse.(Gender.Femalenx)). Learn-
ing these composite predicates would drastically in-
crease the possible space of logical forms, but we
believe that the methods proposed in this paper—
alignment via distant supervision and bridging—can
provide some traction on this problem.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998092555555556">
We would like to thank Thomas Lin, Mausam and
Oren Etzioni for providing us with open IE triples
that are partially-linked to Freebase, and also Arun
Chaganty for helpful comments. The authors grate-
fully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040.
</bodyText>
<page confidence="0.995216">
1542
</page>
<sectionHeader confidence="0.989764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998135427184466">
Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 421–432.
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions
to actions. Transactions of the Association for Com-
putational Linguistics (TACL), 1:49–62.
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In International Joint Conference on
Artificial Intelligence (IJCAI), pages 2670–2676.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL), pages 1268–1277.
S. Branavan, D. Silver, and R. Barzilay. 2011. Learning
to win by reading manuals in a Monte-Carlo frame-
work. In Association for Computational Linguistics
(ACL), pages 268–277.
S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012.
Learning high-level planning from text. In Association
for Computational Linguistics (ACL), pages 126–135.
Q. Cai and A. Yates. 2013. Large-scale semantic parsing
via schema matching and lexicon extension. In Asso-
ciation for Computational Linguistics (ACL).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H.
Jr, and T. M. Mitchell. 2010. Toward an architecture
for never-ending language learning. In Association for
the Advancement of Artificial Intelligence (AAAI).
A. X. Chang and C. Manning. 2012. SUTime: A library
for recognizing and normalizing time expressions. In
Language Resources and Evaluation (LREC), pages
3735–3740.
D. Chen. 2012. Fast online lexicon learning for grounded
language acquisition. In Association for Computa-
tional Linguistics (ACL).
H. H. Clark. 1975. Bridging. In Workshop on theoretical
issues in natural language processing, pages 169–174.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18–27.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive
subgradient methods for online learning and stochas-
tic optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Empirical
Methods in Natural Language Processing (EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question answer-
ing. In Association for Computational Linguistics
(ACL).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL), pages 1486–1495.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Interational Conference on
Computational linguistics, pages 539–545.
R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak super-
vision for information extraction of overlapping rela-
tions. In Association for Computational Linguistics
(ACL), pages 541–550.
J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language to
the physical world. Transactions of the Association for
Computational Linguistics (TACL), 1:193–206.
J. Krishnamurthy and T. Mitchell. 2012. Weakly super-
vised training of semantic parsers. In Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP/CoNLL),
pages 754–765.
N. Kushman and R. Barzilay. 2013. Using semantic uni-
fication to generate regular expressions from natural
language. In Human Language Technology and North
American Association for Computational Linguistics
(HLT/NAACL), pages 826–836.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223–1233.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in CCG
grammar induction for semantic parsing. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 1512–1523.
N. Lao, T. Mitchell, and W. W. Cohen. 2011. Random
walk inference and learning in a large scale knowledge
base. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
T. Lei, F. Long, R. Barzilay, and M. Rinard. 2013.
From natural language specifications to program input
parsers. In Association for Computational Linguistics
(ACL).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In As-
</reference>
<page confidence="0.622372">
1543
</page>
<reference confidence="0.999793602941177">
sociation for Computational Linguistics (ACL), pages
590–599.
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity link-
ing at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 523–534.
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and
D. Fox. 2012. A joint model of language and percep-
tion for grounded attribute learning. In International
Conference on Machine Learning (ICML).
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In As-
sociation for Computational Linguistics (ACL), pages
91–98.
H. Poon. 2013. Grounded unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL).
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI), pages 149–157.
S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD), pages 148–163.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP/CoNLL), pages 455–
465.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Understand-
ing natural language commands for robotic navigation
and mobile manipulation. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
C. Unger, L. Bhmann, J. Lehmann, A. Ngonga, D. Ger-
ber, and P. Cimiano. 2012. Template-based ques-
tion answering over RDF data. In World Wide Web
(WWW), pages 639–648.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960–967.
M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath,
V. Tresp, and G. Weikum. 2012. Natural language
questions for the web of data. In Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL), pages
379–390.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 1050–1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658–666.
C. Zhang, R. Hoffmann, and D. S. Weld. 2012. Onto-
logical smoothing for relation extraction with minimal
supervision. In Association for the Advancement of
Artificial Intelligence (AAAI).
</reference>
<page confidence="0.995909">
1544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886021">
<title confidence="0.999923">Semantic Parsing on Freebase from Question-Answer Pairs</title>
<author confidence="0.999025">Jonathan Berant Andrew Chou Roy Frostig Percy</author>
<affiliation confidence="0.894814">Computer Science Department, Stanford</affiliation>
<abstract confidence="0.9995206">In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we a to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>421--432</pages>
<contexts>
<context position="1712" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="253" endWordPosition="257">ion We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic pars</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In Empirical Methods in Natural Language Processing (EMNLP), pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--49</pages>
<contexts>
<context position="37813" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="6145" endWordPosition="6148"> the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al.,</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="39158" citStr="Banko et al., 2007" startWordPosition="6359" endWordPosition="6362">for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”)</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the web. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="37710" citStr="Branavan et al., 2010" startWordPosition="6132" endWordPosition="6135">rom subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation.</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Association for Computational Linguistics (ACL), pages 1268–1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>D Silver</author>
<author>R Barzilay</author>
</authors>
<title>Learning to win by reading manuals in a Monte-Carlo framework.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>268--277</pages>
<contexts>
<context position="37734" citStr="Branavan et al., 2011" startWordPosition="6136" endWordPosition="6139">. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform rela</context>
</contexts>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>S. Branavan, D. Silver, and R. Barzilay. 2011. Learning to win by reading manuals in a Monte-Carlo framework. In Association for Computational Linguistics (ACL), pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>N Kushman</author>
<author>T Lei</author>
<author>R Barzilay</author>
</authors>
<title>Learning high-level planning from text.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>126--135</pages>
<contexts>
<context position="22180" citStr="Branavan et al., 2012" startWordPosition="3602" endWordPosition="3605">wo ways. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2. Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft</context>
</contexts>
<marker>Branavan, Kushman, Lei, Barzilay, 2012</marker>
<rawString>S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012. Learning high-level planning from text. In Association for Computational Linguistics (ACL), pages 126–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="842" citStr="Cai and Yates (2013)" startWordPosition="122" endWordPosition="125">ct In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (</context>
<context position="2251" citStr="Cai and Yates, 2013" startWordPosition="337" endWordPosition="340">, 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rul</context>
<context position="4105" citStr="Cai and Yates, 2013" startWordPosition="623" endWordPosition="626">light verbs (e.g., “go”) and prepositions is not very informative due to polysemy, and rare predicates (e.g., “cover price”) are difficult to cover even given a large corpus. To improve coverage, we propose a new bridging operation that generates predicates based on adjacent predicates rather than on words. At the compositional level, a semantic parser must combine the predicates into a coherent logical form. Previous work based on CCG requires manually specifying combination rules (Krishnamurthy and Mitchell, 2012) or inducing the rules from annotated logical forms (Kwiatkowski et al., 2010; Cai and Yates, 2013). We instead define a few simple composition rules which over-generate and then use model features to simulate soft rules and categories. In particular, we use POS tag features and features on the denotations of the predicted logical forms. We experimented with two question answering datasets on Freebase. First, on the dataset of Cai and Yates (2013), we showed that our system outperforms their state-of-the-art system 62% to 59%, despite using no annotated logical forms. Second, we collected a new realistic dataset of questions by performing a breadth-first search using the Google Suggest API;</context>
<context position="10891" citStr="Cai and Yates (2013)" startWordPosition="1767" endWordPosition="1770">rrect answer (Jd.zK)C = yi), summing over the latent derivation d. Formally, our training objective is n O(0) = log E po(d |xi). (1) i=1 dED(x):Qd.z➢K=yz Section 4 describes an approximation of this objective that we maximize to choose parameters 0. 3 Approach Our knowledge base has more than 19,000 properties, so a major challenge is generating a manageable set of predicates for an utterance. We propose two strategies for doing this. First (Section 3.1), we construct a lexicon that maps natural language phrases to logical predicates by aligning a large text corpus to Freebase, reminiscent of Cai and Yates (2013). Second, we generate logical predicates compatible with neighboring predicates using the bridging operation (Section 3.2). Bridging is crucial when aligning phrases is difficult or even impossible. The derivations produced by combining these predicates intersection Type.Location was PeopleBornHere.BarackObama ? where BarackObama PeopleBornHere lexicon lexicon Obama born lexicon join 1535 grew up in[Person,Location] born in[Person,Date] married in[Person,Date] born in[Person,Location] DateOfBirth PlaceOfBirth Marriage.StartDate PlacesLived.Location (RandomPerson,Seattle) F(r1) .F(r2) (Michelle</context>
<context position="22235" citStr="Cai and Yates, 2013" startWordPosition="3610" endWordPosition="3613">ord of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2. Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kw</context>
<context position="23724" citStr="Cai and Yates (2013)" startWordPosition="3848" endWordPosition="3851">an logical forms is a drawback since it provides less information, it is less obvious that working with denotations actually gives us additional information. Specifically, we include four features indicating whether the denotation of the predicted logical form has size 0, 1, 2, or at least 3. This feature encodes presupposition constraints in a soft way: when people ask a question, usually there is an answer and it is often unique. This allows us to favor logical forms with this property. 4 Experiments We now evaluate our semantic parser empirically. In Section 4.1, we compare our approach to Cai and Yates (2013) on their recently released dataset (henceforth, FREE917) and present results on a new 1538 dataset that we collected (henceforth, WEBQUESTIONS). In Section 4.2, we provide detailed experiments to provide additional insight on our system. Setup We implemented a standard beam-based bottom-up parser which stores the k-best derivations for each span. We use k = 500 for all our experiments on FREE917 and k = 200 on WEBQUESTIONS. The root beam yields the candidate set D(x) and is used to approximate the sum in the objective function 0(0) in (1). In experiments on WEBQUESTIONS, D(x) contained 197 de</context>
<context position="25669" citStr="Cai and Yates (2013)" startWordPosition="4184" endWordPosition="4187">ns, and binaries must be either a content word, or a verb followed by either a noun phrase or a particle. In addition, we used 17 hand-written rules to map question words such as “where” and “how many” to logical forms such as Type.Location and Count. To compute denotations, we convert a logical form z into a SPARQL query and execute it on our copy of Freebase using the Virtuoso engine. On WEBQUESTIONS, a full run over the training examples involves approximately 600,000 queries. For evaluation, we predict the answer from the derivation with highest probability. 4.1 Main results 4.1.1 FREE917 Cai and Yates (2013) created a dataset consisting of 917 questions involving 635 Freebase relations, annotated with lambda calculus forms. We converted all 917 questions into simple A-DCS, executed them on Freebase and used the resulting answers to train and evaluate. To map phrases to Freebase entities we used the manually-created entity lexicon used by Cai and Yates (2013), which contains 1,100 entries. Because entity disambiguation is a challenging problem in semantic parsing, the entity lexicon simplifies the problem. Following Cai and Yates (2013), we held out 30% of the examples for the final test, and perf</context>
<context position="38954" citStr="Cai and Yates, 2013" startWordPosition="6325" endWordPosition="6328">tion extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R H Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="38440" citStr="Carlson et al., 2010" startWordPosition="6243" endWordPosition="6246">nteracting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr, and T. M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A X Chang</author>
<author>C Manning</author>
</authors>
<title>SUTime: A library for recognizing and normalizing time expressions.</title>
<date>2012</date>
<booktitle>In Language Resources and Evaluation (LREC),</booktitle>
<pages>3735--3740</pages>
<contexts>
<context position="13465" citStr="Chang and Manning, 2012" startWordPosition="2150" endWordPosition="2154">ion triples (e1, r, e2) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4Freebase associates each entity with a set of types using the Type property. were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r E R1 and augment it with a type signature [t1, t2] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1, e2) to the extension of F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1 (t2). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r E R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F(x). For</context>
</contexts>
<marker>Chang, Manning, 2012</marker>
<rawString>A. X. Chang and C. Manning. 2012. SUTime: A library for recognizing and normalizing time expressions. In Language Resources and Evaluation (LREC), pages 3735–3740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
</authors>
<title>Fast online lexicon learning for grounded language acquisition.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="37783" citStr="Chen, 2012" startWordPosition="6143" endWordPosition="6144">” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a </context>
</contexts>
<marker>Chen, 2012</marker>
<rawString>D. Chen. 2012. Fast online lexicon learning for grounded language acquisition. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
</authors>
<date>1975</date>
<booktitle>Bridging. In Workshop on theoretical issues in natural language processing,</booktitle>
<pages>169--174</pages>
<contexts>
<context position="20320" citStr="Clark, 1975" startWordPosition="3292" endWordPosition="3293">ing 1 Type.FormOfGovernment Chile Type.FormOfGovernmentflGovernmentTypeOf.Chile 2 X-Men ComicBookCoverPriceOf.X-Men 3 Marriage.Spouse.TomCruise 2006 Marriage.(Spouse.TomCruise fl StartDate.2006) Table 2: Three examples of the bridging operation. The bridging binary predicate b is in boldface. constructs a logical form p1.(p2.z&apos; n b.z) for each logical predicate b with type (t1, t). In each of the three examples, bridging generates a binary predicate based on neighboring logical predicates rather than on explicit lexical material. In a way, our bridging operation shares with bridging anaphora (Clark, 1975) the idea of establishing a novel relation between distinct parts of a sentence. Naturally, we need features to distinguish between the generated predicates, or decide whether bridging is even appropriate at all. Given a binary b, features include the log of the predicate count log |F(b)|, indicators for the kind of bridging, an indicator on the binary b for injections (Table 1). In addition, we add all text similarity features by comparing the Freebase name of b with content words in the question. 3.3 Composition So far, we have mainly focused on the generation of predicates. We now discuss t</context>
</contexts>
<marker>Clark, 1975</marker>
<rawString>H. H. Clark. 1975. Bridging. In Workshop on theoretical issues in natural language processing, pages 169–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In Computational Natural Language Learning (CoNLL),</journal>
<pages>18--27</pages>
<contexts>
<context position="1637" citStr="Clarke et al., 2010" startWordPosition="241" endWordPosition="244">ion-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai an</context>
<context position="37503" citStr="Clarke et al., 2010" startWordPosition="6099" endWordPosition="6102">y is compatible with many binaries. For example, in “What did Charles Babbage make?”, the system chooses a wrong binary compatible with the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded se</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL), pages 18–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="24674" citStr="Duchi et al., 2010" startWordPosition="4010" endWordPosition="4013">ons for each span. We use k = 500 for all our experiments on FREE917 and k = 200 on WEBQUESTIONS. The root beam yields the candidate set D(x) and is used to approximate the sum in the objective function 0(0) in (1). In experiments on WEBQUESTIONS, D(x) contained 197 derivations on average. We write the approximate objective as O(0; 0) = Ei log EdED(xi;B):Qd.z] lc=yi p(d I xi; 0) to explicitly show dependence on the parameters 0� used for beam search. We optimize the objective by initializing 00 to 0 and applying AdaGrad (stochastic gradient ascent with per-feature adaptive step size control) (Duchi et al., 2010), so that 0t+1 is set based on taking a stochastic approximation of aO(B;Bt) ��B=Bt. aB We make six passes over the training examples. We used POS tagging and named-entity recognition to restrict what phrases in the utterance could be mapped by the lexicon. Entities must be named entities, proper nouns or a sequence of at least two tokens. Unaries must be a sequence of nouns, and binaries must be either a content word, or a verb followed by either a noun phrase or a particle. In addition, we used 17 hand-written rules to map question words such as “where” and “how many” to logical forms such a</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="13078" citStr="Fader et al., 2011" startWordPosition="2088" endWordPosition="2091">truct a set of typed4 phrases R1 (e.g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r E R1 U R2, we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), ... }. The lexicon is generated based on the overlap F(r1) n F(r2), for r1 E R1 and r2 E R2. Typed phrases 15 million triples (e1, r, e2) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4Freebase associates each entity with a set of types using the Type property. were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r E R1 and augment it with a type signature [t1, t2] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1, e2) </context>
<context position="39178" citStr="Fader et al., 2011" startWordPosition="6363" endWordPosition="6366">icon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predic</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="39263" citStr="Fader et al., 2013" startWordPosition="6376" endWordPosition="6379"> phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., Ax.Marriage.Spouse.(Gender.Femalenx)). Learning these composite predicate</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>A. Fader, L. Zettlemoyer, and O. Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1486--1495</pages>
<contexts>
<context position="1682" citStr="Goldwasser et al., 2011" startWordPosition="249" endWordPosition="252">ral baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai and Yates, 2013). The goal of this paper is to </context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>D. Goldwasser, R. Reichart, J. Clarke, and D. Roth. 2011. Confidence driven unsupervised semantic parsing. In Association for Computational Linguistics (ACL), pages 1486–1495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Freebase data dumps (2013-06-09).</title>
<date>2013</date>
<note>https://developers.google.com/ freebase/data.</note>
<contexts>
<context position="5649" citStr="Google, 2013" startWordPosition="878" endWordPosition="879">e downloaded from http://nlp.stanford.edu/ software/sempre/. 2 Setup Problem Statement Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of question-answer pairs {(xi, yi)}Z1, output a semantic parser that maps new questions x to answers y via latent logical forms z and the knowledge base K. 2.1 Knowledge base Let £ denote a set of entities (e.g., BarackObama), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1, p, e2) E £ x P x £ (e.g., (BarackObama, PlaceOfBirth, Honolulu)). We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions.1 2.2 Logical forms To query the knowledge base, we use a logical language called Lambda Dependency-Based Compositional Semantics (λ-DCS)—see Liang (2013) for details. For the purposes of this paper, we use a restricted subset called simple λ-DCS, which we will define below for the sake of completeness. The chief motivation of λ-DCS is to produce logical forms that are simpler than lambda calculus forms. For example, λx.1a.p1(x, a) n lb.p2(a, b) n p3(b, e) is expressed compactly in λ-DCS as p1.p2.p3.e. Like DCS (Liang et</context>
</contexts>
<marker>Google, 2013</marker>
<rawString>Google. 2013. Freebase data dumps (2013-06-09). https://developers.google.com/ freebase/data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Interational Conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="13907" citStr="Hearst, 1992" startWordPosition="2232" endWordPosition="2233">ies by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r E R1 and augment it with a type signature [t1, t2] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1, e2) to the extension of F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1 (t2). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r E R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F(x). For (Honolulu, “is a city in”, Hawaii), we extract x = “city&amp;quot; and add Honolulu to F(“city”). From the initial 15M triples, we extracted 55,081 typed binary phrases (9,456 untyped) and 6,299 unary phrases. Logical predicates Binary logical predicates contain (i) all KB properties6 and (ii) concatenations of two properties p1.p2 if the intermediate type represents an event (e.g., the married to relation is represented by Marriage.Spouse). For </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Interational Conference on Computational linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hoffmann</author>
<author>C Zhang</author>
<author>X Ling</author>
<author>L S Zettlemoyer</author>
<author>D S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>541--550</pages>
<contexts>
<context position="38463" citStr="Hoffmann et al., 2011" startWordPosition="6247" endWordPosition="6250"> world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally,</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and D. S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Association for Computational Linguistics (ACL), pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--193</pages>
<contexts>
<context position="37940" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="6166" endWordPosition="6170">. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure </context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>J. Krishnamurthy and T. Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics (TACL), 1:193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>754--765</pages>
<contexts>
<context position="2950" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="446" endWordPosition="449">thout annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema bridging Education 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light verbs (e.g., “go”) and prepositions is not very informative</context>
<context position="22214" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="3606" endWordPosition="3609">oduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2. Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, </context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>J. Krishnamurthy and T. Mitchell. 2012. Weakly supervised training of semantic parsers. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 754–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushman</author>
<author>R Barzilay</author>
</authors>
<title>Using semantic unification to generate regular expressions from natural language.</title>
<date>2013</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL),</booktitle>
<pages>826--836</pages>
<contexts>
<context position="37644" citStr="Kushman and Barzilay, 2013" startWordPosition="6121" endWordPosition="6124">h the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples f</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>N. Kushman and R. Barzilay. 2013. Using semantic unification to generate regular expressions from natural language. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 826–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="1357" citStr="Kwiatkowski et al., 2010" startWordPosition="198" endWordPosition="201">tion to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to</context>
<context position="4083" citStr="Kwiatkowski et al., 2010" startWordPosition="619" endWordPosition="622">ired predicates. Aligning light verbs (e.g., “go”) and prepositions is not very informative due to polysemy, and rare predicates (e.g., “cover price”) are difficult to cover even given a large corpus. To improve coverage, we propose a new bridging operation that generates predicates based on adjacent predicates rather than on words. At the compositional level, a semantic parser must combine the predicates into a coherent logical form. Previous work based on CCG requires manually specifying combination rules (Krishnamurthy and Mitchell, 2012) or inducing the rules from annotated logical forms (Kwiatkowski et al., 2010; Cai and Yates, 2013). We instead define a few simple composition rules which over-generate and then use model features to simulate soft rules and categories. In particular, we use POS tag features and features on the denotations of the predicted logical forms. We experimented with two question answering datasets on Freebase. First, on the dataset of Cai and Yates (2013), we showed that our system outperforms their state-of-the-art system 62% to 59%, despite using no annotated logical forms. Second, we collected a new realistic dataset of questions by performing a breadth-first search using t</context>
<context position="22857" citStr="Kwiatkowski et al., 2010" startWordPosition="3709" endWordPosition="3712">13; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) encode the combination preferences structurally in non-overlapping rules; these could be emulated with features with weights clamped to −oc. Denotation features While it is clear that learning from denotations rather than logical forms is a drawback since it provides less information, it is less obvious that working with denotations actually gives us additional information. Specifically, we include four features indicating whether the denotation of the predicted logical form has size 0, 1, 2, or at least 3. This feature encodes presupposition constraints in a soft w</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP), pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1512--1523</pages>
<contexts>
<context position="2682" citStr="Kwiatkowski et al., 2011" startWordPosition="404" endWordPosition="407">e use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema bridging Education 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build</context>
<context position="22884" citStr="Kwiatkowski et al., 2011" startWordPosition="3713" endWordPosition="3716">arn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) encode the combination preferences structurally in non-overlapping rules; these could be emulated with features with weights clamped to −oc. Denotation features While it is clear that learning from denotations rather than logical forms is a drawback since it provides less information, it is less obvious that working with denotations actually gives us additional information. Specifically, we include four features indicating whether the denotation of the predicted logical form has size 0, 1, 2, or at least 3. This feature encodes presupposition constraints in a soft way: when people ask a quest</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), pages 1512–1523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lao</author>
<author>T Mitchell</author>
<author>W W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="39665" citStr="Lao et al., 2011" startWordPosition="6441" endWordPosition="6444">taset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., Ax.Marriage.Spouse.(Gender.Femalenx)). Learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem. Acknowledgments We would like to thank Thomas Lin, Mausam and Oren Etzioni for providing us with open IE triples that are partially-linked to Freebase, and also Arun Chaganty for helpful com</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>N. Lao, T. Mitchell, and W. W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lei</author>
<author>F Long</author>
<author>R Barzilay</author>
<author>M Rinard</author>
</authors>
<title>From natural language specifications to program input parsers.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="37663" citStr="Lei et al., 2013" startWordPosition="6125" endWordPosition="6128"> system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) conta</context>
</contexts>
<marker>Lei, Long, Barzilay, Rinard, 2013</marker>
<rawString>T. Lei, F. Long, R. Barzilay, and M. Rinard. 2013. From natural language specifications to program input parsers. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>590--599</pages>
<contexts>
<context position="1657" citStr="Liang et al., 2011" startWordPosition="245" endWordPosition="248">improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates (Cai and Yates, 2013). The </context>
<context position="6260" citStr="Liang et al., 2011" startWordPosition="977" endWordPosition="980">e, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions.1 2.2 Logical forms To query the knowledge base, we use a logical language called Lambda Dependency-Based Compositional Semantics (λ-DCS)—see Liang (2013) for details. For the purposes of this paper, we use a restricted subset called simple λ-DCS, which we will define below for the sake of completeness. The chief motivation of λ-DCS is to produce logical forms that are simpler than lambda calculus forms. For example, λx.1a.p1(x, a) n lb.p2(a, b) n p3(b, e) is expressed compactly in λ-DCS as p1.p2.p3.e. Like DCS (Liang et al., 2011), λ-DCS makes existential quantification implicit, thereby reducing the number of variables. Variables are only used for anaphora and building composite binary predicates; these do not appear in simple λ-DCS. Each logical form in simple λ-DCS is either a unary (which denotes a subset of £) or a binary (which denotes a subset of £ x £). The basic λ- DCS logical forms z and their denotations JzKK are defined recursively as follows: • Unary base case: If e E £ is an entity (e.g., Seattle), then e is a unary logical form with JzKK = {e}. • Binary base case: If p E P is a property (e.g., PlaceOfBir</context>
<context position="9880" citStr="Liang et al. (2011)" startWordPosition="1594" endWordPosition="1597">kip any words, and in general heav3We also discard logical forms are incompatible according to the Freebase types (e.g., Profession.Politician n Type.City would be rejected). Type.LocationflPeopleBornHere.BarackObama Figure 2: An example of a derivation d of the utterance “Where was Obama born?” and its sub-derivations, each labeled with composition rule (in blue) and logical form (in red). The derivation d skips the words “was” and “?”. ily over-generates. We instead rely on features and learning to guide us away from the bad derivations. Modeling Following Zettlemoyer and Collins (2005) and Liang et al. (2011), we define a discriminative log-linear model over derivations d E D(x) given utterances x: po(d |x) = exp{0(x,d)&gt;o}0 T , where O(x, d) is a feature �d0∈D(a) exp{0(x,d) o} vector extracted from the utterance and the derivation, and 0 E Rb is the vector of parameters to be learned. As our training data consists only of question-answer pairs (xi, yi), we maximize the loglikelihood of the correct answer (Jd.zK)C = yi), summing over the latent derivation d. Formally, our training objective is n O(0) = log E po(d |xi). (1) i=1 dED(x):Qd.z➢K=yz Section 4 describes an approximation of this objective </context>
<context position="37524" citStr="Liang et al., 2011" startWordPosition="6103" endWordPosition="6106">many binaries. For example, in “What did Charles Babbage make?”, the system chooses a wrong binary compatible with the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second bod</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Lambda dependency-based compositional semantics.</title>
<date>2013</date>
<tech>Technical report, ArXiv.</tech>
<contexts>
<context position="5877" citStr="Liang (2013)" startWordPosition="911" endWordPosition="912">that maps new questions x to answers y via latent logical forms z and the knowledge base K. 2.1 Knowledge base Let £ denote a set of entities (e.g., BarackObama), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1, p, e2) E £ x P x £ (e.g., (BarackObama, PlaceOfBirth, Honolulu)). We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions.1 2.2 Logical forms To query the knowledge base, we use a logical language called Lambda Dependency-Based Compositional Semantics (λ-DCS)—see Liang (2013) for details. For the purposes of this paper, we use a restricted subset called simple λ-DCS, which we will define below for the sake of completeness. The chief motivation of λ-DCS is to produce logical forms that are simpler than lambda calculus forms. For example, λx.1a.p1(x, a) n lb.p2(a, b) n p3(b, e) is expressed compactly in λ-DCS as p1.p2.p3.e. Like DCS (Liang et al., 2011), λ-DCS makes existential quantification implicit, thereby reducing the number of variables. Variables are only used for anaphora and building composite binary predicates; these do not appear in simple λ-DCS. Each log</context>
<context position="7376" citStr="Liang, 2013" startWordPosition="1184" endWordPosition="1185">unary logical form with JzKK = {e}. • Binary base case: If p E P is a property (e.g., PlaceOfBirth), then p is a binary logical form with JpKK = {(e1, e2) : (e1, p, e2) E K}.2 • Join: If b is a binary and u is a unary, then b.u (e.g., PlaceOfBirth.Seattle) is a unary denoting a join and project: Jb.uKK = {e1 E £ : le2.(e1, e2) E JbKK n e2 E JuKK}. 1In this paper, we condense Freebase names for readability (/people/person becomes Person). 2Binaries can be also built out of lambda abstractions (e.g., Ax.Performance.Actor.x), but as these constructions are not central to this paper, we defer to (Liang, 2013). 1534 • Intersection: If u1 and u2 are both unaries, then u1 fl u2 (e.g., Profession.Scientist fl PlaceOfBirth.Seattle) denotes set intersection: Ju1 n u2K)C = Ju1K)C n Ju2K)C. • Aggregation: If u is a unary, then count(u) denotes the cardinality: Jcount(u)K)C = I|JuK)C|I. As a final example, “number of dramas starring Tom Cruise” in lambda calculus would be represented as count(Ax.Genre(x, Drama) ∧ ly.Performance(x, y) ∧ Actor(y, TomCruise)); in A-DCS, it is simply count(Genre.Drama fl Performance.Actor.TomCruise). It is useful to think of the knowledge base K as a directed graph in which en</context>
</contexts>
<marker>Liang, 2013</marker>
<rawString>P. Liang. 2013. Lambda dependency-based compositional semantics. Technical report, ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lin</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Entity linking at web scale.</title>
<date>2012</date>
<booktitle>In Knowledge Extraction Workshop (AKBC-WEKEX).</booktitle>
<contexts>
<context position="13097" citStr="Lin et al. (2012)" startWordPosition="2092" endWordPosition="2095"> phrases R1 (e.g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r E R1 U R2, we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), ... }. The lexicon is generated based on the overlap F(r1) n F(r2), for r1 E R1 and r2 E R2. Typed phrases 15 million triples (e1, r, e2) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4Freebase associates each entity with a set of types using the Type property. were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r E R1 and augment it with a type signature [t1, t2] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1, e2) to the extension of</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>T. Lin, Mausam, and O. Etzioni. 2012. Entity linking at web scale. In Knowledge Extraction Workshop (AKBC-WEKEX).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schmitz Masaum</author>
<author>R Bart</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>523--534</pages>
<contexts>
<context position="39200" citStr="Masaum et al., 2012" startWordPosition="6367" endWordPosition="6370"> is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., Ax.Marriag</context>
</contexts>
<marker>Masaum, Bart, Soderland, Etzioni, 2012</marker>
<rawString>Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Etzioni. 2012. Open language learning for information extraction. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 523–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>N FitzGerald</author>
<author>L Zettlemoyer</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="37886" citStr="Matuszek et al., 2012" startWordPosition="6158" endWordPosition="6161">Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. 2012. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="21894" citStr="McDonald et al., 2005" startWordPosition="3550" endWordPosition="3554">h can only represent one-sided preferences for having more or fewer of a given operation. Indicator features stabilize the model, preferring derivations with a well-balanced inventory of operations. Part-of-speech tag features To guide the composition of predicates, we use POS tags in two ways. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2. Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijin</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Association for Computational Linguistics (ACL), pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
</authors>
<title>Grounded unsupervised semantic parsing.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="22248" citStr="Poon, 2013" startWordPosition="3614" endWordPosition="3615">g is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2. Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kwiatkowski et </context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>H. Poon. 2013. Grounded unsupervised semantic parsing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
<author>H Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In International Conference on Intelligent User Interfaces (IUI),</booktitle>
<pages>149--157</pages>
<contexts>
<context position="37482" citStr="Popescu et al., 2003" startWordPosition="6095" endWordPosition="6098">n the question’s entity is compatible with many binaries. For example, in “What did Charles Babbage make?”, the system chooses a wrong binary compatible with the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possib</context>
</contexts>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a theory of natural language interfaces to databases. In International Conference on Intelligent User Interfaces (IUI), pages 149–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>L Yao</author>
<author>A McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases (ECML PKDD),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="38418" citStr="Riedel et al., 2010" startWordPosition="6239" endWordPosition="6242">lemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more ex</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>S. Riedel, L. Yao, and A. McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>J Tibshirani</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="38487" citStr="Surdeanu et al., 2012" startWordPosition="6251" endWordPosition="6254">Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has t</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 455– 465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>A G Banerjee</author>
<author>S J Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="37907" citStr="Tellex et al., 2011" startWordPosition="6162" endWordPosition="6165">h two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev8Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in s</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Unger</author>
<author>L Bhmann</author>
<author>J Lehmann</author>
<author>A Ngonga</author>
<author>D Gerber</author>
<author>P Cimiano</author>
</authors>
<title>Template-based question answering over RDF data. In World Wide Web (WWW),</title>
<date>2012</date>
<pages>639--648</pages>
<contexts>
<context position="2894" citStr="Unger et al., 2012" startWordPosition="440" endWordPosition="443"> is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema bridging Education 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light ver</context>
<context position="38932" citStr="Unger et al., 2012" startWordPosition="6321" endWordPosition="6324">l works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting mid</context>
</contexts>
<marker>Unger, Bhmann, Lehmann, Ngonga, Gerber, Cimiano, 2012</marker>
<rawString>C. Unger, L. Bhmann, J. Lehmann, A. Ngonga, D. Gerber, and P. Cimiano. 2012. Template-based question answering over RDF data. In World Wide Web (WWW), pages 639–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="1330" citStr="Wong and Mooney, 2007" startWordPosition="194" endWordPosition="197">we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which college did Obama go to</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yahya</author>
<author>K Berberich</author>
<author>S Elbassuoni</author>
<author>M Ramanath</author>
<author>V Tresp</author>
<author>G Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>379--390</pages>
<contexts>
<context position="2873" citStr="Yahya et al., 2012" startWordPosition="436" endWordPosition="439">e goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema bridging Education 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicate</context>
<context position="38912" citStr="Yahya et al., 2012" startWordPosition="6317" endWordPosition="6320">rect denotation. eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath, V. Tresp, and G. Weikum. 2012. Natural language questions for the web of data. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 379–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic proramming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1276" citStr="Zelle and Mooney, 1996" startWordPosition="185" endWordPosition="188">sing a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignme</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1307" citStr="Zettlemoyer and Collins, 2005" startWordPosition="189" endWordPosition="193">d a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. 1 Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical Occidental College, Columbia University Execute on Database Type.University fl Education.BarackObama Type.University alignment BarackObama alignment Which </context>
<context position="9856" citStr="Zettlemoyer and Collins (2005)" startWordPosition="1589" endWordPosition="1592"> of derivations D(x) allows us to skip any words, and in general heav3We also discard logical forms are incompatible according to the Freebase types (e.g., Profession.Politician n Type.City would be rejected). Type.LocationflPeopleBornHere.BarackObama Figure 2: An example of a derivation d of the utterance “Where was Obama born?” and its sub-derivations, each labeled with composition rule (in blue) and logical form (in red). The derivation d skips the words “was” and “?”. ily over-generates. We instead rely on features and learning to guide us away from the bad derivations. Modeling Following Zettlemoyer and Collins (2005) and Liang et al. (2011), we define a discriminative log-linear model over derivations d E D(x) given utterances x: po(d |x) = exp{0(x,d)&gt;o}0 T , where O(x, d) is a feature �d0∈D(a) exp{0(x,d) o} vector extracted from the utterance and the derivation, and 0 E Rb is the vector of parameters to be learned. As our training data consists only of question-answer pairs (xi, yi), we maximize the loglikelihood of the correct answer (Jd.zK)C = yi), summing over the latent derivation d. Formally, our training objective is n O(0) = log E po(d |xi). (1) i=1 dED(x):Qd.z➢K=yz Section 4 describes an approxim</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhang</author>
<author>R Hoffmann</author>
<author>D S Weld</author>
</authors>
<title>Ontological smoothing for relation extraction with minimal supervision.</title>
<date>2012</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="39702" citStr="Zhang et al., 2012" startWordPosition="6448" endWordPosition="6451">as thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., Ax.Marriage.Spouse.(Gender.Femalenx)). Learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem. Acknowledgments We would like to thank Thomas Lin, Mausam and Oren Etzioni for providing us with open IE triples that are partially-linked to Freebase, and also Arun Chaganty for helpful comments. The authors gratefully acknowl</context>
</contexts>
<marker>Zhang, Hoffmann, Weld, 2012</marker>
<rawString>C. Zhang, R. Hoffmann, and D. S. Weld. 2012. Ontological smoothing for relation extraction with minimal supervision. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994951">
Centering Similarity Measures to Reduce Hubs
</title>
<author confidence="0.994497">
Ikumi Suzuki Kazuo Hara Masashi Shimbo
</author>
<affiliation confidence="0.9278565">
National Institute of Genetics National Institute of Genetics Nara Institute of Science and Technology
Mishima, Shizuoka, Japan Mishima, Shizuoka, Japan Ikoma, Nara, Japan
</affiliation>
<email confidence="0.932987">
suzuki.ikumi@gmail.com kazuo.hara@gmail.com shimbo@is.naist.jp
</email>
<author confidence="0.865035">
Marco Saerens
</author>
<affiliation confidence="0.694344">
Universit´e catholique de Louvain
Louvain-la-Neuve, Belgium
</affiliation>
<email confidence="0.964928">
marco.saerens@uclouvain.be
</email>
<sectionHeader confidence="0.998387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999704277777778">
The performance of nearest neighbor methods
is degraded by the presence of hubs, i.e., ob-
jects in the dataset that are similar to many
other objects. In this paper, we show that the
classical method of centering, the transforma-
tion that shifts the origin of the space to the
data centroid, provides an effective way to re-
duce hubs. We show analytically why hubs
emerge and why they are suppressed by cen-
tering, under a simple probabilistic model of
data. To further reduce hubs, we also move
the origin more aggressively towards hubs,
through weighted centering. Our experimental
results show that (weighted) centering is effec-
tive for natural language data; it improves the
performance of the k-nearest neighbor classi-
fiers considerably in word sense disambigua-
tion and document classification tasks.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.985597">
1.1 Background
</subsectionHeader>
<bodyText confidence="0.999274923076923">
The k-nearest neighbor (kNN) algorithm is a sim-
ple nonparametric method of classification. It has
been applied to various natural language process-
ing (NLP) tasks such as document classification
(Masand et al., 1992; Yang and Liu, 1999), part-
of-speech tagging (Søgaard, 2011), and word sense
disambiguation (Navigli, 2009).
To apply the kNN algorithm, data is typically rep-
resented as a vector object in a feature space, and
(dis)similarity between data is measured by the dis-
tance between the vectors, their inner product, or co-
sine of the angle between them (Jurafsky and Mar-
tin, 2008). With such a (dis)similarity measure, the
</bodyText>
<author confidence="0.380552">
Kenji Fukumizu
</author>
<affiliation confidence="0.8882405">
The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan
</affiliation>
<email confidence="0.910405">
fukumizu@ism.ac.jp
</email>
<bodyText confidence="0.99794825">
unknown class label of a test object is predicted by
a majority vote of the classes of its k most similar
objects in the labeled training set.
Recent studies (Radovanovi´c et al., 2010a;
Radovanovi´c et al., 2010b) have shown that if the
feature space is high-dimensional, some objects in
the dataset emerge as hubs; i.e., these objects fre-
quently appear in the k nearest neighbors of other
objects.
The emergence of hubs may deteriorate the per-
formance of kNN classification and nearest neighbor
search in general:
</bodyText>
<listItem confidence="0.933897285714286">
• If hub objects exist in the training set, they have
a strong chance to be a kNN of many test ob-
jects. Because the class of a test object is pre-
dicted by a majority vote from its k nearest
neighbors, prediction is biased toward the la-
bels of the hubs.
• In information retrieval, nearest neighbor
</listItem>
<bodyText confidence="0.9707548">
search finds objects in the database that are
most relevant, or similar, to user-provided
queries. If particular objects, such as hubs, are
nearly always returned for any query, the re-
trieved results are probably not very useful.
These drawbacks may hinder application of near-
est neighbor methods in NLP, as typical natural lan-
guage data are extremely high-dimensional (Juraf-
sky and Martin, 2008) and thus prone to produce
hubs.
</bodyText>
<subsectionHeader confidence="0.932162">
1.2 Contributions
</subsectionHeader>
<bodyText confidence="0.6414365">
Centering (Mardia et al., 1979; Fisher and Lenz,
1996; Eriksson et al., 2006) is a standard technique
</bodyText>
<page confidence="0.983269">
613
</page>
<note confidence="0.7338735">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613–623,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846235294118">
for removing observation bias in the data. It is a
transformation of feature space in a way that the ori-
gin of the space is moved to the data centroid (sam-
ple mean). The distance between data objects is not
changed by centering, but their inner product and co-
sine are affected; see Section 3 for detail.
In this paper, we advocate the use of centering as a
means of reducing hubs. Specifically, we propose to
measure the similarity of objects by the inner prod-
uct (not distance or cosine) in the centered feature
space.
Our approach is motivated by the observation that
the objects similar to the data centroid tend to be-
come hubs (Radovanovi´c et al., 2010a). This ob-
servation suggests that the number of hubs may be
reduced if we can define a similarity measure that
makes all objects in a dataset equally similar to the
centroid (Suzuki et al., 2012). The inner product in
the centered space indeed enjoys this property.
In Section 4, we analyze why hubs emerge under
a simple probabilistic model of data, and also give
an account of why they are suppressed by centering.
Using both synthetic and real datasets, we show
that objects similar to the centroid also emerge as
hubs in multi-cluster data (Section 5), so the applica-
tion of centering is wider than expected. To further
reduce hubs, we also propose to move the origin of
the space more aggressively towards hubs, through
weighted centering (Section 6).
In Section 7, we show that centering and weighted
centering are effective for natural language data.
these methods markedly improve the performance
of kNN classifiers in word sense disambiguation and
document classification tasks.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999870487804878">
Centering is a classical technique widely used in
many fields of science. For instance, centering
forms a preprocessing step in principal component
analysis and Fisher linear discriminant analysis.
In NLP, however, centering is seldom used; the
use of cosine and inner product similarities is quite
common, but they are nearly always used uncen-
tered. Non-centered cosine is used, for instance, in
word sense disambiguation (Sch¨utze, 1998; Navigli,
2009), paraphrasing (Erk and Pad´o, 2008; Thater
et al., 2010), and compositional semantics (Mitchell
and Lapata, 2008), to name a few.
There have been several approaches to improv-
ing kNN classification: learning similarity/distance
measures from training data (metric learning)
(Weinberger and Saul, 2009; Qamar et al., 2008),
weighting nearest neighbors for similarity-based
classification (Chen et al., 2009), and neighbor-
hood size selection (Wang et al., 2006; Guo and
Chakraborty, 2010). However, none of these have
addressed the reduction of hubs.
More recently, Schnitzer et al. (2012) proposed
the Mutual Proximity transformation that rescales
distance measures to decrease hubs in a dataset.
Suzuki et al. (2012) showed that kernels based on
graph Laplacian, such as the commute-time kernels
(Saerens et al., 2004) and the regularized Laplacian
(Chebotarev and Shamis, 1997; Smola and Kondor,
2003), make all objects equally similar to the data
centroid, which in turn reduce hubs.
In Section 7, we evaluate centering, Mutual Prox-
imity, and Laplacian kernels in NLP tasks, and
demonstrate that centering is equally or even more
effective. Section 4 presents a theoretical justifica-
tion for using centering to reduce hubs, but this kind
of analysis is missing for the Laplacian kernels.
Centering is easier to compute as well. For a
dataset of n objects, it takes O(n2) time to com-
pute, whereas computing a Laplacian-based kernel
requires O(n3) time for matrix inversion. Mutual
Proximity also has a time complexity of O(n2).
</bodyText>
<sectionHeader confidence="0.991972" genericHeader="method">
3 Centering
</sectionHeader>
<bodyText confidence="0.9995468">
Consider a dataset of n objects in an m-dimensional
feature space, x1, · · · , xn E Rm. Throughout this
paper, we use the inner product (xi, xj) as a measure
of similarity between xi and xj. Let K be the Gram
matrix of the n feature vectors, i.e., the n x n matrix
</bodyText>
<equation confidence="0.878295333333333">
whose (i, j) element holds (xi, xj). Using m x n data
matrix X = [x1, · · · , xn], we can write K as
K = XTX,
</equation>
<bodyText confidence="0.995299666666667">
where XT represents the matrix transpose of X.
Centering is a transformation in which the origin
of the feature space is shifted to the data centroid
</bodyText>
<equation confidence="0.686858">
1 n
x¯= n i=1 xi, (1)
</equation>
<page confidence="0.848646">
614
</page>
<bodyText confidence="0.785301">
and object x is mapped to the centered feature vector
</bodyText>
<equation confidence="0.941214">
xcent = x − ¯x. (2)
</equation>
<bodyText confidence="0.9723599">
The similarity between two objects x and x0 is now
measured by hxcent, x0centi = hx − ¯x, x0 − ¯xi.
After centering, the inner product between any
object and the data centroid (which is a zero vector
because ¯xcent = x¯ − x¯ = 0) is uniformly 0; in other
words, all objects in the dataset have an equal simi-
larity to the centroid. According to the observation
that the objects similar to the centroid become hubs
(Radovanovi´c et al., 2010a), we can expect hubs to
be reduced after centering.
Intuitively, centering reduces hubs because it
makes the length of the feature vector xcent short
for (hub) objects x that lie close to the data centroid
¯x; see Eq. (2). And since we measure object simi-
larity by inner product, shorter vectors tend to pro-
duce smaller similarity scores. Hence objects close
to the data centroid become less similar to other ob-
jects after centering, and no longer be hubs. In Sec-
tion 4, we analyze the effect of centering on hubness
in more detail.
</bodyText>
<subsectionHeader confidence="0.997573">
3.1 Centered Gram matrix
</subsectionHeader>
<bodyText confidence="0.999734">
Let I be an n × n identity matrix and ]L be an n-
dimensional all-ones vector. The symmetric matrix
H = I−(1/n)]L]LT is called centering matrix, because
the centered data matrix Xcent = [xcent
</bodyText>
<equation confidence="0.951728">
1 , ··· ,xcent
n ]
</equation>
<bodyText confidence="0.986062">
can be computed by Xcent = XH (Mardia et al.,
1979).
The Gram matrix Kcent of the centered feature
vectors, whose (i, j) element holds the inner prod-
uct hxcent
</bodyText>
<equation confidence="0.9523684">
i , xcent
j i, can be calculated from the original
Gram matrix K by
(Xcent)T (Xcent)
Kcent = = HXTXH = HKH. (3)
</equation>
<bodyText confidence="0.982350571428572">
Eq. (3) implies that the original data matrix X is
not needed to compute the centered Gram matrix
Kcent, provided that K is given. It is hence possi-
ble to use the so-called kernel trick; i.e., centering
can be applied even if data matrix X is not available
but the similarity of objects can be measured by a
kernel function in an implicit feature space.
</bodyText>
<sectionHeader confidence="0.8435905" genericHeader="method">
4 Theoretical analysis of the effect of
centering on hubness
</sectionHeader>
<bodyText confidence="0.99966875">
We now analyze why objects most similar to the
centroid tend to be hubs in the dataset, and give an
explanation as to why centering may suppress the
emergence of hubs.
</bodyText>
<subsectionHeader confidence="0.999425">
4.1 Before centering
</subsectionHeader>
<bodyText confidence="0.9995616">
Consider a dataset of m-dimensional feature vectors,
with each vector x ∈ Rm generated independently
from a distribution with a finite mean vector µ. In
other words, objects x in this dataset are drawn from
a distribution P(x), i.e.,
</bodyText>
<equation confidence="0.945890666666667">
x ∼ P(x),
and
Jµ = E[x] = x dP(x) (4)
</equation>
<bodyText confidence="0.960295529411765">
where E[·] denotes the expectation of a random vari-
able.
We will use the following elementary lemma on
the distributions of inner product subsequently.
Lemma 1. Let a ∈ Rm be a fixed vector, and x ∈ Rm
be an object sampled according to distribution P(x).
Then the inner product ha, xi follows a distribution
with mean ha, µi.
Proof. From the linearity of the inner product and
Eq. (4), we obtain
JE[ha, xi] = ha, xi dP(x)
J= ha, x dP(x)i = ha, µi. ❑
Now, imagine that we have an object x sam-
pled from P(x), and we want to compute its nearest
neighbor in a dataset. Let h and ` be two fixed ob-
jects in the dataset, such that the inner product to the
true mean µ is higher for h than for `, i.e.,
</bodyText>
<equation confidence="0.805551">
hh, µi − h`, µi &gt; 0. (5)
</equation>
<bodyText confidence="0.999588">
We are interested in which of h and ` is more similar
to x (in terms of inner product), or in other words,
the difference of two inner products
</bodyText>
<equation confidence="0.718712">
z = hh, xi − h`, xi = hh − `, xi. (6)
</equation>
<page confidence="0.979481">
615
</page>
<bodyText confidence="0.9918385">
Because x is a random variable, so is z. Let Q(z) be
the distribution of z; i.e., z ∼ Q(z).
Using Lemma 1 with a = h − `, together with
Eq. (5), we have
</bodyText>
<equation confidence="0.996995">
E[z] = hh − `,µi = hh,µi − h`,µi &gt; 0. (7)
</equation>
<bodyText confidence="0.994314612903226">
Note that the above statement is only concerned
about the mean, so it does not in general assure that
hh, xi &gt; h`, xi (8)
holds with high probability; there is a chance that
a small number of outliers are inflating the mean.
To assure that inequality (8) holds with probability
greater than 1/2 for instance, the median rather than
the mean of the distribution Q(z) must be greater
than 0.
If the distribution Q(z) is symmetric, the median
occurs at the same point as the mean, and the above
claim holds. Indeed, if the components of x are gen-
erated independently from (possibly non-identical)
normal distributions, we can show that Q(z) also
obeys a normal distribution. Because it is a symmet-
ric distribution, we can safely say that in this case,
Eq. (8) holds with probability greater than 1/2.
For a general non-symmetric distribution with a
finite variance, the median is known to be within the
standard deviation of the mean (Mallows, 1991), so
we could still say that Eq. (8) is likely to hold if hh −
`, µi is sufficiently large compared to the standard
deviation.
Now, if we let h be the object in a given dataset
with the highest similarity (inner product) to the
mean µ, and let ` be any other object in the set, then
we see from the above discussion that h is likely to
have higher similarity to x, a test sample drawn from
distribution P(x). Because this holds for any ` in
the dataset, the conclusion is that the objects in the
dataset most similar to µ are likely to become hubs.
</bodyText>
<subsectionHeader confidence="0.998054">
4.2 After centering
</subsectionHeader>
<bodyText confidence="0.813828">
Next let us investigate what happens if the dataset
is centered. Let x¯ be the sample (empirical) mean
given by Eq. (1). After centering, the similarity of x
with each of the two fixed objects h and ` are evalu-
ated by hh − ¯x, x − ¯xi and h` − ¯x, x − ¯xi, respectively.
Their difference zcent is given by
zcent = hh − ¯x, x − ¯xi − h` − ¯x, x − ¯xi
= hh − `, x − ¯xi
= hh − `, xi − hh − `, ¯xi
= z − hh − `, ¯xi.
The last equality follows from Eq. (6). By definition
we have z ∼ Q(z), and since hh − `, ¯xi is a constant,
</bodyText>
<equation confidence="0.9179">
zcent = z − hh − `, ¯xi ∼ Q(z + hh − `, ¯xi).
</equation>
<bodyText confidence="0.9998115">
In other words, the shape of the distribution does not
change, but the mean is shifted to
</bodyText>
<equation confidence="0.848686333333333">
E[zcent] = E[z] − hh − `, ¯xi
= hh − `,µi − hh − `, ¯xi
= hh − `,µ − ¯xi,
</equation>
<bodyText confidence="0.999282333333333">
where E[z] is given by Eq. (7). If the sample mean
x¯ is close enough to the true mean µ, i.e., x¯ ≈ µ, we
have an approximation
</bodyText>
<equation confidence="0.997464">
E[zcent] = hh − `, µ − ¯xi ≈ 0. (9)
</equation>
<bodyText confidence="0.9998314">
Thus, if the median and the mean of distribution
Q(z) are again not far apart, Eq. (9) suggests that
h − x¯ and ` − x¯ are about equally likely to be more
similar to x − ¯x; i.e., neither has a greater chance to
become a hub.
</bodyText>
<sectionHeader confidence="0.980342" genericHeader="method">
5 Hubs in multi-cluster data
</sectionHeader>
<bodyText confidence="0.9999359">
In this section, we discuss emergence of hubs when
the data consists of multiple clusters. In fact, the
analysis of Section 4 is distribution-free, and thus
also applies to the case of multi-modal P(x). How-
ever, one might still argue that objects similar to the
data centroid should hardly occur in that case. Us-
ing both synthetic and real datasets, we demonstrate
below that even in multi-cluster data, objects that
are only slightly more similar to the data mean (cen-
troid) may emerge as hubs.
</bodyText>
<subsectionHeader confidence="0.8445535">
5.1 Synthetic data
5.1.1 Data generation
</subsectionHeader>
<bodyText confidence="0.999780666666667">
We generated a high-dimensional multi-cluster
dataset by modeling it as a mixture of ten von Mises-
Fisher distributions (Mardia and Jupp, 2000) in
</bodyText>
<page confidence="0.984941">
616
</page>
<figure confidence="0.999737492753623">
Object ID
Frequency
0.450 50 100 150
N10
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
0 50 100 150
N10
(d) After centering: N10 vs. inner
product similarity to the data cen-
troid
1000
45
800
35
600
25
400
15
200
200 400 600 800 1000
Object ID
(b) Before centering: kNN matrix
150
100
50
0
50
Object ID
(c) Before centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
Similarity with centroid 0.6
0.55
0.5
Similarity with centroid 0.1
0.05
0
−0.05
−0.1
50
40
30
20
10 100
5
150
200 400 600 800 1000
Object ID Object ID
(e) After centering: kNN matrix (f) After centering: Breakdown of
N10 by cluster match/mismatch
between objects and neighbors
1000 50 150
45
800 40 100
35
50
600 30
25 0
400 20
50
15
200 10 100
5
200 400 600 800 1000 150
200 400 600 800 1000
Object ID
Frequency
</figure>
<figureCaption confidence="0.991427">
Figure 1: 300-dimensional synthetic data. (a), (d): scatter plot of the N10 value of objects and their similarity to
centroid. (b), (e): kNN matrices. The points are colored according to the N10 value of object x; warmer colors indicate
higher N10 values. (c), (f): the number of times (y-axis) an object (whose ID is on the x-axis) appears in the 10 nearest
neighbors of objects of the same cluster (black bars), and those of different clusters (magenta).
</figureCaption>
<bodyText confidence="0.999917928571428">
R300. The von Mises-Fisher distribution is a distri-
bution of unit vectors (it can roughly be thought of
as a normal distribution on a unit hypersphere), so
for objects (feature vectors) sampled from this dis-
tribution, inner product reduces to cosine similarity.
We sampled1 100 objects from each of the ten dis-
tributions (clusters), and made a dataset of 1,000 ob-
jects in total.
The von Mises-Fisher distribution has two param-
eters, the mean direction vector µ, and the concen-
tration parameter K characterizing how strongly the
population is concentrated around the direction µ.
We set K = 500 for all ten distributions, but the mean
directions µ were made distinct; all mean direction
</bodyText>
<footnote confidence="0.913122333333333">
1We used the random sampling code available at http:
//people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html
(Banerjee et al., 2005).
</footnote>
<bodyText confidence="0.999933071428572">
vectors had 30 components set to 0.5 while the re-
maining 270 components were set to 1, but the 30
components with value 0.5 were chosen to be dis-
tinct among the ten clusters. This configuration as-
sures that all ten mean directions have the same an-
gle from the all-ones vector [1, ... , 1]T, which is the
direction of the mean of the entire data distribution.
Note that even though all sampled objects reside
on the surface of the unit hypersphere, the data cen-
troid lies not on the surface but inside the hyper-
sphere. And after centering, the length of the fea-
ture vectors may vary from one another, but we do
not normalize these vectors; i.e., object similarity is
measured by raw inner product, not by cosine.
</bodyText>
<page confidence="0.987738">
617
</page>
<subsubsectionHeader confidence="0.596789">
5.1.2 Correlation between hubness and
</subsubsectionHeader>
<bodyText confidence="0.97847575">
centroid similarity
The scatter plot in Figure 1(a) shows the correla-
tion between the degree of hubness (N10) of an ob-
ject and its inner product similarity to the data cen-
troid. The N10 value of an object is defined as the
number of times the object appears in the 10 nearest
neighbors of other objects in the dataset. It was used
in (Radovanovi´c et al., 2010a) to measure the degree
of hubness of individual objects.
The plot clearly shows that the hub objects (i.e.,
those with high N10) consist of objects that are simi-
lar to the centroid. Figure 1(d) shows the scatter plot
after the data is centered, created in the same way
as Figure 1(a). The similarity to the centroid is uni-
formly 0 as a result of centering, and no objects have
an N10 value greater than 33.
</bodyText>
<sectionHeader confidence="0.5105675" genericHeader="method">
5.1.3 Influence of hubs on objects in different
clusters
</sectionHeader>
<bodyText confidence="0.999591416666667">
The kNN matrix of Figure 1(b) depicts the kNN
relations with k = 10 among objects before center-
ing. In this matrix, both the x- and y- axes represent
the ID of the objects. If object x is in the 10 nearest
neighbors of object y, a point is plotted at coordi-
nates (x, y). As a result, there are exactly k = 10
points in each row. The color of points indicates the
degree of hubness of object x; warmer color repre-
sents higher N10 value of the object.
In this matrix, object IDs are sorted by the clus-
ter the objects belong to. Hence in the ideal case in
which the k nearest neighbors of every object consist
genuinely of objects from the same cluster, only the
diagonal blocks would be colored, and off-diagonal
areas would be left blank.
As Figure 1(b) shows, the actual situation is far
from ideal, even though ten diagonal blocks are still
identifiable. The presence of many warm colored
vertical lines suggests that many hub objects appear
in the 10 nearest neighbors of other objects that are
not in the same cluster as the hubs. Thus these hubs
may have a strong influence on the kNN prediction
of other objects.
Figure 1(e) shows the kNN matrix after centering.
The warm colored lines have disappeared, and the
diagonal blocks are now more visible.
The bar graphs of Figures 1(c) and (f) plot the N10
value of each object (whose ID is on the x-axis). Re-
call that N10 is the number of times an object appears
in the 10 nearest neighbors of other objects. The
bar for each object is broken down by whether the
object and its neighbors belong to the same cluster
(black bar) or in different clusters (magenta bar). In
terms of kNN classification, having a large number
of nearest neighbors with the same class improves
the classification performance, so longer black bars
and shorter magenta bars are more desirable.
Before centering (Figure 1(c)), hub objects with
large N10 values are similar not only to objects be-
longing to the same cluster (as indicated by black
bars), but also to objects belonging to different clus-
ters (magenta bars). After centering (Figure 1(f)),
the number of tall magenta bars decreases.
Before centering, 22.7% of the 10 nearest neigh-
bors of an object have the same class label as the
object (as indicated by the ratio of the total height of
black bars relative to that of all bars in Figure 1(c)).
After centering, the percentage increases to 31.6%.
</bodyText>
<subsectionHeader confidence="0.99579">
5.2 Real dataset
</subsectionHeader>
<bodyText confidence="0.999986">
We did the same analysis as Sections 5.1.2–5.1.3
to a real dataset with multiple-cluster structure: the
Reuters Transcribed dataset. This multi-class docu-
ment classification dataset has ten classes, and each
class roughly forms a cluster. We will also use this
dataset in an experiment in Section 7.2.
The results are shown in Figure 2. We can ob-
serve the same trends as we saw in Figure 1 for the
synthetic data: positive correlation between hubness
(N10) and inner product with the data centroid be-
fore centering; hubs appearing in the nearest neigh-
bors of many objects of different classes; and both
are reduced after centering.
The ratio of the height of black bars to that of
all bars in Figure 2(c) is 38.4% before centering,
whereas it improves to 41.0% after centering (Fig-
ure 2(f)).
</bodyText>
<sectionHeader confidence="0.980201" genericHeader="method">
6 Hubness weighted centering
</sectionHeader>
<bodyText confidence="0.9985155">
Centering shifts the origin of the space to the data
centroid, and objects similar to the centroid tend to
become hubs. Thus in a sense, centering can be
interpreted as an operation that shifts the origin to-
wards hubs.
In this section, we extrapolate this interpretation,
</bodyText>
<page confidence="0.982535">
618
</page>
<figure confidence="0.99680253968254">
30
25
20
15
10
5
200
150
100
50
Object ID
Frequency
Similarity with centroid
40
Object ID
(b) Before centering: kNN matrix
40
20
0
20
50 100 150 200
Object ID
(c) Before centering: Breakdown of
N10 by class match/mismatch be-
tween objects and neighbors
0.06
0.05
0.04
0.03
0.02
0.01
00 10 20 30 40 50
N10
(a) Before centering: N10 vs. inner
product similarity to the data cen-
troid
50 100 150 200
200 30 40
25
150 20
20
100 15 0
10
50 20
5
40
50 100 150 200
50 100 150 200
Object ID Object ID
Object ID
Frequency
0.03
0.02
0.01
0
−0.01
−0.02
−0.030 10 20 30 40 50
N10
Similarity with centroid
(d) After centering: N10 vs. inner (e) After centering: kNN matrix (f) After centering: Breakdown of N10
product similarity to the data cen- by class match/mismatch between
troid objects and neighbors
</figure>
<figureCaption confidence="0.99956">
Figure 2: Reuters Transcribed data.
</figureCaption>
<bodyText confidence="0.9968177">
and move the origin more actively towards hub ob-
jects in the dataset, rather than towards the data cen-
troid. To this end, we consider weighted centering,
a variation of centering in which each object is asso-
ciated with a weight, and the origin is shifted to the
weighted mean of the data. Specifically, we define
the weight of an object as the sum of the similarities
(inner products) between the object and all objects,
regarding this sum as the index of how likely the ob-
ject can be a hub.
</bodyText>
<subsectionHeader confidence="0.995538">
6.1 Weighted centering
</subsectionHeader>
<bodyText confidence="0.999901333333333">
In weighted centering, we associate weight wi to
each object i in the dataset, and move the origin to
the weighted centroid
</bodyText>
<equation confidence="0.977465333333333">
n
¯xweighted = wixi
i=1
</equation>
<bodyText confidence="0.979317">
where Eni=1 wi = 1 and 0 &lt;— wi &lt;— 1 for i = 1,... , n.
Thus, object x is mapped to a new feature vector
</bodyText>
<equation confidence="0.990328">
n
xweighted = x − ¯xweighted = x − wixi.
i=1
</equation>
<bodyText confidence="0.9978485">
Notice that the original centering formula (2) is re-
covered by letting wi = 1/n for all i = 1, ... , n.
Weighted centering can also be kernelized by us-
ing the weighted centering matrix H(w) = I − 1wT
in place of H in Eq. (3). The resulting Gram matrix
is
</bodyText>
<equation confidence="0.775336">
Kweighted = H(w)KH(w)T. (10)
</equation>
<subsectionHeader confidence="0.996805">
6.2 Similarity-dependent weighting
</subsectionHeader>
<bodyText confidence="0.999881">
To move the origin towards hubs more aggressively,
we place more weights on objects that are more
likely to become hubs. This likelihood is estimated
by the similarity of individual objects to all objects
in the data set.
</bodyText>
<page confidence="0.998186">
619
</page>
<bodyText confidence="0.992657333333333">
Let di be the sum of the similarity between object
xi and all objects in the dataset. So,
As seen from the last equation, di is proportional to
the similarity (inner product) between object xi and
the data centroid.
Now we define {wi}ni=1 from {di}ni=1 by
</bodyText>
<equation confidence="0.9831712">
dγ
i
Enγ,
�i=1
j
</equation>
<bodyText confidence="0.9999495">
where γ is a parameter controlling how much we
emphasize the effect of di. Setting γ = 0 results in
wi = 1 for every i, and hence is equivalent to normal
centering. When γ &gt; 0, weighted centering moves
the origin closer to the objects with a large di than
normal centering would.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999951434782609">
We evaluated the effect of centering in two natural
language tasks: word sense disambiguation (WSD)
and document classification. We are interested in
whether hubs are actually reduced after centering,
and whether the performance of kNN classification
is improved.
Throughout this section, K denotes cosine simi-
larity matrix; i.e., inner product of feature vectors
normalized to unit length; Kcent denotes the cen-
tered similarity matrix computed by Eq. (3) from K;
Kweignted denotes its hubness weighted variant given
by Eq. (10). Depending on context, these symbols
are also used to denote kNN classifiers using respec-
tive similarity measures.
For comparison, we also tested two recently pro-
posed approaches to hub reduction: transformation
of the base similarity measure (in our case, K) by
Mutual Proximity (Schnitzer et al., 2012)2, and the
one (Suzuki et al., 2012) based on graph Laplacian
kernels. Since the Laplacian kernels are defined for
graph nodes, we computed them by taking the co-
sine similarity matrix K as the weighted adjacency
(affinity) matrix of a graph. For Laplacian kernels,
</bodyText>
<footnote confidence="0.820316">
2We used the Matlab script downloaded from http://www.
ofai.at/∼dominik.schnitzer/mp/.
</footnote>
<bodyText confidence="0.9995912">
we computed both the regularized Laplacian ker-
nel (Chebotarev and Shamis, 1997; Smola and Kon-
dor, 2003) with several parameter values, as well as
the commute-time kernel (Saerens et al., 2004), but
present only the best results among these kernels.
</bodyText>
<subsectionHeader confidence="0.997558">
7.1 Word sense disambiguation
7.1.1 Task and dataset
</subsectionHeader>
<bodyText confidence="0.999987142857143">
In the WSD experiment, we used the dataset for
the Senseval-3 English Lexical Sample (ELS) task
(Mihalcea et al., 2004). It is a collection of sen-
tences containing 57 polysemous words, and each
of these sentences is annotated with a gold standard
sense of the target word. The goal of the ELS task
is to build a classifier for each target word, which,
given a context around the word, predicts a sense
from the known set of senses.
We used a basic bag-of-words representation for
the context surrounding a target word (Mihalcea,
2004; Navigli, 2009). A context is thus represented
as a high-dimensional feature vector holding the tf-
idf weighted frequency of words3 in context.
</bodyText>
<sectionHeader confidence="0.409369" genericHeader="method">
7.1.2 Compared methods
</sectionHeader>
<bodyText confidence="0.999991764705882">
We applied kNN classification using cosine sim-
ilarity K, and its four transformed similarity mea-
sures: centered similarity Kcent, its weighted vari-
ant Kweignted, Mutual Proximity and graph Laplacian
kernels. The sense of a test object was predicted by
voting from the k training objects most similar to the
test object, as measured by the respective similarity
measures.
We used leave-one-out cross validation within the
training data to tune neighborhood size k for the
kNN classification and the voting scheme, i.e., ei-
ther (unweighted) majority vote, or weighted vote in
which votes from individual objects are weighted by
their similarity score to the test objects. We also se-
lected parameter γ in Kweignted and the best graph
Laplacian kernel among the regularized Laplacian
and commute time kernels using the training data.
</bodyText>
<subsectionHeader confidence="0.839123">
7.1.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.9967855">
We computed two indices for each similarity mea-
sure: (i) skewness of the N10 distribution to evaluate
</bodyText>
<footnote confidence="0.974451">
3We removed stop words listed in the on-line appendix of
(Lewis et al., 2004).
</footnote>
<equation confidence="0.999089">
(xi, xj) = n (xi,
n
Z
j=1
di =
1
n
xj).
n
Z
j=1
wi =
</equation>
<page confidence="0.990167">
620
</page>
<table confidence="0.898260875">
Dataset #classes #objects #features
Reuters Transcribed 10 201 2730
Mini Newsgroups 20 2000 8811
1.00 Table 2: Document classification datasets: Number of
4.51 classes, data size, and number of features.
—
Method F1 score Skewness
K 60.3
Kcent 64.0
Kweighted 64.8
Mutual Proximity 63.0
Graph Laplacian 61.2
GAMBL (Decadt et al., 2004) 64.5
4.55
1.19
1.02
</table>
<tableCaption confidence="0.8515305">
Table 1: WSD results: Macro-averaged F1 score (points)
of the compared methods (larger is better) and empirical
skewness of the N10 distribution for each similarity mea-
sure (smaller is better).
</tableCaption>
<bodyText confidence="0.999021777777778">
the emergence of hubs, and (ii) macro-averaged F1
score to evaluate the classification performance.
Skewness To evaluate the degree of hub emer-
gence for each similarity measure, we followed
(Radovanovi´c et al., 2010a) and counted Nk(x), the
number of times object x occurs in the kNN lists
of other objects in the dataset (we fix k = 10 be-
low). The emergence of hubs in a dataset can then
be quantified with skewness, defined as follows:
</bodyText>
<equation confidence="0.819068166666666">
�3]
E [(Nk − µNk
.
3
σ
Nk
</equation>
<bodyText confidence="0.998091">
In this equation, E[ · ] denotes expectation, and µNk
and σNk are the mean and the standard deviation of
the Nk distribution, respectively.
When hubs exist in a dataset, the distribution of
Nk is expected to skew to the right, and yields a large
SNk (Radovanovi´c et al., 2010a). In other words,
similarity measures that yield smaller SNk are more
desirable in terms of hub reduction.
Skewness can only be computed for each dataset,
and in the WSD task, each target word has its own
dataset. Hence we computed the skewness SN10 for
each word and then took average.
Macro-averaged F1 score Classification perfor-
mance was measured by the F1 score macro-
averaged over all the 57 target words in the Senseval-
3 ELS dataset. The standard Senseval-3 ELS scor-
ing method is based on micro average, but we used
macro average to make the evaluation consistent
with skewness computation, which, as mentioned
above, can only be computed for each dataset (i.e.,
word).
</bodyText>
<sectionHeader confidence="0.631499" genericHeader="method">
7.1.4 Result
</sectionHeader>
<bodyText confidence="0.999926444444444">
Table 1 shows the F1 scores and the skewness of
the N10 distributions, macro averaged over the 57
target words. The table also includes the macro-
averaged F1 score4 of the GAMBL system, the best
memory-based system participated in the Senseval-
3 ELS task. Note however that GAMBL uses more
elaborate features (e.g., part-of-speech of words)
than just a plain bag-of-words used by other methods
in this comparison. GAMBL also employs complex
post-processing of the kNN outputs.
After centering (Kcent and Kweighted) skewness
became markedly smaller than that of the non-
centered cosine K. F1 score also improved with the
decrease in skewness. In particular, weighted cen-
tering (Kweighted) slightly outperformed GAMBL,
though the difference was small. Recall however
that Kcent and Kweighted only use naive bag-of-words
features, unlike GAMBL.
</bodyText>
<subsectionHeader confidence="0.986792">
7.2 Document classification
7.2.1 Task and dataset
</subsectionHeader>
<bodyText confidence="0.999752">
Two multiclass document classification datasets
were used: Reuters Transcribed and Mini News-
groups, distributed at http://archive.ics.uci.edu/ml/.
The properties of the datasets are summarized in Ta-
ble 2.
</bodyText>
<subsectionHeader confidence="0.542463">
7.2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99996525">
The performance was evaluated by the F1 score
(equivalent to accuracy in this task) of prediction us-
ing leave-one-out cross validation, due to the limited
number of documents.
</bodyText>
<sectionHeader confidence="0.681125" genericHeader="method">
7.2.3 Compared methods
</sectionHeader>
<bodyText confidence="0.999809666666667">
We used the cosine similarity as the base sim-
ilarity matrix (K). The centered similarity matrix
(Kcent) and its weighted variant (Kweighted), Mutual
</bodyText>
<footnote confidence="0.798432333333333">
4The macro-averaged F1 of GAMBL was calculated from
the per-word F1 scores listed in Table 1 of (Decadt et al., 2004).
SNk =
</footnote>
<page confidence="0.935427">
621
</page>
<figure confidence="0.972400666666667">
Method F1 score Skewness
(a) Reuters Transcribed
Method F1 score Skewness
K
Kcent
Kweighted
Mutual Proximity
Graph Laplacian
(b) Mini Newsgroups
</figure>
<tableCaption confidence="0.848118333333333">
Table 3: Document classification results: F1 score (%)
(larger is better) and skewness of the N10 distribution for
each similarity measure (smaller is better).
</tableCaption>
<bodyText confidence="0.999097857142857">
to reduce hubs.
In WSD and document classification tasks, kNN
classifiers showed much better performance with
centered similarity measures than non-centered
ones. Weighted centering shifts the origin towards
hubs more aggressively, and further improved the
classification performance in some cases.
In future work, we plan to exploit the class distri-
bution in the dataset to make more effective similar-
ity measures; notice that the hubness weighted cen-
tering of Section 6 is an unsupervised method, in the
sense that class information was not used for deter-
mining weights. We will investigate if more effec-
tive weighting can be done using this information.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9887215">
We thank anonymous reviewers for helpful com-
ments.
</bodyText>
<figure confidence="0.9988404">
K
Kcent
Kweighted
Mutual Proximity
Graph Laplacian
56.7 1.61
61.2 0.11
60.2 0.04
60.2 −0.10
57.2 0.37
76.5 4.37
79.0 1.56
79.4 1.68
79.0 0.49
77.6 2.13
</figure>
<bodyText confidence="0.9935812">
Proximity, and graph Laplacian based kernels were
computed from K.
kNN classification was done in a standard way:
The class of object x is predicted by the majority
vote from k = 10 objects most similar to x, mea-
sured by a specified similarity measure. The param-
eter k for the kNN classification, the voting scheme
(i.e., either unweighted or weighted majority vote),
y in Kweighted, and the best graph Laplacian kernel
were selected by leave-one-out cross validation.
</bodyText>
<sectionHeader confidence="0.82191" genericHeader="method">
7.2.4 Result
</sectionHeader>
<bodyText confidence="0.999939">
Table 3 shows the F1 score and the skewness of
the N10 distribution of the respective methods in
document classification. Centered cosine (Kcent)
outperformed uncentered cosine similarity K, and
achieved an F1 score comparable to Mutual Proxim-
ity. Weighted centering (Kweighted) further improved
F1 on the Mini Newsgroups data.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999995">
We have shown that centering similarity matrices re-
duces the emergence of hubs in the data, and conse-
quently improves the accuracy of nearest neighbor
classification. We have theoretically analyzed why
objects most similar to the mean tend to make hubs,
and also proved that centering cancels the bias in the
distribution of inner products, and thus is expected
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999486766666667">
Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,
and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von Mises-Fisher distributions. Journal
of Machine Learning Research, 6:1345–1382.
P. Yu. Chebotarev and E. V. Shamis. 1997. The matrix-
forest theorem and measuring relations in small social
groups. Automation and Remote Control, 58(9):1505–
1514.
Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi,
and Luca Cazzanti. 2009. Similarity-based classifi-
cation: Concepts and algorithms. Journal of Machine
Learning Research, 10:747–776.
Bart Decadt, V´eronique Hoste, Walter Daelemans, and
Antal Van den Bosch. 2004. GAMBL, genetic algo-
rithm optimization of memory-based WSD. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
the 3rd International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text (Senseval-
3), pages 108–112.
L. Eriksson, E. Johansson, N. Kettaneh-Wold, J. Trygg,
C. Wikstr¨om, and S. Wold. 2006. Multi- and
Megavariate Data Analysis, Part 1, Basic Principles
and Applications. Umetrics, Inc.
Katrin Erk and Sebastian Pad´o. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ’08),
pages 897–906, Honolulu, Hawaii, USA.
Douglas H. Fisher and Hans-Joachim Lenz, editors.
1996. Learning from Data: Artificial Intelligence and
</reference>
<page confidence="0.97891">
622
</page>
<reference confidence="0.998582122641509">
Statistics V: Workshop on Artificial Intelligence and
Statistics. Lecture Notes in Statistics 112. Springer.
Ruixin Guo and Sounak Chakraborty. 2010. Bayesian
adaptive nearest neighbor. Statistical Analysis and
Data Mining, 3(2):92–105.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: anew benchmark collection for text
categorization research. Journal of Machine Learning
Research, 5:361–397.
Colin Mallows. 1991. Another comment on O’Cinneide.
The American Statistician, 45(3):257.
K. V. Mardia and P. Jupp. 2000. Directional Statistics.
John Wiley and Sons, 2nd edition.
K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multi-
variate Analysis. Academic Press.
Brij M. Masand, Gordon Linoff, and David L. Waltz.
1992. Classifying news stories using memory based
reasoning. In Proceedings of the 15th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ’92), pages
59–65.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Rada Mihalcea and Phil Edmonds, editors,
Proceedings of the 3rd International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3), pages 25–28, Barcelona, Spain.
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Hwee Tou Ng and
Ellen Riloff, editors, Proceedings of the 8th Confer-
ence on Computational Natural Language Learning
(CoNLL ’04), pages 33–40, Boston, Massachusetts,
USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association of Compu-
tational Linguistics: Human Language Technologies
(ACL ’08), pages 236–244, Columbus, Ohio, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41:10:1–10:69.
Ali Mustafa Qamar, ´Eric Gaussier, Jean-Pierre Cheval-
let, and Joo-Hwee Lim. 2008. Similarity learning for
nearest neighbor classification. In Proceedings of the
8th International Conference on Data Mining (ICDM
’08), pages 983–988, Pisa, Italy.
Milo&amp;quot;s Radovanovi´c, Alexandros Nanopoulos, and Mir-
jana Ivanovi´c. 2010a. Hubs in space: Popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11:2487–2531.
Milo&amp;quot;s Radovanovi´c, Alexandros Nanopoulos, and Mir-
jana Ivanovi´c. 2010b. On the existence of obstinate
results in vector space models. In Proceedings of the
33rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ’10), pages 186–193, Geneva, Switzerland.
Marco Saerens, Franc¸ois Fouss, Luh Yen, and Pierr
Dupont. 2004. The principal components analysis
of graph, and its relationships to spectral clustering.
In Proceedings of the 15th European Conference on
Machine Learning (ECML ’04), Lecture Notes in Ar-
tificial Intelligence 3201, pages 371–383, Pisa, Italy.
Springer.
Dominik Schnitzer, Arthur Flexer, Markus Schedl, and
Gerhard Widmer. 2012. Local and global scaling re-
duce hubs in space. Journal of Machine Learning Re-
search, 13:2871–2902.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24:97–123.
Alexander J. Smola and Risi Kondor. 2003. Kernels and
regularization on graphs. In Learning Theory and Ker-
nel Machines: 16th Annual Conference on Learning
Theory and 7th Kernel Workshop, Proceedings, Lec-
ture Notes in Artificial Intelligence 2777, pages 144–
158. Springer.
Anders Søgaard. 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL ’11), pages 48–
52, Portland, Oregon, USA.
Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Mat-
sumoto, and Marco Saerens. 2012. Investigating the
effectiveness of Laplacian-based kernels in hub reduc-
tion. In Proceedings of the 26th AAAI Conference on
Artificial Intelligence (AAAI-12), pages 1112–1118,
Toronto, Ontario, Canada.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL ’10), pages 948–957,
Uppsala, Sweden.
Jigang Wang, Predrag Neskovic, and Leon N. Cooper.
2006. Neighborhood size selection in the k-nearest-
neighbor rule using statistical confidence. Pattern
Recognition, 39(3):417–423.
Kilian Q. Weinberger and Lawrence K. Saul. 2009. Dis-
tance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research,
10:207–244.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR ’99), pages 42–49, Berkeley, California, USA.
</reference>
<page confidence="0.999142">
623
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497047">
<title confidence="0.999848">Centering Similarity Measures to Reduce Hubs</title>
<author confidence="0.996297">Ikumi Suzuki Kazuo Hara Masashi Shimbo</author>
<affiliation confidence="0.999972">National Institute of Genetics National Institute of Genetics Nara Institute of Science and Technology</affiliation>
<address confidence="0.979086">Mishima, Shizuoka, Japan Mishima, Shizuoka, Japan Ikoma, Nara, Japan</address>
<email confidence="0.966191">suzuki.ikumi@gmail.comkazuo.hara@gmail.comshimbo@is.naist.jp</email>
<author confidence="0.993856">Marco Saerens</author>
<affiliation confidence="0.7880745">Universit´e catholique de Louvain Louvain-la-Neuve, Belgium</affiliation>
<email confidence="0.956176">marco.saerens@uclouvain.be</email>
<abstract confidence="0.998256263157895">The performance of nearest neighbor methods degraded by the presence of i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the method of the transformation that shifts the origin of the space to the centroid, provides an way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental show that (weighted) centering is tive for natural language data; it improves the of the neighbor classifiers considerably in word sense disambiguation and document classification tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arindam Banerjee</author>
<author>Inderjit S Dhillon</author>
<author>Joydeep Ghosh</author>
<author>Suvrit Sra</author>
</authors>
<title>Clustering on the unit hypersphere using von Mises-Fisher distributions.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1345</pages>
<contexts>
<context position="16766" citStr="Banerjee et al., 2005" startWordPosition="2972" endWordPosition="2975">this distribution, inner product reduces to cosine similarity. We sampled1 100 objects from each of the ten distributions (clusters), and made a dataset of 1,000 objects in total. The von Mises-Fisher distribution has two parameters, the mean direction vector µ, and the concentration parameter K characterizing how strongly the population is concentrated around the direction µ. We set K = 500 for all ten distributions, but the mean directions µ were made distinct; all mean direction 1We used the random sampling code available at http: //people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html (Banerjee et al., 2005). vectors had 30 components set to 0.5 while the remaining 270 components were set to 1, but the 30 components with value 0.5 were chosen to be distinct among the ten clusters. This configuration assures that all ten mean directions have the same angle from the all-ones vector [1, ... , 1]T, which is the direction of the mean of the entire data distribution. Note that even though all sampled objects reside on the surface of the unit hypersphere, the data centroid lies not on the surface but inside the hypersphere. And after centering, the length of the feature vectors may vary from one another</context>
</contexts>
<marker>Banerjee, Dhillon, Ghosh, Sra, 2005</marker>
<rawString>Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. 2005. Clustering on the unit hypersphere using von Mises-Fisher distributions. Journal of Machine Learning Research, 6:1345–1382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chebotarev</author>
<author>E V Shamis</author>
</authors>
<title>The matrixforest theorem and measuring relations in small social groups. Automation and Remote Control,</title>
<date>1997</date>
<volume>58</volume>
<issue>9</issue>
<pages>1514</pages>
<contexts>
<context position="6546" citStr="Chebotarev and Shamis, 1997" startWordPosition="1028" endWordPosition="1031"> data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. Section 4 presents a theoretical justification for using centering to reduce hubs, but this kind of analysis is missing for the Laplacian kernels. Centering is easier to compute as well. For a dataset of n objects, it takes O(n2) time to compute, whereas computing a Laplacian-based kernel requires O(n3) time for matrix inversion. Mu</context>
<context position="25861" citStr="Chebotarev and Shamis, 1997" startWordPosition="4592" endWordPosition="4595">asures. For comparison, we also tested two recently proposed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012)2, and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around t</context>
</contexts>
<marker>Chebotarev, Shamis, 1997</marker>
<rawString>P. Yu. Chebotarev and E. V. Shamis. 1997. The matrixforest theorem and measuring relations in small social groups. Automation and Remote Control, 58(9):1505– 1514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihua Chen</author>
<author>Eric K Garcia</author>
<author>Maya R Gupta</author>
<author>Ali Rahimi</author>
<author>Luca Cazzanti</author>
</authors>
<title>Similarity-based classification: Concepts and algorithms.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--747</pages>
<contexts>
<context position="6075" citStr="Chen et al., 2009" startWordPosition="956" endWordPosition="959">he use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we eva</context>
</contexts>
<marker>Chen, Garcia, Gupta, Rahimi, Cazzanti, 2009</marker>
<rawString>Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, and Luca Cazzanti. 2009. Similarity-based classification: Concepts and algorithms. Journal of Machine Learning Research, 10:747–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Decadt</author>
<author>V´eronique Hoste</author>
<author>Walter Daelemans</author>
<author>Antal Van den Bosch</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval3),</booktitle>
<pages>108--112</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<marker>Decadt, Hoste, Daelemans, Van den Bosch, 2004</marker>
<rawString>Bart Decadt, V´eronique Hoste, Walter Daelemans, and Antal Van den Bosch. 2004. GAMBL, genetic algorithm optimization of memory-based WSD. In Rada Mihalcea and Phil Edmonds, editors, Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval3), pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Eriksson</author>
<author>E Johansson</author>
<author>N Kettaneh-Wold</author>
<author>J Trygg</author>
<author>C Wikstr¨om</author>
<author>S Wold</author>
</authors>
<title>Multi- and Megavariate Data Analysis, Part 1, Basic Principles and Applications.</title>
<date>2006</date>
<publisher>Umetrics, Inc.</publisher>
<marker>Eriksson, Johansson, Kettaneh-Wold, Trygg, Wikstr¨om, Wold, 2006</marker>
<rawString>L. Eriksson, E. Johansson, N. Kettaneh-Wold, J. Trygg, C. Wikstr¨om, and S. Wold. 2006. Multi- and Megavariate Data Analysis, Part 1, Basic Principles and Applications. Umetrics, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08),</booktitle>
<pages>897--906</pages>
<location>Honolulu, Hawaii, USA.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 897–906, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas H Fisher</author>
<author>Hans-Joachim Lenz</author>
<author>editors</author>
</authors>
<date>1996</date>
<booktitle>Learning from Data: Artificial Intelligence and Statistics V: Workshop on Artificial Intelligence and Statistics. Lecture Notes in Statistics 112.</booktitle>
<publisher>Springer.</publisher>
<marker>Fisher, Lenz, editors, 1996</marker>
<rawString>Douglas H. Fisher and Hans-Joachim Lenz, editors. 1996. Learning from Data: Artificial Intelligence and Statistics V: Workshop on Artificial Intelligence and Statistics. Lecture Notes in Statistics 112. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruixin Guo</author>
<author>Sounak Chakraborty</author>
</authors>
<title>Bayesian adaptive nearest neighbor.</title>
<date>2010</date>
<journal>Statistical Analysis and Data Mining,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="6155" citStr="Guo and Chakraborty, 2010" startWordPosition="969" endWordPosition="972">y are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demon</context>
</contexts>
<marker>Guo, Chakraborty, 2010</marker>
<rawString>Ruixin Guo and Sounak Chakraborty. 2010. Bayesian adaptive nearest neighbor. Statistical Analysis and Data Mining, 3(2):92–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2008</date>
<publisher>Prentice Hall,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="1847" citStr="Jurafsky and Martin, 2008" startWordPosition="272" endWordPosition="276">on tasks. 1 Introduction 1.1 Background The k-nearest neighbor (kNN) algorithm is a simple nonparametric method of classification. It has been applied to various natural language processing (NLP) tasks such as document classification (Masand et al., 1992; Yang and Liu, 1999), partof-speech tagging (Søgaard, 2011), and word sense disambiguation (Navigli, 2009). To apply the kNN algorithm, data is typically represented as a vector object in a feature space, and (dis)similarity between data is measured by the distance between the vectors, their inner product, or cosine of the angle between them (Jurafsky and Martin, 2008). With such a (dis)similarity measure, the Kenji Fukumizu The Institute of Statistical Mathematics Tachikawa, Tokyo, Japan fukumizu@ism.ac.jp unknown class label of a test object is predicted by a majority vote of the classes of its k most similar objects in the labeled training set. Recent studies (Radovanovi´c et al., 2010a; Radovanovi´c et al., 2010b) have shown that if the feature space is high-dimensional, some objects in the dataset emerge as hubs; i.e., these objects frequently appear in the k nearest neighbors of other objects. The emergence of hubs may deteriorate the performance of k</context>
<context position="3199" citStr="Jurafsky and Martin, 2008" startWordPosition="496" endWordPosition="500">e to be a kNN of many test objects. Because the class of a test object is predicted by a majority vote from its k nearest neighbors, prediction is biased toward the labels of the hubs. • In information retrieval, nearest neighbor search finds objects in the database that are most relevant, or similar, to user-provided queries. If particular objects, such as hubs, are nearly always returned for any query, the retrieved results are probably not very useful. These drawbacks may hinder application of nearest neighbor methods in NLP, as typical natural language data are extremely high-dimensional (Jurafsky and Martin, 2008) and thus prone to produce hubs. 1.2 Contributions Centering (Mardia et al., 1979; Fisher and Lenz, 1996; Eriksson et al., 2006) is a standard technique 613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613–623, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics for removing observation bias in the data. It is a transformation of feature space in a way that the origin of the space is moved to the data centroid (sample mean). The distance between data objects is not changed by centering, but their inner </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: anew benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="27811" citStr="Lewis et al., 2004" startWordPosition="4910" endWordPosition="4913">on within the training data to tune neighborhood size k for the kNN classification and the voting scheme, i.e., either (unweighted) majority vote, or weighted vote in which votes from individual objects are weighted by their similarity score to the test objects. We also selected parameter γ in Kweignted and the best graph Laplacian kernel among the regularized Laplacian and commute time kernels using the training data. 7.1.3 Evaluation We computed two indices for each similarity measure: (i) skewness of the N10 distribution to evaluate 3We removed stop words listed in the on-line appendix of (Lewis et al., 2004). (xi, xj) = n (xi, n Z j=1 di = 1 n xj). n Z j=1 wi = 620 Dataset #classes #objects #features Reuters Transcribed 10 201 2730 Mini Newsgroups 20 2000 8811 1.00 Table 2: Document classification datasets: Number of 4.51 classes, data size, and number of features. — Method F1 score Skewness K 60.3 Kcent 64.0 Kweighted 64.8 Mutual Proximity 63.0 Graph Laplacian 61.2 GAMBL (Decadt et al., 2004) 64.5 4.55 1.19 1.02 Table 1: WSD results: Macro-averaged F1 score (points) of the compared methods (larger is better) and empirical skewness of the N10 distribution for each similarity measure (smaller is b</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: anew benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Mallows</author>
</authors>
<title>Another comment on O’Cinneide.</title>
<date>1991</date>
<journal>The American Statistician,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="12239" citStr="Mallows, 1991" startWordPosition="2108" endWordPosition="2109">the mean of the distribution Q(z) must be greater than 0. If the distribution Q(z) is symmetric, the median occurs at the same point as the mean, and the above claim holds. Indeed, if the components of x are generated independently from (possibly non-identical) normal distributions, we can show that Q(z) also obeys a normal distribution. Because it is a symmetric distribution, we can safely say that in this case, Eq. (8) holds with probability greater than 1/2. For a general non-symmetric distribution with a finite variance, the median is known to be within the standard deviation of the mean (Mallows, 1991), so we could still say that Eq. (8) is likely to hold if hh − `, µi is sufficiently large compared to the standard deviation. Now, if we let h be the object in a given dataset with the highest similarity (inner product) to the mean µ, and let ` be any other object in the set, then we see from the above discussion that h is likely to have higher similarity to x, a test sample drawn from distribution P(x). Because this holds for any ` in the dataset, the conclusion is that the objects in the dataset most similar to µ are likely to become hubs. 4.2 After centering Next let us investigate what ha</context>
</contexts>
<marker>Mallows, 1991</marker>
<rawString>Colin Mallows. 1991. Another comment on O’Cinneide. The American Statistician, 45(3):257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K V Mardia</author>
<author>P Jupp</author>
</authors>
<title>Directional Statistics.</title>
<date>2000</date>
<publisher>John Wiley</publisher>
<note>and Sons, 2nd edition.</note>
<contexts>
<context position="14640" citStr="Mardia and Jupp, 2000" startWordPosition="2601" endWordPosition="2604">ta consists of multiple clusters. In fact, the analysis of Section 4 is distribution-free, and thus also applies to the case of multi-modal P(x). However, one might still argue that objects similar to the data centroid should hardly occur in that case. Using both synthetic and real datasets, we demonstrate below that even in multi-cluster data, objects that are only slightly more similar to the data mean (centroid) may emerge as hubs. 5.1 Synthetic data 5.1.1 Data generation We generated a high-dimensional multi-cluster dataset by modeling it as a mixture of ten von MisesFisher distributions (Mardia and Jupp, 2000) in 616 Object ID Frequency 0.450 50 100 150 N10 (a) Before centering: N10 vs. inner product similarity to the data centroid 0 50 100 150 N10 (d) After centering: N10 vs. inner product similarity to the data centroid 1000 45 800 35 600 25 400 15 200 200 400 600 800 1000 Object ID (b) Before centering: kNN matrix 150 100 50 0 50 Object ID (c) Before centering: Breakdown of N10 by cluster match/mismatch between objects and neighbors Similarity with centroid 0.6 0.55 0.5 Similarity with centroid 0.1 0.05 0 −0.05 −0.1 50 40 30 20 10 100 5 150 200 400 600 800 1000 Object ID Object ID (e) After cent</context>
</contexts>
<marker>Mardia, Jupp, 2000</marker>
<rawString>K. V. Mardia and P. Jupp. 2000. Directional Statistics. John Wiley and Sons, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K V Mardia</author>
<author>J T Kent</author>
<author>J M Bibby</author>
</authors>
<title>Multivariate Analysis.</title>
<date>1979</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="3280" citStr="Mardia et al., 1979" startWordPosition="510" endWordPosition="513"> majority vote from its k nearest neighbors, prediction is biased toward the labels of the hubs. • In information retrieval, nearest neighbor search finds objects in the database that are most relevant, or similar, to user-provided queries. If particular objects, such as hubs, are nearly always returned for any query, the retrieved results are probably not very useful. These drawbacks may hinder application of nearest neighbor methods in NLP, as typical natural language data are extremely high-dimensional (Jurafsky and Martin, 2008) and thus prone to produce hubs. 1.2 Contributions Centering (Mardia et al., 1979; Fisher and Lenz, 1996; Eriksson et al., 2006) is a standard technique 613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 613–623, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics for removing observation bias in the data. It is a transformation of feature space in a way that the origin of the space is moved to the data centroid (sample mean). The distance between data objects is not changed by centering, but their inner product and cosine are affected; see Section 3 for detail. In this paper, we advo</context>
<context position="9095" citStr="Mardia et al., 1979" startWordPosition="1503" endWordPosition="1506">ntroid ¯x; see Eq. (2). And since we measure object similarity by inner product, shorter vectors tend to produce smaller similarity scores. Hence objects close to the data centroid become less similar to other objects after centering, and no longer be hubs. In Section 4, we analyze the effect of centering on hubness in more detail. 3.1 Centered Gram matrix Let I be an n × n identity matrix and ]L be an ndimensional all-ones vector. The symmetric matrix H = I−(1/n)]L]LT is called centering matrix, because the centered data matrix Xcent = [xcent 1 , ··· ,xcent n ] can be computed by Xcent = XH (Mardia et al., 1979). The Gram matrix Kcent of the centered feature vectors, whose (i, j) element holds the inner product hxcent i , xcent j i, can be calculated from the original Gram matrix K by (Xcent)T (Xcent) Kcent = = HXTXH = HKH. (3) Eq. (3) implies that the original data matrix X is not needed to compute the centered Gram matrix Kcent, provided that K is given. It is hence possible to use the so-called kernel trick; i.e., centering can be applied even if data matrix X is not available but the similarity of objects can be measured by a kernel function in an implicit feature space. 4 Theoretical analysis of</context>
</contexts>
<marker>Mardia, Kent, Bibby, 1979</marker>
<rawString>K. V. Mardia, J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brij M Masand</author>
<author>Gordon Linoff</author>
<author>David L Waltz</author>
</authors>
<title>Classifying news stories using memory based reasoning.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’92),</booktitle>
<pages>59--65</pages>
<contexts>
<context position="1475" citStr="Masand et al., 1992" startWordPosition="211" endWordPosition="214">robabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks. 1 Introduction 1.1 Background The k-nearest neighbor (kNN) algorithm is a simple nonparametric method of classification. It has been applied to various natural language processing (NLP) tasks such as document classification (Masand et al., 1992; Yang and Liu, 1999), partof-speech tagging (Søgaard, 2011), and word sense disambiguation (Navigli, 2009). To apply the kNN algorithm, data is typically represented as a vector object in a feature space, and (dis)similarity between data is measured by the distance between the vectors, their inner product, or cosine of the angle between them (Jurafsky and Martin, 2008). With such a (dis)similarity measure, the Kenji Fukumizu The Institute of Statistical Mathematics Tachikawa, Tokyo, Japan fukumizu@ism.ac.jp unknown class label of a test object is predicted by a majority vote of the classes of</context>
</contexts>
<marker>Masand, Linoff, Waltz, 1992</marker>
<rawString>Brij M. Masand, Gordon Linoff, and David L. Waltz. 1992. Classifying news stories using memory based reasoning. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’92), pages 59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>In Rada Mihalcea and Phil Edmonds, editors, Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<pages>25--28</pages>
<location>Barcelona, Spain.</location>
<contexts>
<context position="26204" citStr="Mihalcea et al., 2004" startWordPosition="4648" endWordPosition="4651">ing the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around the word, predicts a sense from the known set of senses. We used a basic bag-of-words representation for the context surrounding a target word (Mihalcea, 2004; Navigli, 2009). A context is thus represented as a high-dimensional feature vector holding the tfidf weighted frequency of words3 in context. 7.1.2 Compared methods We applied kNN clas</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Rada Mihalcea and Phil Edmonds, editors, Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), pages 25–28, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng and Ellen Riloff, editors, Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL ’04),</booktitle>
<pages>33--40</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="26618" citStr="Mihalcea, 2004" startWordPosition="4723" endWordPosition="4724">esults among these kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around the word, predicts a sense from the known set of senses. We used a basic bag-of-words representation for the context surrounding a target word (Mihalcea, 2004; Navigli, 2009). A context is thus represented as a high-dimensional feature vector holding the tfidf weighted frequency of words3 in context. 7.1.2 Compared methods We applied kNN classification using cosine similarity K, and its four transformed similarity measures: centered similarity Kcent, its weighted variant Kweignted, Mutual Proximity and graph Laplacian kernels. The sense of a test object was predicted by voting from the k training objects most similar to the test object, as measured by the respective similarity measures. We used leave-one-out cross validation within the training dat</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In Hwee Tou Ng and Ellen Riloff, editors, Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL ’04), pages 33–40, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL ’08),</booktitle>
<pages>236--244</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="5783" citStr="Mitchell and Lapata, 2008" startWordPosition="916" endWordPosition="919">guation and document classification tasks. 2 Related work Centering is a classical technique widely used in many fields of science. For instance, centering forms a preprocessing step in principal component analysis and Fisher linear discriminant analysis. In NLP, however, centering is seldom used; the use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (201</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL ’08), pages 236–244, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<pages>41--10</pages>
<contexts>
<context position="1582" citStr="Navigli, 2009" startWordPosition="228" endWordPosition="229">ugh weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks. 1 Introduction 1.1 Background The k-nearest neighbor (kNN) algorithm is a simple nonparametric method of classification. It has been applied to various natural language processing (NLP) tasks such as document classification (Masand et al., 1992; Yang and Liu, 1999), partof-speech tagging (Søgaard, 2011), and word sense disambiguation (Navigli, 2009). To apply the kNN algorithm, data is typically represented as a vector object in a feature space, and (dis)similarity between data is measured by the distance between the vectors, their inner product, or cosine of the angle between them (Jurafsky and Martin, 2008). With such a (dis)similarity measure, the Kenji Fukumizu The Institute of Statistical Mathematics Tachikawa, Tokyo, Japan fukumizu@ism.ac.jp unknown class label of a test object is predicted by a majority vote of the classes of its k most similar objects in the labeled training set. Recent studies (Radovanovi´c et al., 2010a; Radova</context>
<context position="5669" citStr="Navigli, 2009" startWordPosition="902" endWordPosition="903">language data. these methods markedly improve the performance of kNN classifiers in word sense disambiguation and document classification tasks. 2 Related work Centering is a classical technique widely used in many fields of science. For instance, centering forms a preprocessing step in principal component analysis and Fisher linear discriminant analysis. In NLP, however, centering is seldom used; the use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the</context>
<context position="26634" citStr="Navigli, 2009" startWordPosition="4725" endWordPosition="4726">se kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around the word, predicts a sense from the known set of senses. We used a basic bag-of-words representation for the context surrounding a target word (Mihalcea, 2004; Navigli, 2009). A context is thus represented as a high-dimensional feature vector holding the tfidf weighted frequency of words3 in context. 7.1.2 Compared methods We applied kNN classification using cosine similarity K, and its four transformed similarity measures: centered similarity Kcent, its weighted variant Kweignted, Mutual Proximity and graph Laplacian kernels. The sense of a test object was predicted by voting from the k training objects most similar to the test object, as measured by the respective similarity measures. We used leave-one-out cross validation within the training data to tune neighb</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41:10:1–10:69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Mustafa Qamar</author>
<author>´Eric Gaussier</author>
<author>Jean-Pierre Chevallet</author>
<author>Joo-Hwee Lim</author>
</authors>
<title>Similarity learning for nearest neighbor classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th International Conference on Data Mining (ICDM ’08),</booktitle>
<pages>983--988</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="5990" citStr="Qamar et al., 2008" startWordPosition="946" endWordPosition="949"> and Fisher linear discriminant analysis. In NLP, however, centering is seldom used; the use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects </context>
</contexts>
<marker>Qamar, Gaussier, Chevallet, Lim, 2008</marker>
<rawString>Ali Mustafa Qamar, ´Eric Gaussier, Jean-Pierre Chevallet, and Joo-Hwee Lim. 2008. Similarity learning for nearest neighbor classification. In Proceedings of the 8th International Conference on Data Mining (ICDM ’08), pages 983–988, Pisa, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Milos Radovanovi´c</author>
</authors>
<title>Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010a. Hubs in space: Popular nearest neighbors in high-dimensional data.</title>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2487</pages>
<marker>Radovanovi´c, </marker>
<rawString>Milo&amp;quot;s Radovanovi´c, Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010a. Hubs in space: Popular nearest neighbors in high-dimensional data. Journal of Machine Learning Research, 11:2487–2531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milos Radovanovi´c</author>
<author>Alexandros Nanopoulos</author>
<author>Mirjana Ivanovi´c</author>
</authors>
<title>On the existence of obstinate results in vector space models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’10),</booktitle>
<pages>186--193</pages>
<location>Geneva, Switzerland.</location>
<marker>Radovanovi´c, Nanopoulos, Ivanovi´c, 2010</marker>
<rawString>Milo&amp;quot;s Radovanovi´c, Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010b. On the existence of obstinate results in vector space models. In Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’10), pages 186–193, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Saerens</author>
<author>Franc¸ois Fouss</author>
<author>Luh Yen</author>
<author>Pierr Dupont</author>
</authors>
<title>The principal components analysis of graph, and its relationships to spectral clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 15th European Conference on Machine Learning (ECML ’04), Lecture Notes in Artificial Intelligence 3201,</booktitle>
<pages>371--383</pages>
<publisher>Springer.</publisher>
<location>Pisa, Italy.</location>
<contexts>
<context position="6487" citStr="Saerens et al., 2004" startWordPosition="1020" endWordPosition="1023">: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. Section 4 presents a theoretical justification for using centering to reduce hubs, but this kind of analysis is missing for the Laplacian kernels. Centering is easier to compute as well. For a dataset of n objects, it takes O(n2) time to compute, whereas computing a Laplacia</context>
<context position="25975" citStr="Saerens et al., 2004" startWordPosition="4611" endWordPosition="4614">ilarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012)2, and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around the word, predicts a sense from the known set of senses. We used a basic bag-of-words representation for the contex</context>
</contexts>
<marker>Saerens, Fouss, Yen, Dupont, 2004</marker>
<rawString>Marco Saerens, Franc¸ois Fouss, Luh Yen, and Pierr Dupont. 2004. The principal components analysis of graph, and its relationships to spectral clustering. In Proceedings of the 15th European Conference on Machine Learning (ECML ’04), Lecture Notes in Artificial Intelligence 3201, pages 371–383, Pisa, Italy. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominik Schnitzer</author>
<author>Arthur Flexer</author>
<author>Markus Schedl</author>
<author>Gerhard Widmer</author>
</authors>
<title>Local and global scaling reduce hubs in space.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--2871</pages>
<contexts>
<context position="6256" citStr="Schnitzer et al. (2012)" startWordPosition="985" endWordPosition="988">tion (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. Section 4 presents a theoretical justificati</context>
<context position="25431" citStr="Schnitzer et al., 2012" startWordPosition="4527" endWordPosition="4530">er the performance of kNN classification is improved. Throughout this section, K denotes cosine similarity matrix; i.e., inner product of feature vectors normalized to unit length; Kcent denotes the centered similarity matrix computed by Eq. (3) from K; Kweignted denotes its hubness weighted variant given by Eq. (10). Depending on context, these symbols are also used to denote kNN classifiers using respective similarity measures. For comparison, we also tested two recently proposed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012)2, and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels.</context>
</contexts>
<marker>Schnitzer, Flexer, Schedl, Widmer, 2012</marker>
<rawString>Dominik Schnitzer, Arthur Flexer, Markus Schedl, and Gerhard Widmer. 2012. Local and global scaling reduce hubs in space. Journal of Machine Learning Research, 13:2871–2902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--97</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24:97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander J Smola</author>
<author>Risi Kondor</author>
</authors>
<title>Kernels and regularization on graphs.</title>
<date>2003</date>
<booktitle>In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, Proceedings, Lecture Notes in Artificial Intelligence 2777,</booktitle>
<pages>144--158</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6571" citStr="Smola and Kondor, 2003" startWordPosition="1032" endWordPosition="1035">berger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. Section 4 presents a theoretical justification for using centering to reduce hubs, but this kind of analysis is missing for the Laplacian kernels. Centering is easier to compute as well. For a dataset of n objects, it takes O(n2) time to compute, whereas computing a Laplacian-based kernel requires O(n3) time for matrix inversion. Mutual Proximity also has a</context>
<context position="25886" citStr="Smola and Kondor, 2003" startWordPosition="4596" endWordPosition="4600">so tested two recently proposed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012)2, and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels. 7.1 Word sense disambiguation 7.1.1 Task and dataset In the WSD experiment, we used the dataset for the Senseval-3 English Lexical Sample (ELS) task (Mihalcea et al., 2004). It is a collection of sentences containing 57 polysemous words, and each of these sentences is annotated with a gold standard sense of the target word. The goal of the ELS task is to build a classifier for each target word, which, given a context around the word, predicts a sense</context>
</contexts>
<marker>Smola, Kondor, 2003</marker>
<rawString>Alexander J. Smola and Risi Kondor. 2003. Kernels and regularization on graphs. In Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, Proceedings, Lecture Notes in Artificial Intelligence 2777, pages 144– 158. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semisupervised condensed nearest neighbor for part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11),</booktitle>
<pages>48--52</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1535" citStr="Søgaard, 2011" startWordPosition="222" endWordPosition="223">the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks. 1 Introduction 1.1 Background The k-nearest neighbor (kNN) algorithm is a simple nonparametric method of classification. It has been applied to various natural language processing (NLP) tasks such as document classification (Masand et al., 1992; Yang and Liu, 1999), partof-speech tagging (Søgaard, 2011), and word sense disambiguation (Navigli, 2009). To apply the kNN algorithm, data is typically represented as a vector object in a feature space, and (dis)similarity between data is measured by the distance between the vectors, their inner product, or cosine of the angle between them (Jurafsky and Martin, 2008). With such a (dis)similarity measure, the Kenji Fukumizu The Institute of Statistical Mathematics Tachikawa, Tokyo, Japan fukumizu@ism.ac.jp unknown class label of a test object is predicted by a majority vote of the classes of its k most similar objects in the labeled training set. Rec</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11), pages 48– 52, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ikumi Suzuki</author>
<author>Kazuo Hara</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
<author>Marco Saerens</author>
</authors>
<title>Investigating the effectiveness of Laplacian-based kernels in hub reduction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI-12),</booktitle>
<pages>1112--1118</pages>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="4407" citStr="Suzuki et al., 2012" startWordPosition="702" endWordPosition="705">their inner product and cosine are affected; see Section 3 for detail. In this paper, we advocate the use of centering as a means of reducing hubs. Specifically, we propose to measure the similarity of objects by the inner product (not distance or cosine) in the centered feature space. Our approach is motivated by the observation that the objects similar to the data centroid tend to become hubs (Radovanovi´c et al., 2010a). This observation suggests that the number of hubs may be reduced if we can define a similarity measure that makes all objects in a dataset equally similar to the centroid (Suzuki et al., 2012). The inner product in the centered space indeed enjoys this property. In Section 4, we analyze why hubs emerge under a simple probabilistic model of data, and also give an account of why they are suppressed by centering. Using both synthetic and real datasets, we show that objects similar to the centroid also emerge as hubs in multi-cluster data (Section 5), so the application of centering is wider than expected. To further reduce hubs, we also propose to move the origin of the space more aggressively towards hubs, through weighted centering (Section 6). In Section 7, we show that centering a</context>
<context position="6385" citStr="Suzuki et al. (2012)" startWordPosition="1004" endWordPosition="1007"> and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian kernels in NLP tasks, and demonstrate that centering is equally or even more effective. Section 4 presents a theoretical justification for using centering to reduce hubs, but this kind of analysis is missing for the Laplacian kernels. Centering is easier to com</context>
<context position="25467" citStr="Suzuki et al., 2012" startWordPosition="4534" endWordPosition="4537">n is improved. Throughout this section, K denotes cosine similarity matrix; i.e., inner product of feature vectors normalized to unit length; Kcent denotes the centered similarity matrix computed by Eq. (3) from K; Kweignted denotes its hubness weighted variant given by Eq. (10). Depending on context, these symbols are also used to denote kNN classifiers using respective similarity measures. For comparison, we also tested two recently proposed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012)2, and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, 2We used the Matlab script downloaded from http://www. ofai.at/∼dominik.schnitzer/mp/. we computed both the regularized Laplacian kernel (Chebotarev and Shamis, 1997; Smola and Kondor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels. 7.1 Word sense disambiguation 7.1.1</context>
</contexts>
<marker>Suzuki, Hara, Shimbo, Matsumoto, Saerens, 2012</marker>
<rawString>Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Matsumoto, and Marco Saerens. 2012. Investigating the effectiveness of Laplacian-based kernels in hub reduction. In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI-12), pages 1112–1118, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10),</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10), pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jigang Wang</author>
<author>Predrag Neskovic</author>
<author>Leon N Cooper</author>
</authors>
<title>Neighborhood size selection in the k-nearestneighbor rule using statistical confidence.</title>
<date>2006</date>
<journal>Pattern Recognition,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="6127" citStr="Wang et al., 2006" startWordPosition="965" endWordPosition="968">ite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 2003), make all objects equally similar to the data centroid, which in turn reduce hubs. In Section 7, we evaluate centering, Mutual Proximity, and Laplacian ker</context>
</contexts>
<marker>Wang, Neskovic, Cooper, 2006</marker>
<rawString>Jigang Wang, Predrag Neskovic, and Leon N. Cooper. 2006. Neighborhood size selection in the k-nearestneighbor rule using statistical confidence. Pattern Recognition, 39(3):417–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Q Weinberger</author>
<author>Lawrence K Saul</author>
</authors>
<title>Distance metric learning for large margin nearest neighbor classification.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--207</pages>
<contexts>
<context position="5969" citStr="Weinberger and Saul, 2009" startWordPosition="942" endWordPosition="945">rincipal component analysis and Fisher linear discriminant analysis. In NLP, however, centering is seldom used; the use of cosine and inner product similarities is quite common, but they are nearly always used uncentered. Non-centered cosine is used, for instance, in word sense disambiguation (Sch¨utze, 1998; Navigli, 2009), paraphrasing (Erk and Pad´o, 2008; Thater et al., 2010), and compositional semantics (Mitchell and Lapata, 2008), to name a few. There have been several approaches to improving kNN classification: learning similarity/distance measures from training data (metric learning) (Weinberger and Saul, 2009; Qamar et al., 2008), weighting nearest neighbors for similarity-based classification (Chen et al., 2009), and neighborhood size selection (Wang et al., 2006; Guo and Chakraborty, 2010). However, none of these have addressed the reduction of hubs. More recently, Schnitzer et al. (2012) proposed the Mutual Proximity transformation that rescales distance measures to decrease hubs in a dataset. Suzuki et al. (2012) showed that kernels based on graph Laplacian, such as the commute-time kernels (Saerens et al., 2004) and the regularized Laplacian (Chebotarev and Shamis, 1997; Smola and Kondor, 200</context>
</contexts>
<marker>Weinberger, Saul, 2009</marker>
<rawString>Kilian Q. Weinberger and Lawrence K. Saul. 2009. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99),</booktitle>
<pages>42--49</pages>
<location>Berkeley, California, USA.</location>
<contexts>
<context position="1496" citStr="Yang and Liu, 1999" startWordPosition="215" endWordPosition="218"> data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classifiers considerably in word sense disambiguation and document classification tasks. 1 Introduction 1.1 Background The k-nearest neighbor (kNN) algorithm is a simple nonparametric method of classification. It has been applied to various natural language processing (NLP) tasks such as document classification (Masand et al., 1992; Yang and Liu, 1999), partof-speech tagging (Søgaard, 2011), and word sense disambiguation (Navigli, 2009). To apply the kNN algorithm, data is typically represented as a vector object in a feature space, and (dis)similarity between data is measured by the distance between the vectors, their inner product, or cosine of the angle between them (Jurafsky and Martin, 2008). With such a (dis)similarity measure, the Kenji Fukumizu The Institute of Statistical Mathematics Tachikawa, Tokyo, Japan fukumizu@ism.ac.jp unknown class label of a test object is predicted by a majority vote of the classes of its k most similar o</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99), pages 42–49, Berkeley, California, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998808">
Exploring Demographic Language Variations to Improve Multilingual
Sentiment Analysis in Social Media
</title>
<author confidence="0.970893">
Svitlana Volkova
</author>
<affiliation confidence="0.746368333333333">
Center for Language and
Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.949837">
Baltimore, MD
</address>
<email confidence="0.999397">
svitlana@jhu.edu
</email>
<author confidence="0.979884">
Theresa Wilson
</author>
<affiliation confidence="0.940447666666667">
Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.935519">
Baltimore, MD
</address>
<email confidence="0.999235">
taw@jhu.edu
</email>
<author confidence="0.969332">
David Yarowsky
</author>
<affiliation confidence="0.744538">
Center for Language and
Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.950095">
Baltimore, MD
</address>
<email confidence="0.999529">
yarowsky@cs.jhu.edu
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990694">
Different demographics, e.g., gender or age,
can demonstrate substantial variation in their
language use, particularly in informal contexts
such as social media. In this paper we focus on
learning gender differences in the use of sub-
jective language in English, Spanish, and Rus-
sian Twitter data, and explore cross-cultural
differences in emoticon and hashtag use for
male and female users. We show that gen-
der differences in subjective language can ef-
fectively be used to improve sentiment anal-
ysis, and in particular, polarity classification
for Spanish and Russian. Our results show
statistically significant relative F-measure im-
provement over the gender-independent base-
line 1.5% and 1% for Russian, 2% and 0.5%
for Spanish, and 2.5% and 5% for English for
polarity and subjectivity classification.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999318227272727">
Sociolinguistics and dialectology have been study-
ing the relationships between language and speech at
the phonological, lexical and morphosyntactic lev-
els and social identity for decades (Picard, 1997;
Gefen and Ridings, 2005; Holmes and Meyerhoff,
2004; Macaulay, 2006; Tagliamonte, 2006). Re-
cent studies have focused on exploring demographic
language variations in personal email communica-
tion, blog posts, and public discussions (Boneva et
al., 2001; Mohammad and Yang, 2011; Eisenstein
et al., 2010; O’Connor et al., 2010; Bamman et al.,
2012). However, one area that remains largely unex-
plored is the effect of demographic language varia-
tion on subjective language use, and whether these
differences may be exploited for automatic senti-
ment analysis. With the growing commercial im-
portance of applications such as personalized rec-
ommender systems and targeted advertising (Fan
and Chang, 2009), detecting helpful product review
(Ott et al., 2011), tracking sentiment in real time
(Resnik, 2013), and large-scale, low-cost, passive
polling (O’Connor et al., 2010), we believe that sen-
timent analysis guided by user demographics is a
very important direction for research.
In this paper, we focus on gender demographics
and language in social media to investigate differ-
ences in the language used to express opinions in
Twitter for three languages: English, Spanish, and
Russian. We focus on Twitter data because of its vol-
ume, dynamic nature, and diverse population world-
wide.1 We find that some words are more or less
likely to be positive or negative in context depend-
ing on the the gender of the author. For example, the
word weakness is more likely to be used in a pos-
itive way by women (Chocolate is my weakness!)
but in a negative way by men (Clearly they know
our weakness. Argggg). The Russian word gOCTHLIb
(achieve) is used in a positive way by male users and
in a negative way by female users.
Our goals of this work are to (1) explore the gen-
der bias in the use of subjective language in so-
cial media, and (2) incorporate this bias into models
to improve sentiment analysis for English, Spanish,
and Russian. Specifically, in this paper we:
</bodyText>
<listItem confidence="0.9927685">
• investigate multilingual lexical variations in the
use of subjective language, and cross-cultural
</listItem>
<footnote confidence="0.8510355">
1As of May 2013, Twitter has 500m users (140m of them
in the US) from more than 100 countries.
</footnote>
<page confidence="0.876045">
1815
</page>
<note confidence="0.738755">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815–1827,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.96783">
emoticon and hashtag usage on a large scale in
Twitter data;2
</bodyText>
<listItem confidence="0.998490444444444">
• show that gender bias in the use of subjec-
tive language can be used to improve sentiment
analysis for multiple languages in Twitter.
• demonstrate that simple, binary features repre-
senting author gender are insufficient; rather, it
is the combination of lexical features, together
with set-count features representing gender-
dependent sentiment terms that is needed for
statistically significant improvements.
</listItem>
<bodyText confidence="0.7924592">
To the best of our knowledge, this work is the first
to show that incorporating gender leads to signifi-
cant improvements for sentiment analysis, particu-
larly subjectivity and polarity classification, for mul-
tiple languages in social media.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998741660377359">
Numerous studies since the early 1970’s have inves-
tigated gender-language differences in interaction,
theme, and grammar among other topics (Schiffman,
2002; Sunderland et al., 2002). More recent research
has studied gender differences in telephone speech
(Cieri et al., 2004; Godfrey et al., 1992) and emails
(Styler, 2011). Mohammad and Yang (2011) ana-
lyzed gender differences in the expression of senti-
ment in love letters, hate mail, and suicide notes, and
emotional word usage across genders in email.
There has also been a considerable amount of
work in subjectivity and sentiment analysis over
the past decade, including, more recently, in mi-
croblogs (Barbosa and Feng, 2010; Bermingham
and Smeaton, 2010; Pak and Paroubek, 2010; Bifet
and Frank, 2010; Davidov et al., 2010; Li et
al., 2010; Kouloumpis et al., 2011; Jiang et al.,
2011; Agarwal et al., 2011; Wang et al., 2011;
Calais Guerra et al., 2011; Tan et al., 2011; Chen
et al., 2012; Li et al., 2012). In spite of the surge of
research in both sentiment and social media, only a
limited amount of work focusing on gender identi-
fication has looked at differences in subjective lan-
guage across genders within social media. Thel-
wall (2010) found that men and women use emoti-
cons to differing degrees on MySpace, e.g., female
2Gender-dependent and independent lexical resources of
subjective terms in Twitter for Russian, Spanish and English can
be found here: http://www.cs.jhu.edu/~svitlana/
users express positive emoticons more than male
users. Other researchers included subjective patterns
as features for gender classification of Twitter users
(Rao et al., 2010). They found that the majority of
emotion-bearing features, e.g., emoticons, repeated
letters, exasperation, are used more by female than
male users, which is consistent with results reported
in other recent work (Garera and Yarowsky, 2009;
Burger et al., 2011; Goswami et al., 2009; Argamon
et al., 2007). Other related work is that of Otter-
bacher (2010), who studied stylistic differences be-
tween male and female reviewers writing product
reviews, and Mukherjee and Liu (2010), who ap-
plied positive, negative and emotional connotation
features for gender classification in microblogs.
Although previous work has investigated gen-
der differences in the use of subjective language,
and features of sentiment have been used in gender
identification, to the best of our knowledge no one
has yet investigated whether gender differences in
the use of subjective language can be exploited to
improve sentiment classification in English or any
other language. In this paper we seek to answer this
question for the domain of social media.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999897047619048">
For the experiments in this paper, we use three sets
of data for each language: a large pool of data (800K
tweets) labeled for gender but unlabeled for senti-
ment, plus 2K development data and 2K test data
labeled for both sentiment and gender. We use the
unlabeled data to bootstrap Twitter-specific lexicons
and investigate gender differences in the use of sub-
jective language. We use the development data for
parameter tuning while bootstrapping, and the test
data for sentiment classification.
For English, we download tweets from the corpus
created by Burger et al. (2011). This dataset con-
tains 2,958,103 tweets from 184K users, excluding
retweets. Retweets are omitted because our focus is
on the sentiment of the person tweeting; in retweets,
the words originate from a different user. All users
in this corpus have gender labels, which Burger et
al. automatically extracted from self-reported gen-
der on Facebook or MySpace profiles linked to by
the Twitter users. English tweets are identified using
a compression-based language identification (LID)
</bodyText>
<page confidence="0.986478">
1816
</page>
<bodyText confidence="0.999409954545455">
tool (Bergsma et al., 2012). According to LID,
there are 1,881,620 (63.6%) English tweets from
which we select a random, gender-balanced sample
of 0.8M tweets. Burger’s corpus does not include
Russian and Spanish data on the same scale as En-
glish. Therefore, for Russian and Spanish we con-
struct a new Twitter corpus by downloading tweets
from followers of region-specific news and media
Twitter feeds. We use LID to identify Russian and
Spanish tweets, and remove retweets as before. In
this data, gender is labeled automatically based on
user first and last name morphology with a precision
above 0.98 for all languages.
Sentiment labels for tweets in the development
and test sets are obtained using Amazon Mechanical
Turk. For each tweet we collect annotations from
five workers and use majority vote to determine the
final label for the tweet. Snow et al. (2008) show
that for a similar task, labeling emotion and valence,
on average four non-expert labelers are needed to
achieve an expert level of annotation. Below are the
example Russian tweets labeled for sentiment:
</bodyText>
<listItem confidence="0.965539333333333">
• Pos: Как же приятно просто лечь в по-
стель после тяжелого дня... (It is a great
pleasure to go to bed after a long day at work...)
• Neg: Уважаемый господин Прохоров ку-
пите эти выборы! (Dear Mr. Prokhorov just
buy the elections!)
• Both: Затолкали меня на местном рынке!
но зато закупилась подарками для всей
семьи :) (It was crowded at the local market!
But I got presents for my family:-))
• Neutral: Киев очень старый город (Kiev is
a very old city).
</listItem>
<tableCaption confidence="0.8427175">
Table 1 gives the distribution of tweets over senti-
ment and gender labels for the development and test
sets for English (EDEV, ETEST), Spanish (SDEV,
STEST), and Russian (RDEV, RTEST).
</tableCaption>
<table confidence="0.998669571428571">
Data Pos Neg Both Neut 9 Cr
EDEV 617 357 202 824 1,176 824
ETEST 596 347 195 862 1,194 806
SDEV 358 354 86 1,202 768 1,232
STEST 317 387 93 1203 700 1,300
RDEV 452 463 156 929 1,016 984
RTEST 488 380 149 983 910 1,090
</table>
<tableCaption confidence="0.9970875">
Table 1: Gender and sentiment label distribution in the
development and test sets for all languages.
</tableCaption>
<sectionHeader confidence="0.801069" genericHeader="method">
4 Subjective Language and Gender
</sectionHeader>
<bodyText confidence="0.99990625">
To study the intersection of subjective language and
gender in social media, ideally we would have a
large corpus labeled for both. Although our large
corpus is labeled for gender, it is not labeled for sen-
timent. Only the 4K tweets for each language that
compose the development and test sets are labeled
for both gender and sentiment. Obtaining sentiment
labels for all tweets would be both impractical and
expensive. Instead we use large multilingual senti-
ment lexicons developed specifically for Twitter as
described below. Using these lexicons we can begin
to explore the relationship between subjective lan-
guage and gender in the large pool of data labeled
for gender but unlabeled for sentiment. We also
look at the relationship between gender and the use
of different hashtags and emoticons. These can be
strong indicators of sentiment in social media, and in
fact are sometimes used to create noisy training data
for sentiment analysis in Twitter (Pak and Paroubek,
2010; Kouloumpis et al., 2011).
</bodyText>
<subsectionHeader confidence="0.999637">
4.1 Bootstrapping Subjectivity Lexicons
</subsectionHeader>
<bodyText confidence="0.998984">
Recent work by Banea et.al (2012) classifies meth-
ods for bootstrapping subjectivity lexicons into two
types: corpus-based and dictionary-based. Corpus-
based methods extract subjectivity lexicons from
unlabeled data using different similarity metrics
to measure the relatedness between words, e.g.,
Pointwise Mutual Information (PMI). Corpus-based
methods have been used to bootstrap lexicons
for ENGLISH (Turney, 2002) and other languages,
including ROMANIAN (Banea et al., 2008) and
JAPANESE (Kaji and Kitsuregawa, 2007).
Dictionary-based methods rely on relations be-
tween words in existing lexical resources. For exam-
ple, Rao and Ravichandran (2009) construct HINDI
and FRENCH sentiment lexicons using relations in
WordNet (Miller, 1995), Rosas et. al. (2012) boot-
strap a SPANISH lexicon using SentiWordNet (Bac-
cianella et al., 2010) and OpinionFinder,3 Clematide
and Klenner (2010), Chetviorkin et al. (2012) and
Abdul-Mageed et. al. (2011) automatically expand
and evaluate GERMAN, RUSSIAN and ARABIC sub-
jective lexicons.
</bodyText>
<footnote confidence="0.978569">
3www.cs.pitt.edu/mpqa/opinionfinder
</footnote>
<page confidence="0.995772">
1817
</page>
<bodyText confidence="0.999995384615385">
We use the corpus-based, language-independent
approach proposed by Volkova et al. (2013) to boot-
strap Twitter-specific subjectivity lexicons. To start,
the new lexicon is seeded with terms from the initial
lexicon LI. On each iteration, tweets in the unla-
beled data are labeled using the current lexicon. If a
tweet contains one or more terms from the lexicon it
is marked subjective, otherwise neutral. Tweet po-
larity is determined in a similar way, but takes into
account negation. For every term not in the lexi-
con with a frequency threshold, the probability of
that word appearing in a subjective sentence is cal-
culated. The top k terms with a subjective probabil-
ity are then added to the lexicon. Bootstrapping con-
tinues until there are no more new terms meeting the
criteria to add to the lexicon. The parameters are op-
timized using a grid search on the development data
using F-measure for subjectivity classification. In
Table 2 we report size and term polarity from the ini-
tial LI and the bootstrapped LB lexicons. Although
more sophisticated bootstrapping methods exist, this
approach has been shown to be effective for atomi-
cally learning subjectivity lexicons in multiple lan-
guages on a large scale without any external, rich,
lexical resources, e.g., WordNet, or advanced NLP
tools, e.g., syntactic parsers (Wiebe, 2000) or infor-
mation extraction tools (Riloff and Wiebe, 2003).
For English, seed terms for bootstrapping are
the strongly subjective terms in the MPQA lexicon
(Wilson et al., 2005). For Spanish and Russian, the
seed terms are obtained by translating the English
seed terms using a bi-lingual dictionary, collecting
subjectivity judgments from MTurk on the transla-
tions, filtering out translations that are not strongly
subjective, and expanding the resulting word lists
with plurals and inflectional forms.
To verify that bootstrapping does provide a bet-
ter resource than existing dictionary-expanded lexi-
cons, we compare our Twitter-specific lexicons LB
</bodyText>
<subsectionHeader confidence="0.69087">
English Spanish Russian
</subsectionHeader>
<table confidence="0.975199">
LE LE LS LS LR LR
I B I B I B
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
</table>
<tableCaption confidence="0.990983">
Table 2: The initial LI and the bootstrapped LB (high-
lighted) lexicon term count (LI ⊂ LB) with polarity
across languages (thousands).
</tableCaption>
<bodyText confidence="0.945409">
to the corresponding initial lexicons LI and the ex-
isting state-of-the-art subjective lexicons including:
</bodyText>
<listItem confidence="0.9992214">
• 8K strongly subjective English terms from Sen-
tiWordNet XE (Baccianella et al., 2010);
• 1.5K full strength terms from the Spanish sen-
timent lexicon XS (Perez-Rosas et al., 2012);
• 5K terms from the Russian sentiment lexicon
</listItem>
<bodyText confidence="0.967772538461538">
X� (Chetviorkin and Loukachevitch, 2012).
For that we apply rule-based subjectivity classi-
fication on the test data.4 This subjectivity classi-
fier predicts that a tweet is subjective if it contains
at least one, or at least two subjective terms from
the lexicon. To make a fair comparison, we auto-
matically expand XE with plurals and inflectional
forms, XS with the inflectional forms for verbs, and
X� with the inflectional forms for adverbs, adjec-
tives and verbs. We report precision, recall and F-
measure results in Table 3 and show that our boot-
strapped lexicons outperform the corresponding ini-
tial lexicons and the external resources.
</bodyText>
<table confidence="0.991139">
Subj &gt;_ 1 Subj &gt;_ 2
P R F P R F
XE 0.67 0.49 0.57 0.76 0.16 0.27
LEI 0.69 0.73 0.71 0.79 0.34 0.48
LEB 0.64 0.91 0.75 0.7 0.74 0.72
XS 0.52 0.39 0.45 0.62 0.07 0.13
LSI 0.50 0.73 0.59 0.59 0.36 0.45
LS B 0.44 0.91 0.59 0.51 0.71 0.59
XR 0.61 0.49 0.55 0.74 0.17 0.29
LR I 0.72 0.34 0.46 0.83 0.07 0.13
LRB 0.64 0.58 0.61 0.74 0.23 0.35
</table>
<tableCaption confidence="0.938448">
Table 3: Precision, recall and F-measure results for sub-
jectivity classification using the external X, initial LI and
bootstrapped LB lexicons for all languages.
</tableCaption>
<subsectionHeader confidence="0.998454">
4.2 Lexical Evaluation
</subsectionHeader>
<bodyText confidence="0.9999479">
With our Twitter-specific sentiment lexicons, we
can now investigate how the subjective use of these
terms differs depending on gender for our three lan-
guages. Figure 1 illustrates what we expect to find.
{F} and {M} are the sets of subjective terms used
by females and males, respectively. We expect that
some terms will be used by males, but never by fe-
males, and vice-versa. The vast majority, however,
will be used by both genders. Within this set of
shared terms, many words will show little difference
</bodyText>
<footnote confidence="0.9923905">
4A similar rule-based approach using terms from the
MPQA lexicon is suggested by (Riloff and Wiebe, 2003).
</footnote>
<page confidence="0.991559">
1818
</page>
<figureCaption confidence="0.999497">
Figure 1: Gender-dependent vs. independent subjectivity
terms (+ and - indicates term polarity).
Figure 2: The distribution of gender-dependent GDep
and gender-independent GInd sentiment terms.
</figureCaption>
<bodyText confidence="0.999978523809524">
in their subjective use when considering gender, but
there will be some words for which gender will have
an influence. Of particular interest for our work are
words in which the polarity of a term as it is used in
context is gender-influenced, the extreme case being
terms that flip their polarity depending on the gender
of the user. Polarity may be different because the
concept represented by the term tends to be viewed
in a different light depending on gender. There are
also words like weakness in which a more positive or
more negative word sense tends to be used by men
or women. In Figure 2 we show the distribution of
gender-specific and gender-independent terms from
the LB lexicons for all languages.
To identify gender-influenced terms in our lexi-
cons, we start by randomly sampling 400K male and
400K female tweets for each language from the data.
Next, for both genders we calculate the probability
of term ti appearing in a tweet with another subjec-
tive term (Eq.1), and the probability of it appearing
with a positive or negative term (Eq.2-3) from LB.
</bodyText>
<equation confidence="0.924161333333334">
(ti,
(subj I g) = c(ti, P, g) + c(ti, N, g) ,(1)
c(ti, g)
</equation>
<bodyText confidence="0.9998715">
where g E F, M and P and N are positive and nega-
tive sets of terms from the initial lexicon LI.
</bodyText>
<equation confidence="0.996705">
c(ti, P, g)
pti(+Ig) = (2)
c(ti, P, g) + c(ti, N, g)
c(ti, N, g)
pti(−Ig) = (3)
c(ti, P, g) + c(ti, N, g)
</equation>
<bodyText confidence="0.889782888888889">
We introduce a novel metric Op+ti to measure po-
larity change across genders. For every subjective
term ti we want to maximize the difference5:
where p(+IF) and p(+IM) are probabilities that
term ti is positive for females and males respec-
tively; tfsubj
ti (F) and tfsubj
ti (M) are correspond-
ing term frequencies (if tfsubj
</bodyText>
<equation confidence="0.802201">
ti (F) &gt; tfsubj
</equation>
<bodyText confidence="0.991659533333334">
ti (M) the
fraction is flipped); λ is a threshold that controls
the level of term frequency similarity6. The terms
in which polarity is most strongly gender-influenced
are those with λ → 0 and Op+ti → 1.
Table 4 shows a sample of the most strongly
gender-influenced terms from the initial LI and the
bootstrapped LB lexicons for all languages. A plus
(+) means that the term tends to be used positively
by women and minus (−) means that the term tends
to be used positively by men. For instance, in En-
glish we found that perfecting is used with negative
polarity by male users but with positive polarity by
female users; the term dogfighting has negative po-
larity for women but positive polarity for men.
</bodyText>
<subsectionHeader confidence="0.999072">
4.3 Hashtags
</subsectionHeader>
<bodyText confidence="0.9999241">
People may also express positive or negative senti-
ment in their tweets using hashtags. From our bal-
anced samples of 800K tweets for each language,
we extracted 611, 879, and 71 unique hashtags for
English, Spanish, and Russian, respectively. As we
did for terms in the previous section, we evaluated
the subjective use of the hashtags. Some of these are
clearly expressing sentiment (#horror), while others
seem to be topics that people are frequently opinion-
ated about (#baseball, #latingrammy, #spartak).
</bodyText>
<footnote confidence="0.585747">
5One can also maximize Ap−t: = Ipt:(−IF) − pt:(−IM)I.
6A = 0 means term frequencies are identical for both gen-
ders; A → 1 indicates increasing gender divergence.
</footnote>
<table confidence="0.966184930232558">
Op+ti = Ipti(+IF) − pti(+IM)I
s.t.
������������ 1 tfsubj ������������ ≤ λ, t ft ubj (M) ≠ 0, (4)
ti (F) i
tfsubj
ti (M)
1819
English Initial Terms L� Δp+ A English Bootstrapped Terms L� � Δp+ A
�
+ 0.7 0.0
+ 0.6 0.4
– 0.6 0.5
– 0.7 0.5
– 1.0 0.3
+ 0.7 0.5
+ 0.7 0.3
– 0.3 0.3
– 0.8 0.3
– 1.0 0.0
+ 0.7 0.3
+ 0.7 0.3
– 1.0 0.0
– 1.0 0.0
– 1.0 0.0
perfecting + 0.7 0.2 pleaseeeeee
weakened + 0.1 0.0 adorably
saddened – 0.1 0.0 creatively
misbehaving – 0.4 0.0 dogfighting
glorifying – 0.7 0.5 overdressed
Spanish Initial Terms L� Spanish Bootstrapped Terms L�
� �
fiasco (fiasco) + 0.7 0.3 cafeína (caffeine)
triunfar (succeed) + 0.7 0.0 claro (clear)
inconsciente (unconscious) – 0.6 0.2 cancio (dog)
horroriza (horrifies) – 0.7 0.3 llevara (take)
groseramente (rudely) – 0.7 0.3 recomendarlo (recommend)
Russian Initial Terms L� Russian Bootstrapped Terms L�
� �
магическая (magical) + 0.7 0.3 мечтайте (dream!)
сенсационный (sensational) + 0.7 0.3 танцуете (dancing)
обожаемый (adorable) – 0.7 0.0 сложны (complicated)
искушение (temptation) – 0.7 0.3 молоденькие (young)
заслуживать (deserve) – 1.0 0.0 достичь (achieve)
</table>
<tableCaption confidence="0.9909795">
Table 4: Sample of subjective terms sorted by Δp+ to show lexical differences and polarity change across genders
(module is not applied as defined in Eq.1 to demonstrate the polarity change direction).
</tableCaption>
<table confidence="0.999844333333333">
English Δp+ A Spanish Δp+ A Russian Δp+ A
#parenting + 0.7 0.0 #rafaelnarro (politician) + 1.0 0.0 #совет (advise) + 1.0 0.0
#vegas – 0.2 0.8 #amores (loves) + 0.2 1.0 #ukrlaw + 1.0 1.0
#horror – 0.6 0.7 #britneyspears + 0.1 0.3 #spartak (soccer team) – 0.7 0.9
#baseball – 0.6 0.9 #latingrammy – 0.5 0.1 #сны (dreams) – 1.0 0.0
#wolframalpha – 0.7 1.0 #metallica (music band) – 0.5 0.8 #iphones – 1.0 1.0
</table>
<tableCaption confidence="0.999623">
Table 5: Hashtag examples with opposite polarity across genders for English, Spanish, and Russian.
</tableCaption>
<bodyText confidence="0.992856">
Table 5 gives the hashtags, correlated with sub-
jective language, that are most strongly gender-
influenced. Analogously to Δp+ values in Table 4, a
plus (+) means the hashtag is more likely to be used
positively by women, and a minus (−) means the
hashtag is more likely to be used positively by men.
For example, in English we found that male users
tend to express positive sentiment in tweets men-
tioning #baseball, while women tend to be nega-
tive about this hashtag. The opposite is true for the
hashtag #parenting.
</bodyText>
<subsectionHeader confidence="0.989026">
4.4 Emoticons
</subsectionHeader>
<bodyText confidence="0.999976">
We investigate how emoticons are used differently
by men and women in social media following the
work by (Bamman et al., 2012). For that we rely on
the lists of emoticons from Wikipedia7 and present
the cross-cultural and gender emoticon differences
in Figure 3. The frequency of each emoticon is given
</bodyText>
<footnote confidence="0.7995575">
7List of emoticons from Wikipedia http://en.
wikipedia.org/wiki/List_of_emoticons
</footnote>
<bodyText confidence="0.999852">
on the right of each language chart, with probability
of use by a male user in that language given on the
x-axis. The top 8 emoticons are the same across lan-
guages and sorted by English frequency.
We found that emoticons in English data are used
more overall by female users, which is consistent
with previous findings in Schnoebelen’s work.8 In
addition, we found that some emoticons like :-)
(smile face) and :-o (surprised) are used equally by
both genders, at least in Twitter. When comparing
English emoticon usage to other languages, there are
some similarities, but also some clear differences. In
Spanish data, several emoticons are more likely to be
used by male than by female users, e.g., :-o (sur-
prised) and :-&amp; (tongue-tied), and the difference in
probability of use by males and females is greater
for the emoticons, as compared to the same emoti-
cons for English. Interestingly, in Russian Twitter
</bodyText>
<footnote confidence="0.9807785">
8Language and emotion (talks, essays and reading notes)
www.stanford.edu/~tylers/emotions.shtml
</footnote>
<page confidence="0.91181">
1820
</page>
<figure confidence="0.894788333333333">
A 34.4K
8) 8.7K
() 4.1K
:-o 2.7K
0.9K
0.7K
0.4K
0.1K
0.1K
0.1K
0.0 0.2 0.4 0.6 0.8 1.0
p(Male|Emoticon)
A 19.1K
8) 9.5K
:-o 1.5K
0.1K
0.3K
0.3K
0.1K
1.5K
0.1K
0.1K
0.0 0.2 0.4 0.6 0.8 1.0
p(Male|Emoticon)
A 41.5K
8) 4.5K
() 4.6K
0.4K
0.4K
0.1K
0.1K
0.4K
0.4K
0.1K
0.0 0.2 0.4 0.6 0.8 1.0
p(Male|Emoticon)
</figure>
<figureCaption confidence="0.999951">
Figure 3: Probability of gender and emoticons for English, Spanish and Russian (from left to right).
</figureCaption>
<bodyText confidence="0.9906735">
data emoticons tend to be used more or equally by
male users rather than female users.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999937333333333">
The previous section showed that there are gender
differences in the use of subjective language, hash-
tags, and emoticons in Twitter. We aim leverage
these differences to improve subjectivity and po-
larity classification for the informal, creative and
dynamically changing multilingual Twitter data.9
For that we conduct experiments using gender-
independent GInd and gender-dependent GDep
features and compare the results to evaluate the in-
fluence of gender on sentiment classification.
We experiment with two classification ap-
proaches: (I) rule-based classifier which uses only
subjective terms from the lexicons designed to verify
if the gender differences in subjective language cre-
ate enough of a signal to influence sentiment classifi-
cation; (II) state-of-the-art supervised models which
rely on lexical features as well as lexicon set-count
features.10,11 Moreover, to show that the gender-
</bodyText>
<footnote confidence="0.800420333333333">
9For polarity classification we distinguish between positive
and negative instances, which is the approach typically reported
in the literature for recognizing polarity (Velikovich et al., 2010;
Yessenalina and Cardie, 2011; Taboada et al., 2011)
10A set-count feature is a count of the number of instances
from a set of terms that appears in a tweet.
11We also experimented with repeated punctuation (!!, ??)
and letters (nooo, reealy), which are often used in sentiment
classification in social media. However, we found these features
</footnote>
<bodyText confidence="0.999779142857143">
sentiment signal can be learned by more than one
classifier we apply a variety of classifiers imple-
mented in Weka (Hall et al., 2009). For that we do
10-fold cross validation over English, Spanish, and
Russian test data (ETEST, STEST and RTEST) la-
beled with subjectivity (pos, neg, both vs. neut) and
polarity (pos vs. neg) as described in Section 3.
</bodyText>
<subsectionHeader confidence="0.979633">
5.1 Models
</subsectionHeader>
<bodyText confidence="0.900895">
For the rule-based GIndR b classifier, tweets are la-
su
beled as subjective or neutral as follows:
</bodyText>
<equation confidence="0.989413">
GIndRB
subj = � 1 if w� f� ≥ 0.5, (5)
0 otherwise
</equation>
<bodyText confidence="0.951219133333333">
where w� f� stands for weighted set features, e.g.,
terms from LI only, emoticons E, or different part-
of-speech tags (POS) from LB weighted using w =
p(subj) = p(subjlM) + p(subjlF) subjectivity
score as shown in Eq.1. We experiment with the
POS tags to show the contribution of each POS to
sentiment classification.
Similarly, for the rule-based GIndRB
pol classifier,
tweets are labeled as positive or negative:
B 1 if w+ �f + ≥ wT− �f −, 6
pol 0 otherwise ( )
where f+, f− are feature sets that include only posi-
tive and negative features from LI or LB; w+ and w−
to be noisy and adding them decreased performance.
</bodyText>
<figure confidence="0.991165377777778">
d
R
GIn
1821
L I
L_I
+E
+E
+A
+A
+R
+V
+N
+R
+V
+N
L_I
L_I
+E
+A
+E
+A
+R
+R
+V
+N
+V
+N
0.6 0.7 0.8 0.9
Recall
0.65 0.70 0.75 0.80 0.85 0.90
Recall
0.62 0.64 0.66 0.68 0.70
Precision
Precision
0.65 0.70 0.75 0.80 0.85
BLR NB AB RF J48 SVM
Classifiers
F-measure
0.55 0.60 0.65 0.70 0.75 0.80 0.85
GIntlSubjAND
GCepSubjAND
GIrldPolAND
GOepPolAND
(a) Rule-based subjectivity (b) Rule-based polarity (c) SL subjectivity and polarity
</figure>
<figureCaption confidence="0.9488495">
Figure 4: Rule-based (RB) and Supervised Learning (SL) sentiment classification results for English. LI - the initial
lexicon, E - emoticons, A, R, V, N are adjectives, adverbs, verbs, nouns from LB.
</figureCaption>
<bodyText confidence="0.670341">
are positive and negative polarity scores estimated
</bodyText>
<equation confidence="0.959525">
using Eq.2 - 3 such as: w+ = p(+SM) + p(+SF) and
w− = p(−SM) + p(−SF).
</equation>
<bodyText confidence="0.999506333333333">
The gender-dependent rule-based classifiers are
defined in a similar way. Specifically, fÑ is replaced
by ÑfM and ÑfF in Eq.5 and Ñf−, Ñf+ are replaced
by ÑfM−, ÑfF− and ÑfM+, ÑfF+ respectively in Eq.6.
We learn subjectivity sÑ and polarity pÑ score vectors
using Eq.1-3. The difference between GInd and
GDep models is that GInd scores Ñw, Ñw+ and Ñw−
are not conditioned on gender.
For gender-independent classification using su-
pervised models, we build feature vectors using lex-
ical features V represented as term frequencies, to-
gether with set-count features from the lexicons:
</bodyText>
<equation confidence="0.97864975">
ÑfGInd
subj = [LI, LB, E, V ];
ÑfGInd
pol = [L+I , L+B,E+,L−I ,L−B,E−,V ].
</equation>
<bodyText confidence="0.99880875">
Finally, for gender-dependent supervised models,
we try different feature combinations. (A) We ex-
tract set-count features for gender-dependent subjec-
tive terms from LI, LB, and E jointly:
</bodyText>
<equation confidence="0.735823307692308">
ÑfGDep−J subj = [LM I , LM B , EM, LF I , LF B, EF , V ];
ÑfDep−J
pol = [LM+
I ,LM+
B , EM+, LF+
I , LF+
B , EF+
LM−
I ,LM−
B ,EM−,LF −
I ,LF −
B ,EF −,V ].
(B) We extract disjoint (prefixed) gender-specific
</equation>
<bodyText confidence="0.962771166666667">
features (in addition to lexical features V ) by rely-
ing only on female set-count features when classify-
ing female tweets; and only male set-count features
for male tweets. We refer to the joint features as
GInd−J and GDep−J, and to the disjoint features
GInd − D and GDep − D.
</bodyText>
<subsectionHeader confidence="0.754811">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999794692307692">
Figures 4a and 4b show performance improvements
for subjectivity and polarity classification under the
rule-based approach when taking into account gen-
der. The left figure shows precision-recall curves
for subjective vs. neutral classification, and the mid-
dle figure shows precision-recall curves for positive
vs. negative classification. We measure performance
starting with features from LI, and then incremen-
tally add emoticon features E and features from LB
one part of speech at a time to show the contribution
of each part of speech for sentiment classification.12
This experiment shows that there is a clear improve-
ment for the models parameterized with gender, at
least for the simple, rule-based model.
For the supervised models we experiment with
a variety of learners for English to show that gen-
der differences in subjective language improve sen-
timent classification for many learning algorithms.
We present the results in Figure 4c. For subjectiv-
ity classification, Support Vector Machines (SVM),
Naive Bayes (NB) and Bayesian Logistic Regres-
sion (BLR) achieve the best results, with improve-
ments in F-measure ranging from 0.5 - 5%. The po-
larity classifiers overall achieve much higher scores,
with improvements for GDep features ranging from
1-2%. BLR with Gaussian prior is the top scorer
</bodyText>
<note confidence="0.428831">
12POS from the Twitter POSTagger (Gimpel et al., 2011).
</note>
<page confidence="0.969195">
1822
</page>
<table confidence="0.989974909090909">
P R F A Arand P R F A Arand
English subj vs. neutral p(subj)=0.57 English pos vs. neg p(pos)=0.63
GIndLR 0.62 0.58 0.60 0.66 – 0.78 0.83 0.80 0.71 –
GDep − J 0.64 0.62 0.63 0.68 0.66 0.80 0.83 0.82 0.73 0.70
ΔR, % +3.23 +6.90 +5.00 +3.03 3.03↓ +2.56 0.00 +2.50 +2.82 4.29↓
GIndSV M 0.66 0.70 0.68 0.72 – 0.79 0.86 0.82 0.77 –
GDep − D 0.66 0.71 0.68 0.72 0.70 0.80 0.87 0.83 0.78 0.76
ΔR, % –0.45 +0.71 0.00 –0.14 2.85↓ +0.38 +0.23 +0.24 +0.41 2.63↓
Spanish subj vs. neutral p(subj)=0.40 Spanish pos vs. neg p(pos)=0.45
GIndLL 0.67 0.71 0.68 0.61 – 0.71 0.63 0.67 0.71 –
GDep − J 0.67 0.72 0.69 0.62 0.61 0.72 0.65 0.68 0.71 0.68
ΔR,% 0.00 +1.40 +0.58 +0.73 1.64↓ +2.53 +3.17 +1.49 0.00 4.41↓
GIndSV M 0.68 0.79 0.73 0.65 – 0.66 0.65 0.65 0.69 –
GDep − D 0.68 0.79 0.73 0.66 0.65 0.68 0.67 0.67 0.71 0.68
ΔR,% +0.35 +0.21 +0.26 +0.54 1.54↓ +2.43 +2.44 +2.51 +2.08 4.41↓
Russian subj vs. neutral p(subj)=0.51 Russian pos vs. neg p(pos)=0.58
GIndLR 0.66 0.68 0.67 0.67 – 0.66 0.72 0.69 0.62 –
GDep − J 0.66 0.69 0.68 0.67 0.66 0.68 0.73 0.70 0.64 0.63
ΔR,% 0.00 +1.47 +0.75 0.00 1.51↓ +3.03 +1.39 +1.45 +3.23 1.58↓
GIndSV M 0.67 0.75 0.71 0.70 – 0.64 0.73 0.68 0.62 –
GDep − D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62
ΔR,% –0.30 +1.46 +0.56 +0.14 1.44↓ +0.93 +1.92 +1.46 +1.49 1.61↓
</table>
<tableCaption confidence="0.9796955">
Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjoint
features for Logistic Regression (LR) and SVM models.
</tableCaption>
<bodyText confidence="0.997735729166667">
for polarity classification with an F-measure of 82%.
We test our results for statistical significance us-
ing McNemar’s Chi-squared test (p-value &lt; 0.01) as
suggested by Dietterich (1998). Only three classi-
fiers, J48, AdaBoostM1 (AB) and Random Forest
(RF) do not always show significant improvements
for GDep features over GInd features. However,
for the majority of classifiers, GDep models outper-
form GInd models for both tasks, demonstrating the
robustness of GDep features for sentiment analysis.
In Table 6 we report results for subjectivity and
polarity classification using the best performing
classifiers (as shown in Figure 4c) :
- Logistic Regression (LR) (Genkin et al., 2007)
for GInd − J and GDep − J models.
- SVM model with radial-based kernel for
GInd − D and GDep − D models. We use
LibSVM implementation (EL-Manzalawy and
Honavar, 2005).
Each ΔR(%) row shows the relative percent im-
provements in terms of precision P, recall R, F-
measure F and accuracy A for GDep compared to
GInd models. Our results show that differences in
subjective language across genders can be exploited
to improve sentiment analysis, not only for English
but for multiple languages. For Spanish and Russian
results are lower for subjectivity classification, we
suspect, because lexical features V are already in-
flected for gender and set-count features are down-
weighted by the classifier. For polarity classifica-
tion, on the other hand, gender-dependent features
provide consistent, significant improvements (1.5-
2.5%) across all languages.
As a reality check, Table 6 also reports accuracies
(in Anand columns) for experiments that use random
permutations of male and female subjective terms,
which are then encoded as gender-dependent set-
count features as before. We found that all gender-
dependent models, GDep − J and GDep − D, out-
performed their random equivalents for both subjec-
tivity and polarity classification (as reflected by rel-
ative accuracy decrease ↓ for Anand compared to A).
These results further confirm the existence of gen-
der bias in subjective language for any of our three
languages and its importance for sentiment analysis.
Finally, we check whether encoding gender as
a binary feature would be sufficient to improve
sentiment classification. For that we encode fea-
</bodyText>
<page confidence="0.9364">
1823
</page>
<table confidence="0.937559428571428">
English Spanish Russian
P R P R P R
0.73 0.93 0.68 0.63 0.66 0.74
0.72 0.94 0.69 0.64 0.66 0.74
0.78 0.83 0.71 0.63 0.66 0.72
0.69 0.93 0.71 0.62 0.65 0.76
0.80 0.83 0.72 0.65 0.68 0.73
</table>
<tableCaption confidence="0.996017666666667">
Table 7: Precision and recall results for polarity classifi-
cation: encoding gender as a binary feature vs. gender-
dependent features GDep − J.
</tableCaption>
<bodyText confidence="0.9993775">
tures such as: (a) unigram term frequencies V , (b)
term frequencies and gender binary V + GBin, (c)
gender-independent GInd, (d) gender-independent
and gender binary GBin + GInd, and (e) gender-
dependent GDep − J. We train logistic-regression
model for polarity classification and report precision
and recall results in Table 7. We observe that includ-
ing gender as a binary feature does not yield signif-
icant improvements compared to GDep − J for all
three languages.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999982333333334">
We presented a qualitative and empirical study that
analyses substantial and interesting differences in
subjective language between male and female users
in Twitter, including hashtag and emoticon usage
across cultures. We showed that incorporating au-
thor gender as a model component can significantly
improve subjectivity and polarity classification for
English (2.5% and 5%), Spanish (1.5% and 1%) and
Russian (1.5% and 1%). In future work we plan to
develop new models for joint modeling of personal-
ized sentiment, user demographics e.g., age and user
preferences e.g., political favorites in social media.
</bodyText>
<sectionHeader confidence="0.986913" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999778">
The authors thank the anonymous reviewers for
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998842949152542">
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and sentiment
analysis of modern standard Arabic. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, pages 587–591.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media (LSM’11), pages 30–38.
Shlomo Argamon, Moshe Koppel, James W. Pen-
nebaker, and Jonathan Schler. 2007. Min-
ing the blogosphere: Age, gender and the va-
rieties of self-expression. First Monday, 12(9).
http://www.firstmonday.org/ojs/index.php/fm/article/
view/2003/1878.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10),
pages 2200–2204.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in Twitter: styles, stances, and so-
cial networks. Computing Research Repository.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008.
A bootstrapping method for building subjectivity lex-
icons for languages with scarce resources. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC’08), pages
2764–2767.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING’10),
pages 36–44.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media (LSM’12), pages 65–74.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM’10), pages 1833–1836.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the 13th International Conference on Discovery
Science (DS’10), pages 1–15.
Bonka Boneva, Robert Kraut, and David Frohlich. 2001.
Using email for personal relationships: The differ-
ence gender makes. American Behavioral Scientist,
45(3):530–549.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on Twit-
ter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301–1309.
</reference>
<page confidence="0.953318">
1824
</page>
<reference confidence="0.999199933962264">
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virgílio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the17th Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD’11), pages 150–158.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from Twitter. In Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM’12), pages 50–57.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for prod-
uct meta-domain. In Proceedings of the 25rd In-
ternational Conference on Computational Linguistics
(COLING’12), pages 593–610.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC’04), pages 69–71.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA’10), pages 7–13.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING’10), pages 241–249.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895–1923.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP’10), pages 1277–1287.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environment.
http://www.cs.iastate.edu/ yasser/wlsvm.
Teng-Kai Fan and Chia-Hui Chang. 2009. Sentiment-
oriented contextual advertising. Advances in Informa-
tion Retrieval, 5478:202–215.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710–718.
David Gefen and Catherine M. Ridings. 2005. If you
spoke as she does, sir, instead of the way you do: a so-
ciolinguistics perspective of gender differences in vir-
tual communities. SIGMIS Database, 36(2):78–92.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49:291–304.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42–47.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: telephone speech corpus
for research and development. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP’92), pages 517–520.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Proceedings of AAAI Conference on Weblogs
and Social Media, pages 214–217.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploratory Newsletter, 11(1):10–18.
Janet Holmes and Miriam Meyerhoff. 2004. The Hand-
book of Language and Gender. Blackwell Publishing.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 151–160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP’07), pages 1075–1083.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM’11), pages 538–541.
Guangxia Li, Steven Hoi, Kuiyu Chang, and Ramesh
Jain. 2010. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
IEEE 10th International Conference on Data Mining
(ICDM’10), pages 893–898.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre senti-
ment analysis. In Proceedings of the 26th Pacific Asia
</reference>
<page confidence="0.829125">
1825
</page>
<reference confidence="0.999707733333334">
Conference on Language,Information and Computa-
tion (PACLIC’12), pages 27–136.
Ronald Macaulay. 2006. Pure grammaticalization: The
development of a teenage intensifier. Language Varia-
tion and Change, 18(03):267–283.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2012.
Multilingual subjectivity and sentiment analysis. In
Proceedings of the Association for Computational Lin-
guistics (ACL’12).
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41.
Saif Mohammad and Tony Yang. 2011. Tracking senti-
ment in mail: How genders differ on emotional axes.
In Proceedings of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA’11), pages 70–79.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’10), pages 207–217.
Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science, pages 1–7.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309–319.
Jahna Otterbacher. 2010. Inferring gender of movie re-
viewers: exploiting writing style, content and meta-
data. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM’10), pages 369–378.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10),
pages 1320–1326.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in Spanish.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC’12),
pages 3077–3081.
Rosalind W. Picard. 1997. Affective computing. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL’09),
pages 675–682.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Contents
(SMUC’10), pages 37–44.
Philip Resnik. 2013. Getting real(-time) with live
polling. http://vimeo.com/68210812.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP’03), pages 105–
112.
Harold Schiffman. 2002. Bibliography of gender and
language. http://ccat.sas.upenn.edu/ haroldfs/popcult/
bibliogs/gender/genbib.htm.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP’08), pages 254–263.
Will Styler. 2011. The EnronSent Corpus.
Technical report, University of Colorado
at Boulder Institute of Cognitive Science.
http://verbs.colorado.edu/enronsent/.
Jane Sunderland, Ren-Feng Duann, and Paul
Bake. 2002. Gender and genre bibliography.
www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267–307.
Sali A. Tagliamonte. 2006. Analysing Sociolinguistic
Variation. Cambridge University Press, 1st. Edition.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th International Conference on Knowledge Dis-
covery and Data Mining (KDD’11), pages 1397–1405.
Mike Thelwall, David Wilkinson, and Sukhvinder Uppal.
2010. Data mining emotion in social network com-
munication: Gender differences in MySpace. Journal
of the American Society for Information Science and
Technology, 61(1):190–199.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL’02), pages 417–424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Proceedings of
</reference>
<page confidence="0.840552">
1826
</page>
<reference confidence="0.999503766666667">
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL’10), pages 777–785.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual Twitter
streams. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL’13), pages 505–510.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM’11), pages 1031–1040.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence (AAAI’00, pages 735–740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP’05), pages 347–354.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP’11), pages
172–182.
</reference>
<page confidence="0.994247">
1827
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.016141">
<title confidence="0.69626">Exploring Demographic Language Variations to Improve Sentiment Analysis in Social Media Svitlana Center for Language Speech</title>
<author confidence="0.578881">Johns Hopkins</author>
<affiliation confidence="0.575504">Baltimore,</affiliation>
<email confidence="0.999359">svitlana@jhu.edu</email>
<author confidence="0.8024685">Theresa Human Language</author>
<affiliation confidence="0.8904305">Center of Johns Hopkins</affiliation>
<address confidence="0.634137">Baltimore,</address>
<email confidence="0.99977">taw@jhu.edu</email>
<author confidence="0.703698">David</author>
<affiliation confidence="0.730630666666667">Center for Language Speech Johns Hopkins</affiliation>
<address confidence="0.46885">Baltimore,</address>
<email confidence="0.999912">yarowsky@cs.jhu.edu</email>
<abstract confidence="0.992762526315789">Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Muhammad Abdul-Mageed</author>
<author>Mona T Diab</author>
<author>Mohammed Korayem</author>
</authors>
<title>Subjectivity and sentiment analysis of modern standard Arabic.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>587--591</pages>
<marker>Abdul-Mageed, Diab, Korayem, 2011</marker>
<rawString>Muhammad Abdul-Mageed, Mona T. Diab, and Mohammed Korayem. 2011. Subjectivity and sentiment analysis of modern standard Arabic. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, pages 587–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media (LSM’11),</booktitle>
<pages>30--38</pages>
<contexts>
<context position="5422" citStr="Agarwal et al., 2011" startWordPosition="833" endWordPosition="836">hone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users ex</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media (LSM’11), pages 30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Moshe Koppel</author>
<author>James W Pennebaker</author>
<author>Jonathan Schler</author>
</authors>
<title>Mining the blogosphere: Age, gender and the varieties of self-expression.</title>
<date>2007</date>
<journal>First Monday,</journal>
<volume>12</volume>
<issue>9</issue>
<note>http://www.firstmonday.org/ojs/index.php/fm/article/ view/2003/1878.</note>
<contexts>
<context position="6494" citStr="Argamon et al., 2007" startWordPosition="1001" endWordPosition="1004">endent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective language can be exploited to improve sentiment c</context>
</contexts>
<marker>Argamon, Koppel, Pennebaker, Schler, 2007</marker>
<rawString>Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and Jonathan Schler. 2007. Mining the blogosphere: Age, gender and the varieties of self-expression. First Monday, 12(9). http://www.firstmonday.org/ojs/index.php/fm/article/ view/2003/1878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2200--2204</pages>
<contexts>
<context position="12244" citStr="Baccianella et al., 2010" startWordPosition="1930" endWordPosition="1934">data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI. On each iteration, tweets in the unlabeled data are labeled using the current lexicon. If a tweet contains one or more terms from the lexicon it is ma</context>
<context position="14932" citStr="Baccianella et al., 2010" startWordPosition="2359" endWordPosition="2362">rms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lexicons, we compare our Twitter-specific lexicons LB English Spanish Russian LE LE LS LS LR LR I B I B I B Pos 2.3 16.8 2.9 7.7 1.4 5.3 Neg 2.8 4.7 5.2 14.6 2.3 5.5 Total 5.1 21.5 8.1 22.3 3.7 10.8 Table 2: The initial LI and the bootstrapped LB (highlighted) lexicon term count (LI ⊂ LB) with polarity across languages (thousands). to the corresponding initial lexicons LI and the existing state-of-the-art subjective lexicons including: • 8K strongly subjective English terms from SentiWordNet XE (Baccianella et al., 2010); • 1.5K full strength terms from the Spanish sentiment lexicon XS (Perez-Rosas et al., 2012); • 5K terms from the Russian sentiment lexicon X� (Chetviorkin and Loukachevitch, 2012). For that we apply rule-based subjectivity classification on the test data.4 This subjectivity classifier predicts that a tweet is subjective if it contains at least one, or at least two subjective terms from the lexicon. To make a fair comparison, we automatically expand XE with plurals and inflectional forms, XS with the inflectional forms for verbs, and X� with the inflectional forms for adverbs, adjectives and </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Jacob Eisenstein</author>
<author>Tyler Schnoebelen</author>
</authors>
<title>Gender in Twitter: styles, stances, and social networks.</title>
<date>2012</date>
<journal>Computing Research Repository.</journal>
<contexts>
<context position="1820" citStr="Bamman et al., 2012" startWordPosition="256" endWordPosition="259">for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we believe that sentiment analysis guided by user demographics is a very impor</context>
<context position="22592" citStr="Bamman et al., 2012" startWordPosition="3702" endWordPosition="3705">ed with subjective language, that are most strongly genderinfluenced. Analogously to Δp+ values in Table 4, a plus (+) means the hashtag is more likely to be used positively by women, and a minus (−) means the hashtag is more likely to be used positively by men. For example, in English we found that male users tend to express positive sentiment in tweets mentioning #baseball, while women tend to be negative about this hashtag. The opposite is true for the hashtag #parenting. 4.4 Emoticons We investigate how emoticons are used differently by men and women in social media following the work by (Bamman et al., 2012). For that we rely on the lists of emoticons from Wikipedia7 and present the cross-cultural and gender emoticon differences in Figure 3. The frequency of each emoticon is given 7List of emoticons from Wikipedia http://en. wikipedia.org/wiki/List_of_emoticons on the right of each language chart, with probability of use by a male user in that language given on the x-axis. The top 8 emoticons are the same across languages and sorted by English frequency. We found that emoticons in English data are used more overall by female users, which is consistent with previous findings in Schnoebelen’s work.</context>
</contexts>
<marker>Bamman, Eisenstein, Schnoebelen, 2012</marker>
<rawString>David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2012. Gender in Twitter: styles, stances, and social networks. Computing Research Repository.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>A bootstrapping method for building subjectivity lexicons for languages with scarce resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<pages>2764--2767</pages>
<contexts>
<context position="11888" citStr="Banea et al., 2008" startWordPosition="1878" endWordPosition="1881">y training data for sentiment analysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the c</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008. A bootstrapping method for building subjectivity lexicons for languages with scarce resources. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), pages 2764–2767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on Twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>36--44</pages>
<contexts>
<context position="5239" citStr="Barbosa and Feng, 2010" startWordPosition="801" endWordPosition="804">ender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., </context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on Twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific Twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media (LSM’12),</booktitle>
<pages>65--74</pages>
<contexts>
<context position="8317" citStr="Bergsma et al., 2012" startWordPosition="1290" endWordPosition="1293">r sentiment classification. For English, we download tweets from the corpus created by Burger et al. (2011). This dataset contains 2,958,103 tweets from 184K users, excluding retweets. Retweets are omitted because our focus is on the sentiment of the person tweeting; in retweets, the words originate from a different user. All users in this corpus have gender labels, which Burger et al. automatically extracted from self-reported gender on Facebook or MySpace profiles linked to by the Twitter users. English tweets are identified using a compression-based language identification (LID) 1816 tool (Bergsma et al., 2012). According to LID, there are 1,881,620 (63.6%) English tweets from which we select a random, gender-balanced sample of 0.8M tweets. Burger’s corpus does not include Russian and Spanish data on the same scale as English. Therefore, for Russian and Spanish we construct a new Twitter corpus by downloading tweets from followers of region-specific news and media Twitter feeds. We use LID to identify Russian and Spanish tweets, and remove retweets as before. In this data, gender is labeled automatically based on user first and last name morphology with a precision above 0.98 for all languages. Sent</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific Twitter collections. In Proceedings of the Second Workshop on Language in Social Media (LSM’12), pages 65–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: Is brevity an advantage?</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10),</booktitle>
<pages>1833--1836</pages>
<contexts>
<context position="5269" citStr="Bermingham and Smeaton, 2010" startWordPosition="805" endWordPosition="808">es in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and i</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: Is brevity an advantage? In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10), pages 1833–1836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in Twitter streaming data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13th International Conference on Discovery Science (DS’10),</booktitle>
<pages>1--15</pages>
<contexts>
<context position="5316" citStr="Bifet and Frank, 2010" startWordPosition="813" endWordPosition="816">cs (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective term</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in Twitter streaming data. In Proceedings of the 13th International Conference on Discovery Science (DS’10), pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonka Boneva</author>
<author>Robert Kraut</author>
<author>David Frohlich</author>
</authors>
<title>Using email for personal relationships: The difference gender makes.</title>
<date>2001</date>
<journal>American Behavioral Scientist,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="1725" citStr="Boneva et al., 2001" startWordPosition="240" endWordPosition="243">gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Conno</context>
</contexts>
<marker>Boneva, Kraut, Frohlich, 2001</marker>
<rawString>Bonka Boneva, Robert Kraut, and David Frohlich. 2001. Using email for personal relationships: The difference gender makes. American Behavioral Scientist, 45(3):530–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1301--1309</pages>
<contexts>
<context position="6449" citStr="Burger et al., 2011" startWordPosition="993" endWordPosition="996">e, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective lan</context>
<context position="7803" citStr="Burger et al. (2011)" startWordPosition="1211" endWordPosition="1214">tion for the domain of social media. 3 Data For the experiments in this paper, we use three sets of data for each language: a large pool of data (800K tweets) labeled for gender but unlabeled for sentiment, plus 2K development data and 2K test data labeled for both sentiment and gender. We use the unlabeled data to bootstrap Twitter-specific lexicons and investigate gender differences in the use of subjective language. We use the development data for parameter tuning while bootstrapping, and the test data for sentiment classification. For English, we download tweets from the corpus created by Burger et al. (2011). This dataset contains 2,958,103 tweets from 184K users, excluding retweets. Retweets are omitted because our focus is on the sentiment of the person tweeting; in retweets, the words originate from a different user. All users in this corpus have gender labels, which Burger et al. automatically extracted from self-reported gender on Facebook or MySpace profiles linked to by the Twitter users. English tweets are identified using a compression-based language identification (LID) 1816 tool (Bergsma et al., 2012). According to LID, there are 1,881,620 (63.6%) English tweets from which we select a </context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John C. Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Henrique Calais Guerra</author>
<author>Adriano Veloso</author>
<author>Wagner Meira Jr</author>
<author>Virgílio Almeida</author>
</authors>
<title>From bias to opinion: a transfer-learning approach to real-time sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the17th International Conference on Knowledge Discovery and Data Mining (KDD’11),</booktitle>
<pages>150--158</pages>
<contexts>
<context position="5469" citStr="Guerra et al., 2011" startWordPosition="842" endWordPosition="845"> 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. </context>
</contexts>
<marker>Guerra, Veloso, Jr, Almeida, 2011</marker>
<rawString>Pedro Henrique Calais Guerra, Adriano Veloso, Wagner Meira Jr, and Virgílio Almeida. 2011. From bias to opinion: a transfer-learning approach to real-time sentiment analysis. In Proceedings of the17th International Conference on Knowledge Discovery and Data Mining (KDD’11), pages 150–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Chen</author>
<author>Wenbo Wang</author>
<author>Meenakshi Nagarajan</author>
<author>Shaojun Wang</author>
<author>Amit P Sheth</author>
</authors>
<title>Extracting diverse sentiment expressions with target-dependent polarity from Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM’12),</booktitle>
<pages>50--57</pages>
<contexts>
<context position="5506" citStr="Chen et al., 2012" startWordPosition="850" endWordPosition="853">mad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective</context>
</contexts>
<marker>Chen, Wang, Nagarajan, Wang, Sheth, 2012</marker>
<rawString>Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P. Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from Twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media (ICWSM’12), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilia Chetviorkin</author>
<author>Natalia V Loukachevitch</author>
</authors>
<title>Extraction of Russian sentiment lexicon for product meta-domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the 25rd International Conference on Computational Linguistics (COLING’12),</booktitle>
<pages>593--610</pages>
<contexts>
<context position="15113" citStr="Chetviorkin and Loukachevitch, 2012" startWordPosition="2388" endWordPosition="2391">ussian LE LE LS LS LR LR I B I B I B Pos 2.3 16.8 2.9 7.7 1.4 5.3 Neg 2.8 4.7 5.2 14.6 2.3 5.5 Total 5.1 21.5 8.1 22.3 3.7 10.8 Table 2: The initial LI and the bootstrapped LB (highlighted) lexicon term count (LI ⊂ LB) with polarity across languages (thousands). to the corresponding initial lexicons LI and the existing state-of-the-art subjective lexicons including: • 8K strongly subjective English terms from SentiWordNet XE (Baccianella et al., 2010); • 1.5K full strength terms from the Spanish sentiment lexicon XS (Perez-Rosas et al., 2012); • 5K terms from the Russian sentiment lexicon X� (Chetviorkin and Loukachevitch, 2012). For that we apply rule-based subjectivity classification on the test data.4 This subjectivity classifier predicts that a tweet is subjective if it contains at least one, or at least two subjective terms from the lexicon. To make a fair comparison, we automatically expand XE with plurals and inflectional forms, XS with the inflectional forms for verbs, and X� with the inflectional forms for adverbs, adjectives and verbs. We report precision, recall and Fmeasure results in Table 3 and show that our bootstrapped lexicons outperform the corresponding initial lexicons and the external resources. </context>
</contexts>
<marker>Chetviorkin, Loukachevitch, 2012</marker>
<rawString>Ilia Chetviorkin and Natalia V. Loukachevitch. 2012. Extraction of Russian sentiment lexicon for product meta-domain. In Proceedings of the 25rd International Conference on Computational Linguistics (COLING’12), pages 593–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The Fisher corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<pages>69--71</pages>
<contexts>
<context position="4833" citStr="Cieri et al., 2004" startWordPosition="735" endWordPosition="738">derdependent sentiment terms that is needed for statistically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et a</context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The Fisher corpus: a resource for the next generations of speech-to-text. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), pages 69–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Clematide</author>
<author>Manfred Klenner</author>
</authors>
<title>Evaluation and extension of a polarity lexicon for German.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA’10),</booktitle>
<pages>7--13</pages>
<contexts>
<context position="12293" citStr="Clematide and Klenner (2010)" startWordPosition="1937" endWordPosition="1940">sure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI. On each iteration, tweets in the unlabeled data are labeled using the current lexicon. If a tweet contains one or more terms from the lexicon it is marked subjective, otherwise neutral. Tweet polarit</context>
</contexts>
<marker>Clematide, Klenner, 2010</marker>
<rawString>Simon Clematide and Manfred Klenner. 2010. Evaluation and extension of a polarity lexicon for German. In Proceedings of the Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA’10), pages 7–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using Twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<pages>241--249</pages>
<contexts>
<context position="5338" citStr="Davidov et al., 2010" startWordPosition="817" endWordPosition="820">nderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russi</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using Twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), pages 241–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="32034" citStr="Dietterich (1998)" startWordPosition="5314" endWordPosition="5315"> 0.73 0.70 0.64 0.63 ΔR,% 0.00 +1.47 +0.75 0.00 1.51↓ +3.03 +1.39 +1.45 +3.23 1.58↓ GIndSV M 0.67 0.75 0.71 0.70 – 0.64 0.73 0.68 0.62 – GDep − D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62 ΔR,% –0.30 +1.46 +0.56 +0.14 1.44↓ +0.93 +1.92 +1.46 +1.49 1.61↓ Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjoint features for Logistic Regression (LR) and SVM models. for polarity classification with an F-measure of 82%. We test our results for statistical significance using McNemar’s Chi-squared test (p-value &lt; 0.01) as suggested by Dietterich (1998). Only three classifiers, J48, AdaBoostM1 (AB) and Random Forest (RF) do not always show significant improvements for GDep features over GInd features. However, for the majority of classifiers, GDep models outperform GInd models for both tasks, demonstrating the robustness of GDep features for sentiment analysis. In Table 6 we report results for subjectivity and polarity classification using the best performing classifiers (as shown in Figure 4c) : - Logistic Regression (LR) (Genkin et al., 2007) for GInd − J and GDep − J models. - SVM model with radial-based kernel for GInd − D and GDep − D m</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’10),</booktitle>
<pages>1277--1287</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’10), pages 1277–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasser EL-Manzalawy</author>
<author>Vasant Honavar</author>
</authors>
<title>WLSVM: Integrating LibSVM into Weka Environment.</title>
<date>2005</date>
<note>http://www.cs.iastate.edu/ yasser/wlsvm.</note>
<contexts>
<context position="32702" citStr="EL-Manzalawy and Honavar, 2005" startWordPosition="5421" endWordPosition="5424">stM1 (AB) and Random Forest (RF) do not always show significant improvements for GDep features over GInd features. However, for the majority of classifiers, GDep models outperform GInd models for both tasks, demonstrating the robustness of GDep features for sentiment analysis. In Table 6 we report results for subjectivity and polarity classification using the best performing classifiers (as shown in Figure 4c) : - Logistic Regression (LR) (Genkin et al., 2007) for GInd − J and GDep − J models. - SVM model with radial-based kernel for GInd − D and GDep − D models. We use LibSVM implementation (EL-Manzalawy and Honavar, 2005). Each ΔR(%) row shows the relative percent improvements in terms of precision P, recall R, Fmeasure F and accuracy A for GDep compared to GInd models. Our results show that differences in subjective language across genders can be exploited to improve sentiment analysis, not only for English but for multiple languages. For Spanish and Russian results are lower for subjectivity classification, we suspect, because lexical features V are already inflected for gender and set-count features are downweighted by the classifier. For polarity classification, on the other hand, gender-dependent features</context>
</contexts>
<marker>EL-Manzalawy, Honavar, 2005</marker>
<rawString>Yasser EL-Manzalawy and Vasant Honavar, 2005. WLSVM: Integrating LibSVM into Weka Environment. http://www.cs.iastate.edu/ yasser/wlsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teng-Kai Fan</author>
<author>Chia-Hui Chang</author>
</authors>
<title>Sentimentoriented contextual advertising.</title>
<date>2009</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>5478--202</pages>
<contexts>
<context position="2171" citStr="Fan and Chang, 2009" startWordPosition="309" endWordPosition="312">onte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we believe that sentiment analysis guided by user demographics is a very important direction for research. In this paper, we focus on gender demographics and language in social media to investigate differences in the language used to express opinions in Twitter for three languages: English, Spanish, and Russian. We focus on Twitter data because of its volume, dynamic nature, and diverse population worldwide.1 We find that som</context>
</contexts>
<marker>Fan, Chang, 2009</marker>
<rawString>Teng-Kai Fan and Chia-Hui Chang. 2009. Sentimentoriented contextual advertising. Advances in Information Retrieval, 5478:202–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="6428" citStr="Garera and Yarowsky, 2009" startWordPosition="989" endWordPosition="992">differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the </context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 710–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Gefen</author>
<author>Catherine M Ridings</author>
</authors>
<title>If you spoke as she does, sir, instead of the way you do: a sociolinguistics perspective of gender differences in virtual communities.</title>
<date>2005</date>
<journal>SIGMIS Database,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="1498" citStr="Gefen and Ridings, 2005" startWordPosition="208" endWordPosition="211">s in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as person</context>
</contexts>
<marker>Gefen, Ridings, 2005</marker>
<rawString>David Gefen and Catherine M. Ridings. 2005. If you spoke as she does, sir, instead of the way you do: a sociolinguistics perspective of gender differences in virtual communities. SIGMIS Database, 36(2):78–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Genkin</author>
<author>David D Lewis</author>
<author>David Madigan</author>
</authors>
<title>Large-scale Bayesian logistic regression for text categorization.</title>
<date>2007</date>
<tech>Technometrics,</tech>
<pages>49--291</pages>
<contexts>
<context position="32535" citStr="Genkin et al., 2007" startWordPosition="5389" endWordPosition="5392">esults for statistical significance using McNemar’s Chi-squared test (p-value &lt; 0.01) as suggested by Dietterich (1998). Only three classifiers, J48, AdaBoostM1 (AB) and Random Forest (RF) do not always show significant improvements for GDep features over GInd features. However, for the majority of classifiers, GDep models outperform GInd models for both tasks, demonstrating the robustness of GDep features for sentiment analysis. In Table 6 we report results for subjectivity and polarity classification using the best performing classifiers (as shown in Figure 4c) : - Logistic Regression (LR) (Genkin et al., 2007) for GInd − J and GDep − J models. - SVM model with radial-based kernel for GInd − D and GDep − D models. We use LibSVM implementation (EL-Manzalawy and Honavar, 2005). Each ΔR(%) row shows the relative percent improvements in terms of precision P, recall R, Fmeasure F and accuracy A for GDep compared to GInd models. Our results show that differences in subjective language across genders can be exploited to improve sentiment analysis, not only for English but for multiple languages. For Spanish and Russian results are lower for subjectivity classification, we suspect, because lexical features </context>
</contexts>
<marker>Genkin, Lewis, Madigan, 2007</marker>
<rawString>Alexander Genkin, David D. Lewis, and David Madigan. 2007. Large-scale Bayesian logistic regression for text categorization. Technometrics, 49:291–304.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP’92),</booktitle>
<pages>517--520</pages>
<contexts>
<context position="4856" citStr="Godfrey et al., 1992" startWordPosition="739" endWordPosition="742">nt terms that is needed for statistically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: telephone speech corpus for research and development. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP’92), pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumit Goswami</author>
<author>Sudeshna Sarkar</author>
<author>Mayur Rustagi</author>
</authors>
<title>Stylometric analysis of bloggers age and gender.</title>
<date>2009</date>
<booktitle>In Proceedings of AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>214--217</pages>
<contexts>
<context position="6471" citStr="Goswami et al., 2009" startWordPosition="997" endWordPosition="1000">er-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective language can be exploited</context>
</contexts>
<marker>Goswami, Sarkar, Rustagi, 2009</marker>
<rawString>Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers age and gender. In Proceedings of AAAI Conference on Weblogs and Social Media, pages 214–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Exploratory Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="25933" citStr="Hall et al., 2009" startWordPosition="4233" endWordPosition="4236">positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ETEST, STEST and RTEST) labeled with subjectivity (pos, neg, both vs. neut) and polarity (pos vs. neg) as described in Section 3. 5.1 Models For the rule-based GIndR b classifier, tweets are lasu beled as subjective or neutral as follows: GIndRB subj = � 1 if w� f� ≥ 0.5, (5) 0 otherwise where w� f� stands for weighted set features, e.g., terms from LI only, emoticons E, or different partof-speech tags (POS) from LB weighted using w = p(subj) = p(subjlM) + p(subjlF) subjectivity score as shown in Eq.1. We e</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Exploratory Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
<author>Miriam Meyerhoff</author>
</authors>
<title>The Handbook of Language and Gender.</title>
<date>2004</date>
<publisher>Blackwell Publishing.</publisher>
<contexts>
<context position="1526" citStr="Holmes and Meyerhoff, 2004" startWordPosition="212" endWordPosition="215">can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems a</context>
</contexts>
<marker>Holmes, Meyerhoff, 2004</marker>
<rawString>Janet Holmes and Miriam Meyerhoff. 2004. The Handbook of Language and Gender. Blackwell Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent Twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="5400" citStr="Jiang et al., 2011" startWordPosition="829" endWordPosition="832">differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.e</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of HTML documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’07),</booktitle>
<pages>1075--1083</pages>
<contexts>
<context position="11930" citStr="Kaji and Kitsuregawa, 2007" startWordPosition="1884" endWordPosition="1887">ysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of HTML documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’07), pages 1075–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM’11),</booktitle>
<pages>538--541</pages>
<contexts>
<context position="5380" citStr="Kouloumpis et al., 2011" startWordPosition="825" endWordPosition="828">earch has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here:</context>
<context position="11368" citStr="Kouloumpis et al., 2011" startWordPosition="1810" endWordPosition="1813">uld be both impractical and expensive. Instead we use large multilingual sentiment lexicons developed specifically for Twitter as described below. Using these lexicons we can begin to explore the relationship between subjective language and gender in the large pool of data labeled for gender but unlabeled for sentiment. We also look at the relationship between gender and the use of different hashtags and emoticons. These can be strong indicators of sentiment in social media, and in fact are sometimes used to create noisy training data for sentiment analysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on rel</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the OMG! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (ICWSM’11), pages 538–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangxia Li</author>
<author>Steven Hoi</author>
<author>Kuiyu Chang</author>
<author>Ramesh Jain</author>
</authors>
<title>Micro-blogging sentiment detection by collaborative online learning.</title>
<date>2010</date>
<booktitle>In Proceedings of IEEE 10th International Conference on Data Mining (ICDM’10),</booktitle>
<pages>893--898</pages>
<contexts>
<context position="5355" citStr="Li et al., 2010" startWordPosition="821" endWordPosition="824">. More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and E</context>
</contexts>
<marker>Li, Hoi, Chang, Jain, 2010</marker>
<rawString>Guangxia Li, Steven Hoi, Kuiyu Chang, and Ramesh Jain. 2010. Micro-blogging sentiment detection by collaborative online learning. In Proceedings of IEEE 10th International Conference on Data Mining (ICDM’10), pages 893–898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Li</author>
<author>Yu Chen</author>
<author>Heng Ji</author>
<author>Smaranda Muresan</author>
<author>Dequan Zheng</author>
</authors>
<title>Combining social cognitive theories with linguistic features for multi-genre sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Pacific Asia Conference on Language,Information and Computation (PACLIC’12),</booktitle>
<pages>27--136</pages>
<contexts>
<context position="5524" citStr="Li et al., 2012" startWordPosition="854" endWordPosition="857"> analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as featu</context>
</contexts>
<marker>Li, Chen, Ji, Muresan, Zheng, 2012</marker>
<rawString>Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and Dequan Zheng. 2012. Combining social cognitive theories with linguistic features for multi-genre sentiment analysis. In Proceedings of the 26th Pacific Asia Conference on Language,Information and Computation (PACLIC’12), pages 27–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Macaulay</author>
</authors>
<title>Pure grammaticalization: The development of a teenage intensifier.</title>
<date>2006</date>
<journal>Language Variation and Change,</journal>
<volume>18</volume>
<issue>03</issue>
<contexts>
<context position="1542" citStr="Macaulay, 2006" startWordPosition="216" endWordPosition="217">mprove sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted adve</context>
</contexts>
<marker>Macaulay, 2006</marker>
<rawString>Ronald Macaulay. 2006. Pure grammaticalization: The development of a teenage intensifier. Language Variation and Change, 18(03):267–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity and sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’12).</booktitle>
<marker>Mihalcea, Banea, Wiebe, 2012</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2012. Multilingual subjectivity and sentiment analysis. In Proceedings of the Association for Computational Linguistics (ACL’12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12148" citStr="Miller, 1995" startWordPosition="1917" endWordPosition="1918"> dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI. On each iteration, tweets in the unlabeled data are la</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking sentiment in mail: How genders differ on emotional axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA’11),</booktitle>
<pages>70--79</pages>
<contexts>
<context position="1750" citStr="Mohammad and Yang, 2011" startWordPosition="244" endWordPosition="247">seline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we belie</context>
<context position="4908" citStr="Mohammad and Yang (2011)" startWordPosition="747" endWordPosition="750">cant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; </context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif Mohammad and Tony Yang. 2011. Tracking sentiment in mail: How genders differ on emotional axes. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA’11), pages 70–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Improving gender classification of blog authors.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’10),</booktitle>
<pages>207--217</pages>
<contexts>
<context position="6667" citStr="Mukherjee and Liu (2010)" startWordPosition="1028" endWordPosition="1031">ons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective language can be exploited to improve sentiment classification in English or any other language. In this paper we seek to answer this question for the domain of social media. 3 Data For the experiments in this paper, we us</context>
</contexts>
<marker>Mukherjee, Liu, 2010</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2010. Improving gender classification of blog authors. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’10), pages 207–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Jacob Eisenstein</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>A mixture model of demographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of NIPS Workshop on Machine Learning in Computational Social Science,</booktitle>
<pages>1--7</pages>
<marker>O’Connor, Eisenstein, Xing, Smith, 2010</marker>
<rawString>Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and Noah A. Smith. 2010. A mixture model of demographic lexical variation. In Proceedings of NIPS Workshop on Machine Learning in Computational Social Science, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>309--319</pages>
<contexts>
<context position="2224" citStr="Ott et al., 2011" startWordPosition="317" endWordPosition="320">mographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we believe that sentiment analysis guided by user demographics is a very important direction for research. In this paper, we focus on gender demographics and language in social media to investigate differences in the language used to express opinions in Twitter for three languages: English, Spanish, and Russian. We focus on Twitter data because of its volume, dynamic nature, and diverse population worldwide.1 We find that some words are more or less likely to be positive or neg</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 309–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
</authors>
<title>Inferring gender of movie reviewers: exploiting writing style, content and metadata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10),</booktitle>
<pages>369--378</pages>
<contexts>
<context position="6544" citStr="Otterbacher (2010)" startWordPosition="1011" endWordPosition="1013">er for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective language can be exploited to improve sentiment classification in English or any other language. In</context>
</contexts>
<marker>Otterbacher, 2010</marker>
<rawString>Jahna Otterbacher. 2010. Inferring gender of movie reviewers: exploiting writing style, content and metadata. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10), pages 369–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1320--1326</pages>
<contexts>
<context position="5293" citStr="Pak and Paroubek, 2010" startWordPosition="809" endWordPosition="812">grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resou</context>
<context position="11342" citStr="Pak and Paroubek, 2010" startWordPosition="1806" endWordPosition="1809">labels for all tweets would be both impractical and expensive. Instead we use large multilingual sentiment lexicons developed specifically for Twitter as described below. Using these lexicons we can begin to explore the relationship between subjective language and gender in the large pool of data labeled for gender but unlabeled for sentiment. We also look at the relationship between gender and the use of different hashtags and emoticons. These can be strong indicators of sentiment in social media, and in fact are sometimes used to create noisy training data for sentiment analysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 1320–1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronica Perez-Rosas</author>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning sentiment lexicons in Spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>3077--3081</pages>
<contexts>
<context position="15025" citStr="Perez-Rosas et al., 2012" startWordPosition="2375" endWordPosition="2378">anded lexicons, we compare our Twitter-specific lexicons LB English Spanish Russian LE LE LS LS LR LR I B I B I B Pos 2.3 16.8 2.9 7.7 1.4 5.3 Neg 2.8 4.7 5.2 14.6 2.3 5.5 Total 5.1 21.5 8.1 22.3 3.7 10.8 Table 2: The initial LI and the bootstrapped LB (highlighted) lexicon term count (LI ⊂ LB) with polarity across languages (thousands). to the corresponding initial lexicons LI and the existing state-of-the-art subjective lexicons including: • 8K strongly subjective English terms from SentiWordNet XE (Baccianella et al., 2010); • 1.5K full strength terms from the Spanish sentiment lexicon XS (Perez-Rosas et al., 2012); • 5K terms from the Russian sentiment lexicon X� (Chetviorkin and Loukachevitch, 2012). For that we apply rule-based subjectivity classification on the test data.4 This subjectivity classifier predicts that a tweet is subjective if it contains at least one, or at least two subjective terms from the lexicon. To make a fair comparison, we automatically expand XE with plurals and inflectional forms, XS with the inflectional forms for verbs, and X� with the inflectional forms for adverbs, adjectives and verbs. We report precision, recall and Fmeasure results in Table 3 and show that our bootstra</context>
</contexts>
<marker>Perez-Rosas, Banea, Mihalcea, 2012</marker>
<rawString>Veronica Perez-Rosas, Carmen Banea, and Rada Mihalcea. 2012. Learning sentiment lexicons in Spanish. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12), pages 3077–3081.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosalind W Picard</author>
</authors>
<title>Affective computing.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1473" citStr="Picard, 1997" startWordPosition="206" endWordPosition="207">der differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of ap</context>
</contexts>
<marker>Picard, 1997</marker>
<rawString>Rosalind W. Picard. 1997. Affective computing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09),</booktitle>
<pages>675--682</pages>
<contexts>
<context position="12060" citStr="Rao and Ravichandran (2009)" startWordPosition="1903" endWordPosition="1906">.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with t</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09), pages 675–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC’10),</booktitle>
<pages>37--44</pages>
<contexts>
<context position="6189" citStr="Rao et al., 2010" startWordPosition="954" endWordPosition="957">iment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although prev</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in Twitter. In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC’10), pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Getting real(-time) with live polling.</title>
<date>2013</date>
<note>http://vimeo.com/68210812.</note>
<contexts>
<context position="2272" citStr="Resnik, 2013" startWordPosition="326" endWordPosition="327">unication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we believe that sentiment analysis guided by user demographics is a very important direction for research. In this paper, we focus on gender demographics and language in social media to investigate differences in the language used to express opinions in Twitter for three languages: English, Spanish, and Russian. We focus on Twitter data because of its volume, dynamic nature, and diverse population worldwide.1 We find that some words are more or less likely to be positive or negative in context depending on the the gender of </context>
</contexts>
<marker>Resnik, 2013</marker>
<rawString>Philip Resnik. 2013. Getting real(-time) with live polling. http://vimeo.com/68210812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’03),</booktitle>
<pages>105--112</pages>
<contexts>
<context position="13866" citStr="Riloff and Wiebe, 2003" startWordPosition="2186" endWordPosition="2189"> the criteria to add to the lexicon. The parameters are optimized using a grid search on the development data using F-measure for subjectivity classification. In Table 2 we report size and term polarity from the initial LI and the bootstrapped LB lexicons. Although more sophisticated bootstrapping methods exist, this approach has been shown to be effective for atomically learning subjectivity lexicons in multiple languages on a large scale without any external, rich, lexical resources, e.g., WordNet, or advanced NLP tools, e.g., syntactic parsers (Wiebe, 2000) or information extraction tools (Riloff and Wiebe, 2003). For English, seed terms for bootstrapping are the strongly subjective terms in the MPQA lexicon (Wilson et al., 2005). For Spanish and Russian, the seed terms are obtained by translating the English seed terms using a bi-lingual dictionary, collecting subjectivity judgments from MTurk on the translations, filtering out translations that are not strongly subjective, and expanding the resulting word lists with plurals and inflectional forms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lexicons, we compare our Twitter-specific lexicons LB Englis</context>
<context position="16847" citStr="Riloff and Wiebe, 2003" startWordPosition="2695" endWordPosition="2698">valuation With our Twitter-specific sentiment lexicons, we can now investigate how the subjective use of these terms differs depending on gender for our three languages. Figure 1 illustrates what we expect to find. {F} and {M} are the sets of subjective terms used by females and males, respectively. We expect that some terms will be used by males, but never by females, and vice-versa. The vast majority, however, will be used by both genders. Within this set of shared terms, many words will show little difference 4A similar rule-based approach using terms from the MPQA lexicon is suggested by (Riloff and Wiebe, 2003). 1818 Figure 1: Gender-dependent vs. independent subjectivity terms (+ and - indicates term polarity). Figure 2: The distribution of gender-dependent GDep and gender-independent GInd sentiment terms. in their subjective use when considering gender, but there will be some words for which gender will have an influence. Of particular interest for our work are words in which the polarity of a term as it is used in context is gender-influenced, the extreme case being terms that flip their polarity depending on the gender of the user. Polarity may be different because the concept represented by the</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’03), pages 105– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Schiffman</author>
</authors>
<title>Bibliography of gender and language.</title>
<date>2002</date>
<note>http://ccat.sas.upenn.edu/ haroldfs/popcult/ bibliogs/gender/genbib.htm.</note>
<contexts>
<context position="4714" citStr="Schiffman, 2002" startWordPosition="719" endWordPosition="720">e insufficient; rather, it is the combination of lexical features, together with set-count features representing genderdependent sentiment terms that is needed for statistically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 20</context>
</contexts>
<marker>Schiffman, 2002</marker>
<rawString>Harold Schiffman. 2002. Bibliography of gender and language. http://ccat.sas.upenn.edu/ haroldfs/popcult/ bibliogs/gender/genbib.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08), pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Styler</author>
</authors>
<title>The EnronSent Corpus.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>University of Colorado at Boulder Institute of Cognitive Science.</institution>
<note>http://verbs.colorado.edu/enronsent/.</note>
<contexts>
<context position="4882" citStr="Styler, 2011" startWordPosition="745" endWordPosition="746">tically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al.,</context>
</contexts>
<marker>Styler, 2011</marker>
<rawString>Will Styler. 2011. The EnronSent Corpus. Technical report, University of Colorado at Boulder Institute of Cognitive Science. http://verbs.colorado.edu/enronsent/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Sunderland</author>
<author>Ren-Feng Duann</author>
<author>Paul Bake</author>
</authors>
<date>2002</date>
<note>Gender and genre bibliography. www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.</note>
<contexts>
<context position="4740" citStr="Sunderland et al., 2002" startWordPosition="721" endWordPosition="724">ather, it is the combination of lexical features, together with set-count features representing genderdependent sentiment terms that is needed for statistically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; </context>
</contexts>
<marker>Sunderland, Duann, Bake, 2002</marker>
<rawString>Jane Sunderland, Ren-Feng Duann, and Paul Bake. 2002. Gender and genre bibliography. www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="25509" citStr="Taboada et al., 2011" startWordPosition="4161" endWordPosition="4164">tion approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ETEST, STEST and RTEST) labeled with subjectivity (pos, neg, both vs. neut) and polarity</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sali A Tagliamonte</author>
</authors>
<title>Analysing Sociolinguistic Variation.</title>
<date>2006</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="1562" citStr="Tagliamonte, 2006" startWordPosition="218" endWordPosition="219"> analysis, and in particular, polarity classification for Spanish and Russian. Our results show statistically significant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classification. 1 Introduction Sociolinguistics and dialectology have been studying the relationships between language and speech at the phonological, lexical and morphosyntactic levels and social identity for decades (Picard, 1997; Gefen and Ridings, 2005; Holmes and Meyerhoff, 2004; Macaulay, 2006; Tagliamonte, 2006). Recent studies have focused on exploring demographic language variations in personal email communication, blog posts, and public discussions (Boneva et al., 2001; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Cha</context>
</contexts>
<marker>Tagliamonte, 2006</marker>
<rawString>Sali A. Tagliamonte. 2006. Analysing Sociolinguistic Variation. Cambridge University Press, 1st. Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining (KDD’11),</booktitle>
<pages>1397--1405</pages>
<contexts>
<context position="5487" citStr="Tan et al., 2011" startWordPosition="846" endWordPosition="849">yler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emoticons more than male users. Other researchers </context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining (KDD’11), pages 1397–1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>David Wilkinson</author>
<author>Sukhvinder Uppal</author>
</authors>
<title>Data mining emotion in social network communication: Gender differences in MySpace.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>1</issue>
<marker>Thelwall, Wilkinson, Uppal, 2010</marker>
<rawString>Mike Thelwall, David Wilkinson, and Sukhvinder Uppal. 2010. Data mining emotion in social network communication: Gender differences in MySpace. Journal of the American Society for Information Science and Technology, 61(1):190–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL’02),</booktitle>
<pages>417--424</pages>
<contexts>
<context position="11827" citStr="Turney, 2002" startWordPosition="1871" endWordPosition="1872">al media, and in fact are sometimes used to create noisy training data for sentiment analysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for ENGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective le</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL’02), pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL’10),</booktitle>
<pages>777--785</pages>
<contexts>
<context position="25456" citStr="Velikovich et al., 2010" startWordPosition="4153" endWordPosition="4156">iment classification. We experiment with two classification approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ETEST, STEST and RTEST) labeled wit</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-derived polarity lexicons. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL’10), pages 777–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual Twitter streams.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13),</booktitle>
<pages>505--510</pages>
<contexts>
<context position="12564" citStr="Volkova et al. (2013)" startWordPosition="1971" endWordPosition="1974">based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct HINDI and FRENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a SPANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate GERMAN, RUSSIAN and ARABIC subjective lexicons. 3www.cs.pitt.edu/mpqa/opinionfinder 1817 We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI. On each iteration, tweets in the unlabeled data are labeled using the current lexicon. If a tweet contains one or more terms from the lexicon it is marked subjective, otherwise neutral. Tweet polarity is determined in a similar way, but takes into account negation. For every term not in the lexicon with a frequency threshold, the probability of that word appearing in a subjective sentence is calculated. The top k terms with a subjective probability are then added to</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual Twitter streams. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL’13), pages 505–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolong Wang</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ming Zhang</author>
</authors>
<title>Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM’11),</booktitle>
<pages>1031--1040</pages>
<contexts>
<context position="5441" citStr="Wang et al., 2011" startWordPosition="837" endWordPosition="840">al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ users express positive emot</context>
</contexts>
<marker>Wang, Wei, Liu, Zhou, Zhang, 2011</marker>
<rawString>Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and Ming Zhang. 2011. Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM’11), pages 1031–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence (AAAI’00,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="13809" citStr="Wiebe, 2000" startWordPosition="2179" endWordPosition="2180">nues until there are no more new terms meeting the criteria to add to the lexicon. The parameters are optimized using a grid search on the development data using F-measure for subjectivity classification. In Table 2 we report size and term polarity from the initial LI and the bootstrapped LB lexicons. Although more sophisticated bootstrapping methods exist, this approach has been shown to be effective for atomically learning subjectivity lexicons in multiple languages on a large scale without any external, rich, lexical resources, e.g., WordNet, or advanced NLP tools, e.g., syntactic parsers (Wiebe, 2000) or information extraction tools (Riloff and Wiebe, 2003). For English, seed terms for bootstrapping are the strongly subjective terms in the MPQA lexicon (Wilson et al., 2005). For Spanish and Russian, the seed terms are obtained by translating the English seed terms using a bi-lingual dictionary, collecting subjectivity judgments from MTurk on the translations, filtering out translations that are not strongly subjective, and expanding the resulting word lists with plurals and inflectional forms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lex</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence (AAAI’00, pages 735–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’05),</booktitle>
<pages>347--354</pages>
<contexts>
<context position="13985" citStr="Wilson et al., 2005" startWordPosition="2205" endWordPosition="2208">ure for subjectivity classification. In Table 2 we report size and term polarity from the initial LI and the bootstrapped LB lexicons. Although more sophisticated bootstrapping methods exist, this approach has been shown to be effective for atomically learning subjectivity lexicons in multiple languages on a large scale without any external, rich, lexical resources, e.g., WordNet, or advanced NLP tools, e.g., syntactic parsers (Wiebe, 2000) or information extraction tools (Riloff and Wiebe, 2003). For English, seed terms for bootstrapping are the strongly subjective terms in the MPQA lexicon (Wilson et al., 2005). For Spanish and Russian, the seed terms are obtained by translating the English seed terms using a bi-lingual dictionary, collecting subjectivity judgments from MTurk on the translations, filtering out translations that are not strongly subjective, and expanding the resulting word lists with plurals and inflectional forms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lexicons, we compare our Twitter-specific lexicons LB English Spanish Russian LE LE LS LS LR LR I B I B I B Pos 2.3 16.8 2.9 7.7 1.4 5.3 Neg 2.8 4.7 5.2 14.6 2.3 5.5 Total 5.1 21.</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’05), pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’11),</booktitle>
<pages>172--182</pages>
<contexts>
<context position="25486" citStr="Yessenalina and Cardie, 2011" startWordPosition="4157" endWordPosition="4160">experiment with two classification approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ETEST, STEST and RTEST) labeled with subjectivity (pos, neg, both</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’11), pages 172–182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
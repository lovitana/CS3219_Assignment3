<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010787">
<title confidence="0.984775">
Detecting Promotional Content in Wikipedia
</title>
<author confidence="0.990983">
Shruti Bhosale Heath Vinicombe Raymond J. Mooney
</author>
<affiliation confidence="0.9996925">
Department of Computer Science
The University of Texas at Austin
</affiliation>
<email confidence="0.999398">
{shruti,vini,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99940125">
This paper presents an approach for detecting
promotional content in Wikipedia. By incor-
porating stylometric features, including fea-
tures based on n-gram and PCFG language
models, we demonstrate improved accuracy
at identifying promotional articles, compared
to using only lexical information and meta-
features.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983">
Wikipedia is a free, collaboratively edited encyclo-
pedia. Since normally anyone can create and edit
pages, some articles are written in a promotional
tone, violating Wikipedia’s policy requiring a neu-
tral viewpoint. Currently, such articles are identified
manually and tagged with an appropriate Cleanup
message1 by Wikipedia editors. Given the scale and
rate of growth of Wikipedia, it is infeasible to man-
ually identify all such articles. Hence, we present
an approach to automatically detect promotional ar-
ticles.
Related work in quality flaw detection in
Wikipedia (Anderka et al., 2012) has relied on
meta-features based on edit history, Wikipedia links,
structural features and counts of words, sentences
and paragraphs. However, we hypothesize that there
are subtle differences in the linguistic style that dis-
tinguish promotional tone, which we attempt to cap-
ture using stylometric features, particularly deeper
syntactic features. We model the style of promo-
tional and normal articles using language models
</bodyText>
<footnote confidence="0.7792495">
1http://en.wikipedia.org/wiki/Wikipedia:
Template_messages/Cleanup
</footnote>
<bodyText confidence="0.99823375">
based on both n-grams and Probabilistic Context
Free Grammars (PCFGs). We show that using such
stylometric features improves over using only shal-
low lexical and meta-features.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999865095238095">
Anderka et al. (2012) developed a general model for
detecting ten of Wikipedia’s most frequent quality
flaws. One of these flaw types, “Advert”2, refers to
articles written like advertisements. Their classifiers
were trained using a set of lexical, structural, net-
work and edit-history related features of Wikipedia
articles. However, they used no features capturing
syntactic structure, at a level deeper than Part-Of-
Speech (POS) tags.
A related area is that of vandalism detection in
Wikipedia. Several systems have been developed
to detect vandalizing edits in Wikipedia. These fall
into two major categories: those analyzing author in-
formation and edit metadata (Wilkinson and Huber-
man, 2007; Stein and Hess, 2007); and those using
NLP techniques such as n-gram language models
and PCFGs (Wang and McKeown, 2010; Harpalani
et al., 2011). We combine relevant features from
both these categories to train a classifier that distin-
guishes promotional content from normal Wikipedia
articles.
</bodyText>
<sectionHeader confidence="0.997316" genericHeader="method">
3 Dataset Collection
</sectionHeader>
<bodyText confidence="0.9728">
We extracted a set of about 13,000 articles from
English Wikipedia’s category, “Category:All arti-
</bodyText>
<footnote confidence="0.8506095">
2“Advert” is the flaw-type of majority of the articles in the
Category ‘Articles with a promotional tone’.
</footnote>
<page confidence="0.817534">
1851
</page>
<note confidence="0.3747565">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851–1857,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.99855896">
Content Features
Number of characters
Number of words
Number of sentences
Average Word Length
Average, Minimum, Maximum Sentence Lengths,
Ratio of Maximum to minimum sentence lengths
Ratio of long sentences (&gt;48 words) to Short Sen-
tences (&lt;33 words)
Percentage of Sentences in the passive voice
Relative Frequencies of POS tags for pronouns, con-
junctions, prepositions, auxiliary verbs, modal verbs,
adjectives and adverbs
Percentage of sentences beginning with a pronoun,
article, conjunction, preposition, adjective, adverb
Percentage of special phrases3 such as peacock
terms (‘legendary’, ‘acclaimed’, ‘world-class’),
weasel terms (‘many scholars state’, ‘it is be-
lieved/regarded’, ‘many are of the opinion’, ‘most
feel’, ‘experts declare’, ‘it is often reported’) , edi-
torializing terms (‘without a doubt’, ‘of course’, ‘es-
sentially’)
Percentage of easy words, difficult words (Dale-
Chall List), long words and stop words
Overall Sentiment Score based on SentiWordNet4
</table>
<tableCaption confidence="0.99986">
Table 1: Content Features of a Wikipedia Article
</tableCaption>
<bodyText confidence="0.999842571428572">
cles with a promotional tone” as a set of positive
examples. We extracted a set of 26,000 untagged
articles to form a noisy set of negative examples,
which may contain some promotional articles that
have not yet been tagged by Wikipedia editors. To
counter this noise, we repeated the experiment us-
ing Wikipedia’s Featured Articles and Good Articles
(approx. 11,000) as a set of clean negative exam-
ples. We used 70% of the articles in each category
to train language models for each of the three cate-
gories (promotional articles, featured/good articles,
untagged articles), and used the remaining 30% to
evaluate classifier performance using 10-fold cross-
validation.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
4 Features
</sectionHeader>
<subsectionHeader confidence="0.999933">
4.1 Content and Meta Features of an Article
</subsectionHeader>
<bodyText confidence="0.9983065">
We used the content and meta features proposed by
Anderka et al. (2012) as given in Tables 1-4. We also
</bodyText>
<footnote confidence="0.998063333333333">
3http://en.wikipedia.org/wiki/Wikipedia:
Manual_of_Style/Words_to_watch
4This feature is not included in Anderka et al. (2012)
</footnote>
<table confidence="0.997911714285714">
Structural Features
Number of Sections
Number of Images
Number of Categories
Number of Wikipedia Templates used
Number of References, Number of References per
sentence and Number of references per section
</table>
<tableCaption confidence="0.940264">
Table 2: Structural Features of a Wikipedia Article
</tableCaption>
<table confidence="0.99960575">
Wikipedia Network Features
Number of Internal Wikilinks (to other Wikipedia
pages)
Number of External Links (to other websites)
Number of Backlinks (i.e. Number of wikilinks from
other Wikipedia articles to an article)
Number of Language Links (i.e. Number of links to
the same article in other languages)
</table>
<tableCaption confidence="0.999706">
Table 3: Network Features of a Wikipedia Article
</tableCaption>
<bodyText confidence="0.999986833333333">
added a new feature, “Overall Sentiment Score” for
an article. This feature is the average of the senti-
ment scores assigned by SentiWordnet (Baccianella
et al., 2010) to all positive and negative sentiment
bearing words in an article. In total, this results in
58 basic document features.
</bodyText>
<subsectionHeader confidence="0.996257">
4.2 N-Gram Language Models
</subsectionHeader>
<bodyText confidence="0.999956125">
Language models are commonly used to measure
stylistic differences in language usage between au-
thors. For this work, we employed them to model
the difference in style of neutral vs. promotional
Wikipedia articles. We trained trigram word lan-
guage models and trigram character language mod-
els5 with Witten-Bell smoothing to produce proba-
bilistic models of both classes.
</bodyText>
<subsectionHeader confidence="0.997739">
4.3 PCFG Language Models
</subsectionHeader>
<bodyText confidence="0.998933111111111">
Probabilistic Context Free Grammars (PCFG) cap-
ture the syntactic structure of language by mod-
eling sentence generation using probabilistic CFG
productions. We hypothesize that sentences in pro-
motional articles and those in neutral articles tend
to have different kinds of syntactic structures and
therefore, we explored the utility of PCFG models
for detecting this difference. Since we do not have
ground-truth parse trees for sentences in our dataset,
</bodyText>
<footnote confidence="0.882187">
5Modeling longer character sequences did not help.
</footnote>
<page confidence="0.942907">
1852
</page>
<bodyText confidence="0.996671692307692">
Features based on PCFG models and n-gram Language models
Difference in the probabilities assigned to an article by the positive and the negative class character trigram
language models (LM char trigram)
Difference in the probabilities assigned to an article by the positive and the negative class word trigram language
models (LM word trigram)
Difference in the mean values of the probabilities assigned to sentences of an article by the positive and negative
class PCFG models (PCFG mean)
Difference in the maximum values of the probabilities assigned to sentences of an article by the positive and
negative class PCFG models (PCFG max)
Difference in the minimum values of the probabilities assigned to sentences of an article by the positive and
negative class PCFG models (PCFG min)
Difference in the standard deviation values of the probabilities of sentences of an article by the positive and
negative class PCFG models (PCFG std deviation)
</bodyText>
<tableCaption confidence="0.986994">
Table 5: Features of a Wikipedia Article based on PCFG models and n-gram Language models
</tableCaption>
<table confidence="0.9949317">
Edit History Features
Age of the article
Days since last revision of the article
Number of edits to the article
Number of unique editors
Number of edits made by registered users and by
anonymous IP addresses
Number of edits per editor
Percentage of edits by top 5% of the top contributors
to the article
</table>
<tableCaption confidence="0.999132">
Table 4: Edit-History Features of a Wikipedia Article
</tableCaption>
<bodyText confidence="0.9998785">
we followed the method of (Raghavan et al., 2010;
Harpalani et al., 2011), which uses the output of
the Stanford parser to train PCFG models for stylis-
tic analysis. We used the PCFG implementation of
Klein and Manning (2003) to learn a PCFG model
for each category.
</bodyText>
<subsectionHeader confidence="0.982551">
4.4 Classification
</subsectionHeader>
<bodyText confidence="0.999987529411765">
The n-gram and PCFG language models were used
to create a set of additional document features. We
used the probability assigned by the language mod-
els to each sentence in a test document to calculate
document-wide statistics such as the mean, maxi-
mum, and minimum probability and standard devia-
tion in probabilities of the set of sentences in an arti-
cle. The language-modeling features used are shown
in Table 5.
Since we have a wide variety of features, we
experimented with various ensemble learning tech-
niques and found that LogitBoost performed best
empirically. We used the Weka implementation of
LogitBoost (Friedman et al., 2000) to train a classi-
fier using various combinations of features. We used
Decision Stumps as a base classifier and ran boost-
ing for 500 iterations.
</bodyText>
<sectionHeader confidence="0.99826" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.8172">
5.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999728">
We used 10-fold cross-validation to test the perfor-
mance of our classifier using various combinations
of features. We ran the classifier on the portion
(30%) of the dataset not used for language model-
ing.6 We measured overall classification accuracy
as well as precision, recall, F-measure, and area un-
der the ROC curve for all experiments. We tested
performance in two settings (Anderka et al., 2012):
</bodyText>
<listItem confidence="0.9958992">
• Pessimistic Setting: The negative class consists
of articles from the Untagged set. Since some
of these could be manually undetected promo-
tional articles, the accuracy measured in this
setting is probably an under-estimate.
• Optimistic Setting: The negative class consists
of articles from the Featured/Good set. These
articles are at one end of the quality spectrum,
making it relatively easier to distinguish them
from promotional articles.
</listItem>
<bodyText confidence="0.999767">
The true performance of the classifier is likely some-
where between that achieved in these two settings.
</bodyText>
<footnote confidence="0.975677">
6We maintain an equal number of positive and negative test
cases in both the settings.
</footnote>
<page confidence="0.659681">
1853
</page>
<table confidence="0.9998634">
Features Pessimistic Setting Optimistic Setting
P R F1 AUC P R F1 AUC
Bag-of-words Baseline 0.823 0.820 0.821 0.893 0.931 0.931 0.931 0.979
PCFG 0.881 0.870 0.865 0.903 0.910 0.910 0.910 0.961
Character trigrams 0.889 0.887 0.888 0.952 0.858 0.843 0.841 0.877
Word trigrams 0.863 0.863 0.863 0.931 0.887 0.883 0.882 0.931
Character trigrams + Word trigrams 0.89 0.888 0.889 0.952 0.908 0.907 0.907 0.962
PCFG+Char. trigrams+Word trigrams 0.914 0.915 0.914 0.974 0.950 0.950 0.950 0.983
58 Content and Meta Features 0.866 0.867 0.867 0.938 0.986 0.986 0.986 0.996
All Features 0.940 0.940 0.940 0.986 0.989 0.989 0.989 0.997
</table>
<tableCaption confidence="0.998664">
Table 6: Performance (Precision(P), Recall(R), F1 score, AUC) of the classifier in the two settings
</tableCaption>
<sectionHeader confidence="0.78128" genericHeader="method">
5.2 Results for Pessimistic Setting
</sectionHeader>
<bodyText confidence="0.999989722222222">
From Table 6, we see that all features perform
better than the bag-of-words baseline. We also
see that character trigrams, one of the simplest
features, gives the best individual performance.
However, deeper syntactic features using PCFGs
also performs quite well. Combining all of the
language-modeling features (PCFG + character tri-
grams + Word trigrams) further improves perfor-
mance. Compared to the 58 content and meta fea-
tures utilized by Anderka et al., (2012) described
in Section 4.1, the PCFG and character trigram fea-
tures give much better performance, both individu-
ally and when combined. It is interesting to note
that adding Anderka et al.’s features to the language-
modeling ones gives a fairly small improvement in
performance. This validates our hypothesis that pro-
motional articles tend to have a distinct linguistic
style that is captured well using language models.
</bodyText>
<sectionHeader confidence="0.771004" genericHeader="evaluation">
5.3 Results for Optimistic Setting
</sectionHeader>
<bodyText confidence="0.999915">
In the Optimistic Setting, as shown in Table 6,
the content and meta features give the best perfor-
mance, which improves only slightly when com-
bined with language-modeling features. The bag-of-
words baseline performs better than all the language
modeling features. This performance could be be-
cause there is a much clearer distinction between
promotional articles and featured/good articles that
can be captured by simple features alone. For exam-
ple, featured/good articles are generally longer than
usual Wikipedia articles and have more references.
</bodyText>
<subsectionHeader confidence="0.8258705">
5.4 Top Ranked Features and their
Performance
</subsectionHeader>
<bodyText confidence="0.9999966">
To analyze the performance of different features, we
determined the top ranked features using Informa-
tion Gain. In the Pessimistic Setting, the top six
features are all language-modeling features (charac-
ter trigram model feature works best), followed by
basic meta-features such as character count, word
count, category count and sentence count. The new
feature we introduced, “Overall Sentiment Score” is
the 18th most informative feature in the pessimistic
setting, indicating that the cumulative sentiment of a
bag of words is not as discriminative as we would in-
tuitively assume. Using the 10 top-ranked features,
we get an F1 of 0.93, which is only slightly worse
than that achieved using all features (F1 = 0.94).
In the Optimistic Setting, the top-ranked features
are the number of references and the number of
references per section. This is consistent with the
observation that featured/good articles have very
long and comprehensive lists of references, since
Wikipedia’s fundamental policy is to maintain ver-
ifiability by citing relevant sources. Features based
on the n-gram and PCFG models also appear in the
list of ten best features. Using only the top 10 fea-
tures, gives an F1 of 0.988, which is almost as good
as using all features (F1 = 0.989).
</bodyText>
<subsectionHeader confidence="0.965537">
5.5 Optimistic and Pessimistic Settings
</subsectionHeader>
<bodyText confidence="0.9999134">
In the optimistic setting, there is a clear distinc-
tion between the positive (promotional) and negative
(featured/good) classes. But there are only subtle
differences between the positive and negative (un-
tagged articles) classes in the pessimistic setting.
</bodyText>
<page confidence="0.982281">
1854
</page>
<table confidence="0.999249636363636">
Best Features in Pessimistic Setting Best Features in Optimistic Setting
LM char trigram Number of References
LM word trigram Number of References per Section
PCFG min LM word trigram
PCFG max Number of Words
PCFG mean PCFG mean
PCFG std deviation Number of Sentences
Number of Characters LM char trigram
Number of Words Number of Words
Number of Categories Number of Characters
Number of Sentences Number of Backlinks
</table>
<tableCaption confidence="0.999891">
Table 7: Top 10 Features (listed in order) in both Settings ranked using Information Gain
</tableCaption>
<bodyText confidence="0.999673857142857">
These two classes are superficially similar, in terms
of length, reference count, section count etc. Stylo-
metric features based on the trained language mod-
els are successful at detecting the subtle linguistic
differences in the two types of articles. This is use-
ful because the pessimistic setting is closer to the
real-world setting of articles in Wikipedia.
</bodyText>
<subsectionHeader confidence="0.955823">
5.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999979702702703">
Since the pessimistic setting is close to the real set-
ting of Wikipedia articles, it is useful to do an error
analysis of the classifier’s performance in this set-
ting. There is an approximately equal proportion of
false positives and false negatives.
A significant number of false positives seem to
be cases of manually undetected promotional arti-
cles. This demonstrates the practical utility of our
classifier. But there are also many false positives
that seem to be truly unbiased. These articles ap-
pear to have been poorly written, without following
Wikipedia’s editing policies. Examples include use
of very long lists of nouns, use of ambiguous terms
like ”many believe” and excessive use of superla-
tives. Other common characteristics of most of the
false positives are presence of a considerable num-
ber of complex sentences with multiple subordinate
clauses. These stylistic cues seem to be misleading
the classifier.
A common thread underlying most of the false
negatives is the fact that they are written in a nar-
rative style or they have excessive details in terms of
the content. Examples include narrating a detailed
story of a fictional character in an unbiased manner
or writing a minutely detailed account of the history
of an organization. Another source of false negatives
comes from biographical Wikipedia pages which are
written in a resume style, listing all their qualifi-
cations and achievements. These cues could help
one manually detect that the article, though not pro-
motional in style, is probably written with the view
of promoting the entity. As possible future work,
we could incorporate features derived from language
models for narrative style trained using an appropri-
ate external corpus of narrative text. This might en-
able the classifier to detect some cases of unbiased
promotional articles.
</bodyText>
<sectionHeader confidence="0.996173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999968545454546">
Our experiments and analysis show that stylomet-
ric features based on n-gram language models and
deeper syntactic PCFG models work very well for
detecting promotional articles in Wikipedia. Af-
ter analyzing the errors that are made during clas-
sification, we realize that though promotional con-
tent is non-neutral in majority of the cases, there do
exist promotional articles that are neutral in style.
Adding additional features based on language mod-
els of narrative style could lead to a better model of
Wikipedia’s promotional content.
</bodyText>
<sectionHeader confidence="0.990036" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999534875">
This research was supported in part by the DARPA
DEFT program under AFRL grant FA8750-13-2-
0026 and by MURI ARO grant W911NF-08-1-
0242. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author and do not necessarily reflect the
view of DARPA, AFRL, ARO, or the US govern-
ment.
</bodyText>
<page confidence="0.988448">
1855
</page>
<sectionHeader confidence="0.996283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636095238095">
Maik Anderka, Benno Stein, and Nedim Lipka. 2012.
Predicting quality flaws in user-generated content: the
case of Wikipedia. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and de-
velopment in Information Retrieval, SIGIR ’12, pages
981–990, New York, NY, USA. ACM.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th Conference on International
Language Resources and Evaluation (LREC’10), Val-
letta, Malta, May.
Joachim Diederich, J¨org Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence, 19(1-
2):109–123.
Hugo J Escalante, Thamar Solorio, and M Montes-y
G´omez. 2011. Local histograms of character n-
grams for authorship attribution. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
volume 1, pages 288–298.
Rudolf Flesch. 1948. A new readability yardstick. The
Journal of Applied Psychology, 32(3):221.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
2000. Additive logistic regression: a statistical view
of boosting (with discussion and a rejoinder by the au-
thors). The Annals of Statistics, 28(2):337–407.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of the 20th International
Conference on Computational Linguistics, page 611.
Association for Computational Linguistics.
Manoj Harpalani, Michael Hart, Sandesh Singh, Rob
Johnson, and Yejin Choi. 2011. Language of van-
dalism: Improving Wikipedia vandalism detection via
stylometric analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers, volume 2, pages 83–88.
Daniel Hasan Dalip, Marcos Andr´e Gonc¸alves, Marco
Cristo, and P´avel Calado. 2009. Automatic qual-
ity assessment of content created collaboratively by
web communities: a case study of Wikipedia. In Pro-
ceedings of the 9th ACM/IEEE-CS Joint Conference
on Digital libraries, JCDL ’09, pages 295–304, New
York, NY, USA. ACM.
Michael Heilman, Kevyn Collins-Thompson, and Max-
ine Eskenazi. 2008. An analysis of statistical models
and features for reading difficulty prediction. In Pro-
ceedings of the Third Workshop on Innovative Use of
NLP for Building Educational Applications, pages 71–
79. Association for Computational Linguistics.
Vlado Keˇselj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for au-
thorship attribution. In Proceedings of the Conference
Pacific Association for Computational Linguistics, PA-
CLING, volume 3, pages 255–264.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Associ-
ation for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
annual meeting of the Association for Computational
Linguistics, pages 544–554. Association for Computa-
tional Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ’10,
pages 38–42, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Congzhou He Ramyaa and Khaled Rasheed. 2004. Us-
ing machine learning techniques for stylometry. In
Proceedings of International Conference on Machine
Learning.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters, 36(1):295–306.
Klaus Stein and Claudia Hess. 2007. Does it matter who
contributes: a study on featured articles in the German
Wikipedia. In Proceedings of the Eighteenth Confer-
ence on Hypertext and Hypermedia, pages 171–174.
ACM.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
2000 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora: held in conjunction with the 38th Annual Meet-
ing of the Association for Computational Linguistics-
Volume 13, pages 63–70. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 173–180. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.837197">
1856
</page>
<reference confidence="0.9979713">
William Yang Wang and Kathleen R. McKeown. 2010.
”Got you!”: Automatic vandalism detection in
Wikipedia with web-based shallow syntactic-semantic
modeling. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
’10, pages 1146–1154, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Dennis M Wilkinson and Bernardo A Huberman. 2007.
Assessing the value of coooperation in Wikipedia.
arXiv preprint cs/0702140.
</reference>
<page confidence="0.994116">
1857
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647119">
<title confidence="0.999674">Detecting Promotional Content in Wikipedia</title>
<author confidence="0.999971">Shruti Bhosale Heath Vinicombe Raymond J Mooney</author>
<affiliation confidence="0.999388">Department of Computer The University of Texas at Austin</affiliation>
<abstract confidence="0.959992888888889">This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Maik Anderka</author>
<author>Benno Stein</author>
<author>Nedim Lipka</author>
</authors>
<title>Predicting quality flaws in user-generated content: the case of Wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR Conference on Research and development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>981--990</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1118" citStr="Anderka et al., 2012" startWordPosition="155" endWordPosition="158"> metafeatures. 1 Introduction Wikipedia is a free, collaboratively edited encyclopedia. Since normally anyone can create and edit pages, some articles are written in a promotional tone, violating Wikipedia’s policy requiring a neutral viewpoint. Currently, such articles are identified manually and tagged with an appropriate Cleanup message1 by Wikipedia editors. Given the scale and rate of growth of Wikipedia, it is infeasible to manually identify all such articles. Hence, we present an approach to automatically detect promotional articles. Related work in quality flaw detection in Wikipedia (Anderka et al., 2012) has relied on meta-features based on edit history, Wikipedia links, structural features and counts of words, sentences and paragraphs. However, we hypothesize that there are subtle differences in the linguistic style that distinguish promotional tone, which we attempt to capture using stylometric features, particularly deeper syntactic features. We model the style of promotional and normal articles using language models 1http://en.wikipedia.org/wiki/Wikipedia: Template_messages/Cleanup based on both n-grams and Probabilistic Context Free Grammars (PCFGs). We show that using such stylometric f</context>
<context position="5037" citStr="Anderka et al. (2012)" startWordPosition="734" endWordPosition="737"> promotional articles that have not yet been tagged by Wikipedia editors. To counter this noise, we repeated the experiment using Wikipedia’s Featured Articles and Good Articles (approx. 11,000) as a set of clean negative examples. We used 70% of the articles in each category to train language models for each of the three categories (promotional articles, featured/good articles, untagged articles), and used the remaining 30% to evaluate classifier performance using 10-fold crossvalidation. 4 Features 4.1 Content and Meta Features of an Article We used the content and meta features proposed by Anderka et al. (2012) as given in Tables 1-4. We also 3http://en.wikipedia.org/wiki/Wikipedia: Manual_of_Style/Words_to_watch 4This feature is not included in Anderka et al. (2012) Structural Features Number of Sections Number of Images Number of Categories Number of Wikipedia Templates used Number of References, Number of References per sentence and Number of references per section Table 2: Structural Features of a Wikipedia Article Wikipedia Network Features Number of Internal Wikilinks (to other Wikipedia pages) Number of External Links (to other websites) Number of Backlinks (i.e. Number of wikilinks from othe</context>
<context position="9931" citStr="Anderka et al., 2012" startWordPosition="1509" endWordPosition="1512">ogitBoost (Friedman et al., 2000) to train a classifier using various combinations of features. We used Decision Stumps as a base classifier and ran boosting for 500 iterations. 5 Experimental Evaluation 5.1 Methodology We used 10-fold cross-validation to test the performance of our classifier using various combinations of features. We ran the classifier on the portion (30%) of the dataset not used for language modeling.6 We measured overall classification accuracy as well as precision, recall, F-measure, and area under the ROC curve for all experiments. We tested performance in two settings (Anderka et al., 2012): • Pessimistic Setting: The negative class consists of articles from the Untagged set. Since some of these could be manually undetected promotional articles, the accuracy measured in this setting is probably an under-estimate. • Optimistic Setting: The negative class consists of articles from the Featured/Good set. These articles are at one end of the quality spectrum, making it relatively easier to distinguish them from promotional articles. The true performance of the classifier is likely somewhere between that achieved in these two settings. 6We maintain an equal number of positive and neg</context>
<context position="11801" citStr="Anderka et al., (2012)" startWordPosition="1801" endWordPosition="1804">9 0.989 0.989 0.997 Table 6: Performance (Precision(P), Recall(R), F1 score, AUC) of the classifier in the two settings 5.2 Results for Pessimistic Setting From Table 6, we see that all features perform better than the bag-of-words baseline. We also see that character trigrams, one of the simplest features, gives the best individual performance. However, deeper syntactic features using PCFGs also performs quite well. Combining all of the language-modeling features (PCFG + character trigrams + Word trigrams) further improves performance. Compared to the 58 content and meta features utilized by Anderka et al., (2012) described in Section 4.1, the PCFG and character trigram features give much better performance, both individually and when combined. It is interesting to note that adding Anderka et al.’s features to the languagemodeling ones gives a fairly small improvement in performance. This validates our hypothesis that promotional articles tend to have a distinct linguistic style that is captured well using language models. 5.3 Results for Optimistic Setting In the Optimistic Setting, as shown in Table 6, the content and meta features give the best performance, which improves only slightly when combined</context>
</contexts>
<marker>Anderka, Stein, Lipka, 2012</marker>
<rawString>Maik Anderka, Benno Stein, and Nedim Lipka. 2012. Predicting quality flaws in user-generated content: the case of Wikipedia. In Proceedings of the 35th International ACM SIGIR Conference on Research and development in Information Retrieval, SIGIR ’12, pages 981–990, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="5975" citStr="Baccianella et al., 2010" startWordPosition="873" endWordPosition="876">s per sentence and Number of references per section Table 2: Structural Features of a Wikipedia Article Wikipedia Network Features Number of Internal Wikilinks (to other Wikipedia pages) Number of External Links (to other websites) Number of Backlinks (i.e. Number of wikilinks from other Wikipedia articles to an article) Number of Language Links (i.e. Number of links to the same article in other languages) Table 3: Network Features of a Wikipedia Article added a new feature, “Overall Sentiment Score” for an article. This feature is the average of the sentiment scores assigned by SentiWordnet (Baccianella et al., 2010) to all positive and negative sentiment bearing words in an article. In total, this results in 58 basic document features. 4.2 N-Gram Language Models Language models are commonly used to measure stylistic differences in language usage between authors. For this work, we employed them to model the difference in style of neutral vs. promotional Wikipedia articles. We trained trigram word language models and trigram character language models5 with Witten-Bell smoothing to produce probabilistic models of both classes. 4.3 PCFG Language Models Probabilistic Context Free Grammars (PCFG) capture the s</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship attribution with support vector machines.</title>
<date>2003</date>
<journal>Applied Intelligence,</journal>
<pages>19--1</pages>
<marker>Diederich, Kindermann, Leopold, Paass, 2003</marker>
<rawString>Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. 2003. Authorship attribution with support vector machines. Applied Intelligence, 19(1-2):109–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo J Escalante</author>
<author>Thamar Solorio</author>
<author>M Montes-y G´omez</author>
</authors>
<title>Local histograms of character ngrams for authorship attribution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>288--298</pages>
<marker>Escalante, Solorio, G´omez, 2011</marker>
<rawString>Hugo J Escalante, Thamar Solorio, and M Montes-y G´omez. 2011. Local histograms of character ngrams for authorship attribution. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 288–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>The Journal of Applied Psychology,</journal>
<volume>32</volume>
<issue>3</issue>
<marker>Flesch, 1948</marker>
<rawString>Rudolf Flesch. 1948. A new readability yardstick. The Journal of Applied Psychology, 32(3):221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
</authors>
<title>Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The Annals of Statistics,</title>
<date>2000</date>
<contexts>
<context position="9343" citStr="Friedman et al., 2000" startWordPosition="1414" endWordPosition="1417"> PCFG language models were used to create a set of additional document features. We used the probability assigned by the language models to each sentence in a test document to calculate document-wide statistics such as the mean, maximum, and minimum probability and standard deviation in probabilities of the set of sentences in an article. The language-modeling features used are shown in Table 5. Since we have a wide variety of features, we experimented with various ensemble learning techniques and found that LogitBoost performed best empirically. We used the Weka implementation of LogitBoost (Friedman et al., 2000) to train a classifier using various combinations of features. We used Decision Stumps as a base classifier and ran boosting for 500 iterations. 5 Experimental Evaluation 5.1 Methodology We used 10-fold cross-validation to test the performance of our classifier using various combinations of features. We ran the classifier on the portion (30%) of the dataset not used for language modeling.6 We measured overall classification accuracy as well as precision, recall, F-measure, and area under the ROC curve for all experiments. We tested performance in two settings (Anderka et al., 2012): • Pessimis</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2000</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2000. Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The Annals of Statistics, 28(2):337–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Linguistic correlates of style: Authorship classification with deep linguistic analysis features.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>611</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Linguistic correlates of style: Authorship classification with deep linguistic analysis features. In Proceedings of the 20th International Conference on Computational Linguistics, page 611. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manoj Harpalani</author>
<author>Michael Hart</author>
<author>Sandesh Singh</author>
<author>Rob Johnson</author>
<author>Yejin Choi</author>
</authors>
<title>Language of vandalism: Improving Wikipedia vandalism detection via stylometric analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers,</booktitle>
<volume>2</volume>
<pages>83--88</pages>
<contexts>
<context position="2641" citStr="Harpalani et al., 2011" startWordPosition="377" endWordPosition="380">trained using a set of lexical, structural, network and edit-history related features of Wikipedia articles. However, they used no features capturing syntactic structure, at a level deeper than Part-OfSpeech (POS) tags. A related area is that of vandalism detection in Wikipedia. Several systems have been developed to detect vandalizing edits in Wikipedia. These fall into two major categories: those analyzing author information and edit metadata (Wilkinson and Huberman, 2007; Stein and Hess, 2007); and those using NLP techniques such as n-gram language models and PCFGs (Wang and McKeown, 2010; Harpalani et al., 2011). We combine relevant features from both these categories to train a classifier that distinguishes promotional content from normal Wikipedia articles. 3 Dataset Collection We extracted a set of about 13,000 articles from English Wikipedia’s category, “Category:All arti2“Advert” is the flaw-type of majority of the articles in the Category ‘Articles with a promotional tone’. 1851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851–1857, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Content Features Nu</context>
<context position="8495" citStr="Harpalani et al., 2011" startWordPosition="1274" endWordPosition="1277">ilities of sentences of an article by the positive and negative class PCFG models (PCFG std deviation) Table 5: Features of a Wikipedia Article based on PCFG models and n-gram Language models Edit History Features Age of the article Days since last revision of the article Number of edits to the article Number of unique editors Number of edits made by registered users and by anonymous IP addresses Number of edits per editor Percentage of edits by top 5% of the top contributors to the article Table 4: Edit-History Features of a Wikipedia Article we followed the method of (Raghavan et al., 2010; Harpalani et al., 2011), which uses the output of the Stanford parser to train PCFG models for stylistic analysis. We used the PCFG implementation of Klein and Manning (2003) to learn a PCFG model for each category. 4.4 Classification The n-gram and PCFG language models were used to create a set of additional document features. We used the probability assigned by the language models to each sentence in a test document to calculate document-wide statistics such as the mean, maximum, and minimum probability and standard deviation in probabilities of the set of sentences in an article. The language-modeling features us</context>
</contexts>
<marker>Harpalani, Hart, Singh, Johnson, Choi, 2011</marker>
<rawString>Manoj Harpalani, Michael Hart, Sandesh Singh, Rob Johnson, and Yejin Choi. 2011. Language of vandalism: Improving Wikipedia vandalism detection via stylometric analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers, volume 2, pages 83–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hasan Dalip</author>
<author>Marcos Andr´e Gonc¸alves</author>
<author>Marco Cristo</author>
<author>P´avel Calado</author>
</authors>
<title>Automatic quality assessment of content created collaboratively by web communities: a case study of Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital libraries, JCDL ’09,</booktitle>
<pages>295--304</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Dalip, Gonc¸alves, Cristo, Calado, 2009</marker>
<rawString>Daniel Hasan Dalip, Marcos Andr´e Gonc¸alves, Marco Cristo, and P´avel Calado. 2009. Automatic quality assessment of content created collaboratively by web communities: a case study of Wikipedia. In Proceedings of the 9th ACM/IEEE-CS Joint Conference on Digital libraries, JCDL ’09, pages 295–304, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Maxine Eskenazi</author>
</authors>
<title>An analysis of statistical models and features for reading difficulty prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>pages</pages>
<marker>Heilman, Collins-Thompson, Eskenazi, 2008</marker>
<rawString>Michael Heilman, Kevyn Collins-Thompson, and Maxine Eskenazi. 2008. An analysis of statistical models and features for reading difficulty prediction. In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, pages 71– 79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlado Keˇselj</author>
<author>Fuchun Peng</author>
<author>Nick Cercone</author>
<author>Calvin Thomas</author>
</authors>
<title>N-gram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference Pacific Association for Computational Linguistics, PACLING,</booktitle>
<volume>3</volume>
<pages>255--264</pages>
<marker>Keˇselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>Vlado Keˇselj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for authorship attribution. In Proceedings of the Conference Pacific Association for Computational Linguistics, PACLING, volume 3, pages 255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8646" citStr="Klein and Manning (2003)" startWordPosition="1300" endWordPosition="1303">on PCFG models and n-gram Language models Edit History Features Age of the article Days since last revision of the article Number of edits to the article Number of unique editors Number of edits made by registered users and by anonymous IP addresses Number of edits per editor Percentage of edits by top 5% of the top contributors to the article Table 4: Edit-History Features of a Wikipedia Article we followed the method of (Raghavan et al., 2010; Harpalani et al., 2011), which uses the output of the Stanford parser to train PCFG models for stylistic analysis. We used the PCFG implementation of Klein and Manning (2003) to learn a PCFG model for each category. 4.4 Classification The n-gram and PCFG language models were used to create a set of additional document features. We used the probability assigned by the language models to each sentence in a test document to calculate document-wide statistics such as the mean, maximum, and minimum probability and standard deviation in probabilities of the set of sentences in an article. The language-modeling features used are shown in Table 5. Since we have a wide variety of features, we experimented with various ensemble learning techniques and found that LogitBoost </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic evaluation of linguistic quality in multidocument summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>544--554</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Pitler, Louis, Nenkova, 2010</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multidocument summarization. In Proceedings of the 48th annual meeting of the Association for Computational Linguistics, pages 544–554. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sindhu Raghavan</author>
<author>Adriana Kovashka</author>
<author>Raymond Mooney</author>
</authors>
<title>Authorship attribution using probabilistic context-free grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>38--42</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8470" citStr="Raghavan et al., 2010" startWordPosition="1270" endWordPosition="1273">on values of the probabilities of sentences of an article by the positive and negative class PCFG models (PCFG std deviation) Table 5: Features of a Wikipedia Article based on PCFG models and n-gram Language models Edit History Features Age of the article Days since last revision of the article Number of edits to the article Number of unique editors Number of edits made by registered users and by anonymous IP addresses Number of edits per editor Percentage of edits by top 5% of the top contributors to the article Table 4: Edit-History Features of a Wikipedia Article we followed the method of (Raghavan et al., 2010; Harpalani et al., 2011), which uses the output of the Stanford parser to train PCFG models for stylistic analysis. We used the PCFG implementation of Klein and Manning (2003) to learn a PCFG model for each category. 4.4 Classification The n-gram and PCFG language models were used to create a set of additional document features. We used the probability assigned by the language models to each sentence in a test document to calculate document-wide statistics such as the mean, maximum, and minimum probability and standard deviation in probabilities of the set of sentences in an article. The lang</context>
</contexts>
<marker>Raghavan, Kovashka, Mooney, 2010</marker>
<rawString>Sindhu Raghavan, Adriana Kovashka, and Raymond Mooney. 2010. Authorship attribution using probabilistic context-free grammars. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 38–42, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congzhou He Ramyaa</author>
<author>Khaled Rasheed</author>
</authors>
<title>Using machine learning techniques for stylometry.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Machine Learning.</booktitle>
<marker>Ramyaa, Rasheed, 2004</marker>
<rawString>Congzhou He Ramyaa and Khaled Rasheed. 2004. Using machine learning techniques for stylometry. In Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Andrew Wilson</author>
<author>Geoffrey Leech</author>
</authors>
<title>Grammatical word class variation within the british national corpus sampler.</title>
<date>2001</date>
<journal>Language and Computers,</journal>
<volume>36</volume>
<issue>1</issue>
<marker>Rayson, Wilson, Leech, 2001</marker>
<rawString>Paul Rayson, Andrew Wilson, and Geoffrey Leech. 2001. Grammatical word class variation within the british national corpus sampler. Language and Computers, 36(1):295–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Stein</author>
<author>Claudia Hess</author>
</authors>
<title>Does it matter who contributes: a study on featured articles in the German Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighteenth Conference on Hypertext and Hypermedia,</booktitle>
<pages>171--174</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2519" citStr="Stein and Hess, 2007" startWordPosition="357" endWordPosition="360">ality flaws. One of these flaw types, “Advert”2, refers to articles written like advertisements. Their classifiers were trained using a set of lexical, structural, network and edit-history related features of Wikipedia articles. However, they used no features capturing syntactic structure, at a level deeper than Part-OfSpeech (POS) tags. A related area is that of vandalism detection in Wikipedia. Several systems have been developed to detect vandalizing edits in Wikipedia. These fall into two major categories: those analyzing author information and edit metadata (Wilkinson and Huberman, 2007; Stein and Hess, 2007); and those using NLP techniques such as n-gram language models and PCFGs (Wang and McKeown, 2010; Harpalani et al., 2011). We combine relevant features from both these categories to train a classifier that distinguishes promotional content from normal Wikipedia articles. 3 Dataset Collection We extracted a set of about 13,000 articles from English Wikipedia’s category, “Category:All arti2“Advert” is the flaw-type of majority of the articles in the Category ‘Articles with a promotional tone’. 1851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 185</context>
</contexts>
<marker>Stein, Hess, 2007</marker>
<rawString>Klaus Stein and Claudia Hess. 2007. Does it matter who contributes: a study on featured articles in the German Wikipedia. In Proceedings of the Eighteenth Conference on Hypertext and Hypermedia, pages 171–174. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational LinguisticsVolume 13,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational LinguisticsVolume 13, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Got you!”: Automatic vandalism detection in Wikipedia with web-based shallow syntactic-semantic modeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1146--1154</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2616" citStr="Wang and McKeown, 2010" startWordPosition="373" endWordPosition="376"> Their classifiers were trained using a set of lexical, structural, network and edit-history related features of Wikipedia articles. However, they used no features capturing syntactic structure, at a level deeper than Part-OfSpeech (POS) tags. A related area is that of vandalism detection in Wikipedia. Several systems have been developed to detect vandalizing edits in Wikipedia. These fall into two major categories: those analyzing author information and edit metadata (Wilkinson and Huberman, 2007; Stein and Hess, 2007); and those using NLP techniques such as n-gram language models and PCFGs (Wang and McKeown, 2010; Harpalani et al., 2011). We combine relevant features from both these categories to train a classifier that distinguishes promotional content from normal Wikipedia articles. 3 Dataset Collection We extracted a set of about 13,000 articles from English Wikipedia’s category, “Category:All arti2“Advert” is the flaw-type of majority of the articles in the Category ‘Articles with a promotional tone’. 1851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851–1857, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Lingui</context>
</contexts>
<marker>Wang, McKeown, 2010</marker>
<rawString>William Yang Wang and Kathleen R. McKeown. 2010. ”Got you!”: Automatic vandalism detection in Wikipedia with web-based shallow syntactic-semantic modeling. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1146–1154, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis M Wilkinson</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Assessing the value of coooperation in Wikipedia. arXiv preprint cs/0702140.</title>
<date>2007</date>
<contexts>
<context position="2496" citStr="Wilkinson and Huberman, 2007" startWordPosition="352" endWordPosition="356">f Wikipedia’s most frequent quality flaws. One of these flaw types, “Advert”2, refers to articles written like advertisements. Their classifiers were trained using a set of lexical, structural, network and edit-history related features of Wikipedia articles. However, they used no features capturing syntactic structure, at a level deeper than Part-OfSpeech (POS) tags. A related area is that of vandalism detection in Wikipedia. Several systems have been developed to detect vandalizing edits in Wikipedia. These fall into two major categories: those analyzing author information and edit metadata (Wilkinson and Huberman, 2007; Stein and Hess, 2007); and those using NLP techniques such as n-gram language models and PCFGs (Wang and McKeown, 2010; Harpalani et al., 2011). We combine relevant features from both these categories to train a classifier that distinguishes promotional content from normal Wikipedia articles. 3 Dataset Collection We extracted a set of about 13,000 articles from English Wikipedia’s category, “Category:All arti2“Advert” is the flaw-type of majority of the articles in the Category ‘Articles with a promotional tone’. 1851 Proceedings of the 2013 Conference on Empirical Methods in Natural Languag</context>
</contexts>
<marker>Wilkinson, Huberman, 2007</marker>
<rawString>Dennis M Wilkinson and Bernardo A Huberman. 2007. Assessing the value of coooperation in Wikipedia. arXiv preprint cs/0702140.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
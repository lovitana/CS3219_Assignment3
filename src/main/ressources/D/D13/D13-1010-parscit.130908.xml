<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.980748">
Measuring Ideological Proportions in Political Speeches
</title>
<author confidence="0.992433">
Yanchuan Sim* Brice D. L. Acree†
</author>
<affiliation confidence="0.883589666666667">
*Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998188">
{ysim,nasmith}@cs.cmu.edu
</email>
<author confidence="0.99573">
Justin H. Gross† Noah A. Smith*
</author>
<affiliation confidence="0.999376">
†Department of Political Science
University of North Carolina at Chapel Hill
</affiliation>
<address confidence="0.890499">
Chapel Hill, NC 27599, USA
</address>
<email confidence="0.999445">
{brice.acree,jhgross}@unc.edu
</email>
<sectionHeader confidence="0.994813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999673307692308">
We seek to measure political candidates’ ideo-
logical positioning from their speeches. To ac-
complish this, we infer ideological cues from
a corpus of political writings annotated with
known ideologies. We then represent the
speeches of U.S. Presidential candidates as se-
quences of cues and lags (filler distinguished
only by its length in words). We apply a
domain-informed Bayesian HMM to infer the
proportions of ideologies each candidate uses
in each campaign. The results are validated
against a set of preregistered, domain expert-
authored hypotheses.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981681818182">
The artful use of language is central to politics, and
the language of politicians has attracted consider-
able interest among scholars of political commu-
nication and rhetoric (Charteris-Black, 2005; Hart,
2009; Deirmeier et al., 2012; Hart et al., 2013)
and computational linguistics (Thomas et al., 2006;
Fader et al., 2007; Gerrish and Blei, 2011, in-
ter alia). In American politics, candidates for of-
fice give speeches and write books and manifestos
expounding their ideas. Every political season,
however, there are accusations of candidates “flip-
flopping” on issues, with opinion shows, late-night
comedies, and talk radio hosts replaying clips of
candidates contradicting earlier statements. Pres-
idential candidate Mitt Romney’s own aide infa-
mously proclaimed in 2012: “I think you hit a reset
button for the fall campaign [i.e., the general elec-
tion]. Everything changes. It’s almost like an Etch-
a-Sketch. You can kind of shake it up and we start
all over again.”
A more general observation, often stated but not
yet, to our knowledge, tested empirically, is that
</bodyText>
<page confidence="0.982882">
91
</page>
<bodyText confidence="0.999821852941177">
successful primary candidates “move to the cen-
ter” before a general election. The expectation fol-
lows directly from long-standing and widely influen-
tial theories of political competition that are collec-
tively referred to in their simplest form as the “me-
dian voter theorem” (Hotelling, 1929; Black, 1948;
Downs, 1957). Thus it is to be expected that when
a set of voters that are more ideologically concen-
trated are replaced by a set who are more widely
dispersed across the ideological spectrum, as occurs
in the transition between the United States primary
and general elections, that candidates will present
themselves as more moderate in an effort to capture
enough votes to win.
Do political candidates in fact stray ideologically
at opportune moments? More specifically, can we
measure candidates’ ideological positions from their
prose at different times? Following much work
on classifying the political ideology expressed by a
piece of text (Laver et al., 2003; Monroe and Maeda,
2004; Hillard et al., 2008), we start from the as-
sumption that a candidate’s choice of words and
phrases reflects a deliberate attempt to signal com-
mon cause with a target audience, and as a broader
strategy, to respond to political competitors. Our
central hypothesis is that, despite candidates’ in-
tentional vagueness, differences in position—among
candidates or over time—can be automatically de-
tected and described as proportions of ideologies ex-
pressed in a speech.
In this work, we operationalize ideologies in a
novel empirical way, exploiting political writings
published in explicitly ideological books and mag-
azines (§2).1 The corpus then serves as evidence for
</bodyText>
<footnote confidence="0.993634666666667">
1We consider general positions in terms of broad ideolog-
ical groups that are widely discussed in current political dis-
course (e.g., “Far Right,” “Religious Right,” “Libertarian,”’
</footnote>
<note confidence="0.8375605">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 91–101,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.544061">
CENTER-LEFT
BACKGROUND
</figure>
<figureCaption confidence="0.997500333333333">
Figure 1: Ideology tree showing the labels for the ide-
ological corpus in §2.1 (excluding BACKGROUND) and
corresponding to states in the HMM (§3.3).
</figureCaption>
<bodyText confidence="0.999887928571428">
a probabilistic model that allows us to automatically
infer compact, human-interpretable lexicons of cues
strongly associated with each ideology.
These lexicons are used, in turn, to create a low-
dimensional representation of political speeches: a
speech is a sequence of cues interspersed with lags.
Lags correspond to the lengths of sequences of non-
cue words, which are treated as irrelevant to the in-
ference problem at hand. In other words, a speech is
represented as a series alternating between cues sig-
naling ideological positions and uninteresting filler.
Our main contribution is a probabilistic technique
for inferring proportions of ideologies expressed by
a candidate (§3). The inputs to the model are the
cue-lag representation of a speech and a domain-
specific topology relating ideologies to each other.
The topology tree (shown in Figure 1) encoding
the closeness of different ideologies and, by exten-
sion, the odds of transitioning between them within a
speech. Bayesian inference is used to manage uncer-
tainty about the associations between cues and ide-
ologies, probabilities of traversing each of the tree’s
edges, and other parameters.
We demonstrate the usefulness of the measure-
ment model by showing that it accurately recov-
ers pre-registered beliefs regarding narratives widely
accepted—but not yet tested empirically—about the
2008 and 2012 U.S. Presidential elections (§4).
</bodyText>
<sectionHeader confidence="0.956219" genericHeader="introduction">
2 First Stage: Cue Extraction
</sectionHeader>
<bodyText confidence="0.999978333333333">
We first present a data-driven technique for automat-
ically constructing “cue lexicons” from texts labeled
with ideologies by domain experts.
</bodyText>
<note confidence="0.665862">
etc.). Analysis of positions on specific issues is left for future
work.
</note>
<table confidence="0.999670705882353">
Total tokens 32,835,190
Total types 138,235
Avg. tokens per book 77,628
Avg. tokens per mag. issue 31,713
Breakdown by ideology: Documents Tokens
LEFT 0 0
FAR LEFT 112 3,334,601
CENTER-LEFT 196 7,396,264
PROGRESSIVE LEFT 138 7,257,723
RELIGIOUS LEFT 7 487,844
CENTER 5 429,480
RIGHT 97 3,282,744
FAR RIGHT 211 7,392,163
LIBERTARIAN RIGHT 88 1,703,343
CENTER-RIGHT 9 702,444
POPULIST RIGHT 5 407,054
RELIGIOUS RIGHT 6 441,530
</table>
<tableCaption confidence="0.9986185">
Table 1: Ideology corpus statistics. Note that some docu-
ments are not labeled with finer-grained ideologies.
</tableCaption>
<subsectionHeader confidence="0.90457">
2.1 Ideological Corpus
</subsectionHeader>
<bodyText confidence="0.9994178">
We start with a collection of contemporary political
writings whose authors are perceived as represen-
tative of one particular ideology. Our corpus con-
sists of two types of documents: books and maga-
zines. Books are usually written by a single author,
while each magazine consists of regularly published
issues with collections of articles written by several
authors. A political science domain expert who is
a co-author of this work manually labeled each ele-
ment in a collection of 112 books and 10 magazine
titles2 with one of three coarse ideologies: LE F T,
RI G H T, or CE N T E R. Documents that were labeled
LE F T and RI G H T were further broken down into
more fine-grained ideologies, shown in Fig. 1.3 Ta-
ble 1 summarizes key details about the ideological
corpus.
In addition to ideology labels, individual chapters
within the books were manually tagged with topics
that the chapter was about. For instance, in Barack
Obama’s book The Audacity of Hope, his chapter
</bodyText>
<footnote confidence="0.9581167">
2There are 765 magazine issues, which are published bi-
weekly to quarterly, depending on the magazine. All of a mag-
azine’s issues are labeled with the same ideology.
3We cannot claim that these texts are “pure” examples of
the ideologies they are labeled with (i.e., they may contain parts
that do not match the label). By finding relatively few terms
strongly associated with texts sharing a label, our model should
be somewhat robust to impurities, focusing on those terms that
are indicative of whatever drew the expert to identify them as
(mostly) sharing an ideology.
</footnote>
<figure confidence="0.9925872">
FAR LEFT
FAR RIGHT
RELIGIOUS RIGHT
RELIGIOUS LEFT
CENTER-RIGHT
LIBERTARIAN
LEFT RIGHT
CENTER
POPULIST
PROGRESSIVE
</figure>
<page confidence="0.960107">
92
</page>
<bodyText confidence="0.999637111111111">
titled “Faith” is labeled as RE L I G I O U S. Not all
chapters have clearly defined topics, and as such,
these chapters are simply labeled MI S C. Maga-
zines are not labeled with topics because each issue
of a magazine generally touches on multiple top-
ics. There are a total of 61 topics; the full list can
be found in the supplementary materials, along with
a table summarizing key details about the corpus,
which contains 32.8 million tokens.
</bodyText>
<subsectionHeader confidence="0.999471">
2.2 Cue Discovery Model
</subsectionHeader>
<bodyText confidence="0.99974047826087">
We use the ideological corpus to infer ideological
cues: terms that are strongly associated with an ide-
ology. Because our ideologies are organized hierar-
chically, we required a technique that can account
for multiple effects within a single text. We further
require that the sets of cue terms be small, so that
they can be inspected by domain experts. We there-
fore turn to the sparse additive generative (SAGE)
models introduced by Eisenstein et al. (2011).
Like other probabilistic language models, SAGE
assigns probability to a text as if it were a bag of
terms. It differs from most language models in pa-
rameterizing the distribution using a generalized lin-
ear model, so that different effects on the log-odds
of terms are additive. In our case, we define the
probability of a term w conditioned on attributes of
the text in which it occurs. These attributes include
both the ideology and its coarsened version (e.g., a
FA R RI G H T book also has the attribute RI G H T).
For simplicity, let A(d) denote the set of attributes
of document d and A = Ud A(d). The parametric
form of the distribution is given, for term w in doc-
ument d, by:
</bodyText>
<equation confidence="0.945137666666667">
( )
exp q0w + Ea∈A(d) qa w
Z(A(d), η)
</equation>
<bodyText confidence="0.9987885">
Each of the q weights can be a positive or negative
value influencing the probability of the word, condi-
tioned on various properties of the document. When
we stack an attribute a’s weights into a vector across
all words, we get an ηa vector, understood as an ef-
fect on the term distribution. (We use η to refer to
the collection of all of these vectors.) The effects in
our model, described in terms of attributes, are:
</bodyText>
<listItem confidence="0.980998266666667">
• η0, the background (log) frequencies of words,
fixed to the empirical frequencies in the corpus.
Hence the other effects can be understood as de-
viations from this background distribution.
• ηi-, the coarse ideology effect, which takes differ-
ent values for LEFT, RIGHT, and CENTER.
• ηi�, the fine ideology effect, which takes different
values for the fine-grained ideologies correspond-
ing to the leaves in Fig. 1.
• ηt, the topic effect, taking different values for
each of the 61 manually assigned topics. We fur-
ther include one effect for each magazine series
(of which there are 10) to account for each maga-
zine’s idiosyncrasies (topical or otherwise).
• ηd, a document-specific effect, which captures id-
</listItem>
<bodyText confidence="0.998661125">
iosyncratic usage within a single document.
Note that the effects above are not mutually exclu-
sive, although some effects never appear together
due to constraints imposed by their semantics (e.g.,
no book is labeled both LE F T and RI G H T).
When estimating the parameters of the model (the
η vectors), we impose a sparsity-inducing E1 prior
that forces many weights to zero. The objective is:
</bodyText>
<equation confidence="0.993217">
logp(w  |A(d); η) − � Aallηall1
a∈A
</equation>
<bodyText confidence="0.999736125">
This objective function is convex but requires spe-
cial treatment due to non-differentiability when any
elements are zero; we use the OWL-QN algorithm
to solve it (Andrew and Gao, 2007). To reduce the
complexity of the hyperparameter space (the possi-
ble values of all Aa) and to encourage similar levels
of sparsity across the different effect vectors, we let,
for each ideology attribute a,
</bodyText>
<equation confidence="0.681004">
Aa = A · |V(a) |/maxa�∈A |V(a0)|
</equation>
<bodyText confidence="0.9999776">
where V(a) is the set of term types appearing in
the data with attribute a (i.e., its vocabulary) , and
A is a hyperparameter we can adjust to control the
amount of sparsity in the SAGE vectors. For the
non-ideology effects, we fix Aa = 10 (not tuned).
</bodyText>
<subsectionHeader confidence="0.99817">
2.3 Bigram and Trigram Lexicons
</subsectionHeader>
<bodyText confidence="0.999238">
After estimating parameters, we are left with sparse
ηa for each attribute. We are only interested, how-
ever, in the ideological attributes I C A. For an
ideological attribute i E I, we take the terms with
positive elements of this vector to be the cues for
ideology i; call this set L(i) and let L = U i∈I L(i).
</bodyText>
<equation confidence="0.7524068">
p(w  |A(d); η) =
�max
η d
�
w∈d
</equation>
<page confidence="0.983267">
93
</page>
<bodyText confidence="0.9999304375">
Because political texts use a fair amount of multi-
word jargon, we initially represented each document
as a bag of unigrams, bigrams, and trigrams, ignor-
ing the fact that these “overlap” with each other.4
While this would be inappropriate in language mod-
eling and is inconsistent with our model’s indepen-
dence assumptions among words, it is sensible since
our goal is to identify cues that are statistically asso-
ciated with attributes like ideologies.
Preliminary trials revealed that unigrams tend to
dominate in such a model, since their frequency
counts are so much higher. Further, domain ex-
perts found them harder to interpret out of context
compared to bigrams and trigrams. We therefore in-
cluded only bigrams and trigrams as terms in our cue
discovery model.
</bodyText>
<subsectionHeader confidence="0.988869">
2.4 Validation
</subsectionHeader>
<bodyText confidence="0.995285774193548">
The term selection method we have described can
be understood as a form of feature selection that
reasons globally about the data and tries to con-
trol for some effects that are not of interest (topic
or document idiosyncrasies). We compared the
approach to two classic, simple methods for fea-
ture selection: ranking based on pointwise mu-
tual information (PMI) and weighted average PMI
(WAPMI) (Schneider, 2005; Cover and Thomas,
2012). Selected features were used to classify the
ideologies of held-out documents from our cor-
pus.5 We evaluated these feature selection methods
within naive Bayes classification in a 5-fold cross-
validation setup. We vary A for the SAGE model
and compare the results to equal-sized sets of terms
selected by PMI and WAPMI. We consider SAGE
with and without topic effects.
Figure 2 visualizes accuracy against the num-
ber of features for each method. Bigrams and
trigrams consistently outperform unigrams (McNe-
mar’s, p &lt; 0.05). Otherwise, there are no sig-
nificant differences in performance except WAPMI
4Generative models that produce the same evidence more
than once are sometimes called “deficient,” but model defi-
ciency does not necessarily imply that the model is ineffective.
Some of the IBM models for statistical machine translation pro-
vide a classic example (Brown et al., 1993).
5The text was tokenized and stopwords removed. Punctu-
ation, numbers, and web addresses were normalized. Tokens
appearing less than 20 times in training data, or in fewer than 5
documents were removed.
</bodyText>
<figure confidence="0.995560285714286">
0.7
0.65
0.6
0.55
0.526 27 28 29 210 211 212 213 214 ∞
PMI SAGE
WAPMI SAGE w/ topics
</figure>
<figureCaption confidence="0.9567925">
Figure 2: Plot of average classification accuracy for
5-fold cross validation against the number of features.
Dashed lines refer to using only unigram features, while
solid lines refer to using bigram and trigram features.
</figureCaption>
<bodyText confidence="0.999925875">
with bigrams/trigrams at its highest point. SAGE
with topics is slightly (but not significantly) bet-
ter than without. We conclude that SAGE is a
competitive choice for cue discovery, noting that a
principled way of controlling for topical and doc-
ument effects—offered by SAGE but not the other
methods—may be even more relevant to our task
than classification accuracy.
</bodyText>
<subsectionHeader confidence="0.997121">
2.5 Cue Lexicon
</subsectionHeader>
<bodyText confidence="0.9999636">
We ran SAGE on the the full ideological book cor-
pus, including topic effects, and setting A = 30, ob-
tained a set of I I = 8, 483 cue terms. The supple-
mentary materials include top cue terms associated
with various ideologies and a heatmap of similarities
among SAGE vectors.
We conducted a small, relatively informal study
in which seven subjects (including four scholars of
American politics) were asked to match brief de-
scriptions of the classes, including prominent proto-
typical individuals exemplifying each, to cue terms.
About 70% of ideologies were correctly matched
by experts, with relatively few confusions between
LE F T and RI G H T. More details are given in sup-
plementary materials.
</bodyText>
<sectionHeader confidence="0.895261" genericHeader="method">
3 Second Stage: Cue-Lag Ideological
Proportions
</sectionHeader>
<bodyText confidence="0.999886">
The main contribution of this paper is a technique
for measuring ideology proportions in the prose of
political candidates. We adopt a Bayesian approach
that manages our uncertainty about the cue lexi-
</bodyText>
<page confidence="0.996533">
94
</page>
<bodyText confidence="0.999973466666667">
con Z, the tendencies of political speakers to “flip-
flop” among ideological types, and the relative “dis-
tances” among different ideologies. The representa-
tion of a candidate’s ideology as a mixture among
discrete, hierarchically related categories can be dis-
tinguished from continuous representations (“scal-
ing” or “spatial” models) often used in political sci-
ence, especially to infer positions from Congres-
sional roll-call voting patterns (Poole and Rosen-
thal, 1985; Poole and Rosenthal, 2000; Clinton
et al., 2004). Moreover, the ability to draw in-
ferences about individual policy-makers’ ideologies
from their votes on proposed legislation is severely
limited by institutional constraints on the types of
legislation that is actually subject to recorded votes.
</bodyText>
<subsectionHeader confidence="0.998895">
3.1 Political Speeches Corpus
</subsectionHeader>
<bodyText confidence="0.999996333333333">
We gathered transcribed speeches given by candi-
dates of the two main parties (Democrats and Re-
publicans) during the 2008 and 2012 Presidential
election seasons. Each election season is comprised
of two stages: (i) the primary elections, where can-
didates seek the support of their respective parties to
be nominated as the party’s Presidential candidate,
and (ii) the general elections where the parties’ cho-
sen candidates travel across the states to garner sup-
port from all citizens. Each candidate’s speeches are
partitioned into epochs for each election; e.g., those
that occur before the candidate has secured enough
pledged delegates to win the party nomination are
“from the primary.” Table 2 presents a breakdown
of the candidates and speeches in our corpus.
</bodyText>
<subsectionHeader confidence="0.998575">
3.2 Cue-Lag Representation
</subsectionHeader>
<bodyText confidence="0.999019357142857">
Our measurement model only considers ideological
cues; other terms are treated as filler. We therefore
transform each speech into a cue-lag representation.
The representation is a sequence of alternating
cues (elements from the ideological lexicon Z) and
integer “lags” (counts of non-cue terms falling be-
tween two cues). This will allow us to capture the in-
tuition that a candidate may use longer lags between
evocations of different ideologies, while nearby cues
are likely to be from similar ideologies.
To map a speech into the cue-lag representation,
we simply match all elements of Z in the speech and
replace sequences of other words by their lengths.
When a trigram cue strictly includes a bigram cue,
</bodyText>
<table confidence="0.999846166666667">
Party Pri’08 Gen’08 Pri’12 Gen’12
Democrats* 167 - - -
Republicans† 50 - 49 -
Obama (D) 78 81 - 99
McCain (R) 9 159 - -
Romney (R) 8 #(13) 19 19
</table>
<tableCaption confidence="0.815521388888889">
*Democrats in our corpus are: Joe Biden, Hillary Clinton, John
Edwards, and Bill Richardson in 2008 and Barack Obama in
both 2008 and 2012.
tRepublicans in our corpus are: Rudy Giuliani, Mike Huck-
abee, John McCain, and Fred Thompson in 2008, Michelle
Bachmann, Herman Cain, Newt Gingrich, Jon Huntsman, Rick
Perry, and Rick Santorum in 2012, and Ron Paul and Mitt Rom-
ney in both 2008 and 2012.
*For Romney, we have 13 speeches which he gave in the period
2008-2011 (between his withdrawal from the 2008 elections
and before the commencement of the 2012 elections). While
these speeches are not technically part of the regular Presiden-
tial election campaign, they can be seen as his preparation to-
wards the 2012 elections, which is particularly interesting as
Romney has been accused of having inconsistent viewpoints.
Table 2: Breakdown of number of speeches in our polit-
ical speech corpus by epoch. On average, 2,998 tokens,
and 95 cue terms are found in each speech document.
</tableCaption>
<bodyText confidence="0.99975025">
we take only the trigram. When two cues partially
overlap, we treat them as consecutive cue terms and
set the lag to 0. Figure 3 shows an example of our
cue-lag representation.
</bodyText>
<subsectionHeader confidence="0.964914">
3.3 CLIP: An Ideology HMM
</subsectionHeader>
<bodyText confidence="0.999652">
The model we use to infer ideologies, cue-lag ide-
ological proportions (CLIP), is a hidden Markov
model. Each state corresponds to an ideology
(Fig. 1) or BA C K G R O U N D. The emission from a state
consists of (i) a cue from Z and (ii) a lag value. The
high-level generative story for a single speech with
T cue-lag pairs is as follows:
</bodyText>
<listItem confidence="0.973989166666667">
1. Parameters are drawn from conjugate priors
(details in §3.3.3).
2. Let the initial state be the BA C K G R O U N D
state.
3. Fort E 11,2,...,T}:6
(a) Transition to state 5t based on the
</listItem>
<bodyText confidence="0.7921455">
transition distribution, discussed in §3.3.1.
This transition is conditioned on the previ-
ous state 5t−1 and the lag at timestep t−1,
denoted by Lt−1.
</bodyText>
<footnote confidence="0.996434">
6The length of the sequence is assumed to be exogenous, so
that no stop state needs to be defined.
</footnote>
<page confidence="0.975113">
95
</page>
<table confidence="0.861163166666667">
Original sentence Just compare this President’s record with Ronald Reagan’s first term. President Reagan also faced
an economic crisis. In fact, in 1982, the unemployment rate peaked at nearly 11 percent. But in the
two years that followed, he delivered a true recovery economic growth and job creation were three
times higher than in the Obama Economy.
Cue-lag representation ... 6−→ ronald reagan 2−→ presid reagan 3−→ econom crisi 5−→ unemploy rate 17 −→ econom growth 1−→
job creation 9−→ .. .
</table>
<figureCaption confidence="0.992302">
Figure 3: Example of the cue-lag representation.
</figureCaption>
<bodyText confidence="0.89648025">
(b) Emit cue term Wt from the lexicon L
and lag Lt based on the emission distribu-
tion, discussed in §3.3.2.
We turn next to the transitions and emissions.
</bodyText>
<subsectionHeader confidence="0.8797595">
3.3.1 Ideology Topology and Transition
Parameterization
</subsectionHeader>
<bodyText confidence="0.999548863636364">
CLIP assumes that each cue term uttered by a
politician is generated from a hidden state corre-
sponding to an ideology. The ideologies are orga-
nized into a tree based on their hierarchical relation-
ships; see Fig. 1. In this study, the tree is fixed ac-
cording to our domain knowledge of current Ameri-
can politics; in future work it might be enriched with
greater detail or its structure learned automatically.
The ideology tree is used in defining the transition
distribution in the HMM, but not to directly define
the topology of the HMM. Importantly, each state
may transition to any other state, but the transition
distribution is defined using the graph, so that ide-
ologies that are closer to each other will tend to be
more likely to transition to each other. To transition
between two states si and sj, a walk must be taken
in the tree from vertex si to vertex sj. We emphasize
that the walk corresponds to a single transition—
the speaker does not emit anything from the states
passed through along the path.
A simplified version of our transition distribution,
for exposition, is given as follows:
</bodyText>
<equation confidence="0.925979">
ptree(sj  |si; C, 0)
)�u,v��Path(si,sj)(1 − ζu)θu,v ζsj
</equation>
<bodyText confidence="0.999514875">
Path(si, sj) refers to the sequence of edges in the
tree along the unique path from si to sj. Each of
these edges (u, v) must be traversed, and the prob-
ability of doing so, conditioned on having already
reached u, is (1−ζu)—i.e., not stopping in u—times
θu,v—i.e., selecting vertex v from among those that
share an edge with u. Eventually, sj is reached, and
the walk ends, incurring probability ζsj.
In order to capture the intuition that a longer lag
after a cue term should increase the entropy over the
next ideology state, we introduce a restart probabil-
ity, which is conditioned on the length of the most
recent lag, `. The probability of restarting the walk
from the BA C K G R O U N D state is a noisy-OR model
with parameter ρ. This gives the transition distribu-
tion:
</bodyText>
<equation confidence="0.997706">
p(sj  |si, `; C, 0, ρ) = (1 − ρ)`+�ptree(sj  |si; C, 0)
+ (1 − (1 −
</equation>
<bodyText confidence="0.996705909090909">
Note that, if ρ = 1, there is no Markovian depen-
dency between states (i.e., there is always a restart),
so CLIP reverts to a mixture model.
This approach allows us to parameterize the full
set of |I|2 transitions with O(|I|) parameters.7 Since
the graph is a tree and the walks are not allowed
to backtrack, the only ambiguity in the transition
is due to the restart probability; this distinguishes
CLIP from other algorithms based on random walks
(Brin and Page, 1998; Mihalcea, 2005; Toutanova et
al., 2004; Collins-Thompson and Callan, 2005).
</bodyText>
<subsubsectionHeader confidence="0.559829">
3.3.2 Emission Parameterization
</subsubsectionHeader>
<bodyText confidence="0.990331235294118">
Recall that, at time step t, CLIP emits a cue from
the lexicon L and an integer-valued lag. For each
state s, we let the probability of emitting cue w
be denoted by ψs,w; 0s is a multinomial distribu-
tion over the entire lexicon L. This allows our ap-
proach to handle ambiguous cues that can associate
with more than one ideology, and also to associate a
cue with a different ideology than our cue discovery
method proposed, if the signal from the data is suffi-
ciently strong. We assume each lag to be generated
by a Poisson distribution with global parameter ν.
7More precisely, there are |I |edges (since there are |I |+ 1
vertices including BACKGROUND), each with a 6-parameter in
each direction. For a vertex with degree d, however, there are
only d−1 degrees of freedom, so that there are 2|I|−(|I|+1) =
|I|−1 degrees of freedom for 6. There are |I |ζ-parameters and
a single p, for a total of 2|I |degrees of freedom.
</bodyText>
<equation confidence="0.998972">
(� =
ρ)`+&apos; /`
)ptree(sj  |sBACKGROUND; s, 0)
</equation>
<page confidence="0.996061">
96
</page>
<subsectionHeader confidence="0.397327">
3.3.3 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.999254581395349">
Above we described CLIP’s transitions and emis-
sions. Because our interest is in measuring
proportions—and, as we will see, in comparing
those proportions across speakers and campaign
periods—we require a way to allow variation in pa-
rameters across different conditions. Specifically,
we seek to measure differences in time spent in each
ideology state. This can be captured by allowing
each speaker to have a different 0 and C in each stage
of the campaign. On the other hand, we expect that a
speaker draws from his ideological lexicon similarly
across different epochs—there is a single z/i shared
between different epochs.
In order to manage uncertainty about the param-
eters of CLIP, to incorporate prior beliefs based on
our ideology-specific cue lexicons JZ(i)}Z, and to
allow sharing of statistical strength across condi-
tions, we adopt a Bayesian approach to inference.
This will allow principled exploration of the poste-
rior distribution over the proportions of interest.
We place a symmetric Dirichlet prior on the tree
walk probabilities 0; its parameter is α. For the
cue emission distribution associated with ideology
i, z/isz, we use an informed Dirichlet prior with two
different values, Qc71e for cues in Z(i), and a smaller
Qdef for those in Z \ Z(i).8
Learning proceeds by collapsed Gibbs sampling
for the hidden states and slice sampling (with vague
priors) for the hyperparameters (α, ,3, p, and C). De-
tails of the sampler are given in the supplementary
materials. At each Gibbs step, we resample the ide-
ology state and restart indicator variable for every
cue term in every speech.
We ran our Gibbs sampler for 75,000 iterations,
discarding the first 25,000 iterations for burn-in, and
collected samples at every 10 iterations. Further, we
perform the slice sampling step at every 5,000 itera-
tions. For each candidate, we collected 5,000 poste-
rior samples which we use to infer his/her ideologi-
cal proportions.
In order to determine the amount of time a candi-
date spends in each ideology, we denote the unit of
time in terms of half the lag before and after each cue
</bodyText>
<footnote confidence="0.910192">
8This implies that a term can, in the posterior distribution,
be associated with an ideology i of whose C(i) it was not a
member. In fact, this occurred frequently in our runs of the
model.
</footnote>
<bodyText confidence="0.995474857142857">
term, i.e., when a candidate draws a cue term from
ideology i during timestep t, we say that he spends
2 (Lt_1 + Lt) amount of time in ideology i. Aver-
aging over all the samples returned by our sampler
and normalizing it by the length of the documents in
each epoch, we obtain a candidate’s expected ideo-
logical proportions within the epoch.
</bodyText>
<sectionHeader confidence="0.997428" genericHeader="method">
4 Pre-registered Hypotheses
</sectionHeader>
<bodyText confidence="0.99999637037037">
The traditional way to evaluate a text analysis model
in NLP is, of course, to evaluate its output against
gold-standard judgements by humans. In the case
of recent political speeches, however, we are doubt-
ful that such judgments can be made objectively at
a fine-grained level. While we are confident about
gross categorization of books and magazines in our
ideological corpus (§2.1), many of which are overtly
marked by their ideological assocations, we believe
that human estimates of ideological proportions, or
even association of particular tokens with ideologies
they may evoke, may be overly clouded by the vari-
ation in annotator ideology and domain expertise.
We therefore adopt a different method for evalua-
tion. Before running our model, we identified a set
of hypotheses, which we pre-registered as expec-
tations. These are categorized into groups based on
their strength and relevance to judging the validity of
the model. Strong hypotheses are those that consti-
tute the lowest bar for face validity; if violated, they
suggest a flaw in the model. Moderate hypotheses
are those that match the intuition of domain experts
conducting the research, or extant theory. Violations
suggest more examination is required, and may raise
the possibility that further testing might be pursued
to demonstrate the hypothesis is false. Our 13 prin-
cipal hypotheses are enumerated in Table 3.
</bodyText>
<sectionHeader confidence="0.997951" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9990385">
We compare the posterior proportions inferred by
CLIP with several baselines:
</bodyText>
<listItem confidence="0.999500333333333">
• HMM: rather than §3.3.1, a fully connected, tra-
ditional transition matrix is used.
• MIX: a mixture model; at each timestep, we al-
</listItem>
<bodyText confidence="0.72769625">
ways restart (p = 1). This eliminates Marko-
vian dependencies between ideologies at nearby
timesteps, but still uses the ideology tree in defin-
ing the probabilities of each state through 0.
</bodyText>
<page confidence="0.998472">
97
</page>
<table confidence="0.998313176470588">
Hypotheses CLIP HMM MIX NORES
Sanity checks (strong):
S1. Republican primary candidates should tend to draw more from RIGHT than *12/12 10/13 13/13 12/13
from LE FT.
S2. Democratic primary candidates should tend to draw more from LE F T than 4/5 5/5 5/5 5/5
from RIGHT.
S3. In general elections, Democrats should draw more from the LE F T than the 4/4 4/4 3/4 0/4
Republicans and vice versa for the RI GH T.
S total 20/21 19/22 21/22 17/22
Primary hypotheses (strong):
P1. Romney, McCain and other Republicans should almost never draw from FAR 29/32 *21/31 27/32 29/32
LE F T, and extremely rarely from PR O G R E S S I V E.
P2. Romney should draw more heavily from the RI G H T than Obama in both stages 2/2 2/2 1/2 1/2
of the 2012 campaign.
Primary hypotheses (moderate):
P3. Romney should draw more heavily on words from the LIBERTARIAN, 2/2 2/2 0/2 2/2
POPULIST, RELIGIOUS RIGHT, and FAR RIGHT in the primary com-
pared to the general election. In the general election, Romney should draw
more heavily on CENTER, CENTER-RIGHT and LEFT vocabularies.
P4. Obama should draw more heavily on words from the PR O G R E S S I V E in the 0/1 0/1 0/1 1/1
2008 primary than in the 2008 general election.
P5. In the 2008 general election, Obama should draw more heavily on the 1/1 1/1 1/1 1/1
CE N T E R, CE N T E R-LE F T, and RI G H T vocabularies than in the 2008 primary.
P6. In the 2012 general election, Obama should sample more from the LE F T than 2/2 2/2 0/2 0/2
from the RIGHT, and should sample more from the LEFT vocabularies than
Romney.
P7. McCain should draw more heavily from the FAR RIGHT, POPULI ST, and 0/1 1/1 1/1 1/1
LI B E R T A R I A N in the 2008 primary than in the 2008 general election.
P8. In the general 2008, McCain should draw more heavily from the CE N T E R, 1/1 1/1 1/1 1/1
CE N T E R-RI G H T, and LE F T vocabularies than in the 2008 primary.
P9. McCain should draw more heavily from the RI G H T than Obama in both stages 2/2 2/2 2/2 1/2
of the campaign.
P10.Obama and other Democrats should very rarely draw from FAR RIGHT. 6/7 5/7 7/7 4/7
P total 45/51 37/50 40/51 41/51
</table>
<tableCaption confidence="0.9059675">
Table 3: Pre-registered hypotheses used to validate the measurement model; number of statements evaluated correctly
by different models. *Some differences were not significant at p = 0.05 and are not included in the results.
</tableCaption>
<listItem confidence="0.968171">
• NORES, where we never restart (p = 0). This
strengthens the Markovian dependencies.
</listItem>
<bodyText confidence="0.997485740740741">
In MIX, there are no temporal effects between cue
terms, although the structure of our ideology tree
encourages the speaker to draw from coarse-grained
ideologies over fine-grained ideologies. On the other
hand, the strong Markovian dependency between
states in NORES would encourage the model to stay
local within the ideology tree. In our experiments,
we will see how that the ideology tree and the ran-
dom treatment of restarting both contribute to our
model’s inferences.
Table 3 presents a summary of which hypothe-
ses the models’ inferences are in accordance with.
CLIP is not consistently outperformed by any of the
competing baselines.
Sanity checks (S1–3) CLIP correctly identifies
sixteen LE F T/RI G H T alignments of primary candi-
dates (S1, S2), but is unable to determine one can-
didate’s orientation; it finds Jon Huntsman to spend
roughly equal proportions of speech-time drawing
on LE F T and RI G H T cue terms. Interestingly,
Huntsman, who had served as U.S. Ambassador to
China under Obama, was considered the one mod-
erate in the 2012 Republican field. MIX correctly
identifies all thirteen Republicans, while NORES
places McCain from the 2008 primaries as mostly
LE F T-leaning and HMM misses three of thirteen,
including Perry and Gingrich, who might be deeply
</bodyText>
<page confidence="0.991929">
98
</page>
<bodyText confidence="0.9998834">
disturbed to find that they are misclassified as LEF T-
leaning. As for the Democratic primary candidates
(S2), CLIP’s one questionable finding is that John
Edwards spoke slightly more from the RI G H T than
the LE FT. For the general elections (S3), CLIP and
HMM correctly identify the relative amount of time
spent in LE F T/RI G H T between Obama and his Re-
publican competitors. NORES had the most trou-
ble, missing all four. CLIP finds Obama spend-
ing slightly more time on the RI G H T than on the
LE F T in the 2008 general elections but nevertheless,
Obama is still found to spend more time engaging in
LE F T-speak than McCain.
Name interference When we looked at the cue
terms actually used in the speeches, we found one
systematic issue: the inclusion of candidates’ names
as cue terms. Terms mentioning John McCain are
associated with the RI G H T, so that Obama’s men-
tions of his opponent are taken as evidence for
rightward positioning; in total, mentions of McCain
contributed 4% absolute to Obama’s RI G H T ide-
ological proportion. Similarly, barack obama and
presid obama are LE F T cues (though senat obama
is a RI G H T cue). In future work, we believe filtering
candidate names in the first stage will be beneficial.
Strong hypotheses P1 and P2 CLIP and the vari-
ants making use of the ideology tree were in agree-
ment on most of the strong primary hypotheses.
Most of these involved our expectation that the
Republican candidates would rarely draw on FAR
LEFT and PROGRESSIVE LEFT. Our qualitative
hypotheses were not specific about how to quantify
“rare” or “almost never.” We chose to find a result
inconsistent with a P1 hypothesis any time a Repub-
lican had proportions greater than 5% for either ide-
ology. The notable deviations for CLIP were Fred
Thompson (13% from the PROGRESSIVE LEFT
during the 2008 primary) and Mitt Romney (12%
from the PROGRESS I V E LEFT between the 2008
and 2012 elections, 13% from the FA R LE F T dur-
ing the 2012 general election). This model did no
worse than other variants here and much better than
one: HMM had 10 inconsistencies out of 32 oppor-
tunities, suggesting the importance of the ideology
tree.
</bodyText>
<figure confidence="0.987487448275862">
Religious (R)
Libertarian (R)
Right
Center
Center-Right
Center-Left
Religious (L) Progre
Far Left
Primaries 2008 General 2008
Far Right
Populist (R)
Libertarian (R)
Right
Center-Left
Left
Progressive (L)
Far Left
Primaries 2008 2008-2011 Primaries 2012 General 2012
Far Right
Religious (R)
Populist (R)
Libertarian (R)
Center-Right
Center
Center-Left
Left
Religious (L)
Progressive (L)
Primaries 2008 General 2008 General 2012
</figure>
<figureCaption confidence="0.992536">
Figure 4: Proportion of time spent in each ideology by
McCain, Romney, and Obama during the 2008 and 2012
Presidential election seasons.
</figureCaption>
<bodyText confidence="0.99992875">
“Etch-a-Sketch” hypotheses Hypotheses P3, P4,
P5, P7, and P8 are all concerned with differences
between the primary and general elections: success-
ful primary candidates are expected to “move to the
center.” A visualization of CLIP’s proportions for
McCain, Romney, and Obama is shown in Figure 4,
with their speeches grouped together by different
epochs. The model is in agreement with most of
these hypotheses. It did not confirm P4—Obama
appears to CLIP to be more PR O G R E S S I V E in the
2008 general election than in the primary, though the
difference is small (3%) and may be within the mar-
gin of error. Likewise, in P7, the difference between
McCain drawing from FAR RIGHT, POPULIST
and LIBERTARIAN between the 2008 primary and
general elections is only 2% and highly uncertain,
with a 95% credible interval of 44–50% during the
primary (vs. 47–50% in the general election).
Fine-grained ideologies Fine-grained ideologies
are expected to account for smaller proportions, so
that making predictions about them is quite difficult.
This is especially true for primary elections, where a
broader palette of ideologies is expected to be drawn
from, but we have fewer speeches from each candi-
</bodyText>
<figure confidence="0.998689166666667">
McCain
Romney
Obama
Religious (R)
Center-Right
Center
Religious (L)
Far Left
Far Right
Populist (R)
Left ssive (L)
Right
</figure>
<page confidence="0.990291">
99
</page>
<bodyText confidence="0.999986238095238">
date. CLIP’s inconsistency with P10, for example,
comes from assigning 5.4% of Obama’s 2008 pri-
mary cues to FAR RIGHT.
CLIP’s inferences on the corpus of political
speeches can be browsed at http://www.ark.
cs.cmu.edu/CLIP. We emphasize that CLIP
and its variants are intended to quantify the ideo-
logical content candidates express in speeches, not
necessarily their beliefs (which may not be perfectly
reflected in their words), or even how they are de-
scribed by pundits and analysts (who draw on far
more information than is expressed in speeches).
CLIP’s deviations from the hypotheses are sug-
gestive of potential improvements to cue extraction
(§2), but also of incorrect hypotheses. We expect
future research to explore a richer set of linguistic
cues and attributes beyond ideology (e.g., topics and
framing on various issues). We plan to use CLIP
as a text analysis method to support substantive in-
quiry in political science, such as following trends
in expressed ideology over time.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999976928571429">
As early as the 1960s, there has been research on
modeling ideological beliefs using automated sys-
tems (Abelson and Carroll, 1965; Carbonell, 1978;
Sack, 1994). These early works model ideology at a
sophisticated level, involving the actors, actions and
goals; they require manually constructed knowledge
bases. Poole and Rosenthal (1985) used congres-
sional roll call data to demonstrate the ideological
divide in Congress, and provided a methodology for
measuring ideological positions. Gerrish and Blei
(2011; 2012) augmented the methodology with text
from congressional bills using probabilistic models
to uncover lawmakers’ positions on specific polit-
ical issues, putting them on a left-right spectrum,
while Thomas et al. (2006) made use of floor de-
bate speeches to predict votes. Likewise, taking ad-
vantage of the proliferation of text today, numer-
ous techniques have been developed to identify top-
ics and perspectives in the media (Gentzkow and
Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009;
Gentzkow and Shapiro, 2010); determine the polit-
ical leanings of a document or author (Laver et al.,
2003; Efron, 2004; Mullen and Malouf, 2006; Fader
et al., 2007); or recognize stances in debates (So-
masundaran and Wiebe, 2009; Anand et al., 2011).
Going beyong lexical indicators, Greene and Resnik
(2009) investigated syntactic features to identify per-
spectives or implicit sentiment.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999700125">
We introduced CLIP, a domain-informed, Bayesian
model of ideological proportions in political lan-
guage. We showed how ideological cues could be
discovered from a lightly labeled corpus of ideolog-
ical writings, then incorporated into CLIP. The re-
sulting inferences are largely consistent with a set
of preregistered hypotheses about candidates in the
2008 and 2012 Presidential elections.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.943471111111111">
For thoughtful feedback on this research, the authors
thank: several anonymous reviewers, Amber Boydstun,
Philip Resnik, members of the ARK group at CMU, and
participants in Princeton University’s Political Methodol-
ogy Colloquium and PolMeth XXX hosted by The Uni-
versity of Virginia. This work was supported in part by an
A*STAR fellowship to Y. Sim, NSF grants IIS-1211201
and IIS-1211277, and Google’s support of the Reading is
Believing project at CMU.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999676909090909">
Robert P. Abelson and J. Douglas Carroll. 1965. Com-
puter simulation of individual belief systems. Ameri-
can Behavioral Scientist, 8(9):24–30.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: Classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of 11-regularized log-linear models. In Proceed-
ings of ICML.
Duncan Black. 1948. On the rationale of group decision-
making. The Journal of Political Economy, 56(1):23–
34.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1):107–117.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.
</reference>
<page confidence="0.868709">
100
</page>
<reference confidence="0.999675333333333">
Jaime G. Carbonell. 1978. Politics: Automated ideolog-
ical reasoning. Cognitive Science, 2(1):27–51.
Jonathan Charteris-Black. 2005. Politicians and
Rhetoric: The Persuasive Power of Metaphor.
Palgrave-MacMillan.
Joshua Clinton, Simon Jackman, and Douglas Rivers.
2004. The statistical analysis of roll call data. Ameri-
can Political Science Review, 98(2):355–370.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In Pro-
ceedings of CIKM.
Thomas M. Cover and Joy A. Thomas. 2012. Elements
of Information Theory. Wiley-Interscience.
Daniel Deirmeier, Jean-Francois Godbout, Bei Yu, and
Stefan Kaufmann. 2012. Language and ideology
in congress. British Journal of Political Science,
42(1):31–55.
Anthony Downs. 1957. An Economic Theory of Democ-
racy. Harper, New York.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cociation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US senate using lexical centrality. In
Proceedings of EMNLP-CoNLL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statistical
learning methods. In Ashok N. Srivastava and Mehran
Sahami, editors, Text Mining: Classification, Cluster-
ing, and Applications, chapter 2, pages 27–50. Chap-
man &amp; Hall/CRC.
Matthew Gentzkow and Jesse Shapiro. 2005. Media bias
and reputation. Technical report, National Bureau of
Economic Research.
Matthew Gentzkow and Jesse M. Shapiro. 2010. What
drives media slant? evidence from u.s. daily newspa-
pers. Econometrica, 78(1):35–71.
Sean M. Gerrish and David M. Blei. 2011. Predict-
ing legislative roll calls from text. In Proceedings of
ICML.
Sean M. Gerrish and David M. Blei. 2012. How they
vote: Issue-adjusted models of legislative behavior. In
Advances in NIPS 25.
Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
Proceedings of NAACL.
Roderick P. Hart, Jay P. Childers, and Colene J. Lind.
2013. Political Tone: How Leaders Talk and Why.
University of Chicago Press.
Roderick P. Hart. 2009. Campaign talk: Why elections
are good for us. Princeton University Press.
Dustin Hillard, Stephen Purpura, and John Wilker-
son. 2008. Computer-assisted topic classification for
mixed-methods social science research. Journal of In-
formation Technology &amp; Politics, 4(4):31–46.
Harold Hotelling. 1929. Stability in competition. The
Economic Journal, 39(153):41–57.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. The American Political Science Review,
97(2):311–331.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
EMNLP.
Burt L. Monroe and Ko Maeda. 2004. Talk’s cheap:
Text-based estimation of rhetorical ideal-points. Pre-
sented at the Annual Meeting of the Society for Politi-
cal Methodology.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal polit-
ical discourse. In AAAI Symposium on Computational
Approaches to Analysing Weblogs.
Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357–384.
Keith T. Poole and Howard Rosenthal. 2000. Congress:
A Political-Economic History of Roll Call Voting. Ox-
ford University Press.
Warren Sack. 1994. Actor-role analysis: ideology, point
of view, and the news. Master’s thesis, Massachusetts
Institute of Technology, Cambridge, MA.
Karl-Michael Schneider. 2005. Weighted average point-
wise mutual information for feature selection in text
categorization. In Proceedings of PKDD.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
EMNLP.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In Proceed-
ings of ICML.
</reference>
<page confidence="0.998608">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212383">
<title confidence="0.999956">Measuring Ideological Proportions in Political Speeches</title>
<author confidence="0.996442">D L Brice</author>
<affiliation confidence="0.751598">Technologies Carnegie Mellon</affiliation>
<address confidence="0.981906">Pittsburgh, PA 15213,</address>
<author confidence="0.45204">H A</author>
<affiliation confidence="0.973937">of Political University of North Carolina at Chapel</affiliation>
<address confidence="0.978857">Chapel Hill, NC 27599,</address>
<abstract confidence="0.997469785714286">We seek to measure political candidates’ ideological positioning from their speeches. To accomplish this, we infer ideological cues from a corpus of political writings annotated with known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert P Abelson</author>
<author>J Douglas Carroll</author>
</authors>
<title>Computer simulation of individual belief systems.</title>
<date>1965</date>
<journal>American Behavioral Scientist,</journal>
<volume>8</volume>
<issue>9</issue>
<contexts>
<context position="38344" citStr="Abelson and Carroll, 1965" startWordPosition="6439" endWordPosition="6442">ormation than is expressed in speeches). CLIP’s deviations from the hypotheses are suggestive of potential improvements to cue extraction (§2), but also of incorrect hypotheses. We expect future research to explore a richer set of linguistic cues and attributes beyond ideology (e.g., topics and framing on various issues). We plan to use CLIP as a text analysis method to support substantive inquiry in political science, such as following trends in expressed ideology over time. 6 Related Work As early as the 1960s, there has been research on modeling ideological beliefs using automated systems (Abelson and Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2</context>
</contexts>
<marker>Abelson, Carroll, 1965</marker>
<rawString>Robert P. Abelson and J. Douglas Carroll. 1965. Computer simulation of individual belief systems. American Behavioral Scientist, 8(9):24–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the Second Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</booktitle>
<contexts>
<context position="39470" citStr="Anand et al., 2011" startWordPosition="6614" endWordPosition="6617">s on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 2012 Presidential elections. Acknowledgments For thoughtful feedback on this research, </context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the Second Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of 11-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="11477" citStr="Andrew and Gao, 2007" startWordPosition="1849" endWordPosition="1852">ures idiosyncratic usage within a single document. Note that the effects above are not mutually exclusive, although some effects never appear together due to constraints imposed by their semantics (e.g., no book is labeled both LE F T and RI G H T). When estimating the parameters of the model (the η vectors), we impose a sparsity-inducing E1 prior that forces many weights to zero. The objective is: logp(w |A(d); η) − � Aallηall1 a∈A This objective function is convex but requires special treatment due to non-differentiability when any elements are zero; we use the OWL-QN algorithm to solve it (Andrew and Gao, 2007). To reduce the complexity of the hyperparameter space (the possible values of all Aa) and to encourage similar levels of sparsity across the different effect vectors, we let, for each ideology attribute a, Aa = A · |V(a) |/maxa�∈A |V(a0)| where V(a) is the set of term types appearing in the data with attribute a (i.e., its vocabulary) , and A is a hyperparameter we can adjust to control the amount of sparsity in the SAGE vectors. For the non-ideology effects, we fix Aa = 10 (not tuned). 2.3 Bigram and Trigram Lexicons After estimating parameters, we are left with sparse ηa for each attribute.</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of 11-regularized log-linear models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan Black</author>
</authors>
<title>On the rationale of group decisionmaking.</title>
<date>1948</date>
<journal>The Journal of Political Economy,</journal>
<volume>56</volume>
<issue>1</issue>
<pages>34</pages>
<contexts>
<context position="2321" citStr="Black, 1948" startWordPosition="348" endWordPosition="349">in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind of shake it up and we start all over again.” A more general observation, often stated but not yet, to our knowledge, tested empirically, is that 91 successful primary candidates “move to the center” before a general election. The expectation follows directly from long-standing and widely influential theories of political competition that are collectively referred to in their simplest form as the “median voter theorem” (Hotelling, 1929; Black, 1948; Downs, 1957). Thus it is to be expected that when a set of voters that are more ideologically concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on classifying the p</context>
</contexts>
<marker>Black, 1948</marker>
<rawString>Duncan Black. 1948. On the rationale of group decisionmaking. The Journal of Political Economy, 56(1):23– 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="23920" citStr="Brin and Page, 1998" startWordPosition="3962" endWordPosition="3965"> state is a noisy-OR model with parameter ρ. This gives the transition distribution: p(sj |si, `; C, 0, ρ) = (1 − ρ)`+�ptree(sj |si; C, 0) + (1 − (1 − Note that, if ρ = 1, there is no Markovian dependency between states (i.e., there is always a restart), so CLIP reverts to a mixture model. This approach allows us to parameterize the full set of |I|2 transitions with O(|I|) parameters.7 Since the graph is a tree and the walks are not allowed to backtrack, the only ambiguity in the transition is due to the restart probability; this distinguishes CLIP from other algorithms based on random walks (Brin and Page, 1998; Mihalcea, 2005; Toutanova et al., 2004; Collins-Thompson and Callan, 2005). 3.3.2 Emission Parameterization Recall that, at time step t, CLIP emits a cue from the lexicon L and an integer-valued lag. For each state s, we let the probability of emitting cue w be denoted by ψs,w; 0s is a multinomial distribution over the entire lexicon L. This allows our approach to handle ambiguous cues that can associate with more than one ideology, and also to associate a cue with a different ideology than our cue discovery method proposed, if the signal from the data is sufficiently strong. We assume each </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14444" citStr="Brown et al., 1993" startWordPosition="2351" endWordPosition="2354"> to equal-sized sets of terms selected by PMI and WAPMI. We consider SAGE with and without topic effects. Figure 2 visualizes accuracy against the number of features for each method. Bigrams and trigrams consistently outperform unigrams (McNemar’s, p &lt; 0.05). Otherwise, there are no significant differences in performance except WAPMI 4Generative models that produce the same evidence more than once are sometimes called “deficient,” but model deficiency does not necessarily imply that the model is ineffective. Some of the IBM models for statistical machine translation provide a classic example (Brown et al., 1993). 5The text was tokenized and stopwords removed. Punctuation, numbers, and web addresses were normalized. Tokens appearing less than 20 times in training data, or in fewer than 5 documents were removed. 0.7 0.65 0.6 0.55 0.526 27 28 29 210 211 212 213 214 ∞ PMI SAGE WAPMI SAGE w/ topics Figure 2: Plot of average classification accuracy for 5-fold cross validation against the number of features. Dashed lines refer to using only unigram features, while solid lines refer to using bigram and trigram features. with bigrams/trigrams at its highest point. SAGE with topics is slightly (but not signifi</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
</authors>
<title>Politics: Automated ideological reasoning.</title>
<date>1978</date>
<journal>Cognitive Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="38361" citStr="Carbonell, 1978" startWordPosition="6443" endWordPosition="6444">in speeches). CLIP’s deviations from the hypotheses are suggestive of potential improvements to cue extraction (§2), but also of incorrect hypotheses. We expect future research to explore a richer set of linguistic cues and attributes beyond ideology (e.g., topics and framing on various issues). We plan to use CLIP as a text analysis method to support substantive inquiry in political science, such as following trends in expressed ideology over time. 6 Related Work As early as the 1960s, there has been research on modeling ideological beliefs using automated systems (Abelson and Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of </context>
</contexts>
<marker>Carbonell, 1978</marker>
<rawString>Jaime G. Carbonell. 1978. Politics: Automated ideological reasoning. Cognitive Science, 2(1):27–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Charteris-Black</author>
</authors>
<title>Politicians and Rhetoric: The Persuasive Power of Metaphor.</title>
<date>2005</date>
<publisher>Palgrave-MacMillan.</publisher>
<contexts>
<context position="1142" citStr="Charteris-Black, 2005" startWordPosition="161" endWordPosition="162"> of political writings annotated with known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a rese</context>
</contexts>
<marker>Charteris-Black, 2005</marker>
<rawString>Jonathan Charteris-Black. 2005. Politicians and Rhetoric: The Persuasive Power of Metaphor. Palgrave-MacMillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Clinton</author>
<author>Simon Jackman</author>
<author>Douglas Rivers</author>
</authors>
<title>The statistical analysis of roll call data.</title>
<date>2004</date>
<journal>American Political Science Review,</journal>
<volume>98</volume>
<issue>2</issue>
<contexts>
<context position="16806" citStr="Clinton et al., 2004" startWordPosition="2733" endWordPosition="2736">of political candidates. We adopt a Bayesian approach that manages our uncertainty about the cue lexi94 con Z, the tendencies of political speakers to “flipflop” among ideological types, and the relative “distances” among different ideologies. The representation of a candidate’s ideology as a mixture among discrete, hierarchically related categories can be distinguished from continuous representations (“scaling” or “spatial” models) often used in political science, especially to infer positions from Congressional roll-call voting patterns (Poole and Rosenthal, 1985; Poole and Rosenthal, 2000; Clinton et al., 2004). Moreover, the ability to draw inferences about individual policy-makers’ ideologies from their votes on proposed legislation is severely limited by institutional constraints on the types of legislation that is actually subject to recorded votes. 3.1 Political Speeches Corpus We gathered transcribed speeches given by candidates of the two main parties (Democrats and Republicans) during the 2008 and 2012 Presidential election seasons. Each election season is comprised of two stages: (i) the primary elections, where candidates seek the support of their respective parties to be nominated as the </context>
</contexts>
<marker>Clinton, Jackman, Rivers, 2004</marker>
<rawString>Joshua Clinton, Simon Jackman, and Douglas Rivers. 2004. The statistical analysis of roll call data. American Political Science Review, 98(2):355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Query expansion using random walk models.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="23996" citStr="Collins-Thompson and Callan, 2005" startWordPosition="3972" endWordPosition="3975">ransition distribution: p(sj |si, `; C, 0, ρ) = (1 − ρ)`+�ptree(sj |si; C, 0) + (1 − (1 − Note that, if ρ = 1, there is no Markovian dependency between states (i.e., there is always a restart), so CLIP reverts to a mixture model. This approach allows us to parameterize the full set of |I|2 transitions with O(|I|) parameters.7 Since the graph is a tree and the walks are not allowed to backtrack, the only ambiguity in the transition is due to the restart probability; this distinguishes CLIP from other algorithms based on random walks (Brin and Page, 1998; Mihalcea, 2005; Toutanova et al., 2004; Collins-Thompson and Callan, 2005). 3.3.2 Emission Parameterization Recall that, at time step t, CLIP emits a cue from the lexicon L and an integer-valued lag. For each state s, we let the probability of emitting cue w be denoted by ψs,w; 0s is a multinomial distribution over the entire lexicon L. This allows our approach to handle ambiguous cues that can associate with more than one ideology, and also to associate a cue with a different ideology than our cue discovery method proposed, if the signal from the data is sufficiently strong. We assume each lag to be generated by a Poisson distribution with global parameter ν. 7More</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Query expansion using random walk models. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>2012</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="13562" citStr="Cover and Thomas, 2012" startWordPosition="2211" endWordPosition="2214"> experts found them harder to interpret out of context compared to bigrams and trigrams. We therefore included only bigrams and trigrams as terms in our cue discovery model. 2.4 Validation The term selection method we have described can be understood as a form of feature selection that reasons globally about the data and tries to control for some effects that are not of interest (topic or document idiosyncrasies). We compared the approach to two classic, simple methods for feature selection: ranking based on pointwise mutual information (PMI) and weighted average PMI (WAPMI) (Schneider, 2005; Cover and Thomas, 2012). Selected features were used to classify the ideologies of held-out documents from our corpus.5 We evaluated these feature selection methods within naive Bayes classification in a 5-fold crossvalidation setup. We vary A for the SAGE model and compare the results to equal-sized sets of terms selected by PMI and WAPMI. We consider SAGE with and without topic effects. Figure 2 visualizes accuracy against the number of features for each method. Bigrams and trigrams consistently outperform unigrams (McNemar’s, p &lt; 0.05). Otherwise, there are no significant differences in performance except WAPMI 4</context>
</contexts>
<marker>Cover, Thomas, 2012</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 2012. Elements of Information Theory. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Deirmeier</author>
<author>Jean-Francois Godbout</author>
<author>Bei Yu</author>
<author>Stefan Kaufmann</author>
</authors>
<title>Language and ideology in congress.</title>
<date>2012</date>
<journal>British Journal of Political Science,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="1178" citStr="Deirmeier et al., 2012" startWordPosition="165" endWordPosition="168">th known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e.</context>
</contexts>
<marker>Deirmeier, Godbout, Yu, Kaufmann, 2012</marker>
<rawString>Daniel Deirmeier, Jean-Francois Godbout, Bei Yu, and Stefan Kaufmann. 2012. Language and ideology in congress. British Journal of Political Science, 42(1):31–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Downs</author>
</authors>
<title>An Economic Theory of Democracy.</title>
<date>1957</date>
<publisher>Harper,</publisher>
<location>New York.</location>
<contexts>
<context position="2335" citStr="Downs, 1957" startWordPosition="350" endWordPosition="351">hink you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind of shake it up and we start all over again.” A more general observation, often stated but not yet, to our knowledge, tested empirically, is that 91 successful primary candidates “move to the center” before a general election. The expectation follows directly from long-standing and widely influential theories of political competition that are collectively referred to in their simplest form as the “median voter theorem” (Hotelling, 1929; Black, 1948; Downs, 1957). Thus it is to be expected that when a set of voters that are more ideologically concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on classifying the political ideol</context>
</contexts>
<marker>Downs, 1957</marker>
<rawString>Anthony Downs. 1957. An Economic Theory of Democracy. Harper, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Efron</author>
</authors>
<title>Cultural orientation: Classifying subjective documents by cociation analysis.</title>
<date>2004</date>
<booktitle>In AAAI Fall Symposium on Style and Meaning in Language,</booktitle>
<location>Art, and Music.</location>
<contexts>
<context position="39340" citStr="Efron, 2004" startWordPosition="6594" endWordPosition="6595">12) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered h</context>
</contexts>
<marker>Efron, 2004</marker>
<rawString>Miles Efron. 2004. Cultural orientation: Classifying subjective documents by cociation analysis. In AAAI Fall Symposium on Style and Meaning in Language, Art, and Music.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="9011" citStr="Eisenstein et al. (2011)" startWordPosition="1412" endWordPosition="1415">t can be found in the supplementary materials, along with a table summarizing key details about the corpus, which contains 32.8 million tokens. 2.2 Cue Discovery Model We use the ideological corpus to infer ideological cues: terms that are strongly associated with an ideology. Because our ideologies are organized hierarchically, we required a technique that can account for multiple effects within a single text. We further require that the sets of cue terms be small, so that they can be inspected by domain experts. We therefore turn to the sparse additive generative (SAGE) models introduced by Eisenstein et al. (2011). Like other probabilistic language models, SAGE assigns probability to a text as if it were a bag of terms. It differs from most language models in parameterizing the distribution using a generalized linear model, so that different effects on the log-odds of terms are additive. In our case, we define the probability of a term w conditioned on attributes of the text in which it occurs. These attributes include both the ideology and its coarsened version (e.g., a FA R RI G H T book also has the attribute RI G H T). For simplicity, let A(d) denote the set of attributes of document d and A = Ud A</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011. Sparse additive generative models of text. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Dragomir R Radev</author>
<author>Michael H Crespin</author>
<author>Burt L Monroe</author>
<author>Kevin M Quinn</author>
<author>Michael Colaresi</author>
</authors>
<title>MavenRank: Identifying influential members of the US senate using lexical centrality.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1269" citStr="Fader et al., 2007" startWordPosition="180" endWordPosition="183">s of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind</context>
<context position="39386" citStr="Fader et al., 2007" startWordPosition="6600" endWordPosition="6603">t from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 201</context>
</contexts>
<marker>Fader, Radev, Crespin, Monroe, Quinn, Colaresi, 2007</marker>
<rawString>Anthony Fader, Dragomir R. Radev, Michael H. Crespin, Burt L. Monroe, Kevin M. Quinn, and Michael Colaresi. 2007. MavenRank: Identifying influential members of the US senate using lexical centrality. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaz Fortuna</author>
<author>Carolina Galleguillos</author>
<author>Nello Cristianini</author>
</authors>
<title>Detecting the bias in media with statistical learning methods.</title>
<date>2009</date>
<booktitle>In Ashok N. Srivastava and Mehran Sahami, editors, Text Mining: Classification, Clustering, and Applications, chapter 2,</booktitle>
<pages>27--50</pages>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="39220" citStr="Fortuna et al., 2009" startWordPosition="6572" endWordPosition="6575">te the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideologi</context>
</contexts>
<marker>Fortuna, Galleguillos, Cristianini, 2009</marker>
<rawString>Blaz Fortuna, Carolina Galleguillos, and Nello Cristianini. 2009. Detecting the bias in media with statistical learning methods. In Ashok N. Srivastava and Mehran Sahami, editors, Text Mining: Classification, Clustering, and Applications, chapter 2, pages 27–50. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gentzkow</author>
<author>Jesse Shapiro</author>
</authors>
<title>Media bias and reputation.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>National Bureau of Economic Research.</institution>
<contexts>
<context position="39180" citStr="Gentzkow and Shapiro, 2005" startWordPosition="6564" endWordPosition="6567">used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered f</context>
</contexts>
<marker>Gentzkow, Shapiro, 2005</marker>
<rawString>Matthew Gentzkow and Jesse Shapiro. 2005. Media bias and reputation. Technical report, National Bureau of Economic Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gentzkow</author>
<author>Jesse M Shapiro</author>
</authors>
<title>What drives media slant? evidence from u.s. daily newspapers.</title>
<date>2010</date>
<journal>Econometrica,</journal>
<volume>78</volume>
<issue>1</issue>
<contexts>
<context position="39249" citStr="Gentzkow and Shapiro, 2010" startWordPosition="6576" endWordPosition="6579">ide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporat</context>
</contexts>
<marker>Gentzkow, Shapiro, 2010</marker>
<rawString>Matthew Gentzkow and Jesse M. Shapiro. 2010. What drives media slant? evidence from u.s. daily newspapers. Econometrica, 78(1):35–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean M Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Predicting legislative roll calls from text.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="1293" citStr="Gerrish and Blei, 2011" startWordPosition="184" endWordPosition="187">filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind of shake it up and we s</context>
<context position="38725" citStr="Gerrish and Blei (2011" startWordPosition="6493" endWordPosition="6496">stantive inquiry in political science, such as following trends in expressed ideology over time. 6 Related Work As early as the 1960s, there has been research on modeling ideological beliefs using automated systems (Abelson and Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 20</context>
</contexts>
<marker>Gerrish, Blei, 2011</marker>
<rawString>Sean M. Gerrish and David M. Blei. 2011. Predicting legislative roll calls from text. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean M Gerrish</author>
<author>David M Blei</author>
</authors>
<title>How they vote: Issue-adjusted models of legislative behavior.</title>
<date>2012</date>
<booktitle>In Advances in NIPS 25.</booktitle>
<marker>Gerrish, Blei, 2012</marker>
<rawString>Sean M. Gerrish and David M. Blei. 2012. How they vote: Issue-adjusted models of legislative behavior. In Advances in NIPS 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="39529" citStr="Greene and Resnik (2009)" startWordPosition="6622" endWordPosition="6625">-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 2012 Presidential elections. Acknowledgments For thoughtful feedback on this research, the authors thank: several anonymous reviewers, Amber Boyds</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: syntactic packaging and implicit sentiment. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roderick P Hart</author>
<author>Jay P Childers</author>
<author>Colene J Lind</author>
</authors>
<title>Political Tone: How Leaders Talk and Why.</title>
<date>2013</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1198" citStr="Hart et al., 2013" startWordPosition="169" endWordPosition="172">then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general electi</context>
</contexts>
<marker>Hart, Childers, Lind, 2013</marker>
<rawString>Roderick P. Hart, Jay P. Childers, and Colene J. Lind. 2013. Political Tone: How Leaders Talk and Why. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roderick P Hart</author>
</authors>
<title>Campaign talk: Why elections are good for us.</title>
<date>2009</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="1154" citStr="Hart, 2009" startWordPosition="163" endWordPosition="164">annotated with known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for</context>
</contexts>
<marker>Hart, 2009</marker>
<rawString>Roderick P. Hart. 2009. Campaign talk: Why elections are good for us. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Stephen Purpura</author>
<author>John Wilkerson</author>
</authors>
<title>Computer-assisted topic classification for mixed-methods social science research.</title>
<date>2008</date>
<journal>Journal of Information Technology &amp; Politics,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="3034" citStr="Hillard et al., 2008" startWordPosition="460" endWordPosition="463">lly concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on classifying the political ideology expressed by a piece of text (Laver et al., 2003; Monroe and Maeda, 2004; Hillard et al., 2008), we start from the assumption that a candidate’s choice of words and phrases reflects a deliberate attempt to signal common cause with a target audience, and as a broader strategy, to respond to political competitors. Our central hypothesis is that, despite candidates’ intentional vagueness, differences in position—among candidates or over time—can be automatically detected and described as proportions of ideologies expressed in a speech. In this work, we operationalize ideologies in a novel empirical way, exploiting political writings published in explicitly ideological books and magazines (</context>
</contexts>
<marker>Hillard, Purpura, Wilkerson, 2008</marker>
<rawString>Dustin Hillard, Stephen Purpura, and John Wilkerson. 2008. Computer-assisted topic classification for mixed-methods social science research. Journal of Information Technology &amp; Politics, 4(4):31–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Stability in competition.</title>
<date>1929</date>
<journal>The Economic Journal,</journal>
<volume>39</volume>
<issue>153</issue>
<contexts>
<context position="2308" citStr="Hotelling, 1929" startWordPosition="346" endWordPosition="347">ously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind of shake it up and we start all over again.” A more general observation, often stated but not yet, to our knowledge, tested empirically, is that 91 successful primary candidates “move to the center” before a general election. The expectation follows directly from long-standing and widely influential theories of political competition that are collectively referred to in their simplest form as the “median voter theorem” (Hotelling, 1929; Black, 1948; Downs, 1957). Thus it is to be expected that when a set of voters that are more ideologically concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on clas</context>
</contexts>
<marker>Hotelling, 1929</marker>
<rawString>Harold Hotelling. 1929. Stability in competition. The Economic Journal, 39(153):41–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data.</title>
<date>2003</date>
<journal>The American Political Science Review,</journal>
<volume>97</volume>
<issue>2</issue>
<contexts>
<context position="2987" citStr="Laver et al., 2003" startWordPosition="452" endWordPosition="455">hen a set of voters that are more ideologically concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on classifying the political ideology expressed by a piece of text (Laver et al., 2003; Monroe and Maeda, 2004; Hillard et al., 2008), we start from the assumption that a candidate’s choice of words and phrases reflects a deliberate attempt to signal common cause with a target audience, and as a broader strategy, to respond to political competitors. Our central hypothesis is that, despite candidates’ intentional vagueness, differences in position—among candidates or over time—can be automatically detected and described as proportions of ideologies expressed in a speech. In this work, we operationalize ideologies in a novel empirical way, exploiting political writings published </context>
<context position="39327" citStr="Laver et al., 2003" startWordPosition="6590" endWordPosition="6593">h and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of pr</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. 2003. Extracting policy positions from political texts using words as data. The American Political Science Review, 97(2):311–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Eric Xing</author>
<author>Alexander Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In Proceedings of ECML-PKDD.</booktitle>
<contexts>
<context position="39198" citStr="Lin et al., 2008" startWordPosition="6568" endWordPosition="6571"> data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labe</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In Proceedings of ECML-PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23936" citStr="Mihalcea, 2005" startWordPosition="3966" endWordPosition="3967">model with parameter ρ. This gives the transition distribution: p(sj |si, `; C, 0, ρ) = (1 − ρ)`+�ptree(sj |si; C, 0) + (1 − (1 − Note that, if ρ = 1, there is no Markovian dependency between states (i.e., there is always a restart), so CLIP reverts to a mixture model. This approach allows us to parameterize the full set of |I|2 transitions with O(|I|) parameters.7 Since the graph is a tree and the walks are not allowed to backtrack, the only ambiguity in the transition is due to the restart probability; this distinguishes CLIP from other algorithms based on random walks (Brin and Page, 1998; Mihalcea, 2005; Toutanova et al., 2004; Collins-Thompson and Callan, 2005). 3.3.2 Emission Parameterization Recall that, at time step t, CLIP emits a cue from the lexicon L and an integer-valued lag. For each state s, we let the probability of emitting cue w be denoted by ψs,w; 0s is a multinomial distribution over the entire lexicon L. This allows our approach to handle ambiguous cues that can associate with more than one ideology, and also to associate a cue with a different ideology than our cue discovery method proposed, if the signal from the data is sufficiently strong. We assume each lag to be genera</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burt L Monroe</author>
<author>Ko Maeda</author>
</authors>
<title>Talk’s cheap: Text-based estimation of rhetorical ideal-points. Presented at the Annual Meeting of the Society for Political Methodology.</title>
<date>2004</date>
<contexts>
<context position="3011" citStr="Monroe and Maeda, 2004" startWordPosition="456" endWordPosition="459">that are more ideologically concentrated are replaced by a set who are more widely dispersed across the ideological spectrum, as occurs in the transition between the United States primary and general elections, that candidates will present themselves as more moderate in an effort to capture enough votes to win. Do political candidates in fact stray ideologically at opportune moments? More specifically, can we measure candidates’ ideological positions from their prose at different times? Following much work on classifying the political ideology expressed by a piece of text (Laver et al., 2003; Monroe and Maeda, 2004; Hillard et al., 2008), we start from the assumption that a candidate’s choice of words and phrases reflects a deliberate attempt to signal common cause with a target audience, and as a broader strategy, to respond to political competitors. Our central hypothesis is that, despite candidates’ intentional vagueness, differences in position—among candidates or over time—can be automatically detected and described as proportions of ideologies expressed in a speech. In this work, we operationalize ideologies in a novel empirical way, exploiting political writings published in explicitly ideologica</context>
</contexts>
<marker>Monroe, Maeda, 2004</marker>
<rawString>Burt L. Monroe and Ko Maeda. 2004. Talk’s cheap: Text-based estimation of rhetorical ideal-points. Presented at the Annual Meeting of the Society for Political Methodology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In AAAI Symposium on Computational Approaches to Analysing Weblogs.</booktitle>
<contexts>
<context position="39365" citStr="Mullen and Malouf, 2006" startWordPosition="6596" endWordPosition="6599"> the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidate</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>Tony Mullen and Robert Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In AAAI Symposium on Computational Approaches to Analysing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith T Poole</author>
<author>Howard Rosenthal</author>
</authors>
<title>A spatial model for legislative roll call analysis.</title>
<date>1985</date>
<journal>American Journal of Political Science,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="16756" citStr="Poole and Rosenthal, 1985" startWordPosition="2724" endWordPosition="2728">nique for measuring ideology proportions in the prose of political candidates. We adopt a Bayesian approach that manages our uncertainty about the cue lexi94 con Z, the tendencies of political speakers to “flipflop” among ideological types, and the relative “distances” among different ideologies. The representation of a candidate’s ideology as a mixture among discrete, hierarchically related categories can be distinguished from continuous representations (“scaling” or “spatial” models) often used in political science, especially to infer positions from Congressional roll-call voting patterns (Poole and Rosenthal, 1985; Poole and Rosenthal, 2000; Clinton et al., 2004). Moreover, the ability to draw inferences about individual policy-makers’ ideologies from their votes on proposed legislation is severely limited by institutional constraints on the types of legislation that is actually subject to recorded votes. 3.1 Political Speeches Corpus We gathered transcribed speeches given by candidates of the two main parties (Democrats and Republicans) during the 2008 and 2012 Presidential election seasons. Each election season is comprised of two stages: (i) the primary elections, where candidates seek the support o</context>
<context position="38553" citStr="Poole and Rosenthal (1985)" startWordPosition="6468" endWordPosition="6471">xplore a richer set of linguistic cues and attributes beyond ideology (e.g., topics and framing on various issues). We plan to use CLIP as a text analysis method to support substantive inquiry in political science, such as following trends in expressed ideology over time. 6 Related Work As early as the 1960s, there has been research on modeling ideological beliefs using automated systems (Abelson and Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media </context>
</contexts>
<marker>Poole, Rosenthal, 1985</marker>
<rawString>Keith T. Poole and Howard Rosenthal. 1985. A spatial model for legislative roll call analysis. American Journal of Political Science, 29(2):357–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith T Poole</author>
<author>Howard Rosenthal</author>
</authors>
<title>Congress: A Political-Economic History of Roll Call Voting.</title>
<date>2000</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="16783" citStr="Poole and Rosenthal, 2000" startWordPosition="2729" endWordPosition="2732">y proportions in the prose of political candidates. We adopt a Bayesian approach that manages our uncertainty about the cue lexi94 con Z, the tendencies of political speakers to “flipflop” among ideological types, and the relative “distances” among different ideologies. The representation of a candidate’s ideology as a mixture among discrete, hierarchically related categories can be distinguished from continuous representations (“scaling” or “spatial” models) often used in political science, especially to infer positions from Congressional roll-call voting patterns (Poole and Rosenthal, 1985; Poole and Rosenthal, 2000; Clinton et al., 2004). Moreover, the ability to draw inferences about individual policy-makers’ ideologies from their votes on proposed legislation is severely limited by institutional constraints on the types of legislation that is actually subject to recorded votes. 3.1 Political Speeches Corpus We gathered transcribed speeches given by candidates of the two main parties (Democrats and Republicans) during the 2008 and 2012 Presidential election seasons. Each election season is comprised of two stages: (i) the primary elections, where candidates seek the support of their respective parties </context>
</contexts>
<marker>Poole, Rosenthal, 2000</marker>
<rawString>Keith T. Poole and Howard Rosenthal. 2000. Congress: A Political-Economic History of Roll Call Voting. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren Sack</author>
</authors>
<title>Actor-role analysis: ideology, point of view, and the news. Master’s thesis,</title>
<date>1994</date>
<institution>Massachusetts Institute of Technology,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="38374" citStr="Sack, 1994" startWordPosition="6445" endWordPosition="6446">P’s deviations from the hypotheses are suggestive of potential improvements to cue extraction (§2), but also of incorrect hypotheses. We expect future research to explore a richer set of linguistic cues and attributes beyond ideology (e.g., topics and framing on various issues). We plan to use CLIP as a text analysis method to support substantive inquiry in political science, such as following trends in expressed ideology over time. 6 Related Work As early as the 1960s, there has been research on modeling ideological beliefs using automated systems (Abelson and Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate </context>
</contexts>
<marker>Sack, 1994</marker>
<rawString>Warren Sack. 1994. Actor-role analysis: ideology, point of view, and the news. Master’s thesis, Massachusetts Institute of Technology, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl-Michael Schneider</author>
</authors>
<title>Weighted average pointwise mutual information for feature selection in text categorization.</title>
<date>2005</date>
<booktitle>In Proceedings of PKDD.</booktitle>
<contexts>
<context position="13537" citStr="Schneider, 2005" startWordPosition="2209" endWordPosition="2210">. Further, domain experts found them harder to interpret out of context compared to bigrams and trigrams. We therefore included only bigrams and trigrams as terms in our cue discovery model. 2.4 Validation The term selection method we have described can be understood as a form of feature selection that reasons globally about the data and tries to control for some effects that are not of interest (topic or document idiosyncrasies). We compared the approach to two classic, simple methods for feature selection: ranking based on pointwise mutual information (PMI) and weighted average PMI (WAPMI) (Schneider, 2005; Cover and Thomas, 2012). Selected features were used to classify the ideologies of held-out documents from our corpus.5 We evaluated these feature selection methods within naive Bayes classification in a 5-fold crossvalidation setup. We vary A for the SAGE model and compare the results to equal-sized sets of terms selected by PMI and WAPMI. We consider SAGE with and without topic effects. Figure 2 visualizes accuracy against the number of features for each method. Bigrams and trigrams consistently outperform unigrams (McNemar’s, p &lt; 0.05). Otherwise, there are no significant differences in p</context>
</contexts>
<marker>Schneider, 2005</marker>
<rawString>Karl-Michael Schneider. 2005. Weighted average pointwise mutual information for feature selection in text categorization. In Proceedings of PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="39449" citStr="Somasundaran and Wiebe, 2009" startWordPosition="6609" endWordPosition="6613">to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 2012 Presidential elections. Acknowledgments For thoughtful feedba</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1249" citStr="Thomas et al., 2006" startWordPosition="176" endWordPosition="179">andidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-</context>
<context position="38948" citStr="Thomas et al. (2006)" startWordPosition="6525" endWordPosition="6528">nd Carroll, 1965; Carbonell, 1978; Sack, 1994). These early works model ideology at a sophisticated level, involving the actors, actions and goals; they require manually constructed knowledge bases. Poole and Rosenthal (1985) used congressional roll call data to demonstrate the ideological divide in Congress, and provided a methodology for measuring ideological positions. Gerrish and Blei (2011; 2012) augmented the methodology with text from congressional bills using probabilistic models to uncover lawmakers’ positions on specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (Somasundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated synta</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: determining support or opposition from congressional floor-debate transcripts. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="23960" citStr="Toutanova et al., 2004" startWordPosition="3968" endWordPosition="3971">eter ρ. This gives the transition distribution: p(sj |si, `; C, 0, ρ) = (1 − ρ)`+�ptree(sj |si; C, 0) + (1 − (1 − Note that, if ρ = 1, there is no Markovian dependency between states (i.e., there is always a restart), so CLIP reverts to a mixture model. This approach allows us to parameterize the full set of |I|2 transitions with O(|I|) parameters.7 Since the graph is a tree and the walks are not allowed to backtrack, the only ambiguity in the transition is due to the restart probability; this distinguishes CLIP from other algorithms based on random walks (Brin and Page, 1998; Mihalcea, 2005; Toutanova et al., 2004; Collins-Thompson and Callan, 2005). 3.3.2 Emission Parameterization Recall that, at time step t, CLIP emits a cue from the lexicon L and an integer-valued lag. For each state s, we let the probability of emitting cue w be denoted by ψs,w; 0s is a multinomial distribution over the entire lexicon L. This allows our approach to handle ambiguous cues that can associate with more than one ideology, and also to associate a cue with a different ideology than our cue discovery method proposed, if the signal from the data is sufficiently strong. We assume each lag to be generated by a Poisson distrib</context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In Proceedings of ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
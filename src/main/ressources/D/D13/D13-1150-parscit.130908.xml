<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.997166">
Where Not to Eat? Improving Public Policy by Predicting Hygiene
Inspections Using Online Reviews
</title>
<author confidence="0.946489">
Jun Seok Kang† Polina Kuznetsova††Department of Computer Science Michael Luca$ Yejin Choi†
</author>
<affiliation confidence="0.925159">
Stony Brook University $Harvard Business School
</affiliation>
<address confidence="0.965503">
Stony Brook, NY 11794-4400 Soldiers Field Road
{junkang,pkuznetsova,ychoi} Boston, MA 02163
</address>
<email confidence="0.9985">
@cs.stonybrook.edu mluca@hbs.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.976104130434783">
This paper offers an approach for governments
to harness the information contained in social
media in order to make public inspections and
disclosure more efficient. As a case study, we
turn to restaurant hygiene inspections – which
are done for restaurants throughout the United
States and in most of the world and are a fre-
quently cited example of public inspections
and disclosure. We present the first empiri-
cal study that shows the viability of statistical
models that learn the mapping between tex-
tual signals in restaurant reviews and the hy-
giene inspection records from the Department
of Public Health. The learned model achieves
over 82% accuracy in discriminating severe
offenders from places with no violation, and
provides insights into salient cues in reviews
that are indicative of the restaurant’s sanitary
conditions. Our study suggests that public
disclosure policy can be improved by mining
public opinions from social media to target in-
spections and to provide alternative forms of
disclosure to customers.
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947023255814">
Public health inspection records help customers to
be wary of restaurants that have violated health
codes. In some counties and cities, e.g., LA, NYC,
it is required for restaurants to post their inspec-
tion grades at their premises, which have shown
to affect the revenue of the business substantially
(e.g., Jin and Leslie (2005), Henson et al. (2006)),
thereby motivating restaurants to improve their sani-
tary practice. Other studies have reported correlation
between the frequency of unannounced inspections
per year, and the average violation scores, confirm-
ing the regulatory role of inspections in improving
the hygiene quality of the restaurants and decreasing
food-borne illness risks (e.g., Jin and Leslie (2003),
Jin and Leslie (2009), Filion and Powell (2009),
NYC-DoHMH (2012)).
However, one practical challenge in the current
inspection system is that the department of health
has only limited resources to dispatch inspectors,
leaving out a large number of restaurants with un-
known hygiene grades. We postulate that online re-
views written by the very citizens who have visited
those restaurants can serve as a proxy for predicting
the likely outcome of the health inspection of any
given restaurant. Such a prediction model can com-
plement the current inspection system by enlight-
ening the department of health to make a more in-
formed decision when allocating inspectors, and by
guiding customers when choosing restaurants.
Our work shares the spirit of recently emerging
studies that explores social media analysis for pub-
lic health surveillance, in particular, monitoring in-
fluenza or food-poisoning outbreaks from micro-
blogs (e.g., Aramaki et al. (2011), Sadilek et al.
(2012b), Sadilek et al. (2012a), Sadilek et al. (2013),
Lamb et al. (2013), Dredze et al. (2013), von Etter
et al. (2010)). However, no prior work has examined
the utility of review analysis as a predictive tool for
accessing hygiene of restaurants, perhaps because
the connection is not entirely conspicuous: after all,
customers are neither familiar with inspection codes,
nor have the full access to the kitchen, nor have been
asked to report on the hygiene aspects of their expe-
</bodyText>
<page confidence="0.828411">
1443
</page>
<note confidence="0.520454">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448,
</note>
<page confidence="0.271553">
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</page>
<figure confidence="0.999425882352941">
0 10 20 30 40 50
0 10 20 30 40 50
(b)
(a)
0.10
0.05
0
Inspection Penalty Score Threshold
CoefÞcient
0.05
0.05
CoefÞcient
0.10
0.05
0
0.10
0.10
0 10 20 30 40 50
(a)
0 10 20 30 40 50
(b)
bimodality*
bimodality (filtered)*
fake review count*
fake review count (filtered)
review count*
review count (filtered)*
np review count*
np review count (filtered)*
avg review rating*
avg review rating (filtered)*
avg review length*
avg review length(filtered)*
Inspection Penalty Score Threshold
</figure>
<figureCaption confidence="0.999681">
Figure 1: Spearman’s coefficients of factors &amp; inspection
penalty scores. ‘*’: statistically significant (p &lt; 0.05)
</figureCaption>
<bodyText confidence="0.996778142857143">
rience.
In this work, we report the first empirical study
demonstrating the utility of review analysis for pre-
dicting health inspections, achieving over 82% accu-
racy in discriminating severe offenders from places
with no violation, and find predictive cues in reviews
that correlate with the inspection results.
</bodyText>
<sectionHeader confidence="0.988848" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9999903">
We scraped entire reviews written for restaurants in
Seattle from Yelp over the period of 2006 to 2013.1
The inspection records of Seattle is publicly avail-
able at www.datakc.org. More than 50% of the
restaurants listed under Yelp did not have inspection
records, implying the limited coverage of inspec-
tions. We converted street addresses into canonical
forms when matching restaurants between Yelp and
inspection database. After integrating reviews with
inspection records, we obtained about 13k inspec-
</bodyText>
<footnote confidence="0.9282595">
1Available at http://www.cs.stonybrook.edu/
˜junkang/hygiene/
</footnote>
<figureCaption confidence="0.9996005">
Figure 2: Spearman’s coefficients of factors &amp; inspection
penalty scores. ‘*’: statistically significant (p &lt; 0.05)
</figureCaption>
<bodyText confidence="0.99989547826087">
tions over 1,756 restaurants with 152k reviews. For
each restaurant, there are typically several inspec-
tion records. We defined an “inspection period” of
each inspection record as the period of time start-
ing from the day after the previous inspection to the
day of the current inspection. If there is no previ-
ous inspection, then the period stretches to the past
6 months in time. Each inspection period corre-
sponds to an instance in the training or test set. We
merge all reviews within an inspection period into
one document when creating the feature vector.
Note that non-zero penalty scores may not nec-
essarily indicate alarming hygiene issues. For ex-
ample, violating codes such as “proper labeling” or
“proper consumer advisory postedfor raw or under-
cookedfoods” seem relatively minor, and unlikely to
be noted and mentioned by reviewers. Therefore, we
focus on restaurants with severe violations, as they
are exactly the set of restaurants that inspectors and
customers need to pay the most attention to. To de-
fine restaurants with ”severe violations” we experi-
ment with a varying threshold t, such that restaurants
with score &gt; t are labeled as “unhygienic”.2
</bodyText>
<sectionHeader confidence="0.77347" genericHeader="method">
3 Correlates of Inspection Penalty Scores
</sectionHeader>
<bodyText confidence="0.997414">
We examine correlation between penalty scores and
several statistics of reviews:
</bodyText>
<subsectionHeader confidence="0.380902">
I. Volume of Reviews:
</subsectionHeader>
<footnote confidence="0.756810666666667">
2For restaurants with “hygienic” labels, we only consider
those without violation, as there are enough number of such
restaurants to keep balanced distribution between two classes.
</footnote>
<figure confidence="0.966549769230769">
0 10 20 30 40 50
(c)
0 10 20 30 40 50
(d)
0.05
0
−0.03
CoefÞcient
−
C
−0.05
1444
Inspection Penalty Score Threshold
</figure>
<figureCaption confidence="0.999652">
Figure 3: Trend of penalty score thresholds &amp; accuracies.
</figureCaption>
<listItem confidence="0.999403">
• count of all reviews
• average length of all reviews
</listItem>
<bodyText confidence="0.8304325">
II. Sentiment of Reviews: We examine whether
the overall sentiment of the customers correlates
with the hygiene of the restaurants based on follow-
ing measures:
</bodyText>
<listItem confidence="0.999636">
• average review rating
• count of negative (G 3) reviews
</listItem>
<bodyText confidence="0.994148888888889">
III. Deceptiveness of Reviews: Restaurants with
bad hygiene status are more likely to attract negative
reviews, which would then motivate the restaurants
to solicit fake reviews. But it is also possible that
some of the most assiduous restaurants that abide
by health codes strictly are also diligent in solicit-
ing fake positive reviews. We therefore examine the
correlation between hygiene violations and the de-
gree of deception as follows.
</bodyText>
<listItem confidence="0.969808">
• bimodal distribution of review ratings
</listItem>
<bodyText confidence="0.9995275">
The work of Feng et al. (2012) has shown
that the shape of the distribution of opinions,
overtly skewed bimodal distributions in partic-
ular, can be a telltale sign of deceptive review-
ing activities. We approximately measure this
by computing the variance of review ratings.
</bodyText>
<listItem confidence="0.9269355">
• volume of deceptive reviews based on linguistic
patterns
</listItem>
<bodyText confidence="0.999635428571429">
We also explore the use of deception classifiers
based on linguistic patterns (Ott et al., 2011)
to measure the degree of deception. Since no
deception corpus is available in the restaurant
domain, we collected a set of fake reviews and
truthful reviews (250 reviews for each class),
following Ott et al. (2011).3
</bodyText>
<footnote confidence="0.8791">
310 fold cross validation on this dataset yields 79.2% accu-
racy based on unigram and bigram features.
</footnote>
<table confidence="0.999846166666667">
Features Acc. MSE SCC
- *50.00 0.500 -
review count *50.00 0.489 0.0005
np review count *52.94 0.522 0.0017
cuisine *66.18 0.227 0.1530
zip code *67.32 0.209 0.1669
avrg. rating *57.52 0.248 0.0091
inspection history *72.22 0.202 0.1961
unigram 78.43 0.461 0.1027
bigram *76.63 0.476 0.0523
unigram + bigram 82.68 0.442 0.0979
all 81.37 0.190 0.2642
</table>
<tableCaption confidence="0.972465333333333">
Table 1: Feature Compositions &amp; Respective Accuracies,
Respective Mean Squared Errors(MSE) &amp; Squared Cor-
relation Coefficients (SCC), np=non-positive
</tableCaption>
<bodyText confidence="0.991885322580645">
Filtering Reviews: When computing above statis-
tics over the set of reviews corresponding to each
restaurant, we also consider removing a subset of re-
views that might be dubious or just noise. In partic-
ular, we remove reviews that are too far away (delta
&gt; 2) from the average review rating. Another filter-
ing rule can be removing all reviews that are clas-
sified as deceptive by the deception classifier ex-
plained above. For brevity, we only show results
based on the first filtering rule, as we did not find
notable differences in different filtering strategies.
Results: Fig 1 and 2 show Spearman’s rank corre-
lation coefficient with respect to the statistics listed
above, with and without filtering, computed at dif-
ferent threshold cutoffs E 10, 10, 20, 30, 40, 50} of
inspection scores. Although coefficients are not
strong,4 they are mostly statistically significant with
p G 0.05 (marked with ’*’), and show interesting
contrastive trends as highlighted below.
In Fig 1, as expected, average review rating is neg-
atively correlated with the inspection penalty scores.
Interestingly, all three statistics corresponding to the
volume of customer reviews are positively corre-
lated with inspection penalty. What is more inter-
esting is that if potentially deceptive reviews are fil-
tered, then the correlation gets stronger, which sug-
gests the existence of deceptive reviews covering up
unhappy customers. Also notice that correlation is
4Spearman’s coefficient assumes monotonic correlation. We
suspect that the actual correlation of these factors and inspection
scores are not entirely monotonic.
</bodyText>
<figure confidence="0.86823">
0 10 20 30 40 50
80
60
61.42 61.46
66.61
70.83
77.16
81.37
Accuracy (%)
</figure>
<page confidence="0.68135">
1445
</page>
<table confidence="0.973105444444444">
Hygienic gross, mess, sticky, smell, restroom, dirty
Basic Ingredients: beef, pork, noodle, egg, soy,
ramen, pho,
Cuisines Vietnamese, Dim Sum, Thai, Mexican,
Japanese, Chinese, American, Pizza, Sushi, Indian,
Italian, Asian
Sentiment: cheap, never,
Service &amp; Atmosphere cash, worth, district, delivery,
think, really, thing, parking, always, usually, definitely
</table>
<listItem confidence="0.8124185">
- door: “The wait is always out the door when I
actually want to go there”,
- sticker: “I had sticker shock when I saw the prices.”,
- student: “heap, large portions and tasty = the perfect
</listItem>
<bodyText confidence="0.892852285714286">
student food!”,
- the size: “i was pretty astonished at the size of all the
plates for the money.”,
- was dry: “The beef was dry, the sweet soy and
anise-like sauce was TOO salty (almost inedible).”,
- pool: “There are pool tables, TV airing soccer games
from around the globe and of course - great drinks!”
</bodyText>
<tableCaption confidence="0.94828">
Table 2: Lexical Cues &amp; Examples - Unhygienic (dirty)
</tableCaption>
<bodyText confidence="0.9999319">
generally stronger when higher cutoffs are used (x-
axis), as expected. Fig 2 looks at the relation be-
tween the deception level and the inspection scores
more directly. As suspected, restaurants with high
penalty scores show increased level of deceptive re-
views.
Although various correlates of hygiene scores ex-
amined so far are insightful, these alone are not in-
formative enough to be used as a predictive tool,
hence we explore content-based classification next.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="method">
4 Content-based Prediction
</sectionHeader>
<bodyText confidence="0.9817895">
We examine the utility of the following features:
Features based on customers’ opinion:
</bodyText>
<listItem confidence="0.952062">
1. Aggregated opinion: average review rating
2. Content of the reviews: unigram, bigram
Features based on restaurant’s metadata:
3. Cuisine: e.g., Thai, Italian, as listed under Yelp
4. Location: first 5 digits of zip code
5. Inspection History: a boolean feature (“hy-
</listItem>
<bodyText confidence="0.725899">
gienic” or “unhygienic”), a numerical feature
(previous penalty score rescaled E [0,1]), a nu-
meric feature (average penalty score over all
previous inspections)
</bodyText>
<table confidence="0.998303357142857">
Hygienic:
Cooking Method &amp; Garnish: brew, frosting, grill,
crush, crust, taco, burrito, toast
Healthy or Fancier Ingredients: celery, calamity,
wine, broccoli, salad, flatbread, olive, pesto
Cuisines : Breakfast, Fish &amp; Chips, Fast Food,
German, Diner, Belgian, European, Sandwiches,
Vegetarian
Whom &amp; When: date, weekend, our, husband,
evening, night
Sentiment: lovely, yummy, generous, friendly, great,
nice
Service &amp; Atmosphere: selection, attitude,
atmosphere, ambiance, pretentious
</table>
<tableCaption confidence="0.997751">
Table 3: Lexical Cues &amp; Examples - Hygienic (clean)
</tableCaption>
<listItem confidence="0.513868">
6. Review Count
7. Non-positive Review Count
</listItem>
<bodyText confidence="0.984931172413793">
Classification Results We use liblinear’s SVM
(Fan et al., 2008) with L1 regularization and 10 fold
cross validation. We filter reviews that are farther
than 2 from the average rating. We also run Sup-
port Vector Regression (SVR) using liblinear. Fig 3
shows the results. As we increase the threshold, the
accuracy also goes up in most cases. Table 1 shows
feature ablation at threshold t = 50, and ‘*’ denotes
statistically significant (p&lt;0.05) difference over the
performance with all features based on student t-test.
We find that metadata information of restaurants
such as location and cuisine alone show good predic-
tive power, both above 66%, which are significantly
higher than the expected accuracy of random guess-
ing (50%).
Somewhat unexpected outcome is aggregated
opinion, which is the average review rating during
the corresponding inspection period, as it performs
not much better than chance (57.52%). This result
suggest that the task of hygiene prediction from re-
views differs from the task of sentiment classifica-
tion of reviews.
Interestingly, the inspection history feature alone
is highly informative, reaching accuracy upto 72%,
suggesting that the past performance is a good pre-
dictor of the future performance.
Textual content of the reviews (unigram+bigram)
turns out to be the most effective features, reaching
upto 82.68% accuracy. Lastly, when all the features
</bodyText>
<page confidence="0.982923">
1446
</page>
<bodyText confidence="0.999964333333333">
are combined together, the performance decreases
slightly to 81.37%, perhaps because n-gram features
perform drastically better than all others.
</bodyText>
<subsectionHeader confidence="0.988563">
4.1 Insightful Cues
</subsectionHeader>
<bodyText confidence="0.999495307692308">
Table 2 and 3 shows representative lexical cues for
each class with example sentences excerpted from
actual reviews when context can be helpful.
Hygiene: Interestingly, hygiene related words are
overwhelmingly negative, e.g., “gross”, “mess”,
“sticky”. What this suggests is that reviewers do
complain when the restaurants are noticeably dirty,
but do not seem to feel the need to complement on
cleanliness as often. Instead, they seem to focus on
other positive aspects of their experience, e.g., de-
tails of food, atmosphere, and their social occasions.
Service and Atmosphere: Discriminative fea-
tures reveal that it is not just the hygiene related
words that are predictive of the inspection results of
restaurants. It turns out that there are other quali-
ties of restaurants, such as service and atmosphere,
that also correlate with the likely outcome of inspec-
tions. For example, when reviewers feel the need
to talk about “door”, “student”, “sticker”, or “the
size” (see Table 2 and 3), one can extrapolate that
the overall experience probably was not glorious. In
contrast, words such as “selection”, “atmosphere”,
“ambiance” are predictive of hygienic restaurants,
even including those with slightly negative connota-
tion such as “attitude” or “pretentious”.
Whom and When: If reviewers talk about details
of their social occasions such as “date”, “husband”,
it seems to be a good sign.
The way food items are described: Another in-
teresting aspect of discriminative words are the way
food items are described by reviewers. In general,
mentions of basic ingredients of dishes, e.g., “noo-
dle”, “egg”, “soy” do not seem like a good sign. In
contrast, words that help describing the way dish is
prepared or decorated, e.g., “grill”, “toast”, “frost-
ing”, “bento box” “sugar” (as in “sugar coated”)
are good signs of satisfied customers.
Cuisines: Finally, cuisines have clear correlations
with inspection outcome, as shown in Table 2 and 3.
</bodyText>
<sectionHeader confidence="0.999617" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999991258064516">
There have been several recent studies that probe the
viability of public health surveillance by measuring
relevant textual signals in social media, in particu-
lar, micro-blogs (e.g., Aramaki et al. (2011), Sadilek
et al. (2012b), Sadilek et al. (2012a), Sadilek et al.
(2013), Lamb et al. (2013), Dredze et al. (2013), von
Etter et al. (2010)). Our work joins this line of re-
search but differs in two distinct ways. First, most
prior work aims to monitor a specific illness, e.g.,
influenza or food-poisoning by paying attention to
a relatively small set of keywords that are directly
relevant to the corresponding sickness. In contrast,
we examine all words people use in online reviews,
and draw insights on correlating terms and concepts
that may not seem immediately relevant to the hy-
giene status of restaurants, but nonetheless are pre-
dictive of the outcome of the inspections. Second,
our work is the first to examine online reviews in the
context of improving public policy, suggesting addi-
tional source of information for public policy mak-
ers to pay attention to.
Our work draws from the rich body of research
that studies online reviews for sentiment analysis
(e.g., Pang and Lee (2008)) and deception detec-
tion (e.g., Mihalcea and Strapparava (2009), Ott et
al. (2011), Feng et al. (2012)), while introducing
the new task of public hygiene prediction. We ex-
pect that previous studies for aspect-based sentiment
analysis (e.g., Titov and McDonald (2008), Brody
and Elhadad (2010), Wang et al. (2010)) would be a
fruitful venue for further investigation.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981333333333">
We have reported the first empirical study demon-
strating the promise of review analysis for predicting
health inspections, introducing a task that has poten-
tially significant societal benefits, while being rele-
vant to much research in NLP for opinion analysis
based on customer reviews.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988478">
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research, and in part by gift from Google. We thank
anonymous reviewers and Adam Sadilek for helpful
comments and suggestions.
</bodyText>
<page confidence="0.993003">
1447
</page>
<sectionHeader confidence="0.988203" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99605753">
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568–1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ’10,
pages 804–812, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu
Tran. 2013. Carmen: A twitter geolocation system
with applications to public health. In AAAI Workshop
on Expanding the Boundaries of Health Informatics
Using AI (HIAI).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871–1874.
Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012. Distributional footprints of deceptive
product reviews. In ICWSM.
Katie Filion and Douglas A Powell. 2009. The use of
restaurant inspection disclosure systems as a means of
communicating food safety information. Journal of
Foodservice, 20(6):287–297.
Spencer Henson, Shannon Majowicz, Oliver Masakure,
Paul Sockett, Anria Johnes, Robert Hart, Debora Carr,
and Lewinda Knowles. 2006. Consumer assessment
of the safety of restaurants: The role of inspection
notices and other information cues. Journal of Food
Safety, 26(4):275–301.
Ginger Zhe Jin and Phillip Leslie. 2003. The effect of
information on product quality: Evidence from restau-
rant hygiene grade cards. The Quarterly Journal of
Economics, 118(2):409–451.
Ginger Zhe Jin and Phillip Leslie. 2005. The case in
support of restaurant hygiene grade cards.
Ginger Zhe Jin and Phillip Leslie. 2009. Reputational
incentives for restaurant hygiene. American Economic
Journal: Microeconomics, pages 237–267.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL-HLT).
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309–
312, Suntec, Singapore, August. Association for Com-
putational Linguistics.
NYC-DoHMH. 2012. Restaurant grading in new york
city at 18 months. New York City Department of
Health and Mental Hygiene.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309–319, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012a. Predicting disease transmission from geo-
tagged micro-blog data. In Twenty-Sixth AAAI Con-
ference on Artificial Intelligence.
Adam Sadilek, Henry A. Kautz, and Vincent Silenzio.
2012b. Modeling spread of disease from social in-
teractions. In John G. Breslin, Nicole B. Ellison,
James G. Shanahan, and Zeynep Tufekci, editors,
ICWSM. The AAAI Press.
Adam Sadilek, Sean Brennan, Henry Kautz, and Vincent
Silenzio. 2013. nemesis: Which restaurants should
you avoid today? First AAAI Conference on Human
Computation and Crowdsourcing.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings ofACL-08: HLT, pages 308–316,
Columbus, Ohio, June. Association for Computational
Linguistics.
Peter von Etter, Silja Huttunen, Arto Vihavainen, Matti
Vuorinen, and Roman Yangarber. 2010. Assess-
ment of utility in web mining for the domain of pub-
lic health. In Proceedings of the NAACL HLT 2010
Second Louhi Workshop on Text and Data Mining of
Health Documents, pages 29–37, Los Angeles, Cal-
ifornia, USA, June. Association for Computational
Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a rat-
ing regression approach. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 783–792.
ACM.
</reference>
<page confidence="0.993987">
1448
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847956">
<title confidence="0.985821333333333">Eat? Improving Public Policy by Predicting Inspections Using Online Reviews Seok of Computer</title>
<author confidence="0.9442485">Stony Brook Business School Soldiers Field Road Boston</author>
<author confidence="0.9442485">MA mlucahbs edu Stony Brook</author>
<author confidence="0.9442485">NY</author>
<email confidence="0.999293">@cs.stonybrook.edu</email>
<abstract confidence="0.998543041666667">This paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efficient. As a case study, we turn to restaurant hygiene inspections – which are done for restaurants throughout the United States and in most of the world and are a frequently cited example of public inspections and disclosure. We present the first empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the Department of Public Health. The learned model achieves over 82% accuracy in discriminating severe offenders from places with no violation, and provides insights into salient cues in reviews that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Sachiko Maskawa</author>
<author>Mizuki Morita</author>
</authors>
<title>Twitter catches the flu: Detecting influenza epidemics using twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1568--1576</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="3080" citStr="Aramaki et al. (2011)" startWordPosition="464" endWordPosition="467">tten by the very citizens who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 14</context>
<context position="16953" citStr="Aramaki et al. (2011)" startWordPosition="2641" endWordPosition="2644"> mentions of basic ingredients of dishes, e.g., “noodle”, “egg”, “soy” do not seem like a good sign. In contrast, words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of</context>
</contexts>
<marker>Aramaki, Maskawa, Morita, 2011</marker>
<rawString>Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics using twitter. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1568–1576, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<contexts>
<context position="18238" citStr="Brody and Elhadad (2010)" startWordPosition="2853" endWordPosition="2856">f the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research, and in part by gift from Google. We thank anonymous reviewers and Adam Sadilek for helpful comments </context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In Human</author>
</authors>
<title>Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>804--812</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Human, 2010</marker>
<rawString>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 804–812, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Michael J Paul</author>
<author>Shane Bergsma</author>
<author>Hieu Tran</author>
</authors>
<title>Carmen: A twitter geolocation system with applications to public health.</title>
<date>2013</date>
<booktitle>In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</booktitle>
<contexts>
<context position="3193" citStr="Dredze et al. (2013)" startWordPosition="484" endWordPosition="487">e of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 0 10 20 3</context>
<context position="17066" citStr="Dredze et al. (2013)" startWordPosition="2661" endWordPosition="2664">words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to </context>
</contexts>
<marker>Dredze, Paul, Bergsma, Tran, 2013</marker>
<rawString>Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu Tran. 2013. Carmen: A twitter geolocation system with applications to public health. In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="13300" citStr="Fan et al., 2008" startWordPosition="2071" endWordPosition="2074">ish: brew, frosting, grill, crush, crust, taco, burrito, toast Healthy or Fancier Ingredients: celery, calamity, wine, broccoli, salad, flatbread, olive, pesto Cuisines : Breakfast, Fish &amp; Chips, Fast Food, German, Diner, Belgian, European, Sandwiches, Vegetarian Whom &amp; When: date, weekend, our, husband, evening, night Sentiment: lovely, yummy, generous, friendly, great, nice Service &amp; Atmosphere: selection, attitude, atmosphere, ambiance, pretentious Table 3: Lexical Cues &amp; Examples - Hygienic (clean) 6. Review Count 7. Non-positive Review Count Classification Results We use liblinear’s SVM (Fan et al., 2008) with L1 regularization and 10 fold cross validation. We filter reviews that are farther than 2 from the average rating. We also run Support Vector Regression (SVR) using liblinear. Fig 3 shows the results. As we increase the threshold, the accuracy also goes up in most cases. Table 1 shows feature ablation at threshold t = 50, and ‘*’ denotes statistically significant (p&lt;0.05) difference over the performance with all features based on student t-test. We find that metadata information of restaurants such as location and cuisine alone show good predictive power, both above 66%, which are signif</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Longfei Xing</author>
<author>Anupam Gogar</author>
<author>Yejin Choi</author>
</authors>
<title>Distributional footprints of deceptive product reviews.</title>
<date>2012</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="7838" citStr="Feng et al. (2012)" startWordPosition="1219" endWordPosition="1222">f the restaurants based on following measures: • average review rating • count of negative (G 3) reviews III. Deceptiveness of Reviews: Restaurants with bad hygiene status are more likely to attract negative reviews, which would then motivate the restaurants to solicit fake reviews. But it is also possible that some of the most assiduous restaurants that abide by health codes strictly are also diligent in soliciting fake positive reviews. We therefore examine the correlation between hygiene violations and the degree of deception as follows. • bimodal distribution of review ratings The work of Feng et al. (2012) has shown that the shape of the distribution of opinions, overtly skewed bimodal distributions in particular, can be a telltale sign of deceptive reviewing activities. We approximately measure this by computing the variance of review ratings. • volume of deceptive reviews based on linguistic patterns We also explore the use of deception classifiers based on linguistic patterns (Ott et al., 2011) to measure the degree of deception. Since no deception corpus is available in the restaurant domain, we collected a set of fake reviews and truthful reviews (250 reviews for each class), following Ott</context>
<context position="18048" citStr="Feng et al. (2012)" startWordPosition="2825" endWordPosition="2828">eviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was s</context>
</contexts>
<marker>Feng, Xing, Gogar, Choi, 2012</marker>
<rawString>Song Feng, Longfei Xing, Anupam Gogar, and Yejin Choi. 2012. Distributional footprints of deceptive product reviews. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katie Filion</author>
<author>Douglas A Powell</author>
</authors>
<title>The use of restaurant inspection disclosure systems as a means of communicating food safety information.</title>
<date>2009</date>
<journal>Journal of Foodservice,</journal>
<volume>20</volume>
<issue>6</issue>
<contexts>
<context position="2181" citStr="Filion and Powell (2009)" startWordPosition="324" endWordPosition="327">NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch inspectors, leaving out a large number of restaurants with unknown hygiene grades. We postulate that online reviews written by the very citizens who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocati</context>
</contexts>
<marker>Filion, Powell, 2009</marker>
<rawString>Katie Filion and Douglas A Powell. 2009. The use of restaurant inspection disclosure systems as a means of communicating food safety information. Journal of Foodservice, 20(6):287–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spencer Henson</author>
<author>Shannon Majowicz</author>
<author>Oliver Masakure</author>
<author>Paul Sockett</author>
<author>Anria Johnes</author>
<author>Robert Hart</author>
<author>Debora Carr</author>
<author>Lewinda Knowles</author>
</authors>
<title>Consumer assessment of the safety of restaurants: The role of inspection notices and other information cues.</title>
<date>2006</date>
<journal>Journal of Food Safety,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="1763" citStr="Henson et al. (2006)" startWordPosition="264" endWordPosition="267">that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers. 1 Introduction Public health inspection records help customers to be wary of restaurants that have violated health codes. In some counties and cities, e.g., LA, NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch inspectors, leaving out</context>
</contexts>
<marker>Henson, Majowicz, Masakure, Sockett, Johnes, Hart, Carr, Knowles, 2006</marker>
<rawString>Spencer Henson, Shannon Majowicz, Oliver Masakure, Paul Sockett, Anria Johnes, Robert Hart, Debora Carr, and Lewinda Knowles. 2006. Consumer assessment of the safety of restaurants: The role of inspection notices and other information cues. Journal of Food Safety, 26(4):275–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ginger Zhe Jin</author>
<author>Phillip Leslie</author>
</authors>
<title>The effect of information on product quality: Evidence from restaurant hygiene grade cards.</title>
<date>2003</date>
<journal>The Quarterly Journal of Economics,</journal>
<volume>118</volume>
<issue>2</issue>
<contexts>
<context position="2132" citStr="Jin and Leslie (2003)" startWordPosition="316" endWordPosition="319">codes. In some counties and cities, e.g., LA, NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch inspectors, leaving out a large number of restaurants with unknown hygiene grades. We postulate that online reviews written by the very citizens who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of heal</context>
</contexts>
<marker>Jin, Leslie, 2003</marker>
<rawString>Ginger Zhe Jin and Phillip Leslie. 2003. The effect of information on product quality: Evidence from restaurant hygiene grade cards. The Quarterly Journal of Economics, 118(2):409–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ginger Zhe Jin</author>
<author>Phillip Leslie</author>
</authors>
<title>The case in support of restaurant hygiene grade cards.</title>
<date>2005</date>
<contexts>
<context position="1741" citStr="Jin and Leslie (2005)" startWordPosition="260" endWordPosition="263">alient cues in reviews that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers. 1 Introduction Public health inspection records help customers to be wary of restaurants that have violated health codes. In some counties and cities, e.g., LA, NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch i</context>
</contexts>
<marker>Jin, Leslie, 2005</marker>
<rawString>Ginger Zhe Jin and Phillip Leslie. 2005. The case in support of restaurant hygiene grade cards.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ginger Zhe Jin</author>
<author>Phillip Leslie</author>
</authors>
<title>Reputational incentives for restaurant hygiene.</title>
<date>2009</date>
<journal>American Economic Journal: Microeconomics,</journal>
<pages>237--267</pages>
<contexts>
<context position="2155" citStr="Jin and Leslie (2009)" startWordPosition="320" endWordPosition="323"> and cities, e.g., LA, NYC, it is required for restaurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch inspectors, leaving out a large number of restaurants with unknown hygiene grades. We postulate that online reviews written by the very citizens who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more infor</context>
</contexts>
<marker>Jin, Leslie, 2009</marker>
<rawString>Ginger Zhe Jin and Phillip Leslie. 2009. Reputational incentives for restaurant hygiene. American Economic Journal: Microeconomics, pages 237–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lamb</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Separating fact from fear: Tracking flu infections on twitter.</title>
<date>2013</date>
<booktitle>In the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</booktitle>
<contexts>
<context position="3171" citStr="Lamb et al. (2013)" startWordPosition="480" endWordPosition="483">ng the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational</context>
<context position="17044" citStr="Lamb et al. (2013)" startWordPosition="2657" endWordPosition="2660"> sign. In contrast, words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our</context>
</contexts>
<marker>Lamb, Paul, Dredze, 2013</marker>
<rawString>Alex Lamb, Michael J. Paul, and Mark Dredze. 2013. Separating fact from fear: Tracking flu infections on twitter. In the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers,</booktitle>
<pages>309--312</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="18009" citStr="Mihalcea and Strapparava (2009)" startWordPosition="2817" endWordPosition="2820">ontrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer revie</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 309– 312, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NYC-DoHMH</author>
</authors>
<title>Restaurant grading in new york city at 18 months.</title>
<date>2012</date>
<institution>Department of Health and Mental Hygiene.</institution>
<location>New York City</location>
<contexts>
<context position="2199" citStr="NYC-DoHMH (2012)" startWordPosition="328" endWordPosition="329">staurants to post their inspection grades at their premises, which have shown to affect the revenue of the business substantially (e.g., Jin and Leslie (2005), Henson et al. (2006)), thereby motivating restaurants to improve their sanitary practice. Other studies have reported correlation between the frequency of unannounced inspections per year, and the average violation scores, confirming the regulatory role of inspections in improving the hygiene quality of the restaurants and decreasing food-borne illness risks (e.g., Jin and Leslie (2003), Jin and Leslie (2009), Filion and Powell (2009), NYC-DoHMH (2012)). However, one practical challenge in the current inspection system is that the department of health has only limited resources to dispatch inspectors, leaving out a large number of restaurants with unknown hygiene grades. We postulate that online reviews written by the very citizens who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and</context>
</contexts>
<marker>NYC-DoHMH, 2012</marker>
<rawString>NYC-DoHMH. 2012. Restaurant grading in new york city at 18 months. New York City Department of Health and Mental Hygiene.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8237" citStr="Ott et al., 2011" startWordPosition="1282" endWordPosition="1285">igent in soliciting fake positive reviews. We therefore examine the correlation between hygiene violations and the degree of deception as follows. • bimodal distribution of review ratings The work of Feng et al. (2012) has shown that the shape of the distribution of opinions, overtly skewed bimodal distributions in particular, can be a telltale sign of deceptive reviewing activities. We approximately measure this by computing the variance of review ratings. • volume of deceptive reviews based on linguistic patterns We also explore the use of deception classifiers based on linguistic patterns (Ott et al., 2011) to measure the degree of deception. Since no deception corpus is available in the restaurant domain, we collected a set of fake reviews and truthful reviews (250 reviews for each class), following Ott et al. (2011).3 310 fold cross validation on this dataset yields 79.2% accuracy based on unigram and bigram features. Features Acc. MSE SCC - *50.00 0.500 - review count *50.00 0.489 0.0005 np review count *52.94 0.522 0.0017 cuisine *66.18 0.227 0.1530 zip code *67.32 0.209 0.1669 avrg. rating *57.52 0.248 0.0091 inspection history *72.22 0.202 0.1961 unigram 78.43 0.461 0.1027 bigram *76.63 0.</context>
<context position="18028" citStr="Ott et al. (2011)" startWordPosition="2821" endWordPosition="2824">ple use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 309–319, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="17945" citStr="Pang and Lee (2008)" startWordPosition="2808" endWordPosition="2811">irectly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to m</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Predicting disease transmission from geotagged micro-blog data.</title>
<date>2012</date>
<booktitle>In Twenty-Sixth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3102" citStr="Sadilek et al. (2012" startWordPosition="468" endWordPosition="471">ns who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, Seattle, Wash</context>
<context position="16975" citStr="Sadilek et al. (2012" startWordPosition="2645" endWordPosition="2648">edients of dishes, e.g., “noodle”, “egg”, “soy” do not seem like a good sign. In contrast, words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but none</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry Kautz, and Vincent Silenzio. 2012a. Predicting disease transmission from geotagged micro-blog data. In Twenty-Sixth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Henry A Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>Modeling spread of disease from social interactions. In</title>
<date>2012</date>
<editor>John G. Breslin, Nicole B. Ellison, James G. Shanahan, and Zeynep Tufekci, editors, ICWSM.</editor>
<publisher>The AAAI Press.</publisher>
<contexts>
<context position="3102" citStr="Sadilek et al. (2012" startWordPosition="468" endWordPosition="471">ns who have visited those restaurants can serve as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, Seattle, Wash</context>
<context position="16975" citStr="Sadilek et al. (2012" startWordPosition="2645" endWordPosition="2648">edients of dishes, e.g., “noodle”, “egg”, “soy” do not seem like a good sign. In contrast, words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but none</context>
</contexts>
<marker>Sadilek, Kautz, Silenzio, 2012</marker>
<rawString>Adam Sadilek, Henry A. Kautz, and Vincent Silenzio. 2012b. Modeling spread of disease from social interactions. In John G. Breslin, Nicole B. Ellison, James G. Shanahan, and Zeynep Tufekci, editors, ICWSM. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Sadilek</author>
<author>Sean Brennan</author>
<author>Henry Kautz</author>
<author>Vincent Silenzio</author>
</authors>
<title>nemesis: Which restaurants should you avoid today?</title>
<date>2013</date>
<booktitle>First AAAI Conference on Human Computation and Crowdsourcing.</booktitle>
<contexts>
<context position="3151" citStr="Sadilek et al. (2013)" startWordPosition="476" endWordPosition="479">as a proxy for predicting the likely outcome of the health inspection of any given restaurant. Such a prediction model can complement the current inspection system by enlightening the department of health to make a more informed decision when allocating inspectors, and by guiding customers when choosing restaurants. Our work shares the spirit of recently emerging studies that explores social media analysis for public health surveillance, in particular, monitoring influenza or food-poisoning outbreaks from microblogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). However, no prior work has examined the utility of review analysis as a predictive tool for accessing hygiene of restaurants, perhaps because the connection is not entirely conspicuous: after all, customers are neither familiar with inspection codes, nor have the full access to the kitchen, nor have been asked to report on the hygiene aspects of their expe1443 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443–1448, Seattle, Washington, USA, 18-21 October 2013. c�2013 Associati</context>
<context position="17024" citStr="Sadilek et al. (2013)" startWordPosition="2653" endWordPosition="2656">do not seem like a good sign. In contrast, words that help describing the way dish is prepared or decorated, e.g., “grill”, “toast”, “frosting”, “bento box” “sugar” (as in “sugar coated”) are good signs of satisfied customers. Cuisines: Finally, cuisines have clear correlations with inspection outcome, as shown in Table 2 and 3. 5 Related Work There have been several recent studies that probe the viability of public health surveillance by measuring relevant textual signals in social media, in particular, micro-blogs (e.g., Aramaki et al. (2011), Sadilek et al. (2012b), Sadilek et al. (2012a), Sadilek et al. (2013), Lamb et al. (2013), Dredze et al. (2013), von Etter et al. (2010)). Our work joins this line of research but differs in two distinct ways. First, most prior work aims to monitor a specific illness, e.g., influenza or food-poisoning by paying attention to a relatively small set of keywords that are directly relevant to the corresponding sickness. In contrast, we examine all words people use in online reviews, and draw insights on correlating terms and concepts that may not seem immediately relevant to the hygiene status of restaurants, but nonetheless are predictive of the outcome of the insp</context>
</contexts>
<marker>Sadilek, Brennan, Kautz, Silenzio, 2013</marker>
<rawString>Adam Sadilek, Sean Brennan, Henry Kautz, and Vincent Silenzio. 2013. nemesis: Which restaurants should you avoid today? First AAAI Conference on Human Computation and Crowdsourcing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>308--316</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="18212" citStr="Titov and McDonald (2008)" startWordPosition="2849" endWordPosition="2852">predictive of the outcome of the inspections. Second, our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research, and in part by gift from Google. We thank anonymous reviewers and Adam Sad</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings ofACL-08: HLT, pages 308–316, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter von Etter</author>
<author>Silja Huttunen</author>
<author>Arto Vihavainen</author>
<author>Matti Vuorinen</author>
<author>Roman Yangarber</author>
</authors>
<title>Assessment of utility in web mining for the domain of public health.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents,</booktitle>
<pages>29--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA,</location>
<marker>von Etter, Huttunen, Vihavainen, Vuorinen, Yangarber, 2010</marker>
<rawString>Peter von Etter, Silja Huttunen, Arto Vihavainen, Matti Vuorinen, and Roman Yangarber. 2010. Assessment of utility in web mining for the domain of public health. In Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 29–37, Los Angeles, California, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Yue Lu</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Latent aspect rating analysis on review text data: a rating regression approach.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>783--792</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18258" citStr="Wang et al. (2010)" startWordPosition="2857" endWordPosition="2860"> our work is the first to examine online reviews in the context of improving public policy, suggesting additional source of information for public policy makers to pay attention to. Our work draws from the rich body of research that studies online reviews for sentiment analysis (e.g., Pang and Lee (2008)) and deception detection (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011), Feng et al. (2012)), while introducing the new task of public hygiene prediction. We expect that previous studies for aspect-based sentiment analysis (e.g., Titov and McDonald (2008), Brody and Elhadad (2010), Wang et al. (2010)) would be a fruitful venue for further investigation. 6 Conclusion We have reported the first empirical study demonstrating the promise of review analysis for predicting health inspections, introducing a task that has potentially significant societal benefits, while being relevant to much research in NLP for opinion analysis based on customer reviews. Acknowledgments This research was supported in part by the Stony Brook University Office of the Vice President for Research, and in part by gift from Google. We thank anonymous reviewers and Adam Sadilek for helpful comments and suggestions. 144</context>
</contexts>
<marker>Wang, Lu, Zhai, 2010</marker>
<rawString>Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783–792. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021688">
<title confidence="0.98509">
Animacy Detection with Voting Models
</title>
<note confidence="0.8048235">
Joshua L. Moore*
Dept. Computer Science
Cornell University
Ithaca, NY 14853
</note>
<email confidence="0.977926">
jlmo@cs.cornell.edu
</email>
<author confidence="0.676502">
Christopher J.C. Burges Erin Renshaw Wen-tau Yih
</author>
<affiliation confidence="0.675634">
Microsoft Research
</affiliation>
<address confidence="0.806686666666667">
One Microsoft Way
Redmond, WA 98052
{cburges, erinren, scottyih}
</address>
<email confidence="0.983267">
@microsoft.com
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997389">
Animacy detection is a problem whose solu-
tion has been shown to be beneficial for a
number of syntactic and semantic tasks. We
present a state-of-the-art system for this task
which uses a number of simple classifiers
with heterogeneous data sources in a voting
scheme. We show how this framework can
give us direct insight into the behavior of the
system, allowing us to more easily diagnose
sources of error.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989419047619">
Animacy detection has proven useful for a va-
riety of syntactic and semantic tasks, such as
anaphora and coreference resolution (Orˇasan and
Evans, 2007; Lee et al., 2013), verb argument dis-
ambiguation (Dell’Orletta et al., 2005) and depen-
dency parsing (Øvrelid and Nivre, 2007). Existing
approaches for animacy detection typically rely on
two types of information: linguistic databases, and
syntactic cues observed from the corpus. They usu-
ally combine two types of approaches: rule based
systems, and machine learning techniques. In this
paper we explore a slightly different angle: we wish
to design an animacy detector whose decisions are
interpretable and correctable, so that downstream
semantic modeling systems can revisit those deci-
sions as needed. Thus here, we avoid defining
a large number of features and then using a ma-
chine learning method such as boosted trees, since
such methods, although powerful, result in hard-to-
interpret systems. Instead, we explore combining
interpretable voting models using machine learning
</bodyText>
<note confidence="0.613133">
* Work performed while visiting Microsoft Research.
</note>
<page confidence="0.992755">
55
</page>
<bodyText confidence="0.999897125">
only to reweight their votes. We show that such
an approach can indeed result in a high perform-
ing system, with animacy detection accuracies in the
mid 90% range, which compares well with other re-
ported rates. Ensemble methods are well known (see
for example, Dietterich (2000)) but our focus here is
on using them for interpretability while still main-
taining accuracy.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.999175">
2.1 Definitions of Animacy
</subsectionHeader>
<bodyText confidence="0.965876208333333">
Previous work uses several different definitions of
animacy. Orˇasan and Evans (2007) define animacy
in the service of anaphora resolution: an NP is con-
sidered animate “if its referent can also be referred
to using one of the pronouns he, she, him, her, his,
hers, himself, herself, or a combination of such pro-
nouns (e.g. his/her )”. Although useful for the task
at hand, this has counterintuitive consequences: for
example, baby may be considered animate or inan-
imate, and ant is considered inanimate (Ibid., Fig-
ure 1). Others have argued that animacy should be
captured by a hierarchy or by categories (Aissen,
2003; Silverstein, 1986). For instance, Zaenen et
al. (2004) propose three levels of animacy (human,
other animate and inanimate), which cover ten cat-
egories of noun phrases, with categories like ORG
(organization), ANIM (animal) and MAC (intelli-
gent machines such as robots) categorised as other
animate. Bowman and Chopra (2012) report results
for animacy defined both this way and with the cat-
egories collapsed to a binary (animate, inanimate)
definition.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.963958">
2.2 Methods for Animacy Detection
</subsectionHeader>
<bodyText confidence="0.999931633333334">
Evans and Orˇasan (2000) propose a rule-based sys-
tem based on the WordNet taxonomy (Fellbaum,
1998). Each synset is ascribed a binary animacy
label based on its unique beginner. A given noun
is then associated with the fraction of its animate
synsets (where all synsets are taken to be animate
or inanimate) and one minus that fraction, similarly
for a given verb. Animacy is then ascribed by ap-
plying a series of rules imposing thresholds on those
fractions, together with rules (and a gazetteer) to de-
tect names and acronyms, and a rule triggered by the
occurrence of who, or reflexives, in the NP. In later
work, Orˇasan and Evans (2007) extend the algorithm
by propagating animacy labels in the WordNet graph
using a chi-squared test, and then apply a k-nearest
neighbor classifier based on four lexical features. In
their work, the only context used was the animacy of
the verb in the NP, for heads of subject NPs (e.g., the
subject of eat is typically animate). Øvrelid (2009)
and Bowman and Chopra (2012) extend this idea by
using dependency relations to generate features for
their classifier, enabled by corpora created by Zae-
nen et al. (2004). In another approach, Ji and Lin
(2009) apply a simple “relative-pronoun” pattern to
the Google n-gram corpus (Brants and Franz, 2006)
to assign animacy (see the List model in Section 5
for details). Although the animacy decision is again
context-independent, such a list provides a strong
baseline and thus benefit applications like anaphora
resolution (Lee et al., 2013).
</bodyText>
<sectionHeader confidence="0.987671" genericHeader="method">
3 The Task
</sectionHeader>
<bodyText confidence="0.999978142857143">
We adopt a definition of animacy closest to the bi-
nary version in Bowman and Chopra (2012): we
define an entity to be animate if it is alive and has
the ability to move under its own will. We adopt
this simple definition because it fits well with the
common meaning and is therefore less error prone,
both in terms of incorporation into higher level mod-
els, and for labeling (Orˇasan and Evans (2007) re-
port that the labeling of animacy tuned for anaphora
proved challenging for the judges). We also ap-
ply the label to single noun tokens where possible:
the only exceptions are compound names (“Sarah
Jones”) which are treated as single units. Thus,
for example, “puppy food” is treated as two words,
with puppy animate and food inanimate. A more
complete definition would extend this to all noun
phrases, so that puppy food as a unit would be inan-
imate, a notion we plan to revisit in future work.
Note that even this simple definition presents chal-
lenges, so that a binary label must be applied de-
pending on the predominant meaning. In “A plate
of chicken,” chicken is treated as inanimate since it
refers to food. In “Caruso (1873-1921) is consid-
ered one of the world’s best opera singers. He...,”
although at the time of writing clearly Caruso was
not alive, the token is still treated as animate here
because the subsequent writing refers to a live per-
son.
</bodyText>
<sectionHeader confidence="0.988646" genericHeader="method">
4 The Data
</sectionHeader>
<bodyText confidence="0.999995142857143">
We used the MC160 dataset, which is a subset of the
MCTest dataset and which is composed of 160 grade
level reading comprehension stories generated using
crowd sourcing (Richardson et al., 2013). Workers
were asked to write a short story (typically less than
300 words) with a target audience of 5 to 7 year
olds. The available vocabulary was limited to ap-
proximately 8000 words, to model the reading abil-
ity of a first or second grader. We labeled this data
for animacy using the definition given above. The
first 100 of the 160 stories were used as the training
set, and the remaining 60 were used for the test set.
These animacy labels will be made available on the
web site for MCTest (Richardson et al., 2013).
</bodyText>
<sectionHeader confidence="0.993974" genericHeader="method">
5 The Models
</sectionHeader>
<bodyText confidence="0.999942333333334">
Since one of our key goals is interpretability we
chose to use an ensemble of simple voting models.
Each model is able to vote for the categories Ani-
mal, Person, Inanimate, or to abstain. The distinc-
tion between Animal and Person is only used when
we combine votes, where Animal and Person votes
appear as distinct inputs for the final voting combi-
nation model. Some voters do not distinguish be-
tween Person and Animal, and vote for Animate or
Inanimate. Our models are:
List: The n-gram list method from (Ji and Lin,
2009). Here, the frequencies with which the rela-
tive pronouns who, where, when, and which occur
are considered. Any noun followed most frequently
by who is classified as Animate, and any other noun
</bodyText>
<page confidence="0.980613">
56
</page>
<bodyText confidence="0.995854157894737">
in the list is classified as Inanimate. This voter ab-
stains when the noun is not present in the list.
Anaphora Design: The WordNet-based approach
of Evans and Orˇasan (2000).
WordNet: A simple approach using WordNet.
This voter chooses Animal or Person if the unique
beginner of the first synset of the noun is either of
these, and Inanimate otherwise.
WordSim: This voter uses the contextual vector
space model of Yih and Qazvinian (2012) computed
using Wikipedia and LA Times data. It uses short
lists of hand-chosen signal words for the categories
Animal, Person, and Inanimate to produce a “re-
sponse” of the word to each category. This response
is equal to the maximum cosine similarity in the vec-
tor space of the query word to any signal word in the
category. The final vote goes to the category with
the highest response.
Name: We used an in-house named entity tagger.
This voter can recognize some inanimate entities
such as cities, but does not distinguish between peo-
ple and animals, and so can only vote Animate, Inan-
imate or Abstain.
Dictionaries: We use three different dictionary
sources (Simple English Wiktionary, Full English
Wiktionary, and the definitions found in Word-
Net) with a recursive dictionary crawling algorithm.
First, we fetch the first definition of the query and
use a dependency tree and simple heuristics to find
the head noun of the definition, ignoring qualifica-
tion NPs like “piece” or “member.” If this noun
belongs to a list of per-category signal words, the
voter stops and votes for that category. Otherwise,
the voter recursively runs on the found head noun.
To prevent cycling, if no prediction is made after 10
recursive lookups, the voter abstains.
Transfer: For each story, we first process each
sentence and detect instances of the patterns x
am/is/was/are/were y and y named x. In each of
these cases, we use majority vote of the remaining
voters to predict the animacy of y and transfer
its vote to x, applying this label (as a vote) to all
instances of x in the text.
The WordSim and Dictionaries voters share lists
of signal words, which were chosen early in the ex-
perimental process using the training set. The sig-
nal words for the Animal category were animal and
mammal1. Person contains person and people. Fi-
nally, Inanimate uses thing, object, space, place,
symbol, food, structure, sound, measure, and unit.
We considered two methods for combining vot-
ers: majority voting (where the reliable Name voter
overrides the others if it does not abstain) and a lin-
ear reweighting of votes. In the reweighting method,
a feature vector is formed from the votes. Except
for WordSim, this vector is an indicator vector of
the vote – either Animal, Person, Animate (if the
voter doesn’t distinguish between animals and peo-
ple), Inanimate, or Abstain.
For Dictionaries, the vector’s non-zero compo-
nent is multiplied by the number of remaining al-
lowed recursive calls that can be performed, plus one
(so that a success on the final lookup gives a 1). For
example, if the third lookup finds a signal word and
chooses Animal, then the component corresponding
to Animal will have a value of 9.
For WordSim, instead of an indicator vector, the
responses to each category are used, or an indica-
tor for abstain if the model does not contain the
word. If the word is in the model, a second vec-
tor is appended containing the ratio of the maximum
response to the second-largest response in the com-
ponent for the maximum response category. These
per-voter feature vectors are concatenated to form a
35 dimensional vector, and a linear SVM is trained
to obtain the weights for combining the votes.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9909787">
We used the POS tagger in MSR SPLAT (Quirk et
al., 2012) to extract nouns from the stories in the
MC160 dataset and used these as labeled examples
for the SVM. This resulted in 5,120 extracted nouns
in the 100 training stories and 3,009 in the 60 test
stories. We use five-fold cross-validation on the
training set to select the SVM parameters. 57.2%
of the training examples were inanimate, as were
58.1% of the test examples.
Table 1 gives the test accuracy of each voter. List
</bodyText>
<footnote confidence="0.9244465">
1This was found to work well given typical dictionary defi-
nitions despite the fact that people are also mammals.
</footnote>
<page confidence="0.996923">
57
</page>
<table confidence="0.9899325">
List Anaphora WNet WSim Dict Name
84.6 77.1 78.8 57.6 74.3 16.0
</table>
<tableCaption confidence="0.966298">
Table 1: Accuracy of various individual voters on the test
set. Abstentions are counted as errors. Note that Transfer
depends on a secondary source for classification, and is
therefore not listed here.
</tableCaption>
<table confidence="0.999923">
Majority SVM
N+WN+D+WS+AD+L 87.7 95.0
N+WN+WS 80.1 95.0
N+WN+D+WS+AD+L+T 87.4 95.0
N+WN+D+WS 86.4 94.8
N+WN+WS+AD+L 86.5 94.7
N+WN+D+WS+T 86.8 94.0
N+WN+D 86.1 93.7
N+WN 89.3 93.0
N+D 82.6 93.0
N+AD 87.6 89.4
N+L 85.4 88.9
</table>
<tableCaption confidence="0.944811666666667">
Table 2: Accuracy of various combinations of voters
among Name (N), Anaphora Design (AD), List (L),
WordNet (WN), WordSim (WS), Dictionary (D), and
Transfer (T) under majority voting and SVM schemes.
Bold indicates a statistically significant difference over
the next lower bolded entry with p &lt; 0.01, for the SVM.
</tableCaption>
<bodyText confidence="0.999627636363636">
comes out on top when taken alone, but we see in
later results that it is less critical when used with
other voters. Name performs poorly on its own, but
later we will see that it is a very accurate voter which
frequently abstains.
Table 2 gives the test performance of various com-
binations of voters, both under majority vote and
reweighting. Statistical significance was tested us-
ing a paired t-test, and bold indicates a method
was significant over the next lower bold line with
p value p &lt; 0.01. We see a very large gain from
the SVM reweighting: 14.9 points in the case of
Name+WordNet+WordSim.
In Table 3, we show the results of ablation exper-
iments on the voters. We see that the most valuable
sources of information are WordSim and Dictionar-
ies.
Finally, in Table 4, we show a breakdown of
which voters cause the most errors, for the majority
vote system. In this table, we considered only “fi-
nal errors,” i.e. errors that the entire system makes.
Over all such errors, we counted the number of times
</bodyText>
<table confidence="0.999729181818182">
Majority SVM
WordSim 87.6 93.7
SimpleWikt (dict) 87.3 94.1
FullWikt (dict) 86.4 94.3
Dict 87.4 94.5
Name 86.6 94.7
List 86.4 94.8
WordNet (dict) 88.7 94.8
WordNet 87.5 94.9
Anaphora Design 88.6 94.9
Transfer 87.7 95.0
</table>
<tableCaption confidence="0.991301">
Table 3: Test accuracy when leaving out various voters,
using both majority vote and and reweighting. Bold indi-
cates statistical significance over the next lower bold line
with p &lt; 0.01.
</tableCaption>
<bodyText confidence="0.999760692307692">
each voter chose incorrectly, giving a count of how
many times each voter contributed to a final error.
We see that the Anaphora Design system has the
largest number of errors on both train and test sets.
After this, WordNet, List, and WordNet (dict) are also
large sources of error. On the other hand, Name and
WordSim have very few errors, indicating high re-
liability. The table also gives the number of criti-
cal errors, where the voter selected the wrong cate-
gory and was a deciding vote (that is, when chang-
ing its vote would have resulted in a correct overall
classification). We see a similar pattern here, with
Anaphora Design causing the most errors and Word-
Sim and Name among the most reliable. We included
Anaphora Design even though it uses a different def-
inition of animacy, to determine if its vote was nev-
ertheless valuable.
Error tables such as these show how voting mod-
els are more interpretable and therefore correctable
compared to more complex learned models. The ta-
bles indicate the largest sources of error and sug-
gest changes that could be made to increase accu-
racy. For example, we could make significant gains
by improving WordNet, WordNet (dictionary), or
List, whereas there is relatively little reason to ad-
just WordSim or Name.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999493333333333">
We have shown that linear combinations of voting
models can give animacy detection rates in the mid
90% range. This is well above the accuracy found
</bodyText>
<page confidence="0.995793">
58
</page>
<table confidence="0.999694181818182">
Errors Critical
Train Test Train Test
Anaphora Design 555 266 117 76
WordNet 480 228 50 45
List 435 195 94 45
Transfer 410 237 54 58
WordNet (dict) 385 194 84 65
SimpleWikt (dict) 175 111 39 16
FullWikt (dict) 158 67 1 5
WordSim 107 89 11 19
Name 71 55 27 19
</table>
<tableCaption confidence="0.756677666666667">
Table 4: Errors column: number of errors on train and
test where a source voted incorrectly, and was thus at
least in part responsible for an error of the overall sys-
tem. Critical column: number of errors on train and test
where a source voted incorrectly, and in addition cast a
deciding vote. Results are for majority vote.
</tableCaption>
<bodyText confidence="0.999964444444444">
by using the n-gram method of (Ji and Lin, 2009),
which is used as an animacy detection component
in other systems. In this sense the work presented
here improves upon the state of the art, but there are
caveats, since other workers define animacy differ-
ently and so a direct comparison with their work is
not possible. Our method has the added advantage
of interpretability, which we believe will be useful
when using it as a component in a larger system.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999795">
We wish to thank Andrzej Pastusiak for his help with
the labeling tool.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998265652173913">
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language &amp; Linguis-
tic Theory, 21(3):435–483.
Samuel Bowman and Harshit Chopra. 2012. Automatic
animacy classification. In Proceedings of the NAACL-
HLT 2012 Student Research Workshop.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Felice Dell’Orletta, Alessandro Lenci, Simonetta Monte-
magni, and Vito Pirrelli. 2005. Climbing the path to
grammar: a maximum entropy model of subject/object
learning. In Proceedings of the Workshop on Psy-
chocomputational Models of Human Language Acqui-
sition, PMHLA ’05, pages 72–81, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple Classifier Systems, pages
1–15.
Richard Evans and Constantin Orˇasan. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154–162.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from Web-scale n-grams for un-
supervised person mention detection. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 220–229, Hong
Kong, December. City University of Hong Kong.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Constantin Orˇasan and Richard J. Evans. 2007. NP an-
imacy identification for anaphora resolution. Journal
of Artificial Intelligence Research (JAIR), 29:79–103.
Lilja Øvrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough – Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447–451.
Lilja Øvrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende. 2012.
MSR SPLAT, a language analysis toolkit. In Proceed-
ings of the Demonstration Session at the Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 21–24, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
Matthew Richardson, Chris Burges, and Erin Renshaw.
2013. MCTest: A challenge dataset for the open-
domain machine comprehension of text. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Michael Silverstein. 1986. Hierarchy of features and
ergativity. In P. Muysken and H. van Riemsdijk, ed-
itors, Features and Projections, pages 163–232. Foris
Publications Holland.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
</reference>
<page confidence="0.983496">
59
</page>
<reference confidence="0.9996046">
models. In Proceedings of NAACL-HLT, pages 616–
620, Montr´eal, Canada, June.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O’Connor, and Tom Wasow. 2004. An-
imacy encoding in English: Why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118–125,
Barcelona, Spain, July. Association for Computational
Linguistics.
</reference>
<page confidence="0.998351">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.338059">
<title confidence="0.995314">Animacy Detection with Voting Models</title>
<author confidence="0.426288">L</author>
<affiliation confidence="0.9552495">Dept. Computer Cornell</affiliation>
<address confidence="0.917454">Ithaca, NY</address>
<email confidence="0.999535">jlmo@cs.cornell.edu</email>
<author confidence="0.995841">Christopher J C Burges Erin Renshaw Wen-tau Yih</author>
<affiliation confidence="0.999704">Microsoft Research</affiliation>
<address confidence="0.994491">One Microsoft Way Redmond, WA 98052</address>
<email confidence="0.983548">erinren,@microsoft.com</email>
<abstract confidence="0.999082363636364">Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Judith Aissen</author>
</authors>
<title>Differential object marking: Iconicity vs.</title>
<date>2003</date>
<journal>economy. Natural Language &amp; Linguistic Theory,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="2834" citStr="Aissen, 2003" startWordPosition="444" endWordPosition="445">us work uses several different definitions of animacy. Orˇasan and Evans (2007) define animacy in the service of anaphora resolution: an NP is considered animate “if its referent can also be referred to using one of the pronouns he, she, him, her, his, hers, himself, herself, or a combination of such pronouns (e.g. his/her )”. Although useful for the task at hand, this has counterintuitive consequences: for example, baby may be considered animate or inanimate, and ant is considered inanimate (Ibid., Figure 1). Others have argued that animacy should be captured by a hierarchy or by categories (Aissen, 2003; Silverstein, 1986). For instance, Zaenen et al. (2004) propose three levels of animacy (human, other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other animate. Bowman and Chopra (2012) report results for animacy defined both this way and with the categories collapsed to a binary (animate, inanimate) definition. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60, Seattle, Washington, USA, 18-21 October 2013</context>
</contexts>
<marker>Aissen, 2003</marker>
<rawString>Judith Aissen. 2003. Differential object marking: Iconicity vs. economy. Natural Language &amp; Linguistic Theory, 21(3):435–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Bowman</author>
<author>Harshit Chopra</author>
</authors>
<title>Automatic animacy classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACLHLT 2012 Student Research Workshop.</booktitle>
<contexts>
<context position="3160" citStr="Bowman and Chopra (2012)" startWordPosition="491" endWordPosition="494">g. his/her )”. Although useful for the task at hand, this has counterintuitive consequences: for example, baby may be considered animate or inanimate, and ant is considered inanimate (Ibid., Figure 1). Others have argued that animacy should be captured by a hierarchy or by categories (Aissen, 2003; Silverstein, 1986). For instance, Zaenen et al. (2004) propose three levels of animacy (human, other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other animate. Bowman and Chopra (2012) report results for animacy defined both this way and with the categories collapsed to a binary (animate, inanimate) definition. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2.2 Methods for Animacy Detection Evans and Orˇasan (2000) propose a rule-based system based on the WordNet taxonomy (Fellbaum, 1998). Each synset is ascribed a binary animacy label based on its unique beginner. A given noun is then associated with the fraction of its animat</context>
<context position="4530" citStr="Bowman and Chopra (2012)" startWordPosition="715" endWordPosition="718">ed by applying a series of rules imposing thresholds on those fractions, together with rules (and a gazetteer) to detect names and acronyms, and a rule triggered by the occurrence of who, or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to the binary version in Bowman </context>
</contexts>
<marker>Bowman, Chopra, 2012</marker>
<rawString>Samuel Bowman and Harshit Chopra. 2012. Automatic animacy classification. In Proceedings of the NAACLHLT 2012 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="4806" citStr="Brants and Franz, 2006" startWordPosition="759" endWordPosition="762"> propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to the binary version in Bowman and Chopra (2012): we define an entity to be animate if it is alive and has the ability to move under its own will. We adopt this simple definition because it fits well with the common meaning and is therefore less error prone, both in terms of incorporation into higher level</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felice Dell’Orletta</author>
<author>Alessandro Lenci</author>
<author>Simonetta Montemagni</author>
<author>Vito Pirrelli</author>
</authors>
<title>Climbing the path to grammar: a maximum entropy model of subject/object learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition, PMHLA ’05,</booktitle>
<pages>72--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Dell’Orletta, Lenci, Montemagni, Pirrelli, 2005</marker>
<rawString>Felice Dell’Orletta, Alessandro Lenci, Simonetta Montemagni, and Vito Pirrelli. 2005. Climbing the path to grammar: a maximum entropy model of subject/object learning. In Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition, PMHLA ’05, pages 72–81, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<booktitle>In Multiple Classifier Systems,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="2080" citStr="Dietterich (2000)" startWordPosition="319" endWordPosition="320"> needed. Thus here, we avoid defining a large number of features and then using a machine learning method such as boosted trees, since such methods, although powerful, result in hard-tointerpret systems. Instead, we explore combining interpretable voting models using machine learning * Work performed while visiting Microsoft Research. 55 only to reweight their votes. We show that such an approach can indeed result in a high performing system, with animacy detection accuracies in the mid 90% range, which compares well with other reported rates. Ensemble methods are well known (see for example, Dietterich (2000)) but our focus here is on using them for interpretability while still maintaining accuracy. 2 Previous Work 2.1 Definitions of Animacy Previous work uses several different definitions of animacy. Orˇasan and Evans (2007) define animacy in the service of anaphora resolution: an NP is considered animate “if its referent can also be referred to using one of the pronouns he, she, him, her, his, hers, himself, herself, or a combination of such pronouns (e.g. his/her )”. Although useful for the task at hand, this has counterintuitive consequences: for example, baby may be considered animate or inan</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble methods in machine learning. In Multiple Classifier Systems, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
<author>Constantin Orˇasan</author>
</authors>
<title>Improving anaphora resolution by identifying animate entities in texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the Discourse Anaphora and Reference Resolution Conference (DAARC2000),</booktitle>
<pages>154--162</pages>
<marker>Evans, Orˇasan, 2000</marker>
<rawString>Richard Evans and Constantin Orˇasan. 2000. Improving anaphora resolution by identifying animate entities in texts. In Proceedings of the Discourse Anaphora and Reference Resolution Conference (DAARC2000), pages 154–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3618" citStr="Fellbaum, 1998" startWordPosition="559" endWordPosition="560">, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other animate. Bowman and Chopra (2012) report results for animacy defined both this way and with the categories collapsed to a binary (animate, inanimate) definition. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2.2 Methods for Animacy Detection Evans and Orˇasan (2000) propose a rule-based system based on the WordNet taxonomy (Fellbaum, 1998). Each synset is ascribed a binary animacy label based on its unique beginner. A given noun is then associated with the fraction of its animate synsets (where all synsets are taken to be animate or inanimate) and one minus that fraction, similarly for a given verb. Animacy is then ascribed by applying a series of rules imposing thresholds on those fractions, together with rules (and a gazetteer) to detect names and acronyms, and a rule triggered by the occurrence of who, or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Dekang Lin</author>
</authors>
<title>Gender and animacy knowledge discovery from Web-scale n-grams for unsupervised person mention detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>220--229</pages>
<institution>Hong Kong, December. City University of Hong Kong.</institution>
<contexts>
<context position="4711" citStr="Ji and Lin (2009)" startWordPosition="745" endWordPosition="748">or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to the binary version in Bowman and Chopra (2012): we define an entity to be animate if it is alive and has the ability to move under its own will. We adopt this simple definition because it fits well with the com</context>
<context position="7682" citStr="Ji and Lin, 2009" startWordPosition="1269" endWordPosition="1272">els will be made available on the web site for MCTest (Richardson et al., 2013). 5 The Models Since one of our key goals is interpretability we chose to use an ensemble of simple voting models. Each model is able to vote for the categories Animal, Person, Inanimate, or to abstain. The distinction between Animal and Person is only used when we combine votes, where Animal and Person votes appear as distinct inputs for the final voting combination model. Some voters do not distinguish between Person and Animal, and vote for Animate or Inanimate. Our models are: List: The n-gram list method from (Ji and Lin, 2009). Here, the frequencies with which the relative pronouns who, where, when, and which occur are considered. Any noun followed most frequently by who is classified as Animate, and any other noun 56 in the list is classified as Inanimate. This voter abstains when the noun is not present in the list. Anaphora Design: The WordNet-based approach of Evans and Orˇasan (2000). WordNet: A simple approach using WordNet. This voter chooses Animal or Person if the unique beginner of the first synset of the noun is either of these, and Inanimate otherwise. WordSim: This voter uses the contextual vector spac</context>
</contexts>
<marker>Ji, Lin, 2009</marker>
<rawString>Heng Ji and Dekang Lin. 2009. Gender and animacy knowledge discovery from Web-scale n-grams for unsupervised person mention detection. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, pages 220–229, Hong Kong, December. City University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entitycentric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="885" citStr="Lee et al., 2013" startWordPosition="133" endWordPosition="136">icrosoft.com Abstract Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error. 1 Introduction Animacy detection has proven useful for a variety of syntactic and semantic tasks, such as anaphora and coreference resolution (Orˇasan and Evans, 2007; Lee et al., 2013), verb argument disambiguation (Dell’Orletta et al., 2005) and dependency parsing (Øvrelid and Nivre, 2007). Existing approaches for animacy detection typically rely on two types of information: linguistic databases, and syntactic cues observed from the corpus. They usually combine two types of approaches: rule based systems, and machine learning techniques. In this paper we explore a slightly different angle: we wish to design an animacy detector whose decisions are interpretable and correctable, so that downstream semantic modeling systems can revisit those decisions as needed. Thus here, we</context>
<context position="5044" citStr="Lee et al., 2013" startWordPosition="796" endWordPosition="799">ubject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to the binary version in Bowman and Chopra (2012): we define an entity to be animate if it is alive and has the ability to move under its own will. We adopt this simple definition because it fits well with the common meaning and is therefore less error prone, both in terms of incorporation into higher level models, and for labeling (Orˇasan and Evans (2007) report that the labeling of animacy tuned for anaphora proved challenging for the judges). We also apply the label to single noun tokens where possible: the only exceptions are compound </context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orˇasan</author>
<author>Richard J Evans</author>
</authors>
<title>NP animacy identification for anaphora resolution.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>29--79</pages>
<marker>Orˇasan, Evans, 2007</marker>
<rawString>Constantin Orˇasan and Richard J. Evans. 2007. NP animacy identification for anaphora resolution. Journal of Artificial Intelligence Research (JAIR), 29:79–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
<author>Joakim Nivre</author>
</authors>
<title>When word order and part-of-speech tags are not enough – Swedish dependency parsing with rich linguistic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP),</booktitle>
<pages>447--451</pages>
<contexts>
<context position="992" citStr="Øvrelid and Nivre, 2007" startWordPosition="149" endWordPosition="152">for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error. 1 Introduction Animacy detection has proven useful for a variety of syntactic and semantic tasks, such as anaphora and coreference resolution (Orˇasan and Evans, 2007; Lee et al., 2013), verb argument disambiguation (Dell’Orletta et al., 2005) and dependency parsing (Øvrelid and Nivre, 2007). Existing approaches for animacy detection typically rely on two types of information: linguistic databases, and syntactic cues observed from the corpus. They usually combine two types of approaches: rule based systems, and machine learning techniques. In this paper we explore a slightly different angle: we wish to design an animacy detector whose decisions are interpretable and correctable, so that downstream semantic modeling systems can revisit those decisions as needed. Thus here, we avoid defining a large number of features and then using a machine learning method such as boosted trees, </context>
</contexts>
<marker>Øvrelid, Nivre, 2007</marker>
<rawString>Lilja Øvrelid and Joakim Nivre. 2007. When word order and part-of-speech tags are not enough – Swedish dependency parsing with rich linguistic features. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP), pages 447–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
</authors>
<title>Empirical evaluations of animacy annotation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="4501" citStr="Øvrelid (2009)" startWordPosition="712" endWordPosition="713">macy is then ascribed by applying a series of rules imposing thresholds on those fractions, together with rules (and a gazetteer) to detect names and acronyms, and a rule triggered by the occurrence of who, or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to </context>
</contexts>
<marker>Øvrelid, 2009</marker>
<rawString>Lilja Øvrelid. 2009. Empirical evaluations of animacy annotation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chris Quirk</author>
<author>Pallavi Choudhury</author>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
<author>Kristina Toutanova</author>
<author>Michael Gamon</author>
<author>Wentau Yih</author>
<author>Colin Cherry</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MSR SPLAT, a language analysis toolkit.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstration Session at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>21--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="11572" citStr="Quirk et al., 2012" startWordPosition="1934" endWordPosition="1937">omponent corresponding to Animal will have a value of 9. For WordSim, instead of an indicator vector, the responses to each category are used, or an indicator for abstain if the model does not contain the word. If the word is in the model, a second vector is appended containing the ratio of the maximum response to the second-largest response in the component for the maximum response category. These per-voter feature vectors are concatenated to form a 35 dimensional vector, and a linear SVM is trained to obtain the weights for combining the votes. 6 Results We used the POS tagger in MSR SPLAT (Quirk et al., 2012) to extract nouns from the stories in the MC160 dataset and used these as labeled examples for the SVM. This resulted in 5,120 extracted nouns in the 100 training stories and 3,009 in the 60 test stories. We use five-fold cross-validation on the training set to select the SVM parameters. 57.2% of the training examples were inanimate, as were 58.1% of the test examples. Table 1 gives the test accuracy of each voter. List 1This was found to work well given typical dictionary definitions despite the fact that people are also mammals. 57 List Anaphora WNet WSim Dict Name 84.6 77.1 78.8 57.6 74.3 1</context>
</contexts>
<marker>Quirk, Choudhury, Gao, Suzuki, Toutanova, Gamon, Yih, Cherry, Vanderwende, 2012</marker>
<rawString>Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami Suzuki, Kristina Toutanova, Michael Gamon, Wentau Yih, Colin Cherry, and Lucy Vanderwende. 2012. MSR SPLAT, a language analysis toolkit. In Proceedings of the Demonstration Session at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 21–24, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Chris Burges</author>
<author>Erin Renshaw</author>
</authors>
<title>MCTest: A challenge dataset for the opendomain machine comprehension of text.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6624" citStr="Richardson et al., 2013" startWordPosition="1077" endWordPosition="1080">ts challenges, so that a binary label must be applied depending on the predominant meaning. In “A plate of chicken,” chicken is treated as inanimate since it refers to food. In “Caruso (1873-1921) is considered one of the world’s best opera singers. He...,” although at the time of writing clearly Caruso was not alive, the token is still treated as animate here because the subsequent writing refers to a live person. 4 The Data We used the MC160 dataset, which is a subset of the MCTest dataset and which is composed of 160 grade level reading comprehension stories generated using crowd sourcing (Richardson et al., 2013). Workers were asked to write a short story (typically less than 300 words) with a target audience of 5 to 7 year olds. The available vocabulary was limited to approximately 8000 words, to model the reading ability of a first or second grader. We labeled this data for animacy using the definition given above. The first 100 of the 160 stories were used as the training set, and the remaining 60 were used for the test set. These animacy labels will be made available on the web site for MCTest (Richardson et al., 2013). 5 The Models Since one of our key goals is interpretability we chose to use an</context>
</contexts>
<marker>Richardson, Burges, Renshaw, 2013</marker>
<rawString>Matthew Richardson, Chris Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the opendomain machine comprehension of text. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Silverstein</author>
</authors>
<title>Hierarchy of features and ergativity.</title>
<date>1986</date>
<booktitle>Features and Projections,</booktitle>
<pages>163--232</pages>
<editor>In P. Muysken and H. van Riemsdijk, editors,</editor>
<publisher>Foris Publications Holland.</publisher>
<contexts>
<context position="2854" citStr="Silverstein, 1986" startWordPosition="446" endWordPosition="447">everal different definitions of animacy. Orˇasan and Evans (2007) define animacy in the service of anaphora resolution: an NP is considered animate “if its referent can also be referred to using one of the pronouns he, she, him, her, his, hers, himself, herself, or a combination of such pronouns (e.g. his/her )”. Although useful for the task at hand, this has counterintuitive consequences: for example, baby may be considered animate or inanimate, and ant is considered inanimate (Ibid., Figure 1). Others have argued that animacy should be captured by a hierarchy or by categories (Aissen, 2003; Silverstein, 1986). For instance, Zaenen et al. (2004) propose three levels of animacy (human, other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other animate. Bowman and Chopra (2012) report results for animacy defined both this way and with the categories collapsed to a binary (animate, inanimate) definition. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association</context>
</contexts>
<marker>Silverstein, 1986</marker>
<rawString>Michael Silverstein. 1986. Hierarchy of features and ergativity. In P. Muysken and H. van Riemsdijk, editors, Features and Projections, pages 163–232. Foris Publications Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Vahed Qazvinian</author>
</authors>
<title>Measuring word relatedness using heterogeneous vector space models.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>616--620</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="8317" citStr="Yih and Qazvinian (2012)" startWordPosition="1376" endWordPosition="1379"> frequencies with which the relative pronouns who, where, when, and which occur are considered. Any noun followed most frequently by who is classified as Animate, and any other noun 56 in the list is classified as Inanimate. This voter abstains when the noun is not present in the list. Anaphora Design: The WordNet-based approach of Evans and Orˇasan (2000). WordNet: A simple approach using WordNet. This voter chooses Animal or Person if the unique beginner of the first synset of the noun is either of these, and Inanimate otherwise. WordSim: This voter uses the contextual vector space model of Yih and Qazvinian (2012) computed using Wikipedia and LA Times data. It uses short lists of hand-chosen signal words for the categories Animal, Person, and Inanimate to produce a “response” of the word to each category. This response is equal to the maximum cosine similarity in the vector space of the query word to any signal word in the category. The final vote goes to the category with the highest response. Name: We used an in-house named entity tagger. This voter can recognize some inanimate entities such as cities, but does not distinguish between people and animals, and so can only vote Animate, Inanimate or Abs</context>
</contexts>
<marker>Yih, Qazvinian, 2012</marker>
<rawString>Wen-tau Yih and Vahed Qazvinian. 2012. Measuring word relatedness using heterogeneous vector space models. In Proceedings of NAACL-HLT, pages 616– 620, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wasow</author>
</authors>
<title>Animacy encoding in English: Why and how.</title>
<date>2004</date>
<booktitle>ACL 2004 Workshop on Discourse Annotation,</booktitle>
<pages>118--125</pages>
<editor>Annie Zaenen, Jean Carletta, Gregory Garretson, Joan Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina, M. Catherine O’Connor, and Tom</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<marker>Wasow, 2004</marker>
<rawString>Annie Zaenen, Jean Carletta, Gregory Garretson, Joan Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina, M. Catherine O’Connor, and Tom Wasow. 2004. Animacy encoding in English: Why and how. In Bonnie Webber and Donna K. Byron, editors, ACL 2004 Workshop on Discourse Annotation, pages 118–125, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
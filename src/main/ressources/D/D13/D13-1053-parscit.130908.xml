<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001137">
<title confidence="0.996774">
Factored Soft Source Syntactic Constraints for Hierarchical Machine
Translation
</title>
<author confidence="0.986922">
Zhongqiang Huang
</author>
<affiliation confidence="0.872846">
Raytheon BBN Technologies
</affiliation>
<address confidence="0.9599725">
50 Moulton St
Cambridge, MA, USA
</address>
<email confidence="0.995042">
zhuang@bbn.com
</email>
<author confidence="0.792649">
Jacob Devlin
</author>
<affiliation confidence="0.705415">
Raytheon BBN Technologies
</affiliation>
<address confidence="0.956234">
50 Moulton St
Cambridge, MA, USA
</address>
<email confidence="0.994295">
jdevlin@bbn.com
</email>
<author confidence="0.563919">
Rabih Zbib
</author>
<affiliation confidence="0.509837">
Raytheon BBN Technologies
</affiliation>
<address confidence="0.951523">
50 Moulton St
Cambridge, MA, USA
</address>
<email confidence="0.998588">
rzbib@bbn.com
</email>
<sectionHeader confidence="0.996654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955185185185">
This paper describes a factored approach to
incorporating soft source syntactic constraints
into a hierarchical phrase-based translation
system. In contrast to traditional approaches
that directly introduce syntactic constraints to
translation rules by explicitly decorating them
with syntactic annotations, which often ex-
acerbate the data sparsity problem and cause
other problems, our approach keeps transla-
tion rules intact and factorizes the use of syn-
tactic constraints through two separate mod-
els: 1) a syntax mismatch model that asso-
ciates each nonterminal of a translation rule
with a distribution of tags that is used to
measure the degree of syntactic compatibil-
ity of the translation rule on source spans; 2)
a syntax-based reordering model that predicts
whether a pair of sibling constituents in the
constituent parse tree of the source sentence
should be reordered or not when translated to
the target language. The features produced
by both models are used as soft constraints
to guide the translation process. Experiments
on Chinese-English translation show that the
proposed approach significantly improves a
strong string-to-dependency translation sys-
tem on multiple evaluation sets.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999062">
Hierarchical phrase-based translation models (Chi-
ang, 2007) are widely used in machine translation
systems due to their ability to achieve local flu-
ency through phrasal translation and handle non-
local phrase reordering using synchronous context-
free grammars. A large number of previous works
have tried to introduce grammaticality to the trans-
lation process by incorporating syntactic constraints
into hierarchical translation models. Despite some
differences in the granularity of syntax units (e.g.,
tree fragments (Galley et al., 2004; Liu et al., 2006),
treebank tags (Shen et al., 2008; Chiang, 2010), and
extended tags (Zollmann and Venugopal, 2006)),
most previous work incorporates syntax into hier-
archical translation models by explicitly decorating
translation rules with syntactic annotations. These
approaches inevitably exacerbate the data sparsity
problem and cause other problems such as increased
grammar size, worsened derivational ambiguity, and
unavoidable parsing errors (Hanneman and Lavie,
2013).
In this paper, we propose a factored approach
that incorporates soft source syntactic constraints
into a hierarchical string-to-dependency translation
model (Shen et al., 2008). The general ideas are ap-
plicable to other hierarchical models as well. Instead
of enriching translation rules with explicit syntactic
annotations, we keep the original translation rules
intact, and factorize the use of source syntactic con-
straints through two separate models.
The first is a syntax mismatch model that intro-
duces source syntax into the nonterminals of transla-
tion rules, and measures the degree of syntactic com-
patibility between a translation rule and the source
spans it is applied to during decoding. When a hi-
erarchical translation rule is extracted from a par-
allel training sentence pair, we determine a tag for
each nonterminal based on the dependency parse of
the source sentence. Instead of fragmenting rule
statistics by directly labeling nonterminals with tags,
</bodyText>
<page confidence="0.974809">
556
</page>
<note confidence="0.7339825">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556–566,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999825419354839">
we keep the original string-to-dependency transla-
tion rules intact and associate each nonterminal with
a distribution of tags. That distribution is then used
to measure the syntactic compatibility between the
syntactic context from which the translation rule is
extracted and the syntactic analysis of a test sen-
tence.
The second is a syntax-based reordering model
that takes advantage of phrasal cohesion in transla-
tion (Fox, 2002). The reordering model takes a pair
of sibling constituents in the source parse tree as in-
put, and uses source syntactic clues to predict the
ordering distribution (straight vs. inverted) of their
translations on the target side. The resulting order-
ing distribution is used in the decoder at the word
pair level to guide the translation process. This sep-
arate reordering model allows us to utilize source
syntax to improve reordering in hierarchical trans-
lation models without having to explicitly annotate
translation rules with source syntax.
Our results show that both the syntax mismatch
model and the syntax-based reordering model are
able to achieve significant gains over a strong
Chinese-English MT baseline. The rest of the pa-
per is organized as follows. Section 2 discusses
related work in the literature. Section 3 provides
an overview of our baseline string-to-dependency
translation system. Section 4 describes the details
of the syntax mismatch and syntax-based reordering
models. Experimental results are presented in Sec-
tion 5. The last section concludes the paper.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999958322580645">
Attempts to use rich syntactic annotations do not
always result in improved performance when com-
pared to purely hierarchical models that do not
use linguistic guidance. For example, as shown
in (Mi and Huang, 2008), tree-to-string translation
models (Huang et al., 2006) only start to outper-
form purely hierarchical models when significant ef-
forts were made to alleviate parsing errors by using
forest-based approaches in both rule extraction and
decoding. Using only syntactic phrases is too re-
strictive in phrasal translation as many useful phrase
pairs are not syntactic constituents (Koehn et al.,
2003). The syntax-augmented translation model
of Zollmann and Venugopal (2006) annotates non-
terminals in hierarchical rules with thousands of ex-
tended syntactic categories in order to capture the
syntactic variations of phrase pairs. This results
in exacerbated data sparsity problems, partially due
to the requirement of exact matches in nonterminal
substitutions between translation rules in the deriva-
tion. Several solutions were proposed. Shen et
al. (2009) and Chiang (2010) used soft match fea-
tures to explicitly model the substitution of nonter-
minals with different labels; Venugopal et al. (2009)
used a preference grammar to soften the syntactic
constraints through the use of a preference distribu-
tion of syntactic categories; and recently Hanneman
and Lavie (2013) proposed a clustering approach
to reduce the number of syntactic categories. Our
proposed syntax mismatch model associates non-
terminals with a distribution of tags. It is simi-
lar to the preference grammar in (Venugopal et al.,
2009); however, we use treebank tags and focus on
the syntactic compatibility between translation rules
and the source sentence. The work of Huang et al.
(2010) is most similar to ours, with the main differ-
ence being that their syntactic categories are latent
and learned automatically in a data driven fashion
while we simply use treebank tags based on depen-
dency parsing. Marton and Resnik (2008) also ex-
ploited soft source syntax constraints without mod-
ifying translation rules. However, they focused on
the quality of translation spans based on the syn-
tactic analysis of the source sentence, while our
method explicitly models the syntactic compatibil-
ity between translation rules and source spans.
Most research on reordering in machine transla-
tion focuses on phrase-based translation models as
they are inherently weak at non-local reordering.
Previous efforts to improve reordering for phrase-
based systems can be largely classified into two cat-
egories. Approaches in the first category try to re-
order words in the source sentence in a preprocess-
ing step to reduce reordering in both word alignment
and MT decoding. The reordering decisions are ei-
ther made using manual or automatically learned
rules (Collins et al., 2005; Xia and McCord, 2004;
Xia and McCord, 2004; Genzel, 2010) based on the
syntactic analysis of the source sentence, or con-
structed through an optimization procedure that uses
feature-based reordering models trained on a word-
aligned parallel corpus (Tromble and Eisner, 2009;
</bodyText>
<page confidence="0.994711">
557
</page>
<bodyText confidence="0.999873">
Khapra et al., 2013). Approaches in the second cate-
gory try to explicitly model phrase reordering in the
translation process. These approaches range from
simple distance based distortion models (Koehn et
al., 2003) that globally penalizes reordering based
on the distorted distance, to lexicalized reordering
models (Koehn et al., 2005; Al-Onaizan and Pap-
ineni, 2006) that assign reordering preferences of
adjacent phrases for individual phrases, and to hi-
erarchical reordering models (Galley and Manning,
2008; Cherry, 2013) that handle reordering prefer-
ences beyond adjacent phrases. Although hierarchi-
cal translation models are capable of handling non-
local reordering, their accuracy is far from perfect.
Xu et al. (2009) showed that the syntax-augmented
hierarchical model (Zollmann and Venugopal, 2006)
also benefits from reordering source words in a pre-
processing step. Explicitly adding syntax to trans-
lation rules helps with reordering in general, but it
introduces additional complexities, and is still lim-
ited by the context-free nature of hierarchical rules.
Our work exploits an alternative direction that uses
an external reordering model to improve word re-
ordering of hierarchical models. Gao et al. (2011),
Xiong et al. (2012), and Li et al. (2013) also studied
external reordering models for hierarchical models.
However, they focused on specific word pairs such
as a word and its dependents or a predicate and its
arguments, while our proposed general framework
considers all word pairs in a sentence. Our syntax-
based reordering model exploits phrasal cohesion in
translation (Fox, 2002) by modeling the reordering
of sibling constituents in the source parse tree, which
is similar to the recent work of Yang et al. (2012).
However, the latter focuses on finding the optimal
reordering of sibling constituents before MT decod-
ing, while our proposed model generates reordering
features that are used together with other MT fea-
tures to determine the optimal reordering during MT
decoding.
</bodyText>
<sectionHeader confidence="0.996878" genericHeader="method">
3 String-to-Dependency Translation
</sectionHeader>
<bodyText confidence="0.961500372549019">
Our baseline translation system is based on a string-
to-dependency translation model similar to the im-
plementation in (Shen et al., 2008). It is an extension
of the hierarchical translation model of Chiang et al.
(2006) that requires the target side of a phrase pair
to have a well-formed dependency structure, defined
as either of the two types:
• fixed structure: a single rooted dependency
sub-tree with each child being a complete con-
stituent. In this case, the phrase has a unique
head word inside the phrase, i.e., the root of
the dependency sub-tree. Each dependent of
the head word, together with all of its descen-
dants, is either completely inside the phrase or
completely outside the phrase. For example,
the phrase give him in Figure 1 (a) has a fixed
dependency structure with head word give.
• floating structure: a sequence of siblings with
each being a complete constituent. In this case,
the phrase is composed of a sequence of sibling
constituents whose common parent is outside
the phrase. For example, the phrase him that
brown coat in Figure 1 is a floating structure
whose common parent give is not in the phrase.
Requiring the target side to have a well-formed
dependency structure is less restrictive than requir-
ing it to be a syntactic constituent, allowing more
translation rules to be extracted. However, it still
results in fewer rules than pure hierarchical transla-
tion models and might hurt MT performance. The
well-formed dependency structure on the target side
makes it possible to introduce syntax features dur-
ing decoding. Shen et al. (2008) obtained signif-
icant improvements from including a dependency
language model score in decoding, outweighing the
negative effect of the dependency constraint. Shen et
al. (2009) proposed an approach to label each non-
terminal, which can be either on the left-hand-side
(LHS) or the right-hand-side (RHS) of the rule, with
the head POS tag of the underlying target phrase if
it has a fixed dependency structure1, and measure
the mismatches between nonterminal labels when a
RHS nonterminal of a rule is substantiated with the
LHS nonterminal of another rule during decoding.
This also resulted in further improvements in MT
performance. Figure 1 (c) shows an example string-
to-dependency translation rule in our baseline sys-
tem.
1Nonterminals corresponding to floating structures keep
their default label “X” as experiments show that it is not bene-
ficial to label them differently.
</bodyText>
<page confidence="0.97187">
558
</page>
<figure confidence="0.8390955625">
X : give X2 X1
WT *31Z� RJ Af� M fl
give him that brown coat
(a) word alignments
VV : give PRP2 NN1
W:0.7
NN : 0.1
X : 0.2
VV : give PRP2 NN1
X : X1 �ft X2
3 3 2 3
NN : 0.8 PN : 0.5
5 4 VV : 0.1 5 4 NN : 0.4 5
X : 0.1 X : 0.1
X : X1 � X2 X : X1 � X2
(b) pure hierarchical rule (c) string-to-dependency rule
</figure>
<figureCaption confidence="0.993100666666667">
Figure 1: An example of extracting a string-to-
dependency translation rule from word alignments. The
nonterminals on the target side of the hierarchical rule
(b) all correspond to fixed dependency structures and so
they are labeled by the respective head tag in the string-
to-dependency rule (c).
</figureCaption>
<sectionHeader confidence="0.982276" genericHeader="method">
4 Factored Syntactic Constraints
</sectionHeader>
<bodyText confidence="0.999945777777778">
Although the string-to-dependency formulation
helps to improve the grammaticality of translations,
it lacks the ability to incorporate source syntax into
the translation process. We next describe a factored
approach to address this problem by utilizing source
syntax through two models: one that introduces syn-
tactic awareness to translation rules themselves, and
another that focuses on reordering based on the syn-
tactic analysis of the source.
</bodyText>
<subsectionHeader confidence="0.988183">
4.1 Syntax Mismatch Model
</subsectionHeader>
<bodyText confidence="0.99997925">
A straightforward method to introduce awareness
of source syntax to translation rules is to apply
the same well-formed dependency constraint and
head POS annotation on the target side of string-
to-dependency translation rules to the source side.
However, as discussed earlier, this would signifi-
cantly reduce the number of rules that can be ex-
tracted, exacerbate data sparsity, and cause other
problems, especially given that the target side is al-
ready constrained by the dependency requirement.
A relaxed method is to bypass the dependency
constraint and only annotate source nonterminals
whose underlying phrase is a fixed dependency
structure with the head POS tag of the phrase. This
method would still extract all of the rules that can
be extracted from the baseline string-to-dependency
</bodyText>
<figure confidence="0.9994195">
(a) nonterminal tag distributions
(b) source span tags
</figure>
<figureCaption confidence="0.980002333333333">
Figure 2: Example distribution of tags for nonterminals
on the source side (a) and example tags for source spans
(b)
</figureCaption>
<bodyText confidence="0.999985296296296">
translation model, but the extra annotation on non-
terminals can split a rule into multiple rules, with the
only difference being the nonterminal labels on the
source side. Unfortunately, our experiments have
shown that even this moderate annotation results
in significantly lower translation quality due to the
fragmentation of translation rules, and the increased
derivational ambiguity. We have also tried to include
some source tag mismatch features (with details de-
scribed later) to measure the syntactic compatibility
between the nonterminal labels of a translation rule
and the corresponding tags of source spans. This im-
proves translation accuracy, but not enough to com-
pensate for the performance drop caused by annotat-
ing source nonterminals.
Our proposed method introduces syntax to trans-
lation rules without sacrificing performance. Instead
of imposing dependency constraints or explicitly an-
notating source nonterminals, we keep the original
string-to-dependency translation rules intact and as-
sociate each nonterminal on the source side with a
distribution of tags. The tags are determined based
on the dependency structure of training samples. If
the underlying source phrase of a nonterminal is a
fixed dependency structure in a training sample, we
use the head POS tag of the phrase as the tag. Oth-
erwise, we use the default tag “X” to denote float-
</bodyText>
<figure confidence="0.991714928571429">
me
RJ 笔 � f
,q
give
pen
VV
� 他
his
NN PN
span tag:
a
dependency:
source:
gross:
</figure>
<page confidence="0.943965">
559
</page>
<bodyText confidence="0.716825">
Feature Condition Value
</bodyText>
<equation confidence="0.9572334">
f1 ts = X P(tr = X)
f2 ts = X P(tr * X)
f3 ts * X P(tr = X)
f� ts * X P(tr = ts)
f� ts * X P(tr * X, tr * ts)
</equation>
<tableCaption confidence="0.956709">
Table 1: Source tag mismatch features. The default value
of each feature is zero if the source span tag ts does not
match the condition
</tableCaption>
<bodyText confidence="0.999654346153846">
ing structures and dependency structures that are not
well formed. As a result, we still extract the same
set of rules as in the baseline string-to-dependency
translation model, and also obtain a distribution of
tags for each nonterminal. Figure 2 (a) illustrates the
example tag distributions of a string-to-dependency
translation rule. The tag distributions provide an ap-
proximation of the source syntax of the training data
from which the translation rules are extracted. They
are used to measure the syntactic compatibility be-
tween a translation rule and the source spans it is
applied to. At decoding time, we parse the source
sentence and assign each span a tag in the same way
as it is done during rule extraction, as shown in the
example in Figure 2 (b). When a translation rule is
used to expand a derivation, for each nonterminal
(which can be on the LHS or RHS) on the source
side of the rule, five source tag mismatch features
are computed based on the distribution of tags P(tr)
on the rule nonterminal, and the tag ts on the cor-
responding source span. The features are defined in
Table 1. We use soft features instead of hard syn-
tactic constraints, and allow the tuning process to
choose the appropriate weight for each feature. As
shown in Section 5, these source syntax mismatch
features help to improve the baseline system.
</bodyText>
<subsectionHeader confidence="0.988619">
4.2 Syntax-based Reordering Model
</subsectionHeader>
<bodyText confidence="0.99992721875">
Most previous research on reordering models has fo-
cused on improving word reordering for statistical
phrase-based translation systems (e.g., (Collins et
al., 2005; Al-Onaizan and Papineni, 2006; Tromble
and Eisner, 2009)). There has been less work on im-
proving the reordering of hierarchical phrase-based
translation systems (see (Xu et al., 2009; Gao et al.,
2011; Xiong et al., 2012) for a few exceptions), ex-
cept through explicit syntactic annotation of transla-
tion rules. It is generally assumed that hierarchical
models are inherently capable of handling both lo-
cal and non-local reorderings. However, many hier-
archical translation rules are noisy and have limited
context, and so may not be able to produce transla-
tions in the right order.
We propose a general framework that incorpo-
rates external reordering information into the decod-
ing process of hierarchical translation models. To
simplify the presentation, we make the assumption
that every source word translates to one or more
target words, and that the translations for a pair
of source words is either straight or inverted. We
discuss the general case later. Given a sentence
w1,•••,wn, suppose we have a separate reordering
model that predicts Porder(oij), the probability distri-
bution of ordering oij E {straight, inverted} between
the translations of any source word pair (wi, wj).
We can measure the goodness of a given hypothe-
sis h with respect to the ordering predicted by the
reordering model as the sum of log probabilities2
for ordering each pair of source words, as defined
in Equation 1:
</bodyText>
<equation confidence="0.977759">
forder(h) = E logPorder(oij = o�ij) (1)
1≤i&lt;j≤n
</equation>
<bodyText confidence="0.9206">
where o� is the ordering between the translations of
source word pair (wi, wj) in hypothesis h. The re-
ordering score forder(h) can be computed efficiently
through recursion during hierarchical decoding as
follows:
• Base case: for phrasal (i.e. non-hierarchical)
rules, the ordering of translations for any word
pair covered by the source phrase can be deter-
mined based on the word alignment of the rule.
The value of the reordering score can be simply
computed according to Equation 1.
</bodyText>
<listItem confidence="0.949857">
• Recursive case: when a hierarchical rule is used
to expand a partial derivation, two types of
word pairs are encountered: a) word pairs that
are covered exclusively by one of the nonter-
minals on the RHS of the rule, and b) other
</listItem>
<footnote confidence="0.995934">
2In practice, the log probability is thresholded to avoid neg-
ative infinity, which would otherwise result in a hard constraint.
</footnote>
<page confidence="0.978792">
560
</page>
<figure confidence="0.9794691875">
VV : give PRP2 NN1
X : X1 � X2
(a)
source:
gross:
� �
his
� �
pen
�
R
me
, �f
q
give
Reordering features
</figure>
<bodyText confidence="0.993894333333333">
The syntactic production rule
The syntactic labels of the nodes in the context
The head POS tags of the nodes in the context
The dep. labels of the nodes in the context
The seq. of dep. labels connecting the two nodes
The length of the nodes in the context
</bodyText>
<table confidence="0.989481818181818">
word pair translation order
(他, � )inverted
(他, 我) inverted
(的, gip) inverted
(的, 我) inverted
(笔, gip)inverted
(笔, 我) inverted
(fp, 我) straight
(他, 的) previously considered
(他, 笔) previously considered
(的, 笔) previously considered
</table>
<figureCaption confidence="0.9954565">
Figure 3: An example rule application (a) with the trans-
lation order of new source word pairs covered by the rule
shown in (b). The translation order of word pairs covered
by Xi is previously considered and is thus not shown.
</figureCaption>
<bodyText confidence="0.99836972">
word pairs. The reordering scores of the for-
mer would be already computed in previous
rule applications, and can simply be retrieved
from the partial derivation. Word pairs of the
latter case are new word pairs introduced by the
hierarchical rule, and their ordering can be de-
termined based on the alignment of the hierar-
chical rule. The value of the reordering score
of the new derivation is the sum of the reorder-
ing scores retrieved from the partial derivations
for the nonterminals and the reordering scores
of the new word pairs.
Figure 3 shows an example of determining the
ordering of translations when applying a string-to-
dependency rule. The alignment in the translation
rule is able to fully determine the translation order
for all new word pairs introduced by the rule. For
example, “笔/pen” is covered by X1 in the rule and
the translation order for X1 and “给/give” is inverted
on the target side. Since “笔/pen” is translated to-
gether with other words covered by X1 as a group,
we can determine that the translation order between
the source word pair “笔/pen” and “给/give” is also
inverted on the target side. The words “他/his”,
“的”, “笔 /pen” are all covered by the same nonter-
</bodyText>
<tableCaption confidence="0.992065">
Table 2: Features in the reordering model
</tableCaption>
<bodyText confidence="0.999907121212121">
minal X1 and thus their pairwise reordering scores
have already been considered in previous rule appli-
cations.
In practice, not all source words in a translation
rule are translated to a target word; sometimes there
is no clear ordering between the translations of two
source words. In such cases we use a binary discount
feature instead of the reordering feature.
This reordering framework relies on an external
model to provide the ordering probability distribu-
tion of source word pairs. In this paper, we inves-
tigate a simple maximum-entropy reordering model
based on the syntactic parse tree of the source sen-
tence. This allows us to take advantage of the source
syntax to improve reordering without using syntactic
annotations in translation rules. The syntax-based
reordering model attempts to predict the reordering
probability of a pair of sibling constituents in the
source parse tree, building on the fact that syntac-
tic phrases tend to move in a group during transla-
tion (Fox, 2002). The reordering model is trained on
a word-aligned corpus. For each pair of sibling con-
stituents in the source parse tree, we determine the
translation order on the target side based on word
alignments. If there is a clear ordering3, i.e., either
straight or inverted, on the target side, we include
the context of the constituent pair and its translation
order as a sample for training or evaluating the max-
imum entropy reordering model. Table 2 lists the
features of the reordering model.
The ordering distributions of source word pairs
are determined based on the ordering distributions
of sibling constituent pairs. For each pair of sib-
</bodyText>
<footnote confidence="0.9491875">
3If the translations overlap with other, the non-overlapping
parts are used to determine the translation order.
</footnote>
<figure confidence="0.891829">
(b)
</figure>
<page confidence="0.993433">
561
</page>
<bodyText confidence="0.9998816">
ling constituents4 in the parse tree of a source sen-
tence, we compute its distribution of translation or-
der using the reordering model. The distribution is
shared among all word pairs covered by the respec-
tive constituents, which guarantees that the order-
ing distribution of any source word pair is computed
exactly once. The ordering distributions of source
word pairs are then used through the general reorder-
ing framework in the decoder to guide the decoding
process.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995371">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999709929824562">
Our main experiments use the Chinese-English par-
allel training data and development sets released by
the LDC, and made available to the DARPA GALE
and BOLT programs. We train the translation model
on 100 million words of parallel data. We use a 8 bil-
lion words of English monolingual data to train two
language models: a trigram language model used in
chart decoding, and a 5-gram language model used
in n-best rescoring. The systems are tuned and eval-
uated on a mixture of newswire and web forum text
from the development sets available for the DARPA
GALE and BOLT programs, with up to 4 indepen-
dent references for each source sentence. We also
evaluate our final systems on both newswire and
web text from the NIST MT06 and MT08 evalua-
tions using an experimental setup compatible with
the NIST MT12 Chinese-English constrained track.
In this setup, the translation and language models
are trained on 35 million words of parallel data and
3.8 billion words of English monolingual data, re-
spectively. The systems are tuned on the MT02-
05 development sets. All systems are tuned and
evaluated on IBM BLEU (Papineni et al., 2002).
The baseline string-to-dependency translation sys-
tem uses more than 10 core features and a large num-
ber of sparse binary features similar to the method
described in (Chiang et al., 2009). It achieves trans-
lation accuracies comparable to the top ranked sys-
tems in the NIST MT12 evaluation.
4Note that the constituent pairs used to train the reordering
model are filtered to only contain these with clear ordering on
the target side, while no such pre-filtering is applied to con-
stituent pairs when applying the reordering model in translation.
We leave it to future work to address this mismatch problem.
GIZA++ (Och and Ney, 2003) is used for auto-
matic word alignment in all of the experiments. We
use Charniak’s parser (Charniak and Johnson, 2005)
on the English side to obtain string-to-dependency
translation rules, and use a latent variable PCFG
parser (Huang and Harper, 2009) to parse the source
side of the parallel training data as well as the
test sentences for extracting syntax mismatch and
reordering features. For both languages, depen-
dency structures are read off constituency trees us-
ing manual head word percolation rules. We use
a lexicon-based longest-match-first word segmenter
to tokenize source Chinese sentences. Since the
source tokenization used in our MT system is dif-
ferent from the treebank tokenization used to train
the Chinese parser, the source sentences are first to-
kenized using the treebank-trained Stanford Chinese
segmenter (Tseng et al., 2005), then parsed with
the Chinese parser, and finally projected to MT tok-
enization based on the character alignment between
the tokens. The syntax-based reordering model is
trained on a set of Chinese-English manual word
alignment corpora released by the LDC5.
</bodyText>
<subsectionHeader confidence="0.998152">
5.2 Syntax Mismatch Model
</subsectionHeader>
<bodyText confidence="0.999942470588235">
We first conduct experiments on the GALE/BOLT
data sets to evaluate different strategies of incor-
porating source syntax into string-to-dependency
translation rules. As mentioned in Section 4.1, con-
straining the source side of translation rules to only
well-formed dependency structures is too restrictive
given that our baseline system already has depen-
dency constraint on the target side. We evaluate
the relaxed method that only annotates source non-
terminals with the head POS tag of the underlying
phrase if the phrase is a fixed dependency structure.
As shown in Table 3, nonterminal annotation results
in a big drop in performance, decreasing the BLEU
score of the baseline from 27.82 to 25.54. This sug-
gests that it is undesirable to further fragment the
translation rules. Introducing the syntax mismatch
features described in Section 4.1 helps to improve
</bodyText>
<footnote confidence="0.980225333333333">
5The alignment corpora are LDC2012E24, LDC2012E72,
LDC2012E95, and LDC2013E02. The reordering model can
also be trained on automatically aligned data; however, our ex-
periments show that using manual alignments results in a bet-
ter accuracy for the reordering model itself and more improve-
ments for the MT system.
</footnote>
<page confidence="0.981688">
562
</page>
<table confidence="0.998531">
BLEU
baseline 27.82
+ tag annotation only 25.54
+ tag annotation, mismatch feat. 25.90
+ tag distribution, mismatch feat. 28.23
</table>
<tableCaption confidence="0.992544666666667">
Table 3: Effects of tag annotation, tag distribution, and
syntax mismatch features on MT performance on the
GALE/BOLT data set.
</tableCaption>
<bodyText confidence="0.999956222222222">
BLEU from 25.54 to 25.90. This improvement is
not large enough to compensate for the performance
drop caused by annotating the nonterminals.
Our proposed approach, on the other hand, does
not modify the translation rules in the baseline sys-
tem, but only associates each nonterminal with a dis-
tribution of tags. For that reason, it does not suffer
from the aforementioned problem. It achieves ex-
actly the same performance as the baseline system
if no source syntactic constraints are imposed dur-
ing decoding. When the source syntax mismatch
features are used, the proposed approach is able to
achieve a gain of 0.41 in BLEU over the baseline
system. Table 4 lists the learned weights of the syn-
tax mismatch features after MT tuning. The nega-
tive weights of f1 and f2 mean that the MT system
penalizes source spans that do not have a fixed de-
pendency structure, and it assigns a higher penalty
to rules whose nonterminals have a high probability
of being extracted from source phrases that do not
have a fixed dependency structure. When the source
span has a fixed dependency structure, the MT sys-
tem prefers translation rules that have a high proba-
bility of matching the tag on the source span (feature
f4) over the ones that do not match (features f3 and
f5). This result is consistent with our expectations
of the syntax mismatch features.
</bodyText>
<subsectionHeader confidence="0.587208">
Feature Description Weight
</subsectionHeader>
<bodyText confidence="0.5851396">
f1 ts = X, tr = X −1.543
f2 ts = X, tr X −0.676
f3 ts X, tr = X 0.380
f4 ts X, tr X, tr = ts 1.677
f5 ts X, tr X, tr ts 0.232
</bodyText>
<tableCaption confidence="0.997701">
Table 4: Learned syntax mismatch feature weight
</tableCaption>
<subsectionHeader confidence="0.996704">
5.3 Syntax-based Reordering Model
</subsectionHeader>
<bodyText confidence="0.99989444">
Before evaluating the syntax-based reordering
model, we would like to establish the upper bound
improvement that could be achieved using the gen-
eral reordering framework for hierarchical transla-
tion models. Towards that goal, we conduct an ora-
cle experiment on the GALE/BOLT development set
that uses the oracle translation order from the ref-
erence as the external reordering model. For each
source sentence in the development set, we pair it
with the first reference translation (out of up to 4 in-
dependent translations). We then add the sentence
pairs from the development set to the parallel train-
ing data and run GIZA++ to obtain word alignments.
We consider the GIZA++ word alignments for the
development set to be all correct, and use it to de-
termine the oracle order in the reference translation.
For the ordering distribution, we set the log proba-
bility of the reference translation order to 0 and the
reverse order to -1 to avoid negative infinity. As
shown in Table 5, the system tuned and evaluated
with the oracle reordering model significantly out-
performs the baseline by a large margin of 2.32 in
BLEU on the GALE/BOLT test set. This suggests
that there is room for potential improvement by us-
ing a fairly trained reordering model.
</bodyText>
<table confidence="0.995895">
BLEU
baseline 27.82
+ oracle reorder 30.14
+ syntax reorder 28.40
</table>
<tableCaption confidence="0.989108">
Table 5: Effects of external reordering features on MT
performance on the GALE/BOLT test set.
</tableCaption>
<bodyText confidence="0.999955166666667">
We next evaluate the syntax-based reordering
model. We train the model on manually aligned
Chinese-English corpora. Since the tokenization
used in the manual alignment corpora is different
from the tokenization used in our MT system, the
manual alignment is projected to the MT tokeniza-
tion based on the character alignment between the
tokens. Some extraneously tagged alignment links
in the manual alignment corpora are not useful for
machine translation and are thus removed before
projecting the alignment. As described in Sec-
tion 4.2, the syntax-based reordering method mod-
</bodyText>
<page confidence="0.996732">
563
</page>
<bodyText confidence="0.999917357142857">
els the translation order of sibling constituent pairs
in the source parse tree. As a result of strong phrasal
cohesion (Fox, 2002), we find that 94% of con-
stituent pairs have a clear ordering on the target
side. We only retain these constituent pairs for train-
ing and evaluating the reordering model. In order
to evaluate the accuracy of the maximum entropy
reordering model, we divide the manual alignment
corpora into 2/3 for training and 1/3 for evaluation.
A baseline that only chooses the majority order (i.e.
straight) has an accuracy of 69%, while the syntax-
based reordering model improves the accuracy to
79%.
The final reordering model used in MT is trained
on all of the samples extracted from the manual
alignment corpora. As shown in Table 5, the syntax-
based reordering feature improves the baseline by
0.58 in BLEU, which is a good improvement given
our strong baseline. Table 6 lists the number of
shifting errors in TER measurement (Snover et al.,
2006) of various systems on the GALE/BOLT test
set. The syntax-based reordering model achieves a
6.1% reduction in the number of shifting errors in
the baseline system, and its combination with the
syntax mismatch model achieves an additional re-
duction of 0.6%. This suggests that the proposed
method helps to improve word reordering in transla-
tion.
</bodyText>
<table confidence="0.9944984">
Shifting errors
baseline 3205
+ syntax mismatch 3089
+ syntax reorder 3010
+ syntax mismatch and reorder 2990
</table>
<tableCaption confidence="0.975724">
Table 6: Number of shifting errors in TER measurement
of multiple systems on the GALE/BOLT test set
</tableCaption>
<subsectionHeader confidence="0.967763">
5.4 Final Results
</subsectionHeader>
<bodyText confidence="0.999944428571429">
Table 7 shows the final results on the GALE/BOLT
test set, as well as the NIST MT06 and MT08 test
sets. Both the syntax mismatch and the syntax-based
reordering features improve the baseline system, re-
sulting in moderate to significant gains in all of the
five test sets. The two features are complementary
to each other and their combination results in better
improvement in four out of the five test sets com-
pared to adding them separately. In three out of the
five test sets, the improvement from the combina-
tion of the two features is statistically significant at
the 95% confidence level over the baseline, with the
largest absolute improvement of 1.43 in BLEU ob-
tained on MT08 web.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999964681818182">
In this paper, We have discussed problems resulting
from explicitly decorating translation rules with syn-
tactic annotations. We presented a factored approach
to incorporate soft source syntax mismatch and re-
ordering constraints to hierarchical machine transla-
tion, and showed how our models avoid the pitfalls
of the explicit decoration approach. Experiments on
Chinese-English translation show that the proposed
approach significantly improves a strong string-to-
dependency translation baseline on multiple evalu-
ation sets. There are many directions in which this
work can be continued. The syntax mismatch model
can be extended to dynamically adjust the transla-
tion distribution based on the syntactic compatibil-
ity between a translation rule and a source sentence.
It also might be beneficial to look beyond syntactic
constituent pairs when modeling reordering, given
that phrasal cohesion does not always hold in trans-
lation. The general framework that uses an external
reordering model in hierarchical models via features
can also be naturally extended to use multiple re-
ordering models.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999949285714286">
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. The authors would like to thank the anony-
mous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.988593" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.67217875">
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
</reference>
<page confidence="0.991652">
564
</page>
<table confidence="0.998905833333333">
MT06 news MT06 web MT08 news MT08 web GALE/BOLT
baseline 43.76 36.13 40.52 27.78 27.82
+ syntax mismatch 43.89 36.72 40.82+ 28.54&amp;quot; 28.23&amp;quot;
+ syntax reorder 44.01 36.40 41.23&amp;quot; 28.95&amp;quot; 28.40&amp;quot;
+ syntax mismatch and reorder 44.28+ 36.43+ 41.14&amp;quot; 29.21&amp;quot; 28.62&amp;quot;
improvement over baseline +0.52 +0.30 +0.62 +1.43 +0.8
</table>
<tableCaption confidence="0.995744">
Table 7: Results on Chinese-English MT. The symbols �, and 5� indicate that the system is better than the baseline
at the 85%, 95%, and 99% confidence levels, respectively, as defined in (Koehn, 2004).
</tableCaption>
<reference confidence="0.998578841463414">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the Conference of the North American Chapter
of the Association for Computational Linguistics.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Conference of the European Chapter of the
Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What’s in a translation rule. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the International Conference
on Computational Linguistics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsening
the label set. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing.
Zhongqiang Huang, Martin ˇCmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Mitesh M. Khapra, Ananthakrishnan Ramanathan, and
Karthik Visweswariah. 2013. Improving reordering
performance using higher order and structural features.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris
Callison-Burch, Miles Osborne, and David Talbot.
2005. Edinburgh system description for the 2005 iwslt
</reference>
<page confidence="0.981781">
565
</page>
<reference confidence="0.999101855421687">
speech translation evaluation. In International Work-
shop on Spoken Language Translation.
Philipp Koehn. 2004. Pharaoh: A bean search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Conference of Association
for Machine Translation in the Americas.
Junhui Li, Philip Resnik, and Hal Daume. 2013. Mod-
eling syntactic and semantic structures in hierarchi-
cal phrase-based translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings
of the SIGHAN Workshop on Chinese Language Pro-
cessing.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of the Conference
of the North American Chapter of the Association for
Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the International Confer-
ence on Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve smt
for subject-object-verb languages. In Proceeding of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation.
</reference>
<page confidence="0.998455">
566
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090779">
<title confidence="0.928194666666667">Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang</title>
<author confidence="0.619531">Raytheon BBN</author>
<address confidence="0.904129">50 Moulton Cambridge, MA,</address>
<email confidence="0.999565">zhuang@bbn.com</email>
<author confidence="0.974492">Jacob</author>
<affiliation confidence="0.780995">Raytheon BBN</affiliation>
<address confidence="0.9544395">50 Moulton Cambridge, MA,</address>
<email confidence="0.999637">jdevlin@bbn.com</email>
<author confidence="0.504945">Rabih</author>
<affiliation confidence="0.463096">Raytheon BBN</affiliation>
<address confidence="0.9280785">50 Moulton Cambridge, MA,</address>
<email confidence="0.999812">rzbib@bbn.com</email>
<abstract confidence="0.999477678571428">This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8764" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="1311" endWordPosition="1315">, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it intr</context>
<context position="18249" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="2881" endWordPosition="2884">are computed based on the distribution of tags P(tr) on the rule nonterminal, and the tag ts on the corresponding source span. The features are defined in Table 1. We use soft features instead of hard syntactic constraints, and allow the tuning process to choose the appropriate weight for each feature. As shown in Section 5, these source syntax mismatch features help to improve the baseline system. 4.2 Syntax-based Reordering Model Most previous research on reordering models has focused on improving word reordering for statistical phrase-based translation systems (e.g., (Collins et al., 2005; Al-Onaizan and Papineni, 2006; Tromble and Eisner, 2009)). There has been less work on improving the reordering of hierarchical phrase-based translation systems (see (Xu et al., 2009; Gao et al., 2011; Xiong et al., 2012) for a few exceptions), except through explicit syntactic annotation of translation rules. It is generally assumed that hierarchical models are inherently capable of handling both local and non-local reorderings. However, many hierarchical translation rules are noisy and have limited context, and so may not be able to produce translations in the right order. We propose a general framework that incorporate</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26707" citStr="Charniak and Johnson, 2005" startWordPosition="4308" endWordPosition="4311">features similar to the method described in (Chiang et al., 2009). It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation. 4Note that the constituent pairs used to train the reordering model are filtered to only contain these with clear ordering on the target side, while no such pre-filtering is applied to constituent pairs when applying the reordering model in translation. We leave it to future work to address this mismatch problem. GIZA++ (Och and Ney, 2003) is used for automatic word alignment in all of the experiments. We use Charniak’s parser (Charniak and Johnson, 2005) on the English side to obtain string-to-dependency translation rules, and use a latent variable PCFG parser (Huang and Harper, 2009) to parse the source side of the parallel training data as well as the test sentences for extracting syntax mismatch and reordering features. For both languages, dependency structures are read off constituency trees using manual head word percolation rules. We use a lexicon-based longest-match-first word segmenter to tokenize source Chinese sentences. Since the source tokenization used in our MT system is different from the treebank tokenization used to train the</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8922" citStr="Cherry, 2013" startWordPosition="1336" endWordPosition="1337">ordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10654" citStr="Chiang et al. (2006)" startWordPosition="1602" endWordPosition="1605">bling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT decoding, while our proposed model generates reordering features that are used together with other MT features to determine the optimal reordering during MT decoding. 3 String-to-Dependency Translation Our baseline translation system is based on a stringto-dependency translation model similar to the implementation in (Shen et al., 2008). It is an extension of the hierarchical translation model of Chiang et al. (2006) that requires the target side of a phrase pair to have a well-formed dependency structure, defined as either of the two types: • fixed structure: a single rooted dependency sub-tree with each child being a complete constituent. In this case, the phrase has a unique head word inside the phrase, i.e., the root of the dependency sub-tree. Each dependent of the head word, together with all of its descendants, is either completely inside the phrase or completely outside the phrase. For example, the phrase give him in Figure 1 (a) has a fixed dependency structure with head word give. • floating str</context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic dialects. In Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26145" citStr="Chiang et al., 2009" startWordPosition="4214" endWordPosition="4217">b text from the NIST MT06 and MT08 evaluations using an experimental setup compatible with the NIST MT12 Chinese-English constrained track. In this setup, the translation and language models are trained on 35 million words of parallel data and 3.8 billion words of English monolingual data, respectively. The systems are tuned on the MT02- 05 development sets. All systems are tuned and evaluated on IBM BLEU (Papineni et al., 2002). The baseline string-to-dependency translation system uses more than 10 core features and a large number of sparse binary features similar to the method described in (Chiang et al., 2009). It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation. 4Note that the constituent pairs used to train the reordering model are filtered to only contain these with clear ordering on the target side, while no such pre-filtering is applied to constituent pairs when applying the reordering model in translation. We leave it to future work to address this mismatch problem. GIZA++ (Och and Ney, 2003) is used for automatic word alignment in all of the experiments. We use Charniak’s parser (Charniak and Johnson, 2005) on the English side to obtain string-</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="1627" citStr="Chiang, 2007" startWordPosition="229" endWordPosition="231">compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets. 1 Introduction Hierarchical phrase-based translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), m</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2173" citStr="Chiang, 2010" startWordPosition="312" endWordPosition="313">duction Hierarchical phrase-based translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependency translation model (Shen et al., 2008). The gene</context>
<context position="6366" citStr="Chiang (2010)" startWordPosition="938" endWordPosition="939">syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) used a preference grammar to soften the syntactic constraints through the use of a preference distribution of syntactic categories; and recently Hanneman and Lavie (2013) proposed a clustering approach to reduce the number of syntactic categories. Our proposed syntax mismatch model associates nonterminals with a distribution of tags. It is similar to the preference grammar in (Venugopal et al., 2009); however, we use treebank tags and focus on the syntactic compatibilit</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4188" citStr="Fox, 2002" startWordPosition="605" endWordPosition="606">013 Conference on Empirical Methods in Natural Language Processing, pages 556–566, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics we keep the original string-to-dependency translation rules intact and associate each nonterminal with a distribution of tags. That distribution is then used to measure the syntactic compatibility between the syntactic context from which the translation rule is extracted and the syntactic analysis of a test sentence. The second is a syntax-based reordering model that takes advantage of phrasal cohesion in translation (Fox, 2002). The reordering model takes a pair of sibling constituents in the source parse tree as input, and uses source syntactic clues to predict the ordering distribution (straight vs. inverted) of their translations on the target side. The resulting ordering distribution is used in the decoder at the word pair level to guide the translation process. This separate reordering model allows us to utilize source syntax to improve reordering in hierarchical translation models without having to explicitly annotate translation rules with source syntax. Our results show that both the syntax mismatch model an</context>
<context position="10001" citStr="Fox, 2002" startWordPosition="1501" endWordPosition="1502">ies, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents or a predicate and its arguments, while our proposed general framework considers all word pairs in a sentence. Our syntaxbased reordering model exploits phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT decoding, while our proposed model generates reordering features that are used together with other MT features to determine the optimal reordering during MT decoding. 3 String-to-Dependency Translation Our baseline translation system is based on a stringto-dependency translation model similar to the implementation in (Shen et al., 2008). It is an extension of the h</context>
<context position="23560" citStr="Fox, 2002" startWordPosition="3786" endWordPosition="3787">amework relies on an external model to provide the ordering probability distribution of source word pairs. In this paper, we investigate a simple maximum-entropy reordering model based on the syntactic parse tree of the source sentence. This allows us to take advantage of the source syntax to improve reordering without using syntactic annotations in translation rules. The syntax-based reordering model attempts to predict the reordering probability of a pair of sibling constituents in the source parse tree, building on the fact that syntactic phrases tend to move in a group during translation (Fox, 2002). The reordering model is trained on a word-aligned corpus. For each pair of sibling constituents in the source parse tree, we determine the translation order on the target side based on word alignments. If there is a clear ordering3, i.e., either straight or inverted, on the target side, we include the context of the constituent pair and its translation order as a sample for training or evaluating the maximum entropy reordering model. Table 2 lists the features of the reordering model. The ordering distributions of source word pairs are determined based on the ordering distributions of siblin</context>
<context position="32852" citStr="Fox, 2002" startWordPosition="5321" endWordPosition="5322">lish corpora. Since the tokenization used in the manual alignment corpora is different from the tokenization used in our MT system, the manual alignment is projected to the MT tokenization based on the character alignment between the tokens. Some extraneously tagged alignment links in the manual alignment corpora are not useful for machine translation and are thus removed before projecting the alignment. As described in Section 4.2, the syntax-based reordering method mod563 els the translation order of sibling constituent pairs in the source parse tree. As a result of strong phrasal cohesion (Fox, 2002), we find that 94% of constituent pairs have a clear ordering on the target side. We only retain these constituent pairs for training and evaluating the reordering model. In order to evaluate the accuracy of the maximum entropy reordering model, we divide the manual alignment corpora into 2/3 for training and 1/3 for evaluation. A baseline that only chooses the majority order (i.e. straight) has an accuracy of 69%, while the syntaxbased reordering model improves the accuracy to 79%. The final reordering model used in MT is trained on all of the samples extracted from the manual alignment corpo</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8907" citStr="Galley and Manning, 2008" startWordPosition="1332" endWordPosition="1335">that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative directi</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</booktitle>
<contexts>
<context position="2105" citStr="Galley et al., 2004" startWordPosition="298" endWordPosition="301">ring-to-dependency translation system on multiple evaluation sets. 1 Introduction Hierarchical phrase-based translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2004</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2004. What’s in a translation rule. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Gao</author>
<author>Philipp Koehn</author>
<author>Alexandra Birch</author>
</authors>
<title>Soft dependency constraints for reordering in hierarchical phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9617" citStr="Gao et al. (2011)" startWordPosition="1438" endWordPosition="1441">ical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents or a predicate and its arguments, while our proposed general framework considers all word pairs in a sentence. Our syntaxbased reordering model exploits phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling consti</context>
<context position="18420" citStr="Gao et al., 2011" startWordPosition="2909" endWordPosition="2912"> instead of hard syntactic constraints, and allow the tuning process to choose the appropriate weight for each feature. As shown in Section 5, these source syntax mismatch features help to improve the baseline system. 4.2 Syntax-based Reordering Model Most previous research on reordering models has focused on improving word reordering for statistical phrase-based translation systems (e.g., (Collins et al., 2005; Al-Onaizan and Papineni, 2006; Tromble and Eisner, 2009)). There has been less work on improving the reordering of hierarchical phrase-based translation systems (see (Xu et al., 2009; Gao et al., 2011; Xiong et al., 2012) for a few exceptions), except through explicit syntactic annotation of translation rules. It is generally assumed that hierarchical models are inherently capable of handling both local and non-local reorderings. However, many hierarchical translation rules are noisy and have limited context, and so may not be able to produce translations in the right order. We propose a general framework that incorporates external reordering information into the decoding process of hierarchical translation models. To simplify the presentation, we make the assumption that every source word</context>
</contexts>
<marker>Gao, Koehn, Birch, 2011</marker>
<rawString>Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft dependency constraints for reordering in hierarchical phrase-based translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="8177" citStr="Genzel, 2010" startWordPosition="1226" endWordPosition="1227">tion rules and source spans. Most research on reordering in machine translation focuses on phrase-based translation models as they are inherently weak at non-local reordering. Previous efforts to improve reordering for phrasebased systems can be largely classified into two categories. Approaches in the first category try to reorder words in the source sentence in a preprocessing step to reduce reordering in both word alignment and MT decoding. The reordering decisions are either made using manual or automatically learned rules (Collins et al., 2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign </context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Improving syntax-augmented machine translation by coarsening the label set.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2585" citStr="Hanneman and Lavie, 2013" startWordPosition="363" endWordPosition="366">ints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependency translation model (Shen et al., 2008). The general ideas are applicable to other hierarchical models as well. Instead of enriching translation rules with explicit syntactic annotations, we keep the original translation rules intact, and factorize the use of source syntactic constraints through two separate models. The first is a syntax mismatch model that introduces source syntax into the nonterminals of translation rules, and measures the degree of synta</context>
<context position="6662" citStr="Hanneman and Lavie (2013)" startWordPosition="982" endWordPosition="985"> syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) used a preference grammar to soften the syntactic constraints through the use of a preference distribution of syntactic categories; and recently Hanneman and Lavie (2013) proposed a clustering approach to reduce the number of syntactic categories. Our proposed syntax mismatch model associates nonterminals with a distribution of tags. It is similar to the preference grammar in (Venugopal et al., 2009); however, we use treebank tags and focus on the syntactic compatibility between translation rules and the source sentence. The work of Huang et al. (2010) is most similar to ours, with the main difference being that their syntactic categories are latent and learned automatically in a data driven fashion while we simply use treebank tags based on dependency parsing</context>
</contexts>
<marker>Hanneman, Lavie, 2013</marker>
<rawString>Greg Hanneman and Alon Lavie. 2013. Improving syntax-augmented machine translation by coarsening the label set. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="26840" citStr="Huang and Harper, 2009" startWordPosition="4328" endWordPosition="4331">s in the NIST MT12 evaluation. 4Note that the constituent pairs used to train the reordering model are filtered to only contain these with clear ordering on the target side, while no such pre-filtering is applied to constituent pairs when applying the reordering model in translation. We leave it to future work to address this mismatch problem. GIZA++ (Och and Ney, 2003) is used for automatic word alignment in all of the experiments. We use Charniak’s parser (Charniak and Johnson, 2005) on the English side to obtain string-to-dependency translation rules, and use a latent variable PCFG parser (Huang and Harper, 2009) to parse the source side of the parallel training data as well as the test sentences for extracting syntax mismatch and reordering features. For both languages, dependency structures are read off constituency trees using manual head word percolation rules. We use a lexicon-based longest-match-first word segmenter to tokenize source Chinese sentences. Since the source tokenization used in our MT system is different from the treebank tokenization used to train the Chinese parser, the source sentences are first tokenized using the treebank-trained Stanford Chinese segmenter (Tseng et al., 2005),</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A syntax-directed translator with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing.</booktitle>
<contexts>
<context position="5555" citStr="Huang et al., 2006" startWordPosition="816" endWordPosition="819">zed as follows. Section 2 discusses related work in the literature. Section 3 provides an overview of our baseline string-to-dependency translation system. Section 4 describes the details of the syntax mismatch and syntax-based reordering models. Experimental results are presented in Section 5. The last section concludes the paper. 2 Related Work Attempts to use rich syntactic annotations do not always result in improved performance when compared to purely hierarchical models that do not use linguistic guidance. For example, as shown in (Mi and Huang, 2008), tree-to-string translation models (Huang et al., 2006) only start to outperform purely hierarchical models when significant efforts were made to alleviate parsing errors by using forest-based approaches in both rule extraction and decoding. Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data s</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A syntax-directed translator with extended domain of locality. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin ˇCmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Huang, ˇCmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin ˇCmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Karthik Visweswariah</author>
</authors>
<title>Improving reordering performance using higher order and structural features.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8417" citStr="Khapra et al., 2013" startWordPosition="1261" endWordPosition="1264">stems can be largely classified into two categories. Approaches in the first category try to reorder words in the source sentence in a preprocessing step to reduce reordering in both word alignment and MT decoding. The reordering decisions are either made using manual or automatically learned rules (Collins et al., 2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation </context>
</contexts>
<marker>Khapra, Ramanathan, Visweswariah, 2013</marker>
<rawString>Mitesh M. Khapra, Ananthakrishnan Ramanathan, and Karthik Visweswariah. 2013. Improving reordering performance using higher order and structural features. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</booktitle>
<contexts>
<context position="5892" citStr="Koehn et al., 2003" startWordPosition="868" endWordPosition="871">elated Work Attempts to use rich syntactic annotations do not always result in improved performance when compared to purely hierarchical models that do not use linguistic guidance. For example, as shown in (Mi and Huang, 2008), tree-to-string translation models (Huang et al., 2006) only start to outperform purely hierarchical models when significant efforts were made to alleviate parsing errors by using forest-based approaches in both rule extraction and decoding. Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) </context>
<context position="8611" citStr="Koehn et al., 2003" startWordPosition="1290" endWordPosition="1293">nt and MT decoding. The reordering decisions are either made using manual or automatically learned rules (Collins et al., 2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also b</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 iwslt speech translation evaluation.</title>
<date>2005</date>
<booktitle>In International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="8732" citStr="Koehn et al., 2005" startWordPosition="1307" endWordPosition="1310">2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with re</context>
</contexts>
<marker>Koehn, Axelrod, Birch, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 iwslt speech translation evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A bean search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of Association for Machine Translation in the Americas.</booktitle>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A bean search decoder for phrase-based statistical machine translation models. In Proceedings of the Conference of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Philip Resnik</author>
<author>Hal Daume</author>
</authors>
<title>Modeling syntactic and semantic structures in hierarchical phrase-based translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9660" citStr="Li et al. (2013)" startWordPosition="1447" endWordPosition="1450">ing nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents or a predicate and its arguments, while our proposed general framework considers all word pairs in a sentence. Our syntaxbased reordering model exploits phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT decoding, while our propos</context>
</contexts>
<marker>Li, Resnik, Daume, 2013</marker>
<rawString>Junhui Li, Philip Resnik, and Hal Daume. 2013. Modeling syntactic and semantic structures in hierarchical phrase-based translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2124" citStr="Liu et al., 2006" startWordPosition="302" endWordPosition="305">anslation system on multiple evaluation sets. 1 Introduction Hierarchical phrase-based translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependenc</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7288" citStr="Marton and Resnik (2008)" startWordPosition="1084" endWordPosition="1087">roposed a clustering approach to reduce the number of syntactic categories. Our proposed syntax mismatch model associates nonterminals with a distribution of tags. It is similar to the preference grammar in (Venugopal et al., 2009); however, we use treebank tags and focus on the syntactic compatibility between translation rules and the source sentence. The work of Huang et al. (2010) is most similar to ours, with the main difference being that their syntactic categories are latent and learned automatically in a data driven fashion while we simply use treebank tags based on dependency parsing. Marton and Resnik (2008) also exploited soft source syntax constraints without modifying translation rules. However, they focused on the quality of translation spans based on the syntactic analysis of the source sentence, while our method explicitly models the syntactic compatibility between translation rules and source spans. Most research on reordering in machine translation focuses on phrase-based translation models as they are inherently weak at non-local reordering. Previous efforts to improve reordering for phrasebased systems can be largely classified into two categories. Approaches in the first category try t</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5499" citStr="Mi and Huang, 2008" startWordPosition="809" endWordPosition="812">ese-English MT baseline. The rest of the paper is organized as follows. Section 2 discusses related work in the literature. Section 3 provides an overview of our baseline string-to-dependency translation system. Section 4 describes the details of the syntax mismatch and syntax-based reordering models. Experimental results are presented in Section 5. The last section concludes the paper. 2 Related Work Attempts to use rich syntactic annotations do not always result in improved performance when compared to purely hierarchical models that do not use linguistic guidance. For example, as shown in (Mi and Huang, 2008), tree-to-string translation models (Huang et al., 2006) only start to outperform purely hierarchical models when significant efforts were made to alleviate parsing errors by using forest-based approaches in both rule extraction and decoding. Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variat</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models. Computional Linguistics.</title>
<date>2003</date>
<contexts>
<context position="26589" citStr="Och and Ney, 2003" startWordPosition="4288" endWordPosition="4291"> string-to-dependency translation system uses more than 10 core features and a large number of sparse binary features similar to the method described in (Chiang et al., 2009). It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation. 4Note that the constituent pairs used to train the reordering model are filtered to only contain these with clear ordering on the target side, while no such pre-filtering is applied to constituent pairs when applying the reordering model in translation. We leave it to future work to address this mismatch problem. GIZA++ (Och and Ney, 2003) is used for automatic word alignment in all of the experiments. We use Charniak’s parser (Charniak and Johnson, 2005) on the English side to obtain string-to-dependency translation rules, and use a latent variable PCFG parser (Huang and Harper, 2009) to parse the source side of the parallel training data as well as the test sentences for extracting syntax mismatch and reordering features. For both languages, dependency structures are read off constituency trees using manual head word percolation rules. We use a lexicon-based longest-match-first word segmenter to tokenize source Chinese senten</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computional Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25957" citStr="Papineni et al., 2002" startWordPosition="4183" endWordPosition="4186"> the development sets available for the DARPA GALE and BOLT programs, with up to 4 independent references for each source sentence. We also evaluate our final systems on both newswire and web text from the NIST MT06 and MT08 evaluations using an experimental setup compatible with the NIST MT12 Chinese-English constrained track. In this setup, the translation and language models are trained on 35 million words of parallel data and 3.8 billion words of English monolingual data, respectively. The systems are tuned on the MT02- 05 development sets. All systems are tuned and evaluated on IBM BLEU (Papineni et al., 2002). The baseline string-to-dependency translation system uses more than 10 core features and a large number of sparse binary features similar to the method described in (Chiang et al., 2009). It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation. 4Note that the constituent pairs used to train the reordering model are filtered to only contain these with clear ordering on the target side, while no such pre-filtering is applied to constituent pairs when applying the reordering model in translation. We leave it to future work to address this mismatch pro</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2158" citStr="Shen et al., 2008" startWordPosition="308" endWordPosition="311">ation sets. 1 Introduction Hierarchical phrase-based translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependency translation model (Shen et al., </context>
<context position="10572" citStr="Shen et al., 2008" startWordPosition="1588" endWordPosition="1591">its phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT decoding, while our proposed model generates reordering features that are used together with other MT features to determine the optimal reordering during MT decoding. 3 String-to-Dependency Translation Our baseline translation system is based on a stringto-dependency translation model similar to the implementation in (Shen et al., 2008). It is an extension of the hierarchical translation model of Chiang et al. (2006) that requires the target side of a phrase pair to have a well-formed dependency structure, defined as either of the two types: • fixed structure: a single rooted dependency sub-tree with each child being a complete constituent. In this case, the phrase has a unique head word inside the phrase, i.e., the root of the dependency sub-tree. Each dependent of the head word, together with all of its descendants, is either completely inside the phrase or completely outside the phrase. For example, the phrase give him in</context>
<context position="12009" citStr="Shen et al. (2008)" startWordPosition="1829" endWordPosition="1832">tituents whose common parent is outside the phrase. For example, the phrase him that brown coat in Figure 1 is a floating structure whose common parent give is not in the phrase. Requiring the target side to have a well-formed dependency structure is less restrictive than requiring it to be a syntactic constituent, allowing more translation rules to be extracted. However, it still results in fewer rules than pure hierarchical translation models and might hurt MT performance. The well-formed dependency structure on the target side makes it possible to introduce syntax features during decoding. Shen et al. (2008) obtained significant improvements from including a dependency language model score in decoding, outweighing the negative effect of the dependency constraint. Shen et al. (2009) proposed an approach to label each nonterminal, which can be either on the left-hand-side (LHS) or the right-hand-side (RHS) of the rule, with the head POS tag of the underlying target phrase if it has a fixed dependency structure1, and measure the mismatches between nonterminal labels when a RHS nonterminal of a rule is substantiated with the LHS nonterminal of another rule during decoding. This also resulted in furth</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6348" citStr="Shen et al. (2009)" startWordPosition="933" endWordPosition="936">d decoding. Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) used a preference grammar to soften the syntactic constraints through the use of a preference distribution of syntactic categories; and recently Hanneman and Lavie (2013) proposed a clustering approach to reduce the number of syntactic categories. Our proposed syntax mismatch model associates nonterminals with a distribution of tags. It is similar to the preference grammar in (Venugopal et al., 2009); however, we use treebank tags and focus on the synt</context>
<context position="12186" citStr="Shen et al. (2009)" startWordPosition="1854" endWordPosition="1857">. Requiring the target side to have a well-formed dependency structure is less restrictive than requiring it to be a syntactic constituent, allowing more translation rules to be extracted. However, it still results in fewer rules than pure hierarchical translation models and might hurt MT performance. The well-formed dependency structure on the target side makes it possible to introduce syntax features during decoding. Shen et al. (2008) obtained significant improvements from including a dependency language model score in decoding, outweighing the negative effect of the dependency constraint. Shen et al. (2009) proposed an approach to label each nonterminal, which can be either on the left-hand-side (LHS) or the right-hand-side (RHS) of the rule, with the head POS tag of the underlying target phrase if it has a fixed dependency structure1, and measure the mismatches between nonterminal labels when a RHS nonterminal of a rule is substantiated with the LHS nonterminal of another rule during decoding. This also resulted in further improvements in MT performance. Figure 1 (c) shows an example stringto-dependency translation rule in our baseline system. 1Nonterminals corresponding to floating structures </context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas.</booktitle>
<contexts>
<context position="33690" citStr="Snover et al., 2006" startWordPosition="5462" endWordPosition="5465">m entropy reordering model, we divide the manual alignment corpora into 2/3 for training and 1/3 for evaluation. A baseline that only chooses the majority order (i.e. straight) has an accuracy of 69%, while the syntaxbased reordering model improves the accuracy to 79%. The final reordering model used in MT is trained on all of the samples extracted from the manual alignment corpora. As shown in Table 5, the syntaxbased reordering feature improves the baseline by 0.58 in BLEU, which is a good improvement given our strong baseline. Table 6 lists the number of shifting errors in TER measurement (Snover et al., 2006) of various systems on the GALE/BOLT test set. The syntax-based reordering model achieves a 6.1% reduction in the number of shifting errors in the baseline system, and its combination with the syntax mismatch model achieves an additional reduction of 0.6%. This suggests that the proposed method helps to improve word reordering in translation. Shifting errors baseline 3205 + syntax mismatch 3089 + syntax reorder 3010 + syntax mismatch and reorder 2990 Table 6: Number of shifting errors in TER measurement of multiple systems on the GALE/BOLT test set 5.4 Final Results Table 7 shows the final res</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8391" citStr="Tromble and Eisner, 2009" startWordPosition="1256" endWordPosition="1259"> reordering for phrasebased systems can be largely classified into two categories. Approaches in the first category try to reorder words in the source sentence in a preprocessing step to reduce reordering in both word alignment and MT decoding. The reordering decisions are either made using manual or automatically learned rules (Collins et al., 2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although</context>
<context position="18276" citStr="Tromble and Eisner, 2009" startWordPosition="2885" endWordPosition="2888">ibution of tags P(tr) on the rule nonterminal, and the tag ts on the corresponding source span. The features are defined in Table 1. We use soft features instead of hard syntactic constraints, and allow the tuning process to choose the appropriate weight for each feature. As shown in Section 5, these source syntax mismatch features help to improve the baseline system. 4.2 Syntax-based Reordering Model Most previous research on reordering models has focused on improving word reordering for statistical phrase-based translation systems (e.g., (Collins et al., 2005; Al-Onaizan and Papineni, 2006; Tromble and Eisner, 2009)). There has been less work on improving the reordering of hierarchical phrase-based translation systems (see (Xu et al., 2009; Gao et al., 2011; Xiong et al., 2012) for a few exceptions), except through explicit syntactic annotation of translation rules. It is generally assumed that hierarchical models are inherently capable of handling both local and non-local reorderings. However, many hierarchical translation rules are noisy and have limited context, and so may not be able to produce translations in the right order. We propose a general framework that incorporates external reordering infor</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Proceedings of the SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="27439" citStr="Tseng et al., 2005" startWordPosition="4420" endWordPosition="4423">ng and Harper, 2009) to parse the source side of the parallel training data as well as the test sentences for extracting syntax mismatch and reordering features. For both languages, dependency structures are read off constituency trees using manual head word percolation rules. We use a lexicon-based longest-match-first word segmenter to tokenize source Chinese sentences. Since the source tokenization used in our MT system is different from the treebank tokenization used to train the Chinese parser, the source sentences are first tokenized using the treebank-trained Stanford Chinese segmenter (Tseng et al., 2005), then parsed with the Chinese parser, and finally projected to MT tokenization based on the character alignment between the tokens. The syntax-based reordering model is trained on a set of Chinese-English manual word alignment corpora released by the LDC5. 5.2 Syntax Mismatch Model We first conduct experiments on the GALE/BOLT data sets to evaluate different strategies of incorporating source syntax into string-to-dependency translation rules. As mentioned in Section 4.1, constraining the source side of translation rules to only well-formed dependency structures is too restrictive given that </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Proceedings of the SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6491" citStr="Venugopal et al. (2009)" startWordPosition="956" endWordPosition="959">nts (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) used a preference grammar to soften the syntactic constraints through the use of a preference distribution of syntactic categories; and recently Hanneman and Lavie (2013) proposed a clustering approach to reduce the number of syntactic categories. Our proposed syntax mismatch model associates nonterminals with a distribution of tags. It is similar to the preference grammar in (Venugopal et al., 2009); however, we use treebank tags and focus on the syntactic compatibility between translation rules and the source sentence. The work of Huang et al. (2010) is most similar to ours, with the main d</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: softening syntactic constraints to improve statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="8140" citStr="Xia and McCord, 2004" startWordPosition="1218" endWordPosition="1221"> the syntactic compatibility between translation rules and source spans. Most research on reordering in machine translation focuses on phrase-based translation models as they are inherently weak at non-local reordering. Previous efforts to improve reordering for phrasebased systems can be largely classified into two categories. Approaches in the first category try to reorder words in the source sentence in a preprocessing step to reduce reordering in both word alignment and MT decoding. The reordering decisions are either made using manual or automatically learned rules (Collins et al., 2005; Xia and McCord, 2004; Xia and McCord, 2004; Genzel, 2010) based on the syntactic analysis of the source sentence, or constructed through an optimization procedure that uses feature-based reordering models trained on a wordaligned parallel corpus (Tromble and Eisner, 2009; 557 Khapra et al., 2013). Approaches in the second category try to explicitly model phrase reordering in the translation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Ona</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Modeling the translation of predicate-argument structure for smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9638" citStr="Xiong et al. (2012)" startWordPosition="1442" endWordPosition="1445">dels are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents or a predicate and its arguments, while our proposed general framework considers all word pairs in a sentence. Our syntaxbased reordering model exploits phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT deco</context>
<context position="18441" citStr="Xiong et al., 2012" startWordPosition="2913" endWordPosition="2916">yntactic constraints, and allow the tuning process to choose the appropriate weight for each feature. As shown in Section 5, these source syntax mismatch features help to improve the baseline system. 4.2 Syntax-based Reordering Model Most previous research on reordering models has focused on improving word reordering for statistical phrase-based translation systems (e.g., (Collins et al., 2005; Al-Onaizan and Papineni, 2006; Tromble and Eisner, 2009)). There has been less work on improving the reordering of hierarchical phrase-based translation systems (see (Xu et al., 2009; Gao et al., 2011; Xiong et al., 2012) for a few exceptions), except through explicit syntactic annotation of translation rules. It is generally assumed that hierarchical models are inherently capable of handling both local and non-local reorderings. However, many hierarchical translation rules are noisy and have limited context, and so may not be able to produce translations in the right order. We propose a general framework that incorporates external reordering information into the decoding process of hierarchical translation models. To simplify the presentation, we make the assumption that every source word translates to one or</context>
</contexts>
<marker>Xiong, Zhang, Li, 2012</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In Proceeding of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9121" citStr="Xu et al. (2009)" startWordPosition="1364" endWordPosition="1367">nslation process. These approaches range from simple distance based distortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical mod</context>
<context position="18402" citStr="Xu et al., 2009" startWordPosition="2905" endWordPosition="2908">use soft features instead of hard syntactic constraints, and allow the tuning process to choose the appropriate weight for each feature. As shown in Section 5, these source syntax mismatch features help to improve the baseline system. 4.2 Syntax-based Reordering Model Most previous research on reordering models has focused on improving word reordering for statistical phrase-based translation systems (e.g., (Collins et al., 2005; Al-Onaizan and Papineni, 2006; Tromble and Eisner, 2009)). There has been less work on improving the reordering of hierarchical phrase-based translation systems (see (Xu et al., 2009; Gao et al., 2011; Xiong et al., 2012) for a few exceptions), except through explicit syntactic annotation of translation rules. It is generally assumed that hierarchical models are inherently capable of handling both local and non-local reorderings. However, many hierarchical translation rules are noisy and have limited context, and so may not be able to produce translations in the right order. We propose a general framework that incorporates external reordering information into the decoding process of hierarchical translation models. To simplify the presentation, we make the assumption that</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceeding of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Nenghai Yu</author>
</authors>
<title>A ranking-based approach to word reordering for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10136" citStr="Yang et al. (2012)" startWordPosition="1523" endWordPosition="1526">s an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents or a predicate and its arguments, while our proposed general framework considers all word pairs in a sentence. Our syntaxbased reordering model exploits phrasal cohesion in translation (Fox, 2002) by modeling the reordering of sibling constituents in the source parse tree, which is similar to the recent work of Yang et al. (2012). However, the latter focuses on finding the optimal reordering of sibling constituents before MT decoding, while our proposed model generates reordering features that are used together with other MT features to determine the optimal reordering during MT decoding. 3 String-to-Dependency Translation Our baseline translation system is based on a stringto-dependency translation model similar to the implementation in (Shen et al., 2008). It is an extension of the hierarchical translation model of Chiang et al. (2006) that requires the target side of a phrase pair to have a well-formed dependency s</context>
</contexts>
<marker>Yang, Li, Zhang, Yu, 2012</marker>
<rawString>Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="2223" citStr="Zollmann and Venugopal, 2006" startWordPosition="317" endWordPosition="320">translation models (Chiang, 2007) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars. A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models. Despite some differences in the granularity of syntax units (e.g., tree fragments (Galley et al., 2004; Liu et al., 2006), treebank tags (Shen et al., 2008; Chiang, 2010), and extended tags (Zollmann and Venugopal, 2006)), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations. These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors (Hanneman and Lavie, 2013). In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependency translation model (Shen et al., 2008). The general ideas are applicable to other hierarchical mod</context>
<context position="5965" citStr="Zollmann and Venugopal (2006)" startWordPosition="877" endWordPosition="880">lways result in improved performance when compared to purely hierarchical models that do not use linguistic guidance. For example, as shown in (Mi and Huang, 2008), tree-to-string translation models (Huang et al., 2006) only start to outperform purely hierarchical models when significant efforts were made to alleviate parsing errors by using forest-based approaches in both rule extraction and decoding. Using only syntactic phrases is too restrictive in phrasal translation as many useful phrase pairs are not syntactic constituents (Koehn et al., 2003). The syntax-augmented translation model of Zollmann and Venugopal (2006) annotates nonterminals in hierarchical rules with thousands of extended syntactic categories in order to capture the syntactic variations of phrase pairs. This results in exacerbated data sparsity problems, partially due to the requirement of exact matches in nonterminal substitutions between translation rules in the derivation. Several solutions were proposed. Shen et al. (2009) and Chiang (2010) used soft match features to explicitly model the substitution of nonterminals with different labels; Venugopal et al. (2009) used a preference grammar to soften the syntactic constraints through the</context>
<context position="9204" citStr="Zollmann and Venugopal, 2006" startWordPosition="1374" endWordPosition="1377">stortion models (Koehn et al., 2003) that globally penalizes reordering based on the distorted distance, to lexicalized reordering models (Koehn et al., 2005; Al-Onaizan and Papineni, 2006) that assign reordering preferences of adjacent phrases for individual phrases, and to hierarchical reordering models (Galley and Manning, 2008; Cherry, 2013) that handle reordering preferences beyond adjacent phrases. Although hierarchical translation models are capable of handling nonlocal reordering, their accuracy is far from perfect. Xu et al. (2009) showed that the syntax-augmented hierarchical model (Zollmann and Venugopal, 2006) also benefits from reordering source words in a preprocessing step. Explicitly adding syntax to translation rules helps with reordering in general, but it introduces additional complexities, and is still limited by the context-free nature of hierarchical rules. Our work exploits an alternative direction that uses an external reordering model to improve word reordering of hierarchical models. Gao et al. (2011), Xiong et al. (2012), and Li et al. (2013) also studied external reordering models for hierarchical models. However, they focused on specific word pairs such as a word and its dependents</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000473">
<title confidence="0.945226">
Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet
</title>
<author confidence="0.829101">
Marco Guerini
</author>
<affiliation confidence="0.452772">
Trento RISE
</affiliation>
<address confidence="0.657922">
Via Sommarive 18
38123 Povo in Trento, Italy
</address>
<email confidence="0.992111">
m.guerini@trentorise.eu
</email>
<author confidence="0.607449">
Lorenzo Gatti
</author>
<affiliation confidence="0.331861">
Trento RISE
</affiliation>
<address confidence="0.608527">
Via Sommarive 18
38123 Povo in Trento, Italy
</address>
<email confidence="0.989317">
l.gatti@trentorise.eu
</email>
<author confidence="0.631902">
Marco Turchi
</author>
<affiliation confidence="0.421378">
Fondazione Bruno Kessler
</affiliation>
<address confidence="0.5623325">
Via Sommarive 18
38123 Povo in Trento, Italy
</address>
<email confidence="0.993036">
turchi@fbk.eu
</email>
<sectionHeader confidence="0.995572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999539190476191">
Assigning a positive or negative score to a
word out of context (i.e. a word’s prior polar-
ity) is a challenging task for sentiment analy-
sis. In the literature, various approaches based
on SentiWordNet have been proposed. In this
paper, we compare the most often used tech-
niques together with newly proposed ones and
incorporate all of them in a learning frame-
work to see whether blending them can fur-
ther improve the estimation of prior polarity
scores. Using two different versions of Sen-
tiWordNet and testing regression and classifi-
cation models across tasks and datasets, our
learning approach consistently outperforms
the single metrics, providing a new state-of-
the-art approach in computing words’ prior
polarity for sentiment analysis. We conclude
our investigation showing interesting biases
in calculated prior polarity scores when word
Part of Speech and annotator gender are con-
sidered.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953355555556">
Many approaches to sentiment analysis make use of
lexical resources – i.e. lists of positive and neg-
ative words – often deployed as baselines or as
features for other methods (usually machine learn-
ing based) for sentiment analysis research (Liu and
Zhang, 2012). In these lexica, words are associated
with their prior polarity, i.e. if that word out of con-
text evokes something positive or something nega-
tive. For example, wonderful has a positive connota-
tion – prior polarity – while horrible has a negative
one. These approaches have the advantage of not
needing deep semantic analysis or word sense dis-
ambiguation to assign an affective score to a word
and are domain independent (they are thus less pre-
cise but more portable).
SentiWordNet (henceforth SWN) is one of these
resources and has been widely adopted since it pro-
vides a broad-coverage lexicon – built in a semi-
automatic manner – for English (Esuli and Sebas-
tiani, 2006). Given that SWN provides polarities
scores for each word sense (also called ‘posterior
polarities’), it is necessary to derive prior polarities
from the posteriors. For example, the word cold has
a posterior polarity for the meaning “having a low
temperature” – like in “cold beer” – that is different
from the one in “cold person” which refers to “being
emotionless”. This information must be considered
when reconstructing the prior polarity of cold.
Several formulae to compute prior polarities start-
ing from posterior polarities scores have been used
in the literature. However, their performance varies
significantly depending on the adopted variant. We
show that researchers have not paid sufficient atten-
tion to this posterior-to-prior polarity issue. Indeed,
we show that some variants outperform others on
different datasets and can represent a fairer state-of-
the-art approach using SWN. On top of this, we at-
tempt to outperform the state-of-the-art formula us-
ing a learning framework that combines the various
formulae together.
In detail, we will address five main research
questions: (i) is there any relevant difference in
the posterior-to-prior polarity formulae performance
(both in regression and classification tasks), (ii) is
there any relevant variation in prior polarity values
</bodyText>
<page confidence="0.955274">
1259
</page>
<note confidence="0.7314635">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1259–1269,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999964846153846">
if we use different releases of SWN (i.e. 5WN1 or
5WN3), (iii) can a learning framework boost per-
formance of such formulae, (iv) considering word
Part of Speech (PoS), is there any relevant difference
in formulae performance, (v) considering the gender
dimension of the annotators (male/female) and the
sentiment dimension (positive/negative), is there any
relevant difference in SWN performance.
In Section 2 we briefly describe our approach and
how it differentiates from similar sentiment analysis
tasks. Then, in Sections 3 and 4, we present Sen-
tiWordNet and overview various posterior-to-prior
polarity formulae based on this resource that ap-
peared in the literature (included some new ones
we identified as potentially relevant). In Section 5
we describe the learning approach adopted on prior-
polarity formulae. In Section 6 we introduce the
ANEW and General Inquirer resources that will be
used as gold standards. Finally, in the two last sec-
tions, we present a series of experiments, both in
regression and classification tasks, that give an an-
swer to the aforementioned research questions. The
results support the hypothesis that using a learning
framework we can improve on state-of-the-art per-
formance and that there are some interesting phe-
nomena connected to PoS and annotator gender.
</bodyText>
<sectionHeader confidence="0.965964" genericHeader="introduction">
2 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999994725806452">
In the broad field of Sentiment Analysis we will fo-
cus on the specific problem of posterior-to-prior po-
larity assessment, using both regression and classifi-
cation experiments. A general overview on the field
and possible approaches can be found in (Pang and
Lee, 2008) or (Liu and Zhang, 2012).
For the regression task, we tackled the problem
of assigning affective scores (along a continuum be-
tween -1 and 1) to words using the posterior-to-prior
polarity formulae. For the classification task (assess-
ing whether a word is either positive or negative) we
used the same formulae, but considering just the sign
of the result. In these experiments we will also use a
learning framework which combines the various for-
mulae together. The underlying hypothesis is that by
blending these formulae, and looking at the same in-
formation from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways), we can give a better prediction.
The regression task is harder than binary classifi-
cation, since we want to assess not only that pretty,
beautiful and gorgeous are positive words, but also
to define a partial or total order so that gorgeous
is more positive than beautiful which, in turn, is
more positive than pretty. This is fundamental for
tasks such as affective modification of existing texts,
where words’ polarity together with their score are
necessary for creating multiple graded variations of
the original text (Guerini et al., 2008). Some of
the work that addresses the problem of sentiment
strength are presented in (Wilson et al., 2004; Pal-
toglou et al., 2010), however, their approach is mod-
eled as a multi-class classification problem (neutral,
low, medium or high sentiment) at the sentence level,
rather than a regression problem at the word level.
Other works such as (Neviarouskaya et al., 2011)
use a fine grained classification approach too, but
they consider emotion categories (anger, joy, fear,
etc.), rather than sentiment strength categories. On
the other hand, even if approaches that go beyond
pure prior polarities – e.g. using word bigram fea-
tures (Wang and Manning, 2012) – are better for
sentiment analysis tasks, there are tasks that are in-
trinsically based on the notion of words’ prior polar-
ity. Consider copywriting, where evocative names
are a key element to a successful product ( ¨Ozbal and
Strapparava, 2012; ¨Ozbal et al., 2012). In such cases
no context is given and the brand name alone, with
its perceived prior polarity, is responsible for stating
the area of competition and evoking semantic asso-
ciations. For example Mitsubishi changed the name
of one of its SUV for the Spanish market, since the
original name Pajero had a very negative prior po-
larity, as it meant ‘wanker’ in Spanish (Piller, 2003).
To our knowledge, the only work trying to address
the SWN posterior-to-prior polarity issue, compar-
ing some of the approaches appeared in the literature
is (Gatti and Guerini, 2012). However, in our previ-
ous study we only considered a regression frame-
work, we did not use machine learning and we only
tested 5WN1. So, we took this work as a starting
point for our analysis and expanded on it.
</bodyText>
<sectionHeader confidence="0.996741" genericHeader="method">
3 SentiWordNet
</sectionHeader>
<bodyText confidence="0.936005">
SentiWordNet (Esuli and Sebastiani, 2006) is a
lexical resource in which each entry is a set of
</bodyText>
<page confidence="0.979384">
1260
</page>
<bodyText confidence="0.99992835">
lemma-PoS pairs sharing the same meaning, called
“synset”. Each synset s is associated with the nu-
merical scores Pos(s) and Neg(s), which range
from 0 to 1. These scores – automatically as-
signed starting from a bunch of seed terms – rep-
resent the positive and negative valence (or pos-
terior polarity) of the synset and are inherited by
each lemma-PoS in the synset. According to the
structure of SentiWordNet, each pair can have more
than one sense and each of them takes the form of
lemma#PoS#sense-number, where the small-
est sense-number corresponds to the most frequent
sense.
Obviously, different senses can have different po-
larities. In Table 1, the first 5 senses of cold#a
present all possible combinations, included mixed
scores (cold#a#4), where positive and negative
valences are assigned to the same sense. Intuitively,
mixed scores for the same sense are acceptable, as
in “cold beer” (positive) vs. “cold pizza” (negative).
</bodyText>
<figure confidence="0.998384833333333">
PoS Offset Pos(s) Neg(s)
a 1207406 0.0 0.75
a 1212558 0.0 0.75
a 1024433 0.0 0.0
a 2443231 0.125 0.375
a 1695706 0.625 0.0
</figure>
<tableCaption confidence="0.993804">
Table 1: First five SentiWordNet entries for cold#a
</tableCaption>
<bodyText confidence="0.999958571428571">
In our experiments we use two different versions
of SWN: SentiWordNet 1.0 (SWN1), the first re-
lease of SWN, and its updated version SentiWord-
Net 3.0 (Baccianella et al., 2010) – SWN3. In
SWN3 the annotation algorithm used in SWN1
was revised, leading to an increase in the accuracy
of posterior polarities over the previous version.
</bodyText>
<sectionHeader confidence="0.989311" genericHeader="method">
4 Prior Polarities Formulae
</sectionHeader>
<bodyText confidence="0.997375666666667">
In this section we review the main strategies for
computing prior polarities used in previous stud-
ies. All the proposed approaches try to estimate
the prior polarity score from the posterior polari-
ties of all the senses for a single lemma-PoS. Given
a lemma-PoS with n senses (lemma#PoS#n), ev-
ery formula f is independently applied to all the
Pos(s) and Neg(s) . This produces two scores,
f(posScore) and f(negScore), for each lemma-
PoS. To obtain a unique prior polarity for each
lemma-PoS, f(posScore) and f(negScore) can be
mapped according to different strategies:
</bodyText>
<equation confidence="0.9963">
{ f(posScore) if f(posScore) &gt;
f(negScore)
−f(negScore) otherwise
fd = f(posScore) − f(negScore)
</equation>
<bodyText confidence="0.999420230769231">
where fm computes the absolute maximum of
the two scores, while fd computes the difference
between them. It is worth noting that f(negScore)
is always positive by construction. To obtain
a final prior polarity that ranges from -1 to 1,
the negative sign is imposed. So, consider-
ing the first 5 senses of cold#a in Table 1,
f(posScore) will be derived from the Pos(s) val-
ues &lt;0.0, 0.0, 0.0, 0.125, 0.625&gt;, while f(negScore)
from &lt;0.750,0.750,0.0,0.375,0.0&gt;. Then, the fi-
nal polarity strength returned will be either fm or fd.
The formulae (f) we tested are the following:
fs. In this formula only the first (and thus
most frequent) sense is considered for the given
lemma#PoS. This is equivalent to considering only
the SWN score for lemma#PoS#1. Based on
(Neviarouskaya et al., 2009; Agrawal and Siddiqui,
2009; Guerini et al., 2008; Chowdhury et al., 2013),
this is the most basic form of prior polarities.
mean. It calculates the mean of the positive
and negative scores for all the senses of the given
lemma#PoS. This formula has been used in (Thet
et al., 2009; Denecke, 2009; Devitt and Ahmad,
2007; Sing et al., 2012).
uni. Based on (Neviarouskaya et al., 2009), it
considers only those senses that have a Pos(s)
greater than or equal to the corresponding Neg(s),
and greater than 0 (the stronglyPos set). In case
posScore is equal to negScore, the one with the
highest weight is returned, where weights are de-
fined as the cardinality of stronglyPos divided by
the total number of senses. The same applies for the
negative senses. This is the only method, together
with rnd, for which we cannot apply fd, as it returns
a positive or negative score according to the weight.
uniw. Like uni but without the weighting system.
w1. This formula weighs each sense with a geo-
metric series of ratio 1/2. The rationale behind this
choice is based on the assumption that more frequent
</bodyText>
<equation confidence="0.582803714285714">
SynsetTerms
cold#a#1
cold#a#2
cold#a#3
cold#a#4
cold#a#5
fm =
</equation>
<page confidence="0.865362">
1261
</page>
<bodyText confidence="0.999118019230769">
senses should bear more “affective weight” than rare
senses when computing the prior polarity of a word.
The system presented in (Chaumartin, 2007) uses a
similar approach of weighted mean.
w2. Similar to the previous one, this formula
weighs each lemma with a harmonic series, see for
example (Denecke, 2008).
On top of these formulae, we implemented some
new formulae that were relevant to our task and
have not been implemented before. These for-
mulae mimic the ones discussed previously, but
they are built under a different assumption: that
the saliency (Giora, 1997) of a word’s prior polar-
ity might be more related to its posterior polari-
ties score, rather than to sense frequencies. Thus
we ordered pos5core and neg5core by strength,
giving more relevance to ‘valenced’ senses. For
instance, in Table 1, pos5core and neg5core
for cold#a become &lt;0.625,0.125, 0.0, 0.0, 0.0&gt; and
&lt;0.750,0.750,0.375, 0.0, 0.0&gt; respectively.
w1s and w1n. Like w1 and w2, but senses are
ordered by strength (sorting Pos(s) and Neg(s) in-
dependently).
w1n and w2n. Like w1 and w2 respectively, but
without considering senses that have a 0 score for
both Pos(s) and Neg(s). Our motivation is that
“empty” senses are mostly noise.
w1sn and w2sn. Like w1s and w2s, but with-
out considering senses that have a 0 score for both
Pos(s) and Neg(s).
median: return the median of the senses ordered
by polarity score.
All these prior polarities formulae are compared
against two gold standards (one for regression, one
for classification) both one by one, as in the works
mentioned above, and combined together in a learn-
ing framework (to see whether combining these fea-
tures – that capture different aspect of prior polari-
ties – can further improve the results).
Finally, we implemented two variants of a prior
polarity random baseline to asses possible advan-
tages of approaches using SWN:
rnd. This formula represents the basic baseline
random approach. It simply returns a random num-
ber between -1 and 1 for any given lemma#PoS.
swnrnd. This formula represents an advanced
random approach that incorporates some “knowl-
edge” from SWN. It takes the scores of a random
sense for the given lemma#PoS. We believe this
is a fairer baseline than rnd since SWN informa-
tion can possibly constrain the values. A similar ap-
proach has been used in (Qu et al., 2008).
</bodyText>
<sectionHeader confidence="0.944889" genericHeader="method">
5 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.9999723">
We used two non-parametric learning approaches,
Support Vector Machines (SVMs) (Shawe-Taylor
and Cristianini, 2004) and Gaussian Processes (GPs)
(Rasmussen and Williams, 2006), to test the perfor-
mance of all the metrics in conjunction. SVMs are
non-parametric deterministic algorithms that have
been widely used in several fields, in particular in
NLP where they are the state-of-the-art for various
tasks. GPs, on the other hand, are an extremely flex-
ible non-parametric probabilistic framework able to
explicitly model uncertainty, that, despite being con-
sidered state-of-the-art in regression, have rarely
been used in NLP. To our knowledge only two pre-
vious works did so (Polajnar et al., 2011; Cohn and
Specia, 2013).
Both methods take advantage of the kernel trick,
a technique used to embed the original feature space
into an alternative space where data may be linearly
separable. This is performed by the kernel func-
tion that transforms the input data in a new structure,
called kernel. How it is used to produce the predic-
tion is one of the main differences between SVMs
and GPs. In classification SVMs use the geomet-
ric mean to discriminate between the positive and
negative classes, while the GP model uses the pos-
terior probability distribution over each class. Both
frameworks support learning algorithms for regres-
sion and classification. An exhaustive explanation
of the two methodologies can be found in (Shawe-
Taylor and Cristianini, 2004) and (Rasmussen and
Williams, 2006).
In the SVM experiments, we use C-SVM and &amp;
SVM implemented in the LIBSVM toolbox (Chang
and Lin, 2011). The selection of the kernel (linear,
polynomial, radial basis function and sigmoid) and
the optimization of the parameters are carried out
through grid search in 10-fold cross-validation.
GP regression models with Gaussian noise are a
rare exception where the exact inference with like-
lihood functions is tractable, see §2 in (Rasmussen
</bodyText>
<page confidence="0.966057">
1262
</page>
<bodyText confidence="0.999986139534884">
and Williams, 2006). Unfortunately, this is not valid
for the classification task – see §3 in (Rasmussen and
Williams, 2006) – where an approximation method
is required. In this work, we use the Laplace ap-
proximation method proposed in (Williams and Bar-
ber, 1998). Different kernels are tested (covariance
for constant functions, linear with and without au-
tomatic relevance determination (ARD)1, Matern,
neural network, etc.2) and the linear logistic (lll)
and probit regression (prl) likelihood functions are
evaluated in classification. In our classification ex-
periments we tried all possible combinations of ker-
nels and likelihood functions, while in the regression
tests we ranged only on different kernels. All the GP
models were implemented using the GPML Matlab
toolbox3. Unlike SVMs, the optimization of the ker-
nel parameters can be performed without using grid
search, but the optimal parameters can be obtained
iteratively, by maximizing the marginal likelihood
(or in classification, the Laplace approximation of
the marginal likelihood). We fix at 100 the maxi-
mum number of iterations.
An interesting property of the GPs is their capa-
bility of weighting the features differently accord-
ing to their importance in the data. This is referred
to as the automatic variance determination kernel.
As demonstrated in (Weston et al., 2000), SVMs
can benefit from the application of feature selec-
tion techniques especially when there are highly re-
dundant features. Since the prior polarities formu-
lae tend to cluster in groups that provide similar re-
sults (Gatti and Guerini, 2012) – creating noise for
the learner – we want to understand whether feature
selection approaches can boost the performance of
SVMs. For this reason, we also test feature selection
prior to the SVM training. For that we used Ran-
domized Lasso, or stability selection (Meinshausen
and B¨uhlmann, 2010). Re-sampling of the training
data is performed several times and a Lasso regres-
sion model is fit on each sample. Features that ap-
pear in a given number of samples are retained. Both
the fraction of the data to be sampled and the thresh-
old to select the features can be configured. In our
</bodyText>
<footnote confidence="0.9170218">
1linone and linard in the result tables, respectively.
2More detailed information on the available kernels are in
§4 (Rasmussen and Williams, 2006)
3http://www.gaussianprocess.org/gpml/
code/matlab/doc/
</footnote>
<bodyText confidence="0.999572333333333">
experiments we set the sampling fraction to 75%,
the selection threshold to 25% and the number of re-
samples to 1,000. We refer to these as SVMfs.
</bodyText>
<sectionHeader confidence="0.991198" genericHeader="method">
6 Gold Standards
</sectionHeader>
<bodyText confidence="0.999966916666667">
To assess how well prior polarity formulae perform,
a gold standard with word polarities provided by hu-
man annotators is needed. There are many such re-
sources in the literature, each with different cover-
age and annotation characteristics. ANEW (Bradley
and Lang, 1999) rates the valence score of 1,034
words, which were presented in isolation to anno-
tators. The SO-CAL entries (Taboada et al., 2011)
were collected from corpus data and then manu-
ally tagged by a small number of annotators with
a multi-class label. These ratings were further vali-
dated through crowdsourcing. Other resources, such
as the General Inquirer lexicon (Stone et al., 1966),
provide a binomial classification (either positive or
negative) of sentiment-bearing words. The resource
presented in (Wilson et al., 2005) uses a similar bi-
nomial annotation for single words; another inter-
esting resource is WordNetAffect (Strapparava and
Valitutti, 2004) but it labels words senses and it can-
not be used for the prior polarity validation task.
In the following we describe in detail the two
resources we used for our experiments, namely
ANEW for the regression experiments and the Gen-
eral Inquirer (GI) for the classification ones.
</bodyText>
<subsectionHeader confidence="0.903773">
6.1 ANEW
</subsectionHeader>
<bodyText confidence="0.999965625">
ANEW (Bradley and Lang, 1999) is a resource de-
veloped to provide a set of normative emotional rat-
ings for a large number of words (roughly 1 thou-
sand) in the English language. It contains a set of
words that have been rated in terms of pleasure (af-
fective valence), arousal, and dominance. In par-
ticular for our task we considered the valence di-
mension. Since words were presented to subjects
in isolation (i.e. no context was provided) this re-
source represents a human validation of prior polar-
ities scores for the given words, and can be used as a
gold standard. For each word ANEW provides two
main metrics: anewµ, which correspond to the av-
erage of annotators votes, and anew,, which gives
the variance in annotators scores for the given word.
In the same way these metrics are also provided for
</bodyText>
<page confidence="0.856122">
1263
</page>
<bodyText confidence="0.956776">
the male/female annotator groups.
</bodyText>
<subsectionHeader confidence="0.994621">
6.2 General Inquirer
</subsectionHeader>
<bodyText confidence="0.999982916666667">
The Harvard General Inquirer dictionary is a widely
used resource, built for automatic text analysis
(Stone et al., 1966). Its latest revision4 contains
11789 words, tagged with 182 semantic and prag-
matic labels, as well as with their part of speech.
Words and their categories were initially taken
from the Harvard IV-4 Psychosociological Dictio-
nary (Dunphy et al., 1974) and the Lasswell Value
Dictionary (Lasswell and Namenwirth, 1969). For
this paper we consider the Positiv and Negativ
categories (1,915 words the former, 2,291 words the
latter, for a total of 4,206 affective words).
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999985423076923">
In order to use the ANEW dataset to measure
prior polarities formulae performance, we had to
assign a PoS to all the words to obtain the SWN
lemma#PoS format. To do so, we proceeded as
follows: for each word, check if it is present among
both 5WN1 and 5WN3 lemmas; if not, lemmatize
the word with the TextPro tool suite (Pianta et al.,
2008) and check if the lemma is present instead5.
If it is not found (i.e., the word cannot be aligned
automatically), remove the word from the list (this
was the case for 30 words of the 1,034 present in
ANEW). The remaining 1,004 lemmas were then
associated with all the PoS present in SWN to get
the final lemma#PoS. Note that a lemma can have
more than one PoS, for example, writer is present
only as a noun (writer#n), while yellow is present
as a verb, a noun and an adjective (yellow#v,
yellow#n, yellow#a). This gave us a list of
1,484 words in the lemma#PoS format.
In a similar way we pre-processed the GI words
that uses the generic modif label to indicate ei-
ther adjective or adverb (noun and verb PoS were
instead consistently used). Finally, all the sense-
disambiguated words in the lemma#PoS#n format
were discarded (1,114 words out of the 4,206 words
with positive or negative valence).
</bodyText>
<footnote confidence="0.9096445">
4http://www.wjh.harvard.edu/˜inquirer/
5We did not lemmatize everything to avoid duplications (for
example, if we lemmatize the ANEW entry addicted, we obtain
addict, which is already present in ANEW).
</footnote>
<bodyText confidence="0.997467333333333">
After the two datasets were built this way, we
removed the words for which the pos5core and
neg5core contained all 0 in both 5WN1 and
5WN3 (523 lemma#PoS for ANEW and 484 for
the GI dataset), since these words are not informa-
tive for our experiments. The final dataset included
961 entries for ANEW and 2,557 for GI. For each
lemma#PoS in GI and ANEW, we then applied the
prior polarity formulae described in Section 4, using
both 5WN1 and 5WN3 and annotated the results.
According to the nature of the human labels (real
numbers or -1/1), we ran several regression and clas-
sification experiments. In both cases, each dataset
was randomly split into 70% for training and the re-
maining for test. This process was repeated 5 times
to generate different splits. For each partition, opti-
mization of the learning algorithm parameters was
performed on the training data (in 10-fold cross-
validation for SVMs). Training and test sets were
normalized using the z-score.
To evaluate the performance of our regression ex-
periments on ANEW we used the Mean Absolute
Error (MAE), that averages the error over a given
test set. Accuracy was used for the classification ex-
periments on GI instead. We opted for accuracy –
rather than F1 – since for us True Negatives have
same importance as True Positives. For each experi-
ments we reported the average performance and the
standard deviation over the 5 random splits. In the
following sections, to check if there was a statisti-
cally significant difference in the results, we used
Student’s t-test for regression experiments, while
an approximate randomization test (Yeh, 2000) was
used for the classification experiments.
In Tables 2 and 3, the results of regression exper-
iments over the ANEW dataset, using 5WN1 and
5WN3, are presented. The results of the classifica-
tion experiments over the GI dataset, using 5WN1
and 5WN3 are shown in Tables 4 and 5. For the
sake of interpretability, results are divided accord-
ing to the main approaches: randoms, posterior-to-
prior formulae, learning algorithms. Note that for
classification we report the generics f and not the
fm and fd variants. In fact, both versions always
return the same classification answer (we are clas-
sifying according to the sign of f result and not its
strength). For the GPs, we report the two best con-
figurations only.
</bodyText>
<page confidence="0.978788">
1264
</page>
<table confidence="0.999123555555556">
MAE p MAE a
rnd 0.652 0.026
swnrndm 0.427 0.011
swnrndd 0.426 0.009
uniwm 0.420 0.009
maim 0.419 0.009
fsd 0.413 0.011
fsm 0.412 0.009
uni 0.410 0.010
uniwd 0.406 0.007
w1snm 0.405 0.011
maid 0.404 0.005
w2snm 0.402 0.011
mediand 0.401 0.014
w1d 0.401 0.010
w1nd 0.399 0.008
meand 0.398 0.010
w2d 0.398 0.010
medianm 0.397 0.015
w1snd 0.397 0.008
w2snd 0.397 0.008
w2nd 0.397 0.008
w1sm 0.396 0.010
w1m 0.396 0.010
w1nm 0.394 0.009
meanm 0.393 0.011
w2sd 0.393 0.008
w1sd 0.393 0.009
w2sm 0.392 0.010
w2m 0.391 0.011
w2nm 0.391 0.012
GPlinard 0.398 0.014
GPlinone 0.398 0.014
SVM 0.367 0.010
SVMfs 0.366 0.011
AVERAGE 0.398 0.010
</table>
<tableCaption confidence="0.959762">
Table 2: MAE results for metrics using SWN,
</tableCaption>
<table confidence="0.999812166666667">
MAE p MAE a
rnd 0.652 0.026
swnrndd 0.404 0.013
swnrndm 0.402 0.010
maim 0.393 0.009
fsd 0.382 0.008
uniwm 0.382 0.015
fsm 0.381 0.010
medianm 0.377 0.008
uniwd 0.377 0.012
mediand 0.377 0.011
uni 0.376 0.010
maid 0.372 0.011
meand 0.371 0.010
w1snm 0.371 0.011
w2snm 0.369 0.010
w1d 0.368 0.010
w2d 0.367 0.010
meanm 0.367 0.010
w1m 0.365 0.010
w2snd 0.364 0.011
w1snd 0.364 0.010
w1sm 0.363 0.009
w1nd 0.362 0.009
w2sd 0.362 0.010
w2m 0.362 0.010
w1sd 0.362 0.009
w1nm 0.362 0.007
w2nd 0.361 0.010
w2sm 0.360 0.009
w2nm 0.359 0.009
GPlinone 0.356 0.008
GPlinard 0.355 0.008
SVM 0.333 0.004
SVMfs 0.333 0.003
AVERAGE 0.366 0.009
</table>
<tableCaption confidence="0.999509">
Table 3: MAE results for regression using SWN3
</tableCaption>
<sectionHeader confidence="0.993834" genericHeader="method">
8 General Discussion
</sectionHeader>
<bodyText confidence="0.99939224137931">
In this section we sum up the main results of our
analysis, providing an answer to the various ques-
tions we introduced at the beginning of the paper:
SentiWordNet improves over random. One of
the first things worth noting – in Tables 2, 3, 4 and
5 – is that the random approach (rnd), as expected,
is the worst performing metric, while all other ap-
proaches, based on SWN, have statistically signif-
icant improvements both for MAE and for Accu-
racy (p &lt; 0.001). So, using SWN for posterior-
to-prior polarity computation brings benefits, since
it increases the performance above the baseline in
words’ prior polarity assessment.
SWN3 is better than SWN1. With respect to
SWN1, using SWN3 enhances performance, both
in regression (MAE p 0.398 vs. 0.366, p &lt; 0.001)
and classification (Accuracy p 0.710 vs. 0.771,
p &lt; 0.001) tasks. Since many of the approaches
described in the literature use SWN1 their results
should be revised and SWN3 should be used as
standard. This difference in performance can be
partially explained by the fact that, even after pre-
processing, for the ANEW dataset 137 lemma#PoS
have all senses equal to 0 in SWN1, while in SWN3
they are just 48. In the GI lexicon the numbers are
233 for SWN1 and 69 for SWN3.
Not all formulae are created equal. The formu-
lae described in Section 4 have very different results,
along a continuum. While inspecting every differ-
</bodyText>
<page confidence="0.97254">
1265
</page>
<table confidence="0.99985564">
Acc. µ Acc. a
rnd 0.447 0.019
swn rnd,,,, 0.639 0.026
swn rndd 0.646 0.021
fs m 0.659 0.020
uni 0.684 0.017
median 0.686 0.022
uniw 0.702 0.019
max 0.710 0.022
w1 0.712 0.021
w1n 0.713 0.022
w2n 0.714 0.023
w2 0.715 0.021
mean 0.718 0.023
w2s 0.719 0.023
w2sn 0.719 0.023
w1s 0.719 0.023
w1sn 0.719 0.023
GPlll 0.721 0.026
linard
GPprl 0.722 0.025
linard
SVM 0.733 0.021
SVMfs 0.743 0.021
Average 0.710 0.022
</table>
<tableCaption confidence="0.999795">
Table 4: Accuracy results for classification using SWN,
</tableCaption>
<bodyText confidence="0.987735923076923">
ence in performance is out of the scope of the present
paper, we can see that there is a strong difference be-
tween best and worst performing formulae both in
regression (in Table 2 w2nm is better than uniwm,
in Table 3 w2nm is better than maxm) and classifi-
cation (in Table 4 w1snm is better than fsm,in Ta-
ble 5 w2m is better than fsm) and these differences
are all statistically significant (p &lt; 0.001). Again,
these results indicate that the previous experiments
in the literature that use SWN as a baseline should
be revised to take these results into account. Further-
more, the new formulae we introduced, based on the
“posterior polarities saliency” hypothesis, proved to
be among the best performing in all experiments.
This entails that there is room for inspecting new
formulae variants other than those already proposed
in the literature.
Selecting just one sense is not a good choice.
On a side note, the approaches that rely on only one
sense polarity (namely fs, median and max) have
similar results which do not differ significantly from
swnrnd (for maxm, fsd and fsm in Table 2, and
for maxm in Table 3). These same approaches are
also far from the best performing formulae: in Ta-
ble 3, mediand differs from w2nm (p &lt; 0.05), as
do maxm, maxd, fsm and fsd (p &lt; 0.001); in Ta-
</bodyText>
<table confidence="0.999644826086957">
Acc. µ Acc. a
rnd 0.447 0.019
swn rndd 0.700 0.030
swn rnd,,,, 0.706 0.034
fs 0.723 0.014
medianm 0.742 0.016
uni 0.750 0.015
uniw 0.762 0.023
max 0.769 0.019
w2s 0.777 0.017
w2sn 0.777 0.017
w1s 0.777 0.017
w1sn 0.777 0.017
w1n 0.780 0.021
w2n 0.780 0.022
mean 0.781 0.018
w1 0.781 0.021
w2 0.781 0.021
SVM 0.779 0.016
GPl 0.779 0.018
GPg 0.781 0.018
SVMfs 0.792 0.014
Average 0.771 0.018
</table>
<tableCaption confidence="0.999696">
Table 5: Accuracy results for classification using SWN3
</tableCaption>
<bodyText confidence="0.999896125">
ble 3, fs, max and median in both their fm and fd
variants are significantly different from the best per-
forming w2nm (p &lt; 0.001). For classification, in
Table 4 and 5 the difference between the correspond-
ing best performing formula and the single senses
formulae is always significant (at least p &lt; 0.01).
Among other things, this finding entails, surpris-
ingly, that taking the first sense of a lemma#PoS in
some cases has no improvement over taking a ran-
dom sense, and that in all cases it is one of the worst
approaches with SWN. This is surprising since in
many NLP tasks, such as word sense disambigua-
tion, algorithms based on most frequent sense repre-
sent a very strong baseline6.
Learning improvements. Combining the formu-
lae in a learning framework further improves the
results over the best performing formulae, both in
regression (MAEµ with SWN1 0.366 vs. 0.391,
p &lt; 0.001; MAEµ with SWN3 0.333 vs. 0.359,
p &lt; 0.001) and in classification (Accuracyµ for
SWN1 is 0.743 vs. 0.719, p &lt; 0.001; Accuracyµ
for SWN3 is 0.792 vs. 0.781, not significant p =
0.07). Another thing worth noting is that, in re-
gression, GPs are outperformed by both versions of
</bodyText>
<footnote confidence="0.987014">
6In SemEval 2010, only 5 participants out of 29 performed
better than the most frequent threshold (Agirre et al., 2010).
</footnote>
<page confidence="0.991742">
1266
</page>
<bodyText confidence="0.999976111111111">
SVM (p &lt; 0.001), see Tables 2 and 3. This is in
contrast with the results presented in (Cohn and Spe-
cia, 2013), where GPs on the single task are on av-
erage better than SVMs. In classification, GPs have
similar performance to SVM without feature selec-
tion, and in some cases (see Table 5) even slightly
better. Analyzing the selected kernels for GPs and
SVMs, we notice that in most of the splits SVMs
prefer the radial based function, while the best per-
formance with the GPs are obtained with linear ker-
nels with and without ARD. There is no significant
difference in using linear logistic and probit regres-
sion likelihoods. In all our experiments, SVM with
feature selection leads to the best performance. This
is not surprising due the high level of redundancy
in the formulae scores. Interestingly, inspecting the
most frequent selected features by SV Mfs, we see
that features from different groups are selected, and
even the worst performing formulae can add infor-
mation, confirming the idea that viewing the same
information from different perspectives (i.e. the pos-
terior polarities provided by SWN combined in var-
ious ways) can give better predictions.
To sum up: the new state-of-the-art performance
level in prior-polarity computation is represented
by the SV Mfs approach using SWN3, and this
should be used as the reference from now on.
</bodyText>
<sectionHeader confidence="0.771897" genericHeader="evaluation">
9 PoS and Gender Experiments
</sectionHeader>
<bodyText confidence="0.999823636363636">
Next, we wanted to understand if the performance of
our approach, using SWN3, was consistent across
word PoS. In Table 6 we report the results for the
best performing formulae and learning algorithm on
the GI PoS classes. In particular for ADJ there are
1,073 words, 922 for NOUN and 508 for VERB. We
discarded adverbs since the class was too small to
allow reliable evaluation and efficient learning (only
54 instances). The results show a greater accuracy
for adjectives (p &lt; 0.01), while performance for
nouns and verbs are similar.
</bodyText>
<table confidence="0.9656658">
SVMfs best f
Acc. p Acc. v Acc. p Acc. v
ADJ 0.829 0.019 0.821 0.016
NOUN 0.784 0.021 0.765 0.023
VERBS 0.782 0.052 0.744 0.046
</table>
<tableCaption confidence="0.998703">
Table 6: Accuracy results for PoS using SWN3
</tableCaption>
<bodyText confidence="0.999928076923077">
Finally we test against the male and female ratings
provided by ANEW. As can be seen from Table 7,
SWN approaches are far more precise in predicting
Male judgments rather than Female ones (MAEp
goes from 0.392 to 0.323 with the best formula and
from 0.369 to 0.292 with SV Mfs, both differences
are significant p &lt; 0.001). Instead, in Table 8 –
which displays the results along gender and polarity
dimensions – there is no statistically significant dif-
ference in MAE on positive words between male
and female, while there is a strong statistical signifi-
cance for negative words (p &lt; 0.001).
Interestingly, there is also a large difference be-
tween positive and negative affective words (both
for male and female dimensions). This difference
is maximum for male scores on positive words com-
pared to female scores on negative words (0.283 vs.
0.399, p &lt; 0.001). Recent work by Warriner et al.
(2013) inspected the differences in prior polarity as-
sessment due to gender.
At this stage we can only note that prior polari-
ties calculated with SWN are closer to ANEW male
annotations than female ones. Understanding why
this happens would require an accurate examination
of the methods used to create WordNet and SWN
(which will be the focus of our future work).
</bodyText>
<table confidence="0.826639">
Male female
MAE p MAE v MAE p MAE v
SVMfs 0.292 0.020 0.369 0.008
best f 0.323 0.022 0.392 0.010
</table>
<tableCaption confidence="0.982172">
Table 7: MAE results for Male vs Female using SWN3
</tableCaption>
<table confidence="0.99799975">
Male female
MAE p MAE v MAE p MAE v
Pos 0.283 0.022 0.340 0.009
Neg 0.301 0.029 0.399 0.013
</table>
<tableCaption confidence="0.966054">
Table 8: MAE for Male/Female - Pos/Neg using SWN3
</tableCaption>
<sectionHeader confidence="0.994909" genericHeader="conclusions">
10 Conclusions
</sectionHeader>
<bodyText confidence="0.999976571428571">
We have presented a study on the posterior-to-prior
polarity issue, i.e. the problem of computing words’
prior polarity starting from their posterior polarities.
Using two different versions of SentiWordNet and
30 different approaches that have been proposed in
the literature, we have shown that researchers have
not paid sufficient attention to this issue. Indeed, we
</bodyText>
<page confidence="0.977146">
1267
</page>
<bodyText confidence="0.999966416666667">
showed that the better variants outperform the oth-
ers on different datasets both in regression and clas-
sification tasks, and that they can represent a fairer
state-of-art baseline approach using SentiWordNet.
On top of this, we also showed that these state-of-
the-art formulae can be further outperformed using
a learning framework that combines the various for-
mulae together. We conclude our analysis with some
experiments investigating the impact of word PoS
and annotator gender in gold standards, showing in-
teresting phenomena that requires further investiga-
tion.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999949">
The authors thanks Jos´e Camargo De Souza for his
help with feature selection. This work has been par-
tially supported by the Trento RISE PerTe project.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999343976190476">
E. Agirre, O.L. De Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation (IWSE ’10), pages 75–80, Uppsala, Sweden.
S. Agrawal and T.J. Siddiqui. 2009. Using syntactic and
contextual information for sentiment polarity analysis.
In Proceedings of the 2nd International Conference on
Interaction Sciences: Information Technology, Culture
and Human (ICIS ’09), pages 620–623, Seoul, Repub-
lic of Korea.
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Senti-
WordNet 3.0: An enhanced lexical resource for sen-
timent analysis and opinion mining. In Proceed-
ings of the 7th Conference on International Language
Resources and Evaluation (LREC ’10), pages 2200–
2204, Valletta, Malta.
M.M. Bradley and P.J. Lang. 1999. Affective norms for
English words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-1, University of
Florida.
C.C. Chang and C.J. Lin. 2011. LIBSVM: A library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2:27:1–27:27.
F.R. Chaumartin. 2007. UPAR7: A knowledge-based
system for headline sentiment tagging. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (IWSE ’07), pages 422–425, Prague, Czech Re-
public.
F.M. Chowdhury, M. Guerini, S. Tonelli, and A. Lavelli.
2013. Fbk: Sentiment analysis in twitter with tweet-
sted. In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM): Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval ’13), volume 2, pages 466–470, Atlanta,
Georgia, USA, June.
T. Cohn and L. Specia. 2013. Modelling annotator bias
with multi-task gaussian processes: An application to
machine translation quality estimation. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL ’13), pages 32–
42, Sofia, Bulgaria.
K. Denecke. 2008. Accessing medical experiences
and information. In Proceedings of the 18th Euro-
pean Conference on Arti�cial Intelligence, Workshop
on Mining Social Data (MSoDa ’08), Patras, Greece.
K. Denecke. 2009. Are SentiWordNet scores suited for
multi-domain sentiment classification? In Proceed-
ings of the 4th International Conference on Digital
Information Management (ICDIM ’09), pages 32–37,
Ann Arbor, MI, USA.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based ap-
proach. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
’07), pages 984–991, Prague, Czech Republic.
D.C. Dunphy, C.G. Bullard, and E.E.M. Crossing. 1974.
Validation of the General Inquirer Harvard IV Dictio-
nary. Paper presented at the Pisa Conference on Con-
tent Analysis.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC
’06), pages 417–422, Genova, Italy.
L. Gatti and M. Guerini. 2012. Assessing sentiment
strength in words prior polarities. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING ’12), pages 361–370, Mumbai,
India.
R. Giora. 1997. Understanding figurative and literal lan-
guage: The graded salience hypothesis. Cognitive Lin-
guistics, 8:183–206.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ’08), pages 243–246, Marrakech, Morocco.
H.D. Lasswell and J.Z. Namenwirth. 1969. The Lasswell
value dictionary. New Haven.
B. Liu and L. Zhang. 2012. A survey of opinion mining
and sentiment analysis. Mining Text Data, pages 415–
463.
</reference>
<page confidence="0.793932">
1268
</page>
<reference confidence="0.999713703703704">
N. Meinshausen and P. B¨uhlmann. 2010. Stability selec-
tion. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 72(4):417–473.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Sentiful: Generating a reliable lexicon for
sentiment analysis. In Proceedings of the 3rd Affec-
tive Computing and Intelligent Interaction (ACII ’09),
pages 363—368, Amsterdam, Netherlands.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2011. Affect analysis model: novel rule-based ap-
proach to affect sensing from text. Natural Language
Engineering, 17(1):95.
G. ¨Ozbal and C. Strapparava. 2012. A computational ap-
proach to the automation of creative naming. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL ’12), pages 703–
711, Jeju Island, Korea.
G. ¨Ozbal, C. Strapparava, and M. Guerini. 2012. Brand
Pitt: A corpus to explore the art of naming. In Pro-
ceedings of the 8th International Conference on Lan-
guage Resources and Evaluation (LREC ’12), pages
1822–1828, Istanbul, Turkey.
G. Paltoglou, M. Thelwall, and K. Buckley. 2010. On-
line textual communications annotated with grades of
emotion strength. In Proceedings of the 3rd Interna-
tional Workshop of Emotion: Corpora for research on
Emotion and Affect (satellite of LREC ’10), pages 25–
31, Valletta, Malta.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC ’08), pages 2603–2607, Marrakech, Morocco.
I. Piller. 2003. Advertising as a site of language contact.
Annual Review ofApplied Linguistics, 23:170–183.
T. Polajnar, S. Rogers, and M. Girolami. 2011. Protein
interaction detection in sentences via gaussian pro-
cesses: a preliminary evaluation. International jour-
nal of data mining and bioinformatics, 5(1):52–72.
L. Qu, C. Toprak, N. Jakob, and I. Gurevych. 2008.
Sentence level subjectivity and sentiment analysis ex-
periments in NTCIR-7 MOAT challenge. In Proceed-
ings of the 7th NTCIR Workshop Meeting (NTCIR ’08),
pages 210–217, Tokyo, Japan.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian
processes for machine learning. MIT Press.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel meth-
ods for pattern analysis. Cambridge university press.
J.K. Sing, S. Sarkar, and T.K. Mitra. 2012. Devel-
opment of a novel algorithm for sentiment analysis
based on adverb-adjective-noun combinations. In Pro-
ceedings of the 3rd National Conference on Emerging
Trends and Applications in Computer Science (NC-
ETACS ’12), pages 38–40, Shillong, India.
P.J. Stone, D.C. Dunphy, and M.S. Smith. 1966. The
General Inquirer: A Computer Approach to Content
Analysis. MIT press.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC ’04), pages 1083 –
1086, Lisbon, Portugal.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for senti-
ment analysis. Computational linguistics, 37(2):267–
307.
T.T. Thet, J.C. Na, C.S.G. Khoo, and S. Shakthikumar.
2009. Sentiment analysis of movie reviews on dis-
cussion boards using a linguistic approach. In Pro-
ceedings of the 1st international CIKM workshop on
Topic-sentiment analysis for mass opinion (TSA ’09),
pages 81–84, Hong Kong.
S. Wang and C.D. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (ACL ’12),
pages 90–94, Jeju Island, Korea.
A.B. Warriner, V. Kuperman, and M. Brysbaert. 2013.
Norms of valence, arousal, and dominance for 13,915
english lemmas. Behavior research methods, pages 1–
17.
J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Pog-
gio, and V. Vapnik. 2000. Feature selection for SVMs.
In Proceedings of the 14th Conference on Neural In-
formation Processing Systems (NIPS ’00), pages 668–
674, Denver, CO, USA.
C.K.I. Williams and D. Barber. 1998. Bayesian clas-
sification with gaussian processes. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
20(12):1342–1351.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the 19th National Conference on Artifi-
cial Intelligence (AAAI ’04), pages 761–769, San Jose,
CA, USA.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP ’05), pages
347–354, Vancouver, Canada.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proceedings of the
18th International Conference on Computational Lin-
guistics (COLING ’00), pages 947–953, Saarbr¨ucken,
Germany.
</reference>
<page confidence="0.994984">
1269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048118">
<title confidence="0.998379">Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet</title>
<author confidence="0.967071">Marco</author>
<affiliation confidence="0.651145">Trento</affiliation>
<address confidence="0.6900495">Via Sommarive 38123 Povo in Trento, Italy</address>
<email confidence="0.996854">m.guerini@trentorise.eu</email>
<author confidence="0.750528">Lorenzo</author>
<affiliation confidence="0.885443">Trento</affiliation>
<address confidence="0.753891">Via Sommarive 38123 Povo in Trento, Italy</address>
<email confidence="0.997469">l.gatti@trentorise.eu</email>
<author confidence="0.823821">Marco Fondazione Bruno Via Sommarive</author>
<address confidence="0.987355">38123 Povo in Trento, Italy</address>
<email confidence="0.997718">turchi@fbk.eu</email>
<abstract confidence="0.984378318181818">Assigning a positive or negative score to a word out of context (i.e. a word’s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words’ prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O L De Lacalle</author>
<author>C Fellbaum</author>
<author>S K Hsieh</author>
<author>M Tesconi</author>
<author>M Monachini</author>
<author>P Vossen</author>
<author>R Segers</author>
</authors>
<title>Semeval-2010 task 17: All-words word sense disambiguation on a specific domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation (IWSE ’10),</booktitle>
<pages>75--80</pages>
<location>Uppsala,</location>
<marker>Agirre, De Lacalle, Fellbaum, Hsieh, Tesconi, Monachini, Vossen, Segers, 2010</marker>
<rawString>E. Agirre, O.L. De Lacalle, C. Fellbaum, S.K. Hsieh, M. Tesconi, M. Monachini, P. Vossen, and R. Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Proceedings of the 5th International Workshop on Semantic Evaluation (IWSE ’10), pages 75–80, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Agrawal</author>
<author>T J Siddiqui</author>
</authors>
<title>Using syntactic and contextual information for sentiment polarity analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human (ICIS ’09),</booktitle>
<pages>620--623</pages>
<location>Seoul, Republic of</location>
<contexts>
<context position="11268" citStr="Agrawal and Siddiqui, 2009" startWordPosition="1804" endWordPosition="1807">in a final prior polarity that ranges from -1 to 1, the negative sign is imposed. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values &lt;0.0, 0.0, 0.0, 0.125, 0.625&gt;, while f(negScore) from &lt;0.750,0.750,0.0,0.375,0.0&gt;. Then, the final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights </context>
</contexts>
<marker>Agrawal, Siddiqui, 2009</marker>
<rawString>S. Agrawal and T.J. Siddiqui. 2009. Using syntactic and contextual information for sentiment polarity analysis. In Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human (ICIS ’09), pages 620–623, Seoul, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC ’10),</booktitle>
<pages>2200--2204</pages>
<location>Valletta,</location>
<contexts>
<context position="9607" citStr="Baccianella et al., 2010" startWordPosition="1534" endWordPosition="1537"> senses of cold#a present all possible combinations, included mixed scores (cold#a#4), where positive and negative valences are assigned to the same sense. Intuitively, mixed scores for the same sense are acceptable, as in “cold beer” (positive) vs. “cold pizza” (negative). PoS Offset Pos(s) Neg(s) a 1207406 0.0 0.75 a 1212558 0.0 0.75 a 1024433 0.0 0.0 a 2443231 0.125 0.375 a 1695706 0.625 0.0 Table 1: First five SentiWordNet entries for cold#a In our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWordNet 3.0 (Baccianella et al., 2010) – SWN3. In SWN3 the annotation algorithm used in SWN1 was revised, leading to an increase in the accuracy of posterior polarities over the previous version. 4 Prior Polarities Formulae In this section we review the main strategies for computing prior polarities used in previous studies. All the proposed approaches try to estimate the prior polarity score from the posterior polarities of all the senses for a single lemma-PoS. Given a lemma-PoS with n senses (lemma#PoS#n), every formula f is independently applied to all the Pos(s) and Neg(s) . This produces two scores, f(posScore) and f(negScor</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC ’10), pages 2200– 2204, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Bradley</author>
<author>P J Lang</author>
</authors>
<title>Affective norms for English words (ANEW): Instruction manual and affective ratings.</title>
<date>1999</date>
<tech>Technical Report C-1,</tech>
<institution>University of Florida.</institution>
<contexts>
<context position="19499" citStr="Bradley and Lang, 1999" startWordPosition="3141" endWordPosition="3144"> linard in the result tables, respectively. 2More detailed information on the available kernels are in §4 (Rasmussen and Williams, 2006) 3http://www.gaussianprocess.org/gpml/ code/matlab/doc/ experiments we set the sampling fraction to 75%, the selection threshold to 25% and the number of resamples to 1,000. We refer to these as SVMfs. 6 Gold Standards To assess how well prior polarity formulae perform, a gold standard with word polarities provided by human annotators is needed. There are many such resources in the literature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting reso</context>
</contexts>
<marker>Bradley, Lang, 1999</marker>
<rawString>M.M. Bradley and P.J. Lang. 1999. Affective norms for English words (ANEW): Instruction manual and affective ratings. Technical Report C-1, University of Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="16353" citStr="Chang and Lin, 2011" startWordPosition="2644" endWordPosition="2647">ture, called kernel. How it is used to produce the prediction is one of the main differences between SVMs and GPs. In classification SVMs use the geometric mean to discriminate between the positive and negative classes, while the GP model uses the posterior probability distribution over each class. Both frameworks support learning algorithms for regression and classification. An exhaustive explanation of the two methodologies can be found in (ShaweTaylor and Cristianini, 2004) and (Rasmussen and Williams, 2006). In the SVM experiments, we use C-SVM and &amp; SVM implemented in the LIBSVM toolbox (Chang and Lin, 2011). The selection of the kernel (linear, polynomial, radial basis function and sigmoid) and the optimization of the parameters are carried out through grid search in 10-fold cross-validation. GP regression models with Gaussian noise are a rare exception where the exact inference with likelihood functions is tractable, see §2 in (Rasmussen 1262 and Williams, 2006). Unfortunately, this is not valid for the classification task – see §3 in (Rasmussen and Williams, 2006) – where an approximation method is required. In this work, we use the Laplace approximation method proposed in (Williams and Barber</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>C.C. Chang and C.J. Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Chaumartin</author>
</authors>
<title>UPAR7: A knowledge-based system for headline sentiment tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (IWSE ’07),</booktitle>
<pages>422--425</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12552" citStr="Chaumartin, 2007" startWordPosition="2025" endWordPosition="2026">number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight. uniw. Like uni but without the weighting system. w1. This formula weighs each sense with a geometric series of ratio 1/2. The rationale behind this choice is based on the assumption that more frequent SynsetTerms cold#a#1 cold#a#2 cold#a#3 cold#a#4 cold#a#5 fm = 1261 senses should bear more “affective weight” than rare senses when computing the prior polarity of a word. The system presented in (Chaumartin, 2007) uses a similar approach of weighted mean. w2. Similar to the previous one, this formula weighs each lemma with a harmonic series, see for example (Denecke, 2008). On top of these formulae, we implemented some new formulae that were relevant to our task and have not been implemented before. These formulae mimic the ones discussed previously, but they are built under a different assumption: that the saliency (Giora, 1997) of a word’s prior polarity might be more related to its posterior polarities score, rather than to sense frequencies. Thus we ordered pos5core and neg5core by strength, giving</context>
</contexts>
<marker>Chaumartin, 2007</marker>
<rawString>F.R. Chaumartin. 2007. UPAR7: A knowledge-based system for headline sentiment tagging. In Proceedings of the 4th International Workshop on Semantic Evaluations (IWSE ’07), pages 422–425, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Chowdhury</author>
<author>M Guerini</author>
<author>S Tonelli</author>
<author>A Lavelli</author>
</authors>
<title>Fbk: Sentiment analysis in twitter with tweetsted.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval ’13),</booktitle>
<volume>2</volume>
<pages>466--470</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="11315" citStr="Chowdhury et al., 2013" startWordPosition="1812" endWordPosition="1815">, the negative sign is imposed. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values &lt;0.0, 0.0, 0.0, 0.125, 0.625&gt;, while f(negScore) from &lt;0.750,0.750,0.0,0.375,0.0&gt;. Then, the final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos d</context>
</contexts>
<marker>Chowdhury, Guerini, Tonelli, Lavelli, 2013</marker>
<rawString>F.M. Chowdhury, M. Guerini, S. Tonelli, and A. Lavelli. 2013. Fbk: Sentiment analysis in twitter with tweetsted. In Second Joint Conference on Lexical and Computational Semantics (*SEM): Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval ’13), volume 2, pages 466–470, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>L Specia</author>
</authors>
<title>Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL ’13),</booktitle>
<pages>32--42</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="15479" citStr="Cohn and Specia, 2013" startWordPosition="2500" endWordPosition="2503">istianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-of-the-art in regression, have rarely been used in NLP. To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia, 2013). Both methods take advantage of the kernel trick, a technique used to embed the original feature space into an alternative space where data may be linearly separable. This is performed by the kernel function that transforms the input data in a new structure, called kernel. How it is used to produce the prediction is one of the main differences between SVMs and GPs. In classification SVMs use the geometric mean to discriminate between the positive and negative classes, while the GP model uses the posterior probability distribution over each class. Both frameworks support learning algorithms fo</context>
<context position="32015" citStr="Cohn and Specia, 2013" startWordPosition="5297" endWordPosition="5301">s the results over the best performing formulae, both in regression (MAEµ with SWN1 0.366 vs. 0.391, p &lt; 0.001; MAEµ with SWN3 0.333 vs. 0.359, p &lt; 0.001) and in classification (Accuracyµ for SWN1 is 0.743 vs. 0.719, p &lt; 0.001; Accuracyµ for SWN3 is 0.792 vs. 0.781, not significant p = 0.07). Another thing worth noting is that, in regression, GPs are outperformed by both versions of 6In SemEval 2010, only 5 participants out of 29 performed better than the most frequent threshold (Agirre et al., 2010). 1266 SVM (p &lt; 0.001), see Tables 2 and 3. This is in contrast with the results presented in (Cohn and Specia, 2013), where GPs on the single task are on average better than SVMs. In classification, GPs have similar performance to SVM without feature selection, and in some cases (see Table 5) even slightly better. Analyzing the selected kernels for GPs and SVMs, we notice that in most of the splits SVMs prefer the radial based function, while the best performance with the GPs are obtained with linear kernels with and without ARD. There is no significant difference in using linear logistic and probit regression likelihoods. In all our experiments, SVM with feature selection leads to the best performance. Thi</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>T. Cohn and L. Specia. 2013. Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL ’13), pages 32– 42, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Denecke</author>
</authors>
<title>Accessing medical experiences and information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Arti�cial Intelligence, Workshop on Mining Social Data (MSoDa ’08),</booktitle>
<location>Patras, Greece.</location>
<contexts>
<context position="12714" citStr="Denecke, 2008" startWordPosition="2052" endWordPosition="2053">ative score according to the weight. uniw. Like uni but without the weighting system. w1. This formula weighs each sense with a geometric series of ratio 1/2. The rationale behind this choice is based on the assumption that more frequent SynsetTerms cold#a#1 cold#a#2 cold#a#3 cold#a#4 cold#a#5 fm = 1261 senses should bear more “affective weight” than rare senses when computing the prior polarity of a word. The system presented in (Chaumartin, 2007) uses a similar approach of weighted mean. w2. Similar to the previous one, this formula weighs each lemma with a harmonic series, see for example (Denecke, 2008). On top of these formulae, we implemented some new formulae that were relevant to our task and have not been implemented before. These formulae mimic the ones discussed previously, but they are built under a different assumption: that the saliency (Giora, 1997) of a word’s prior polarity might be more related to its posterior polarities score, rather than to sense frequencies. Thus we ordered pos5core and neg5core by strength, giving more relevance to ‘valenced’ senses. For instance, in Table 1, pos5core and neg5core for cold#a become &lt;0.625,0.125, 0.0, 0.0, 0.0&gt; and &lt;0.750,0.750,0.375, 0.0, </context>
</contexts>
<marker>Denecke, 2008</marker>
<rawString>K. Denecke. 2008. Accessing medical experiences and information. In Proceedings of the 18th European Conference on Arti�cial Intelligence, Workshop on Mining Social Data (MSoDa ’08), Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Denecke</author>
</authors>
<title>Are SentiWordNet scores suited for multi-domain sentiment classification?</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th International Conference on Digital Information Management (ICDIM ’09),</booktitle>
<pages>32--37</pages>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="11537" citStr="Denecke, 2009" startWordPosition="1854" endWordPosition="1855"> final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight. </context>
</contexts>
<marker>Denecke, 2009</marker>
<rawString>K. Denecke. 2009. Are SentiWordNet scores suited for multi-domain sentiment classification? In Proceedings of the 4th International Conference on Digital Information Management (ICDIM ’09), pages 32–37, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Devitt</author>
<author>K Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesion-based approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07),</booktitle>
<pages>984--991</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11561" citStr="Devitt and Ahmad, 2007" startWordPosition="1856" endWordPosition="1859"> strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight. uniw. Like uni but witho</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>A. Devitt and K. Ahmad. 2007. Sentiment polarity identification in financial news: A cohesion-based approach. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07), pages 984–991, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Dunphy</author>
<author>C G Bullard</author>
<author>E E M Crossing</author>
</authors>
<title>Validation of the General Inquirer Harvard IV Dictionary. Paper presented at the Pisa Conference on Content Analysis.</title>
<date>1974</date>
<contexts>
<context position="21673" citStr="Dunphy et al., 1974" startWordPosition="3501" endWordPosition="3504">newµ, which correspond to the average of annotators votes, and anew,, which gives the variance in annotators scores for the given word. In the same way these metrics are also provided for 1263 the male/female annotator groups. 6.2 General Inquirer The Harvard General Inquirer dictionary is a widely used resource, built for automatic text analysis (Stone et al., 1966). Its latest revision4 contains 11789 words, tagged with 182 semantic and pragmatic labels, as well as with their part of speech. Words and their categories were initially taken from the Harvard IV-4 Psychosociological Dictionary (Dunphy et al., 1974) and the Lasswell Value Dictionary (Lasswell and Namenwirth, 1969). For this paper we consider the Positiv and Negativ categories (1,915 words the former, 2,291 words the latter, for a total of 4,206 affective words). 7 Experiments In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format. To do so, we proceeded as follows: for each word, check if it is present among both 5WN1 and 5WN3 lemmas; if not, lemmatize the word with the TextPro tool suite (Pianta et al., 2008) and check if the lemma is </context>
</contexts>
<marker>Dunphy, Bullard, Crossing, 1974</marker>
<rawString>D.C. Dunphy, C.G. Bullard, and E.E.M. Crossing. 1974. Validation of the General Inquirer Harvard IV Dictionary. Paper presented at the Pisa Conference on Content Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on International Language Resources and Evaluation (LREC ’06),</booktitle>
<pages>417--422</pages>
<location>Genova, Italy.</location>
<contexts>
<context position="2209" citStr="Esuli and Sebastiani, 2006" startWordPosition="346" endWordPosition="350">heir prior polarity, i.e. if that word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. These approaches have the advantage of not needing deep semantic analysis or word sense disambiguation to assign an affective score to a word and are domain independent (they are thus less precise but more portable). SentiWordNet (henceforth SWN) is one of these resources and has been widely adopted since it provides a broad-coverage lexicon – built in a semiautomatic manner – for English (Esuli and Sebastiani, 2006). Given that SWN provides polarities scores for each word sense (also called ‘posterior polarities’), it is necessary to derive prior polarities from the posteriors. For example, the word cold has a posterior polarity for the meaning “having a low temperature” – like in “cold beer” – that is different from the one in “cold person” which refers to “being emotionless”. This information must be considered when reconstructing the prior polarity of cold. Several formulae to compute prior polarities starting from posterior polarities scores have been used in the literature. However, their performanc</context>
<context position="8260" citStr="Esuli and Sebastiani, 2006" startWordPosition="1307" endWordPosition="1310">shi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, as it meant ‘wanker’ in Spanish (Piller, 2003). To our knowledge, the only work trying to address the SWN posterior-to-prior polarity issue, comparing some of the approaches appeared in the literature is (Gatti and Guerini, 2012). However, in our previous study we only considered a regression framework, we did not use machine learning and we only tested 5WN1. So, we took this work as a starting point for our analysis and expanded on it. 3 SentiWordNet SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each entry is a set of 1260 lemma-PoS pairs sharing the same meaning, called “synset”. Each synset s is associated with the numerical scores Pos(s) and Neg(s), which range from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative valence (or posterior polarity) of the synset and are inherited by each lemma-PoS in the synset. According to the structure of SentiWordNet, each pair can have more than one sense and each of them takes the form of lemma#PoS#sense-number, where the smallest sense-number </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th Conference on International Language Resources and Evaluation (LREC ’06), pages 417–422, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gatti</author>
<author>M Guerini</author>
</authors>
<title>Assessing sentiment strength in words prior polarities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12),</booktitle>
<pages>361--370</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="7992" citStr="Gatti and Guerini, 2012" startWordPosition="1260" endWordPosition="1263">ul product ( ¨Ozbal and Strapparava, 2012; ¨Ozbal et al., 2012). In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, as it meant ‘wanker’ in Spanish (Piller, 2003). To our knowledge, the only work trying to address the SWN posterior-to-prior polarity issue, comparing some of the approaches appeared in the literature is (Gatti and Guerini, 2012). However, in our previous study we only considered a regression framework, we did not use machine learning and we only tested 5WN1. So, we took this work as a starting point for our analysis and expanded on it. 3 SentiWordNet SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each entry is a set of 1260 lemma-PoS pairs sharing the same meaning, called “synset”. Each synset s is associated with the numerical scores Pos(s) and Neg(s), which range from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative val</context>
<context position="18280" citStr="Gatti and Guerini, 2012" startWordPosition="2942" endWordPosition="2945">l likelihood (or in classification, the Laplace approximation of the marginal likelihood). We fix at 100 the maximum number of iterations. An interesting property of the GPs is their capability of weighting the features differently according to their importance in the data. This is referred to as the automatic variance determination kernel. As demonstrated in (Weston et al., 2000), SVMs can benefit from the application of feature selection techniques especially when there are highly redundant features. Since the prior polarities formulae tend to cluster in groups that provide similar results (Gatti and Guerini, 2012) – creating noise for the learner – we want to understand whether feature selection approaches can boost the performance of SVMs. For this reason, we also test feature selection prior to the SVM training. For that we used Randomized Lasso, or stability selection (Meinshausen and B¨uhlmann, 2010). Re-sampling of the training data is performed several times and a Lasso regression model is fit on each sample. Features that appear in a given number of samples are retained. Both the fraction of the data to be sampled and the threshold to select the features can be configured. In our 1linone and lin</context>
</contexts>
<marker>Gatti, Guerini, 2012</marker>
<rawString>L. Gatti and M. Guerini. 2012. Assessing sentiment strength in words prior polarities. In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12), pages 361–370, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Giora</author>
</authors>
<title>Understanding figurative and literal language: The graded salience hypothesis. Cognitive Linguistics,</title>
<date>1997</date>
<pages>8--183</pages>
<contexts>
<context position="12976" citStr="Giora, 1997" startWordPosition="2095" endWordPosition="2096">a#2 cold#a#3 cold#a#4 cold#a#5 fm = 1261 senses should bear more “affective weight” than rare senses when computing the prior polarity of a word. The system presented in (Chaumartin, 2007) uses a similar approach of weighted mean. w2. Similar to the previous one, this formula weighs each lemma with a harmonic series, see for example (Denecke, 2008). On top of these formulae, we implemented some new formulae that were relevant to our task and have not been implemented before. These formulae mimic the ones discussed previously, but they are built under a different assumption: that the saliency (Giora, 1997) of a word’s prior polarity might be more related to its posterior polarities score, rather than to sense frequencies. Thus we ordered pos5core and neg5core by strength, giving more relevance to ‘valenced’ senses. For instance, in Table 1, pos5core and neg5core for cold#a become &lt;0.625,0.125, 0.0, 0.0, 0.0&gt; and &lt;0.750,0.750,0.375, 0.0, 0.0&gt; respectively. w1s and w1n. Like w1 and w2, but senses are ordered by strength (sorting Pos(s) and Neg(s) independently). w1n and w2n. Like w1 and w2 respectively, but without considering senses that have a 0 score for both Pos(s) and Neg(s). Our motivation </context>
</contexts>
<marker>Giora, 1997</marker>
<rawString>R. Giora. 1997. Understanding figurative and literal language: The graded salience hypothesis. Cognitive Linguistics, 8:183–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guerini</author>
<author>O Stock</author>
<author>C Strapparava</author>
</authors>
<title>Valentino: A tool for valence shifting of natural language texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC ’08),</booktitle>
<pages>243--246</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="6505" citStr="Guerini et al., 2008" startWordPosition="1015" endWordPosition="1018">es (i.e. the posterior polarities provided by SWN combined in various ways), we can give a better prediction. The regression task is harder than binary classification, since we want to assess not only that pretty, beautiful and gorgeous are positive words, but also to define a partial or total order so that gorgeous is more positive than beautiful which, in turn, is more positive than pretty. This is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008). Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities</context>
<context position="11290" citStr="Guerini et al., 2008" startWordPosition="1808" endWordPosition="1811">at ranges from -1 to 1, the negative sign is imposed. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values &lt;0.0, 0.0, 0.0, 0.125, 0.625&gt;, while f(negScore) from &lt;0.750,0.750,0.0,0.375,0.0&gt;. Then, the final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the car</context>
</contexts>
<marker>Guerini, Stock, Strapparava, 2008</marker>
<rawString>M. Guerini, O. Stock, and C. Strapparava. 2008. Valentino: A tool for valence shifting of natural language texts. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC ’08), pages 243–246, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D Lasswell</author>
<author>J Z Namenwirth</author>
</authors>
<title>The Lasswell value dictionary.</title>
<date>1969</date>
<location>New Haven.</location>
<contexts>
<context position="21739" citStr="Lasswell and Namenwirth, 1969" startWordPosition="3510" endWordPosition="3513">s, and anew,, which gives the variance in annotators scores for the given word. In the same way these metrics are also provided for 1263 the male/female annotator groups. 6.2 General Inquirer The Harvard General Inquirer dictionary is a widely used resource, built for automatic text analysis (Stone et al., 1966). Its latest revision4 contains 11789 words, tagged with 182 semantic and pragmatic labels, as well as with their part of speech. Words and their categories were initially taken from the Harvard IV-4 Psychosociological Dictionary (Dunphy et al., 1974) and the Lasswell Value Dictionary (Lasswell and Namenwirth, 1969). For this paper we consider the Positiv and Negativ categories (1,915 words the former, 2,291 words the latter, for a total of 4,206 affective words). 7 Experiments In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format. To do so, we proceeded as follows: for each word, check if it is present among both 5WN1 and 5WN3 lemmas; if not, lemmatize the word with the TextPro tool suite (Pianta et al., 2008) and check if the lemma is present instead5. If it is not found (i.e., the word cannot be ali</context>
</contexts>
<marker>Lasswell, Namenwirth, 1969</marker>
<rawString>H.D. Lasswell and J.Z. Namenwirth. 1969. The Lasswell value dictionary. New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>L Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis. Mining Text Data,</title>
<date>2012</date>
<pages>415--463</pages>
<contexts>
<context position="1536" citStr="Liu and Zhang, 2012" startWordPosition="232" endWordPosition="235">s tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words’ prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered. 1 Introduction Many approaches to sentiment analysis make use of lexical resources – i.e. lists of positive and negative words – often deployed as baselines or as features for other methods (usually machine learning based) for sentiment analysis research (Liu and Zhang, 2012). In these lexica, words are associated with their prior polarity, i.e. if that word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. These approaches have the advantage of not needing deep semantic analysis or word sense disambiguation to assign an affective score to a word and are domain independent (they are thus less precise but more portable). SentiWordNet (henceforth SWN) is one of these resources and has been widely adopted since it provides a broad-coverage lexicon – b</context>
<context position="5324" citStr="Liu and Zhang, 2012" startWordPosition="823" endWordPosition="826"> both in regression and classification tasks, that give an answer to the aforementioned research questions. The results support the hypothesis that using a learning framework we can improve on state-of-the-art performance and that there are some interesting phenomena connected to PoS and annotator gender. 2 Proposed Approach In the broad field of Sentiment Analysis we will focus on the specific problem of posterior-to-prior polarity assessment, using both regression and classification experiments. A general overview on the field and possible approaches can be found in (Pang and Lee, 2008) or (Liu and Zhang, 2012). For the regression task, we tackled the problem of assigning affective scores (along a continuum between -1 and 1) to words using the posterior-to-prior polarity formulae. For the classification task (assessing whether a word is either positive or negative) we used the same formulae, but considering just the sign of the result. In these experiments we will also use a learning framework which combines the various formulae together. The underlying hypothesis is that by blending these formulae, and looking at the same information from different perspectives (i.e. the posterior polarities provid</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>B. Liu and L. Zhang. 2012. A survey of opinion mining and sentiment analysis. Mining Text Data, pages 415– 463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Meinshausen</author>
<author>P B¨uhlmann</author>
</authors>
<title>Stability selection.</title>
<date>2010</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>72</volume>
<issue>4</issue>
<marker>Meinshausen, B¨uhlmann, 2010</marker>
<rawString>N. Meinshausen and P. B¨uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4):417–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Neviarouskaya</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Sentiful: Generating a reliable lexicon for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd Affective Computing and Intelligent Interaction (ACII ’09),</booktitle>
<pages>363--368</pages>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="11240" citStr="Neviarouskaya et al., 2009" startWordPosition="1800" endWordPosition="1803">ive by construction. To obtain a final prior polarity that ranges from -1 to 1, the negative sign is imposed. So, considering the first 5 senses of cold#a in Table 1, f(posScore) will be derived from the Pos(s) values &lt;0.0, 0.0, 0.0, 0.125, 0.625&gt;, while f(negScore) from &lt;0.750,0.750,0.0,0.375,0.0&gt;. Then, the final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2009</marker>
<rawString>A. Neviarouskaya, H. Prendinger, and M. Ishizuka. 2009. Sentiful: Generating a reliable lexicon for sentiment analysis. In Proceedings of the 3rd Affective Computing and Intelligent Interaction (ACII ’09), pages 363—368, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Neviarouskaya</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Affect analysis model: novel rule-based approach to affect sensing from text.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="6876" citStr="Neviarouskaya et al., 2011" startWordPosition="1076" endWordPosition="1079"> is more positive than pretty. This is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008). Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities – e.g. using word bigram features (Wang and Manning, 2012) – are better for sentiment analysis tasks, there are tasks that are intrinsically based on the notion of words’ prior polarity. Consider copywriting, where evocative names are a key element to a successful product ( ¨Ozbal and Strapparava, 2012; ¨Ozbal et al., 2012). In such cases no context is given and the b</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2011</marker>
<rawString>A. Neviarouskaya, H. Prendinger, and M. Ishizuka. 2011. Affect analysis model: novel rule-based approach to affect sensing from text. Natural Language Engineering, 17(1):95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G ¨Ozbal</author>
<author>C Strapparava</author>
</authors>
<title>A computational approach to the automation of creative naming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12),</booktitle>
<pages>703--711</pages>
<location>Jeju Island,</location>
<marker>¨Ozbal, Strapparava, 2012</marker>
<rawString>G. ¨Ozbal and C. Strapparava. 2012. A computational approach to the automation of creative naming. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12), pages 703– 711, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G ¨Ozbal</author>
<author>C Strapparava</author>
<author>M Guerini</author>
</authors>
<title>Brand Pitt: A corpus to explore the art of naming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC ’12),</booktitle>
<pages>1822--1828</pages>
<location>Istanbul, Turkey.</location>
<marker>¨Ozbal, Strapparava, Guerini, 2012</marker>
<rawString>G. ¨Ozbal, C. Strapparava, and M. Guerini. 2012. Brand Pitt: A corpus to explore the art of naming. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC ’12), pages 1822–1828, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paltoglou</author>
<author>M Thelwall</author>
<author>K Buckley</author>
</authors>
<title>Online textual communications annotated with grades of emotion strength.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd International Workshop of Emotion: Corpora for research on Emotion and Affect (satellite of LREC ’10),</booktitle>
<pages>25--31</pages>
<location>Valletta,</location>
<contexts>
<context position="6635" citStr="Paltoglou et al., 2010" startWordPosition="1037" endWordPosition="1041">k is harder than binary classification, since we want to assess not only that pretty, beautiful and gorgeous are positive words, but also to define a partial or total order so that gorgeous is more positive than beautiful which, in turn, is more positive than pretty. This is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008). Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities – e.g. using word bigram features (Wang and Manning, 2012) – are better for sentiment analysis tasks, there are tasks that are in</context>
</contexts>
<marker>Paltoglou, Thelwall, Buckley, 2010</marker>
<rawString>G. Paltoglou, M. Thelwall, and K. Buckley. 2010. Online textual communications annotated with grades of emotion strength. In Proceedings of the 3rd International Workshop of Emotion: Corpora for research on Emotion and Affect (satellite of LREC ’10), pages 25– 31, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="5299" citStr="Pang and Lee, 2008" startWordPosition="818" endWordPosition="821">a series of experiments, both in regression and classification tasks, that give an answer to the aforementioned research questions. The results support the hypothesis that using a learning framework we can improve on state-of-the-art performance and that there are some interesting phenomena connected to PoS and annotator gender. 2 Proposed Approach In the broad field of Sentiment Analysis we will focus on the specific problem of posterior-to-prior polarity assessment, using both regression and classification experiments. A general overview on the field and possible approaches can be found in (Pang and Lee, 2008) or (Liu and Zhang, 2012). For the regression task, we tackled the problem of assigning affective scores (along a continuum between -1 and 1) to words using the posterior-to-prior polarity formulae. For the classification task (assessing whether a word is either positive or negative) we used the same formulae, but considering just the sign of the result. In these experiments we will also use a learning framework which combines the various formulae together. The underlying hypothesis is that by blending these formulae, and looking at the same information from different perspectives (i.e. the po</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>C Girardi</author>
<author>R Zanoli</author>
</authors>
<title>The TextPro tool suite.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC ’08),</booktitle>
<pages>2603--2607</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="22246" citStr="Pianta et al., 2008" startWordPosition="3601" endWordPosition="3604">hosociological Dictionary (Dunphy et al., 1974) and the Lasswell Value Dictionary (Lasswell and Namenwirth, 1969). For this paper we consider the Positiv and Negativ categories (1,915 words the former, 2,291 words the latter, for a total of 4,206 affective words). 7 Experiments In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format. To do so, we proceeded as follows: for each word, check if it is present among both 5WN1 and 5WN3 lemmas; if not, lemmatize the word with the TextPro tool suite (Pianta et al., 2008) and check if the lemma is present instead5. If it is not found (i.e., the word cannot be aligned automatically), remove the word from the list (this was the case for 30 words of the 1,034 present in ANEW). The remaining 1,004 lemmas were then associated with all the PoS present in SWN to get the final lemma#PoS. Note that a lemma can have more than one PoS, for example, writer is present only as a noun (writer#n), while yellow is present as a verb, a noun and an adjective (yellow#v, yellow#n, yellow#a). This gave us a list of 1,484 words in the lemma#PoS format. In a similar way we pre-proces</context>
</contexts>
<marker>Pianta, Girardi, Zanoli, 2008</marker>
<rawString>E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro tool suite. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC ’08), pages 2603–2607, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Piller</author>
</authors>
<title>Advertising as a site of language contact. Annual Review ofApplied Linguistics,</title>
<date>2003</date>
<pages>23--170</pages>
<contexts>
<context position="7809" citStr="Piller, 2003" startWordPosition="1233" endWordPosition="1234">ysis tasks, there are tasks that are intrinsically based on the notion of words’ prior polarity. Consider copywriting, where evocative names are a key element to a successful product ( ¨Ozbal and Strapparava, 2012; ¨Ozbal et al., 2012). In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, as it meant ‘wanker’ in Spanish (Piller, 2003). To our knowledge, the only work trying to address the SWN posterior-to-prior polarity issue, comparing some of the approaches appeared in the literature is (Gatti and Guerini, 2012). However, in our previous study we only considered a regression framework, we did not use machine learning and we only tested 5WN1. So, we took this work as a starting point for our analysis and expanded on it. 3 SentiWordNet SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each entry is a set of 1260 lemma-PoS pairs sharing the same meaning, called “synset”. Each synset s is associated wi</context>
</contexts>
<marker>Piller, 2003</marker>
<rawString>I. Piller. 2003. Advertising as a site of language contact. Annual Review ofApplied Linguistics, 23:170–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Polajnar</author>
<author>S Rogers</author>
<author>M Girolami</author>
</authors>
<title>Protein interaction detection in sentences via gaussian processes: a preliminary evaluation. International journal of data mining and bioinformatics,</title>
<date>2011</date>
<pages>5--1</pages>
<contexts>
<context position="15455" citStr="Polajnar et al., 2011" startWordPosition="2496" endWordPosition="2499">s) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-of-the-art in regression, have rarely been used in NLP. To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia, 2013). Both methods take advantage of the kernel trick, a technique used to embed the original feature space into an alternative space where data may be linearly separable. This is performed by the kernel function that transforms the input data in a new structure, called kernel. How it is used to produce the prediction is one of the main differences between SVMs and GPs. In classification SVMs use the geometric mean to discriminate between the positive and negative classes, while the GP model uses the posterior probability distribution over each class. Both frameworks suppor</context>
</contexts>
<marker>Polajnar, Rogers, Girolami, 2011</marker>
<rawString>T. Polajnar, S. Rogers, and M. Girolami. 2011. Protein interaction detection in sentences via gaussian processes: a preliminary evaluation. International journal of data mining and bioinformatics, 5(1):52–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qu</author>
<author>C Toprak</author>
<author>N Jakob</author>
<author>I Gurevych</author>
</authors>
<title>Sentence level subjectivity and sentiment analysis experiments in NTCIR-7 MOAT challenge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop Meeting (NTCIR ’08),</booktitle>
<pages>210--217</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="14734" citStr="Qu et al., 2008" startWordPosition="2389" endWordPosition="2392">r improve the results). Finally, we implemented two variants of a prior polarity random baseline to asses possible advantages of approaches using SWN: rnd. This formula represents the basic baseline random approach. It simply returns a random number between -1 and 1 for any given lemma#PoS. swnrnd. This formula represents an advanced random approach that incorporates some “knowledge” from SWN. It takes the scores of a random sense for the given lemma#PoS. We believe this is a fairer baseline than rnd since SWN information can possibly constrain the values. A similar approach has been used in (Qu et al., 2008). 5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-of-th</context>
</contexts>
<marker>Qu, Toprak, Jakob, Gurevych, 2008</marker>
<rawString>L. Qu, C. Toprak, N. Jakob, and I. Gurevych. 2008. Sentence level subjectivity and sentiment analysis experiments in NTCIR-7 MOAT challenge. In Proceedings of the 7th NTCIR Workshop Meeting (NTCIR ’08), pages 210–217, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Rasmussen</author>
<author>C K I Williams</author>
</authors>
<title>Gaussian processes for machine learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14933" citStr="Rasmussen and Williams, 2006" startWordPosition="2414" endWordPosition="2417">sic baseline random approach. It simply returns a random number between -1 and 1 for any given lemma#PoS. swnrnd. This formula represents an advanced random approach that incorporates some “knowledge” from SWN. It takes the scores of a random sense for the given lemma#PoS. We believe this is a fairer baseline than rnd since SWN information can possibly constrain the values. A similar approach has been used in (Qu et al., 2008). 5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-of-the-art in regression, have rarely been used in NLP. To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia, 2013). Both methods take advantage of the kernel trick, a t</context>
<context position="16249" citStr="Rasmussen and Williams, 2006" startWordPosition="2625" endWordPosition="2628">may be linearly separable. This is performed by the kernel function that transforms the input data in a new structure, called kernel. How it is used to produce the prediction is one of the main differences between SVMs and GPs. In classification SVMs use the geometric mean to discriminate between the positive and negative classes, while the GP model uses the posterior probability distribution over each class. Both frameworks support learning algorithms for regression and classification. An exhaustive explanation of the two methodologies can be found in (ShaweTaylor and Cristianini, 2004) and (Rasmussen and Williams, 2006). In the SVM experiments, we use C-SVM and &amp; SVM implemented in the LIBSVM toolbox (Chang and Lin, 2011). The selection of the kernel (linear, polynomial, radial basis function and sigmoid) and the optimization of the parameters are carried out through grid search in 10-fold cross-validation. GP regression models with Gaussian noise are a rare exception where the exact inference with likelihood functions is tractable, see §2 in (Rasmussen 1262 and Williams, 2006). Unfortunately, this is not valid for the classification task – see §3 in (Rasmussen and Williams, 2006) – where an approximation me</context>
<context position="19012" citStr="Rasmussen and Williams, 2006" startWordPosition="3066" endWordPosition="3069">t the performance of SVMs. For this reason, we also test feature selection prior to the SVM training. For that we used Randomized Lasso, or stability selection (Meinshausen and B¨uhlmann, 2010). Re-sampling of the training data is performed several times and a Lasso regression model is fit on each sample. Features that appear in a given number of samples are retained. Both the fraction of the data to be sampled and the threshold to select the features can be configured. In our 1linone and linard in the result tables, respectively. 2More detailed information on the available kernels are in §4 (Rasmussen and Williams, 2006) 3http://www.gaussianprocess.org/gpml/ code/matlab/doc/ experiments we set the sampling fraction to 75%, the selection threshold to 25% and the number of resamples to 1,000. We refer to these as SVMfs. 6 Gold Standards To assess how well prior polarity formulae perform, a gold standard with word polarities provided by human annotators is needed. There are many such resources in the literature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Tab</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian processes for machine learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel methods for pattern analysis. Cambridge university press.</title>
<date>2004</date>
<contexts>
<context position="14873" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2406" endWordPosition="2409">tages of approaches using SWN: rnd. This formula represents the basic baseline random approach. It simply returns a random number between -1 and 1 for any given lemma#PoS. swnrnd. This formula represents an advanced random approach that incorporates some “knowledge” from SWN. It takes the scores of a random sense for the given lemma#PoS. We believe this is a fairer baseline than rnd since SWN information can possibly constrain the values. A similar approach has been used in (Qu et al., 2008). 5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs) (Rasmussen and Williams, 2006), to test the performance of all the metrics in conjunction. SVMs are non-parametric deterministic algorithms that have been widely used in several fields, in particular in NLP where they are the state-of-the-art for various tasks. GPs, on the other hand, are an extremely flexible non-parametric probabilistic framework able to explicitly model uncertainty, that, despite being considered state-of-the-art in regression, have rarely been used in NLP. To our knowledge only two previous works did so (Polajnar et al., 2011; Cohn and Specia,</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel methods for pattern analysis. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Sing</author>
<author>S Sarkar</author>
<author>T K Mitra</author>
</authors>
<title>Development of a novel algorithm for sentiment analysis based on adverb-adjective-noun combinations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd National Conference on Emerging Trends and Applications in Computer Science (NCETACS ’12),</booktitle>
<pages>38--40</pages>
<location>Shillong, India.</location>
<contexts>
<context position="11581" citStr="Sing et al., 2012" startWordPosition="1860" endWordPosition="1863">be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according to the weight. uniw. Like uni but without the weighting sys</context>
</contexts>
<marker>Sing, Sarkar, Mitra, 2012</marker>
<rawString>J.K. Sing, S. Sarkar, and T.K. Mitra. 2012. Development of a novel algorithm for sentiment analysis based on adverb-adjective-noun combinations. In Proceedings of the 3rd National Conference on Emerging Trends and Applications in Computer Science (NCETACS ’12), pages 38–40, Shillong, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>D C Dunphy</author>
<author>M S Smith</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="19880" citStr="Stone et al., 1966" startWordPosition="3203" endWordPosition="3206">ity formulae perform, a gold standard with word polarities provided by human annotators is needed. There are many such resources in the literature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity validation task. In the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer (GI) for the classification ones. 6.1 ANEW ANEW (Bradley and Lang, 1999) is a reso</context>
<context position="21422" citStr="Stone et al., 1966" startWordPosition="3461" endWordPosition="3464">rds were presented to subjects in isolation (i.e. no context was provided) this resource represents a human validation of prior polarities scores for the given words, and can be used as a gold standard. For each word ANEW provides two main metrics: anewµ, which correspond to the average of annotators votes, and anew,, which gives the variance in annotators scores for the given word. In the same way these metrics are also provided for 1263 the male/female annotator groups. 6.2 General Inquirer The Harvard General Inquirer dictionary is a widely used resource, built for automatic text analysis (Stone et al., 1966). Its latest revision4 contains 11789 words, tagged with 182 semantic and pragmatic labels, as well as with their part of speech. Words and their categories were initially taken from the Harvard IV-4 Psychosociological Dictionary (Dunphy et al., 1974) and the Lasswell Value Dictionary (Lasswell and Namenwirth, 1969). For this paper we consider the Positiv and Negativ categories (1,915 words the former, 2,291 words the latter, for a total of 4,206 affective words). 7 Experiments In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the </context>
</contexts>
<marker>Stone, Dunphy, Smith, 1966</marker>
<rawString>P.J. Stone, D.C. Dunphy, and M.S. Smith. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNet-Affect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC ’04),</booktitle>
<pages>1083--1086</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="20154" citStr="Strapparava and Valitutti, 2004" startWordPosition="3241" endWordPosition="3244">e of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity validation task. In the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer (GI) for the classification ones. 6.1 ANEW ANEW (Bradley and Lang, 1999) is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1 thousand) in the English language. It contains a set of words that have been rated in terms of pleasure (affective valence), arousal, and dominance. In particular for our t</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC ’04), pages 1083 – 1086, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>J Brooke</author>
<author>M Tofiloski</author>
<author>K Voll</author>
<author>M Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<pages>307</pages>
<contexts>
<context position="19630" citStr="Taboada et al., 2011" startWordPosition="3163" endWordPosition="3166">06) 3http://www.gaussianprocess.org/gpml/ code/matlab/doc/ experiments we set the sampling fraction to 75%, the selection threshold to 25% and the number of resamples to 1,000. We refer to these as SVMfs. 6 Gold Standards To assess how well prior polarity formulae perform, a gold standard with word polarities provided by human annotators is needed. There are many such resources in the literature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity val</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics, 37(2):267– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T T Thet</author>
<author>J C Na</author>
<author>C S G Khoo</author>
<author>S Shakthikumar</author>
</authors>
<title>Sentiment analysis of movie reviews on discussion boards using a linguistic approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion (TSA ’09),</booktitle>
<pages>81--84</pages>
<location>Hong Kong.</location>
<contexts>
<context position="11522" citStr="Thet et al., 2009" startWordPosition="1850" endWordPosition="1853">375,0.0&gt;. Then, the final polarity strength returned will be either fm or fd. The formulae (f) we tested are the following: fs. In this formula only the first (and thus most frequent) sense is considered for the given lemma#PoS. This is equivalent to considering only the SWN score for lemma#PoS#1. Based on (Neviarouskaya et al., 2009; Agrawal and Siddiqui, 2009; Guerini et al., 2008; Chowdhury et al., 2013), this is the most basic form of prior polarities. mean. It calculates the mean of the positive and negative scores for all the senses of the given lemma#PoS. This formula has been used in (Thet et al., 2009; Denecke, 2009; Devitt and Ahmad, 2007; Sing et al., 2012). uni. Based on (Neviarouskaya et al., 2009), it considers only those senses that have a Pos(s) greater than or equal to the corresponding Neg(s), and greater than 0 (the stronglyPos set). In case posScore is equal to negScore, the one with the highest weight is returned, where weights are defined as the cardinality of stronglyPos divided by the total number of senses. The same applies for the negative senses. This is the only method, together with rnd, for which we cannot apply fd, as it returns a positive or negative score according </context>
</contexts>
<marker>Thet, Na, Khoo, Shakthikumar, 2009</marker>
<rawString>T.T. Thet, J.C. Na, C.S.G. Khoo, and S. Shakthikumar. 2009. Sentiment analysis of movie reviews on discussion boards using a linguistic approach. In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion (TSA ’09), pages 81–84, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>C D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12),</booktitle>
<pages>90--94</pages>
<location>Jeju Island,</location>
<contexts>
<context position="7164" citStr="Wang and Manning, 2012" startWordPosition="1121" endWordPosition="1124">e problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities – e.g. using word bigram features (Wang and Manning, 2012) – are better for sentiment analysis tasks, there are tasks that are intrinsically based on the notion of words’ prior polarity. Consider copywriting, where evocative names are a key element to a successful product ( ¨Ozbal and Strapparava, 2012; ¨Ozbal et al., 2012). In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUV for the Spanish market, since the original name Pajero had a very negative prior polarity, a</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>S. Wang and C.D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12), pages 90–94, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Warriner</author>
<author>V Kuperman</author>
<author>M Brysbaert</author>
</authors>
<title>Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior research methods,</title>
<date>2013</date>
<pages>1--17</pages>
<contexts>
<context position="34888" citStr="Warriner et al. (2013)" startWordPosition="5786" endWordPosition="5789">fs, both differences are significant p &lt; 0.001). Instead, in Table 8 – which displays the results along gender and polarity dimensions – there is no statistically significant difference in MAE on positive words between male and female, while there is a strong statistical significance for negative words (p &lt; 0.001). Interestingly, there is also a large difference between positive and negative affective words (both for male and female dimensions). This difference is maximum for male scores on positive words compared to female scores on negative words (0.283 vs. 0.399, p &lt; 0.001). Recent work by Warriner et al. (2013) inspected the differences in prior polarity assessment due to gender. At this stage we can only note that prior polarities calculated with SWN are closer to ANEW male annotations than female ones. Understanding why this happens would require an accurate examination of the methods used to create WordNet and SWN (which will be the focus of our future work). Male female MAE p MAE v MAE p MAE v SVMfs 0.292 0.020 0.369 0.008 best f 0.323 0.022 0.392 0.010 Table 7: MAE results for Male vs Female using SWN3 Male female MAE p MAE v MAE p MAE v Pos 0.283 0.022 0.340 0.009 Neg 0.301 0.029 0.399 0.013 T</context>
</contexts>
<marker>Warriner, Kuperman, Brysbaert, 2013</marker>
<rawString>A.B. Warriner, V. Kuperman, and M. Brysbaert. 2013. Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior research methods, pages 1– 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>S Mukherjee</author>
<author>O Chapelle</author>
<author>M Pontil</author>
<author>T Poggio</author>
<author>V Vapnik</author>
</authors>
<title>Feature selection for SVMs.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th Conference on Neural Information Processing Systems (NIPS ’00),</booktitle>
<pages>668--674</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="18039" citStr="Weston et al., 2000" startWordPosition="2903" endWordPosition="2906"> models were implemented using the GPML Matlab toolbox3. Unlike SVMs, the optimization of the kernel parameters can be performed without using grid search, but the optimal parameters can be obtained iteratively, by maximizing the marginal likelihood (or in classification, the Laplace approximation of the marginal likelihood). We fix at 100 the maximum number of iterations. An interesting property of the GPs is their capability of weighting the features differently according to their importance in the data. This is referred to as the automatic variance determination kernel. As demonstrated in (Weston et al., 2000), SVMs can benefit from the application of feature selection techniques especially when there are highly redundant features. Since the prior polarities formulae tend to cluster in groups that provide similar results (Gatti and Guerini, 2012) – creating noise for the learner – we want to understand whether feature selection approaches can boost the performance of SVMs. For this reason, we also test feature selection prior to the SVM training. For that we used Randomized Lasso, or stability selection (Meinshausen and B¨uhlmann, 2010). Re-sampling of the training data is performed several times a</context>
</contexts>
<marker>Weston, Mukherjee, Chapelle, Pontil, Poggio, Vapnik, 2000</marker>
<rawString>J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. 2000. Feature selection for SVMs. In Proceedings of the 14th Conference on Neural Information Processing Systems (NIPS ’00), pages 668– 674, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K I Williams</author>
<author>D Barber</author>
</authors>
<title>Bayesian classification with gaussian processes.</title>
<date>1998</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context position="16960" citStr="Williams and Barber, 1998" startWordPosition="2739" endWordPosition="2743">hang and Lin, 2011). The selection of the kernel (linear, polynomial, radial basis function and sigmoid) and the optimization of the parameters are carried out through grid search in 10-fold cross-validation. GP regression models with Gaussian noise are a rare exception where the exact inference with likelihood functions is tractable, see §2 in (Rasmussen 1262 and Williams, 2006). Unfortunately, this is not valid for the classification task – see §3 in (Rasmussen and Williams, 2006) – where an approximation method is required. In this work, we use the Laplace approximation method proposed in (Williams and Barber, 1998). Different kernels are tested (covariance for constant functions, linear with and without automatic relevance determination (ARD)1, Matern, neural network, etc.2) and the linear logistic (lll) and probit regression (prl) likelihood functions are evaluated in classification. In our classification experiments we tried all possible combinations of kernels and likelihood functions, while in the regression tests we ranged only on different kernels. All the GP models were implemented using the GPML Matlab toolbox3. Unlike SVMs, the optimization of the kernel parameters can be performed without usin</context>
</contexts>
<marker>Williams, Barber, 1998</marker>
<rawString>C.K.I. Williams and D. Barber. 1998. Bayesian classification with gaussian processes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(12):1342–1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI ’04),</booktitle>
<pages>761--769</pages>
<location>San Jose, CA, USA.</location>
<contexts>
<context position="6610" citStr="Wilson et al., 2004" startWordPosition="1033" endWordPosition="1036">n. The regression task is harder than binary classification, since we want to assess not only that pretty, beautiful and gorgeous are positive words, but also to define a partial or total order so that gorgeous is more positive than beautiful which, in turn, is more positive than pretty. This is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Guerini et al., 2008). Some of the work that addresses the problem of sentiment strength are presented in (Wilson et al., 2004; Paltoglou et al., 2010), however, their approach is modeled as a multi-class classification problem (neutral, low, medium or high sentiment) at the sentence level, rather than a regression problem at the word level. Other works such as (Neviarouskaya et al., 2011) use a fine grained classification approach too, but they consider emotion categories (anger, joy, fear, etc.), rather than sentiment strength categories. On the other hand, even if approaches that go beyond pure prior polarities – e.g. using word bigram features (Wang and Manning, 2012) – are better for sentiment analysis tasks, th</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI ’04), pages 761–769, San Jose, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP ’05),</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="20021" citStr="Wilson et al., 2005" startWordPosition="3222" endWordPosition="3225">rature, each with different coverage and annotation characteristics. ANEW (Bradley and Lang, 1999) rates the valence score of 1,034 words, which were presented in isolation to annotators. The SO-CAL entries (Taboada et al., 2011) were collected from corpus data and then manually tagged by a small number of annotators with a multi-class label. These ratings were further validated through crowdsourcing. Other resources, such as the General Inquirer lexicon (Stone et al., 1966), provide a binomial classification (either positive or negative) of sentiment-bearing words. The resource presented in (Wilson et al., 2005) uses a similar binomial annotation for single words; another interesting resource is WordNetAffect (Strapparava and Valitutti, 2004) but it labels words senses and it cannot be used for the prior polarity validation task. In the following we describe in detail the two resources we used for our experiments, namely ANEW for the regression experiments and the General Inquirer (GI) for the classification ones. 6.1 ANEW ANEW (Bradley and Lang, 1999) is a resource developed to provide a set of normative emotional ratings for a large number of words (roughly 1 thousand) in the English language. It c</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP ’05), pages 347–354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING ’00),</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="24953" citStr="Yeh, 2000" startWordPosition="4060" endWordPosition="4061">gression experiments on ANEW we used the Mean Absolute Error (MAE), that averages the error over a given test set. Accuracy was used for the classification experiments on GI instead. We opted for accuracy – rather than F1 – since for us True Negatives have same importance as True Positives. For each experiments we reported the average performance and the standard deviation over the 5 random splits. In the following sections, to check if there was a statistically significant difference in the results, we used Student’s t-test for regression experiments, while an approximate randomization test (Yeh, 2000) was used for the classification experiments. In Tables 2 and 3, the results of regression experiments over the ANEW dataset, using 5WN1 and 5WN3, are presented. The results of the classification experiments over the GI dataset, using 5WN1 and 5WN3 are shown in Tables 4 and 5. For the sake of interpretability, results are divided according to the main approaches: randoms, posterior-toprior formulae, learning algorithms. Note that for classification we report the generics f and not the fm and fd variants. In fact, both versions always return the same classification answer (we are classifying ac</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>A. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International Conference on Computational Linguistics (COLING ’00), pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000115">
<title confidence="0.912532">
Overcoming the Lack of Parallel Data in Sentence Compression
</title>
<author confidence="0.602544">
Katja Filippova and Yasemin Altun
</author>
<affiliation confidence="0.503568">
Google
</affiliation>
<address confidence="0.823371">
Brandschenkestr. 110
Z¨urich, 8004 Switzerland
</address>
<email confidence="0.997958">
katjaf|altun@google.com
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999425041666666">
A major challenge in supervised sentence
compression is making use of rich feature rep-
resentations because of very scarce parallel
data. We address this problem and present
a method to automatically build a compres-
sion corpus with hundreds of thousands of
instances on which deletion-based algorithms
can be trained. In our corpus, the syntactic
trees of the compressions are subtrees of their
uncompressed counterparts, and hence super-
vised systems which require a structural align-
ment between the input and output can be suc-
cessfully trained. We also extend an exist-
ing unsupervised compression method with a
learning module. The new system uses struc-
tured prediction to learn from lexical, syntac-
tic and other features. An evaluation with hu-
man raters shows that the presented data har-
vesting method indeed produces a parallel cor-
pus of high quality. Also, the supervised sys-
tem trained on this corpus gets high scores
both from human raters and in an automatic
evaluation setting, significantly outperforming
a strong baseline.
</bodyText>
<sectionHeader confidence="0.990888" genericHeader="keywords">
1 Introduction and related work
</sectionHeader>
<bodyText confidence="0.999893325">
Sentence compression is a paraphrasing task where
the goal is to generate sentences shorter than given
while preserving the essential content. A robust
compression system would be useful for mobile de-
vices as well as a module in an extractive sum-
marization system (Mani, 2001). Although a com-
pression may differ lexically and structurally from
the source sentence, to date most systems are ex-
tractive and proceed by deleting words from the
input (Knight &amp; Marcu, 2000; Dorr et al., 2003;
Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008;
Berg-Kirkpatrick et al., 2011, inter alia). To de-
cide which words, dependencies or phrases can be
dropped, (i) rule-based approaches (Grefenstette,
1998; Jing &amp; McKeown, 2000; Dorr et al., 2003;
Zajic et al., 2007), (ii) supervised models trained
on parallel data (Knight &amp; Marcu, 2000; Turner &amp;
Charniak, 2005; McDonald, 2006; Gillick &amp; Favre,
2009; Galanis &amp; Androutsopoulos, 2010, inter alia)
and (iii) unsupervised methods which make use of
statistics collected from non-parallel data (Hori &amp;
Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata,
2008; Filippova &amp; Strube, 2008) have been investi-
gated. Since it is infeasible to manually devise a set
of accurate deletion rules with high coverage, recent
research has been devoted to developing statistical
methods and possibly augmenting them with a few
linguistic rules to improve output readability (Clarke
&amp; Lapata, 2008; Nomoto, 2009).
Supervised models. A major problem for super-
vised deletion-based systems is very limited amount
of parallel data. Many approaches make use of a
small portion of the Ziff-Davis corpus which has
about 1K sentence-compression pairs1. Other main
sources of training data are the two manually crafted
compression corpora from the University of Edin-
burgh (“written” and “spoken”, each approx. 1.4K
pairs). Galanis &amp; Androutsopoulos (2011) attempt
at getting more parallel data by applying a deletion-
based compressor together with an automatic para-
</bodyText>
<footnote confidence="0.9923605">
1The method of Galley &amp; McKeown (2007) could benefit
from a larger number of sentences.
</footnote>
<page confidence="0.859776">
1481
</page>
<note confidence="0.740051">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994324137931">
phraser and generating multiple alternative com-
pressions. To our knowledge, this extended data set
has not yet been used for successful training of com-
pression systems.
Scarce parallel data makes it hard to go beyond a
small set of features and explore lexicalization. For
example, Knight &amp; Marcu (2000) only induce non-
lexicalized CFG rules, many of which occurred only
once in the training data. The features of McDon-
ald (2006) are formulated exclusively in terms of
syntactic categories. Berg-Kirkpatrick et al. (2011)
have as few as 13 features to decide whether a con-
stituent can be dropped. Galanis &amp; Androutsopou-
los (2010) use many features when deciding which
branches of the input dependency tree can be pruned
but require a reranker to select most fluent com-
pressions from a pool of candidates generated in the
pruning phase, many of which are ungrammatical.
Even further data limitations exist for the algo-
rithms which operate on syntactic trees and refor-
mulate the compression task as a tree pruning one
(Nomoto, 2008; Filippova &amp; Strube, 2008; Cohn &amp;
Lapata, 2009; Galanis &amp; Androutsopoulos, 2010, in-
ter alia). These methods are sensitive to alignment
errors, their performance degrades if the syntactic
structure of the compression is very different from
that of the input. For example, see Nomoto’s 2009
analysis of the poor performance of the T3 system of
Cohn &amp; Lapata (2009) when retrained on a corpus of
loosely similar RSS feeds and news.
Unsupervised models. Few approaches require
no training data at all. The model of Hori &amp; Fu-
rui (2004) combines scores estimated from mono-
lingual corpora to generate compressions of tran-
scribed speech. Adopting an integer linear program-
ming (ILP) framework, Clarke &amp; Lapata (2008) use
hand-crafted syntactic constraints and an ngram lan-
guage model, trained on uncompressed sentences, to
find best compressions. The model of Filippova &amp;
Strube (2008) also uses ILP but the problem is for-
mulated over dependencies and not ngrams. Condi-
tional probabilities and word counts collected from
a large treebank are combined in an ad hoc man-
ner to assess grammatical importance and informa-
tiveness of dependencies. Similarly, Woodsend &amp;
Lapata (2010) formulate an ILP problem to gener-
ate news story highlights using precomputed scores.
Again, an ad hoc combination of the scores learned
independently of the task is used in the objective
function.
Contributions of this paper. Our work is moti-
vated by the obvious need for a large parallel corpus
of sentences and compressions on which extractive
systems can be trained. Furthermore, we want the
compressions in the corpus to be structurally very
close to the input. Ideally, in every pair, the com-
pression should correspond to a subtree of the input.
To this end, our contributions are three-fold:
</bodyText>
<listItem confidence="0.8918516">
• We describe an automatic procedure of con-
structing a parallel corpus of 250,000 sentence-
compression pairs such that the dependency
tree of the compression is a subtree of the
source tree. An evaluation with human raters
demonstrates high quality of the parallel data
in terms of readability and informativeness.
• We successfully apply the acquired data to train
a novel supervised compression system which
produces readable and informative compres-
sions without employing a separate reranker.
In particular, we start with the unsupervised
method of Filippova &amp; Strube (2008) and re-
place the ad hoc edge weighting with a lin-
ear function over a rich feature representation.
The parameter vector is learned from our cor-
pus specifically for the compression task us-
ing structured prediction (Collins, 2002). The
new system significantly outperforms the base-
line and hence provides further evidence for the
utility of the parallel data.
• We demonstrate that sparse lexical features are
very useful for sentence compression, and that
a large parallel corpus is a requirement for ap-
plying them successfully.
</listItem>
<bodyText confidence="0.999927111111111">
The compression framework we adopt and the un-
supervised baseline are introduced in Section 2, the
training algorithm for learning edge weights from
parallel data is described in Section 3. In Section
4 we explain how to obtain the data and present an
evaluation of its quality. In Section 5 we compare
the baseline with our system and report the results
of an experiment with humans as well as the results
of an automatic evaluation.
</bodyText>
<page confidence="0.996728">
1482
</page>
<sectionHeader confidence="0.953801" genericHeader="introduction">
2 Framework and baseline
</sectionHeader>
<bodyText confidence="0.999258722222222">
We adopt the unsupervised compression framework
of Filippova &amp; Strube (2008) as our baseline and ex-
tend it to a supervised structured prediction problem.
In the experiments reported by Filippova &amp; Strube
(2008), the system was evaluated on the Edinburgh
corpora. It achieved an F-score (Riezler et al., 2003)
higher than reported by other systems on the same
data under an aggressive compression rate and thus
presents a competitive baseline.
Tree pruning as optimization. In this framework,
compressions are obtained by deleting edges of the
source dependency structure so that (1) the retained
edges form a valid syntactic tree, and (2) their to-
tal edge weight is maximized. The objective func-
tion is defined over set X = {xe7 e E E} of bi-
nary variables, corresponding to the set E of the
source edges, subject to the structural and length
constraints,
</bodyText>
<equation confidence="0.99716">
f(X) =1: xe x w(e) (1)
eEE
</equation>
<bodyText confidence="0.999768">
Here, w(e) denotes the weight of edge e. This con-
strained optimization problem is solved under the
tree structure and length constraints using ILP. If xe
is resolved to 1, the respective edge is retained, oth-
erwise it is deleted. The tree structure constraints en-
force at most one parent for every node and structure
connectivity (i.e., no disconnected subtrees). Given
that length(node(e)) denotes the length of the node
to which edge e points and a is the maximum per-
mitted length for the compression, the length con-
straint is simply
</bodyText>
<equation confidence="0.9961945">
1: xe x length(node(e)) G a (2)
eEE
</equation>
<bodyText confidence="0.997767311111111">
Word limit is used in the original paper, whereas we
use character length which is more appropriate for
system comparisons (Napoles et al., 2011). If uni-
form weights are used in Eq. (1), the optimal so-
lution would correspond to a subtree covering as
many edges as possible while keeping the compres-
sion length under given limit.
The solution to the surface realization problem
(Belz et al., 2011) is standard: the words in the com-
pression subtree are put in the same order they are
found in the source.
Due to space limitations, we refer the reader to
(Filippova &amp; Strube, 2008) for a detailed descrip-
tion on the method. Essential for the present discus-
sion is that source dependency trees are transformed
to dependency graphs in that (1) auxiliary, deter-
miner, preposition, negation and possessive nodes
are collapsed with their heads; (2) prepositions re-
place labels on the edges to their arguments; (3) the
dummy root node is connected with every inflected
verb. Figures 1(a)-1(b) illustrate most of the trans-
formations. The transformations are deterministic
and reversible, they can be implemented in a single
top-down tree traversal2.
The set E of edges in Eq. (1) is thus the set of
edges of the transformed dependency graph, like in
Fig. 1(b). A benefit of the transformations is that
function words and negation appear in the compres-
sion if and only if their head words are present.
Hence no separate constraints are required to en-
sure that negation or a determiner is preserved. The
dummy root node makes constraint formulation eas-
ier and also allows for the generation of compres-
sions from any finite clause of the source.
The described pruning optimization framework
is used both for the unsupervised baseline and for
our supervised system. The difference between the
baseline and our system is in how edge weights,
w(e)’s in Eq. (1), are instantiated.
Baseline edge weights. The precomputed edge
weights reflect syntactic importance as well as infor-
mativeness of the nodes they point to. Given edge
e from head node h to node n, the edge weight is
the product of the syntactic and the informativeness
weights,
</bodyText>
<equation confidence="0.965459666666667">
w(e) = wsynt(e) x winfo(e) (3)
The syntactic weight is defined as
wsynt(e) = P(label(e)|lemma(h)) (4)
</equation>
<bodyText confidence="0.9992624">
For example, verb kill may have multiple argu-
ments realized with dependency labels subj, dobj, in,
etc. However, these argument labels are not equally
likely, e.g., P(subj|kill) &gt; P(in|kill). When forced
to prune an edge, the system would prefer to keep
</bodyText>
<footnote confidence="0.768846">
2Some of the transformations are comparable to what is im-
plemented in the Stanford parser (de Marneffe et al., 2006).
</footnote>
<page confidence="0.95627">
1483
</page>
<figure confidence="0.999447375">
root
ps
poss
prep pobj
nsubj
det
amod
ccomp
nsubj
auxpass
prep
pobj
det
amod
prep
pobj
amod
Britain ’s Ministry of Defense says a British soldier was killed in a roadside blast in southern Afghanistan
(a) Source dependency tree
(b) Transformed graph
(c) Tree of extracted headline A British soldier was killed in a blast in
Afghanistan
A British soldier was killed in a blast in Afghanistan
(d) Tree of extracted headline with transformations undone
</figure>
<figureCaption confidence="0.996419">
Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan
</figureCaption>
<figure confidence="0.991874769230769">
root
root
ccomp
in
amod
subj
of
amod
subj
in
amod
of Defense
British
a soldier
was killed
roadside
in a blast
southern
in Afghanistan
root
says
Britain’s Ministry
root
amod subj
in in
root
British
a soldier
was killed
in a blast
in Afghanistan
root
det
amod
subj
auxpass
prep
pobj
det prep pobj
</figure>
<bodyText confidence="0.9926235">
the subject edge over the preposition-in edge since it
contributes more weight to the objective function.
The informativeness score is inspired by Wood-
send &amp; Lapata (2012) and is defined as
</bodyText>
<equation confidence="0.974007333333333">
Pheadline(lemma(n))
winfo(e) = (5)
Particle(lemma(n))
</equation>
<bodyText confidence="0.999074166666666">
This weight tells us how likely it is that a word
from an article appears in the headline. For exam-
ple, given two edges one of which points to verb say
and another one to verb kill, the latter would be pre-
ferred over the former because kill is more “head-
liny” than say. When collecting counts for the syn-
tactic and informativeness scores, we used 9M news
articles crawled from the Internet, much more than
Filippova &amp; Strube (2008). As a result our estimates
are probably more accurate than theirs.
Although both wsynt and winfo have a meaning-
ful interpretation, there is no guarantee that product
is the best way to combine the two when assign-
ing edge weights. Also, it is unclear how to inte-
grate other signals, such as distance to the root, node
length or information about the siblings, which pre-
sumably all play a role in determining the overall
edge importance.
</bodyText>
<sectionHeader confidence="0.949196" genericHeader="method">
3 Learning edge weights
</sectionHeader>
<bodyText confidence="0.99968575">
Our supervised system differs from the unsupervised
baseline in that instead of relying on precomputed
scores, we define edge weight w(e) in Eq. (1) with a
linear function over a feature representation,
</bodyText>
<equation confidence="0.998275">
w(e) = w · f(e) (6)
</equation>
<bodyText confidence="0.99886025">
Here f(e) is a vector of binary variables for every
feature from the set of all possible but very infre-
quent features in the training set. f(e) has 1 for every
feature extracted for edge e and zero otherwise.
Table 1 gives an overview of the feature types
we use (edge e points from head h to node n).
Note that syntactic, structural and semantic features
are closed-class. For all the structural features but
char length, seven is used as maximum possible
value; all possible character lengths are bucketed
into six classes. All the features are local – for a
given edge, contextual information is included about
</bodyText>
<page confidence="0.97698">
1484
</page>
<table confidence="0.5705165">
syntactic label(e); for e* to h, label(e*); pos(h); pos(n)
structural depth(n); #children(n); #children(h); char length(n); #words in(n)
semantic NE tag(h); NE tag(n); is negated(n)
lexical lemma(n); lemma(h)-label(e); for e* to n’s siblings, lemma(h)-label(e*)
</table>
<tableCaption confidence="0.998979">
Table 1: Types of features extracted for edge e from h to n
</tableCaption>
<bodyText confidence="0.999882636363636">
the head and the target nodes, and the siblings as
well as the children of the latter. The negation fea-
ture is only applicable to verb nodes which contain
a negative particle, like not, after the tree transfor-
mations. Lexical features which combine lemmas
and syntactic labels are inspired by the unsupervised
baseline and are very sparse.
In what follows, our assumption is that we have a
compression corpus at our disposal where for every
input sentence there is a correct “oracle” compres-
sion such that its transformed parse tree matches a
subtree of the transformed input graph. Given such
a corpus, we can apply structured prediction meth-
ods to learn the parameter vector w. In our study
we employ an averaged variant of online structured
perceptron (Collins, 2002). In the context of sen-
tence fusion, a similar dependency structure prun-
ing framework and a similar learning approach was
adopted by Elsner &amp; Santhanam (2011).
At every iteration, for every input graph, we find
the optimal solution with ILP under the current pa-
rameter vector w. The maximum permitted com-
pression length is set to be the same as the length
of the oracle compression. Since the oracle com-
pression is a subtree of the input graph, it represents
a feasible solution for ILP. The parameter vector is
updated if there is a mismatch between the predicted
and the oracle sets of edges for all the features with
a non-zero net count. More formally, given an input
graph with the set of edges E, oracle compression
C C E and compression Ct C E predicted at itera-
tion t , the parameter update vector at t + 1 is given
by
</bodyText>
<equation confidence="0.97253">
�wt+1 = wt + f(e) − � f(e) (7)
eEC\Ct eECt\C
</equation>
<bodyText confidence="0.9951827">
w is averaged over all the wt’s so that features
whose weight fluctuated a lot during training are pe-
nalized (Freund &amp; Shapire, 1999).
Of course, training a model with a large number
of features, such as a lexicalized model, is only pos-
sible if there is a large compression corpus where
the dependency tree of the compression is a subtree
of the source sentence. In the next section we in-
troduce our method of getting a sufficient amount of
such data.
</bodyText>
<sectionHeader confidence="0.957155" genericHeader="method">
4 Acquiring parallel data automatically
</sectionHeader>
<bodyText confidence="0.999499931034483">
In this section we explain how we obtained a parallel
corpus of sentences and compressions. The underly-
ing idea is to harvest news articles from the Internet
where the headline appears to be similar to the first
sentence and use it to find an extractive compression
of the sentence.
Collecting headline-sentence pairs. Using a
news crawler, we collected a corpus of news arti-
cles in English from the Internet. Similarly to previ-
ous work (Dolan et al., 2004; Wubben et al., 2009;
Bejan &amp; Harabagiu, 2010, inter alia), the Google
News service3 was used to identify news. From ev-
ery article, the headline and the first sentence, which
are known to be semantically similar (Dorr et al.,
2003), were extracted. Predictably, very few head-
lines are extractive compressions of the first sen-
tence, therefore simply looking for pairs where the
headline is a subsequence of the words from the first
sentence would not solve the problem of getting a
large amount of parallel data. Importantly, headlines
are syntactically quite different from “normal” sen-
tences. For example, they may have no main verb,
omit determiners and appear incomplete, making it
hard for a supervised deletion-based system to learn
useful rules. Moreover, we observed poor parsing
accuracy for headlines which would make syntactic
annotations for headlines hardly useful.
Thus, instead of taking the headline as it is, we use
it to find a proper extractive compression of the sen-
</bodyText>
<footnote confidence="0.96325">
3http://news.google.com, Jan-Dec 2012.
</footnote>
<page confidence="0.989135">
1485
</page>
<bodyText confidence="0.996707111111111">
tence by matching lemmas of content words (nouns,
verbs, adjectives, adverbs) and coreference IDs of
entities from the headline with those of the sentence.
The exact procedure is as follows (H, S and T stand
for headline, sentence and transformed graph of the
sentence):
PREPROCESSING H and S are preprocessed in a
standard way: tokenized, lemmatized, PoS and NE
tagged. Additionally, S is parsed with a dependency
parser (Nivre, 2006) and transformed as described in
Section 2 to obtain T. Finally, pronominal anaphora
is resolved in S. Recall that S is the first sentence,
so the antecedent must be located in a preceding,
higher-level clause.
FILTERING To restrict the corpus to grammatical
and informative headlines, we implemented a cas-
cade of filters. Pair (H, S) is discarded if any of the
questions in Table 2 is answered positively.
Is H a question?
Is H or S too short? (less than four word tokens)
Is H about as long as S? (min ratio: 1.5)
Does H lack a verb?
Does H begin with a verb?
Is there a noun, verb, adj, adv lemma from H
not found in S?
Are the noun, verb, adj, adv lemmas from H
found in S in a different order?
</bodyText>
<tableCaption confidence="0.987754">
Table 2: Filters applied to candidate pair (H, S)
</tableCaption>
<bodyText confidence="0.999922459016394">
MATCHING Given the content words of H, a sub-
set of nodes in T is selected based on lemma or
coreference identity of the main (head) word in the
nodes. For example, the main word of a collapsed
node in T, which covers two words was killed, is
killed; was is its child attached with label aux in the
untransformed parse tree. This node is marked if H
contains word killed or killing because of the lemma
identity. In some cases there are multiple possible
matches. For example, given S Barack Obama said
he will attend G20 and H mentioning Obama, both
Barack Obama and he nodes are marked in T. Once
all the nodes in T which match content words and
entities from H are identified, a minimum subtree
covering these nodes is found such that every word
or entity from H occurs as many times in T as in
H. So if H mentions Obama only once, then either
Barack Obama or he must be covered by the subtree
but not both. This minimum subtree corresponds to
an extractive headline, H*, which we generate by
ordering the surface forms of all the words in the
subtree nodes by their offsets in S. Finally, the char-
acter length of H* is compared with the length of
H. If H* is much longer than H, the pair (H, S) is
discarded (max ratio 1.5).
As an illustration to the procedure, consider the
example from Figure 1 with the extracted headline
and its tree presented in Figure 1(c). Given the
headline British soldier killed in Afghanistan, the
extracted headline would be A British soldier was
killed in a blast in Afghanistan. The lemmas british,
soldier, kill, afghanistan from the headline match the
nodes British, a soldier, was killed, in Afghanistan
in the transformed graph. The node in a blast is
added because it is on the path from was killed to in
Afghanistan. Of course, it is possible to determinis-
tically undo the transformations in order to obtain a
standard dependency tree. In this case the extracted
headline would still correspond to a subtree of the
input (compare Fig. 1(d) with Fig. 1(a)). Also note
that a similar procedure can be implemented for con-
stituency parses.
The resulting corpus consists of 250K tuples (S,
T, H, H*), Appendix provides more examples of
source sentences, original headlines and extracted
headlines. We did not attempt to tune the values for
minimum/maximum length and ratio – lower thresh-
olds may have produced comparable results.
Evaluating data quality. The described proce-
dure produces a comparatively large compression
corpus but how good are automatically constructed
compressions? To answer this question, we ran-
domly selected 50 tuples from the corpus and set up
an experiment with human raters to validate and as-
sess data quality in terms of readability4 and infor-
mativeness5 which are standard measures of com-
pression quality (Clarke &amp; Lapata, 2006). Raters
were asked to read a sentence and a compression
(original H or extracted H* headline) and then rate
the compression on two five-point scales. Three rat-
ings were collected for every item. Table 3 gives
</bodyText>
<footnote confidence="0.9981005">
4Also called grammaticality and fluency.
5Also called importance and representativeness.
</footnote>
<page confidence="0.99391">
1486
</page>
<figure confidence="0.954575">
AVG read AVG info
ORIG. HEADLINE
4.66† 4.10†$
OUR SYSTEM
UNSUP. SYSTEM
4.30† 3.52†
3.70 2.70
average ratings with standard deviation.
</figure>
<table confidence="0.948691666666667">
AVG read AVG info
ORIG. HEADLINE 4.36 (0.75) 3.86 (0.79)
EXTR. HEADLINE 4.26 (1.01) 3.70 (1.04)
</table>
<tableCaption confidence="0.998904">
Table 3: Results for two kinds of headlines
</tableCaption>
<bodyText confidence="0.99996">
In terms of readability and informativeness the
extracted headlines are comparable with human-
written ones: at 95% confidence there is no statis-
tically significant difference between the two.
Encouraged by the results of the validation exper-
iment we proceeded to our next question: Can a su-
pervised compression system be successfully trained
on this corpus?
</bodyText>
<sectionHeader confidence="0.925945" genericHeader="method">
5 System evaluation and discussion
</sectionHeader>
<bodyText confidence="0.999981888888889">
From the corpus of 250K tuples we used 100K to
get pairs of extracted headlines and sentences for
training (on the development set we did not observe
much improvement from using more training data),
250 for development and the rest for testing. We
ran the learning algorithm for 20 iterations, checking
the performance on the development set. Features
which applied to less than 20 edges were pruned,
the size of the feature set is about 28K.
</bodyText>
<subsectionHeader confidence="0.998232">
5.1 Evaluation with humans
</subsectionHeader>
<bodyText confidence="0.962391125">
50 pairs of original headlines and sentences (differ-
ent from the data validation set in Sec. 4) were ran-
domly selected for an evaluation with humans from
the test data. As in the data quality validation ex-
periment, we asked raters to assess the readability
and informativeness of proposed compressions for
the unsupervised system, our system and human-
written headlines. The latter provide us with upper
bounds on the evaluation criteria. Three ratings per
item per parameter were collected. To get compara-
ble results, the unsupervised and our systems used
the same compression rate: for both, the requested
maximum length was set to the length of the head-
line. Table 4 summarizes the results.
The results indicate that the trained model signifi-
cantly outperforms the unsupervised system, getting
particularly good marks for readability. The differ-
ence in readability between our system and original
headlines is not statistically significant. Note that
Table 4: Results for the systems and original headline: †
and I stand for significantly better than Unsupervised and
Our system at 95% confidence, respectively
the unsupervised baseline is also capable of generat-
ing readable compressions but does a much poorer
job in selecting most important information. Our
trained model successfully learned to optimize both
scores. We refer the reader to Appendix for input
and compression examples. Note that the ratings for
the human-written headlines in this experiment are
slightly different from the ratings in the data valida-
tion experiment because a different data sample was
used.
</bodyText>
<subsectionHeader confidence="0.999816">
5.2 Automatic evaluation
</subsectionHeader>
<bodyText confidence="0.9999212">
Our automatic evaluation had the goal of explic-
itly addressing two relevant questions related to our
claims about (1) the benefits of having a large paral-
lel corpus and (2) employing a supervised approach
with a rich feature representation.
</bodyText>
<listItem confidence="0.68675485">
1. Our primary motivation for collecting parallel
data has been that having access to sparse lex-
ical features, which considerably increase the
feature space, would benefit compression sys-
tems. But is it really the case for sentence com-
pression? Can a comparable performance be
achieved with a closed, moderately sized set of
dense, non-lexical features? If yes, then a large
compression corpus is probably not needed.
Furthermore, to demonstrate that a large corpus
is not only sufficient but also necessary to learn
weights for thousands of features, we need to
compare the performance of the system when
trained on the full data set and a small portion
of it.
2. The syntactic and informativeness scores in Eq.
(3) were calculated over millions of news arti-
cles and do provide us with meaninful statis-
tics (see Sec. 2). Is there any benefit in re-
placing those scores with weights learned for
</listItem>
<page confidence="0.992562">
1487
</page>
<bodyText confidence="0.998174033333333">
their feature counterparts? Recall that one of
our feature types in Table 1 is the concate-
nation of lemma(h) (parent lemma) and la-
bel(e) which relies on the same information
as wsynt = P(label(e)|lemma(h)). The fea-
ture counterpart of winfo defined in Eq. (5) is
lemma(n)–the lemma of the node to which edge
points. How would the supervised system per-
form against the unsupervised one, if it only ex-
tracted features of these two types?
To answer these questions, we sampled 1,000 tu-
ples from the unused test data and measured F1
score (Riezler et al., 2003) by comparing the trees
of the generated compression and the “correct”, ex-
tracted headline. The systems we compared are the
unsupervised baseline (UNSUP. SYSTEM) and the
supervised model trained on three kinds of feature
sets: (1) SYNT-INFO FEATURES, corresponding to
the supervised training of the unsupervised base-
line model (i.e., lemma(h)-label(e) and lemma(n));
(2) NON-LEX FEATURES, corresponding to a dense,
non-lexical feature representation (i.e., all the fea-
ture types from Table 1 excluding the three involv-
ing lemmas); (3) ALL FEATURES (same as OUR
SYSTEM). Additionally, we trained the system on
10% of the data–10K as opposed to 100K tuples,
ALL FEATURES (10K)–for 20 iterations ignoring
features which applied to less than three edges6. As
before, the same compression rate was used for all
the systems. The results are summarized in Table 5.
</bodyText>
<tableCaption confidence="0.9801905">
Table 5: Results for the unsupervised baseline and the
supervised system trained on three kinds of feature sets
</tableCaption>
<bodyText confidence="0.9994815">
Clearly, having more features, lexicalized and un-
lexicalized, is important: there is a significant im-
</bodyText>
<footnote confidence="0.6670462">
6Recall from the beginning of the section that for the full
(100K) training set the threshold was set to 20 with no tuning.
For the 10K training set, we tried values of two, three, five and
varied the number of iterations. The result we report is the high-
est we could get for 10K.
</footnote>
<bodyText confidence="0.999968052631579">
provement in going beyond the closed set of 330
non-lexical features to all, from 79.6 to 84.3 points.
Moreover, successful training requires a large cor-
pus since the performance of the system degrades if
only 10K training instances are used. Note that this
number already exceeds all the existing compression
corpora taken together. Hence, sparse lexical fea-
tures are useful for compression and a large paral-
lel corpus is a requirement for successful supervised
training.
Concerning our second question, learning feature
weights from the data produces significantly better
results than the hand-crafted way of making use of
the same information, even if a much larger data
set is used to collect statistics. We observed a dra-
matic increase from 52.3 to 75.0 points. Thus, we
may conclude that training with dense and sparse
features directly from data definitely improves the
performance of the dependency pruning system.
</bodyText>
<subsectionHeader confidence="0.991728">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999986">
It is important to note that the data we used is chal-
lenging: first sentences in news articles tend to be
long, in fact longer than other news sentences, which
implies less reliable syntactic analysis and noisier
input to the syntax-based systems. In the test set
we used for the evaluation with humans, the mean
sentence length is 165 characters. The average com-
pression rate in characters is 0.46 f 0.16 which is
quite aggressive7. Recall that we used the very same
framework for the unsupervised baseline and our
system as well as the same compression rate. All the
preprocessing errors affect both systems equally and
the comparison of the two is fair. Predictably, wrong
syntactic parses significantly increase chances of an
ungrammatical compression, and parser errors seem
to be a major source of readability deficiencies.
A property of the described compression frame-
work is that a desired compression length is ex-
pected to be provided by the user. This can be seen
both as a strength and as a weakness, depending on
the application. In a scenario where mobile devices
with a limited screen size are used, or in a summa-
rization scenario where a total summary length is
provided (see the DUC/TAC guidelines8), being able
</bodyText>
<footnote confidence="0.985928">
7We follow the standard terminology where smaller values
imply shorter compressions.
8http://www.nist.gov/tac/
</footnote>
<figure confidence="0.920491909090909">
F1 score #features
UNSUP. SYSTEM
52.3 N.A.
SYNT-INFO FEATURES
NON-LEX FEATURES
ALL FEATURES
ALL FEATURES (10K)
75.0 12,490
79.6 330
84.3 27,813
81.4 22,529
</figure>
<page confidence="0.980704">
1488
</page>
<bodyText confidence="0.99995325">
to specify a length is definitely an advantage. How-
ever, one can also think of other applications where
the user does not have a strict length constraint but
wants the text to be somewhat shorter. In this case,
a reranker which compares compressions generated
for a range of possible lengths can be employed to
find a single compression (e.g., mean edge weight in
the solution or a language model-based score).
</bodyText>
<sectionHeader confidence="0.999529" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999968769230769">
We have addressed a major problem for supervised
extractive compression models – the lack of a large
parallel corpus. To this end, we presented a method
to automatically build such a corpus from web doc-
uments available on the Internet. An evaluation
with humans demonstrates that the quality of the
corpus is high – the compressions are grammati-
cal and informative. We also significantly improved
a competitive unsupervised method achieving high
readability and informativeness scores by incorpo-
rating thousands of features and learning the feature
weights from our corpus. This result further con-
firms the practical utility of the automatically ob-
tained data. We have shown that employing lexi-
cal features is important for sentence compression,
and that our supervised module can successfully
learn their weights from the corpus. To our knowl-
edge, we are the first to empirically demonstrate that
sparse features are useful for compression and that a
large parallel corpus is a requirement for a success-
ful learning of their weights. We believe that other
supervised deletion-based systems can benefit from
our work.
Acknowledgements: The authors are thankful to
the EMNLP reviewers for their feedback and sug-
gestions.
</bodyText>
<sectionHeader confidence="0.992334" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.9999205">
The appendix presents examples of source sentences
(S), original headlines (H), extracted headlines (H*),
unsupervised baseline (U) and our system (O) com-
pressions.
</bodyText>
<sectionHeader confidence="0.967973" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.883045675">
Bejan, C. &amp; S. Harabagiu (2010). Unsupervised
event coreference resolution with rich linguistic
features. In Proc. ofACL-10, pp. 1412–1422.
Belz, A., M. White, D. Espinosa, E. Kow, D. Hogan
&amp; A. Stent (2011). The first surface realization
shared task: Overview and evaluation results. In
Proc. ofENLG-11, pp. 217–226.
Berg-Kirkpatrick, T., D. Gillick &amp; D. Klein (2011).
Jointly learning to extract and compress. In Proc.
of ACL-11.
Clarke, J. &amp; M. Lapata (2006). Models for sen-
tence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proc. of COLING-ACL-06, pp. 377–385.
Clarke, J. &amp; M. Lapata (2008). Global inference
for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelli-
gence Research, 31:399–429.
Cohn, T. &amp; M. Lapata (2009). Sentence compres-
sion as tree transduction. Journal of Artificial In-
telligence Research, 34:637–674.
Collins, M. (2002). Discriminative training methods
for Hidden Markov Models: Theory and exper-
iments with perceptron algorithms. In Proc. of
EMNLP-02, pp. 1–8.
de Marneffe, M.-C., B. MacCartney &amp; C. D. Man-
ning (2006). Generating typed dependency parses
from phrase structure parses. In Proc. of LREC-
06, pp. 449–454.
Dolan, B., C. Quirk &amp; C. Brokett (2004). Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference
on Computational Linguistics, Geneva, Switzer-
land, 23–27 August 2004, pp. 350–356.
Dorr, B., D. Zajic &amp; R. Schwartz (2003). Hedge
trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the Text Summa-
rization Workshop at HLT-NAACL-03, Edmonton,
Alberta, Canada, 2003, pp. 1–8.
</bodyText>
<page confidence="0.990638">
1489
</page>
<table confidence="0.902999355932203">
S Country star Sara Evans has married former University of Alabama quarterback Jay Barker.
H Country star Sara Evans marries
H* Country star Sara Evans has married
U Sara Evans has married Jay Barker
O Sara Evans has married Jay Barker
S Intel would be building car batteries, expanding its business beyond its core strength, the company said in a statement
H Intel to build car batteries
H* Intel would be building car batteries
U would be building the company said
O Intel would be building car batteries
S A New Orleans Saints team spokesman says tight end Jeremy Shockey was taken to a hospital but is doing fine.
H Spokesman: Shockey taken to hospital, doing fine
H* spokesman says Jeremy Shockey was taken to a hospital but is doing fine
U A New Orleans Saints team spokesman says Jeremy Shockey was taken
O tight end Jeremy Shockey was taken to a hospital but is doing fine
S President Obama declared a major disaster exists in the State of Florida and ordered Federal aid to supplement
H State and local recovery efforts in the area struck by severe storms, flooding, tornadoes, and straight-line winds
H* beginning on May 17, 2009, and continuing.
U President Obama declares major disaster exists in the State of Florida
O President Obama declared a major disaster exists in the State of Florida
President Obama declared a major disaster exists and ordered Federal aid
President Obama declared a major disaster exists in the State of Florida
S Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amid
H mounting loan defaults.
H* Regulators shut down small Florida bank
U Regulators shut down a small Florida bank
O shut down bringing the number of failures
Regulators shut down a small Florida bank
S Three men were arrested Wednesday night and Dayton police said their arrests are in connection to a west Dayton
H bank robbery.
H* 3 men arrested in connection with Bank robbery
U Three men were arrested are in connection to a bank robbery
O were arrested and Dayton police said their arrests are
Three men were arrested and police said their arrests are
S The government and the social partners will resume the talks on the introduction of the so-called crisis tax,
H which will be levied on all salaries, pensions and incomes over HRK 3,000.
H* Government, social partners to resume talks on introduction of “crisis” tax.
U The government and the social partners will resume the talks on the introduction of the crisis tax
O The government will resume the talks on the introduction of the crisis tax which will be levied
The government and the social partners will resume the talks on the introduction of the crisis tax
S England star David Beckham may have the chance to return to AC Milan after the Italian club’s coach said
H he was open to his move on Sunday.
H* Beckham has chance of returning to Milan
U David Beckham may have the chance to return to AC Milan
O David Beckham may have the chance to return said star was
David Beckham may have the chance to return to AC Milan
S Eastern Health and its insurance company have accepted liability for some patients involved in the breast cancer
H testing scandal, according to a statement released Friday afternoon.
H* Eastern Health accepts liability for some patients
U Eastern Health have accepted liability for some patients
O Health have accepted liability according to a statement
Eastern Health have accepted liability for some patients
S Frontier Communications Corp., a provider of phone, TV and Internet services, said Thursday
H it has started a cash tender offer to purchase up to $700 million of its notes.
H* Frontier Communications starts tender offer for up to $700 million of notes
U Frontier Communications has started a tender offer to purchase $700 million of its notes
O Frontier Communications said Thursday a provider has started a tender offer
Frontier Communications has started a tender offer to purchase $700 million of its notes
1490
</table>
<reference confidence="0.99856215">
Elsner, M. &amp; D. Santhanam (2011). Learning to fuse
disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 54–63.
Filippova, K. &amp; M. Strube (2008). Dependency tree
based sentence compression. In Proc. of INLG-
08, pp. 25–32.
Freund, Y. &amp; R. E. Shapire (1999). Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277–296.
Galanis, D. &amp; I. Androutsopoulos (2010). An ex-
tractive supervised two-stage method for sentence
compression. In Proc. of NAACL-HLT-10, pp.
885–893.
Galanis, D. &amp; I. Androutsopoulos (2011). A new
sentence compression dataset and its use in an ab-
stractive generate-and-rank sentence compressor.
In Proc. of UCNLG+Eval-11, pp. 1–11.
Galley, M. &amp; K. R. McKeown (2007). Lexicalized
Markov grammars for sentence compression. In
Proc. of NAACL-HLT-07, pp. 180–187.
Gillick, D. &amp; B. Favre (2009). A scalable global
model for summarization. In ILP for NLP-09, pp.
10–18.
Grefenstette, G. (1998). Producing intelligent tele-
graphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of
the Workshop on Intelligent Text Summarization,
Palo Alto, Cal., 23 March 1998, pp. 111–117.
Hori, C. &amp; S. Furui (2004). Speech summariza-
tion: An approach through word extraction and
a method for evaluation. IEEE Transactions on
Information and Systems, E87-D(1):15–25.
Jing, H. &amp; K. McKeown (2000). Cut and paste based
text summarization. In Proc. of NAACL-00, pp.
178–185.
Knight, K. &amp; D. Marcu (2000). Statistics-based
summarization – step one: Sentence compression.
In Proc. of AAAI-00, pp. 703–711.
Mani, I. (2001). Automatic Summarization. Amster-
dam, Philadelphia: John Benjamins.
McDonald, R. (2006). Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
EACL-06, pp. 297–304.
Napoles, C., C. Callison-Burch, J. Ganitkevitch &amp;
B. Van Durme (2011). Paraphrastic sentence com-
pression with a character-based metric: Tighten-
ing without deletion. In Proceedings of the Work-
shop on Monolingual Text-to-text Generation, Prt-
land, OR, June 24 2011, pp. 84–90.
Nivre, J. (2006). Inductive Dependency Parsing.
Springer.
Nomoto, T. (2008). A generic sentence trimmer with
CRFs. In Proc. of ACL-HLT-08, pp. 299–307.
Nomoto, T. (2009). A comparison of model free ver-
sus model intensive approaches to sentence com-
pression. In Proc. of EMNLP-09, pp. 391–399.
Riezler, S., T. H. King, R. Crouch &amp; A. Zaenen
(2003). Statistical sentence condensation using
ambiguity packing and stochastic disambiguation
methods for Lexical-Functional Grammar. In
Proc. of HLT-NAACL-03, pp. 118–125.
Turner, J. &amp; E. Charniak (2005). Supervised and
unsupervised learning for sentence compression.
In Proc. of ACL-05, pp. 290–297.
Woodsend, K. &amp; M. Lapata (2010). Automatic gen-
eration of story highlights. In Proc. of ACL-10,
pp. 565–574.
Woodsend, K. &amp; M. Lapata (2012). Multiple as-
pect summarization using Integer Linear Pro-
gramming. In Proc. of EMNLP-12, pp. 233–243.
Wubben, S., A. van den Bosch, E. Krahmer &amp;
E. Marsi (2009). Clustering and matching head-
lines for automatic paraphrase acquisition. In
Proc. of ENLG-09, pp. 122–125.
Zajic, D., B. J. Dorr, J. Lin &amp; R. Schwartz (2007).
Multi-candidate reduction: Sentence compression
as a tool for document summarization tasks. In-
formation Processing &amp; Management, Special Is-
sue on Text Summarization, 43(6):1549–1570.
</reference>
<page confidence="0.993061">
1491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265552">
<title confidence="0.973436">Overcoming the Lack of Parallel Data in Sentence Compression</title>
<author confidence="0.3358">Filippova</author>
<affiliation confidence="0.383141">Brandschenkestr.</affiliation>
<address confidence="0.895375">Z¨urich, 8004</address>
<email confidence="0.999464">katjaf|altun@google.com</email>
<abstract confidence="0.99959036">A major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data. We address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained. In our corpus, the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence supervised systems which require a structural alignment between the input and output can be successfully trained. We also extend an existing unsupervised compression method with a learning module. The new system uses structured prediction to learn from lexical, syntactic and other features. An evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality. Also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>D Santhanam</author>
</authors>
<title>Learning to fuse disparate sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-to-text Generation,</booktitle>
<volume>24</volume>
<pages>54--63</pages>
<location>Prtland, OR,</location>
<contexts>
<context position="16001" citStr="Elsner &amp; Santhanam (2011)" startWordPosition="2618" endWordPosition="2621">rvised baseline and are very sparse. In what follows, our assumption is that we have a compression corpus at our disposal where for every input sentence there is a correct “oracle” compression such that its transformed parse tree matches a subtree of the transformed input graph. Given such a corpus, we can apply structured prediction methods to learn the parameter vector w. In our study we employ an averaged variant of online structured perceptron (Collins, 2002). In the context of sentence fusion, a similar dependency structure pruning framework and a similar learning approach was adopted by Elsner &amp; Santhanam (2011). At every iteration, for every input graph, we find the optimal solution with ILP under the current parameter vector w. The maximum permitted compression length is set to be the same as the length of the oracle compression. Since the oracle compression is a subtree of the input graph, it represents a feasible solution for ILP. The parameter vector is updated if there is a mismatch between the predicted and the oracle sets of edges for all the features with a non-zero net count. More formally, given an input graph with the set of edges E, oracle compression C C E and compression Ct C E predict</context>
</contexts>
<marker>Elsner, Santhanam, 2011</marker>
<rawString>Elsner, M. &amp; D. Santhanam (2011). Learning to fuse disparate sentences. In Proceedings of the Workshop on Monolingual Text-to-text Generation, Prtland, OR, June 24 2011, pp. 54–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proc. of INLG08,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2360" citStr="Filippova &amp; Strube, 2008" startWordPosition="366" endWordPosition="369">urner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited amount of parallel data. Many approaches make use of a small portion of the Ziff-Davis corpus which has about 1K sentence-compression pairs1. Other main sources of training data are the two manually</context>
<context position="4561" citStr="Filippova &amp; Strube, 2008" startWordPosition="711" endWordPosition="714">re formulated exclusively in terms of syntactic categories. Berg-Kirkpatrick et al. (2011) have as few as 13 features to decide whether a constituent can be dropped. Galanis &amp; Androutsopoulos (2010) use many features when deciding which branches of the input dependency tree can be pruned but require a reranker to select most fluent compressions from a pool of candidates generated in the pruning phase, many of which are ungrammatical. Even further data limitations exist for the algorithms which operate on syntactic trees and reformulate the compression task as a tree pruning one (Nomoto, 2008; Filippova &amp; Strube, 2008; Cohn &amp; Lapata, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia). These methods are sensitive to alignment errors, their performance degrades if the syntactic structure of the compression is very different from that of the input. For example, see Nomoto’s 2009 analysis of the poor performance of the T3 system of Cohn &amp; Lapata (2009) when retrained on a corpus of loosely similar RSS feeds and news. Unsupervised models. Few approaches require no training data at all. The model of Hori &amp; Furui (2004) combines scores estimated from monolingual corpora to generate compressions of transcribed spe</context>
<context position="6879" citStr="Filippova &amp; Strube (2008)" startWordPosition="1082" endWordPosition="1085">. To this end, our contributions are three-fold: • We describe an automatic procedure of constructing a parallel corpus of 250,000 sentencecompression pairs such that the dependency tree of the compression is a subtree of the source tree. An evaluation with human raters demonstrates high quality of the parallel data in terms of readability and informativeness. • We successfully apply the acquired data to train a novel supervised compression system which produces readable and informative compressions without employing a separate reranker. In particular, we start with the unsupervised method of Filippova &amp; Strube (2008) and replace the ad hoc edge weighting with a linear function over a rich feature representation. The parameter vector is learned from our corpus specifically for the compression task using structured prediction (Collins, 2002). The new system significantly outperforms the baseline and hence provides further evidence for the utility of the parallel data. • We demonstrate that sparse lexical features are very useful for sentence compression, and that a large parallel corpus is a requirement for applying them successfully. The compression framework we adopt and the unsupervised baseline are intr</context>
<context position="9902" citStr="Filippova &amp; Strube, 2008" startWordPosition="1597" endWordPosition="1600"> is simply 1: xe x length(node(e)) G a (2) eEE Word limit is used in the original paper, whereas we use character length which is more appropriate for system comparisons (Napoles et al., 2011). If uniform weights are used in Eq. (1), the optimal solution would correspond to a subtree covering as many edges as possible while keeping the compression length under given limit. The solution to the surface realization problem (Belz et al., 2011) is standard: the words in the compression subtree are put in the same order they are found in the source. Due to space limitations, we refer the reader to (Filippova &amp; Strube, 2008) for a detailed description on the method. Essential for the present discussion is that source dependency trees are transformed to dependency graphs in that (1) auxiliary, determiner, preposition, negation and possessive nodes are collapsed with their heads; (2) prepositions replace labels on the edges to their arguments; (3) the dummy root node is connected with every inflected verb. Figures 1(a)-1(b) illustrate most of the transformations. The transformations are deterministic and reversible, they can be implemented in a single top-down tree traversal2. The set E of edges in Eq. (1) is thus </context>
<context position="13448" citStr="Filippova &amp; Strube (2008)" startWordPosition="2191" endWordPosition="2194">e it contributes more weight to the objective function. The informativeness score is inspired by Woodsend &amp; Lapata (2012) and is defined as Pheadline(lemma(n)) winfo(e) = (5) Particle(lemma(n)) This weight tells us how likely it is that a word from an article appears in the headline. For example, given two edges one of which points to verb say and another one to verb kill, the latter would be preferred over the former because kill is more “headliny” than say. When collecting counts for the syntactic and informativeness scores, we used 9M news articles crawled from the Internet, much more than Filippova &amp; Strube (2008). As a result our estimates are probably more accurate than theirs. Although both wsynt and winfo have a meaningful interpretation, there is no guarantee that product is the best way to combine the two when assigning edge weights. Also, it is unclear how to integrate other signals, such as distance to the root, node length or information about the siblings, which presumably all play a role in determining the overall edge importance. 3 Learning edge weights Our supervised system differs from the unsupervised baseline in that instead of relying on precomputed scores, we define edge weight w(e) i</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Filippova, K. &amp; M. Strube (2008). Dependency tree based sentence compression. In Proc. of INLG08, pp. 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Shapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="16848" citStr="Freund &amp; Shapire, 1999" startWordPosition="2779" endWordPosition="2782"> Since the oracle compression is a subtree of the input graph, it represents a feasible solution for ILP. The parameter vector is updated if there is a mismatch between the predicted and the oracle sets of edges for all the features with a non-zero net count. More formally, given an input graph with the set of edges E, oracle compression C C E and compression Ct C E predicted at iteration t , the parameter update vector at t + 1 is given by �wt+1 = wt + f(e) − � f(e) (7) eEC\Ct eECt\C w is averaged over all the wt’s so that features whose weight fluctuated a lot during training are penalized (Freund &amp; Shapire, 1999). Of course, training a model with a large number of features, such as a lexicalized model, is only possible if there is a large compression corpus where the dependency tree of the compression is a subtree of the source sentence. In the next section we introduce our method of getting a sufficient amount of such data. 4 Acquiring parallel data automatically In this section we explain how we obtained a parallel corpus of sentences and compressions. The underlying idea is to harvest news articles from the Internet where the headline appears to be similar to the first sentence and use it to find a</context>
</contexts>
<marker>Freund, Shapire, 1999</marker>
<rawString>Freund, Y. &amp; R. E. Shapire (1999). Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>I Androutsopoulos</author>
</authors>
<title>An extractive supervised two-stage method for sentence compression.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT-10,</booktitle>
<pages>885--893</pages>
<contexts>
<context position="2164" citStr="Galanis &amp; Androutsopoulos, 2010" startWordPosition="335" endWordPosition="338">a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited am</context>
<context position="4135" citStr="Galanis &amp; Androutsopoulos (2010)" startWordPosition="639" endWordPosition="643">ics phraser and generating multiple alternative compressions. To our knowledge, this extended data set has not yet been used for successful training of compression systems. Scarce parallel data makes it hard to go beyond a small set of features and explore lexicalization. For example, Knight &amp; Marcu (2000) only induce nonlexicalized CFG rules, many of which occurred only once in the training data. The features of McDonald (2006) are formulated exclusively in terms of syntactic categories. Berg-Kirkpatrick et al. (2011) have as few as 13 features to decide whether a constituent can be dropped. Galanis &amp; Androutsopoulos (2010) use many features when deciding which branches of the input dependency tree can be pruned but require a reranker to select most fluent compressions from a pool of candidates generated in the pruning phase, many of which are ungrammatical. Even further data limitations exist for the algorithms which operate on syntactic trees and reformulate the compression task as a tree pruning one (Nomoto, 2008; Filippova &amp; Strube, 2008; Cohn &amp; Lapata, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia). These methods are sensitive to alignment errors, their performance degrades if the syntactic structure of</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2010</marker>
<rawString>Galanis, D. &amp; I. Androutsopoulos (2010). An extractive supervised two-stage method for sentence compression. In Proc. of NAACL-HLT-10, pp. 885–893.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>I Androutsopoulos</author>
</authors>
<title>A new sentence compression dataset and its use in an abstractive generate-and-rank sentence compressor.</title>
<date>2011</date>
<booktitle>In Proc. of UCNLG+Eval-11,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="3105" citStr="Galanis &amp; Androutsopoulos (2011)" startWordPosition="479" endWordPosition="482">erage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited amount of parallel data. Many approaches make use of a small portion of the Ziff-Davis corpus which has about 1K sentence-compression pairs1. Other main sources of training data are the two manually crafted compression corpora from the University of Edinburgh (“written” and “spoken”, each approx. 1.4K pairs). Galanis &amp; Androutsopoulos (2011) attempt at getting more parallel data by applying a deletionbased compressor together with an automatic para1The method of Galley &amp; McKeown (2007) could benefit from a larger number of sentences. 1481 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics phraser and generating multiple alternative compressions. To our knowledge, this extended data set has not yet been used for successful training of compression systems. Scarce parallel data makes it</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2011</marker>
<rawString>Galanis, D. &amp; I. Androutsopoulos (2011). A new sentence compression dataset and its use in an abstractive generate-and-rank sentence compressor. In Proc. of UCNLG+Eval-11, pp. 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K R McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT-07,</booktitle>
<pages>180--187</pages>
<contexts>
<context position="3252" citStr="Galley &amp; McKeown (2007)" startWordPosition="503" endWordPosition="506">lity (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited amount of parallel data. Many approaches make use of a small portion of the Ziff-Davis corpus which has about 1K sentence-compression pairs1. Other main sources of training data are the two manually crafted compression corpora from the University of Edinburgh (“written” and “spoken”, each approx. 1.4K pairs). Galanis &amp; Androutsopoulos (2011) attempt at getting more parallel data by applying a deletionbased compressor together with an automatic para1The method of Galley &amp; McKeown (2007) could benefit from a larger number of sentences. 1481 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics phraser and generating multiple alternative compressions. To our knowledge, this extended data set has not yet been used for successful training of compression systems. Scarce parallel data makes it hard to go beyond a small set of features and explore lexicalization. For example, Knight &amp; Marcu (2000) only induce nonlexicalized CFG rules, man</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Galley, M. &amp; K. R. McKeown (2007). Lexicalized Markov grammars for sentence compression. In Proc. of NAACL-HLT-07, pp. 180–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
<author>B Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In ILP for NLP-09,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="2131" citStr="Gillick &amp; Favre, 2009" startWordPosition="331" endWordPosition="334">(Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Gillick, D. &amp; B. Favre (2009). A scalable global model for summarization. In ILP for NLP-09, pp. 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind.</title>
<date>1998</date>
<booktitle>In Working Notes of the Workshop on Intelligent Text Summarization,</booktitle>
<pages>111--117</pages>
<location>Palo Alto, Cal., 23</location>
<contexts>
<context position="1934" citStr="Grefenstette, 1998" startWordPosition="300" endWordPosition="301">ate sentences shorter than given while preserving the essential content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statist</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>Grefenstette, G. (1998). Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind. In Working Notes of the Workshop on Intelligent Text Summarization, Palo Alto, Cal., 23 March 1998, pp. 111–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>S Furui</author>
</authors>
<title>Speech summarization: An approach through word extraction and a method for evaluation.</title>
<date>2004</date>
<journal>IEEE Transactions on Information and Systems,</journal>
<pages>87--1</pages>
<contexts>
<context position="2290" citStr="Hori &amp; Furui, 2004" startWordPosition="354" endWordPosition="357">ords from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited amount of parallel data. Many approaches make use of a small portion of the Ziff-Davis corpus which has about 1K sentence-compre</context>
<context position="5066" citStr="Hori &amp; Furui (2004)" startWordPosition="795" endWordPosition="799">syntactic trees and reformulate the compression task as a tree pruning one (Nomoto, 2008; Filippova &amp; Strube, 2008; Cohn &amp; Lapata, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia). These methods are sensitive to alignment errors, their performance degrades if the syntactic structure of the compression is very different from that of the input. For example, see Nomoto’s 2009 analysis of the poor performance of the T3 system of Cohn &amp; Lapata (2009) when retrained on a corpus of loosely similar RSS feeds and news. Unsupervised models. Few approaches require no training data at all. The model of Hori &amp; Furui (2004) combines scores estimated from monolingual corpora to generate compressions of transcribed speech. Adopting an integer linear programming (ILP) framework, Clarke &amp; Lapata (2008) use hand-crafted syntactic constraints and an ngram language model, trained on uncompressed sentences, to find best compressions. The model of Filippova &amp; Strube (2008) also uses ILP but the problem is formulated over dependencies and not ngrams. Conditional probabilities and word counts collected from a large treebank are combined in an ad hoc manner to assess grammatical importance and informativeness of dependencie</context>
</contexts>
<marker>Hori, Furui, 2004</marker>
<rawString>Hori, C. &amp; S. Furui (2004). Speech summarization: An approach through word extraction and a method for evaluation. IEEE Transactions on Information and Systems, E87-D(1):15–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeown</author>
</authors>
<title>Cut and paste based text summarization.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL-00,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="1956" citStr="Jing &amp; McKeown, 2000" startWordPosition="302" endWordPosition="305">r than given while preserving the essential content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possi</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Jing, H. &amp; K. McKeown (2000). Cut and paste based text summarization. In Proc. of NAACL-00, pp. 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization – step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI-00,</booktitle>
<pages>703--711</pages>
<contexts>
<context position="1713" citStr="Knight &amp; Marcu, 2000" startWordPosition="264" endWordPosition="267">igh scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline. 1 Introduction and related work Sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; C</context>
<context position="3810" citStr="Knight &amp; Marcu (2000)" startWordPosition="586" endWordPosition="589">er with an automatic para1The method of Galley &amp; McKeown (2007) could benefit from a larger number of sentences. 1481 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics phraser and generating multiple alternative compressions. To our knowledge, this extended data set has not yet been used for successful training of compression systems. Scarce parallel data makes it hard to go beyond a small set of features and explore lexicalization. For example, Knight &amp; Marcu (2000) only induce nonlexicalized CFG rules, many of which occurred only once in the training data. The features of McDonald (2006) are formulated exclusively in terms of syntactic categories. Berg-Kirkpatrick et al. (2011) have as few as 13 features to decide whether a constituent can be dropped. Galanis &amp; Androutsopoulos (2010) use many features when deciding which branches of the input dependency tree can be pruned but require a reranker to select most fluent compressions from a pool of candidates generated in the pruning phase, many of which are ungrammatical. Even further data limitations exist</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Knight, K. &amp; D. Marcu (2000). Statistics-based summarization – step one: Sentence compression. In Proc. of AAAI-00, pp. 703–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<location>Amsterdam, Philadelphia: John Benjamins.</location>
<contexts>
<context position="1522" citStr="Mani, 2001" startWordPosition="234" endWordPosition="235">luation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality. Also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline. 1 Introduction and related work Sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Fa</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, I. (2001). Automatic Summarization. Amsterdam, Philadelphia: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="2108" citStr="McDonald, 2006" startWordPosition="329" endWordPosition="330">rization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem </context>
<context position="3935" citStr="McDonald (2006)" startWordPosition="609" endWordPosition="611">f the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics phraser and generating multiple alternative compressions. To our knowledge, this extended data set has not yet been used for successful training of compression systems. Scarce parallel data makes it hard to go beyond a small set of features and explore lexicalization. For example, Knight &amp; Marcu (2000) only induce nonlexicalized CFG rules, many of which occurred only once in the training data. The features of McDonald (2006) are formulated exclusively in terms of syntactic categories. Berg-Kirkpatrick et al. (2011) have as few as 13 features to decide whether a constituent can be dropped. Galanis &amp; Androutsopoulos (2010) use many features when deciding which branches of the input dependency tree can be pruned but require a reranker to select most fluent compressions from a pool of candidates generated in the pruning phase, many of which are ungrammatical. Even further data limitations exist for the algorithms which operate on syntactic trees and reformulate the compression task as a tree pruning one (Nomoto, 2008</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>McDonald, R. (2006). Discriminative sentence compression with soft syntactic evidence. In Proc. of EACL-06, pp. 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Napoles</author>
<author>C Callison-Burch</author>
<author>J Ganitkevitch</author>
<author>B Van Durme</author>
</authors>
<title>Paraphrastic sentence compression with a character-based metric: Tightening without deletion.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-to-text Generation,</booktitle>
<volume>24</volume>
<pages>84--90</pages>
<location>Prtland, OR,</location>
<marker>Napoles, Callison-Burch, Ganitkevitch, Van Durme, 2011</marker>
<rawString>Napoles, C., C. Callison-Burch, J. Ganitkevitch &amp; B. Van Durme (2011). Paraphrastic sentence compression with a character-based metric: Tightening without deletion. In Proceedings of the Workshop on Monolingual Text-to-text Generation, Prtland, OR, June 24 2011, pp. 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="19128" citStr="Nivre, 2006" startWordPosition="3155" endWordPosition="3156"> headlines hardly useful. Thus, instead of taking the headline as it is, we use it to find a proper extractive compression of the sen3http://news.google.com, Jan-Dec 2012. 1485 tence by matching lemmas of content words (nouns, verbs, adjectives, adverbs) and coreference IDs of entities from the headline with those of the sentence. The exact procedure is as follows (H, S and T stand for headline, sentence and transformed graph of the sentence): PREPROCESSING H and S are preprocessed in a standard way: tokenized, lemmatized, PoS and NE tagged. Additionally, S is parsed with a dependency parser (Nivre, 2006) and transformed as described in Section 2 to obtain T. Finally, pronominal anaphora is resolved in S. Recall that S is the first sentence, so the antecedent must be located in a preceding, higher-level clause. FILTERING To restrict the corpus to grammatical and informative headlines, we implemented a cascade of filters. Pair (H, S) is discarded if any of the questions in Table 2 is answered positively. Is H a question? Is H or S too short? (less than four word tokens) Is H about as long as S? (min ratio: 1.5) Does H lack a verb? Does H begin with a verb? Is there a noun, verb, adj, adv lemma </context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Nivre, J. (2006). Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>A generic sentence trimmer with CRFs.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT-08,</booktitle>
<pages>299--307</pages>
<contexts>
<context position="4535" citStr="Nomoto, 2008" startWordPosition="709" endWordPosition="710">onald (2006) are formulated exclusively in terms of syntactic categories. Berg-Kirkpatrick et al. (2011) have as few as 13 features to decide whether a constituent can be dropped. Galanis &amp; Androutsopoulos (2010) use many features when deciding which branches of the input dependency tree can be pruned but require a reranker to select most fluent compressions from a pool of candidates generated in the pruning phase, many of which are ungrammatical. Even further data limitations exist for the algorithms which operate on syntactic trees and reformulate the compression task as a tree pruning one (Nomoto, 2008; Filippova &amp; Strube, 2008; Cohn &amp; Lapata, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia). These methods are sensitive to alignment errors, their performance degrades if the syntactic structure of the compression is very different from that of the input. For example, see Nomoto’s 2009 analysis of the poor performance of the T3 system of Cohn &amp; Lapata (2009) when retrained on a corpus of loosely similar RSS feeds and news. Unsupervised models. Few approaches require no training data at all. The model of Hori &amp; Furui (2004) combines scores estimated from monolingual corpora to generate compr</context>
</contexts>
<marker>Nomoto, 2008</marker>
<rawString>Nomoto, T. (2008). A generic sentence trimmer with CRFs. In Proc. of ACL-HLT-08, pp. 299–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>A comparison of model free versus model intensive approaches to sentence compression.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP-09,</booktitle>
<pages>391--399</pages>
<contexts>
<context position="2671" citStr="Nomoto, 2009" startWordPosition="416" endWordPosition="417">rcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguistic rules to improve output readability (Clarke &amp; Lapata, 2008; Nomoto, 2009). Supervised models. A major problem for supervised deletion-based systems is very limited amount of parallel data. Many approaches make use of a small portion of the Ziff-Davis corpus which has about 1K sentence-compression pairs1. Other main sources of training data are the two manually crafted compression corpora from the University of Edinburgh (“written” and “spoken”, each approx. 1.4K pairs). Galanis &amp; Androutsopoulos (2011) attempt at getting more parallel data by applying a deletionbased compressor together with an automatic para1The method of Galley &amp; McKeown (2007) could benefit from</context>
</contexts>
<marker>Nomoto, 2009</marker>
<rawString>Nomoto, T. (2009). A comparison of model free versus model intensive approaches to sentence compression. In Proc. of EMNLP-09, pp. 391–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R Crouch</author>
<author>A Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL-03,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="8178" citStr="Riezler et al., 2003" startWordPosition="1296" endWordPosition="1299">allel data is described in Section 3. In Section 4 we explain how to obtain the data and present an evaluation of its quality. In Section 5 we compare the baseline with our system and report the results of an experiment with humans as well as the results of an automatic evaluation. 1482 2 Framework and baseline We adopt the unsupervised compression framework of Filippova &amp; Strube (2008) as our baseline and extend it to a supervised structured prediction problem. In the experiments reported by Filippova &amp; Strube (2008), the system was evaluated on the Edinburgh corpora. It achieved an F-score (Riezler et al., 2003) higher than reported by other systems on the same data under an aggressive compression rate and thus presents a competitive baseline. Tree pruning as optimization. In this framework, compressions are obtained by deleting edges of the source dependency structure so that (1) the retained edges form a valid syntactic tree, and (2) their total edge weight is maximized. The objective function is defined over set X = {xe7 e E E} of binary variables, corresponding to the set E of the source edges, subject to the structural and length constraints, f(X) =1: xe x w(e) (1) eEE Here, w(e) denotes the wei</context>
<context position="27410" citStr="Riezler et al., 2003" startWordPosition="4552" endWordPosition="4555"> benefit in replacing those scores with weights learned for 1487 their feature counterparts? Recall that one of our feature types in Table 1 is the concatenation of lemma(h) (parent lemma) and label(e) which relies on the same information as wsynt = P(label(e)|lemma(h)). The feature counterpart of winfo defined in Eq. (5) is lemma(n)–the lemma of the node to which edge points. How would the supervised system perform against the unsupervised one, if it only extracted features of these two types? To answer these questions, we sampled 1,000 tuples from the unused test data and measured F1 score (Riezler et al., 2003) by comparing the trees of the generated compression and the “correct”, extracted headline. The systems we compared are the unsupervised baseline (UNSUP. SYSTEM) and the supervised model trained on three kinds of feature sets: (1) SYNT-INFO FEATURES, corresponding to the supervised training of the unsupervised baseline model (i.e., lemma(h)-label(e) and lemma(n)); (2) NON-LEX FEATURES, corresponding to a dense, non-lexical feature representation (i.e., all the feature types from Table 1 excluding the three involving lemmas); (3) ALL FEATURES (same as OUR SYSTEM). Additionally, we trained the s</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Riezler, S., T. H. King, R. Crouch &amp; A. Zaenen (2003). Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar. In Proc. of HLT-NAACL-03, pp. 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-05,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="1757" citStr="Turner &amp; Charniak, 2005" startWordPosition="272" endWordPosition="275">an automatic evaluation setting, significantly outperforming a strong baseline. 1 Introduction and related work Sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 20</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Turner, J. &amp; E. Charniak (2005). Supervised and unsupervised learning for sentence compression. In Proc. of ACL-05, pp. 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>In Proc. of ACL-10,</booktitle>
<pages>565--574</pages>
<contexts>
<context position="5704" citStr="Woodsend &amp; Lapata (2010)" startWordPosition="894" endWordPosition="897">es estimated from monolingual corpora to generate compressions of transcribed speech. Adopting an integer linear programming (ILP) framework, Clarke &amp; Lapata (2008) use hand-crafted syntactic constraints and an ngram language model, trained on uncompressed sentences, to find best compressions. The model of Filippova &amp; Strube (2008) also uses ILP but the problem is formulated over dependencies and not ngrams. Conditional probabilities and word counts collected from a large treebank are combined in an ad hoc manner to assess grammatical importance and informativeness of dependencies. Similarly, Woodsend &amp; Lapata (2010) formulate an ILP problem to generate news story highlights using precomputed scores. Again, an ad hoc combination of the scores learned independently of the task is used in the objective function. Contributions of this paper. Our work is motivated by the obvious need for a large parallel corpus of sentences and compressions on which extractive systems can be trained. Furthermore, we want the compressions in the corpus to be structurally very close to the input. Ideally, in every pair, the compression should correspond to a subtree of the input. To this end, our contributions are three-fold: •</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>Woodsend, K. &amp; M. Lapata (2010). Automatic generation of story highlights. In Proc. of ACL-10, pp. 565–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Multiple aspect summarization using Integer Linear Programming.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-12,</booktitle>
<pages>233--243</pages>
<contexts>
<context position="12944" citStr="Woodsend &amp; Lapata (2012)" startWordPosition="2101" endWordPosition="2105">Tree of extracted headline with transformations undone Figure 1: Source, transformed and extracted trees given headline British soldier killed in Afghanistan root root ccomp in amod subj of amod subj in amod of Defense British a soldier was killed roadside in a blast southern in Afghanistan root says Britain’s Ministry root amod subj in in root British a soldier was killed in a blast in Afghanistan root det amod subj auxpass prep pobj det prep pobj the subject edge over the preposition-in edge since it contributes more weight to the objective function. The informativeness score is inspired by Woodsend &amp; Lapata (2012) and is defined as Pheadline(lemma(n)) winfo(e) = (5) Particle(lemma(n)) This weight tells us how likely it is that a word from an article appears in the headline. For example, given two edges one of which points to verb say and another one to verb kill, the latter would be preferred over the former because kill is more “headliny” than say. When collecting counts for the syntactic and informativeness scores, we used 9M news articles crawled from the Internet, much more than Filippova &amp; Strube (2008). As a result our estimates are probably more accurate than theirs. Although both wsynt and winf</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Woodsend, K. &amp; M. Lapata (2012). Multiple aspect summarization using Integer Linear Programming. In Proc. of EMNLP-12, pp. 233–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wubben</author>
<author>A van den Bosch</author>
<author>E Krahmer</author>
<author>E Marsi</author>
</authors>
<title>Clustering and matching headlines for automatic paraphrase acquisition.</title>
<date>2009</date>
<booktitle>In Proc. of ENLG-09,</booktitle>
<pages>122--125</pages>
<marker>Wubben, van den Bosch, Krahmer, Marsi, 2009</marker>
<rawString>Wubben, S., A. van den Bosch, E. Krahmer &amp; E. Marsi (2009). Clustering and matching headlines for automatic paraphrase acquisition. In Proc. of ENLG-09, pp. 122–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B J Dorr</author>
<author>J Lin</author>
<author>R Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<journal>Information Processing &amp; Management, Special Issue on Text Summarization,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="1996" citStr="Zajic et al., 2007" startWordPosition="310" endWordPosition="313">al content. A robust compression system would be useful for mobile devices as well as a module in an extractive summarization system (Mani, 2001). Although a compression may differ lexically and structurally from the source sentence, to date most systems are extractive and proceed by deleting words from the input (Knight &amp; Marcu, 2000; Dorr et al., 2003; Turner &amp; Charniak, 2005; Clarke &amp; Lapata, 2008; Berg-Kirkpatrick et al., 2011, inter alia). To decide which words, dependencies or phrases can be dropped, (i) rule-based approaches (Grefenstette, 1998; Jing &amp; McKeown, 2000; Dorr et al., 2003; Zajic et al., 2007), (ii) supervised models trained on parallel data (Knight &amp; Marcu, 2000; Turner &amp; Charniak, 2005; McDonald, 2006; Gillick &amp; Favre, 2009; Galanis &amp; Androutsopoulos, 2010, inter alia) and (iii) unsupervised methods which make use of statistics collected from non-parallel data (Hori &amp; Furui, 2004; Zajic et al., 2007; Clarke &amp; Lapata, 2008; Filippova &amp; Strube, 2008) have been investigated. Since it is infeasible to manually devise a set of accurate deletion rules with high coverage, recent research has been devoted to developing statistical methods and possibly augmenting them with a few linguisti</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>Zajic, D., B. J. Dorr, J. Lin &amp; R. Schwartz (2007). Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing &amp; Management, Special Issue on Text Summarization, 43(6):1549–1570.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
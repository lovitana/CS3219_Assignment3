<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.993743">
Deep Learning for Chinese Word Segmentation and POS Tagging
</title>
<author confidence="0.9964">
Xiaoqing Zheng
</author>
<affiliation confidence="0.996399">
Fudan University
</affiliation>
<address confidence="0.8558705">
220 Handan Road
Shanghai, 200433, China
</address>
<email confidence="0.994701">
zhengxq@fudan.edu.cn
</email>
<author confidence="0.982749">
Hanyang Chen
</author>
<affiliation confidence="0.988736">
Fudan University
</affiliation>
<address confidence="0.852662">
220 Handan Road
Shanghai, 200433, China
</address>
<email confidence="0.989737">
chenhy12345@gmail.com
</email>
<author confidence="0.984675">
Tianyu Xu
</author>
<affiliation confidence="0.98464">
Fudan University
</affiliation>
<address confidence="0.9223685">
220 Handan Road
Shanghai, 200433, China
</address>
<email confidence="0.995409">
xty213@gmail.com
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940294117647">
This study explores the feasibility of perform-
ing Chinese word segmentation (CWS) and
POS tagging by deep learning. We try to avoid
task-specific feature engineering, and use deep
layers of neural networks to discover relevant
features to the tasks. We leverage large-scale
unlabeled data to improve internal representa-
tion of Chinese characters, and use these im-
proved representations to enhance supervised
word segmentation and POS tagging models.
Our networks achieved close to state-of-the-
art performance with minimal computational
cost. We also describe a perceptron-style al-
gorithm for training the neural networks, as
an alternative to maximum-likelihood method,
to speed up the training process and make the
learning algorithm easier to be implemented.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941208333334">
Word segmentation has been a long-standing chal-
lenge for the Chinese NLP community. It has re-
ceived steady attention over the past two decades.
Previous studies show that joint solutions usually
lead to the improvement in accuracy over pipelined
systems by exploiting POS information to help word
segmentation and avoiding error propagation. How-
ever, traditional joint approaches usually involve a
great number of features, which arises four limita-
tions. First, the size of the result models is too large
for practical use due to the storage and computing
constraints of certain real-world applications. Sec-
ond, the number of parameters is so large that the
trained model is apt to overfit on training corpus.
Third, a longer training time is required. Last but
not the least, the decoding by dynamic programming
technique might be intractable since a large search
space is faced by the decoder.
The choice of features, therefore, is a critical suc-
cess factor for these systems. Most of the state-of-
the-art systems address their tasks by applying linear
statistical models to the features carefully optimized
for the tasks. This approach is effective because re-
searchers can incorporate a large body of linguistic
knowledge into the models. However, the approach
does not scale well when it is used to perform more
complex joint tasks, for example, the task of joint
word segmentation, POS tagging, parsing, and se-
mantic role labeling. A challenge for such a joint
model is the large combined search space, which
makes engineering effective task-specific features
and structured learning of parameters very hard. In-
stead, we use multilayer neural networks to discover
the useful features from the input sentences.
There are two main contributions in this paper. (1)
We describe a perceptron-style algorithm for train-
ing the neural networks, which not only speeds up
the training of the networks with negligible loss in
performance, but also can be implemented more eas-
ily; (2) We show that the tasks of Chinese word seg-
mentation and POS tagging can be effectively per-
formed by the deep learning. Our networks achieved
close to state-of-the-art performance by transferring
the unsupervised internal representations of Chinese
characters into the supervised models.
Section 2 presents the general architecture of
neural networks, and our perceptron-style training
algorithm for tagging. Section 3 describes how to
</bodyText>
<page confidence="0.974867">
647
</page>
<note confidence="0.733767">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647–657,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999811">
leverage large unlabeled data to obtain more useful
character embeddings, and reports the experimental
results of our systems. Section 4 presents a brief
overview of related work. The conclusions are given
in section 5.
</bodyText>
<sectionHeader confidence="0.972044" genericHeader="introduction">
2 The Neural Network Architecture
</sectionHeader>
<bodyText confidence="0.99983562962963">
Chinese word segmentation and part-of-speech tag-
ging tasks can be formulated as assigning labels to
characters of an input sentence. The performance
of the traditional tagging approaches is heavily de-
pendent on the choice of features, for example, con-
ditional random fields (CRFs), often with a set of
feature templates. For that reason, much of the ef-
fort in designing such systems goes into the feature
engineering, which is important but labor-intensive,
mainly based on human ingenuity and linguistic in-
tuition.
In order to make learning algorithms less depen-
dent on the feature engineering, we chose to use a
variant of the neural network architecture first pro-
posed by (Bengio et al., 2003) for probabilistic lan-
guage model, and reintroduced later by (Collobert
et al., 2011) for multiple NLP tasks. The network
takes the input sentence and discovers multiple lev-
els of feature extraction from the inputs, with higher
levels representing more abstract aspects of the in-
puts. The architecture is shown in Figure 1. The first
layer extracts features for each Chinese character.
The next layer extracts features from a window of
characters. The following layers are classical neural
network layers. The output of the network is a graph
over which tag inference is achieved with a Viterbi
algorithm.
</bodyText>
<subsectionHeader confidence="0.995118">
2.1 Mapping Characters into Feature Vectors
</subsectionHeader>
<bodyText confidence="0.99909525">
The characters are fed into the network as indices
that are used by a lookup operation to transform
characters into their feature vectors. We consider
a fixed-sized character dictionary D1. The vector
representations are stored in a character embedding
matrix M E Rd&amp;quot;|D|, where d is the dimensionality
of the vector space (a hyper-parameter to be chosen)
and |D |is the size of the dictionary.
</bodyText>
<footnote confidence="0.992702333333333">
1Unless otherwise specified, the character dictionary is ex-
tracted from the training set. Unknown characters are mapped
to a special symbol that is not used elsewhere.
</footnote>
<figureCaption confidence="0.999943">
Figure 1: The neural network architecture.
</figureCaption>
<bodyText confidence="0.999937">
Formally, assume we are given a Chinese sen-
tence c[1:n] that is a sequence of n characters ci,1 &lt;
i &lt; n. For each character ci E D that has an associ-
ated index ki into the column of the embedding ma-
trix, an d-dimensional feature vector representation
is retrieved by the lookup table layer ZD(�) E Rd:
</bodyText>
<equation confidence="0.999165">
ZD(ci) = Meki (1)
</equation>
<bodyText confidence="0.999069428571429">
where we use a binary vector eki E R|D|&amp;quot;1 which is
zero in all positions except at the ki-th index. The
lookup operation can be seen as a simple projection
layer. The feature vector of each character, starting
from a random initialization, can be automatically
trained by back propagation to be relevant to the task
of interest.
In practice, it is common that one might want to
provide other additional features that is thought to
be helpful for the task. For example, for the name
entity recognition task, one could provide a feature
which says if a character is in a list of the common
Chinese surnames or not. Another common practice
is to introduce some statistics-based measures, such
</bodyText>
<figure confidence="0.999090282051282">
Tag Inference
B
E
Sigmoid
Lookup Table
Linear
Input Window
Linear
S
I
W 2
W 3 × + b3
Features
g( )
× +
Text
J(t|1) J(t|2) J(t|i) J(t|n−1) J(t|n)
b2
d − 1
d
2
3
4
5
1
6
...
.. .
OBJ ILS a a
&amp;quot;A dog sits in the corner&amp;quot;
Number of Hidden Units
Number of Hidden Units
....
. .. ...
. .
Number of tags
Concatenate
Aij
A
</figure>
<page confidence="0.993143">
648
</page>
<bodyText confidence="0.999655333333333">
as boundary entropy (Jin and Tanaka-Ishii, 2006)
and accessor variety (Feng et al., 2004), which are
commonly used in unsupervised CWS models. We
associate a lookup table to each additional feature,
and the character feature vector becomes the con-
catenation of the outputs of all these lookup tables.
</bodyText>
<subsectionHeader confidence="0.999766">
2.2 Tag scoring
</subsectionHeader>
<bodyText confidence="0.9999875">
A neural network can be considered as a function
fo(·) with parameters 0. Any feed-forward neural
network with L layers can be seen as a composition
of functions fe(·) defined for each layer l:
</bodyText>
<equation confidence="0.9997235">
fo(·) = fe (f�−1
o (... f1o (·) ...)) (2)
</equation>
<bodyText confidence="0.9963445">
For each character in a sentence, a score is pro-
duced for every tag by applying several layers of the
neural network over the feature vectors produced by
the lookup table layer. We use a window approach
to handle the sequences of variable sentence length.
The window approach assumes that the tag of a char-
acter depends mainly on its neighboring characters.
More precisely, given an input sentence c[1:n], we
consider all successive windows of size w (a hyper-
parameter), siding over the sentence, from character
c1 to cn. At position ci, the character feature win-
dow produced by the first lookup table layer can be
written as:
where the matrices W2 E RH×(wd), b2 E RH,
W3 E R|T |×H and b3 E R|T  |are the parameters to
be trained. The hyper-parameter H is usually called
the number of hidden units. As non-linear function,
we chose a sigmoidal function2:
</bodyText>
<equation confidence="0.997778">
g(x) = 1/(1 + ex) (5)
</equation>
<subsectionHeader confidence="0.99739">
2.3 Tag Inference
</subsectionHeader>
<bodyText confidence="0.999986588235294">
There are strong dependencies between character
tags in a sentence for the tasks like word segmen-
tation and POS tagging. The tags are organized in
chunks, and it is impossible for some tags to fol-
low other tags. We introduce a transition score Ai9
for jumping from i E T to j E T tags in succes-
sive characters, and an initial scores A0i for starting
from the i-th tag for taking into account the sentence
structure. We want the valid paths of tags to be en-
couraged, while discouraging all other paths.
Given an input sentence c[1:n], the network out-
puts the matrix of scores fo(c[1:n]). We use a nota-
tion fo(t|i) to indicate the score output by the net-
work with parameters 0, for the sentence c[1:n] and
for the t-th tag, at the i-th character. The score of a
sentence c[1:n] along a path of tags t[1:n] is then given
by the sum of transition and network scores:
</bodyText>
<equation confidence="0.987869642857143">
�
� � � � � � �
f1o (ci) =
�
� � � � � � �
(3)
OD(ci−w/2)
...
OD(ci)
...
OD(ci+w/2)
n
s(c[1:n], t[1:n], 0) = (At,−1t, + fo(ti|i)) (6)
i=1
</equation>
<bodyText confidence="0.999961230769231">
Given a sentence c[1:n], we can find the best tag path
t∗[1:n] by maximizing the sentence score:
The characters with indices exceeding the sentence
boundaries are mapped to one of two special sym-
bols, namely “start” and “stop” symbols.
The fixed-sized vector f1o is fed to two stan-
dard Linear Layers that successively perform affine
transformations over f1o , interleaved with some non-
linearity function g(·), to extract highly non-linear
features. Given a set of tags T for the task of inter-
est, the network outputs a vector of size |T  |for each
character at position i, interpreted as a score for each
tag in T and each character ci in the sentence:
</bodyText>
<equation confidence="0.983989">
fo(ci) = f3o (g(f2o (f1o (ci))))
t∗ s(c[1:n],t0[1:n],0) (7)
[1:n] = arg max
∀t,
[1:n]
</equation>
<bodyText confidence="0.999915">
The Viterbi algorithm can be used for this inference.
Now we are prepared to show how to train the para-
meters of the network in an end-to-end fashion.
</bodyText>
<subsectionHeader confidence="0.990614">
2.4 Training
</subsectionHeader>
<bodyText confidence="0.999489">
The training problem is to determine all the para-
meters of the network 0 = (M, W2, b2, W3, b3, A)
from training data. The network generally is trained
</bodyText>
<footnote confidence="0.915577">
2In our experiments, the sigmoidal function performs
slightly better than the “hard” version of the hyperbolic tangent
used by (Collobert, 2011).
</footnote>
<equation confidence="0.925233">
= W3g(W2f1o (ci) + b2) + b3 (4)
</equation>
<page confidence="0.989336">
649
</page>
<bodyText confidence="0.9979525">
by maximizing a likelihood over all the sentences in
the training set R with respect to 0:
</bodyText>
<equation confidence="0.9902915">
�0 �� log p(t|c, 0) (8)
∀(c,t)∈R
</equation>
<bodyText confidence="0.999794909090909">
where c represents a sentence and its associated fea-
tures, and t denotes the corresponding tag sequence.
We drop the subscript [1 : n] from now for nota-
tion simplification. The probability p(·) is calcu-
lated from the outputs of the neural network. We
will present in the following section how to interpret
neural network outputs as probabilities.
Maximizing the log-likelihood (8) with the gradi-
ent ascent algorithm3 is achieved by iteratively se-
lecting a example (c, t) and applying the following
gradient update rule:
</bodyText>
<equation confidence="0.9962835">
0 +— 0 + Aa log p(t|c, 0) (9)
a0
</equation>
<bodyText confidence="0.9999016">
where A is the learning rate (a hyper-parameter).
The gradient in (9) can be computed by a classical
back propagation: the differentiation chain rule is
applied through the network, until the character em-
bedding layer.
</bodyText>
<subsectionHeader confidence="0.919102">
2.4.1 Sentence-Level Log-Likelihood
</subsectionHeader>
<bodyText confidence="0.9999935">
The score of a sentence (6) is interpreted as a con-
ditional tag path probability by taking it to the expo-
nential (making the score positive) and normalizing
it over all possible tag paths (summing to 1 over all
paths). Taking the log, the conditional probability of
the true path t is given by4:
</bodyText>
<equation confidence="0.9799485">
exp{s(c, t0, 0)}
(10)
</equation>
<bodyText confidence="0.99641804">
3We did not use the stochastic gradient ascent algorithm
(Bottou, 1991) to train the network as (Collobert et al.,
2011). The gradient ascent algorithm was used instead for fairly
comparing our algorithm with the sentence-level maximum-
likelihood method (see Section 2.4.1). The gradient ascent al-
gorithm requires a loop over all the examples to compute the
gradient of the cost function, which will not cause a problem
since all the training sets used in this article are finite.
4The cost functions are differentiable everywhere thanks
to the differentiability of sigmoidal function chosen as non-
linearity instead of a “hard” version of the hyperbolic tangent.
For details about gradient computations, see Appendix A of
(Collobert et al., 2011).
The number of terms in (10) grows exponentially
with the length of the input sentence. Although one
can compute it in linear time with the Viterbi algo-
rithm, it is quite computationally expensive to com-
pute the conditional probability of the true path, and
its derivatives with respect to fθ(t|i) and Aij. The
gradients with respect to the trainable parameters
other than fθ(t|i) and Aij can all be computed using
the derivatives with respect to fθ(t|i) by applying
the differentiation chain rule. We will see in the next
section our training algorithm that has the advantage
of being much cheaper to compute the gradients.
</bodyText>
<subsectionHeader confidence="0.978051">
2.5 A New Training Method
</subsectionHeader>
<bodyText confidence="0.999969529411765">
The log-likelihood (10) can be seen as the difference
between the forward score constrained over the valid
path and the sum of the scores of all possible paths.
While this training criterion is used, the neural net-
works are trained by maximizing the likelihood of
training data. In fact, a CRF maximizes the same
log-likelihood (Lafferty et al., 2001) by using a lin-
ear model in stead of a nonlinear neural network.
As an alternative to maximum-likelihood method,
we propose the following training algorithm inspired
by the work of (Collins, 2002). Given a training
example (c, t), the network outputs the matrix of
scores fθ(c) under the current parameter settings.
The highest scoring sequence of tags for the input
sentence c then can be found using the Viterbi al-
gorithm: this tagged sequence is denoted by t0. For
every character ci where ti =� t0i, we simply set
</bodyText>
<equation confidence="0.7569045">
aLθ(t, t0|c) ++,
afθ(ti|i)
</equation>
<bodyText confidence="0.669271">
and for every transition where ti−1 =� t0i−1 or ti =� t0i,
we set
</bodyText>
<equation confidence="0.9759745">
aLθ(t, t0|c) ++,
aAti−1ti
</equation>
<bodyText confidence="0.999936333333333">
where “++” (which increases a value by one) and
“−−” (which decreases a value by one) are two
unary operators, and Lθ(t, t0|c) is a new function
which we now want to maximize over all the training
pairs (c, t). The function Lθ(t, t0|c) can be viewed as
the difference between the score of the correct path
and that of the incorrect one (which is the highest
scoring sequence produced by the network under the
current parameters 0).
</bodyText>
<equation confidence="0.990546888888889">
�
log p(t|c, 0) = s(c, t, 0) − log
∀t/
(11)
aLθ(t, t0|c)
afθ(t0i|i)
(12)
aLθ(t, t0|c)
aAti−1ti
</equation>
<page confidence="0.931764">
650
</page>
<bodyText confidence="0.940561470588235">
As an example, say the correct tag sequence of the
sentence ✝94✸♣✍ ‘A dog sits in the corner’ is
✝/S 94/S ✸/S ♣/B ✍/E
and under the current parameter settings the highest
scoring tag sequence is
✝/S 94/B ✸/E ♣/B ✍/E
Then the derivatives with respect to fθ(S|94), and
fθ(S|✸) will be set to 1, that with respect to ASB,
ABE, AEB, fθ(B|94), and fθ(E|✸) to −1, and that
with respect to ASS to 2 respectively5. Intuitively
these assignments have the effect of updating the pa-
rameter values in a way that increases the score of
the correct tag sequence and decreases the score of
the incorrect one output by the network with the cur-
rent parameter settings. If the tag sequence produced
by the network is correct, no changes are made to the
values of parameters.
</bodyText>
<figure confidence="0.495550260869565">
Inputs:
R: a training set.
N: a specified maximum number of iterations.
E: a desired tagging precision.
Initialization: set the initial parameters of the network with
small random values.
Output: the trained parameters B
Algorithm:
do
for each example (c, t) E R
get the matrix fe(c) by the neural network under the
current parameters B
find the highest scoring sequence of tags t&apos; for c with
fe(c) and Az, by using the Viterbi algorithm
if(t#t&apos;)
compute the gradients with respect to fe(c) and
Az, as (11) and (12)
compute the gradients with respect to the weights
from output layer to character embedding layer
update the parameters of the network by (9)
until the desired precision E achieved or maximum num-
ber of iterations N reached
return B
</figure>
<figureCaption confidence="0.999606">
Figure 2: The training algorithm for tagging.
</figureCaption>
<bodyText confidence="0.999777">
We propose the training algorithm in Figure 2.
Note that the perceptron algorithm of (Collins,
2002) was designed for discriminatively training an
</bodyText>
<footnote confidence="0.972469">
5The derivatives with respect to ASB will be set to 0, be-
cause it is increased first and decreased afterwards.
</footnote>
<bodyText confidence="0.9998622">
HMM-style tagger, while our algorithm is used to
calculate the “direction” in which the parameters are
updated (i.e. the gradient of the function we want to
maximize). Due to space limitations, we do not give
convergence theorems justifying the training algo-
rithm in this paper. Intuitively it can be achieved by
combining the theorems of convergence for the per-
ceptron applied to tagging problem from (Collins,
2002) with the convergence results of backpropaga-
tion algorithm from (Rumelhart et al., 1986).
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999911633333333">
We conducted three sets of experiments. The goal
of the first one is to test several variants for each
training algorithm on the development set, to gain
some understanding of how the choice of hyper-
parameters impacts upon the performance. We ap-
plied the network both with the sentence-level log-
likelihood (SLL) and our perceptron-style training
algorithm (PSA) to the two Chinese NLP problems:
word segmentation, and joint CWS and POS tag-
ging. We ran this set of experiments on the part of
Chinese Treebank 4 (CTB-4)6. Ninety percent of the
sentences (1529) were randomly chosen for training
and the rest (168) were used as development set.
The second set of experiments was run on the
Chinese Treebank (CTB) data sets from Bakeoff-3
(Levow, 2006), which contains a training and a test
corpus for supervised word segmentation and POS
tagging tasks. The results were obtained without us-
ing any extra knowledge (i.e. the closed test), and
are comparable with other models in the literature.
In the third experiment, we study to see how well
large unlabeled texts can be used to enhance the
supervised learning. Following (Collobert et al.,
2011), we first use large unlabeled data set to ob-
tain character embeddings carrying more syntactic
and semantic information, and then use these im-
proved embeddings to initialize the character lookup
tables of the networks instead of previous random
values. Our corpus is the Sina news7 that contains
about 325MB data.
</bodyText>
<footnote confidence="0.9997318">
6The data set was sections 1–43, 144–169, and 900–931 of
the treebank, containing 78,023 characters, 45,135 words and
1,697 sentences. These files are double-annotated and can be
regarded as golden standard files.
7Available at http://www.sina.com.cn/
</footnote>
<page confidence="0.996229">
651
</page>
<bodyText confidence="0.971644083333333">
We implemented two versions of the network:
one for the sentence-level log-likelihood and one
for our perceptron-style training algorithm. Both
are written in Java language. All experiments were
run on a computer equipped with an Intel Core i3
processor working at 2.13GHz, with 2GB RAM,
running Linux and Java Development Kit 1.6. The
standard F-score was used to evaluate the perfor-
mance of both word segmentation and joint word
segmentation and POS tagging tasks. F-score is the
harmonic mean of precision p and recall r, which is
defined as 2pr/(p + r).
</bodyText>
<figure confidence="0.991908">
1 3 5 7 9
Window Size
</figure>
<figureCaption confidence="0.999813">
Figure 3: Average F-score versus window size.
</figureCaption>
<subsectionHeader confidence="0.999214">
3.1 Tagging Schemes
</subsectionHeader>
<bodyText confidence="0.999799214285714">
The network will output the scores for all the possi-
ble tags for the task of interest. For word segmen-
tation, each character will be assigned one of four
possible boundary tags: “B” for a character located
at the beginning of a word, “I” for that inside of a
word, “E” for that at the end of a word, and “S” for
a character that is a word by itself.
Following Ng and Lou (2004) we perform joint
word segmentation and POS tagging task in a la-
beling fashion by expanding boundary labels to in-
clude POS tags. For instance, we describe verb
phases using four different tags. Tag “S VP” is used
to mark a verb phase containing a single character.
Other tags “B VP”, “I VP”, and “E VP” are used to
</bodyText>
<figure confidence="0.554418">
Number of Hidden Units
</figure>
<figureCaption confidence="0.999509">
Figure 4: Average F-score versus number of hidden units.
</figureCaption>
<bodyText confidence="0.999061">
mark the first, in-between and last characters of the
verb phrase. In fact, we used the “IOBES” tagging
scheme, and tag “O” is not applicable to Chinese
word segmentation and POS tagging tasks.
</bodyText>
<subsectionHeader confidence="0.998548">
3.2 The Choice of Hyper-parameters
</subsectionHeader>
<bodyText confidence="0.999971363636364">
We tuned the hyper-parameters by trying only a
few different networks. We report in Figure 3
the F-scores on the development set versus win-
dow size for word segmentation (SEG) and joint
word segmentation and POS tagging (JWP) tasks
with the sentence-level log-likelihood (SLL) and our
perceptron-style training algorithm (PSA), and re-
port in Figure 4 the F-scores on the same data set
versus number of hidden units. The average F-
scores were obtained over 5 runs with different ran-
dom initialization for each setting of the network.
The F-scores of the word segmentation, out-of-
vocabulary, and POS tagging are denoted by Fwo,.d,
Foo„ and Fpos respectively.
Generally, the number of hidden units has a lim-
ited impact on the performance if it is large enough,
which is consistent with the findings of (Collobert
et al., 2011) for English. It can be seen from Fig-
ure 3 that the performance drops smoothly when the
window size is larger than 3. In particularly, the F-
score of out-of-vocabulary identification decreases
relatively fast beyond window size 5, which shows
</bodyText>
<figure confidence="0.995226181818182">
F-score
100
90
80
70
60
Foo,,: SEG (SSL)
Foo,,: SEG (PSA)
Fword: SEG (PSA)
Fword: SEG (SLL)
Fword: JWP (SLL)
Fword: JWP (PSA)
Foo,,: JWP (SSL)
Foo,,: JWP (PSA)
Fpos: JWP (SSL)
Fpos: JWP (PSA)
F-score
100
90
80
70
60
100 300 500 700 900
Foo,,: SEG (SSL)
Foo,,: SEG (PSA)
Fkord: SEG (PSA)
Fkord: SEG (SLL)
Fkord: JWP (SLL)
Fkord: JWP (PSA)
Foo,,: JWP (SSL)
Foo,,: JWP (PSA)
Fpos: JWP (SSL)
Fpos: JWP (PSA)
</figure>
<page confidence="0.994123">
652
</page>
<bodyText confidence="0.999793722222222">
that the size of window (and the number of parame-
ters) is too large that the trained network has over-
fitted on training data. An explanation for this result
is that most Chinese words are less than 3 charac-
ters, and the neighboring characters outside of the
window (size 5) become “noise” when we perform
word segmentation.
The hyper-parameters of the network used in all
the following experiments are shown in Table 1. Al-
though the top performance was obtained by the net-
work with window size 3, we chose the architecture
with window size 5 because a larger training corpus
will be used in the following experiments, and the
sparseness problem would be alleviated. Further-
more, in order to obtain character embeddings by
using large unlabeled data, we prefer to “observe”
a character within a slightly larger window to better
discover its syntactic and semantic information.
</bodyText>
<table confidence="0.9963804">
Hyper-parameter Value
Window size 5
Number of hidden units 300
Character feature dimension 50
Learning rate 0.02
</table>
<tableCaption confidence="0.999702">
Table 1: Hyper-parameters of the network.
</tableCaption>
<bodyText confidence="0.99838625">
We report in Table 2 the F-score of the first five
iterations on the development set for word segmen-
tation with SLL and PSA. The data in the fourth
and fifth rows of the table shows the convergence
of PSA. The difference of the F-scores between the
networks with SLL and PSA can be negligible after
the number of iteration is greater than 5. In our im-
plementation, for each iteration the training time is
reduced at least 10% by using PSA, compared with
SLL. The training time can be reduced further for
more complex tasks like POS tagging and semantic
role labeling in which a larger tag set is used.
</bodyText>
<table confidence="0.999717714285714">
Iteration 1 2 3 4 5
SSL Fword 49.89 69.56 88.91 90.19 91.24
Foo, 15.92 27.54 54.37 55.89 59.74
Time (s) 209 398 586 737 886
Fword 49.04 68.54 87.79 89.07 91.19
PSA Foo, 13.61 25.79 52.30 55.15 60.49
Time (s) 184 343 497 610 754
</table>
<tableCaption confidence="0.895623">
Table 2: Word segmentation results with SLL and PSA
for the first five iterations.
</tableCaption>
<bodyText confidence="0.999857727272727">
Many exponential sums (Ei exp(xi)) are re-
quired for training the networks with SLL, and in
most cases the values of exponential sums will ex-
ceed the range of double-precision floating-point
arithmetic defined in popular programming lan-
guages. These sums need to be estimated by an-
alytic number theory. In comparison, a lot of the
computation-intensive exponential sums are avoided
in our training algorithm, which not only speed up
the training of the networks but also make it easier
to be implemented.
</bodyText>
<subsectionHeader confidence="0.998792">
3.3 Closed Test on the SIGHAN Bakeoff
</subsectionHeader>
<bodyText confidence="0.999982542857143">
We trained the networks with PSA on the Chinese
Treebank (CTB) data set from Bakeoff-3 for both
SEG and JWP tasks. The results are reported in Ta-
ble 3. The hyper-parameters of our networks are re-
ported in Table 1. Although results show that our
networks with PSA are behind the state-of-the-art
systems, the networks perform comparatively well,
considering we did not use any extra information.
Many other systems used some extra heuristics or re-
sources to improve their performance. For example,
a key parameter in the system of (Wang et al., 2006)
was optimized in advance by using an external seg-
mented corpus, and a manually prepared list of char-
acters as well as their types was used in (Zhao et al.,
2006; Zhu et al., 2006; Kruengkrai et al., 2009).
It is worth noting that the comparison for joint
word segmentation and POS tagging task is indirect
because the different versions of CTB were used.
We reported the results on CTB-3 from SIGHAN
Bakeoff-3, while both (Jiang et al., 2008) and (Kru-
engkrai et al., 2009) used CTB-5. Both (Ng and
Lou, 2004) and (Zhang and Clark, 2008) evenly par-
titioned the sentences in CTB3 into ten groups, and
used nine groups for training and the rest for testing.
Following (Bengio et al., 2003; Collobert et al.,
2011), we want semantically and syntactically sim-
ilar characters to be close in the embedding space.
If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar
semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’,
we could generalize from ✝$O✸♣✍ ‘A dog sits
in the corner’ to ❝94✸♣✍ ‘A cat sits in the cor-
ner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in
the same way. We describe the way to obtain these
character embeddings by using large unlabeled data
in the next section.
</bodyText>
<page confidence="0.998302">
653
</page>
<table confidence="0.863322714285714">
Approach Fword Roov Fpos
(Zhao et al., 2006) 93.30 70.70 −
(Wang et al., 2006) 93.00 68.30 −
(Zhu et al., 2006) 92.70 63.40 −
SEG (Zhang et al., 2006) 92.60 61.70 −
(Feng et al., 2006) 91.70 68.00 −
PSA 92.59 64.24 −
PSA + LM 94.57 70.12 −
(Ng and Lou, 2004) 95.20 − −
(Zhang and Clark, 2008) 95.90 − 91.34
JWP (Jiang et al., 2008) 97.30 − 92.50
(Kruengkrai et al., 2009) 96.11 − 90.85
PSA 93.83 68.21 90.79
PSA + LM 95.23 72.38 91.82
</table>
<tableCaption confidence="0.987718">
Table 3: Comparison of the F-scores on the Penn Chinese
Treebank
</tableCaption>
<subsectionHeader confidence="0.990778">
3.4 Combined Approach
</subsectionHeader>
<bodyText confidence="0.999963125">
We used the corpus of Sina news to obtain charac-
ter embeddings carrying more semantic and syntac-
tic information by training a language model that
evaluates the acceptability of a piece of text. This
language model is again the neural network, and we
also use PSA to train the language model. Following
(Collobert et al., 2011), we minimize the following
criterion with respect to the parameters 0:
</bodyText>
<equation confidence="0.861778333333333">
�0 �� E max{0,1 − fe(cIh) + fe(c&apos;Ih)I
VhEW Vc&apos;ED
(13)
</equation>
<bodyText confidence="0.999848243243243">
where the score fe(clh) is the output of the network
with parameters 0 for a character c at the center of
a window h, D is the dictionary of characters, H is
the set of all possible text windows (i.e. character se-
quences) from the training data, and c&apos; h denotes the
window obtained by replacing the central character
of the window h by the character c&apos;.
We used a dictionary consisting of the charac-
ters extracted from all the data sets in Bakeoff-
3, which contains about eight thousand characters.
The total unsupervised training time was about two
weeks. Our combined approach works as initializing
the lookup tables of the supervised networks with
the character embeddings obtained by unsupervised
learning, and then performing supervised training on
CTB-3. The lookup tables will not be modified at the
supervised training stage.
We reported the results in Table 3, in which our
combined approach is indicated by “PSA + LM”.
It can be seen from Table 3 that this approach re-
sults in a performance boost for both SEG and JWP
tasks. The POS tagging F-score of our approach
was comparable to but still less than the model of
(Jiang et al., 2008). They achieved the best score by
first separately training multiple word-, character-,
and POS n-gram based models, and then integrat-
ing them by cascading method. In comparison, our
networks achieve the performance by automatically
discovering useful features by itself and avoiding the
task-specific engineering.
Table 4 compares the decoding speeds on the test
data from CTB-3 for our system and for two CRFs-
based word segmentation systems. Regardless of
the differences in implementation, the neural net-
works clearly run considerably faster than the sys-
tems based on the CRFs model. They also require
much more memory than our neural networks.
</bodyText>
<table confidence="0.96639275">
System Number of parameters Time (s)
(Tsai et al., 2006) 3.1 x 106 1669
(Zhao et al., 2006) 3.8 x 106 2382
Neural network 4.7 x 105 138
</table>
<tableCaption confidence="0.999903">
Table 4: Comparison of computational cost.
</tableCaption>
<sectionHeader confidence="0.999773" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.997190238095238">
Word segmentation has been pursued with consid-
erable efforts in the Chinese NLP community, and
statistical approaches are clearly dominant in the
last decade. A popular statistical approach is the
character-based tagging solution that treats word
segmentation as a sequence tagging problem, as-
signing labels to the characters indicating whether a
character locates at the beginning of, inside, or at the
end of a word. The character-based tagging solution
was first proposed in (Xue, 2003). This work caused
quite a number of character position tagging based
CWS studies because known and unknown words
can be treated in the same way. Peng, Feng and Mc-
Callum (2004) first introduced a linear-chain CRFs
model to the character tagging based word segmen-
tation. Zhang and Clark (2007) proposed a word-
based CWS approach using a discriminative percep-
tron learning algorithm, which allows word-level in-
formation to be added as features.
Recent years have seen a rise of joint word seg-
mentation and POS tagging approach that improves
</bodyText>
<page confidence="0.997647">
654
</page>
<bodyText confidence="0.999993145161291">
the accuracies of both tasks and does not suffer from
the error propagation. Ng and Lou (2004) perform
such joint task in a labeling fashion by expanding
boundary labels to include POS tags. Zhang and
Clark (2008) proposed a linear model for the same
joint task, which overcomed the disadvantage of
(Ng and Lou, 2004), in which it was unable to incor-
porate “whole word + POS tag” features. Sun (2011)
described a sub-word model using stacked learn-
ing technique for the joint task, which explored the
complementary strength of different character- and
word-based segmenters with different views.
The majority of the state-of-the-art systems ad-
dress their tasks by applying linear statistical mod-
els to ad-hoc features. The researchers first chose
task-specific features which are then fed to a classi-
fication algorithm. The selected features may vary
greatly because they are usually chosen in a empir-
ical process, mainly based first on linguistic intu-
ition, and then trial and error. It seems reasonable
to assume that the number and effectiveness of fea-
tures constitutes a major factor in the performance
of the various systems, and might even more impor-
tant than the particular statistical models they used.
In comparison, we try to avoid task-specific feature
engineering, and use the neural network to learn sev-
eral layers of feature extraction from the inputs. To
the best of our knowledge, this study is among the
first ones to perform Chinese word segmentation and
POS tagging by deep learning.
It was reported that supervised and unsupervised
approaches can be integrated to improve on the over-
all performance of word segmentation by combin-
ing the strengths of both. Zhao and Kit (2011) ex-
plored the feasibility of enhancing supervised seg-
mentation by informing the supervised learner of
goodness scores obtained from large unlabeled cor-
pus. Sun and Xu (2011) investigated how to improve
on the accuracy of supervised word segmentation by
leveraging the statistics-based features derived from
large unlabeled in-domain corpus and the document
to be segmented. The basic idea of these integration
solutions is to incorporate a set of statistics-based
measures into a CRFs model after these measures
are derived from unlabeled data and discretized into
feature values. In comparison, we use large unla-
beled data to obtain the character embeddings with
more syntactic and semantic information.
Several works have investigated how to use deep
learning for NLP applications (Bengio et al., 2003;
Collobert et al., 2011; Collobert, 2011; Socher et
al., 2011). In most cases, words are fed to the neural
networks as inputs, and the lookup tables map each
word to a vector representation. Our network is dif-
ferent in that the inputs to the network are charac-
ters, more raw units than words. In many Asian
languages, such as Chinese and Japanese, they are
written without using whitespace to delimit words.
For these languages, the character becomes a more
natural form of input. Furthermore, a perceptron-
style algorithm for tagging is proposed for training
the networks.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999997476190476">
We have described a perceptron-style algorithm for
training the neural networks, which is much easier to
be implemented, and has speed advantage over the
maximum-likelihood scheme, while the loss in per-
formance is negligible. The neural networks trained
with PSA have been applied to Chinese word seg-
mentation and POS tagging tasks, and the networks
achieved close to state-of-the-art performance by us-
ing the character representations learned from large
unlabeled corpus.
Although we focus on the question of how far we
can go for Chinese word segmentation and POS tag-
ging without using the extra task-specific features in
this study, there are at least three ways to further im-
prove the performance of the networks, which are
worthy to be explored in the future: (1) introduce
specific linguistic features (e.g. gazetteer features)
that are helpful for the tasks; (2) incorporate some
common techniques, such as cascading, voting, and
ensemble; and (3) use the special network architec-
ture tailored for the tasks of interest.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999636875">
The authors would like to thank the anonymous re-
viewers for their valuable comments. The work
was supported by a grant from the National Nat-
ural Science Foundation of China (No. 60903078),
a grant from Shanghai Leading Academic Disci-
pline Project (No. B114), a grant from Shang-
hai Municipal Natural Science Foundation (No.
13ZR1403800), and a grant from FDUROP.
</bodyText>
<page confidence="0.998767">
655
</page>
<sectionHeader confidence="0.996348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679575471698">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic langu-
gage model. Journal of Machine Learning Research,
3: 1137–1155.
L´eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of the Neuro-Nimes.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the In-
ternational Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP’02).
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In Proceedings of the 14th In-
ternational Conference on Artificial Intelligence and
Statistics (AISTATS’11).
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12: 2493–
2537.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chinese
word extraction. Computational Linguistics, 30(1):
75–93.
Yuanyong Feng, Sun Le, and Yuanhua Lv. 2006. Chi-
nese word segmentation and name entity recognition
based on conditional random fields models. In Pro-
ceedings of the Fifth SIGHAN Workshop on Chinese
Language Processing (SIGHAN’06).
Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008.
A cascaded linear model for joint Chinese word seg-
mentation and part-of-speech tagging. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL’08).
Zhihui Jin, and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branch-
ing entropy. In Proceedings of the International Con-
ference on Computational Linguistics and the Annual
Meeting of the Association for Computational Linguis-
tics (COLING/ACL’06).
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
pos tagging. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’09).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
learning (ICML’01).
Gina A. Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In Proceedings of the
SIGHAN Workshop on Chinese Language Processing
(SIGHAN’06).
Hwee T. Ng and Jin K. Lou. 2004. Chinese part-of-
speech tagging: one-at-atime or all-at-once? word-
based or character-based? In Proceedings of the Inter-
national Conference on Empirical Methods in Natural
Language Processing (EMNLP’04).
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th International Conference on Computational Lin-
guistics (COLING’04).
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning internal representations by
backpropagating errors. Parallel Distributed Process-
ing: Explorations in the Microstructure of Cognition,
1: 318–362. MIT Press.
Richard Socher, Cliff C-Y. Lin, Andrew Y. Ng, and
Christopher D. Manning 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the International Con-
ference on Machine learning (ICML’11).
Weiwei Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech tag-
ging. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL’11).
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word
segmentation using unlabeled data. In Proceedings of
the International Conference on Empirical Methods in
Natural Language Processing (EMNLP’11).
Richard T.-H. Tsai, Hsieh C. Hung, Chenglung Sung,
Hongjie Dai, and Wenlian Hsu. 2006. On closed
task of Chinese word segmentation: An improved CRF
model coupled with character clustering and automat-
ically generated template matching. In Proceedings
of the Fifth SIGHAN Workshop on Chinese Language
Processing (SIGHAN’06).
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing (SIGHAN’06).
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1): 29–48.
Min Zhang, GuoDong Zhou, LingPeng Yang, and
DongHong Ji. 2006. Chinese word segmentation
and named entity recognition based on a context-
dependent mutual information independence Model.
In Proceedings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing (SIGHAN’06).
</reference>
<page confidence="0.987871">
656
</page>
<reference confidence="0.994904681818182">
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-base perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL’07).
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’08).
Hai Zhao, Chang N. Huang, and Mu Li. 2006. An im-
proved Chinese word segmentation system with con-
ditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing
(SIGHAN’06).
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The role
of goodness measures. Information Sciences, 181(1):
163–183.
Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen
Wang, and Jingbo Zhu. 2006. Designing special post-
processing rules for SVM-based Chinese word seg-
mentation. In Proceedings of the Fifth SIGHAN Work-
shop on Chinese Language Processing (SIGHAN’06).
</reference>
<page confidence="0.998022">
657
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.118672">
<title confidence="0.780889666666667">Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Fudan</title>
<address confidence="0.996994">220 Handan Shanghai, 200433,</address>
<email confidence="0.929468">zhengxq@fudan.edu.cn</email>
<affiliation confidence="0.449271">Hanyang</affiliation>
<address confidence="0.889492333333333">Fudan 220 Handan Shanghai, 200433,</address>
<email confidence="0.998602">chenhy12345@gmail.com</email>
<author confidence="0.974204">Tianyu Xu</author>
<affiliation confidence="0.99988">Fudan University</affiliation>
<address confidence="0.998">220 Handan Shanghai, 200433,</address>
<email confidence="0.998568">xty213@gmail.com</email>
<abstract confidence="0.998634388888889">This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic langugage model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="4678" citStr="Bengio et al., 2003" startWordPosition="711" endWordPosition="714"> as assigning labels to characters of an input sentence. The performance of the traditional tagging approaches is heavily dependent on the choice of features, for example, conditional random fields (CRFs), often with a set of feature templates. For that reason, much of the effort in designing such systems goes into the feature engineering, which is important but labor-intensive, mainly based on human ingenuity and linguistic intuition. In order to make learning algorithms less dependent on the feature engineering, we chose to use a variant of the neural network architecture first proposed by (Bengio et al., 2003) for probabilistic language model, and reintroduced later by (Collobert et al., 2011) for multiple NLP tasks. The network takes the input sentence and discovers multiple levels of feature extraction from the inputs, with higher levels representing more abstract aspects of the inputs. The architecture is shown in Figure 1. The first layer extracts features for each Chinese character. The next layer extracts features from a window of characters. The following layers are classical neural network layers. The output of the network is a graph over which tag inference is achieved with a Viterbi algor</context>
<context position="25992" citStr="Bengio et al., 2003" startWordPosition="4411" endWordPosition="4414">d a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006</context>
<context position="32972" citStr="Bengio et al., 2003" startWordPosition="5586" endWordPosition="5589">ed how to improve on the accuracy of supervised word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information. Several works have investigated how to use deep learning for NLP applications (Bengio et al., 2003; Collobert et al., 2011; Collobert, 2011; Socher et al., 2011). In most cases, words are fed to the neural networks as inputs, and the lookup tables map each word to a vector representation. Our network is different in that the inputs to the network are characters, more raw units than words. In many Asian languages, such as Chinese and Japanese, they are written without using whitespace to delimit words. For these languages, the character becomes a more natural form of input. Furthermore, a perceptronstyle algorithm for tagging is proposed for training the networks. 5 Conclusion We have descr</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic langugage model. Journal of Machine Learning Research, 3: 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Stochastic gradient learning in neural networks.</title>
<date>1991</date>
<booktitle>In Proceedings of the Neuro-Nimes.</booktitle>
<contexts>
<context position="12264" citStr="Bottou, 1991" startWordPosition="2063" endWordPosition="2064"> hyper-parameter). The gradient in (9) can be computed by a classical back propagation: the differentiation chain rule is applied through the network, until the character embedding layer. 2.4.1 Sentence-Level Log-Likelihood The score of a sentence (6) is interpreted as a conditional tag path probability by taking it to the exponential (making the score positive) and normalizing it over all possible tag paths (summing to 1 over all paths). Taking the log, the conditional probability of the true path t is given by4: exp{s(c, t0, 0)} (10) 3We did not use the stochastic gradient ascent algorithm (Bottou, 1991) to train the network as (Collobert et al., 2011). The gradient ascent algorithm was used instead for fairly comparing our algorithm with the sentence-level maximumlikelihood method (see Section 2.4.1). The gradient ascent algorithm requires a loop over all the examples to compute the gradient of the cost function, which will not cause a problem since all the training sets used in this article are finite. 4The cost functions are differentiable everywhere thanks to the differentiability of sigmoidal function chosen as nonlinearity instead of a “hard” version of the hyperbolic tangent. For detai</context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L´eon Bottou. 1991. Stochastic gradient learning in neural networks. In Proceedings of the Neuro-Nimes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’02).</booktitle>
<contexts>
<context position="14137" citStr="Collins, 2002" startWordPosition="2369" endWordPosition="2370">g much cheaper to compute the gradients. 2.5 A New Training Method The log-likelihood (10) can be seen as the difference between the forward score constrained over the valid path and the sum of the scores of all possible paths. While this training criterion is used, the neural networks are trained by maximizing the likelihood of training data. In fact, a CRF maximizes the same log-likelihood (Lafferty et al., 2001) by using a linear model in stead of a nonlinear neural network. As an alternative to maximum-likelihood method, we propose the following training algorithm inspired by the work of (Collins, 2002). Given a training example (c, t), the network outputs the matrix of scores fθ(c) under the current parameter settings. The highest scoring sequence of tags for the input sentence c then can be found using the Viterbi algorithm: this tagged sequence is denoted by t0. For every character ci where ti =� t0i, we simply set aLθ(t, t0|c) ++, afθ(ti|i) and for every transition where ti−1 =� t0i−1 or ti =� t0i, we set aLθ(t, t0|c) ++, aAti−1ti where “++” (which increases a value by one) and “−−” (which decreases a value by one) are two unary operators, and Lθ(t, t0|c) is a new function which we now w</context>
<context position="16759" citStr="Collins, 2002" startWordPosition="2834" endWordPosition="2835">e(c) by the neural network under the current parameters B find the highest scoring sequence of tags t&apos; for c with fe(c) and Az, by using the Viterbi algorithm if(t#t&apos;) compute the gradients with respect to fe(c) and Az, as (11) and (12) compute the gradients with respect to the weights from output layer to character embedding layer update the parameters of the network by (9) until the desired precision E achieved or maximum number of iterations N reached return B Figure 2: The training algorithm for tagging. We propose the training algorithm in Figure 2. Note that the perceptron algorithm of (Collins, 2002) was designed for discriminatively training an 5The derivatives with respect to ASB will be set to 0, because it is increased first and decreased afterwards. HMM-style tagger, while our algorithm is used to calculate the “direction” in which the parameters are updated (i.e. the gradient of the function we want to maximize). Due to space limitations, we do not give convergence theorems justifying the training algorithm in this paper. Intuitively it can be achieved by combining the theorems of convergence for the perceptron applied to tagging problem from (Collins, 2002) with the convergence res</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS’11).</booktitle>
<contexts>
<context position="10905" citStr="Collobert, 2011" startWordPosition="1828" endWordPosition="1829">ed as a score for each tag in T and each character ci in the sentence: fo(ci) = f3o (g(f2o (f1o (ci)))) t∗ s(c[1:n],t0[1:n],0) (7) [1:n] = arg max ∀t, [1:n] The Viterbi algorithm can be used for this inference. Now we are prepared to show how to train the parameters of the network in an end-to-end fashion. 2.4 Training The training problem is to determine all the parameters of the network 0 = (M, W2, b2, W3, b3, A) from training data. The network generally is trained 2In our experiments, the sigmoidal function performs slightly better than the “hard” version of the hyperbolic tangent used by (Collobert, 2011). = W3g(W2f1o (ci) + b2) + b3 (4) 649 by maximizing a likelihood over all the sentences in the training set R with respect to 0: �0 �� log p(t|c, 0) (8) ∀(c,t)∈R where c represents a sentence and its associated features, and t denotes the corresponding tag sequence. We drop the subscript [1 : n] from now for notation simplification. The probability p(·) is calculated from the outputs of the neural network. We will present in the following section how to interpret neural network outputs as probabilities. Maximizing the log-likelihood (8) with the gradient ascent algorithm3 is achieved by iterat</context>
<context position="33013" citStr="Collobert, 2011" startWordPosition="5594" endWordPosition="5595">ed word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information. Several works have investigated how to use deep learning for NLP applications (Bengio et al., 2003; Collobert et al., 2011; Collobert, 2011; Socher et al., 2011). In most cases, words are fed to the neural networks as inputs, and the lookup tables map each word to a vector representation. Our network is different in that the inputs to the network are characters, more raw units than words. In many Asian languages, such as Chinese and Japanese, they are written without using whitespace to delimit words. For these languages, the character becomes a more natural form of input. Furthermore, a perceptronstyle algorithm for tagging is proposed for training the networks. 5 Conclusion We have described a perceptron-style algorithm for tra</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2493--2537</pages>
<contexts>
<context position="4763" citStr="Collobert et al., 2011" startWordPosition="724" endWordPosition="727">aditional tagging approaches is heavily dependent on the choice of features, for example, conditional random fields (CRFs), often with a set of feature templates. For that reason, much of the effort in designing such systems goes into the feature engineering, which is important but labor-intensive, mainly based on human ingenuity and linguistic intuition. In order to make learning algorithms less dependent on the feature engineering, we chose to use a variant of the neural network architecture first proposed by (Bengio et al., 2003) for probabilistic language model, and reintroduced later by (Collobert et al., 2011) for multiple NLP tasks. The network takes the input sentence and discovers multiple levels of feature extraction from the inputs, with higher levels representing more abstract aspects of the inputs. The architecture is shown in Figure 1. The first layer extracts features for each Chinese character. The next layer extracts features from a window of characters. The following layers are classical neural network layers. The output of the network is a graph over which tag inference is achieved with a Viterbi algorithm. 2.1 Mapping Characters into Feature Vectors The characters are fed into the net</context>
<context position="12313" citStr="Collobert et al., 2011" startWordPosition="2070" endWordPosition="2073">can be computed by a classical back propagation: the differentiation chain rule is applied through the network, until the character embedding layer. 2.4.1 Sentence-Level Log-Likelihood The score of a sentence (6) is interpreted as a conditional tag path probability by taking it to the exponential (making the score positive) and normalizing it over all possible tag paths (summing to 1 over all paths). Taking the log, the conditional probability of the true path t is given by4: exp{s(c, t0, 0)} (10) 3We did not use the stochastic gradient ascent algorithm (Bottou, 1991) to train the network as (Collobert et al., 2011). The gradient ascent algorithm was used instead for fairly comparing our algorithm with the sentence-level maximumlikelihood method (see Section 2.4.1). The gradient ascent algorithm requires a loop over all the examples to compute the gradient of the cost function, which will not cause a problem since all the training sets used in this article are finite. 4The cost functions are differentiable everywhere thanks to the differentiability of sigmoidal function chosen as nonlinearity instead of a “hard” version of the hyperbolic tangent. For details about gradient computations, see Appendix A of</context>
<context position="18582" citStr="Collobert et al., 2011" startWordPosition="3130" endWordPosition="3133">inety percent of the sentences (1529) were randomly chosen for training and the rest (168) were used as development set. The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test corpus for supervised word segmentation and POS tagging tasks. The results were obtained without using any extra knowledge (i.e. the closed test), and are comparable with other models in the literature. In the third experiment, we study to see how well large unlabeled texts can be used to enhance the supervised learning. Following (Collobert et al., 2011), we first use large unlabeled data set to obtain character embeddings carrying more syntactic and semantic information, and then use these improved embeddings to initialize the character lookup tables of the networks instead of previous random values. Our corpus is the Sina news7 that contains about 325MB data. 6The data set was sections 1–43, 144–169, and 900–931 of the treebank, containing 78,023 characters, 45,135 words and 1,697 sentences. These files are double-annotated and can be regarded as golden standard files. 7Available at http://www.sina.com.cn/ 651 We implemented two versions of</context>
<context position="21621" citStr="Collobert et al., 2011" startWordPosition="3647" endWordPosition="3650">tion and POS tagging (JWP) tasks with the sentence-level log-likelihood (SLL) and our perceptron-style training algorithm (PSA), and report in Figure 4 the F-scores on the same data set versus number of hidden units. The average Fscores were obtained over 5 runs with different random initialization for each setting of the network. The F-scores of the word segmentation, out-ofvocabulary, and POS tagging are denoted by Fwo,.d, Foo„ and Fpos respectively. Generally, the number of hidden units has a limited impact on the performance if it is large enough, which is consistent with the findings of (Collobert et al., 2011) for English. It can be seen from Figure 3 that the performance drops smoothly when the window size is larger than 3. In particularly, the Fscore of out-of-vocabulary identification decreases relatively fast beyond window size 5, which shows F-score 100 90 80 70 60 Foo,,: SEG (SSL) Foo,,: SEG (PSA) Fword: SEG (PSA) Fword: SEG (SLL) Fword: JWP (SLL) Fword: JWP (PSA) Foo,,: JWP (SSL) Foo,,: JWP (PSA) Fpos: JWP (SSL) Fpos: JWP (PSA) F-score 100 90 80 70 60 100 300 500 700 900 Foo,,: SEG (SSL) Foo,,: SEG (PSA) Fkord: SEG (PSA) Fkord: SEG (SLL) Fkord: JWP (SLL) Fkord: JWP (PSA) Foo,,: JWP (SSL) Foo</context>
<context position="26017" citStr="Collobert et al., 2011" startWordPosition="4415" endWordPosition="4418"> list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − SEG (Zhan</context>
<context position="27330" citStr="Collobert et al., 2011" startWordPosition="4661" endWordPosition="4664"> 94.57 70.12 − (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 JWP (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates the acceptability of a piece of text. This language model is again the neural network, and we also use PSA to train the language model. Following (Collobert et al., 2011), we minimize the following criterion with respect to the parameters 0: �0 �� E max{0,1 − fe(cIh) + fe(c&apos;Ih)I VhEW Vc&apos;ED (13) where the score fe(clh) is the output of the network with parameters 0 for a character c at the center of a window h, D is the dictionary of characters, H is the set of all possible text windows (i.e. character sequences) from the training data, and c&apos; h denotes the window obtained by replacing the central character of the window h by the character c&apos;. We used a dictionary consisting of the characters extracted from all the data sets in Bakeoff3, which contains about ei</context>
<context position="32996" citStr="Collobert et al., 2011" startWordPosition="5590" endWordPosition="5593">the accuracy of supervised word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information. Several works have investigated how to use deep learning for NLP applications (Bengio et al., 2003; Collobert et al., 2011; Collobert, 2011; Socher et al., 2011). In most cases, words are fed to the neural networks as inputs, and the lookup tables map each word to a vector representation. Our network is different in that the inputs to the network are characters, more raw units than words. In many Asian languages, such as Chinese and Japanese, they are written without using whitespace to delimit words. For these languages, the character becomes a more natural form of input. Furthermore, a perceptronstyle algorithm for tagging is proposed for training the networks. 5 Conclusion We have described a perceptron-style </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12: 2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin Zheng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>75--93</pages>
<contexts>
<context position="7349" citStr="Feng et al., 2004" startWordPosition="1182" endWordPosition="1185">ame entity recognition task, one could provide a feature which says if a character is in a list of the common Chinese surnames or not. Another common practice is to introduce some statistics-based measures, such Tag Inference B E Sigmoid Lookup Table Linear Input Window Linear S I W 2 W 3 × + b3 Features g( ) × + Text J(t|1) J(t|2) J(t|i) J(t|n−1) J(t|n) b2 d − 1 d 2 3 4 5 1 6 ... .. . OBJ ILS a a &amp;quot;A dog sits in the corner&amp;quot; Number of Hidden Units Number of Hidden Units .... . .. ... . . Number of tags Concatenate Aij A 648 as boundary entropy (Jin and Tanaka-Ishii, 2006) and accessor variety (Feng et al., 2004), which are commonly used in unsupervised CWS models. We associate a lookup table to each additional feature, and the character feature vector becomes the concatenation of the outputs of all these lookup tables. 2.2 Tag scoring A neural network can be considered as a function fo(·) with parameters 0. Any feed-forward neural network with L layers can be seen as a composition of functions fe(·) defined for each layer l: fo(·) = fe (f�−1 o (... f1o (·) ...)) (2) For each character in a sentence, a score is produced for every tag by applying several layers of the neural network over the feature ve</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for Chinese word extraction. Computational Linguistics, 30(1): 75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanyong Feng</author>
<author>Sun Le</author>
<author>Yuanhua Lv</author>
</authors>
<title>Chinese word segmentation and name entity recognition based on conditional random fields models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="26666" citStr="Feng et al., 2006" startWordPosition="4539" endWordPosition="4542">actically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − SEG (Zhang et al., 2006) 92.60 61.70 − (Feng et al., 2006) 91.70 68.00 − PSA 92.59 64.24 − PSA + LM 94.57 70.12 − (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 JWP (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates the acceptability of a piece of text. This language model is again the neural network, and we also use PSA</context>
</contexts>
<marker>Feng, Le, Lv, 2006</marker>
<rawString>Yuanyong Feng, Sun Le, and Yuanhua Lv. 2006. Chinese word segmentation and name entity recognition based on conditional random fields models. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).</booktitle>
<contexts>
<context position="25749" citStr="Jiang et al., 2008" startWordPosition="4367" endWordPosition="4370">ny extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corn</context>
<context position="28602" citStr="Jiang et al., 2008" startWordPosition="4884" endWordPosition="4887">ing time was about two weeks. Our combined approach works as initializing the lookup tables of the supervised networks with the character embeddings obtained by unsupervised learning, and then performing supervised training on CTB-3. The lookup tables will not be modified at the supervised training stage. We reported the results in Table 3, in which our combined approach is indicated by “PSA + LM”. It can be seen from Table 3 that this approach results in a performance boost for both SEG and JWP tasks. The POS tagging F-score of our approach was comparable to but still less than the model of (Jiang et al., 2008). They achieved the best score by first separately training multiple word-, character-, and POS n-gram based models, and then integrating them by cascading method. In comparison, our networks achieve the performance by automatically discovering useful features by itself and avoiding the task-specific engineering. Table 4 compares the decoding speeds on the test data from CTB-3 for our system and for two CRFsbased word segmentation systems. Regardless of the differences in implementation, the neural networks clearly run considerably faster than the systems based on the CRFs model. They also req</context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Unsupervised segmentation of Chinese text by use of branching entropy.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06).</booktitle>
<contexts>
<context position="7308" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="1175" endWordPosition="1178">to be helpful for the task. For example, for the name entity recognition task, one could provide a feature which says if a character is in a list of the common Chinese surnames or not. Another common practice is to introduce some statistics-based measures, such Tag Inference B E Sigmoid Lookup Table Linear Input Window Linear S I W 2 W 3 × + b3 Features g( ) × + Text J(t|1) J(t|2) J(t|i) J(t|n−1) J(t|n) b2 d − 1 d 2 3 4 5 1 6 ... .. . OBJ ILS a a &amp;quot;A dog sits in the corner&amp;quot; Number of Hidden Units Number of Hidden Units .... . .. ... . . Number of tags Concatenate Aij A 648 as boundary entropy (Jin and Tanaka-Ishii, 2006) and accessor variety (Feng et al., 2004), which are commonly used in unsupervised CWS models. We associate a lookup table to each additional feature, and the character feature vector becomes the concatenation of the outputs of all these lookup tables. 2.2 Tag scoring A neural network can be considered as a function fo(·) with parameters 0. Any feed-forward neural network with L layers can be seen as a composition of functions fe(·) defined for each layer l: fo(·) = fe (f�−1 o (... f1o (·) ...)) (2) For each character in a sentence, a score is produced for every tag by applying several layers </context>
</contexts>
<marker>Jin, Tanaka-Ishii, 2006</marker>
<rawString>Zhihui Jin, and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branching entropy. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING/ACL’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL’09).</booktitle>
<contexts>
<context position="25511" citStr="Kruengkrai et al., 2009" startWordPosition="4327" endWordPosition="4330"> reported in Table 3. The hyper-parameters of our networks are reported in Table 1. Although results show that our networks with PSA are behind the state-of-the-art systems, the networks perform comparatively well, considering we did not use any extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space</context>
<context position="26853" citStr="Kruengkrai et al., 2009" startWordPosition="4578" endWordPosition="4581">ralize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − SEG (Zhang et al., 2006) 92.60 61.70 − (Feng et al., 2006) 91.70 68.00 − PSA 92.59 64.24 − PSA + LM 94.57 70.12 − (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 JWP (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates the acceptability of a piece of text. This language model is again the neural network, and we also use PSA to train the language model. Following (Collobert et al., 2011), we minimize the following criterion with respect to the parameters 0: �0 �� E max{0,1 − fe(cIh) + fe(c&apos;Ih)I VhEW Vc&apos;ED (1</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine learning (ICML’01).</booktitle>
<contexts>
<context position="13941" citStr="Lafferty et al., 2001" startWordPosition="2335" endWordPosition="2338"> and Aij can all be computed using the derivatives with respect to fθ(t|i) by applying the differentiation chain rule. We will see in the next section our training algorithm that has the advantage of being much cheaper to compute the gradients. 2.5 A New Training Method The log-likelihood (10) can be seen as the difference between the forward score constrained over the valid path and the sum of the scores of all possible paths. While this training criterion is used, the neural networks are trained by maximizing the likelihood of training data. In fact, a CRF maximizes the same log-likelihood (Lafferty et al., 2001) by using a linear model in stead of a nonlinear neural network. As an alternative to maximum-likelihood method, we propose the following training algorithm inspired by the work of (Collins, 2002). Given a training example (c, t), the network outputs the matrix of scores fθ(c) under the current parameter settings. The highest scoring sequence of tags for the input sentence c then can be found using the Viterbi algorithm: this tagged sequence is denoted by t0. For every character ci where ti =� t0i, we simply set aLθ(t, t0|c) ++, afθ(ti|i) and for every transition where ti−1 =� t0i−1 or ti =� t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine learning (ICML’01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina A Levow</author>
</authors>
<title>The third international Chinese language processing bakeoff: Word segmentation and named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="18186" citStr="Levow, 2006" startWordPosition="3067" endWordPosition="3068">ment set, to gain some understanding of how the choice of hyperparameters impacts upon the performance. We applied the network both with the sentence-level loglikelihood (SLL) and our perceptron-style training algorithm (PSA) to the two Chinese NLP problems: word segmentation, and joint CWS and POS tagging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4)6. Ninety percent of the sentences (1529) were randomly chosen for training and the rest (168) were used as development set. The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test corpus for supervised word segmentation and POS tagging tasks. The results were obtained without using any extra knowledge (i.e. the closed test), and are comparable with other models in the literature. In the third experiment, we study to see how well large unlabeled texts can be used to enhance the supervised learning. Following (Collobert et al., 2011), we first use large unlabeled data set to obtain character embeddings carrying more syntactic and semantic information, and then use these improved embeddings to initialize the character lookup tables of</context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Gina A. Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee T Ng</author>
<author>Jin K Lou</author>
</authors>
<title>Chinese part-ofspeech tagging: one-at-atime or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’04).</booktitle>
<contexts>
<context position="20175" citStr="Ng and Lou (2004)" startWordPosition="3400" endWordPosition="3403">mentation and joint word segmentation and POS tagging tasks. F-score is the harmonic mean of precision p and recall r, which is defined as 2pr/(p + r). 1 3 5 7 9 Window Size Figure 3: Average F-score versus window size. 3.1 Tagging Schemes The network will output the scores for all the possible tags for the task of interest. For word segmentation, each character will be assigned one of four possible boundary tags: “B” for a character located at the beginning of a word, “I” for that inside of a word, “E” for that at the end of a word, and “S” for a character that is a word by itself. Following Ng and Lou (2004) we perform joint word segmentation and POS tagging task in a labeling fashion by expanding boundary labels to include POS tags. For instance, we describe verb phases using four different tags. Tag “S VP” is used to mark a verb phase containing a single character. Other tags “B VP”, “I VP”, and “E VP” are used to Number of Hidden Units Figure 4: Average F-score versus number of hidden units. mark the first, in-between and last characters of the verb phrase. In fact, we used the “IOBES” tagging scheme, and tag “O” is not applicable to Chinese word segmentation and POS tagging tasks. 3.2 The Cho</context>
<context position="25815" citStr="Ng and Lou, 2004" startWordPosition="4380" endWordPosition="4383">or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character</context>
<context position="30569" citStr="Ng and Lou (2004)" startWordPosition="5203" endWordPosition="5206">quite a number of character position tagging based CWS studies because known and unknown words can be treated in the same way. Peng, Feng and McCallum (2004) first introduced a linear-chain CRFs model to the character tagging based word segmentation. Zhang and Clark (2007) proposed a wordbased CWS approach using a discriminative perceptron learning algorithm, which allows word-level information to be added as features. Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the error propagation. Ng and Lou (2004) perform such joint task in a labeling fashion by expanding boundary labels to include POS tags. Zhang and Clark (2008) proposed a linear model for the same joint task, which overcomed the disadvantage of (Ng and Lou, 2004), in which it was unable to incorporate “whole word + POS tag” features. Sun (2011) described a sub-word model using stacked learning technique for the joint task, which explored the complementary strength of different character- and word-based segmenters with different views. The majority of the state-of-the-art systems address their tasks by applying linear statistical mod</context>
</contexts>
<marker>Ng, Lou, 2004</marker>
<rawString>Hwee T. Ng and Jin K. Lou. 2004. Chinese part-ofspeech tagging: one-at-atime or all-at-once? wordbased or character-based? In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04).</booktitle>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning internal representations by backpropagating errors.</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing: Explorations in the Microstructure of Cognition,</booktitle>
<volume>1</volume>
<pages>318--362</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17422" citStr="Rumelhart et al., 1986" startWordPosition="2938" endWordPosition="2941">ing an 5The derivatives with respect to ASB will be set to 0, because it is increased first and decreased afterwards. HMM-style tagger, while our algorithm is used to calculate the “direction” in which the parameters are updated (i.e. the gradient of the function we want to maximize). Due to space limitations, we do not give convergence theorems justifying the training algorithm in this paper. Intuitively it can be achieved by combining the theorems of convergence for the perceptron applied to tagging problem from (Collins, 2002) with the convergence results of backpropagation algorithm from (Rumelhart et al., 1986). 3 Experiments We conducted three sets of experiments. The goal of the first one is to test several variants for each training algorithm on the development set, to gain some understanding of how the choice of hyperparameters impacts upon the performance. We applied the network both with the sentence-level loglikelihood (SLL) and our perceptron-style training algorithm (PSA) to the two Chinese NLP problems: word segmentation, and joint CWS and POS tagging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4)6. Ninety percent of the sentences (1529) were randomly chosen for </context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning internal representations by backpropagating errors. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, 1: 318–362. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C-Y Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Machine learning (ICML’11).</booktitle>
<contexts>
<context position="33035" citStr="Socher et al., 2011" startWordPosition="5596" endWordPosition="5599">ion by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information. Several works have investigated how to use deep learning for NLP applications (Bengio et al., 2003; Collobert et al., 2011; Collobert, 2011; Socher et al., 2011). In most cases, words are fed to the neural networks as inputs, and the lookup tables map each word to a vector representation. Our network is different in that the inputs to the network are characters, more raw units than words. In many Asian languages, such as Chinese and Japanese, they are written without using whitespace to delimit words. For these languages, the character becomes a more natural form of input. Furthermore, a perceptronstyle algorithm for tagging is proposed for training the networks. 5 Conclusion We have described a perceptron-style algorithm for training the neural netwo</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C-Y. Lin, Andrew Y. Ng, and Christopher D. Manning 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the International Conference on Machine learning (ICML’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11).</booktitle>
<contexts>
<context position="30875" citStr="Sun (2011)" startWordPosition="5259" endWordPosition="5260"> a discriminative perceptron learning algorithm, which allows word-level information to be added as features. Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the error propagation. Ng and Lou (2004) perform such joint task in a labeling fashion by expanding boundary labels to include POS tags. Zhang and Clark (2008) proposed a linear model for the same joint task, which overcomed the disadvantage of (Ng and Lou, 2004), in which it was unable to incorporate “whole word + POS tag” features. Sun (2011) described a sub-word model using stacked learning technique for the joint task, which explored the complementary strength of different character- and word-based segmenters with different views. The majority of the state-of-the-art systems address their tasks by applying linear statistical models to ad-hoc features. The researchers first chose task-specific features which are then fed to a classification algorithm. The selected features may vary greatly because they are usually chosen in a empirical process, mainly based first on linguistic intuition, and then trial and error. It seems reasona</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’11).</booktitle>
<contexts>
<context position="32342" citStr="Sun and Xu (2011)" startWordPosition="5491" endWordPosition="5494">ific feature engineering, and use the neural network to learn several layers of feature extraction from the inputs. To the best of our knowledge, this study is among the first ones to perform Chinese word segmentation and POS tagging by deep learning. It was reported that supervised and unsupervised approaches can be integrated to improve on the overall performance of word segmentation by combining the strengths of both. Zhao and Kit (2011) explored the feasibility of enhancing supervised segmentation by informing the supervised learner of goodness scores obtained from large unlabeled corpus. Sun and Xu (2011) investigated how to improve on the accuracy of supervised word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we use large unlabeled data to obtain the character embeddings with more syntactic and semantic information. Several works have investigated how to use deep learning for NLP app</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard T-H Tsai</author>
<author>Hsieh C Hung</author>
<author>Chenglung Sung</author>
<author>Hongjie Dai</author>
<author>Wenlian Hsu</author>
</authors>
<title>On closed task of Chinese word segmentation: An improved CRF model coupled with character clustering and automatically generated template matching.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="29306" citStr="Tsai et al., 2006" startWordPosition="4994" endWordPosition="4997">, and POS n-gram based models, and then integrating them by cascading method. In comparison, our networks achieve the performance by automatically discovering useful features by itself and avoiding the task-specific engineering. Table 4 compares the decoding speeds on the test data from CTB-3 for our system and for two CRFsbased word segmentation systems. Regardless of the differences in implementation, the neural networks clearly run considerably faster than the systems based on the CRFs model. They also require much more memory than our neural networks. System Number of parameters Time (s) (Tsai et al., 2006) 3.1 x 106 1669 (Zhao et al., 2006) 3.8 x 106 2382 Neural network 4.7 x 105 138 Table 4: Comparison of computational cost. 4 Related Work Word segmentation has been pursued with considerable efforts in the Chinese NLP community, and statistical approaches are clearly dominant in the last decade. A popular statistical approach is the character-based tagging solution that treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside, or at the end of a word. The character-based tagging solution was fi</context>
</contexts>
<marker>Tsai, Hung, Sung, Dai, Hsu, 2006</marker>
<rawString>Richard T.-H. Tsai, Hsieh C. Hung, Chenglung Sung, Hongjie Dai, and Wenlian Hsu. 2006. On closed task of Chinese word segmentation: An improved CRF model coupled with character clustering and automatically generated template matching. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhao Wang</author>
<author>Xiaojun Lin</author>
<author>Dianhai Yu</author>
<author>Hao Tian</author>
<author>Xihong Wu</author>
</authors>
<title>Chinese word segmentation with maximum entropy and n-gram language model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="25306" citStr="Wang et al., 2006" startWordPosition="4288" endWordPosition="4291">e it easier to be implemented. 3.3 Closed Test on the SIGHAN Bakeoff We trained the networks with PSA on the Chinese Treebank (CTB) data set from Bakeoff-3 for both SEG and JWP tasks. The results are reported in Table 3. The hyper-parameters of our networks are reported in Table 1. Although results show that our networks with PSA are behind the state-of-the-art systems, the networks perform comparatively well, considering we did not use any extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and </context>
<context position="26560" citStr="Wang et al., 2006" startWordPosition="4517" endWordPosition="4520">e rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − SEG (Zhang et al., 2006) 92.60 61.70 − (Feng et al., 2006) 91.70 68.00 − PSA 92.59 64.24 − PSA + LM 94.57 70.12 − (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 JWP (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates </context>
</contexts>
<marker>Wang, Lin, Yu, Tian, Wu, 2006</marker>
<rawString>Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong Wu. 2006. Chinese word segmentation with maximum entropy and n-gram language model. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<journal>Computational Linguistics and Chinese Language Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<contexts>
<context position="29933" citStr="Xue, 2003" startWordPosition="5098" endWordPosition="5099">Zhao et al., 2006) 3.8 x 106 2382 Neural network 4.7 x 105 138 Table 4: Comparison of computational cost. 4 Related Work Word segmentation has been pursued with considerable efforts in the Chinese NLP community, and statistical approaches are clearly dominant in the last decade. A popular statistical approach is the character-based tagging solution that treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside, or at the end of a word. The character-based tagging solution was first proposed in (Xue, 2003). This work caused quite a number of character position tagging based CWS studies because known and unknown words can be treated in the same way. Peng, Feng and McCallum (2004) first introduced a linear-chain CRFs model to the character tagging based word segmentation. Zhang and Clark (2007) proposed a wordbased CWS approach using a discriminative perceptron learning algorithm, which allows word-level information to be added as features. Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the </context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1): 29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>GuoDong Zhou</author>
<author>LingPeng Yang</author>
<author>DongHong Ji</author>
</authors>
<title>Chinese word segmentation and named entity recognition based on a contextdependent mutual information independence Model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="26632" citStr="Zhang et al., 2006" startWordPosition="4532" endWordPosition="4535">011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large unlabeled data in the next section. 653 Approach Fword Roov Fpos (Zhao et al., 2006) 93.30 70.70 − (Wang et al., 2006) 93.00 68.30 − (Zhu et al., 2006) 92.70 63.40 − SEG (Zhang et al., 2006) 92.60 61.70 − (Feng et al., 2006) 91.70 68.00 − PSA 92.59 64.24 − PSA + LM 94.57 70.12 − (Ng and Lou, 2004) 95.20 − − (Zhang and Clark, 2008) 95.90 − 91.34 JWP (Jiang et al., 2008) 97.30 − 92.50 (Kruengkrai et al., 2009) 96.11 − 90.85 PSA 93.83 68.21 90.79 PSA + LM 95.23 72.38 91.82 Table 3: Comparison of the F-scores on the Penn Chinese Treebank 3.4 Combined Approach We used the corpus of Sina news to obtain character embeddings carrying more semantic and syntactic information by training a language model that evaluates the acceptability of a piece of text. This language model is again the n</context>
</contexts>
<marker>Zhang, Zhou, Yang, Ji, 2006</marker>
<rawString>Min Zhang, GuoDong Zhou, LingPeng Yang, and DongHong Ji. 2006. Chinese word segmentation and named entity recognition based on a contextdependent mutual information independence Model. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-base perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07).</booktitle>
<contexts>
<context position="30225" citStr="Zhang and Clark (2007)" startWordPosition="5145" endWordPosition="5148">A popular statistical approach is the character-based tagging solution that treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside, or at the end of a word. The character-based tagging solution was first proposed in (Xue, 2003). This work caused quite a number of character position tagging based CWS studies because known and unknown words can be treated in the same way. Peng, Feng and McCallum (2004) first introduced a linear-chain CRFs model to the character tagging based word segmentation. Zhang and Clark (2007) proposed a wordbased CWS approach using a discriminative perceptron learning algorithm, which allows word-level information to be added as features. Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the error propagation. Ng and Lou (2004) perform such joint task in a labeling fashion by expanding boundary labels to include POS tags. Zhang and Clark (2008) proposed a linear model for the same joint task, which overcomed the disadvantage of (Ng and Lou, 2004), in which it was unable to incor</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-base perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).</booktitle>
<contexts>
<context position="25843" citStr="Zhang and Clark, 2008" startWordPosition="4385" endWordPosition="4388"> their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be close in the embedding space. If we knew that ✝ ‘dog’ and ❝ ‘cat’ were similar semantically, and similarly for $4 ‘sit’ and ✷ ‘lie’, we could generalize from ✝$O✸♣✍ ‘A dog sits in the corner’ to ❝94✸♣✍ ‘A cat sits in the corner’, and to ❝✷✸♣✍ ‘A cat lies in the corner’ in the same way. We describe the way to obtain these character embeddings by using large u</context>
<context position="30688" citStr="Zhang and Clark (2008)" startWordPosition="5223" endWordPosition="5226">he same way. Peng, Feng and McCallum (2004) first introduced a linear-chain CRFs model to the character tagging based word segmentation. Zhang and Clark (2007) proposed a wordbased CWS approach using a discriminative perceptron learning algorithm, which allows word-level information to be added as features. Recent years have seen a rise of joint word segmentation and POS tagging approach that improves 654 the accuracies of both tasks and does not suffer from the error propagation. Ng and Lou (2004) perform such joint task in a labeling fashion by expanding boundary labels to include POS tags. Zhang and Clark (2008) proposed a linear model for the same joint task, which overcomed the disadvantage of (Ng and Lou, 2004), in which it was unable to incorporate “whole word + POS tag” features. Sun (2011) described a sub-word model using stacked learning technique for the joint task, which explored the complementary strength of different character- and word-based segmenters with different views. The majority of the state-of-the-art systems address their tasks by applying linear statistical models to ad-hoc features. The researchers first chose task-specific features which are then fed to a classification algor</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang N Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="25467" citStr="Zhao et al., 2006" startWordPosition="4319" endWordPosition="4322">th SEG and JWP tasks. The results are reported in Table 3. The hyper-parameters of our networks are reported in Table 1. Although results show that our networks with PSA are behind the state-of-the-art systems, the networks perform comparatively well, considering we did not use any extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar c</context>
<context position="29341" citStr="Zhao et al., 2006" startWordPosition="5002" endWordPosition="5005">then integrating them by cascading method. In comparison, our networks achieve the performance by automatically discovering useful features by itself and avoiding the task-specific engineering. Table 4 compares the decoding speeds on the test data from CTB-3 for our system and for two CRFsbased word segmentation systems. Regardless of the differences in implementation, the neural networks clearly run considerably faster than the systems based on the CRFs model. They also require much more memory than our neural networks. System Number of parameters Time (s) (Tsai et al., 2006) 3.1 x 106 1669 (Zhao et al., 2006) 3.8 x 106 2382 Neural network 4.7 x 105 138 Table 4: Comparison of computational cost. 4 Related Work Word segmentation has been pursued with considerable efforts in the Chinese NLP community, and statistical approaches are clearly dominant in the last decade. A popular statistical approach is the character-based tagging solution that treats word segmentation as a sequence tagging problem, assigning labels to the characters indicating whether a character locates at the beginning of, inside, or at the end of a word. The character-based tagging solution was first proposed in (Xue, 2003). This w</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang N. Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrating unsupervised and supervised word segmentation: The role of goodness measures.</title>
<date>2011</date>
<journal>Information Sciences,</journal>
<volume>181</volume>
<issue>1</issue>
<pages>163--183</pages>
<contexts>
<context position="32169" citStr="Zhao and Kit (2011)" startWordPosition="5464" endWordPosition="5467">ajor factor in the performance of the various systems, and might even more important than the particular statistical models they used. In comparison, we try to avoid task-specific feature engineering, and use the neural network to learn several layers of feature extraction from the inputs. To the best of our knowledge, this study is among the first ones to perform Chinese word segmentation and POS tagging by deep learning. It was reported that supervised and unsupervised approaches can be integrated to improve on the overall performance of word segmentation by combining the strengths of both. Zhao and Kit (2011) explored the feasibility of enhancing supervised segmentation by informing the supervised learner of goodness scores obtained from large unlabeled corpus. Sun and Xu (2011) investigated how to improve on the accuracy of supervised word segmentation by leveraging the statistics-based features derived from large unlabeled in-domain corpus and the document to be segmented. The basic idea of these integration solutions is to incorporate a set of statistics-based measures into a CRFs model after these measures are derived from unlabeled data and discretized into feature values. In comparison, we u</context>
</contexts>
<marker>Zhao, Kit, 2011</marker>
<rawString>Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1): 163–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yilin Wang</author>
<author>Zhenxing Wang</author>
<author>Huizhen Wang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Designing special postprocessing rules for SVM-based Chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</booktitle>
<contexts>
<context position="25485" citStr="Zhu et al., 2006" startWordPosition="4323" endWordPosition="4326">s. The results are reported in Table 3. The hyper-parameters of our networks are reported in Table 1. Although results show that our networks with PSA are behind the state-of-the-art systems, the networks perform comparatively well, considering we did not use any extra information. Many other systems used some extra heuristics or resources to improve their performance. For example, a key parameter in the system of (Wang et al., 2006) was optimized in advance by using an external segmented corpus, and a manually prepared list of characters as well as their types was used in (Zhao et al., 2006; Zhu et al., 2006; Kruengkrai et al., 2009). It is worth noting that the comparison for joint word segmentation and POS tagging task is indirect because the different versions of CTB were used. We reported the results on CTB-3 from SIGHAN Bakeoff-3, while both (Jiang et al., 2008) and (Kruengkrai et al., 2009) used CTB-5. Both (Ng and Lou, 2004) and (Zhang and Clark, 2008) evenly partitioned the sentences in CTB3 into ten groups, and used nine groups for training and the rest for testing. Following (Bengio et al., 2003; Collobert et al., 2011), we want semantically and syntactically similar characters to be cl</context>
</contexts>
<marker>Zhu, Wang, Wang, Wang, Zhu, 2006</marker>
<rawString>Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, and Jingbo Zhu. 2006. Designing special postprocessing rules for SVM-based Chinese word segmentation. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN’06).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
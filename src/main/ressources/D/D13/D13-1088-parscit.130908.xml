<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005725">
<title confidence="0.986863">
Russian Stress Prediction using Maximum Entropy Ranking
</title>
<author confidence="0.972869">
Keith Hall Richard Sproat
</author>
<affiliation confidence="0.921852">
Google, Inc
</affiliation>
<address confidence="0.900492">
New York, NY, USA
</address>
<email confidence="0.999613">
{kbhall,rws}@google.com
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999233625">
We explore a model of stress prediction
in Russian using a combination of lo-
cal contextual features and linguistically-
motivated features associated with the
word’s stem and suffix. We frame this
as a ranking problem, where the objec-
tive is to rank the pronunciation with the
correct stress above those with incorrect
stress. We train our models using a simple
Maximum Entropy ranking framework al-
lowing for efficient prediction. An empir-
ical evaluation shows that a model com-
bining the local contextual features and
the linguistically-motivated non-local fea-
tures performs best in identifying both
primary and secondary stress.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896055555555">
In many languages, one component of accu-
rate word pronunciation prediction is predict-
ing the placement of lexical stress. While in
some languages (e.g. Spanish) the lexical stress
system is relatively simple, in others (e.g. En-
glish, Russian) stress prediction is quite compli-
cated. Much as with other work on pronuncia-
tion prediction, previous work on stress assign-
ment has fallen into two camps, namely systems
based on linguistically motivated rules (Church,
1985, for example) and more recently data-
driven techniques where the models are derived
directly from labeled training data (Dou et al.,
2009). In this work, we present a machine-
learned system for predicting Russian stress
which incorporates both data-driven contextual
features as well as linguistically-motivated word
features.
</bodyText>
<sectionHeader confidence="0.992823" genericHeader="introduction">
2 Previous Work on Stress
Prediction
</sectionHeader>
<bodyText confidence="0.999723473684211">
Pronunciation prediction, of which stress pre-
diction is a part, is important for many speech
applications including automatic speech recog-
nition, text-to-speech synthesis, and translit-
eration for, say, machine translation. While
there is by now a sizable literature on pro-
nunciation prediction from spelling (often
termed “grapheme-to-phoneme” conversion),
work that specifically focuses on stress predic-
tion is more limited. One of the best-known
early pieces of work is (Church, 1985), which
uses morphological rules and stress pattern
templates to predict stress in novel words. An-
other early piece of work is (Williams, 1987).
The work we present here is closer in spirit to
data-driven approaches such as (Webster, 2004;
Pearson et al., 2000) and particularly (Dou et
al., 2009), whose features we use in the work
described below.
</bodyText>
<sectionHeader confidence="0.988571" genericHeader="method">
3 Russian Stress Patterns
</sectionHeader>
<bodyText confidence="0.999924285714286">
Russian stress preserves many features of Indo-
European accenting patterns (Halle, 1997). In
order to know the stress of a morphologically
complex word consisting of a stem plus a suf-
fix, one needs to know if the stem has an accent,
and if so on what syllable; and similarly for the
suffix. For words where the stem is accented,
</bodyText>
<page confidence="0.986741">
879
</page>
<bodyText confidence="0.317190333333333">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879–883,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
acc unacc postacc
</bodyText>
<table confidence="0.550477">
DAT SG rOp&apos;Oxy r&apos;OpOAy KOpOn&apos;10
gor&apos;oxu g&apos;orodu korolj&apos;u
DAT PL rOp&apos;OxaM rOpOA&apos;aM KOpOn&apos;FIM
gor&apos;oxam gorod&apos;am korolj&apos;am
&apos;pea&apos; ’town’ ’king’
</table>
<tableCaption confidence="0.987749333333333">
Table 1: Examples of accented, unaccented and
postaccented nouns in Russian, for dative singular
and plural forms.
</tableCaption>
<bodyText confidence="0.999904815789474">
this accent overrides any accent that may oc-
cur on the suffix. With unaccented stems, if
the suffix has an accent, then stress for the
whole word will be on the suffix; if there is
also no stress on the suffix, then a default rule
places stress on the first syllable of the word.
In addition to these patterns, there are also
postaccented words, where accent is placed uni-
formly on the first syllable of the suffix — an
innovation of East and South Slavic languages
(Halle, 1997). These latter cases can be handled
by assigning an accent to the stem, indicating
that it is associated with the syllable after the
stem. Some examples of each of these classes,
from (Halle, 1997, example 11), are given in
Table 1. According to Halle (1997), consid-
ering just nouns, 91.6% are accented (on the
stem), 6.6% are postaccented and 0.8% are un-
accented, with about 1.0% falling into other
patterns.
Stress placement in Russian is important for
speech applications since over and above the
phonetic effects of stress itself (prominence, du-
ration, etc.), the position of stress strongly in-
fluences vowel quality. To take an example
of the lexically unaccented noun rOpOA gorod
‘city’, the genitive singular r&apos;OpOAa g&apos;oroda
/g&amp;quot;Or@d@/ contrasts with the nominative plural
rOpOA&apos;a gorod&apos;a /g@rAd&amp;quot;a/. All non-stressed
/a/ are reduced to schwa — or by most ac-
counts if before the stressed syllable to /A/; see
(Wade, 1992).
The stress patterns of Russian suggest that
useful features for predicting stress might in-
clude (string) prefix and suffix features of the
word in order to capture properties of the stem,
since some stems are (un)accented, or of the
suffix, since some suffixes are accented.
</bodyText>
<sectionHeader confidence="0.969832" genericHeader="method">
4 Maximum Entropy Rankers
</sectionHeader>
<bodyText confidence="0.999960913043478">
Similarly to Dou et al. (2009), we frame the
stress prediction problem as a ranking problem.
For each word, we identify stressable vowels and
generate a set of alternatives, each represent-
ing a different primary stress placement. Some
words also have secondary stress which, if it oc-
curs, always occurs before the primary stressed
syllable. For each primary stress alternative,
we generate all possible secondary stressed al-
ternatives, including an alternative that has no
secondary stress. (In the experiments reported
below we actually consider two conditions: one
where we ignore secondary stress in training
and evaluation; and one where we include it.)
Formally, we model the problem using a Max-
imum Entropy ranking framework similar to
that presented in Collins and Koo (2005). For
each example, xi, we generate the set of possible
stress patterns Yi. Our goal is to rank the items
in Yi such that all of the valid stress patterns
Yz are above all of the invalid stress patterns.
Our objective function is the likelihood, G of
this conditional distribution:
</bodyText>
<equation confidence="0.979611083333333">
G = � p(Yz |Yi, xi) (1)
i
log G = � log p(Yz |Yi, xi) (2)
i
� * e∑k θkfk(y1,x)
y EYi (3)
Z
Z is defined as the sum of the conditional like-
lihood over all hypothesized stress predictions
for example xi:
∑k θkfk(y11,x)
e (4)
</equation>
<bodyText confidence="0.999971571428571">
The objective function in Equation 3 can be
optimized using a gradient-based optimization.
In our case, we use a variety of stochastic gra-
dient descent (SGD) which can be parallelized
for efficient training.
During training, we provide all plausibly cor-
rect primary stress patterns as the positive set
</bodyText>
<equation confidence="0.9933375">
�= log
i
�Z =
y11EYi
</equation>
<page confidence="0.915088">
880
</page>
<bodyText confidence="0.999589666666667">
Y, . At prediction-time, we evaluate all possi-
ble stress predictions and pick the one with the
highest score under the trained model O:
</bodyText>
<equation confidence="0.665429666666667">
arg max �
y&apos;EYi p(y�jYi) = arg max Bkfk(y&apos;, x) (5)
y&apos;EYi k
</equation>
<bodyText confidence="0.999912647058824">
The primary motivation for using Maximum
Entropy rather the ranking-SVM is for efficient
training and inference. Under the above Max-
imum Entropy model, we apply a linear model
to each hypothesis (i.e., we compute the dot-
product) and sort according to this score. This
makes inference (prediction) fast in comparison
to the ranking SVM-based approach proposed
in Dou et al. (2009).
All experiments presented in this paper used
the Iterative Parameter Mixtures distributed
SGD training optimizer (Hall et al., 2010). Un-
der this training approach, per-iteration aver-
aging has a regularization-like effect for sparse
feature spaces. We also experimented with L1-
regularization, but it offered no additional im-
provements.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999748181818182">
The features used in (Dou et al., 2009) are
based on trigrams consisting of a vowel letter,
the preceding consonant letter (if any) and the
following consonant letter (if any). Attached
to each trigram is the stress level of the tri-
gram’s vowel — 1, 2 or 0 (for no stress). For
the English word overdo with the stress pattern
2-0-1, the basic features would be ov:2, ver:0,
and do:1. Notating these pairs as si : ti, where
si is the triple, ti is the stress pattern and i is
the position in the word, the complete feature
set is given in Table 2, where the stress pat-
tern for the whole word is given in the last row
as t1t2...tN. Dou and colleagues use an SVM-
based ranking approach, so they generated fea-
tures for all possible stress assignments for each
word, assigning the highest rank to the correct
assignment. The ranker was then trained to
associate feature combinations to the correct
ranking of alternative stress possibilities.
Given the discussion in Section 3, plausible
additional features are all prefixes and suffixes
</bodyText>
<table confidence="0.998682125">
Substring si, ti
si, i, ti
Context si1, ti
si1si, ti
si+1, ti
sisi+1, ti
si1sisi+1, ti
Stress Pattern t1t2...tN
</table>
<tableCaption confidence="0.994349">
Table 2: Features used in (Dou et al., 2009, Table 2).
</tableCaption>
<figure confidence="0.732198">
vowel a,e,H,O,y,3,1O,H,b1
stop 6,A,r,n,T,K
nasal M,H
fricative CM,w,uLt,x,3,Nc
hard/soft b,b
yo e
semivowel A,s
liquid p,n
affricate 4,4
</figure>
<tableCaption confidence="0.91462025">
Table 3: Abstract phonetic classes used for con-
structing “abstract” versions of a word. Note that
etymologically, and in some ways phonologically, s
v behaves like a semivowel in Russian.
</tableCaption>
<bodyText confidence="0.999959772727273">
of the word, which might be expected to better
capture some of the properties of Russian stress
patterns discussed above, than the much more
local features from (Dou et al., 2009). In this
case for all stress variants of the word we collect
prefixes of length 1 through the length of the
word, and similarly for suffixes, except that for
the stress symbol we treat that together with
the vowel it marks as a single symbol. Thus for
the word gorod&apos;a, all prefixes of the word would
be g, go, gor, goro, gorod, gorod&apos;a.
In addition, we include prefixes and suffixes
of an “abstract” version of the word where most
consonants and vowels have been replaced by
a phonetic class. The mappings for these are
shown in Table 3.
Note that in Russian the vowel a /jO/ is al-
ways stressed, but is rarely written in text: it
is usually spelled as e, whose stressed pronun-
cation is /(j)E/. Since written a is in general
ambiguous between a and e, when we compute
stress variants of a word for the purpose of rank-
</bodyText>
<page confidence="0.995682">
881
</page>
<bodyText confidence="0.9976695">
ing, we include both variants that have е and
ё.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="method">
6 Data
</sectionHeader>
<bodyText confidence="0.999940869565218">
Our data were 2,004,044 fully inflected words
with assigned stress expanded from Zaliznyak&apos;s
Grammatical Dictionary of the Russian Lan-
guage (Zaliznyak, 1977). These were split ran-
domly into 1,904,044 training examples and
100,000 test examples. The 100,000 test ex-
amples obviously contain no forms that were
found in the training data, but most of them
are word forms that derive from lemmata from
which some training data forms are also de-
rived. Given the fact that Russian stress is lex-
ically determined as outlined in Section 3, this
is perfectly reasonable: in order to know how
to stress a form, it is often necessary to have
seen other words that share the same lemma.
Nonetheless, it is also of interest to know how
well the system works on words that do not
share any lemmata with words in the training
data. To that end, we collected a set of 248
forms that shared no lemmata with the train-
ing data. The two sets will be referred to in the
next section as the “shared lemmata” and “no
shared lemmata” sets.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999530611111111">
Table 4 gives word accuracy results for the dif-
ferent feature combinations, as follows: Dou et
al’s features (Dou et al., 2009); our affix fea-
tures; our affix features plus affix features based
on the abstract phonetic class versions of words;
Dou et al’s features plus our affix features; Dou
et al’s features plus our affix features plus the
abstract affix features.
When we consider only primary stress (col-
umn 2 in Table 4, for the shared-lemmata test
data, Dou et al’s features performed the worst
at 97.2% accuracy, with all feature combina-
tions that include the affix features performing
at the same level, 98.7%. For the no-shared-
lemmata test data, using Dou et al’s features
alone achieved an accuracy of 80.6%. The affix
features alone performed worse, at 79.8%, pre-
sumably because it is harder for them to gener-
</bodyText>
<table confidence="0.999411692307692">
Features 1 stress 1+2 stress
shared lemmata
Dou et al 0.972 0.965
Aff 0.987 0.985
Aff+Abstr Aff 0.987 0.985
Dou et al+Aff 0.987 0.986
Dou et al+Aff+Abstr Aff 0.987 0.986
no shared lemmata
Dou et al 0.806 0.798
Aff 0.798 0.782
Aff+Abstr 0.810 0.790
Dou et al+Aff 0.823 0.810
Dou et al+Aff+Abstr Aff 0.839 0.815
</table>
<tableCaption confidence="0.959873333333333">
Table 4: Word accuracies for various feature combi-
nations for both shared lemmata and no-shared lem-
mata conditions. The second column reports results
</tableCaption>
<bodyText confidence="0.970891935483871">
where we consider only primary stress, the third col-
umn results where we also predict secondary stress.
alize to unseen cases, but using the abstract af-
fix features increased the performance to 81.0%,
better than that of using Dou et al’s features
alone. As can be seen combining Dou et al’s
features with various combinations of the affix
features improved the performance further.
For primary and secondary stress prediction
(column 3 in the table), the results are over-
all degraded for most conditions but otherwise
very similar in terms of ranking of the fea-
tures to what we find with primary stress alone.
Note though that for the shared-lemmata con-
dition the results with affix features are almost
as good as for the primary-stress-only case,
whereas there is a significant drop in perfor-
mance for the Dou et al. features. For the
no-shared-lemmata condition, Dou et al.&apos;s fea-
tures fare rather better compared to the affix
features. On the other hand there is a sub-
stantial benefit to combining the features, as
the results for “Dou et al+Aff&amp;quot; and “Dou et
al+Aff+Abstr Aff&amp;quot; show. Note that in the
no-shared-lemmata condition, there is only one
word that is marked with a secondary stress,
and that stress is actually correctly predicted
by all methods. Much of the difference between
the Dou et al. features and the affix condition
can be accounted for by three cases involving
the same root, which the affix condition misas-
</bodyText>
<page confidence="0.991451">
882
</page>
<bodyText confidence="0.999745">
signs secondary stress to.
For the shared-lemmata task however there
were a substantial number of differences, as
one might expect given the nature of the fea-
tures. Comparing just the Dou et al. fea-
tures and the all-features condition, system-
atic benefit for the all-features condition was
found for secondary stress assignment for pro-
ductive prefixes where secondary stress is typ-
ically found. For example, the prefix a3po
(‘aero-’) as in a`3poAMHa&apos;MMKa (`aerodynam-
ics&apos;) typically has secondary stress. This is usu-
ally missed by the Dou et al. features, but is
uniformly correct for the all-features condition.
Since the no-shared-lemmata data set is
small, we tested significance using two permu-
tation tests. The first computed a distribu-
tion of scores for the test data where succes-
sive single test examples were removed. The
second randomly permuted the test data 248
times, after each random permutation, remov-
ing the first ten examples, and computing the
score. Pairwise t-tests between all conditions
for the primary-stress-only and for the primary
plus secondary stress predictions, were highly
significant in all cases.
We also experimented with a postaccent fea-
ture to model the postaccented class of nouns
described in Section 3. For each prefix of the
word, we record whether the following vowel
is stressed or unstressed. This feature yielded
only very slight improvements, and we do not
report these results here.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.998258230769231">
In this paper we have presented a Maximum
Entropy ranking-based approach to Russian
stress prediction. The approach is similar in
spirit to the SVM-based ranking approach pre-
sented in (Dou et al., 2009), but incorporates
additional affix-based features, which are moti-
vated by linguistic analyses of the problem. We
have shown that these additional features gen-
eralize better than the Dou et al. features in
cases where we have seen a related form of the
test word, and that combing the additional fea-
tures with the Dou et al. features always yields
an improvement.
</bodyText>
<sectionHeader confidence="0.998653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999810609756097">
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In Associ-
ation for Computational Linguistics, pages 246–
253.
Michael Collins and Terry Koo. 2005. Discrim-
inative reranking for natural language parsing.
Computational Linguistics, 31:25–69, March.
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,
and Grzegorz Kondrak. 2009. A ranking ap-
proach to stress prediction for letter-to-phoneme
conversion. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
118–126, Suntec, Singapore, August. Association
for Computational Linguistics.
Keith B. Hall, Scott Gilpin, and Gideon Mann.
2010. Mapreduce/bigtable for distributed opti-
mization. In Neural Information Processing Sys-
tems Workshop on Leaning on Cores, Clusters,
and Clouds.
Morris Halle. 1997. On stress and accent in Indo-
European. Language, 73(2):275–313.
Steve Pearson, Roland Kuhn, Steven Fincke, and
Nick Kibre. 2000. Automatic methods for lexical
stress assignment and syllabification. In Interna-
tional Conference on Spoken Language Process-
ing, pages 423–426.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell, Oxford.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
International Conference on Spoken Language
Processing, pages 2573–2576.
Briony Williams. 1987. Word stress assignment
in a text-to-speech synthesis system for British
English. Computer Speech and Language, 2:235–
272.
Andrey Zaliznyak. 1977. Grammaticheskij slovar&apos;
russkogo jazyka. Russkiy Yazik, Moscow.
</reference>
<page confidence="0.99922">
883
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550139">
<title confidence="0.994798">Russian Stress Prediction using Maximum Entropy Ranking</title>
<author confidence="0.866302">Keith Hall Richard</author>
<affiliation confidence="0.584328">Google,</affiliation>
<address confidence="0.835972">New York, NY,</address>
<abstract confidence="0.999540235294118">We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suffix. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for efficient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>Stress assignment in letter to sound rules for speech synthesis.</title>
<date>1985</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>246--253</pages>
<contexts>
<context position="1254" citStr="Church, 1985" startWordPosition="190" endWordPosition="191">xtual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1 Introduction In many languages, one component of accurate word pronunciation prediction is predicting the placement of lexical stress. While in some languages (e.g. Spanish) the lexical stress system is relatively simple, in others (e.g. English, Russian) stress prediction is quite complicated. Much as with other work on pronunciation prediction, previous work on stress assignment has fallen into two camps, namely systems based on linguistically motivated rules (Church, 1985, for example) and more recently datadriven techniques where the models are derived directly from labeled training data (Dou et al., 2009). In this work, we present a machinelearned system for predicting Russian stress which incorporates both data-driven contextual features as well as linguistically-motivated word features. 2 Previous Work on Stress Prediction Pronunciation prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is</context>
</contexts>
<marker>Church, 1985</marker>
<rawString>Kenneth Church. 1985. Stress assignment in letter to sound rules for speech synthesis. In Association for Computational Linguistics, pages 246– 253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--25</pages>
<contexts>
<context position="5772" citStr="Collins and Koo (2005)" startWordPosition="908" endWordPosition="911">alternatives, each representing a different primary stress placement. Some words also have secondary stress which, if it occurs, always occurs before the primary stressed syllable. For each primary stress alternative, we generate all possible secondary stressed alternatives, including an alternative that has no secondary stress. (In the experiments reported below we actually consider two conditions: one where we ignore secondary stress in training and evaluation; and one where we include it.) Formally, we model the problem using a Maximum Entropy ranking framework similar to that presented in Collins and Koo (2005). For each example, xi, we generate the set of possible stress patterns Yi. Our goal is to rank the items in Yi such that all of the valid stress patterns Yz are above all of the invalid stress patterns. Our objective function is the likelihood, G of this conditional distribution: G = � p(Yz |Yi, xi) (1) i log G = � log p(Yz |Yi, xi) (2) i � * e∑k θkfk(y1,x) y EYi (3) Z Z is defined as the sum of the conditional likelihood over all hypothesized stress predictions for example xi: ∑k θkfk(y11,x) e (4) The objective function in Equation 3 can be optimized using a gradient-based optimization. In o</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31:25–69, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Dou</author>
<author>Shane Bergsma</author>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>A ranking approach to stress prediction for letter-to-phoneme conversion.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>118--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1392" citStr="Dou et al., 2009" startWordPosition="210" endWordPosition="213">Introduction In many languages, one component of accurate word pronunciation prediction is predicting the placement of lexical stress. While in some languages (e.g. Spanish) the lexical stress system is relatively simple, in others (e.g. English, Russian) stress prediction is quite complicated. Much as with other work on pronunciation prediction, previous work on stress assignment has fallen into two camps, namely systems based on linguistically motivated rules (Church, 1985, for example) and more recently datadriven techniques where the models are derived directly from labeled training data (Dou et al., 2009). In this work, we present a machinelearned system for predicting Russian stress which incorporates both data-driven contextual features as well as linguistically-motivated word features. 2 Previous Work on Stress Prediction Pronunciation prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion), work that specific</context>
<context position="5020" citStr="Dou et al. (2009)" startWordPosition="791" endWordPosition="794">f the lexically unaccented noun rOpOA gorod ‘city’, the genitive singular r&apos;OpOAa g&apos;oroda /g&amp;quot;Or@d@/ contrasts with the nominative plural rOpOA&apos;a gorod&apos;a /g@rAd&amp;quot;a/. All non-stressed /a/ are reduced to schwa — or by most accounts if before the stressed syllable to /A/; see (Wade, 1992). The stress patterns of Russian suggest that useful features for predicting stress might include (string) prefix and suffix features of the word in order to capture properties of the stem, since some stems are (un)accented, or of the suffix, since some suffixes are accented. 4 Maximum Entropy Rankers Similarly to Dou et al. (2009), we frame the stress prediction problem as a ranking problem. For each word, we identify stressable vowels and generate a set of alternatives, each representing a different primary stress placement. Some words also have secondary stress which, if it occurs, always occurs before the primary stressed syllable. For each primary stress alternative, we generate all possible secondary stressed alternatives, including an alternative that has no secondary stress. (In the experiments reported below we actually consider two conditions: one where we ignore secondary stress in training and evaluation; an</context>
<context position="7178" citStr="Dou et al. (2009)" startWordPosition="1160" endWordPosition="1163"> the positive set �= log i �Z = y11EYi 880 Y, . At prediction-time, we evaluate all possible stress predictions and pick the one with the highest score under the trained model O: arg max � y&apos;EYi p(y�jYi) = arg max Bkfk(y&apos;, x) (5) y&apos;EYi k The primary motivation for using Maximum Entropy rather the ranking-SVM is for efficient training and inference. Under the above Maximum Entropy model, we apply a linear model to each hypothesis (i.e., we compute the dotproduct) and sort according to this score. This makes inference (prediction) fast in comparison to the ranking SVM-based approach proposed in Dou et al. (2009). All experiments presented in this paper used the Iterative Parameter Mixtures distributed SGD training optimizer (Hall et al., 2010). Under this training approach, per-iteration averaging has a regularization-like effect for sparse feature spaces. We also experimented with L1- regularization, but it offered no additional improvements. 5 Features The features used in (Dou et al., 2009) are based on trigrams consisting of a vowel letter, the preceding consonant letter (if any) and the following consonant letter (if any). Attached to each trigram is the stress level of the trigram’s vowel — 1, </context>
<context position="8715" citStr="Dou et al., 2009" startWordPosition="1421" endWordPosition="1424"> for the whole word is given in the last row as t1t2...tN. Dou and colleagues use an SVMbased ranking approach, so they generated features for all possible stress assignments for each word, assigning the highest rank to the correct assignment. The ranker was then trained to associate feature combinations to the correct ranking of alternative stress possibilities. Given the discussion in Section 3, plausible additional features are all prefixes and suffixes Substring si, ti si, i, ti Context si1, ti si1si, ti si+1, ti sisi+1, ti si1sisi+1, ti Stress Pattern t1t2...tN Table 2: Features used in (Dou et al., 2009, Table 2). vowel a,e,H,O,y,3,1O,H,b1 stop 6,A,r,n,T,K nasal M,H fricative CM,w,uLt,x,3,Nc hard/soft b,b yo e semivowel A,s liquid p,n affricate 4,4 Table 3: Abstract phonetic classes used for constructing “abstract” versions of a word. Note that etymologically, and in some ways phonologically, s v behaves like a semivowel in Russian. of the word, which might be expected to better capture some of the properties of Russian stress patterns discussed above, than the much more local features from (Dou et al., 2009). In this case for all stress variants of the word we collect prefixes of length 1 t</context>
<context position="11262" citStr="Dou et al., 2009" startWordPosition="1871" endWordPosition="1874"> is perfectly reasonable: in order to know how to stress a form, it is often necessary to have seen other words that share the same lemma. Nonetheless, it is also of interest to know how well the system works on words that do not share any lemmata with words in the training data. To that end, we collected a set of 248 forms that shared no lemmata with the training data. The two sets will be referred to in the next section as the “shared lemmata” and “no shared lemmata” sets. 7 Results Table 4 gives word accuracy results for the different feature combinations, as follows: Dou et al’s features (Dou et al., 2009); our affix features; our affix features plus affix features based on the abstract phonetic class versions of words; Dou et al’s features plus our affix features; Dou et al’s features plus our affix features plus the abstract affix features. When we consider only primary stress (column 2 in Table 4, for the shared-lemmata test data, Dou et al’s features performed the worst at 97.2% accuracy, with all feature combinations that include the affix features performing at the same level, 98.7%. For the no-sharedlemmata test data, using Dou et al’s features alone achieved an accuracy of 80.6%. The af</context>
</contexts>
<marker>Dou, Bergsma, Jiampojamarn, Kondrak, 2009</marker>
<rawString>Qing Dou, Shane Bergsma, Sittichai Jiampojamarn, and Grzegorz Kondrak. 2009. A ranking approach to stress prediction for letter-to-phoneme conversion. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 118–126, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith B Hall</author>
<author>Scott Gilpin</author>
<author>Gideon Mann</author>
</authors>
<title>Mapreduce/bigtable for distributed optimization.</title>
<date>2010</date>
<booktitle>In Neural Information Processing Systems Workshop on Leaning on Cores, Clusters, and Clouds.</booktitle>
<contexts>
<context position="7312" citStr="Hall et al., 2010" startWordPosition="1179" endWordPosition="1182"> the highest score under the trained model O: arg max � y&apos;EYi p(y�jYi) = arg max Bkfk(y&apos;, x) (5) y&apos;EYi k The primary motivation for using Maximum Entropy rather the ranking-SVM is for efficient training and inference. Under the above Maximum Entropy model, we apply a linear model to each hypothesis (i.e., we compute the dotproduct) and sort according to this score. This makes inference (prediction) fast in comparison to the ranking SVM-based approach proposed in Dou et al. (2009). All experiments presented in this paper used the Iterative Parameter Mixtures distributed SGD training optimizer (Hall et al., 2010). Under this training approach, per-iteration averaging has a regularization-like effect for sparse feature spaces. We also experimented with L1- regularization, but it offered no additional improvements. 5 Features The features used in (Dou et al., 2009) are based on trigrams consisting of a vowel letter, the preceding consonant letter (if any) and the following consonant letter (if any). Attached to each trigram is the stress level of the trigram’s vowel — 1, 2 or 0 (for no stress). For the English word overdo with the stress pattern 2-0-1, the basic features would be ov:2, ver:0, and do:1. </context>
</contexts>
<marker>Hall, Gilpin, Mann, 2010</marker>
<rawString>Keith B. Hall, Scott Gilpin, and Gideon Mann. 2010. Mapreduce/bigtable for distributed optimization. In Neural Information Processing Systems Workshop on Leaning on Cores, Clusters, and Clouds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris Halle</author>
</authors>
<title>On stress and accent in IndoEuropean.</title>
<date>1997</date>
<journal>Language,</journal>
<volume>73</volume>
<issue>2</issue>
<contexts>
<context position="2566" citStr="Halle, 1997" startWordPosition="387" endWordPosition="388">oneme” conversion), work that specifically focuses on stress prediction is more limited. One of the best-known early pieces of work is (Church, 1985), which uses morphological rules and stress pattern templates to predict stress in novel words. Another early piece of work is (Williams, 1987). The work we present here is closer in spirit to data-driven approaches such as (Webster, 2004; Pearson et al., 2000) and particularly (Dou et al., 2009), whose features we use in the work described below. 3 Russian Stress Patterns Russian stress preserves many features of IndoEuropean accenting patterns (Halle, 1997). In order to know the stress of a morphologically complex word consisting of a stem plus a suffix, one needs to know if the stem has an accent, and if so on what syllable; and similarly for the suffix. For words where the stem is accented, 879 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879–883, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics acc unacc postacc DAT SG rOp&apos;Oxy r&apos;OpOAy KOpOn&apos;10 gor&apos;oxu g&apos;orodu korolj&apos;u DAT PL rOp&apos;OxaM rOpOA&apos;aM KOpOn&apos;FIM gor&apos;oxam gorod&apos;am korolj&apos;am &apos;pea&apos; ’town’ ’king’</context>
<context position="3957" citStr="Halle, 1997" startWordPosition="621" endWordPosition="622">ith unaccented stems, if the suffix has an accent, then stress for the whole word will be on the suffix; if there is also no stress on the suffix, then a default rule places stress on the first syllable of the word. In addition to these patterns, there are also postaccented words, where accent is placed uniformly on the first syllable of the suffix — an innovation of East and South Slavic languages (Halle, 1997). These latter cases can be handled by assigning an accent to the stem, indicating that it is associated with the syllable after the stem. Some examples of each of these classes, from (Halle, 1997, example 11), are given in Table 1. According to Halle (1997), considering just nouns, 91.6% are accented (on the stem), 6.6% are postaccented and 0.8% are unaccented, with about 1.0% falling into other patterns. Stress placement in Russian is important for speech applications since over and above the phonetic effects of stress itself (prominence, duration, etc.), the position of stress strongly influences vowel quality. To take an example of the lexically unaccented noun rOpOA gorod ‘city’, the genitive singular r&apos;OpOAa g&apos;oroda /g&amp;quot;Or@d@/ contrasts with the nominative plural rOpOA&apos;a gorod&apos;a /</context>
</contexts>
<marker>Halle, 1997</marker>
<rawString>Morris Halle. 1997. On stress and accent in IndoEuropean. Language, 73(2):275–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Pearson</author>
<author>Roland Kuhn</author>
<author>Steven Fincke</author>
<author>Nick Kibre</author>
</authors>
<title>Automatic methods for lexical stress assignment and syllabification.</title>
<date>2000</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<pages>423--426</pages>
<contexts>
<context position="2364" citStr="Pearson et al., 2000" startWordPosition="354" endWordPosition="357">speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion), work that specifically focuses on stress prediction is more limited. One of the best-known early pieces of work is (Church, 1985), which uses morphological rules and stress pattern templates to predict stress in novel words. Another early piece of work is (Williams, 1987). The work we present here is closer in spirit to data-driven approaches such as (Webster, 2004; Pearson et al., 2000) and particularly (Dou et al., 2009), whose features we use in the work described below. 3 Russian Stress Patterns Russian stress preserves many features of IndoEuropean accenting patterns (Halle, 1997). In order to know the stress of a morphologically complex word consisting of a stem plus a suffix, one needs to know if the stem has an accent, and if so on what syllable; and similarly for the suffix. For words where the stem is accented, 879 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879–883, Seattle, Washington, USA, 18-21 October 2013. c�20</context>
</contexts>
<marker>Pearson, Kuhn, Fincke, Kibre, 2000</marker>
<rawString>Steve Pearson, Roland Kuhn, Steven Fincke, and Nick Kibre. 2000. Automatic methods for lexical stress assignment and syllabification. In International Conference on Spoken Language Processing, pages 423–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Wade</author>
</authors>
<title>A Comprehensive Russian Grammar.</title>
<date>1992</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4687" citStr="Wade, 1992" startWordPosition="738" endWordPosition="739">), 6.6% are postaccented and 0.8% are unaccented, with about 1.0% falling into other patterns. Stress placement in Russian is important for speech applications since over and above the phonetic effects of stress itself (prominence, duration, etc.), the position of stress strongly influences vowel quality. To take an example of the lexically unaccented noun rOpOA gorod ‘city’, the genitive singular r&apos;OpOAa g&apos;oroda /g&amp;quot;Or@d@/ contrasts with the nominative plural rOpOA&apos;a gorod&apos;a /g@rAd&amp;quot;a/. All non-stressed /a/ are reduced to schwa — or by most accounts if before the stressed syllable to /A/; see (Wade, 1992). The stress patterns of Russian suggest that useful features for predicting stress might include (string) prefix and suffix features of the word in order to capture properties of the stem, since some stems are (un)accented, or of the suffix, since some suffixes are accented. 4 Maximum Entropy Rankers Similarly to Dou et al. (2009), we frame the stress prediction problem as a ranking problem. For each word, we identify stressable vowels and generate a set of alternatives, each representing a different primary stress placement. Some words also have secondary stress which, if it occurs, always o</context>
</contexts>
<marker>Wade, 1992</marker>
<rawString>Terence Wade. 1992. A Comprehensive Russian Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Webster</author>
</authors>
<title>Improving letterto-pronunciation accuracy with automatic morphologically-based stress prediction.</title>
<date>2004</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<pages>2573--2576</pages>
<contexts>
<context position="2341" citStr="Webster, 2004" startWordPosition="352" endWordPosition="353">ding automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion), work that specifically focuses on stress prediction is more limited. One of the best-known early pieces of work is (Church, 1985), which uses morphological rules and stress pattern templates to predict stress in novel words. Another early piece of work is (Williams, 1987). The work we present here is closer in spirit to data-driven approaches such as (Webster, 2004; Pearson et al., 2000) and particularly (Dou et al., 2009), whose features we use in the work described below. 3 Russian Stress Patterns Russian stress preserves many features of IndoEuropean accenting patterns (Halle, 1997). In order to know the stress of a morphologically complex word consisting of a stem plus a suffix, one needs to know if the stem has an accent, and if so on what syllable; and similarly for the suffix. For words where the stem is accented, 879 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879–883, Seattle, Washington, USA, 1</context>
</contexts>
<marker>Webster, 2004</marker>
<rawString>Gabriel Webster. 2004. Improving letterto-pronunciation accuracy with automatic morphologically-based stress prediction. In International Conference on Spoken Language Processing, pages 2573–2576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Briony Williams</author>
</authors>
<title>Word stress assignment in a text-to-speech synthesis system for British English. Computer Speech and Language,</title>
<date>1987</date>
<pages>2--235</pages>
<contexts>
<context position="2246" citStr="Williams, 1987" startWordPosition="336" endWordPosition="337">prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion), work that specifically focuses on stress prediction is more limited. One of the best-known early pieces of work is (Church, 1985), which uses morphological rules and stress pattern templates to predict stress in novel words. Another early piece of work is (Williams, 1987). The work we present here is closer in spirit to data-driven approaches such as (Webster, 2004; Pearson et al., 2000) and particularly (Dou et al., 2009), whose features we use in the work described below. 3 Russian Stress Patterns Russian stress preserves many features of IndoEuropean accenting patterns (Halle, 1997). In order to know the stress of a morphologically complex word consisting of a stem plus a suffix, one needs to know if the stem has an accent, and if so on what syllable; and similarly for the suffix. For words where the stem is accented, 879 Proceedings of the 2013 Conference </context>
</contexts>
<marker>Williams, 1987</marker>
<rawString>Briony Williams. 1987. Word stress assignment in a text-to-speech synthesis system for British English. Computer Speech and Language, 2:235– 272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrey Zaliznyak</author>
</authors>
<title>Grammaticheskij slovar&apos; russkogo jazyka.</title>
<date>1977</date>
<location>Russkiy Yazik, Moscow.</location>
<contexts>
<context position="10266" citStr="Zaliznyak, 1977" startWordPosition="1691" endWordPosition="1692">rd where most consonants and vowels have been replaced by a phonetic class. The mappings for these are shown in Table 3. Note that in Russian the vowel a /jO/ is always stressed, but is rarely written in text: it is usually spelled as e, whose stressed pronuncation is /(j)E/. Since written a is in general ambiguous between a and e, when we compute stress variants of a word for the purpose of rank881 ing, we include both variants that have е and ё. 6 Data Our data were 2,004,044 fully inflected words with assigned stress expanded from Zaliznyak&apos;s Grammatical Dictionary of the Russian Language (Zaliznyak, 1977). These were split randomly into 1,904,044 training examples and 100,000 test examples. The 100,000 test examples obviously contain no forms that were found in the training data, but most of them are word forms that derive from lemmata from which some training data forms are also derived. Given the fact that Russian stress is lexically determined as outlined in Section 3, this is perfectly reasonable: in order to know how to stress a form, it is often necessary to have seen other words that share the same lemma. Nonetheless, it is also of interest to know how well the system works on words tha</context>
</contexts>
<marker>Zaliznyak, 1977</marker>
<rawString>Andrey Zaliznyak. 1977. Grammaticheskij slovar&apos; russkogo jazyka. Russkiy Yazik, Moscow.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
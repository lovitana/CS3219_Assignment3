<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014541">
<title confidence="0.982068">
Improving Statistical Machine Translation with Word Class Models
</title>
<author confidence="0.998409">
Joern Wuebker, Stephan Peitz, Felix Rietig and Hermann Ney
</author>
<affiliation confidence="0.904465333333333">
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.985887">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.985733" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.741087444444445">
Automatically clustering words from a mono-
lingual or bilingual training corpus into
classes is a widely used technique in statisti-
cal natural language processing. We present
a very simple and easy to implement method
for using these word classes to improve trans-
lation quality. It can be applied across differ-
ent machine translation paradigms and with
arbitrary types of models. We show its ef-
ficacy on a small German—*English and a
larger French—*German translation task with
both standard phrase-based and hierarchical
phrase-based translation systems for a com-
mon set of models. Our results show that with
word class models, the baseline can be im-
proved by up to 1.4% BLEU and 1.0% TER
on the French—*German task and 0.3% BLEU
and 1.1% TER on the German—*English task.
</figureCaption>
<sectionHeader confidence="0.99808" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834170212766">
Data sparsity is one of the major problems for statis-
tical learning methods in natural language process-
ing (NLP) today. Even with the huge training data
sets available in some tasks, for many phenomena
that need to be modeled only few training instances
can be observed. This is partly due to the large vo-
cabularies of natural languages. One possiblity to
reduce the sparsity for model estimation is to re-
duce the vocabulary size. By clustering the vocab-
ulary into a fixed number of word classes, it is pos-
sible to train models that are less prone to sparsity
issues. This work investigates the performance of
standard models used in statistical machine transla-
tion when they are trained on automatically learned
word classes rather than the actual word identities.
In the popular tooklit GIZA++ (Och and Ney,
2003), word classes are an essential ingredient to
model alignment probabilities with the HMM or
IBM translation models. It contains the mkcls tool
(Och, 1999), which can automatically cluster the vo-
cabulary into classes.
Using this tool, we propose to re-parameterize the
standard models used in statistical machine transla-
tion (SMT), which are usually conditioned on word
identities rather than word classes. The idea is that
this should lead to a smoother distribution, which
is more reliable due to less sparsity. Here, we fo-
cus on the phrase-based and lexical channel models
in both directions, simple count models identifying
frequency thresholds, lexicalized reordering models
and an n-gram language model. Although our re-
sults show that it is not a good idea to replace the
original models, we argue that adding them to the
log-linear feature combination can improve transla-
tion quality. They can easily be computed for dif-
ferent translation paradigms and arbitrary models.
Training and decoding is possible without or with
only little change to the code base.
Our experiments are conducted on a medium-
sized French—*German task and a small
German—*English task and with both phrase-
based and hierarchical phrase-based translation
decoders. By using word class models, we can
improve our respective baselines by 1.4% BLEU and
1.0% TER on the French—*German task and 0.3%
BLEU and 1.1% TER on the German—*English task.
Training an additional language model for trans-
</bodyText>
<page confidence="0.922447">
1377
</page>
<bodyText confidence="0.944662875">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377–1381,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
lation based on word classes has been proposed in
(Wuebker et al., 2012; Mediani et al., 2012; Koehn
and Hoang, 2007). In addition to the reduced spar-
sity, an advantage of the smaller vocabulary is that
longer n-gram context can be modeled efficiently.
Mathematically, our idea is equivalent to a special
case of the Factored Translation Models proposed
by Koehn and Hoang (2007). We will go into more
detail in Section 4. Also related to our work, Cherry
(2013) proposes to parameterize a hierarchical re-
ordering model with sparse features that are condi-
tioned on word classes trained with mkcls. How-
ever, the features are trained with MIRA rather than
estimated by relative frequencies.
</bodyText>
<sectionHeader confidence="0.986269" genericHeader="method">
2 Word Class Models
</sectionHeader>
<subsectionHeader confidence="0.955488">
2.1 Standard Models
</subsectionHeader>
<bodyText confidence="0.999931344827586">
The translation model of most phrase-based and hi-
erarchical phrase-based SMT systems is parameter-
ized by two phrasal and two lexical channel models
(Koehn et al., 2003) which are estimated as relative
frequencies. Their counts are extracted heuristically
from a word aligned bilingual training corpus.
In addition to the four channel models, our base-
line contains binary count features that fire, if the
extraction count of the corresponding phrase pair is
greater or equal to a given threshold τ. We use the
thresholds τ = 12, 3, 4}.
Our phrase-based baseline contains the hierarchi-
cal reordering model (HRM) described by Galley
and Manning (2008). Similar to (Cherry et al.,
2012), we apply it in both translation directions
with separate scaling factors for the three orientation
classes, leading to a total of six feature weights.
An n-gram language model (LM) is another im-
portant feature of our translation systems. The
baselines apply 4-gram LMs trained by the SRILM
toolkit (Stolcke, 2002) with interpolated modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
The smaller vocabulary size allows us to efficiently
model larger context, so in addition to the 4-gram
LM, we also train a 7-gram LM based on word
classes. In contrast to an LM of the same size trained
on word identities, the increase in computational re-
sources needed for translation is negligible for the
7-gram word class LM (wcLM).
</bodyText>
<subsectionHeader confidence="0.997396">
2.2 Training
</subsectionHeader>
<bodyText confidence="0.999972928571428">
By replacing the words on both source and target
side of the training data with their respective word
classes and keeping the word alignment unchanged,
all of the above models can easily be trained con-
ditioned on word classes by using the same training
procedure as usual. We end up with two separate
model files, usually in the form of large tables, one
with word identities and one with classes. Next, we
sort both tables by their word classes. By walking
through both sorted tables simultaneously, we can
then efficiently augment the standard model file with
an additonal feature (or additional features) based on
word classes. The word class LM is directly passed
on to the decoder.
</bodyText>
<subsectionHeader confidence="0.996331">
2.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999864666666667">
The decoder searches for the best translation given
a set of models hm(eI1, sK1 , fJ1 ) by maximizing the
log-linear feature score (Och and Ney, 2004):
</bodyText>
<equation confidence="0.999674666666667">
�e1 = arg max � M
ˆI λmhm(eI 1,sK 1 , fJ 1 ) , (1)
I,ei m=1
</equation>
<bodyText confidence="0.999953571428571">
where fJ1 = f1 . . . fJ is the source sentence, eI1 =
e1 ... eI the target sentence and sK1 = s1 ... sK the
hidden alignment or derivation.
All the above mentioned models can easily be in-
tegrated into this framework as additional features
hm. The feature weights λm are tuned with mini-
mum error rate training (MERT) (Och, 2003).
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993103">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999594727272727">
Our experiments are performed on a
French—*German task. In addition to some
project-internal data, we train the system on the data
provided for the WMT 2012 shared task1. Both the
dev and the test set are composed of a mixture
of broadcast news and broadcast conversations
crawled from the web and have two references.
Table 1 shows the data statistics.
To confirm our results we also run experiments
on the German—*English task of the IWSLT 2012
evaluation campaign2.
</bodyText>
<footnote confidence="0.9999645">
1http://www.statmt.org/wmt12/
2http://hltc.cs.ust.hk/iwslt/
</footnote>
<page confidence="0.84548">
1378
</page>
<table confidence="0.999861142857143">
French German
train Sentences 1.9M
Running Words 57M 50M
dev Sentences 1900
Running Words 61K 55K
test Sentences 2037
Running Words 60K 54K
</table>
<tableCaption confidence="0.991122">
Table 1: Corpus statistics for the French→German task.
The running word counts for the German side of dev and
test are averaged over both references.
</tableCaption>
<subsectionHeader confidence="0.997169">
3.2 Setup
</subsectionHeader>
<bodyText confidence="0.998504363636364">
In the French→German task, our baseline is a stan-
dard phrase-based system augmented with the hier-
archical reordering model (HRM) described in Sec-
tion 2.1. The language model is a 4-gram LM
trained on all German monolingual sources provided
for WMT 2012. For the class-based models, we
run mkcls on the source and target side of the
bilingual training data to cluster the vocabulary into
100 classes each. This clustering is used to train
the models described above for word classes on the
same training data as their counterparts based on
word identity. This also holds for the wcLM, which
is a 4-gram LM trained on the same data as the base-
line LM. Further, the smaller vocabulary allows us
to build an additional wcLM with a 7-gram context
length. On this task we also run additional experi-
ments with 200 and 500 classes.
On the German→English task, we evaluate our
method for both a standard phrase-based and the hi-
erarchical phrase-based baseline. Again, the phrase-
based baseline contains the HRM model. As bilin-
gual training data we use the TED talks, which we
cluster into 100 classes on both source and target
side. The 4-gram LM is trained on the TED, Eu-
roparl and news-commentary corpora. On this data
set, we directly use a 7-gram wcLM.
In all setups, the feature weights are optimized
with MERT. Results are reported in BLEU (Pap-
ineni et al., 2002) and TER (Snover et al., 2006),
confidence level computation is based on (Koehn,
2004). Our experiments are conducted with the open
source toolkit Jane (Wuebker et al., 2012; Vilar et
al., 2010).
</bodyText>
<table confidence="0.999560615384615">
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
-TM +wcTM 21.2 64.2 24.7 59.5
-LM +wcLM 22.2 62.9 25.9 58.9
-HRM +wcHRM 24.6 61.9 27.5 58.1
phrase-based 24.6 61.8 27.8 57.6
+ wcTM 24.7 61.4 28.1 57.1
+ wcLM 24.9 61.2 28.4 57.1
+ wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡
+ wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡
+ wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡
+ wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡
</table>
<tableCaption confidence="0.85375325">
Table 2: BLEU and TER results on the French→German
task. Results marked with ‡ are statistically significant
with 95% confidence, results marked with † with 90%
confidence. -X +wcX denote the systems, where the
model X in the baseline is replaced by its word class
counterpart. The 7-gram word class LM is denoted
as wcLM7. wcModelsX denotes all word class models
trained on X classes.
</tableCaption>
<subsectionHeader confidence="0.820915">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999539625">
Results for the French→German task are given in
Table 2. In a first set of experiments we replaced one
of the standard TM, LM and HRM models by the
same model based on word classes. Unsurprisingly,
this degrades performance with different levels of
severity. The strongest degradation can be seen
when replacing the TM, while replacing the HRM
only leads to a small drop in performance. However,
when the word class models are added as additional
features to the baseline, we observe improvements.
The wcTM yields 0.3% BLEU and 0.5% TER on
test. By adding the 4-gram wcLM, we get another
0.3% BLEU and the wcHRM shows further improve-
ments of 0.5% BLEU and 0.2% TER. Extending the
context length of the wcLM to 7-grams gives an ad-
ditional boost, reaching a total gain over the baseline
of 1.4% BLEU and 1.0% TER. Using 200 classes
instead of 100 seems to perform slightly better on
test, but with 500 classes, translation quality de-
grades again.
On the German→English task, the results shown
in Table 3 are similar in TER, but less pronounced
in BLEU. Here we are able to improve over the
phrase-based baseline by 0.3% BLEU and 1.1% TER
</bodyText>
<page confidence="0.98656">
1379
</page>
<table confidence="0.999544">
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
phrase-based 30.2 49.6 28.6 51.6
+ wcTM 30.2 49.2 28.9 51.3
+ wcLM7 30.5 48.3‡ 29.0 50.6†
+ wcHRM 30.8 48.3‡ 28.9 50.5‡
hiero 29.6 50.3 27.9 52.5
+ wcTM 29.8 50.3 28.1 52.3
+ wcLM7 30.0 49.8 28.2 51.7
</table>
<tableCaption confidence="0.9833875">
Table 3: BLEU and TER results on the German→English
task. Results marked with ‡ are statistically significant
with 95% confidence, results marked with † with 90%
confidence.
</tableCaption>
<bodyText confidence="0.999558">
by adding the wcTM, the 7-gram wcLM and the
wcHRM. With the hierarchical decoder we gain
0.3% BLEU and 0.8% TER by adding the wcTM and
the 7-gram wcLM.
</bodyText>
<sectionHeader confidence="0.945424" genericHeader="method">
4 Equivalence to Factored Translation
</sectionHeader>
<bodyText confidence="0.99994136">
Koehn and Hoang (2007) propose to integrate differ-
ent levels of annotation (e.g. morphologial analysis)
as factors into the translation process. Here, the sur-
face form of the source word is analyzed to produce
the factors, which are then translated and finally the
surface form of the target word is generated from the
target factors. Although the translations of the fac-
tors operate on the same phrase segmentation, they
are assumed to be independent. In practice this is
done by phrase expansion, which generates a joint
phrase table as the cross product from the phrase ta-
bles of the individual factors.
In contrast, in this work each word is mapped to
a single class, which means that when we have se-
lected a translation option for the surface form, the
target side on the word class level is predetermined.
Thus, no phrase expansion or generation steps are
necessary to incorporate the word class information.
The phrase table can simply be extended with addi-
tional scores, keeping the set of phrases constant.
Although the implementation is simpler, our ap-
proach is mathematically equivalent to a special
case of the factored translation framework, which is
shown in Figure 1. The generation step from target
word e to its target class c(e) assigns all probability
</bodyText>
<figure confidence="0.980450333333333">
Input Output
translation
word f word e
generation
class c(f) class c(e)
translation
</figure>
<figureCaption confidence="0.999024333333333">
Figure 1: The factored translation model equivalent to
our approach. The generation step assigns all probability
mass to a single event: pge,2(c(e)|e) = 1.
</figureCaption>
<bodyText confidence="0.914109">
mass to a single event:
</bodyText>
<equation confidence="0.980679333333333">
�
1, if c = c(e)
pgen(c|e) = (2) 0, else
</equation>
<sectionHeader confidence="0.995069" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999155">
We have presented a simple and very easy to im-
plement method to make use of word clusters for
improving machine translation quality. It is appli-
cable across different paradigms and for arbitrary
types of models. Depending on the model type,
it requires little or no change to the training and
decoding software. We have shown the efficacy
of this method on two translation tasks and with
both the standard phrase-based and the hierarchi-
cal phrase-based translation paradigm. It was ap-
plied to relative frequency translation probabilities,
the n-gram language model and a hierarchical re-
ordering model. In our experiments, the baseline
is improved by 1.4% BLEU and 1.0% TER on the
French→German task and by 0.3% BLEU and 1.1%
TER on the German→English task.
In future work we plan to apply our method to a
wider range of languages. Intuitively, it should be
most effective for morphologically rich languages,
which naturally have stronger sparsity problems.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.885181714285714">
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation. The research leading to these
results has also received funding from the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement no 287658.
analysis
</bodyText>
<page confidence="0.951899">
1380
</page>
<sectionHeader confidence="0.988091" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999781147727273">
Stanley F. Chen and Joshuo Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
August.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding. In Proceedings of the 7th
Workshop on Statistical Machine Translation, WMT
’12, pages 200–209, Montral, Canada.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In The 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), pages 22–
31, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847–855, Honolulu, Hawaii, USA, October.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868–876, Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of the 2003
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-03), pages
127–133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 388–395, Barcelona, Spain, July.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunah Cho, Teresa Herrmann, and Alex
Waibel. 2012. The kit translation systems for iwslt
2012. In Proceedings of the International Work-
shop for Spoken Language Translation (IWSLT 2012),
Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449, De-
cember.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. of the Ninth Conf.
of the Europ. Chapter of the Association of Compu-
tational Linguistics, pages 71–76, Bergen, Norway,
June.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311–318, Philadelphia, Penn-
sylvania, USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA, Au-
gust.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901–904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262–270, Upp-
sala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mumbai,
India, December.
</reference>
<page confidence="0.992977">
1381
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.393986">
<title confidence="0.999885">Improving Statistical Machine Translation with Word Class Models</title>
<author confidence="0.737719">Joern Wuebker</author>
<author confidence="0.737719">Stephan Peitz</author>
<author confidence="0.737719">Felix Rietig</author>
<author confidence="0.737719">Hermann</author>
<affiliation confidence="0.6416235">Human Language Technology and Pattern Recognition RWTH Aachen</affiliation>
<address confidence="0.877632">Aachen,</address>
<email confidence="0.998279"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.997618">Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efon a small and a translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be imby up to 1.4% 1.0% the task and 0.3% 1.1% the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshuo Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="5328" citStr="Chen and Goodman, 1998" startWordPosition="841" endWordPosition="844">e pair is greater or equal to a given threshold τ. We use the thresholds τ = 12, 3, 4}. Our phrase-based baseline contains the hierarchical reordering model (HRM) described by Galley and Manning (2008). Similar to (Cherry et al., 2012), we apply it in both translation directions with separate scaling factors for the three orientation classes, leading to a total of six feature weights. An n-gram language model (LM) is another important feature of our translation systems. The baselines apply 4-gram LMs trained by the SRILM toolkit (Stolcke, 2002) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). The smaller vocabulary size allows us to efficiently model larger context, so in addition to the 4-gram LM, we also train a 7-gram LM based on word classes. In contrast to an LM of the same size trained on word identities, the increase in computational resources needed for translation is negligible for the 7-gram word class LM (wcLM). 2.2 Training By replacing the words on both source and target side of the training data with their respective word classes and keeping the word alignment unchanged, all of the above models can easily be trained conditioned on word classes by using the same trai</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshuo Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>On Hierarchical Re-ordering and Permutation Parsing for Phrase-based Decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>200--209</pages>
<location>Montral, Canada.</location>
<contexts>
<context position="4940" citStr="Cherry et al., 2012" startWordPosition="782" endWordPosition="785">rase-based SMT systems is parameterized by two phrasal and two lexical channel models (Koehn et al., 2003) which are estimated as relative frequencies. Their counts are extracted heuristically from a word aligned bilingual training corpus. In addition to the four channel models, our baseline contains binary count features that fire, if the extraction count of the corresponding phrase pair is greater or equal to a given threshold τ. We use the thresholds τ = 12, 3, 4}. Our phrase-based baseline contains the hierarchical reordering model (HRM) described by Galley and Manning (2008). Similar to (Cherry et al., 2012), we apply it in both translation directions with separate scaling factors for the three orientation classes, leading to a total of six feature weights. An n-gram language model (LM) is another important feature of our translation systems. The baselines apply 4-gram LMs trained by the SRILM toolkit (Stolcke, 2002) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). The smaller vocabulary size allows us to efficiently model larger context, so in addition to the 4-gram LM, we also train a 7-gram LM based on word classes. In contrast to an LM of the same size trained on word</context>
</contexts>
<marker>Cherry, Moore, Quirk, 2012</marker>
<rawString>Colin Cherry, Robert C. Moore, and Chris Quirk. 2012. On Hierarchical Re-ordering and Permutation Parsing for Phrase-based Decoding. In Proceedings of the 7th Workshop on Statistical Machine Translation, WMT ’12, pages 200–209, Montral, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</booktitle>
<pages>22--31</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3991" citStr="Cherry (2013)" startWordPosition="632" endWordPosition="633">ethods in Natural Language Processing, pages 1377–1381, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics lation based on word classes has been proposed in (Wuebker et al., 2012; Mediani et al., 2012; Koehn and Hoang, 2007). In addition to the reduced sparsity, an advantage of the smaller vocabulary is that longer n-gram context can be modeled efficiently. Mathematically, our idea is equivalent to a special case of the Factored Translation Models proposed by Koehn and Hoang (2007). We will go into more detail in Section 4. Also related to our work, Cherry (2013) proposes to parameterize a hierarchical reordering model with sparse features that are conditioned on word classes trained with mkcls. However, the features are trained with MIRA rather than estimated by relative frequencies. 2 Word Class Models 2.1 Standard Models The translation model of most phrase-based and hierarchical phrase-based SMT systems is parameterized by two phrasal and two lexical channel models (Koehn et al., 2003) which are estimated as relative frequencies. Their counts are extracted heuristically from a word aligned bilingual training corpus. In addition to the four channel</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), pages 22– 31, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>847--855</pages>
<location>Honolulu, Hawaii, USA,</location>
<contexts>
<context position="4906" citStr="Galley and Manning (2008)" startWordPosition="776" endWordPosition="779">f most phrase-based and hierarchical phrase-based SMT systems is parameterized by two phrasal and two lexical channel models (Koehn et al., 2003) which are estimated as relative frequencies. Their counts are extracted heuristically from a word aligned bilingual training corpus. In addition to the four channel models, our baseline contains binary count features that fire, if the extraction count of the corresponding phrase pair is greater or equal to a given threshold τ. We use the thresholds τ = 12, 3, 4}. Our phrase-based baseline contains the hierarchical reordering model (HRM) described by Galley and Manning (2008). Similar to (Cherry et al., 2012), we apply it in both translation directions with separate scaling factors for the three orientation classes, leading to a total of six feature weights. An n-gram language model (LM) is another important feature of our translation systems. The baselines apply 4-gram LMs trained by the SRILM toolkit (Stolcke, 2002) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). The smaller vocabulary size allows us to efficiently model larger context, so in addition to the 4-gram LM, we also train a 7-gram LM based on word classes. In contrast to an L</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 847–855, Honolulu, Hawaii, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>868--876</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3646" citStr="Koehn and Hoang, 2007" startWordPosition="572" endWordPosition="575">d with both phrasebased and hierarchical phrase-based translation decoders. By using word class models, we can improve our respective baselines by 1.4% BLEU and 1.0% TER on the French—*German task and 0.3% BLEU and 1.1% TER on the German—*English task. Training an additional language model for trans1377 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics lation based on word classes has been proposed in (Wuebker et al., 2012; Mediani et al., 2012; Koehn and Hoang, 2007). In addition to the reduced sparsity, an advantage of the smaller vocabulary is that longer n-gram context can be modeled efficiently. Mathematically, our idea is equivalent to a special case of the Factored Translation Models proposed by Koehn and Hoang (2007). We will go into more detail in Section 4. Also related to our work, Cherry (2013) proposes to parameterize a hierarchical reordering model with sparse features that are conditioned on word classes trained with mkcls. However, the features are trained with MIRA rather than estimated by relative frequencies. 2 Word Class Models 2.1 Stan</context>
<context position="11864" citStr="Koehn and Hoang (2007)" startWordPosition="1985" endWordPosition="1988"> BLEU TER BLEU TER [%] [%] [%] [%] phrase-based 30.2 49.6 28.6 51.6 + wcTM 30.2 49.2 28.9 51.3 + wcLM7 30.5 48.3‡ 29.0 50.6† + wcHRM 30.8 48.3‡ 28.9 50.5‡ hiero 29.6 50.3 27.9 52.5 + wcTM 29.8 50.3 28.1 52.3 + wcLM7 30.0 49.8 28.2 51.7 Table 3: BLEU and TER results on the German→English task. Results marked with ‡ are statistically significant with 95% confidence, results marked with † with 90% confidence. by adding the wcTM, the 7-gram wcLM and the wcHRM. With the hierarchical decoder we gain 0.3% BLEU and 0.8% TER by adding the wcTM and the 7-gram wcLM. 4 Equivalence to Factored Translation Koehn and Hoang (2007) propose to integrate different levels of annotation (e.g. morphologial analysis) as factors into the translation process. Here, the surface form of the source word is analyzed to produce the factors, which are then translated and finally the surface form of the target word is generated from the target factors. Although the translations of the factors operate on the same phrase segmentation, they are assumed to be independent. In practice this is done by phrase expansion, which generates a joint phrase table as the cross product from the phrase tables of the individual factors. In contrast, in</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 868–876, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="4426" citStr="Koehn et al., 2003" startWordPosition="699" endWordPosition="702">is equivalent to a special case of the Factored Translation Models proposed by Koehn and Hoang (2007). We will go into more detail in Section 4. Also related to our work, Cherry (2013) proposes to parameterize a hierarchical reordering model with sparse features that are conditioned on word classes trained with mkcls. However, the features are trained with MIRA rather than estimated by relative frequencies. 2 Word Class Models 2.1 Standard Models The translation model of most phrase-based and hierarchical phrase-based SMT systems is parameterized by two phrasal and two lexical channel models (Koehn et al., 2003) which are estimated as relative frequencies. Their counts are extracted heuristically from a word aligned bilingual training corpus. In addition to the four channel models, our baseline contains binary count features that fire, if the extraction count of the corresponding phrase pair is greater or equal to a given threshold τ. We use the thresholds τ = 12, 3, 4}. Our phrase-based baseline contains the hierarchical reordering model (HRM) described by Galley and Manning (2008). Similar to (Cherry et al., 2012), we apply it in both translation directions with separate scaling factors for the thr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 127–133, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9223" citStr="Koehn, 2004" startWordPosition="1517" endWordPosition="1518">rman→English task, we evaluate our method for both a standard phrase-based and the hierarchical phrase-based baseline. Again, the phrasebased baseline contains the HRM model. As bilingual training data we use the TED talks, which we cluster into 100 classes on both source and target side. The 4-gram LM is trained on the TED, Europarl and news-commentary corpora. On this data set, we directly use a 7-gram wcLM. In all setups, the feature weights are optimized with MERT. Results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), confidence level computation is based on (Koehn, 2004). Our experiments are conducted with the open source toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). dev test BLEU TER BLEU TER [%] [%] [%] [%] -TM +wcTM 21.2 64.2 24.7 59.5 -LM +wcLM 22.2 62.9 25.9 58.9 -HRM +wcHRM 24.6 61.9 27.5 58.1 phrase-based 24.6 61.8 27.8 57.6 + wcTM 24.7 61.4 28.1 57.1 + wcLM 24.9 61.2 28.4 57.1 + wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡ + wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡ + wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡ + wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡ Table 2: BLEU and TER results on the French→German task. Results marked with ‡ are statistically significant with 95% confidence</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Mediani</author>
<author>Yuqi Zhang</author>
<author>Jan Niehues Thanh-Le Ha</author>
<author>Eunah Cho</author>
<author>Teresa Herrmann</author>
<author>Alex Waibel</author>
</authors>
<title>The kit translation systems for iwslt</title>
<date>2012</date>
<booktitle>In Proceedings of the International Workshop for Spoken Language Translation (IWSLT</booktitle>
<location>Hong Kong.</location>
<marker>Mediani, Zhang, Thanh-Le Ha, Cho, Herrmann, Waibel, 2012</marker>
<rawString>Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan Niehues, Eunah Cho, Teresa Herrmann, and Alex Waibel. 2012. The kit translation systems for iwslt 2012. In Proceedings of the International Workshop for Spoken Language Translation (IWSLT 2012), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1858" citStr="Och and Ney, 2003" startWordPosition="294" endWordPosition="297">mena that need to be modeled only few training instances can be observed. This is partly due to the large vocabularies of natural languages. One possiblity to reduce the sparsity for model estimation is to reduce the vocabulary size. By clustering the vocabulary into a fixed number of word classes, it is possible to train models that are less prone to sparsity issues. This work investigates the performance of standard models used in statistical machine translation when they are trained on automatically learned word classes rather than the actual word identities. In the popular tooklit GIZA++ (Och and Ney, 2003), word classes are an essential ingredient to model alignment probabilities with the HMM or IBM translation models. It contains the mkcls tool (Och, 1999), which can automatically cluster the vocabulary into classes. Using this tool, we propose to re-parameterize the standard models used in statistical machine translation (SMT), which are usually conditioned on word identities rather than word classes. The idea is that this should lead to a smoother distribution, which is more reliable due to less sparsity. Here, we focus on the phrase-based and lexical channel models in both directions, simpl</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="6530" citStr="Och and Ney, 2004" startWordPosition="1048" endWordPosition="1051">ng the same training procedure as usual. We end up with two separate model files, usually in the form of large tables, one with word identities and one with classes. Next, we sort both tables by their word classes. By walking through both sorted tables simultaneously, we can then efficiently augment the standard model file with an additonal feature (or additional features) based on word classes. The word class LM is directly passed on to the decoder. 2.3 Decoding The decoder searches for the best translation given a set of models hm(eI1, sK1 , fJ1 ) by maximizing the log-linear feature score (Och and Ney, 2004): �e1 = arg max � M ˆI λmhm(eI 1,sK 1 , fJ 1 ) , (1) I,ei m=1 where fJ1 = f1 . . . fJ is the source sentence, eI1 = e1 ... eI the target sentence and sK1 = s1 ... sK the hidden alignment or derivation. All the above mentioned models can easily be integrated into this framework as additional features hm. The feature weights λm are tuned with minimum error rate training (MERT) (Och, 2003). 3 Experiments 3.1 Data Our experiments are performed on a French—*German task. In addition to some project-internal data, we train the system on the data provided for the WMT 2012 shared task1. Both the dev an</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proc. of the Ninth Conf. of the Europ. Chapter of the Association of Computational Linguistics,</booktitle>
<pages>71--76</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="2012" citStr="Och, 1999" startWordPosition="320" endWordPosition="321">uce the sparsity for model estimation is to reduce the vocabulary size. By clustering the vocabulary into a fixed number of word classes, it is possible to train models that are less prone to sparsity issues. This work investigates the performance of standard models used in statistical machine translation when they are trained on automatically learned word classes rather than the actual word identities. In the popular tooklit GIZA++ (Och and Ney, 2003), word classes are an essential ingredient to model alignment probabilities with the HMM or IBM translation models. It contains the mkcls tool (Och, 1999), which can automatically cluster the vocabulary into classes. Using this tool, we propose to re-parameterize the standard models used in statistical machine translation (SMT), which are usually conditioned on word identities rather than word classes. The idea is that this should lead to a smoother distribution, which is more reliable due to less sparsity. Here, we focus on the phrase-based and lexical channel models in both directions, simple count models identifying frequency thresholds, lexicalized reordering models and an n-gram language model. Although our results show that it is not a go</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F. J. Och. 1999. An efficient method for determining bilingual word classes. In Proc. of the Ninth Conf. of the Europ. Chapter of the Association of Computational Linguistics, pages 71–76, Bergen, Norway, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6919" citStr="Och, 2003" startWordPosition="1131" endWordPosition="1132">. The word class LM is directly passed on to the decoder. 2.3 Decoding The decoder searches for the best translation given a set of models hm(eI1, sK1 , fJ1 ) by maximizing the log-linear feature score (Och and Ney, 2004): �e1 = arg max � M ˆI λmhm(eI 1,sK 1 , fJ 1 ) , (1) I,ei m=1 where fJ1 = f1 . . . fJ is the source sentence, eI1 = e1 ... eI the target sentence and sK1 = s1 ... sK the hidden alignment or derivation. All the above mentioned models can easily be integrated into this framework as additional features hm. The feature weights λm are tuned with minimum error rate training (MERT) (Och, 2003). 3 Experiments 3.1 Data Our experiments are performed on a French—*German task. In addition to some project-internal data, we train the system on the data provided for the WMT 2012 shared task1. Both the dev and the test set are composed of a mixture of broadcast news and broadcast conversations crawled from the web and have two references. Table 1 shows the data statistics. To confirm our results we also run experiments on the German—*English task of the IWSLT 2012 evaluation campaign2. 1http://www.statmt.org/wmt12/ 2http://hltc.cs.ust.hk/iwslt/ 1378 French German train Sentences 1.9M Runnin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="9137" citStr="Papineni et al., 2002" startWordPosition="1500" endWordPosition="1504">text length. On this task we also run additional experiments with 200 and 500 classes. On the German→English task, we evaluate our method for both a standard phrase-based and the hierarchical phrase-based baseline. Again, the phrasebased baseline contains the HRM model. As bilingual training data we use the TED talks, which we cluster into 100 classes on both source and target side. The 4-gram LM is trained on the TED, Europarl and news-commentary corpora. On this data set, we directly use a 7-gram wcLM. In all setups, the feature weights are optimized with MERT. Results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), confidence level computation is based on (Koehn, 2004). Our experiments are conducted with the open source toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). dev test BLEU TER BLEU TER [%] [%] [%] [%] -TM +wcTM 21.2 64.2 24.7 59.5 -LM +wcLM 22.2 62.9 25.9 58.9 -HRM +wcHRM 24.6 61.9 27.5 58.1 phrase-based 24.6 61.8 27.8 57.6 + wcTM 24.7 61.4 28.1 57.1 + wcLM 24.9 61.2 28.4 57.1 + wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡ + wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡ + wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡ + wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡ Table 2: BLEU and TER results on the Frenc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="9167" citStr="Snover et al., 2006" startWordPosition="1507" endWordPosition="1510">o run additional experiments with 200 and 500 classes. On the German→English task, we evaluate our method for both a standard phrase-based and the hierarchical phrase-based baseline. Again, the phrasebased baseline contains the HRM model. As bilingual training data we use the TED talks, which we cluster into 100 classes on both source and target side. The 4-gram LM is trained on the TED, Europarl and news-commentary corpora. On this data set, we directly use a 7-gram wcLM. In all setups, the feature weights are optimized with MERT. Results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), confidence level computation is based on (Koehn, 2004). Our experiments are conducted with the open source toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). dev test BLEU TER BLEU TER [%] [%] [%] [%] -TM +wcTM 21.2 64.2 24.7 59.5 -LM +wcLM 22.2 62.9 25.9 58.9 -HRM +wcHRM 24.6 61.9 27.5 58.1 phrase-based 24.6 61.8 27.8 57.6 + wcTM 24.7 61.4 28.1 57.1 + wcLM 24.9 61.2 28.4 57.1 + wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡ + wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡ + wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡ + wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡ Table 2: BLEU and TER results on the French→German task. Results marked </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="5255" citStr="Stolcke, 2002" startWordPosition="834" endWordPosition="835">es that fire, if the extraction count of the corresponding phrase pair is greater or equal to a given threshold τ. We use the thresholds τ = 12, 3, 4}. Our phrase-based baseline contains the hierarchical reordering model (HRM) described by Galley and Manning (2008). Similar to (Cherry et al., 2012), we apply it in both translation directions with separate scaling factors for the three orientation classes, leading to a total of six feature weights. An n-gram language model (LM) is another important feature of our translation systems. The baselines apply 4-gram LMs trained by the SRILM toolkit (Stolcke, 2002) with interpolated modified Kneser-Ney smoothing (Chen and Goodman, 1998). The smaller vocabulary size allows us to efficiently model larger context, so in addition to the 4-gram LM, we also train a 7-gram LM based on word classes. In contrast to an LM of the same size trained on word identities, the increase in computational resources needed for translation is negligible for the 7-gram word class LM (wcLM). 2.2 Training By replacing the words on both source and target side of the training data with their respective word classes and keeping the word alignment unchanged, all of the above models</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9331" citStr="Vilar et al., 2010" startWordPosition="1533" endWordPosition="1536">e-based baseline. Again, the phrasebased baseline contains the HRM model. As bilingual training data we use the TED talks, which we cluster into 100 classes on both source and target side. The 4-gram LM is trained on the TED, Europarl and news-commentary corpora. On this data set, we directly use a 7-gram wcLM. In all setups, the feature weights are optimized with MERT. Results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), confidence level computation is based on (Koehn, 2004). Our experiments are conducted with the open source toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). dev test BLEU TER BLEU TER [%] [%] [%] [%] -TM +wcTM 21.2 64.2 24.7 59.5 -LM +wcLM 22.2 62.9 25.9 58.9 -HRM +wcHRM 24.6 61.9 27.5 58.1 phrase-based 24.6 61.8 27.8 57.6 + wcTM 24.7 61.4 28.1 57.1 + wcLM 24.9 61.2 28.4 57.1 + wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡ + wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡ + wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡ + wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡ Table 2: BLEU and TER results on the French→German task. Results marked with ‡ are statistically significant with 95% confidence, results marked with † with 90% confidence. -X +wcX denote the systems, where the model X in the baseline i</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open source hierarchical translation, extended with reordering and lexicon models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open source phrase-based and hierarchical statistical machine translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="3600" citStr="Wuebker et al., 2012" startWordPosition="564" endWordPosition="567">man task and a small German—*English task and with both phrasebased and hierarchical phrase-based translation decoders. By using word class models, we can improve our respective baselines by 1.4% BLEU and 1.0% TER on the French—*German task and 0.3% BLEU and 1.1% TER on the German—*English task. Training an additional language model for trans1377 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics lation based on word classes has been proposed in (Wuebker et al., 2012; Mediani et al., 2012; Koehn and Hoang, 2007). In addition to the reduced sparsity, an advantage of the smaller vocabulary is that longer n-gram context can be modeled efficiently. Mathematically, our idea is equivalent to a special case of the Factored Translation Models proposed by Koehn and Hoang (2007). We will go into more detail in Section 4. Also related to our work, Cherry (2013) proposes to parameterize a hierarchical reordering model with sparse features that are conditioned on word classes trained with mkcls. However, the features are trained with MIRA rather than estimated by rela</context>
<context position="9310" citStr="Wuebker et al., 2012" startWordPosition="1529" endWordPosition="1532">the hierarchical phrase-based baseline. Again, the phrasebased baseline contains the HRM model. As bilingual training data we use the TED talks, which we cluster into 100 classes on both source and target side. The 4-gram LM is trained on the TED, Europarl and news-commentary corpora. On this data set, we directly use a 7-gram wcLM. In all setups, the feature weights are optimized with MERT. Results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), confidence level computation is based on (Koehn, 2004). Our experiments are conducted with the open source toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). dev test BLEU TER BLEU TER [%] [%] [%] [%] -TM +wcTM 21.2 64.2 24.7 59.5 -LM +wcLM 22.2 62.9 25.9 58.9 -HRM +wcHRM 24.6 61.9 27.5 58.1 phrase-based 24.6 61.8 27.8 57.6 + wcTM 24.7 61.4 28.1 57.1 + wcLM 24.9 61.2 28.4 57.1 + wcHRM 25.4‡ 60.9‡ 28.9‡ 56.9‡ + wcLM7 25.5‡ 60.7‡ 29.2‡ 56.6‡ + wcModels200 25.5‡ 60.8‡ 29.3‡ 56.4‡ + wcModels500 25.2† 60.8‡ 29.0‡ 56.6‡ Table 2: BLEU and TER results on the French→German task. Results marked with ‡ are statistically significant with 95% confidence, results marked with † with 90% confidence. -X +wcX denote the systems, where the mode</context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open source phrase-based and hierarchical statistical machine translation. In International Conference on Computational Linguistics, pages 483–491, Mumbai, India, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
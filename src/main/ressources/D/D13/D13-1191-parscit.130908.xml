<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.980129">
Learning Topics and Positions from Debatepedia
</title>
<author confidence="0.983297">
Swapna Gottipati† Minghui Qiu† Yanchuan Sim‡ Jing Jiang† Noah A. Smith‡
</author>
<affiliation confidence="0.943933">
†School of Information Systems, Singapore Management University, Singapore
‡Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.930033">
†{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
‡{ysim,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99981375">
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation’s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995265">
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (§2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al., 2011).
In this paper, we develop a generative model for
discovering such a representation (§3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al. (2008) and Ahmed and
</bodyText>
<footnote confidence="0.977304">
1http://dbp.idebate.org
</footnote>
<bodyText confidence="0.999484689655172">
Xing (2010), who used generative models to infer
topics—distributions over words—and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(§4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model’s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al., 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
</bodyText>
<sectionHeader confidence="0.996474" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.997476">
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
</bodyText>
<footnote confidence="0.9549385">
2This variable might serve to cluster debate sides according
to “abstract beliefs commonly shared by a group of people,”
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see §4).
</footnote>
<page confidence="0.89586">
1858
</page>
<note confidence="0.888796166666667">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858–1868,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense – Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a “right” to guns as a means Argument: The protection of property is not a good
</note>
<figureCaption confidence="0.712835769230769">
to self-defense: Many groups argue that a citizen justification for yielding a lethal weapon. While peo-
should have the “right” to defend themselves, and that ple have a right to their property, this should not justify
a gun is frequently the ... wielding a lethal ...
Argument: Gun restrictions and bans disadvantage cit- Argument: Robert F. Drinan, Former Democratic US
izens against armed criminals. Citizens that are not al- Congressman, “Gun Control: The Good Outweighs
lowed to carry guns are disadvantaged against lawless the Evil”, 1976 – “These graphic examples of individ-
criminals that ... ual instances of ...
Question: Economic benefits – Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically Argument: Gun sports have economic benefits. Field
costly. The Coalition for Gun Control claims that, “in sports bring money into poor rural economies and pro-
Canada, the costs of firearms death and injury alone vide a motivation for landowners to value environmen-
have been estimated at ... tal protection.
</figureCaption>
<tableCaption confidence="0.997124">
Table 1: An example of a Debatepedia debate on the topic “Gun control.”
</tableCaption>
<bodyText confidence="0.99954032">
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (§4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains “questions,”
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the “No” side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
</bodyText>
<footnote confidence="0.97761">
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
</footnote>
<figure confidence="0.611432333333333">
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
</figure>
<tableCaption confidence="0.9517605">
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
</tableCaption>
<bodyText confidence="0.9996182">
eral, the questions are phrased so that a consistent
“pro” and “con” structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the “Yes” sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense “Yes” arguing “no” to the high-level de-
bate question—Should laws be passed to limit gun
ownership further?—and the economic “Yes” argu-
ing “yes” to the high-level question.
</bodyText>
<tableCaption confidence="0.520983">
Table 2 presents statistics of our corpus.
</tableCaption>
<subsectionHeader confidence="0.980524">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999962">
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
</bodyText>
<page confidence="0.993367">
1859
</page>
<bodyText confidence="0.999918736842105">
erpipe (Kohlsch¨utter et al., 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al., 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (§4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
</bodyText>
<sectionHeader confidence="0.99308" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999987333333333">
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple (d, q, s, a)
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
</bodyText>
<listItem confidence="0.993298333333333">
• Each question q within debate d is associated
with a distribution over topics, denoted 9d,q.8
• Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution t that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
</listItem>
<footnote confidence="0.940212461538462">
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in §2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al. (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia’s questions are the most topically
coherent level, and work with a single topic mixture at this level.
</footnote>
<figureCaption confidence="0.9993565">
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. α, ,Q, -y
and all rl are fixed hyperparameters (§3.1).
</figureCaption>
<bodyText confidence="0.9461955">
tion is not always correct, though it tends to
hold most of the time.
</bodyText>
<listItem confidence="0.990553125">
• Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value “entity” (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: “gen-
eral position” (i), “topic-specific position” (o),
“topic” (t), or “background” (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
• For some term types (the ones where y E
to, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
</listItem>
<bodyText confidence="0.919903">
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
</bodyText>
<subsectionHeader confidence="0.997805">
3.1 Priors
</subsectionHeader>
<bodyText confidence="0.9933665">
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
</bodyText>
<figure confidence="0.997581708333333">
α
L
3 77i 77o 77t 77e
0
z w y
Nd,q,s,a
Ad,q,s
Qd
i
Sd
oii
K
oo
i,t
KT
ot
t
D
ob
77b
oet
&apos;Y
Ft
T
</figure>
<page confidence="0.797105">
1860
</page>
<listItem confidence="0.948090857142857">
1. ∀ topics t, draw topic-term distribution φtt ∼ Dirichlet(ηt) and topic-entity distribution φet ∼ Dirichlet(ηe).
2. ∀ positions i, draw position-term distribution φi i ∼ Dirichlet(ηi).
3. ∀ topics t, ∀ positions i, draw topic-position term distribution φoi,t ∼ Dirichlet(ηo).
4. Draw background term distribution φb ∼ Dirichlet(ηb).
5. Draw functional term type distribution µ ∼ Dirichlet(γ).
6. Draw position distribution ι ∼ Dirichlet(β).
7. ∀ debates d:
</listItem>
<figure confidence="0.975867714285714">
a. Draw id,�, id,2 ∼ Multinomial(ι), assigning each of the two sides to a position.
b. ∀ questions q in d:
i. Draw topic mixture proportions θd,q ∼ Dirichlet(α).
ii. ∀ arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ∼ Multinomial(θd,q).
B. Draw functional term type yd,q,s,a ∼ Multinomial(µ).
C. Draw term wd,q,s,a ∼ Multinomial (φyd,H,s,a |id,�, id,2, zd,q,s,a).
</figure>
<figureCaption confidence="0.999867">
Figure 2: Generative story for our model of Debatepedia.
</figureCaption>
<bodyText confidence="0.99967552">
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al., 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al., 2005) to construct 77i and 77o.
Specifically, terms w in the lexicon were given pa-
rameters qiw = qow = 0.01, and other terms were
given qiw = qow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
“boost” through this prior.
Information retrieval has long exploited the ob-
servation that a term’s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in 77b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term’s argument frequency.
The other priors were set to be symmetric: qe =
0.01 (entity topics), qt = 0.001 (topics), α =
50/T = 1.25 (topic mixture coefficients), Q = 0.01
(positions), and -y = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
</bodyText>
<subsectionHeader confidence="0.995757">
3.2 Inference and Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999164">
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
</bodyText>
<footnote confidence="0.790051">
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
</footnote>
<bodyText confidence="0.9993964">
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,� and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in §3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
</bodyText>
<subsectionHeader confidence="0.97333">
3.3 T and K
</subsectionHeader>
<bodyText confidence="0.9999858">
In all experiments, we use T = 40 topics and K = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
</bodyText>
<sectionHeader confidence="0.997834" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999715">
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in §2 as
supporting texts for arguments, treating each one’s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
</bodyText>
<footnote confidence="0.5415145">
10Because this technique is well known in NLP, details are
relegated to supplementary material.
</footnote>
<page confidence="0.991269">
1861
</page>
<figureCaption confidence="0.995123">
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
</figureCaption>
<bodyText confidence="0.998847">
our model’s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
</bodyText>
<subsectionHeader confidence="0.9802475">
4.1 Quantitative Evaluation
4.1.1 Topics
</subsectionHeader>
<bodyText confidence="0.99934624">
As described in §2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al., 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p &lt; 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
</bodyText>
<footnote confidence="0.741279">
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST’s prior in the same way it is used
in our model.
</footnote>
<figure confidence="0.74999">
5 10 15 20 25 inf
K
</figure>
<figureCaption confidence="0.998907">
Figure 4: Mean reciprocal ranks for the association task.
</figureCaption>
<bodyText confidence="0.999672">
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p &lt;
0.001). We found the same pattern for MRR@k,
k E 15,10,15, 20, 25, oc1, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation’s quality.
</bodyText>
<subsectionHeader confidence="0.57871">
4.1.2 Positions
</subsectionHeader>
<bodyText confidence="0.999213">
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for iii t and O ,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas’ arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women’s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model’s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,,
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
</bodyText>
<figure confidence="0.997806217391304">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
JS Divergence
1400
1200
1000
400
200
800
600
0
LDA
JST
Our Model
LDA
JST
Our Model
No of Articles
MRR 0.18
0.16
0.14
0.12
0.1
0.08
</figure>
<page confidence="0.880249">
1862
</page>
<sectionHeader confidence="0.647071" genericHeader="method">
JS Divergence Score
</sectionHeader>
<bodyText confidence="0.935114333333333">
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
</bodyText>
<equation confidence="0.866343333333333">
i = 1 i = 2
i* = 1 1,272 216
i* = 2 199 1,313
</equation>
<tableCaption confidence="0.9948995">
Table 3: Confusion matrix for position prediction on
held-out arguments.
</tableCaption>
<bodyText confidence="0.987065363636364">
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
</bodyText>
<equation confidence="0.966818666666667">
i = 1 i = 2
i* = 1 1,042 623
i* = 2 1,043 644
</equation>
<tableCaption confidence="0.992355">
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
</tableCaption>
<bodyText confidence="0.994914362068966">
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen’s n = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model’s suggestion that blogs and editori-
als may be more “Debatepedia argument-like” than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
president, washington, obama, american, america
united, states, president, administration, foreign
peace, state, west, united, action
teachers, pay, test, left, merit
women, religious, abortion, god, life
israel, gaza, hamas, israeli, palestinian
government, social, governments, state, programs
economy, financial, spending, economic, government
military, war, iraq, forces, march
oil, water, production, ethanol, environmental
countries, eu, european, international, states
times, york, ban, june, january
college, cloning, game, football, incest
law, workers, union, rights, legal
companies, market, industry, business, bailout
information, torture, science, evidence, wikipedia
circumcision, men, sexual, circumcised, foreskin
health, care, insurance, public, private
speech, corporations, corporate, public, money
orleans, euthanasia, city, suicide, priests
english, language, violence, people, video
international, court, war, crimes, icc
global, emissions, climate, carbon, warming
china, tibet, chinese, people, tibetan
school, schools, students, education, public
children, child, sex, parents, sexual
human, rights, animals, life, animal
south, kosovo, independence, state, republic
rights, law, people, individual, amendment
marriage, gay, mars, space, moon
marijuana, drug, drugs, alcohol, age
death, crime, punishment, penalty, justice
food, consumers, products, calorie, information
people, dont, time, lot, make
immigration, cameras, police, immigrants, crime
tax, economic, trade, cost, percent
energy, gas, power, fuel, wind
party, vote, republican, political, voters
nuclear, weapons, iran, states, threat
comment, minimum, wage, poverty, capitalism
0.1 0.2 0.3 0.4
</bodyText>
<figureCaption confidence="0.983321666666667">
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from Ot.
</figureCaption>
<subsectionHeader confidence="0.6488395">
4.1.3 Comparison to Human Judgments of
Positions
</subsectionHeader>
<bodyText confidence="0.99980625">
We compared our model’s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
</bodyText>
<figure confidence="0.990510444444445">
0.7
0.65
0.6
0.55
0.5
0.45
0.4
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
Article type(% of articles)
</figure>
<figureCaption confidence="0.994689">
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
</figureCaption>
<page confidence="0.763837">
1863
</page>
<table confidence="0.916222533333333">
“Israel-Palestine” “Same-sex marriage” “Drugs” “Healthcare” “Death penalty” “Abortion”
it pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2 two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
– war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+ peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model’s topics.
</table>
<tableCaption confidence="0.997045">
Table 6: Terms associated with selected topics. The labels and alignments between the two models’ topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log 0oi�,t,w −log 0oi�,t,w. We show the top three terms for each position (b.) JST:
we show the top three terms for each sentiment (negative and positive).
</tableCaption>
<table confidence="0.9994995">
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
</table>
<tableCaption confidence="0.976994">
Table 5: Variation of information scores for each pairing
of annotators and model.
</tableCaption>
<bodyText confidence="0.977892823529412">
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,,;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2’s coarse clustering most closely, and in fact is
closer to A2’s clustering than A2 is to A3’s; it also
agrees with A2’s coarse clustering better than A2’s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate’s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
</bodyText>
<footnote confidence="0.9991165">
15This was determined using a Monte Carlo simulation with
1,000 samples.
</footnote>
<page confidence="0.981742">
1864
</page>
<bodyText confidence="0.8299660625">
Topic i = 1 i = 2
None (φ�) vice president, c sections, twenty four, cross pressures, cross examination, under runs, hand outs, half million,
pre dates, anti ballistic, cost effectiveness, anti land- non christians, break down, counter argument, seventy
mine, court appointed, child poverty five, coworkers, runup
“Israel- pre emptive, israeli palestinian, open and shut, first two state, long term, self destructive, secretary general,
Palestine” time, hamas controlled, democratically elected right wing, all out, near daily, short term
“Same-sex same sex, long term, second class, blankenhorn rauch, opposite sex, well intentioned, day time, planet wide,
marriage” wrong headed, self denial, left handed day night, child rearing, low earth, one way, one third
“Drugs” hands free, performance enhancing, in depth, hand long term, high speed, short term, peer reviewed, alco-
held, best kept, non pharmaceutical, anti marijuana hol related, mind altering, inner city, long lasting
“Healthcare” single payer, so called, self sustaining, public private, government run, government approved, high risk, two
for profit, long run, high cost, multi payer tier, government appointed, low cost, set up
“Death anti death, non violent, african american, self help, cut semi automatic, high profile, hate crime, assault
penalty” and cover, heavy handed, dp equivalent weapons, military style, high dollar, self protective
“Abortion” pro choice, pro life, non muslim, well educated, anti would be, full time, late term, judeo christian, life
abortion, much needed, church state, birth control style, day to day, non christian, child bearing
</bodyText>
<tableCaption confidence="0.994936">
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
</tableCaption>
<table confidence="0.993349666666667">
Topic Terms Person entity mentions
“Israel- israel, gaza, hamas, israeli, pales- Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Palestine” tinian Steven R. David
“Same-sex marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
marriage”
“Drugs” marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
“Healthcare” health, care, insurance, public, pri- Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
vate
“Death death, crime, punishment, penalty, Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
penalty” justice
“Abortion” women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
</table>
<tableCaption confidence="0.953373">
Table 8: For 6 selected topics (labels assigned manually), top terms (0) and person entities (die). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
</tableCaption>
<bodyText confidence="0.998210684210526">
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
“syntactic priming” of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al., 2013). We leave these di-
rections to future work.
</bodyText>
<subsectionHeader confidence="0.998199">
4.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.9998786">
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
</bodyText>
<page confidence="0.985882">
1865
</page>
<bodyText confidence="0.981972555555556">
Table 7 shows bigrams most strongly associated
with general position distributions and selected
topic-position distributions 01.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man’s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.997891080000001">
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al., 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al., 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al., 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al., 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al., 2007). Mei
et al. (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He’s senti-
ments, our model’s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al. (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al. (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al. (2012)
and Dasigi et al. (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al. (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al., 2012). Somasundaran and Wiebe (2009)
and Anand et al. (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al. (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization’s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet allocation, and
Eisenstein et al. (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al., 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
</bodyText>
<page confidence="0.976693">
1866
</page>
<bodyText confidence="0.9998685">
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999910625">
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model’s strengths
and weaknesses.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999689545454545">
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A*STAR fellowship to Y.S., and by
Google’s support of the Reading is Believing project
at CMU.
</bodyText>
<sectionHeader confidence="0.998557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827388888889">
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
’03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66–71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27–50. Chapman
&amp; Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228–5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11–21.
Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
</reference>
<page confidence="0.850431">
1867
</page>
<reference confidence="0.999887084745763">
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Sch¨olkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173–187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118–178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
</reference>
<page confidence="0.994201">
1868
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.261281">
<title confidence="0.996723">Learning Topics and Positions from Debatepedia</title>
<note confidence="0.320857666666667">A. of Information Systems, Singapore Management University, Singapore Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA</note>
<abstract confidence="0.998187769230769">We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Mona Diab</author>
<author>Pradeep Dasigi</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35110" citStr="Abu-Jbara et al. (2012)" startWordPosition="5508" endWordPosition="5511">introduces a switching variable that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides of a debate, and we incorporate topics at the level of questions within debates. Some studies have specifically analyzed contrastive viewpoints or stances in general discussion text.Agrawal et al. (2003) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models </context>
</contexts>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In WWW ’03.</booktitle>
<contexts>
<context position="34883" citStr="Agrawal et al. (2003)" startWordPosition="5471" endWordPosition="5474">l in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching variable that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides of a debate, and we incorporate topics at the level of questions within debates. Some studies have specifically analyzed contrastive viewpoints or stances in general discussion text.Agrawal et al. (2003) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awa</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In WWW ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="36337" citStr="Ahmed and Xing (2010)" startWordPosition="5702" endWordPosition="5705">llah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation, and Eisenstein et al. (2011) introduced sparse additive generative models, both conceived as extensions to well-established probabilistic modeling techniques (Blei et al., 2003); these were applied to debates and political blog datasets. Our approach builds on these models (especially the switching variables of Ahmed and Xing). We go farther in jointly modeling 1866 text across many debates evidenced by the structure of Debatepedia, thus grounding our models more solidly in familiar sociopolitical issues, and in making extensive use of exist</context>
</contexts>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Amr Ahmed and Eric P. Xing. 2010. Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the Second Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</booktitle>
<contexts>
<context position="35789" citStr="Anand et al. (2011)" startWordPosition="5614" endWordPosition="5617">e problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation,</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: classifying stance in online debate. In Proceedings of the Second Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rawia Awadallah</author>
<author>Maya Ramanath</author>
<author>Gerhard Weikum</author>
</authors>
<title>OpinioNetIt: Understanding the opinions-people network for politically controversial topics.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="35503" citStr="Awadallah et al. (2011)" startWordPosition="5567" endWordPosition="5570">03) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates </context>
</contexts>
<marker>Awadallah, Ramanath, Weikum, 2011</marker>
<rawString>Rawia Awadallah, Maya Ramanath, and Gerhard Weikum. 2011. OpinioNetIt: Understanding the opinions-people network for politically controversial topics. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rawia Awadallah</author>
<author>Maya Ramanath</author>
<author>Gerhard Weikum</author>
</authors>
<title>PolariCQ: Polarity classification of political quotations.</title>
<date>2012</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="35734" citStr="Awadallah et al., 2012" startWordPosition="5604" endWordPosition="5608">and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (20</context>
</contexts>
<marker>Awadallah, Ramanath, Weikum, 2012</marker>
<rawString>Rawia Awadallah, Maya Ramanath, and Gerhard Weikum. 2012. PolariCQ: Polarity classification of political quotations. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2887" citStr="Blei et al., 2003" startWordPosition="401" endWordPosition="404">cs using the hierarchical organization of arguments within Debatepedia. Further, we use named entity recognition as a preprocessing step, an existing sentiment lexicon to construct an informed prior, and we incorporate a latent, discrete position variable that cuts across debates.2 We evaluate the model informally and formally (§4). Subjectively, the model identifies reasonable topic and perspective terms, and it associates topics sensibly with important public figures. In quantitative evaluations, we find the model’s representation superior to topics from vanilla latent Dirichlet allocation (Blei et al., 2003) and the joint sentiment topic model (Lin and He, 2009) in matching external texts to debates. Further, the position variables can be used to infer the side of an argument within a debate; our model performs with an accuracy of 86% on position prediction of the debate argument. The cross-cutting position variable is not especially consistent with human judgments, suggesting that further knowledge sources may be required to improve interpretability across issues. 2 Data Debatepedia, like Wikipedia, is constructed by volunteer contributors and has a system of community 2This variable might serve</context>
<context position="16219" citStr="Blei et al., 2003" startWordPosition="2548" endWordPosition="2551"> arguments to human annotated clusterings. Finally, we present qualitative discussion. 4.1 Quantitative Evaluation 4.1.1 Topics As described in §2, our corpus includes 3,352 articles hyperlinked by Debatepedia arguments.11 Our model can be used to infer the posterior over topics associated with such an article, and we compare that distribution to that of the Debatepedia article that links to it. Calculating the similarity of these distributions, we get an estimate of how closely our model can associate text related to a debate with the specific argument that linked to it. We compare with LDA (Blei et al., 2003), which ignores sentiment, and the joint sentiment topic (JST) model (Lin and He, 2009), an unsupervised model that jointly captures sentiment and topic.12 Using Jensen-Shannon divergence, we find that our approach embeds these pairs significantly closer than LDA and JST (also trained with 40 topics), under a Wilcoxon signed rank test (p &lt; 0.001). Figure 3 shows the histogram of divergences between our model, JST, and LDA. Associating external articles. More challenging, of course, is selecting the argument to which an external article should be associated. We used the Jensen-Shannon divergenc</context>
<context position="36567" citStr="Blei et al., 2003" startWordPosition="5731" endWordPosition="5734">logy we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation, and Eisenstein et al. (2011) introduced sparse additive generative models, both conceived as extensions to well-established probabilistic modeling techniques (Blei et al., 2003); these were applied to debates and political blog datasets. Our approach builds on these models (especially the switching variables of Ahmed and Xing). We go farther in jointly modeling 1866 text across many debates evidenced by the structure of Debatepedia, thus grounding our models more solidly in familiar sociopolitical issues, and in making extensive use of existing NLP resources. 6 Conclusion Using text from Debatepedia, we inferred topics and position term lexicons in the domain of sociopolitical debates. Our approach brings together tools from information extraction and sentiment analy</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Serena Villata</author>
</authors>
<title>Combining textual entailment and argumentation theory for supporting online debates interactions.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35347" citStr="Cabrio and Villata (2012)" startWordPosition="5545" endWordPosition="5548">the level of questions within debates. Some studies have specifically analyzed contrastive viewpoints or stances in general discussion text.Agrawal et al. (2003) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the metho</context>
</contexts>
<marker>Cabrio, Villata, 2012</marker>
<rawString>Elena Cabrio and Serena Villata. 2012. Combining textual entailment and argumentation theory for supporting online debates interactions. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Cohen</author>
<author>James T Hamilton</author>
<author>Fred Turner</author>
</authors>
<title>Computational journalism.</title>
<date>2011</date>
<journal>Communications of the ACM,</journal>
<volume>54</volume>
<issue>10</issue>
<contexts>
<context position="1693" citStr="Cohen et al., 2011" startWordPosition="228" endWordPosition="231">, community-authored encyclopedia of debates (§2), seek to organize some of this exchange into structured information resources that summarize arguments and link externally to texts (editorials, blog posts, etc.) that express and evoke them. Empirical NLP, we propose, has a role to play in creating a more compact and easilyinterpretable way to understand the opinion space. In particular, we envision applications to computational journalism, where there is high demand for transformation of and pattern discovery in unmanageable, unstructured, evolving data (including text) to inform the public (Cohen et al., 2011). In this paper, we develop a generative model for discovering such a representation (§3), using Debatepedia as a corpus of evidence. We draw inspiration from Lin et al. (2008) and Ahmed and 1http://dbp.idebate.org Xing (2010), who used generative models to infer topics—distributions over words—and other wordassociated variables representing perspectives or ideologies. We view topics as lexicons, and propose that grounding a topic model with evidence beyond bags of words can lead to more lexicon-like representations. Specifically, our generative topic model grounds topics using the hierarchica</context>
</contexts>
<marker>Cohen, Hamilton, Turner, 2011</marker>
<rawString>Sarah Cohen, James T. Hamilton, and Fred Turner. 2011. Computational journalism. Communications of the ACM, 54(10):66–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep Dasigi</author>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Genre independent subgroup detection in online discussion threads: a pilot study of implicit attitude using latent textual semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35135" citStr="Dasigi et al. (2012)" startWordPosition="5513" endWordPosition="5516">ble that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides of a debate, and we incorporate topics at the level of questions within debates. Some studies have specifically analyzed contrastive viewpoints or stances in general discussion text.Agrawal et al. (2003) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012).</context>
</contexts>
<marker>Dasigi, Guo, Diab, 2012</marker>
<rawString>Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012. Genre independent subgroup detection in online discussion threads: a pilot study of implicit attitude using latent textual semantics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="33266" citStr="Dave et al., 2003" startWordPosition="5218" endWordPosition="5221">gestive. The separation of personal name mentions into their own distributions, shown for some topics in Table 8, gives a distinctive characterization of topics based on relevant personalities. Subjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10070" citStr="Eisenstein et al. (2011)" startWordPosition="1524" endWordPosition="1527"> positions, and the two sides of a debate are constrained to associate with opposing positions. As illustrated by Table 1, this assump5http://code.google.com/p/boilerpipe 6www.ranks.nl/resources/stopwords.html 7Recall that our model includes bigrams. We treat each unigram and bigram token (after filtering discussed in §2.1) as a separate term. 8In future work, more sharing across questions within a debate, or more differentiation among the topic distributions for arguments under a question, might be explored. Wallach (2006) describes suitable techniques using hierarchical Dirichlet draws, and Eisenstein et al. (2011) suggests the use of sparse shocks to log-odds at different levels. Here we work on the assumption that Debatepedia’s questions are the most topically coherent level, and work with a single topic mixture at this level. Figure 1: Plate diagram. K is the number of positions, and T is number of topics. The shaded variables are observed and dashed variables are marginalized. α, ,Q, -y and all rl are fixed hyperparameters (§3.1). tion is not always correct, though it tends to hold most of the time. • Each term wd,q,s,a,n (n is the position index of the term within an argument) is associated with on</context>
<context position="36418" citStr="Eisenstein et al. (2011)" startWordPosition="5713" endWordPosition="5716"> interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation, and Eisenstein et al. (2011) introduced sparse additive generative models, both conceived as extensions to well-established probabilistic modeling techniques (Blei et al., 2003); these were applied to debates and political blog datasets. Our approach builds on these models (especially the switching variables of Ahmed and Xing). We go farther in jointly modeling 1866 text across many debates evidenced by the structure of Debatepedia, thus grounding our models more solidly in familiar sociopolitical issues, and in making extensive use of existing NLP resources. 6 Conclusion Using text from Debatepedia, we inferred topics a</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011. Sparse additive generative models of text. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8440" citStr="Finkel et al., 2005" startWordPosition="1266" endWordPosition="1269">elements such as navigation and advertisments) using Boil1859 erpipe (Kohlsch¨utter et al., 2010).5 We tokenized the text and filtered stopwords.6 We considered both unigrams and bigrams in our model, keeping all unigrams and removing bigram types that appeared less than 5 times in the corpus. Although our modeling approach ultimately treats texts as bags of terms (unigrams and bigrams), one important preprocessing step was taken to further improve the interpretability of the inferred representation: named entity mentions of persons. We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token. In our qualitative analysis of the model (§4.2), we will show how this special treatment of person mentions enables the association of well-known individuals with debate topics. Though not part of our experimental evaluation in this paper, such associations are, we believe, an interesting direction for future applications of the model. 3 Model Our model defines a probability distribution over terms7 that are observed in the corpus. Each term occurs in a context defined by the tuple (d, q, s, a) (respectively, a debate, a question within the d</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaz Fortuna</author>
<author>Carolina Galleguillos</author>
<author>Nello Cristianini</author>
</authors>
<title>Detecting the bias in media with statistical learning methods.</title>
<date>2009</date>
<booktitle>In Ashok N . Srivastava and Mehran Sahami, editors, Text Mining: Classification, Clustering, and Applications,</booktitle>
<pages>27--50</pages>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="36161" citStr="Fortuna et al. (2009)" startWordPosition="5670" endWordPosition="5673">xtracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation, and Eisenstein et al. (2011) introduced sparse additive generative models, both conceived as extensions to well-established probabilistic modeling techniques (Blei et al., 2003); these were applied to debates and political blog datasets. Our approach builds on these models (especially the switching variables of Ahmed and Xing). We go farther in jointly modeling 1866 te</context>
</contexts>
<marker>Fortuna, Galleguillos, Cristianini, 2009</marker>
<rawString>Blaz Fortuna, Carolina Galleguillos, and Nello Cristianini. 2009. Detecting the bias in media with statistical learning methods. In Ashok N . Srivastava and Mehran Sahami, editors, Text Mining: Classification, Clustering, and Applications, pages 27–50. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murthy Ganapathibhotla</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinions in comparative sentences.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="33411" citStr="Ganapathibhotla and Liu, 2008" startWordPosition="5238" endWordPosition="5241"> characterization of topics based on relevant personalities. Subjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to t</context>
</contexts>
<marker>Ganapathibhotla, Liu, 2008</marker>
<rawString>Murthy Ganapathibhotla and Bing Liu. 2008. Mining opinions in comparative sentences. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="30808" citStr="Greene and Resnik, 2009" startWordPosition="4831" endWordPosition="4834">la Alsan Table 8: For 6 selected topics (labels assigned manually), top terms (0) and person entities (die). Bigrams were included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex marriage with the space program. ful, perhaps due to the less argumentative nature of other kinds of articles. Noting the vast literature focusing on ideological positions expressed in text, we believe this failure suggests (i) that broadbased positions that hold across many topics may require richer textual representations (see, e.g., the “syntactic priming” of Greene and Resnik, 2009), or (ii) that an alternative representation of positions, such as the spatial models favored by political scientists (Poole and Rosenthal, 1991), may be more discoverable. Aside from those issues, a stronger theory of positions may be required. Such a theory could be encoded in a more informative prior or weaker independence assumptions across debates. Finally, exploiting explicitly ideological texts alongside the moderated arguments of Debatepedia might also help to identify textual associations with general positions (Sim et al., 2013). We leave these directions to future work. 4.2 Qualitat</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="13983" citStr="Griffiths and Steyvers, 2004" startWordPosition="2188" endWordPosition="2191"> logarithm of the term’s argument frequency. The other priors were set to be symmetric: qe = 0.01 (entity topics), qt = 0.001 (topics), α = 50/T = 1.25 (topic mixture coefficients), Q = 0.01 (positions), and -y = 0.01 (functional term types). Preliminary tests showed that final topics are relatively insensitive to the values of the hyperparameters. 3.2 Inference and Parameter Estimation Exact inference under this model, like most latentvariable topic models, is intractable. We apply collapsed Gibbs sampling, a standard approach for such 9http://mpqa.cs.pitt.edu/lexicons/subj_ lexicon/ models (Griffiths and Steyvers, 2004).10 The notable deviations from typical uses of collapsed Gibbs sampling are: (i) we jointly sample id,� and id,2 to respect the constraint that they differ; and (ii) we fix the priors, in some cases to be asymmetric, as discussed in §3.1. We perform Gibbs sampling for 2,000 iterations over the dataset, discarding the first 500 iterations for burn-in, and averaging over every 10th iteration thereafter to get estimates for our term distributions. 3.3 T and K In all experiments, we use T = 40 topics and K = 2 positions. We did not extensively explore different values for T and K; preliminary exp</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="33379" citStr="Hu and Liu, 2004" startWordPosition="5234" endWordPosition="5237">ives a distinctive characterization of topics based on relevant personalities. Subjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao e</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of documentation,</journal>
<pages>28--1</pages>
<contexts>
<context position="13252" citStr="Jones, 1972" startWordPosition="2076" endWordPosition="2077">(Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct 77i and 77o. Specifically, terms w in the lexicon were given parameters qiw = qow = 0.01, and other terms were given qiw = qow = 0.001, capturing our prior belief that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We encode this in 77b, the prior over the background term distribution, by setting each value to the logarithm of the term’s argument frequency. The other priors were set to be symmetric: qe = 0.01 (entity topics), qt = 0.001 (topics), α = 50/T = 1.25 (topic mixture coefficients), Q = 0.01 (positions), and -y = 0.01 (functional term types). Preliminary tests showed that final topics are relatively insensitive to the values of the hyperparameters. 3.2 Inference and Parameter Estimation Exact inference under this model, like most latentvariable topic models, is intractable. We apply collapsed </context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlsch¨utter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<marker>Kohlsch¨utter, Fankhauser, Nejdl, 2010</marker>
<rawString>Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="2942" citStr="Lin and He, 2009" startWordPosition="411" endWordPosition="414">in Debatepedia. Further, we use named entity recognition as a preprocessing step, an existing sentiment lexicon to construct an informed prior, and we incorporate a latent, discrete position variable that cuts across debates.2 We evaluate the model informally and formally (§4). Subjectively, the model identifies reasonable topic and perspective terms, and it associates topics sensibly with important public figures. In quantitative evaluations, we find the model’s representation superior to topics from vanilla latent Dirichlet allocation (Blei et al., 2003) and the joint sentiment topic model (Lin and He, 2009) in matching external texts to debates. Further, the position variables can be used to infer the side of an argument within a debate; our model performs with an accuracy of 86% on position prediction of the debate argument. The cross-cutting position variable is not especially consistent with human judgments, suggesting that further knowledge sources may be required to improve interpretability across issues. 2 Data Debatepedia, like Wikipedia, is constructed by volunteer contributors and has a system of community 2This variable might serve to cluster debate sides according to “abstract beliefs</context>
<context position="12677" citStr="Lin and He, 2009" startWordPosition="1978" endWordPosition="1981">a. Draw id,�, id,2 ∼ Multinomial(ι), assigning each of the two sides to a position. b. ∀ questions q in d: i. Draw topic mixture proportions θd,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θd,q). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,H,s,a |id,�, id,2, zd,q,s,a). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct 77i and 77o. Specifically, terms w in the lexicon were given parameters qiw = qow = 0.01, and other terms were given qiw = qow = 0.001, capturing our prior belief that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We encode this in 77b, </context>
<context position="16306" citStr="Lin and He, 2009" startWordPosition="2562" endWordPosition="2565">.1 Quantitative Evaluation 4.1.1 Topics As described in §2, our corpus includes 3,352 articles hyperlinked by Debatepedia arguments.11 Our model can be used to infer the posterior over topics associated with such an article, and we compare that distribution to that of the Debatepedia article that links to it. Calculating the similarity of these distributions, we get an estimate of how closely our model can associate text related to a debate with the specific argument that linked to it. We compare with LDA (Blei et al., 2003), which ignores sentiment, and the joint sentiment topic (JST) model (Lin and He, 2009), an unsupervised model that jointly captures sentiment and topic.12 Using Jensen-Shannon divergence, we find that our approach embeds these pairs significantly closer than LDA and JST (also trained with 40 topics), under a Wilcoxon signed rank test (p &lt; 0.001). Figure 3 shows the histogram of divergences between our model, JST, and LDA. Associating external articles. More challenging, of course, is selecting the argument to which an external article should be associated. We used the Jensen-Shannon divergence between topic distributions of articles and arguments to rank the latter, for each ar</context>
<context position="34081" citStr="Lin and He, 2009" startWordPosition="5336" endWordPosition="5339"> above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching variable that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides o</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Eric Xing</author>
<author>Alexander Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In Proceedings of ECML-PKDD.</booktitle>
<contexts>
<context position="1869" citStr="Lin et al. (2008)" startWordPosition="259" endWordPosition="262">xts (editorials, blog posts, etc.) that express and evoke them. Empirical NLP, we propose, has a role to play in creating a more compact and easilyinterpretable way to understand the opinion space. In particular, we envision applications to computational journalism, where there is high demand for transformation of and pattern discovery in unmanageable, unstructured, evolving data (including text) to inform the public (Cohen et al., 2011). In this paper, we develop a generative model for discovering such a representation (§3), using Debatepedia as a corpus of evidence. We draw inspiration from Lin et al. (2008) and Ahmed and 1http://dbp.idebate.org Xing (2010), who used generative models to infer topics—distributions over words—and other wordassociated variables representing perspectives or ideologies. We view topics as lexicons, and propose that grounding a topic model with evidence beyond bags of words can lead to more lexicon-like representations. Specifically, our generative topic model grounds topics using the hierarchical organization of arguments within Debatepedia. Further, we use named entity recognition as a preprocessing step, an existing sentiment lexicon to construct an informed prior, </context>
<context position="35984" citStr="Lin et al. (2008)" startWordPosition="5644" endWordPosition="5647">based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level latent Dirichlet allocation, and Eisenstein et al. (2011) introduced sparse additive generative models, both conceived as extensions to well-established probabilistic modeling techniques (Blei et al., 2003); these were appl</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In Proceedings of ECML-PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="34100" citStr="Mei et al., 2007" startWordPosition="5340" endWordPosition="5343"> conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching variable that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides of a debate, and we </context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing clusterings by the variation of information.</title>
<date>2003</date>
<booktitle>Learning Theory and Kernel Machines,</booktitle>
<volume>2777</volume>
<pages>173--187</pages>
<editor>In Bernhard Sch¨olkopf and Manfred K. Warmuth, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="26602" citStr="Meila, 2003" startWordPosition="4187" endWordPosition="4188">ine, and one unlabeled side). A2 provided a coarse annotation along with each American politics. 14In a small number of cases, an annotator declined to label a side. Each unlabeled item received its own cluster. fine-grained one (liberal, conservative, ?, and two unlabeled sides). We used 100 samples from our Gibbs sampler to estimate posteriors for each id,,; these were always 99% or more in agreement, so we mapped each debate side into its single most probable cluster. Recall that the two sides of each debate must be in different clusters. Table 5 shows the variation of information measure (Meila, 2003) for each pairing among the three annotators and our model. The model agrees with A2’s coarse clustering most closely, and in fact is closer to A2’s clustering than A2 is to A3’s; it also agrees with A2’s coarse clustering better than A2’s coarse and fine clusterings agree (3.36, not shown in the table). This is promising, but we do not have confidence that the positional dimension is being captured especially well in this model; for those debate-sides labeled liberal or conservative by A2, the best match of our two positions was still only in agreement only about 60% of the time, and agreemen</context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>Marina Meila. 2003. Comparing clusterings by the variation of information. In Bernhard Sch¨olkopf and Manfred K. Warmuth, editors, Learning Theory and Kernel Machines, volume 2777 of Lecture Notes in Computer Science, pages 173–187. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Mining contentions from discussions and debates.</title>
<date>2012</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="33909" citStr="Mukherjee and Liu, 2012" startWordPosition="5309" endWordPosition="5312">tive polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Mining contentions from discussions and debates. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
</authors>
<title>Statistical entity-topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="33834" citStr="Newman et al., 2006" startWordPosition="5299" endWordPosition="5302">tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the p</context>
</contexts>
<marker>Newman, Chemudugunta, Smyth, 2006</marker>
<rawString>David Newman, Chaitanya Chemudugunta, and Padhraic Smyth. 2006. Statistical entity-topic models. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33333" citStr="Pang et al., 2002" startWordPosition="5227" endWordPosition="5230">stributions, shown for some topics in Table 8, gives a distinctive characterization of topics based on relevant personalities. Subjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opi</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="34993" citStr="Paul et al. (2010)" startWordPosition="5491" endWordPosition="5494">e refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching variable that assigns some tokens to positions, some to topics, and some to both. Unlike Lin and He’s sentiments, our model’s positions are associated with the two sides of a debate, and we incorporate topics at the level of questions within debates. Some studies have specifically analyzed contrastive viewpoints or stances in general discussion text.Agrawal et al. (2003) used graph mining based method to classify authors in to opposite camps for a given topic. Paul et al. (2010) developed an unsupervised method for summarizing contrastive opinions from customer reviews. Abu-Jbara et al. (2012) and Dasigi et al. (2012) developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and o</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Poole</author>
<author>Howard Rosenthal</author>
</authors>
<title>Patterns of congressional voting.</title>
<date>1991</date>
<journal>American Journal of Political Science,</journal>
<pages>118--178</pages>
<contexts>
<context position="30953" citStr="Poole and Rosenthal, 1991" startWordPosition="4853" endWordPosition="4856">rank in the top five for these topics. The model has conflated debates relating to same-sex marriage with the space program. ful, perhaps due to the less argumentative nature of other kinds of articles. Noting the vast literature focusing on ideological positions expressed in text, we believe this failure suggests (i) that broadbased positions that hold across many topics may require richer textual representations (see, e.g., the “syntactic priming” of Greene and Resnik, 2009), or (ii) that an alternative representation of positions, such as the spatial models favored by political scientists (Poole and Rosenthal, 1991), may be more discoverable. Aside from those issues, a stronger theory of positions may be required. Such a theory could be encoded in a more informative prior or weaker independence assumptions across debates. Finally, exploiting explicitly ideological texts alongside the moderated arguments of Debatepedia might also help to identify textual associations with general positions (Sim et al., 2013). We leave these directions to future work. 4.2 Qualitative Analysis Of the T = 40 topics our model inferred, we subjectively judged 37 to be coherent; a glimpse of each is given in Figure 5. We manual</context>
</contexts>
<marker>Poole, Rosenthal, 1991</marker>
<rawString>Keith Poole and Howard Rosenthal. 1991. Patterns of congressional voting. American Journal of Political Science, pages 118–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="31352" citStr="Sim et al., 2013" startWordPosition="4914" endWordPosition="4917">entations (see, e.g., the “syntactic priming” of Greene and Resnik, 2009), or (ii) that an alternative representation of positions, such as the spatial models favored by political scientists (Poole and Rosenthal, 1991), may be more discoverable. Aside from those issues, a stronger theory of positions may be required. Such a theory could be encoded in a more informative prior or weaker independence assumptions across debates. Finally, exploiting explicitly ideological texts alongside the moderated arguments of Debatepedia might also help to identify textual associations with general positions (Sim et al., 2013). We leave these directions to future work. 4.2 Qualitative Analysis Of the T = 40 topics our model inferred, we subjectively judged 37 to be coherent; a glimpse of each is given in Figure 5. We manually selected six of the most interpretable topics for further evaluation. As a generative modeling approach, our model was designed for the purpose of reducing the dimensionality of the sociopolitical debate space, as evidenced by Debatepedia. It is like other topic models in this regard, but we believe that some effects of our design choices are noteworthy. Table 6 compares the positional bigrams</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Yanchuan Sim, Brice Acree, Justin H. Gross, and Noah A. Smith. 2013. Measuring ideological proportions in political speeches. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35765" citStr="Somasundaran and Wiebe (2009)" startWordPosition="5609" endWordPosition="5612">developed techniques to address the problem of automatically detecting subgroups of people holding similar stances in a discussion thread. Several prior studies have considered debates. Cabrio and Villata (2012) developed a system based on argumentation theory which recognizes the entailment and contradiction relationships between two texts. Awadallah et al. (2011) used a debate corpus as a seed for extracting person-opinion-topic tuples from news and other web documents and in later work classified the quotations to specific topics and polarity using language models (Awadallah et al., 2012). Somasundaran and Wiebe (2009) and Anand et al. (2011) were interested in ideological content in debates, relying on discourse structure and leveraging sentiment lexicons to recognize stances. Closer to the methodology we describe, Lin et al. (2008) presented a statistical model for political discourse that incorporates both topics and ideologies; they used debates on the Israeli-Palestinian conflict. Fortuna et al. (2009) showed that it is possible to isolate a subset of terms from media content that are informative of a news organization’s bias towards a particular issue. Ahmed and Xing (2010) introduced multi-level late</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="33788" citStr="Titov and McDonald, 2008" startWordPosition="5292" endWordPosition="5295">n (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to th</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teun A Van Dijk</author>
</authors>
<title>Ideology: A Multidisciplinary Approach.</title>
<date>1998</date>
<publisher>Sage Publications Limited.</publisher>
<marker>Van Dijk, 1998</marker>
<rawString>Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary Approach. Sage Publications Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The trec-8 question answering track report.</title>
<date>1999</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="16961" citStr="Voorhees, 1999" startWordPosition="2664" endWordPosition="2665">tures sentiment and topic.12 Using Jensen-Shannon divergence, we find that our approach embeds these pairs significantly closer than LDA and JST (also trained with 40 topics), under a Wilcoxon signed rank test (p &lt; 0.001). Figure 3 shows the histogram of divergences between our model, JST, and LDA. Associating external articles. More challenging, of course, is selecting the argument to which an external article should be associated. We used the Jensen-Shannon divergence between topic distributions of articles and arguments to rank the latter, for each article. The mean reciprocal rank scores (Voorhees, 1999) for LDA, JST, and our model were 11We consider only those articles linked by a single Debatepedia argument. 12JST multiplies topics out by the set of sentiment labels, assigning each token to both a topic and a sentment. We use the OpinionFinder lexicon in JST’s prior in the same way it is used in our model. 5 10 15 20 25 inf K Figure 4: Mean reciprocal ranks for the association task. 0.1272, 0.1421, and 0.1507, respectively; the difference is significant (Wilcoxon signed rank test, p &lt; 0.001). We found the same pattern for MRR@k, k E 15,10,15, 20, 25, oc1, as shown in Figure 4. It is likely </context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Ellen M. Voorhees. 1999. The trec-8 question answering track report. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="9975" citStr="Wallach (2006)" startWordPosition="1513" endWordPosition="1514">that cuts across different questions and arguments. In our experiments, there are two positions, and the two sides of a debate are constrained to associate with opposing positions. As illustrated by Table 1, this assump5http://code.google.com/p/boilerpipe 6www.ranks.nl/resources/stopwords.html 7Recall that our model includes bigrams. We treat each unigram and bigram token (after filtering discussed in §2.1) as a separate term. 8In future work, more sharing across questions within a debate, or more differentiation among the topic distributions for arguments under a question, might be explored. Wallach (2006) describes suitable techniques using hierarchical Dirichlet draws, and Eisenstein et al. (2011) suggests the use of sparse shocks to log-odds at different levels. Here we work on the assumption that Debatepedia’s questions are the most topically coherent level, and work with a single topic mixture at this level. Figure 1: Plate diagram. K is the number of positions, and T is number of topics. The shaded variables are observed and dashed variables are marginalized. α, ,Q, -y and all rl are fixed hyperparameters (§3.1). tion is not always correct, though it tends to hold most of the time. • Each</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: beyond bagof-words. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<contexts>
<context position="33189" citStr="Wiebe and Riloff, 2005" startWordPosition="5205" endWordPosition="5208">erve consistent alignment across topics, and the general distributions are not suggestive. The separation of personal name mentions into their own distributions, shown for some topics in Table 8, gives a distinctive characterization of topics based on relevant personalities. Subjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008),</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="12744" citStr="Wilson et al., 2005" startWordPosition="1989" endWordPosition="1992">ides to a position. b. ∀ questions q in d: i. Draw topic mixture proportions θd,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θd,q). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,H,s,a |id,�, id,2, zd,q,s,a). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct 77i and 77o. Specifically, terms w in the lexicon were given parameters qiw = qow = 0.01, and other terms were given qiw = qow = 0.001, capturing our prior belief that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We encode this in 77b, the prior over the background term distribution, by setting each va</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33460" citStr="Wu et al., 2009" startWordPosition="5246" endWordPosition="5249">bjectively, the top individuals are relevant to the subject matter associated with each topic (though the topics are not always pure; same-sex marriage and the space program are merged, for example). 5 Related Work Insofar as debates are subjective, our study is related to opinion mining. Subjective text classification (Wiebe and Riloff, 2005) leads to opinion mining tasks such as opinion extraction (Dave et al., 2003), positive and negative polarity classification (Pang et al., 2002), sentiment target detection (Hu and Liu, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extract</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a maxent-lda hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12658" citStr="Zhao et al., 2010" startWordPosition="1974" endWordPosition="1977">). 7. ∀ debates d: a. Draw id,�, id,2 ∼ Multinomial(ι), assigning each of the two sides to a position. b. ∀ questions q in d: i. Draw topic mixture proportions θd,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θd,q). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,H,s,a |id,�, id,2, zd,q,s,a). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct 77i and 77o. Specifically, terms w in the lexicon were given parameters qiw = qow = 0.01, and other terms were given qiw = qow = 0.001, capturing our prior belief that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We e</context>
<context position="33991" citStr="Zhao et al., 2010" startWordPosition="5322" endWordPosition="5325">, 2004; Ganapathibhotla and Liu, 2008), and featureopinion extraction (Wu et al., 2009). The above studies are conducted mostly on product reviews, a domain with a simpler opinion landscape and more concrete rationales for those opinions, compared to sociopolitical debates. Generative topic models have been successfully implemented in opinion mining tasks such as feature identification (Titov and McDonald, 2008), entitytopic extraction (Newman et al., 2006), mining contentious expressions and interactions (Mukherjee and Liu, 2012) and specific aspect-opinion word extraction from labeled data (Zhao et al., 2010). Most relevant to this research is work on feature-sentiment extraction (Lin and He, 2009; Mei et al., 2007). Mei et al. (2007) built on PLSI, which is problematic for generalizing beyond the training sample. The JST model of Lin and He (2009) is an LDA-based topic model in which each word token is assigned both a sentiment and a topic; they exploited a sen16For more topics, please refer to the supplementary notes. timent lexicon in the prior distribution. Our model is closely related, but introduces a switching variable that assigns some tokens to positions, some to topics, and some to both.</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a maxent-lda hybrid. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
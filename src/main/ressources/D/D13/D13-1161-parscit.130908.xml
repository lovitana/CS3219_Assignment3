<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000172">
<title confidence="0.992827">
Scaling Semantic Parsers with On-the-fly Ontology Matching
</title>
<author confidence="0.998358">
Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke Zettlemoyer
</author>
<affiliation confidence="0.995053">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.960717">
Seattle, WA 98195
</address>
<email confidence="0.999538">
{tomk,eunsol,yoav,lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965416666667">
We consider the challenge of learning seman-
tic parsers that scale to large, open-domain
problems, such as question answering with
Freebase. In such settings, the sentences cover
a wide variety of topics and include many
phrases whose meaning is difficult to rep-
resent in a fixed target ontology. For ex-
ample, even simple phrases such as ‘daugh-
ter’ and ‘number of people living in’ can-
not be directly represented in Freebase, whose
ontology instead encodes facts about gen-
der, parenthood, and population. In this pa-
per, we introduce a new semantic parsing ap-
proach that learns to resolve such ontologi-
cal mismatches. The parser is learned from
question-answer pairs, uses a probabilistic
CCG to build linguistically motivated logical-
form meaning representations, and includes
an ontology matching model that adapts the
output logical forms for each target ontology.
Experiments demonstrate state-of-the-art per-
formance on two benchmark semantic parsing
datasets, including a nine point accuracy im-
provement on a recent Freebase QA corpus.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999695764705883">
Semantic parsers map sentences to formal represen-
tations of their underlying meaning. Recently, al-
gorithms have been developed to learn such parsers
for many applications, including question answering
(QA) (Kwiatkowski et al., 2011; Liang et al., 2011),
relation extraction (Krishnamurthy and Mitchell,
2012), robot control (Matuszek et al., 2012; Kr-
ishnamurthy and Kollar, 2013), interpreting instruc-
tions (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), and generating programs (Kushman
and Barzilay, 2013).
In each case, the parser uses a predefined set
of logical constants, or an ontology, to construct
meaning representations. In practice, the choice
of ontology significantly impacts learning. For
example, consider the following questions (Q) and
candidate meaning representations (MR):
</bodyText>
<listItem confidence="0.80534225">
Q1: What is the population of Seattle?
Q2: How many people live in Seattle?
MR1: Ax.population(Seattle, x)
MR2: count(Ax.person(x) ∧ live(x, Seattle))
</listItem>
<bodyText confidence="0.9999581">
A semantic parser might aim to construct MR1 for
Q1 and MR2 for Q2; these pairings align constants
(count, person, etc.) directly to phrases (‘How
many,’ ‘people,’ etc.). Unfortunately, few ontologies
have sufficient coverage to support both meaning
representations, for example many QA databases
would only include the population relation required
for MR1. Most existing approaches would, given
this deficiency, simply aim to produce MR1 for Q2,
thereby introducing significant lexical ambiguity
that complicates learning. Such ontological mis-
matches become increasingly common as domain
and language complexity increases.
In this paper, we introduce a semantic parsing ap-
proach that supports scalable, open-domain ontolog-
ical reasoning. The parser first constructs a linguis-
tically motivated domain-independent meaning rep-
resentation. For example, possibly producing MR1
for Q1 and MR2 for Q2 above. It then uses a learned
ontology matching model to transform this represen-
</bodyText>
<page confidence="0.941966">
1545
</page>
<note confidence="0.729896">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<equation confidence="0.981353875">
x : How many people visit the public library of New York annually
l0 : ax.eq(x, count(ay.people(y) n 3e.visit(y, cz.public(z) n library(z) n of(z, new york), e) n annually(e)))
y : Ax.library.public library system.annual visits(x, new york public library)
a : 13,554,002
x : What works did Mozart dedicate to Joseph Haydn
l0 : ax.works(x) n 3e.dedicate(mozart, x, e) n to(haydn, e)))
y : Ax.dedicated work(x) n 3e.dedicated by(mozart, e) n dedication(x, e) n dedicated to(haydn, e)))
a : { String Quartet No. 19, Haydn Quartets, String Quartet No. 16, String Quartet No. 18, String Quartet No. 17 }
</equation>
<figureCaption confidence="0.9754205">
Figure 1: Examples of sentences x, domain-independent underspecified logical forms l0, fully specified
logical forms y, and answers a drawn from the Freebase domain.
</figureCaption>
<bodyText confidence="0.999938673913043">
tation for the target domain. In our example, pro-
ducing either MR1, MR2 or another more appropri-
ate option, depending on the QA database schema.
This two stage approach enables parsing without
any domain-dependent lexicon that pairs words with
logical constants. Instead, word meaning is filled
in on-the-fly through ontology matching, enabling
the parser to infer the meaning of previously un-
seen words and more easily transfer across domains.
Figure 1 shows the desired outputs for two example
Freebase sentences.
The first parsing stage uses a probabilistic combi-
natory categorial grammar (CCG) (Steedman, 2000;
Clark and Curran, 2007) to map sentences to
new, underspecified logical-form meaning represen-
tations containing generic logical constants that are
not tied to any specific ontology. This approach en-
ables us to share grammar structure across domains,
instead of repeatedly re-learning different grammars
for each target ontology. The ontology-matching
step considers a large number of type-equivalent
domain-specific meanings. It enables us to incorpo-
rate a number of cues, including the target ontology
structure and lexical similarity between the names of
the domain-independent and dependent constants, to
construct the final logical forms.
During learning, we estimate a linear model over
derivations that include all of the CCG parsing de-
cisions and the choices for ontology matching. Fol-
lowing a number of recent approaches (Clarke et al.,
2010; Liang et al., 2011), we treat all intermediate
decisions as latent and learn from data containing
only easily gathered question answer pairs. This ap-
proach aligns naturally with our two-stage parsing
setup, where the final logical expression can be di-
rectly used to provide answers.
We report performance on two benchmark
datasets: GeoQuery (Zelle and Mooney, 1996) and
Freebase QA (FQ) (Cai and Yates, 2013a). Geo-
Query includes a geography database with a small
ontology and questions with relatively complex,
compositional structure. FQ includes questions to
Freebase, a large community-authored database that
spans many sub-domains. Experiments demonstrate
state-of-the-art performance in both cases, including
a nine point improvement in recall for the FQ test.
</bodyText>
<sectionHeader confidence="0.991528" genericHeader="introduction">
2 Formal Overview
</sectionHeader>
<bodyText confidence="0.999974347826087">
Task Let an ontology O be a set of logical con-
stants and a knowledge base K be a collection of
logical statements constructed with constants from
O. For example, K could be facts in Freebase (Bol-
lacker et al., 2008) and O would define the set
of entities and relation types used to encode those
facts. Also, let y be a logical expression that can
be executed against K to return an answer a =
EXEC(y, K). Figure 1 shows example queries and
answers for Freebase. Our goal is to build a function
y = PARSE(x, O) for mapping a natural language
sentence x to a domain-dependent logical form y.
Parsing We use a two-stage approach to define
the space of possible parses GEN(x, O) (Section 5).
First, we use a CCG and word-class information
from Wiktionary1 to build domain-independent un-
derspecified logical forms, which closely mirror the
linguistic structure of the sentence but do not use
constants from O. For example, in Figure 1, l0 de-
notes the underspecified logical forms paired with
each sentence x. The parser then maps this interme-
diate representation to a logical form that uses con-
stants from O, such as the y seen in Figure 1.
</bodyText>
<footnote confidence="0.995261">
1www.wiktionary.com
</footnote>
<page confidence="0.995735">
1546
</page>
<bodyText confidence="0.999624583333333">
Learning We assume access to data containing
question-answer pairs {(xZ, az) : i = 1... n} and
a corresponding knowledge base K. The learn-
ing algorithm (Section 7.1) estimates the parame-
ters of a linear model for ranking the possible en-
tires in GEN(x, O). Unlike much previous work
(e.g., Zettlemoyer and Collins (2005)), we do not
induce a CCG lexicon. The lexicon is open domain,
using no symbols from the ontology O for K. This
allows us to write a single set of lexical templates
that are reused in every domain (Section 5.1). The
burden of learning word meaning is shifted to the
second, ontology matching, stage of parsing (Sec-
tion 5.2), and modeled with a number of new fea-
tures (Section 7.2) as part of the joint model.
Evaluation We evaluate on held out question-
answer pairs in two benchmark domains, Freebase
and GeoQuery. Following Cai and Yates (2013a),
we also report a cross-domain evaluation where the
Freebase data is divided by topics such as sports,
film, and business. This condition ensures that the
test data has a large percentage of previously unseen
words, allowing us to measure the effectiveness of
the real time ontology matching.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99997105">
Supervised approaches for learning semantic parsers
have received significant attention, e.g. (Kate and
Mooney, 2006; Wong and Mooney, 2007; Muresan,
2011; Kwiatkowski et al., 2010, 2011, 2012; Jones
et al., 2012). However, these techniques require
training data with hand-labeled domain-specific log-
ical expressions. Recently, alternative forms of su-
pervision were introduced, including learning from
question-answer pairs (Clarke et al., 2010; Liang
et al., 2011), from conversational logs (Artzi and
Zettlemoyer, 2011), with distant supervision (Kr-
ishnamurthy and Mitchell, 2012; Cai and Yates,
2013b), and from sentences paired with system
behavior (Goldwasser and Roth, 2011; Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013). Our
work adds to these efforts by demonstrating a new
approach for learning with latent meaning represen-
tations that scales to large databases like Freebase.
Cai and Yates (2013a) present the most closely
related work. They applied schema matching tech-
niques to expand a CCG lexicon learned with the
UBL algorithm (Kwiatkowski et al., 2010). This ap-
proach was one of the first to scale to Freebase, but
required labeled logical forms and did not jointly
model semantic parsing and ontological reasoning.
This method serves as the state of the art for our
comparison in Section 9.
We build on a number of existing algorithmic
ideas, including using CCGs to build meaning rep-
resentations (Zettlemoyer and Collins, 2005, 2007;
Kwiatkowski et al., 2010, 2011), building deriva-
tions to transform the output of the CCG parser
based on context (Zettlemoyer and Collins, 2009),
and using weakly supervised margin-sensitive pa-
rameter updates (Artzi and Zettlemoyer, 2011,
2013). However, we introduce the idea of learning
an open-domain CCG semantic parser; all previous
methods suffered, to various degrees, from the onto-
logical mismatch problem that motivates our work.
The challenge of ontological mismatch has been
previously recognized in many settings. Hobbs
(1985) describes the need for ontological promiscu-
ity in general language understanding. Many pre-
vious hand-engineered natural language understand-
ing systems (Grosz et al., 1987; Alshawi, 1992; Bos,
2008) are designed to build general meaning rep-
resentations that are adapted for different domains.
Recent efforts to build natural language interfaces to
large databases, for example DBpedia (Yahya et al.,
2012; Unger et al., 2012), have also used hand-
engineered ontology matching techniques. Fader
et al. (2013) recently presented a scalable approach
to learning an open domain QA system, where onto-
logical mismatches are resolved with learned para-
phrases. Finally, the databases research commu-
nity has a long history of developing schema match-
ing techniques (Doan et al., 2004; Euzenat et al.,
2007), which has inspired more recent work on dis-
tant supervision for relation extraction with Free-
base (Zhang et al., 2012).
</bodyText>
<sectionHeader confidence="0.997109" genericHeader="method">
4 Background
</sectionHeader>
<bodyText confidence="0.981342833333333">
Semantic Modeling We use the typed lambda cal-
culus to build logical forms that represent the mean-
ings of words, phrases and sentences. Logical forms
contain constants, variables, lambda abstractions,
and literals. In this paper, we use the term literal to
refer to the application of a constant to a sequence of
</bodyText>
<page confidence="0.974654">
1547
</page>
<figure confidence="0.987131714285714">
library of new york
N N\N NP NP
Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC
N\N
Af.Ax.f(x) n loc(x, NYC)
N
Ax.library(x) n loc(x, NYC)
</figure>
<figureCaption confidence="0.999803">
Figure 2: A sample CCG parse.
</figureCaption>
<bodyText confidence="0.986783590909091">
arguments. We include types for entities e, truth val-
ues t, numbers i, events ev, and higher-order func-
tions, such as (e, t) and ((e, t), e). We use David-
sonian event semantics (Davidson, 1967) to explic-
itly represent events using event-typed variables and
conjunctive modifiers to capture thematic roles.
Combinatory Categorial Grammars (CCG)
CCGs are a linguistically-motivated formalism
for modeling a wide range of language phenom-
ena (Steedman, 1996, 2000). A CCG is defined by
a lexicon and a set of combinators. The lexicon
contains entries that pair words or phrases with
CCG categories. For example, the lexical entry
library �- N : Ax.library(x) in Figure 2 pairs
the word ‘library’ with the CCG category that has
syntactic category N and meaning Ax.library(x).
A CCG parse starts from assigning lexical entries to
words and phrases. These are then combined using
the set of CCG combinators to build a logical form
that captures the meaning of the entire sentence. We
use the application, composition, and coordination
combinators. Figure 2 shows an example parse.
</bodyText>
<sectionHeader confidence="0.935424" genericHeader="method">
5 Parsing Sentences to Meanings
</sectionHeader>
<bodyText confidence="0.999910777777778">
The function GEN(x, O) defines the set of possible
derivations for an input sentence x. Each derivation
d = (H, M) builds a logical form y using constants
from the ontology O. H is a CCG parse tree that
maps x to an underspecified logical form l0. M is an
ontological match that maps l0 onto the fully spec-
ified logical form y. This section describes, with
reference to the example in Figure 3, the operations
used by H and M.
</bodyText>
<subsectionHeader confidence="0.973684">
5.1 Domain Independent Parsing
</subsectionHeader>
<bodyText confidence="0.999483">
Domain-independent CCG parse trees H are built
using a predefined set of 56 underspecified lexi-
cal categories, 49 domain-independent lexical items,
and the combinatory rules introduced in Section 4.
An underspecified CCG lexical category has a
syntactic category and a logical form containing no
constants from the domain ontology O. Instead, the
logical form includes underspecified constants that
are typed placeholders which will later be replaced
during ontology matching. For example, a noun
might be assigned the lexical category N : Ax.p(x),
where p is an underspecified (e, t)-type constant.
During parsing, lexical categories are created dy-
namically. We manually define a set of POS tags for
each underspecified lexical category, and use Wik-
tionary as a tag dictionary to define the possible POS
tags for words and phrases. Each phrase is assigned
every matching lexical category. For example, the
word ‘visit’ can be either a verb or a noun in Wik-
tionary. We accordingly assign it all underspecified
categories for the classes, including:
</bodyText>
<equation confidence="0.728438">
N:Ax.p(x) , S\NP/NP:AxAy]ev.p(y, x, ev)
</equation>
<bodyText confidence="0.999537285714286">
for nouns and transitive verbs respectively.
We also define domain-independent lexical items
for function words such as ‘what,’ ‘when,’ and
‘how many,’ ‘and,’ and ‘is.’ These lexi-
cal items pair a word with a lexical cate-
gory containing only domain-independent con-
stants. For example, how many �- S/(S\NP)/N :
Af.Ag.Ax.eq(x, count(Ay.f(y) n g(y))) contains
the function count and the predicate eq.
Figure 3a shows the lexical categories and combi-
nator applications used to construct the underspeci-
fied logical form l0. Underspecified constants in this
figure have been labeled with the words that they are
associated with for readability.
</bodyText>
<subsectionHeader confidence="0.999114">
5.2 Ontological Matching
</subsectionHeader>
<bodyText confidence="0.999964111111111">
The second, domain specific, step M maps the un-
derspecified logical form l0 onto the fully specified
logical form y. The mapping from constants in l0
to constants in y is not one-to-one. For example, in
Figure 3, l0 contains 11 constants but y contains only
2. The ontological match is a sequence of matching
operations M = (o1 ..., on) that can transform the
structure of the logical form or replace underspeci-
fied constants with constants from O.
</bodyText>
<page confidence="0.963677">
1548
</page>
<figure confidence="0.914319">
(a) Underspecified CCG parse II: Map words onto underspecified lexical categories as described in Section 5.1. Use
the CCG combinators to combine lexical categories to give the full underpecified logical form l0.
(b) Structure Matching Steps in M: Use the operators described in Section 5.2.1 and Figure 4 to transform l0. In
</figure>
<bodyText confidence="0.7603075">
each step one of the operators is applied to a subexpression of the existing logical form to generate a modified logical
form with a new underspecified constant marked in bold.
</bodyText>
<equation confidence="0.998117666666667">
l0 : Ax.eq(x, count(Ay.People(y) ∧ ∃e.Visit(y, tz.Public(z) ∧ Library(z) ∧ Of(z, NewYork), e) ∧ Annually(e)))
l1 : Ax.eq(x, count(Ay.People(y) ∧ ∃e.V isit(y, PublicLibraryOfNewYork, e) ∧ Annually(e)))
l2 : Ax.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewY ork)))
</equation>
<listItem confidence="0.663228">
(c) Constant Matching Steps in M: Replace all underspecified constants in the transformed logical form with a
similarly typed constant from O, as described in Section 5.2.2. The underspecified constant to be replaced is marked
in bold and constants from O are written in typeset.
</listItem>
<equation confidence="0.951513">
Ax.HowManyPeopleVisitAnnually(x, PublicLibraryOfNewYork)
l3 : l--&gt; Ax.HowManyPeopleVisitAnnually(x, new york public library)
Ax.HowManyPeopleVisitAnnually(x,new york public library)
y : i--&gt; Ax.public library system.annual visits(x,new york public library)
</equation>
<figureCaption confidence="0.985768333333333">
Figure 3: Example derivation for the query ‘how many people visit the public library of new york annu-
ally.’ Underspecified constants are labelled with the words from the query that they are associated with for
readability. Constants from O, written in typeset, are introduced in step (c).
</figureCaption>
<bodyText confidence="0.97070452631579">
Operator Definition and Conditions Example
Collapse P(a1, ... , an) 7→ c �z.Public(z) ∧ Library(z) ∧ Of(z, NewY ork))
Literal s.t. type(P(a1, ... , an)) = type(c) 7→ PublicLibraryOfNewY ork
to type(c) ∈ {e, i} Input and output have type e.
Constant freev(P(a1, ... , an)) = ∅ e is allowed in O.
Input contains no free variables.
Collapse P(a1, . . . , an) 7→ Q(b1, . . ., bm) eq(x, count(Ay.People(y) ∧ ∃e.V isit(y,
Literal s.t. type(P(a1, ... , an)) = type(Q(b1, ... , bm)) PublicLibraryOfNewYork) ∧ Annually(e)))
to type(Q) ∈ {type(c) : c ∈ O} 7→ CountPeopleV isitAnnually(x,
Literal freev(P(a1, ... , an)) = freev(Q(b1, ... , bm)) PublicLibraryOfNewY ork)
{b1, ... , bm} ∈ subexps(P(a1, ... , an)) Input and output have type t.
New constant has type hi, he, tii, allowed in O.
Input and output contain single free variable x.
Arguments of output literal are subexpressions of input.
c. Split P(a1, ... , ak, x, ak+1, ... , an) Dedicate(Mozart, Haydn, ev)
Literal 7→ Q(b1, ... , x,... bn) ∧ Q��(c1, ... , x,... cm) 7→ Dedicate(Mozart, ev) ∧ Dedicate��(Haydn, ev)
s.t. type(P(... )) = t Input has type t. This matches output type by definition.
{type(Q), type(Q&amp;quot;)} ∈ {type(c) : c ∈ O} New constants have allowed type he, hev, tii.
{b1, ... , bn, c1, ..., cm} = {a1, ... , an} All arguments of input literal are preserved in output.
</bodyText>
<figureCaption confidence="0.9994275">
Figure 4: Definition of the operations used to transform the structure of the underspecified logical form l0 to
match the ontology O. The function type(c) calculates a constant c’s type. The function freev(lf) returns
the set of variables that are free in lf (not bound by a lambda term or quantifier). The function subexps(lf)
generates the set of all subexpressions of the lambda calculus expression lf.
</figureCaption>
<figure confidence="0.966845833333333">
how many people visit the public library of new york annually
S/(S\NP)/N N S\NP/NP NP/N N/N N N\N/NP NP AP
af.ag.ax.eq(x, count( ax.People(x) ax.ay.�ev. af.tx.f(x) af.ax.f(x)∧ ax.Library(x) ay.af.ax.Of NewYork aev.Annually(ev)
ay.f(y) ∧ g(y))) Visit(y, x, ev) Public(x) (x, y) ∧ f(x)
� �
�
�
�
� �
�
S
l0 : ax.eq(x, count(ay.People(y) ∧ 3e.Visit(y, tz.Public(z) ∧ Library(z) ∧ Of(z, NewYork)) ∧ Annually(e)))
</figure>
<page confidence="0.937655">
1549
</page>
<subsubsectionHeader confidence="0.477732">
5.2.1 Structure Matching
</subsubsectionHeader>
<bodyText confidence="0.998557650793651">
Three structure matching operators, illustrated in
Figure 4, are used to collapse or expand literals in
l0. Collapses merge a subexpression from l0 to cre-
ate a new underspecified constant, generating a log-
ical form with fewer constants. Expansions split a
subexpression from l0 to generate a new logical form
containing one extra constant.
Collapsing Operators The collapsing operator
defined in Figure 4a merges all constants in a
literal to generate a single constant of the same
type. This operator is used to map cz.Public(z)n
Library(z)nOf(z,NewY ork) to PublicLibraryOfNewY ork
in Figure 3b. Its operation is limited to entity typed
expressions that do not contain free variables.
The operator in Figure 4b, in contrast, can be used
to collapse the expression eq(x,count(Ay.People(y)n
�le.Visit(y,PublicLibraryOfNewY ork,e))nAnnually(e))),
which contains free variable x onto a new expression
CountPeopleVisitAnnually(x,PublicLibraryOfNewY ork).
This is only possible when the type of the newly
created constant is allowed in 0 and the variable x
is free in the output expression. Subsets of conjuncts
can be collapsed using the operator in Figure 4b by
creating ad-hoc conjunctions that encapsulate them.
Disjunctions are treated similarly.
Performing collapses on the underspecified logi-
cal form allows non-contiguous phrases to be rep-
resented in the collapsed form. In this exam-
ple, the logical form representing the phrase ‘how
many people visit’ has been merged with the logi-
cal form representing the non-adjacent adverb ‘an-
nually.’ This generates a new underspecified con-
stant that can be mapped onto the Freebase relation
public library system annual visits that re-
lates to both phrases.
The collapsing operations preserve semantic type,
ensuring that all logical forms generated by the
derivation sequence are well typed. The full set of
allowed collapses of l0 is given by the transitive clo-
sure of the collapsing operations. The size of this
set is limited by the number of constants in l0, since
each collapse removes at least one constant. At each
step, the number of possible collapses is polynomial
in the number of constants in l0 and exponential in
the arity of the most complex type in 0. For do-
mains of interest this arity is unlikely to be high and
for triple stores such as Freebase it is 2.
Expansion Operators The fully specified logical
form y can contain constants relating to multiple
words in x. It can also use multiple constants to rep-
resent the meaning of a single word. For example,
Freebase does not contain a relation representing the
concept ‘daughter’, instead using two relations rep-
resenting ‘female’ and ‘child’. The expansion oper-
ator in Figure 4c allows a single predicate to be split
into a pair of conjoined predicates sharing an argu-
ment variable. For example, in Figure 1, the constant
for ‘dedicate’ is split in two to match its represen-
tation in Freebase. Underspecified constants from
l0 can be split once. For the experiments in Sec-
tion 8, we constrain the expansion operator to work
on event modifiers but the procedure generalizes to
all predicates.
</bodyText>
<subsubsectionHeader confidence="0.846739">
5.2.2 Constant Matching
</subsubsectionHeader>
<bodyText confidence="0.9999665">
To build an executable logical form y, all under-
specified constants must be replaced with constants
from 0. This is done through a sequence of con-
stant replacement operations, each of which replaces
a single underspecified constant with a constant of
the same type from 0. Two example replacements
are shown in Figure 3c. The output from the last re-
placement operation is a fully specified logical form.
</bodyText>
<sectionHeader confidence="0.965045" genericHeader="method">
6 Building and Scoring Derivations
</sectionHeader>
<bodyText confidence="0.9996895">
This section introduces a dynamic program used to
construct derivations and a linear scoring model.
</bodyText>
<subsectionHeader confidence="0.998823">
6.1 Building Derivations
</subsectionHeader>
<bodyText confidence="0.999913428571429">
The space of derivations is too large to explicitly
enumerate. However, each logical form (both final
and interim) can be constructed with many differ-
ent derivations, and we only need to find the highest
scoring one. This allows us to develop a simple dy-
namic program for our two-stage semantic parser.
We use a CKY style chart parser to calculate the
k-best logical forms output by parses of x. We then
store each interim logical form generated by an op-
erator in M once in a hyper-graph chart structure.
The branching factor of this hypergraph is polyno-
mial in the number of constants in l0 and linear in
the size of 0. Subsequently, there are too many
possible logical forms to enumerate explicitly; we
</bodyText>
<page confidence="0.96415">
1550
</page>
<bodyText confidence="0.9998264">
prune as follows. We allow the top N scoring on-
tological matches for each original subexpression in
l0 and remove matches that differ from score from
the maximum scoring match by more than a thresh-
old T. When building derivations, we apply constant
matching operators as soon as they are applicable to
new underspecified constants created by collapses
and expansions. This allows the scoring function
used by the pruning strategy to take advantage of all
features defined in Section 7.2.
</bodyText>
<subsectionHeader confidence="0.999283">
6.2 Ranking Derivations
</subsectionHeader>
<bodyText confidence="0.9999685">
Given feature vector 0 and weight vector B, the score
of a derivation d = (H, M) is a linear function that
decomposes over the parse tree H and the individual
ontology-matching steps o.
</bodyText>
<equation confidence="0.995460333333333">
SCORE(d) = 0(d)B (1)
= 0(H)B + � 0(o)B
oEM
</equation>
<bodyText confidence="0.999691333333333">
The function PARSE(x, O) introduced as our goal in
Section 2 returns the logical form associated with
the highest scoring derivation of x:
</bodyText>
<equation confidence="0.9884845">
PARSE(x, O) = arg max(SCORE(d))
dEGEN(x,O)
</equation>
<bodyText confidence="0.9960665">
The features and learning algorithm used to estimate
B are defined in the next section.
</bodyText>
<sectionHeader confidence="0.989706" genericHeader="method">
7 Learning
</sectionHeader>
<bodyText confidence="0.999905666666667">
This section describes an online learning algorithm
for question-answering data, along with the domain-
independent feature set.
</bodyText>
<subsectionHeader confidence="0.988933">
7.1 Learning Model Parameters
</subsectionHeader>
<bodyText confidence="0.96677775">
Our learning algorithm estimates the parameters B
from a set {(xi, ai) : i = 1... n} of questions xi
paired with answers ai from the knowledge base
K. Each derivation d generated by the parser is
associated with a fully specified logical form y =
YIELD(d) that can be executed in K. A derivation d
of xi is correct if EXEC(YIELD(d), K) = ai. We use
a perceptron to estimate a weight vector B that sup-
port a separation of -y between correct and incorrect
answers. Figure 5 presents the learning algorithm.
Input: Q/A pairs {(xi, ai) : i = 1 ... n}; Knowledge base
K; Ontology O; Function GEN(x, O) that computes deriva-
tions of x; Function YIELD(d)that returns logical form yield
of derivation d; Function EXEC(y, K) that calculates execu-
tion of y in K; Margin -y; Number of iterations T.
Output: Linear model parameters B.
</bodyText>
<equation confidence="0.907336875">
Algorithm:
Fort = 1 ... T, i = 1 ... n :
C = {d : d ∈ GEN(xi, O); EXEC(YIELD(d), K) = ai}
W = {d : d ∈ GEN(xi, O); EXEC(YIELD(d), K) =6 ai}
C* = arg maxdEC(0(d)B)
W* = {d : d ∈ W; ∃c ∈ C* s.t. 0(c)B − 0(d)B &lt; -y)}
If |C* |&gt; 0 ∧ |W* |&gt; 0 :
B = B + |C * |EIEC* O(c) −|W* |EIEW* O(e)
</equation>
<figureCaption confidence="0.989346">
Figure 5: Parameter estimation from Q/A pairs.
</figureCaption>
<subsectionHeader confidence="0.705381">
7.2 Features
</subsectionHeader>
<bodyText confidence="0.994416931034482">
The feature vector 0(d) introduced in Section 6.2
decomposes over each of the derivation steps in d.
CCG Parse Features Each lexical item in H has
three indicator features. The first indicates the num-
ber of times each underspecified category is used.
For example, the parse in Figure 3a uses the under-
specified category N : Ax.p(x) twice. The second
feature indicates (word, category) pairings — e.g.
that N : Ax.p(x) is paired with ‘library’ and ‘pub-
lic’ once each in Figure 3a. The final lexical feature
indicates (part-of-speech, category) pairings for all
parts of speech associated with the word.
Structural Features The structure matching op-
erators (Section 5.2.1) in M generate new under-
specified constants that define the types of constants
in the output logical form y. These operators are
scored using features that indicate the type of each
complex-typed constant present in y and the iden-
tity of domain-independent functional constants in
y. The logical form y generated in Figure 3 contains
one complex typed constant with type (i, (e, t)) and
no domain-independent functional constants. Struc-
tural features allow the model to adapt to different
knowledge bases K. They allow it to determine, for
example, whether a numeric quantity such as ‘pop-
ulation’ is likely to be explicitly listed in K or if it
should be computed with the count function.
Lexical Features Each constant replacement op-
erator (Section 5.2.2) in M replaces an underspec-
</bodyText>
<page confidence="0.985365">
1551
</page>
<bodyText confidence="0.993625376344086">
ified constant cu with a constant co from O. The
underspecified constant cu is associated with the se-
quence of words ~wu used in the CCG lexical en-
tries that introduced it in H. We assume that each
of the constants cO in O is associated with a string
label ~wO. This allows us to introduce five domain-
independent features that measure the similarity of
~wu and ~wO.
The feature φnp(cu, cO) signals the replacement
of an entity-typed constant cu with entity cO that has
label ~wu. For the second example in Figure 1 this
feature indicates the replacement of the underspeci-
fied constant associated with the word ‘mozart’ with
the Freebase entity mozart. Stem and synonymy
features φstem(cu, cO) and φsyn(cu, cO) signal the
existence of words wu E ~wu and wu E ~wO that
share a stem or synonym respectively. Stems are
computed with the Porter stemmer and synonyms
are extracted from Wiktionary. A single Freebase
specific feature φfp;stem(cu, cO) indicates a word
stem match between wu E ~wu and the word filling
the most specific position in ~wu under Freebase’s hi-
erarchical naming schema.
A final feature φgl(cu, cO) calculates the overlap
between Wiktionary definitions for ~wu and ~wO. Let
gl(w) be the Wiktionary definition for w. Then:
indicates entities separated from a predicate by one
join in y, such as mozart and dedicated to in Fig-
ure 1, that exist in the same relationship in K.
If two predicates that share a variable in y
do not share an argument in that position in K
then the execution of y against K will fail. The
predicate-predicate φpp(y, K) feature indicates pairs
of predicates that share a variable in y but can-
not occur in this relationship in K. For ex-
ample, since the subject of the Freebase prop-
erty date of birth does not take arguments of
type location, φpp(y, K) will fire if y con-
tains the logical form λxλy.date of birth(x, y)n
location(x).
Both the predicate-argument and predicate-
predicate features operate on subexpressions of y.
We also define the execution features: φemp(y, K) to
signal an empty answer for y in K; φ0(y, K) to sig-
nal a zero-valued answer created by counting over
an empty set; and φ1(y, K) to signal a one-valued
answer created by counting over a singleton set.
As with the lexical cues, we use knowledge base
features as soft constraints since it is possible for
natural language queries to refer to concepts that do
not exist in K.
φgl(cu, cO) = r 2·|gl(wO)∩gl(wc) |8 Experimental Setup
wu∈ wu;wO∈ wO
 |wO|· |wu|·|gl(wO)|+|gl(wc)|
Domain-indepedent lexical features allow the
model to reason about the meaning of unseen words.
In small domains, however, the majority of word us-
ages may be covered by training data. We make use
of this fact in the GeoQuery domain with features
φm(cu, cO) that indicate the pairing of ~wu with cO.
Knowledge Base Features Guided by the obser-
vation that we generally want to create queries y
which have answers in knowledge base K, we de-
fine features to signal whether each operation could
build a logical form y with an answer in K.
If a predicate-argument relation in y does not
exist in K, then the execution of y against K
will not return an answer. Two features indicate
whether predicate-argument relations in y exist in K.
φdirect(y, K) indicates predicate-argument applica-
tions in y that exists in K. For example, if the appli-
cation of dedicated by to mozart in Figure 1 ex-
ists in Freebase, φdirect(y, K) will fire. φjoin(y, K)
Data We evaluate performance on the benchmark
GeoQuery dataset (Zelle and Mooney, 1996), and a
newly introduced Freebase Query (FQ) dataset (Cai
and Yates, 2013a). FQ contains 917 questions la-
beled with logical form meaning representations for
querying Freebase. We gathered question answer la-
bels by executing the logical forms against Freebase,
and manually correcting any inconsistencies.
Freebase (Bollacker et al., 2008) is a large, col-
laboratively authored database containing almost 40
million entities and two billion facts, covering more
than 100 domains. We filter Freebase to cover the
domains contained in the FQ dataset resulting in a
database containing 18 million entities, 2072 rela-
tions, 635 types, 135 million facts and 81 domains,
including for example film, sports, and business. We
use this schema to define our target domain, allow-
ing for a wider variety of queries than could be en-
coded with the 635 collapsed relations previously
used to label the FQ data.
</bodyText>
<page confidence="0.990678">
1552
</page>
<bodyText confidence="0.999929195121951">
We report two different experiments on the FQ
data: test results on the existing 642/275 train/test
split and domain adaptation results where the data is
split three ways, partitioning the topics so that the
logical meaning expressions do not share any sym-
bols across folds. We report on the standard 600/280
training/test split for GeoQuery.
Parameter Initialization and Training We ini-
tialize weights for 0np and 0direct to 10, and weights
for 0stem and 0join to 5. This promotes the use of
entities and relations named in sentences. We ini-
tialize weights for 0pp and 0emp to -1 to favour log-
ical forms that have an interpretation in the knowl-
edge base K. All other feature weights are initial-
ized to 0. We run the training algorithm for one it-
eration on the Freebase data, at which point perfor-
mance on the development set had converged. This
fast convergence is due to the very small number of
matching parameters used (5 lexical features and 8
K features). For GeoQuery, we include the larger
domain specific feature set introduced in Section 7.2
and train for 10 iterations. We set the pruning pa-
rameters from Section 6.1 as follows: k = 5 for
Freebase, k = 30 for GeoQuery, N = 50,,r = 10.
Comparison Systems We compare performance
to state-of-the-art systems in both domains. On
GeoQuery, we report results from DCS (Liang
et al., 2011) without special initialization (DCS) and
with an small hand-engineered lexicon (DCS with
L+). We also include results for the FUBL algo-
rithm (Kwiatkowski et al., 2011), the CCG learning
approach that is most closely related to our work. On
FQ, we compare to Cai and Yates (2013a) (CY13).
Evaluation We evaluate by comparing the pro-
duced question answers to the labeled ones, with no
partial credit. Because the parser can fail to pro-
duce a complete query, we report recall, the percent
of total questions answered correctly, and precision,
the percentage of produced queries with correct an-
swers. CY13 and FUBL report fully correct logical
forms, which is a close proxy to our numbers.
</bodyText>
<sectionHeader confidence="0.999064" genericHeader="evaluation">
9 Results
</sectionHeader>
<bodyText confidence="0.862323666666667">
Quantitative Analysis For FQ, we report results
on the test set and in the cross-domain setting, as de-
fined in Section 8. Figure 6 shows both results. Our
</bodyText>
<table confidence="0.9656378">
Setting System R P F1
Test Our Approach 68.0 76.7 72.1
CY13 59 67 63
Cross Our Approach 67.9 73.5 71.5
Domain CY13 60 69 65
Figure 6: Results on the FQ dataset.
R P F1
All Features 68.6 72.0 70.3
Without Wiktionary 67.2 70.7 68.9
Without K Features 61.8 62.5 62.1
</table>
<figureCaption confidence="0.998532">
Figure 7: Ablation Results
</figureCaption>
<bodyText confidence="0.999932769230769">
approach outperforms the previous state of the art,
achieving a nine point improvement in test recall,
while not requiring labeled logical forms in train-
ing. We also see consistent improvements on both
scenarios, indicating that our approach is generaliz-
ing well across topic domains. The learned ontology
matching model is able to reason about previously
unseen ontological subdomains as well as if it was
provided explicit, in-domain training data.
We also performed feature ablations with 5-fold
cross validation on the training set, as seen in Fig-
ure 7. Both the Wiktionary features and knowledge
base features were helpful. Without the Wiktionary
features, the model must rely on word stem matches
which, in combination with graph constraints, can
still recover many of the correct queries. However,
without the knowledge base constraints, the model
produces many queries that return empty answers,
and significantly impacts overall performance.
For GeoQuery, we report test results in Figure 8.
Our approach outperforms the most closely related
CCG model (FUBL) and DCS without initialization,
but falls short of DCS with a small hand-built initial
lexicon. Given the small size of the test set, it is fair
to say that all algorithms are performing at state-of-
the-art levels. This result demonstrates that our ap-
</bodyText>
<table confidence="0.8632836">
Recall
FUBL 88.6
DCS 87.9
DCS with L+ 91.1
Our Approach 89.0
</table>
<figureCaption confidence="0.826201">
Figure 8: GeoQuery Results
</figureCaption>
<page confidence="0.721214">
1553
</page>
<table confidence="0.994966">
Parse Failures (20%)
1. Query in what year did motorola have the most revenue
2 Query on how many projects was james walker a design engineer
Structural Matching Failure (30%)
Query how many children does jerry seinfeld have
3. Labeled Ax.eq(x, count(Ay.people.person.children(jerry seinfeld, y)))
Predicted
Ax.eq(x, count(Ay.people.person.children(y, jerry seinfeld)))
Incomplete Database (10%)
Query how many countries participated in the 2006 winter olympics
Labeled Ay.olympics.olympic games.number of countries(2006 winter olympics, y)
Predicted Ay.eq(y, count(Ay.olympic participation country.olympics participated in(x, 2006 winter olympics)))
Query what programming languages were used for aol instant messenger
Labeled Ay.computer.software.languages used(aol instant messenger, y)
Predicted Ay.computer.software.languages used(aol instant messenger, y) ∧ computer.programming language(y)
Lexical Ambiguity (35%)
Query when was the frida kahlo exhibit at the philadelphia art museum
Labeled Ay.∃x.exhibition run.exhibition(x, frida kahlo)∧
Predicted exhibition venue.exhibitions at(philadelphia art museum, x) ∧ exhibition run.opened on(x,y)
Ay.∃x.exhibition run.exhibition(x, frida kahlo)∧
exhibition venue.exhibitions at(philadelphia art museum, x) ∧ exhibition run.closed on(x,y)
</table>
<figureCaption confidence="0.997094">
Figure 9: Example error cases, with associated frequencies, illustrating system output and gold standard
references. 5% of the cases were miscellaneous or otherwise difficult to categorize.
</figureCaption>
<bodyText confidence="0.999250346153846">
proach can handle the high degree of lexical ambi-
guity in the FQ data, without sacrificing the ability
to understanding the rich, compositional phenomena
that are common in the GeoQuery data.
Qualitative Analysis We also did a qualitative
analysis of errors in the FQ domain. The model
learns to correctly produce complex forms that join
multiple relations. However, there are a number of
systematic error cases, grouped into four categories
as seen in Figure 9.
The first and second examples show parse fail-
ures, where the underspecified CCG grammar did
not have sufficient coverage. The third shows a
failed structural match, where all of the correct logi-
cal constants are selected, but the argument order is
reversed for one of the literals. The fourth and fifth
examples demonstrate a failures due to database in-
completeness. In both cases, the predicted queries
would have returned the same answers as the gold-
truth ones if Freebase contained all of the required
facts. Developing models that are robust to database
incompleteness is a challenging problem for future
work. Finally, the last example demonstrates a lex-
ical ambiguity, where the system was unable to de-
termine if the query should include the opening date
or the closing date for the exhibit.
</bodyText>
<sectionHeader confidence="0.99665" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999996176470588">
We considered the problem of learning domain-
independent semantic parsers, with application to
QA against large knowledge bases. We introduced
a new approach for learning a two-stage semantic
parser that enables scalable, on-the-fly ontological
matching. Experiments demonstrated state-of-the-
art performance on benchmark datasets, including
effective generalization to previously unseen words.
We would like to investigate more nuanced no-
tions of semantic correctness, for example to support
many of the essentially equivalent meaning repre-
sentations we found in the error analysis. Although
we focused exclusively on QA applications, the gen-
eral two-stage analysis approach should allow for
the reuse of learned grammars across a number of
different domains, including robotics or dialog ap-
plications, where data is more challenging to gather.
</bodyText>
<sectionHeader confidence="0.976514" genericHeader="acknowledgments">
11 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999828">
This research was supported in part by DARPA un-
der the DEFT program through the AFRL (FA8750-
13-2-0019) and the CSSG (N11AP20020), the ARO
(W911NF-12-1-0197), the NSF (IIS-1115966), and
by a gift from Google. The authors thank Anthony
Fader, Nicholas FitzGerald, Adrienne Wang, Daniel
Weld, and the anonymous reviewers for their helpful
comments and feedback.
</bodyText>
<page confidence="0.995078">
1554
</page>
<sectionHeader confidence="0.989811" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98994361627907">
Alshawi, H. (1992). The core language engine. The
MIT Press.
Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping
semantic parsers from conversations. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing.
Artzi, Y. and Zettlemoyer, L. (2013). Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Associ-
ation for Computational Linguistics, 1(1):49–62.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and
Taylor, J. (2008). Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In Proceedings of the ACM SIGMOD Inter-
national Conference on Management of Data.
Bos, J. (2008). Wide-coverage semantic analysis
with boxer. In Proceedings of the Conference on
Semantics in Text Processing.
Cai, Q. and Yates, A. (2013a). Large-scale semantic
parsing via schema matching and lexicon exten-
sion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Cai, Q. and Yates, A. (2013b). Semantic parsing
freebase: Towards open-domain semantic pars-
ing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Chen, D. and Mooney, R. (2011). Learning to inter-
pret natural language navigation instructions from
observations. In Proceedings of the National Con-
ference on Artificial Intelligence.
Clark, S. and Curran, J. (2007). Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493–
552.
Clarke, J., Goldwasser, D., Chang, M., and Roth,
D. (2010). Driving semantic parsing from the
world’s response. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing.
Davidson, D. (1967). The logical form of action sen-
tences. Essays on actions and events, pages 105–
148.
Doan, A., Madhavan, J., Domingos, P., and Halevy,
A. (2004). Ontology matching: A machine
learning approach. In Handbook on ontologies.
Springer.
Euzenat, J., Euzenat, J., Shvaiko, P., et al. (2007).
Ontology matching. Springer.
Fader, A., Zettlemoyer, L., and Etzioni, O. (2013).
Paraphrase-driven learning for open question an-
swering. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Goldwasser, D. and Roth, D. (2011). Learning from
natural instructions. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence.
Grosz, B. J., Appelt, D. E., Martin, P. A., and
Pereira, F. (1987). TEAM: An experiment in
the design of transportable natural language inter-
faces. Artificial Intelligence, 32(2):173–243.
Hobbs, J. R. (1985). Ontological promiscuity. In
Proceedings of the Annual Meeting on Associa-
tion for Computational Linguistics.
Jones, B. K., Johnson, M., and Goldwater, S. (2012).
Semantic parsing with bayesian tree transducers.
In Proceedings of the 50th Annual Meeting of the
Association of Computational Linguistics.
Kate, R. and Mooney, R. (2006). Using string-
kernels for learning semantic parsers. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics.
Krishnamurthy, J. and Kollar, T. (2013). Jointly
learning to parse and perceive: Connecting nat-
ural language to the physical world. Transactions
of the Association for Computational Linguistics,
1(2).
Krishnamurthy, J. and Mitchell, T. (2012). Weakly
supervised training of semantic parsers. In Pro-
ceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Kushman, N. and Barzilay, R. (2013). Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.843942">
1555
</page>
<reference confidence="0.996290835616438">
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L.,
and Steedman, M. (2012). A probabilistic model
of syntactic and semantic acquisition from child-
directed utterances and their meanings. Proceed-
ings of the Conference of the European Chapter
of the Association of Computational Linguistics.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2010). Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S.,
and Steedman, M. (2011). Lexical generalization
in CCG grammar induction for semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Liang, P., Jordan, M., and Klein, D. (2011). Learn-
ing dependency-based compositional semantics.
In Proceedings of the Conference of the Associ-
ation for Computational Linguistics.
Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo,
L., and Fox, D. (2012). A joint model of language
and perception for grounded attribute learning. In
Proceedings of the International Conference on
Machine Learning.
Muresan, S. (2011). Learning for deep language un-
derstanding. In Proceedings of the International
Joint Conference on Artificial Intelligence.
Steedman, M. (1996). Surface Structure and Inter-
pretation. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The
MIT Press.
Unger, C., B¨uhmann, L., Lehmann, J.,
Ngonga Ngomo, A., Gerber, D., and Cimiano, P.
(2012). Template-based question answering over
RDF data. In Proceedings of the International
Conference on World Wide Web.
Wong, Y. and Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Proceedings of the Confer-
ence of the Association for Computational Lin-
guistics.
Yahya, M., Berberich, K., Elbassuoni, S., Ramanath,
M., Tresp, V., and Weikum, G. (2012). Natural
language questions for the web of data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Zelle, J. and Mooney, R. (1996). Learning to parse
database queries using inductive logic program-
ming. In Proceedings of the National Conference
on Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2005). Learning
to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars.
In Proceedings of the Conference on Uncertainty
in Artificial Intelligence.
Zettlemoyer, L. and Collins, M. (2007). Online
learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
Zettlemoyer, L. and Collins, M. (2009). Learn-
ing context-dependent mappings from sentences
to logical form. In Proceedings of the Joint Con-
ference of the Association for Computational Lin-
guistics and International Joint Conference on
Natural Language Processing.
Zhang, C., Hoffmann, R., and Weld, D. S. (2012).
Ontological smoothing for relation extraction
with minimal supervision. In Proceeds of the
Conference on Artificial Intelligence.
</reference>
<page confidence="0.993996">
1556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939943">
<title confidence="0.999975">Scaling Semantic Parsers with On-the-fly Ontology Matching</title>
<author confidence="0.999925">Tom Kwiatkowski Eunsol Choi Yoav Artzi Luke</author>
<affiliation confidence="0.999816">Computer Science &amp; University of</affiliation>
<address confidence="0.99634">Seattle, WA</address>
<abstract confidence="0.99771896">We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>The core language engine.</title>
<date>1992</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="10954" citStr="Alshawi, 1992" startWordPosition="1682" endWordPosition="1683">t (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et a</context>
</contexts>
<marker>Alshawi, 1992</marker>
<rawString>Alshawi, H. (1992). The core language engine. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9293" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="1425" endWordPosition="1428"> unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale t</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Artzi, Y. and Zettlemoyer, L. (2011). Bootstrapping semantic parsers from conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1751" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="251" endWordPosition="255">rate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) MR2: count(Ax.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, p</context>
<context position="9504" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="1456" endWordPosition="1459">Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a numb</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Artzi, Y. and Zettlemoyer, L. (2013). Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM SIGMOD International Conference on Management of Data.</booktitle>
<contexts>
<context position="6653" citStr="Bollacker et al., 2008" startWordPosition="987" endWordPosition="991">se QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in Freebase (Bollacker et al., 2008) and O would define the set of entities and relation types used to encode those facts. Also, let y be a logical expression that can be executed against K to return an answer a = EXEC(y, K). Figure 1 shows example queries and answers for Freebase. Our goal is to build a function y = PARSE(x, O) for mapping a natural language sentence x to a domain-dependent logical form y. Parsing We use a two-stage approach to define the space of possible parses GEN(x, O) (Section 5). First, we use a CCG and word-class information from Wiktionary1 to build domain-independent underspecified logical forms, which</context>
<context position="31771" citStr="Bollacker et al., 2008" startWordPosition="5138" endWordPosition="5141">y, K) indicates predicate-argument applications in y that exists in K. For example, if the application of dedicated by to mozart in Figure 1 exists in Freebase, φdirect(y, K) will fire. φjoin(y, K) Data We evaluate performance on the benchmark GeoQuery dataset (Zelle and Mooney, 1996), and a newly introduced Freebase Query (FQ) dataset (Cai and Yates, 2013a). FQ contains 917 questions labeled with logical form meaning representations for querying Freebase. We gathered question answer labels by executing the logical forms against Freebase, and manually correcting any inconsistencies. Freebase (Bollacker et al., 2008) is a large, collaboratively authored database containing almost 40 million entities and two billion facts, covering more than 100 domains. We filter Freebase to cover the domains contained in the FQ dataset resulting in a database containing 18 million entities, 2072 relations, 635 types, 135 million facts and 81 domains, including for example film, sports, and business. We use this schema to define our target domain, allowing for a wider variety of queries than could be encoded with the 635 collapsed relations previously used to label the FQ data. 1552 We report two different experiments on </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Bollacker, K., Evans, C., Paritosh, P., Sturge, T., and Taylor, J. (2008). Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Semantics in Text Processing.</booktitle>
<contexts>
<context position="10966" citStr="Bos, 2008" startWordPosition="1684" endWordPosition="1685">and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), w</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Bos, J. (2008). Wide-coverage semantic analysis with boxer. In Proceedings of the Conference on Semantics in Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6061" citStr="Cai and Yates, 2013" startWordPosition="896" endWordPosition="899">ms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in Freebase (Bollacker et al., 2008) and O w</context>
<context position="8461" citStr="Cai and Yates (2013" startWordPosition="1304" endWordPosition="1307">n GEN(x, O). Unlike much previous work (e.g., Zettlemoyer and Collins (2005)), we do not induce a CCG lexicon. The lexicon is open domain, using no symbols from the ontology O for K. This allows us to write a single set of lexical templates that are reused in every domain (Section 5.1). The burden of learning word meaning is shifted to the second, ontology matching, stage of parsing (Section 5.2), and modeled with a number of new features (Section 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-s</context>
<context position="9683" citStr="Cai and Yates (2013" startWordPosition="1485" endWordPosition="1488">ific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations</context>
<context position="31506" citStr="Cai and Yates, 2013" startWordPosition="5101" endWordPosition="5104">ration could build a logical form y with an answer in K. If a predicate-argument relation in y does not exist in K, then the execution of y against K will not return an answer. Two features indicate whether predicate-argument relations in y exist in K. φdirect(y, K) indicates predicate-argument applications in y that exists in K. For example, if the application of dedicated by to mozart in Figure 1 exists in Freebase, φdirect(y, K) will fire. φjoin(y, K) Data We evaluate performance on the benchmark GeoQuery dataset (Zelle and Mooney, 1996), and a newly introduced Freebase Query (FQ) dataset (Cai and Yates, 2013a). FQ contains 917 questions labeled with logical form meaning representations for querying Freebase. We gathered question answer labels by executing the logical forms against Freebase, and manually correcting any inconsistencies. Freebase (Bollacker et al., 2008) is a large, collaboratively authored database containing almost 40 million entities and two billion facts, covering more than 100 domains. We filter Freebase to cover the domains contained in the FQ dataset resulting in a database containing 18 million entities, 2072 relations, 635 types, 135 million facts and 81 domains, including </context>
<context position="33954" citStr="Cai and Yates (2013" startWordPosition="5513" endWordPosition="5516">n specific feature set introduced in Section 7.2 and train for 10 iterations. We set the pruning parameters from Section 6.1 as follows: k = 5 for Freebase, k = 30 for GeoQuery, N = 50,,r = 10. Comparison Systems We compare performance to state-of-the-art systems in both domains. On GeoQuery, we report results from DCS (Liang et al., 2011) without special initialization (DCS) and with an small hand-engineered lexicon (DCS with L+). We also include results for the FUBL algorithm (Kwiatkowski et al., 2011), the CCG learning approach that is most closely related to our work. On FQ, we compare to Cai and Yates (2013a) (CY13). Evaluation We evaluate by comparing the produced question answers to the labeled ones, with no partial credit. Because the parser can fail to produce a complete query, we report recall, the percent of total questions answered correctly, and precision, the percentage of produced queries with correct answers. CY13 and FUBL report fully correct logical forms, which is a close proxy to our numbers. 9 Results Quantitative Analysis For FQ, we report results on the test set and in the cross-domain setting, as defined in Section 8. Figure 6 shows both results. Our Setting System R P F1 Test</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Cai, Q. and Yates, A. (2013a). Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Semantic parsing freebase: Towards open-domain semantic parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="6061" citStr="Cai and Yates, 2013" startWordPosition="896" endWordPosition="899">ms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in Freebase (Bollacker et al., 2008) and O w</context>
<context position="8461" citStr="Cai and Yates (2013" startWordPosition="1304" endWordPosition="1307">n GEN(x, O). Unlike much previous work (e.g., Zettlemoyer and Collins (2005)), we do not induce a CCG lexicon. The lexicon is open domain, using no symbols from the ontology O for K. This allows us to write a single set of lexical templates that are reused in every domain (Section 5.1). The burden of learning word meaning is shifted to the second, ontology matching, stage of parsing (Section 5.2), and modeled with a number of new features (Section 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-s</context>
<context position="9683" citStr="Cai and Yates (2013" startWordPosition="1485" endWordPosition="1488">ific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations</context>
<context position="31506" citStr="Cai and Yates, 2013" startWordPosition="5101" endWordPosition="5104">ration could build a logical form y with an answer in K. If a predicate-argument relation in y does not exist in K, then the execution of y against K will not return an answer. Two features indicate whether predicate-argument relations in y exist in K. φdirect(y, K) indicates predicate-argument applications in y that exists in K. For example, if the application of dedicated by to mozart in Figure 1 exists in Freebase, φdirect(y, K) will fire. φjoin(y, K) Data We evaluate performance on the benchmark GeoQuery dataset (Zelle and Mooney, 1996), and a newly introduced Freebase Query (FQ) dataset (Cai and Yates, 2013a). FQ contains 917 questions labeled with logical form meaning representations for querying Freebase. We gathered question answer labels by executing the logical forms against Freebase, and manually correcting any inconsistencies. Freebase (Bollacker et al., 2008) is a large, collaboratively authored database containing almost 40 million entities and two billion facts, covering more than 100 domains. We filter Freebase to cover the domains contained in the FQ dataset resulting in a database containing 18 million entities, 2072 relations, 635 types, 135 million facts and 81 domains, including </context>
<context position="33954" citStr="Cai and Yates (2013" startWordPosition="5513" endWordPosition="5516">n specific feature set introduced in Section 7.2 and train for 10 iterations. We set the pruning parameters from Section 6.1 as follows: k = 5 for Freebase, k = 30 for GeoQuery, N = 50,,r = 10. Comparison Systems We compare performance to state-of-the-art systems in both domains. On GeoQuery, we report results from DCS (Liang et al., 2011) without special initialization (DCS) and with an small hand-engineered lexicon (DCS with L+). We also include results for the FUBL algorithm (Kwiatkowski et al., 2011), the CCG learning approach that is most closely related to our work. On FQ, we compare to Cai and Yates (2013a) (CY13). Evaluation We evaluate by comparing the produced question answers to the labeled ones, with no partial credit. Because the parser can fail to produce a complete query, we report recall, the percent of total questions answered correctly, and precision, the percentage of produced queries with correct answers. CY13 and FUBL report fully correct logical forms, which is a close proxy to our numbers. 9 Results Quantitative Analysis For FQ, we report results on the test set and in the cross-domain setting, as defined in Section 8. Figure 6 shows both results. Our Setting System R P F1 Test</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Cai, Q. and Yates, A. (2013b). Semantic parsing freebase: Towards open-domain semantic parsing. In Proceedings of the Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>R Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1721" citStr="Chen and Mooney, 2011" startWordPosition="247" endWordPosition="250">gy. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) MR2: count(Ax.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pair</context>
<context position="9474" citStr="Chen and Mooney, 2011" startWordPosition="1452" endWordPosition="1455">ention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>Chen, D. and Mooney, R. (2011). Learning to interpret natural language navigation instructions from observations. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>552</pages>
<contexts>
<context position="4826" citStr="Clark and Curran, 2007" startWordPosition="709" endWordPosition="712">target domain. In our example, producing either MR1, MR2 or another more appropriate option, depending on the QA database schema. This two stage approach enables parsing without any domain-dependent lexicon that pairs words with logical constants. Instead, word meaning is filled in on-the-fly through ontology matching, enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains. Figure 1 shows the desired outputs for two example Freebase sentences. The first parsing stage uses a probabilistic combinatory categorial grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) to map sentences to new, underspecified logical-form meaning representations containing generic logical constants that are not tied to any specific ontology. This approach enables us to share grammar structure across domains, instead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the f</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S. and Curran, J. (2007). Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493– 552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5652" citStr="Clarke et al., 2010" startWordPosition="831" endWordPosition="834">ure across domains, instead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the final logical forms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored </context>
<context position="9216" citStr="Clarke et al., 2010" startWordPosition="1414" endWordPosition="1417">tion ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algori</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>Clarke, J., Goldwasser, D., Chang, M., and Roth, D. (2010). Driving semantic parsing from the world’s response. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<title>The logical form of action sentences. Essays on actions and events,</title>
<date>1967</date>
<pages>105--148</pages>
<contexts>
<context position="12370" citStr="Davidson, 1967" startWordPosition="1917" endWordPosition="1918">d logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables, lambda abstractions, and literals. In this paper, we use the term literal to refer to the application of a constant to a sequence of 1547 library of new york N N\N NP NP Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC N\N Af.Ax.f(x) n loc(x, NYC) N Ax.library(x) n loc(x, NYC) Figure 2: A sample CCG parse. arguments. We include types for entities e, truth values t, numbers i, events ev, and higher-order functions, such as (e, t) and ((e, t), e). We use Davidsonian event semantics (Davidson, 1967) to explicitly represent events using event-typed variables and conjunctive modifiers to capture thematic roles. Combinatory Categorial Grammars (CCG) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with CCG categories. For example, the lexical entry library �- N : Ax.library(x) in Figure 2 pairs the word ‘library’ with the CCG category that has syntactic category N and meaning Ax.library(x). A CCG parse starts fro</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Davidson, D. (1967). The logical form of action sentences. Essays on actions and events, pages 105– 148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Doan</author>
<author>J Madhavan</author>
<author>P Domingos</author>
<author>A Halevy</author>
</authors>
<title>Ontology matching: A machine learning approach.</title>
<date>2004</date>
<booktitle>In Handbook on ontologies.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="11540" citStr="Doan et al., 2004" startWordPosition="1771" endWordPosition="1774">osz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables, lambda abstractions, and literals. In this paper, we use the term literal to refer to the application of a constant to a sequence of 1547 library of new york N N\N NP NP Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC N\N Af.Ax.f(x) n loc(x, NYC) N Ax.library(x) n loc(x</context>
</contexts>
<marker>Doan, Madhavan, Domingos, Halevy, 2004</marker>
<rawString>Doan, A., Madhavan, J., Domingos, P., and Halevy, A. (2004). Ontology matching: A machine learning approach. In Handbook on ontologies. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Euzenat</author>
<author>J Euzenat</author>
<author>P Shvaiko</author>
</authors>
<title>Ontology matching.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="11563" citStr="Euzenat et al., 2007" startWordPosition="1775" endWordPosition="1778">lshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables, lambda abstractions, and literals. In this paper, we use the term literal to refer to the application of a constant to a sequence of 1547 library of new york N N\N NP NP Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC N\N Af.Ax.f(x) n loc(x, NYC) N Ax.library(x) n loc(x, NYC) Figure 2: A samp</context>
</contexts>
<marker>Euzenat, Euzenat, Shvaiko, 2007</marker>
<rawString>Euzenat, J., Euzenat, J., Shvaiko, P., et al. (2007). Ontology matching. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11274" citStr="Fader et al. (2013)" startWordPosition="1729" endWordPosition="1732">tes our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Fader, A., Zettlemoyer, L., and Etzioni, O. (2013). Paraphrase-driven learning for open question answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Learning from natural instructions.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9451" citStr="Goldwasser and Roth, 2011" startWordPosition="1448" endWordPosition="1451">ve received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the ar</context>
</contexts>
<marker>Goldwasser, Roth, 2011</marker>
<rawString>Goldwasser, D. and Roth, D. (2011). Learning from natural instructions. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>D E Appelt</author>
<author>P A Martin</author>
<author>F Pereira</author>
</authors>
<title>TEAM: An experiment in the design of transportable natural language interfaces.</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="10939" citStr="Grosz et al., 1987" startWordPosition="1678" endWordPosition="1681">rser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 200</context>
</contexts>
<marker>Grosz, Appelt, Martin, Pereira, 1987</marker>
<rawString>Grosz, B. J., Appelt, D. E., Martin, P. A., and Pereira, F. (1987). TEAM: An experiment in the design of transportable natural language interfaces. Artificial Intelligence, 32(2):173–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Ontological promiscuity.</title>
<date>1985</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10768" citStr="Hobbs (1985)" startWordPosition="1656" endWordPosition="1657">Gs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontologica</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, J. R. (1985). Ontological promiscuity. In Proceedings of the Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B K Jones</author>
<author>M Johnson</author>
<author>S Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="8985" citStr="Jones et al., 2012" startWordPosition="1385" endWordPosition="1388">tionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning wit</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Jones, B. K., Johnson, M., and Goldwater, S. (2012). Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kate</author>
<author>R Mooney</author>
</authors>
<title>Using stringkernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8888" citStr="Kate and Mooney, 2006" startWordPosition="1369" endWordPosition="1372">r of new features (Section 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Ze</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, R. and Mooney, R. (2006). Using stringkernels for learning semantic parsers. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="1671" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="239" endWordPosition="243"> that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) MR2: count(Ax.person(x) ∧ live(x, Seattle)) A semantic parser might aim </context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Krishnamurthy, J. and Kollar, T. (2013). Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1600" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="229" endWordPosition="232">icalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) M</context>
<context position="9353" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="1432" endWordPosition="1436">of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not j</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Krishnamurthy, J. and Mitchell, T. (2012). Weakly supervised training of semantic parsers. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushman</author>
<author>R Barzilay</author>
</authors>
<title>Using semantic unification to generate regular expressions from natural language.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1805" citStr="Kushman and Barzilay, 2013" startWordPosition="259" endWordPosition="262">ntic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) MR2: count(Ax.person(x) ∧ live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases (‘How many,’ ‘people,</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>Kushman, N. and Barzilay, R. (2013). Using semantic unification to generate regular expressions from natural language. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>S Goldwater</author>
<author>L Zettlemoyer</author>
<author>M Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings.</title>
<date>2012</date>
<booktitle>Proceedings of the Conference of the European Chapter of the Association of Computational Linguistics.</booktitle>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., and Steedman, M. (2012). A probabilistic model of syntactic and semantic acquisition from childdirected utterances and their meanings. Proceedings of the Conference of the European Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8952" citStr="Kwiatkowski et al., 2010" startWordPosition="1379" endWordPosition="1382">valuation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstratin</context>
<context position="10254" citStr="Kwiatkowski et al., 2010" startWordPosition="1579" endWordPosition="1582">to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Man</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., and Steedman, M. (2010). Inducing probabilistic CCG grammars from logical form with higherorder unification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1523" citStr="Kwiatkowski et al., 2011" startWordPosition="219" endWordPosition="222">airs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of S</context>
<context position="33844" citStr="Kwiatkowski et al., 2011" startWordPosition="5492" endWordPosition="5495"> number of matching parameters used (5 lexical features and 8 K features). For GeoQuery, we include the larger domain specific feature set introduced in Section 7.2 and train for 10 iterations. We set the pruning parameters from Section 6.1 as follows: k = 5 for Freebase, k = 30 for GeoQuery, N = 50,,r = 10. Comparison Systems We compare performance to state-of-the-art systems in both domains. On GeoQuery, we report results from DCS (Liang et al., 2011) without special initialization (DCS) and with an small hand-engineered lexicon (DCS with L+). We also include results for the FUBL algorithm (Kwiatkowski et al., 2011), the CCG learning approach that is most closely related to our work. On FQ, we compare to Cai and Yates (2013a) (CY13). Evaluation We evaluate by comparing the produced question answers to the labeled ones, with no partial credit. Because the parser can fail to produce a complete query, we report recall, the percent of total questions answered correctly, and precision, the percentage of produced queries with correct answers. CY13 and FUBL report fully correct logical forms, which is a close proxy to our numbers. 9 Results Quantitative Analysis For FQ, we report results on the test set and in </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., and Steedman, M. (2011). Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1544" citStr="Liang et al., 2011" startWordPosition="223" endWordPosition="226"> CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many </context>
<context position="5673" citStr="Liang et al., 2011" startWordPosition="835" endWordPosition="838">nstead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the final logical forms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans m</context>
<context position="9237" citStr="Liang et al., 2011" startWordPosition="1418" endWordPosition="1421"> test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to these efforts by demonstrating a new approach for learning with latent meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et a</context>
<context position="33676" citStr="Liang et al., 2011" startWordPosition="5466" endWordPosition="5469"> algorithm for one iteration on the Freebase data, at which point performance on the development set had converged. This fast convergence is due to the very small number of matching parameters used (5 lexical features and 8 K features). For GeoQuery, we include the larger domain specific feature set introduced in Section 7.2 and train for 10 iterations. We set the pruning parameters from Section 6.1 as follows: k = 5 for Freebase, k = 30 for GeoQuery, N = 50,,r = 10. Comparison Systems We compare performance to state-of-the-art systems in both domains. On GeoQuery, we report results from DCS (Liang et al., 2011) without special initialization (DCS) and with an small hand-engineered lexicon (DCS with L+). We also include results for the FUBL algorithm (Kwiatkowski et al., 2011), the CCG learning approach that is most closely related to our work. On FQ, we compare to Cai and Yates (2013a) (CY13). Evaluation We evaluate by comparing the produced question answers to the labeled ones, with no partial credit. Because the parser can fail to produce a complete query, we report recall, the percent of total questions answered correctly, and precision, the percentage of produced queries with correct answers. CY</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Liang, P., Jordan, M., and Klein, D. (2011). Learning dependency-based compositional semantics. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>N FitzGerald</author>
<author>L Zettlemoyer</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1638" citStr="Matuszek et al., 2012" startWordPosition="235" endWordPosition="238">ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 1 Introduction Semantic parsers map sentences to formal representations of their underlying meaning. Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (Kwiatkowski et al., 2011; Liang et al., 2011), relation extraction (Krishnamurthy and Mitchell, 2012), robot control (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), interpreting instructions (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), and generating programs (Kushman and Barzilay, 2013). In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations. In practice, the choice of ontology significantly impacts learning. For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle? Q2: How many people live in Seattle? MR1: Ax.population(Seattle, x) MR2: count(Ax.person(x) ∧ live(x, Seatt</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Matuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., and Fox, D. (2012). A joint model of language and perception for grounded attribute learning. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muresan</author>
</authors>
<title>Learning for deep language understanding.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8926" citStr="Muresan, 2011" startWordPosition="1377" endWordPosition="1378"> joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our work adds to the</context>
</contexts>
<marker>Muresan, 2011</marker>
<rawString>Muresan, S. (2011). Learning for deep language understanding. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12630" citStr="Steedman, 1996" startWordPosition="1952" endWordPosition="1953">brary of new york N N\N NP NP Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC N\N Af.Ax.f(x) n loc(x, NYC) N Ax.library(x) n loc(x, NYC) Figure 2: A sample CCG parse. arguments. We include types for entities e, truth values t, numbers i, events ev, and higher-order functions, such as (e, t) and ((e, t), e). We use Davidsonian event semantics (Davidson, 1967) to explicitly represent events using event-typed variables and conjunctive modifiers to capture thematic roles. Combinatory Categorial Grammars (CCG) CCGs are a linguistically-motivated formalism for modeling a wide range of language phenomena (Steedman, 1996, 2000). A CCG is defined by a lexicon and a set of combinators. The lexicon contains entries that pair words or phrases with CCG categories. For example, the lexical entry library �- N : Ax.library(x) in Figure 2 pairs the word ‘library’ with the CCG category that has syntactic category N and meaning Ax.library(x). A CCG parse starts from assigning lexical entries to words and phrases. These are then combined using the set of CCG combinators to build a logical form that captures the meaning of the entire sentence. We use the application, composition, and coordination combinators. Figure 2 sho</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Steedman, M. (1996). Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4801" citStr="Steedman, 2000" startWordPosition="707" endWordPosition="708"> tation for the target domain. In our example, producing either MR1, MR2 or another more appropriate option, depending on the QA database schema. This two stage approach enables parsing without any domain-dependent lexicon that pairs words with logical constants. Instead, word meaning is filled in on-the-fly through ontology matching, enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains. Figure 1 shows the desired outputs for two example Freebase sentences. The first parsing stage uses a probabilistic combinatory categorial grammar (CCG) (Steedman, 2000; Clark and Curran, 2007) to map sentences to new, underspecified logical-form meaning representations containing generic logical constants that are not tied to any specific ontology. This approach enables us to share grammar structure across domains, instead of repeatedly re-learning different grammars for each target ontology. The ontology-matching step considers a large number of type-equivalent domain-specific meanings. It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent cons</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Unger</author>
<author>L B¨uhmann</author>
<author>J Lehmann</author>
<author>Ngonga Ngomo</author>
<author>A Gerber</author>
<author>D</author>
<author>P Cimiano</author>
</authors>
<title>Template-based question answering over RDF data.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on World Wide Web.</booktitle>
<marker>Unger, B¨uhmann, Lehmann, Ngomo, Gerber, D, Cimiano, 2012</marker>
<rawString>Unger, C., B¨uhmann, L., Lehmann, J., Ngonga Ngomo, A., Gerber, D., and Cimiano, P. (2012). Template-based question answering over RDF data. In Proceedings of the International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8911" citStr="Wong and Mooney, 2007" startWordPosition="1373" endWordPosition="1376">ion 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Freebase data is divided by topics such as sports, film, and business. This condition ensures that the test data has a large percentage of previously unseen words, allowing us to measure the effectiveness of the real time ontology matching. 3 Related Work Supervised approaches for learning semantic parsers have received significant attention, e.g. (Kate and Mooney, 2006; Wong and Mooney, 2007; Muresan, 2011; Kwiatkowski et al., 2010, 2011, 2012; Jones et al., 2012). However, these techniques require training data with hand-labeled domain-specific logical expressions. Recently, alternative forms of supervision were introduced, including learning from question-answer pairs (Clarke et al., 2010; Liang et al., 2011), from conversational logs (Artzi and Zettlemoyer, 2011), with distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013b), and from sentences paired with system behavior (Goldwasser and Roth, 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Our w</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y. and Mooney, R. (2007). Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yahya</author>
<author>K Berberich</author>
<author>S Elbassuoni</author>
<author>M Ramanath</author>
<author>V Tresp</author>
<author>G Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11172" citStr="Yahya et al., 2012" startWordPosition="1713" endWordPosition="1716"> all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are designed to build general meaning representations that are adapted for different domains. Recent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms t</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Yahya, M., Berberich, K., Elbassuoni, S., Ramanath, M., Tresp, V., and Weikum, G. (2012). Natural language questions for the web of data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zelle</author>
<author>R Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6019" citStr="Zelle and Mooney, 1996" startWordPosition="888" endWordPosition="891"> constants, to construct the final logical forms. During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching. Following a number of recent approaches (Clarke et al., 2010; Liang et al., 2011), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs. This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers. We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ) (Cai and Yates, 2013a). GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure. FQ includes questions to Freebase, a large community-authored database that spans many sub-domains. Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test. 2 Formal Overview Task Let an ontology O be a set of logical constants and a knowledge base K be a collection of logical statements constructed with constants from O. For example, K could be facts in</context>
<context position="31433" citStr="Zelle and Mooney, 1996" startWordPosition="5089" endWordPosition="5092">ve answers in knowledge base K, we define features to signal whether each operation could build a logical form y with an answer in K. If a predicate-argument relation in y does not exist in K, then the execution of y against K will not return an answer. Two features indicate whether predicate-argument relations in y exist in K. φdirect(y, K) indicates predicate-argument applications in y that exists in K. For example, if the application of dedicated by to mozart in Figure 1 exists in Freebase, φdirect(y, K) will fire. φjoin(y, K) Data We evaluate performance on the benchmark GeoQuery dataset (Zelle and Mooney, 1996), and a newly introduced Freebase Query (FQ) dataset (Cai and Yates, 2013a). FQ contains 917 questions labeled with logical form meaning representations for querying Freebase. We gathered question answer labels by executing the logical forms against Freebase, and manually correcting any inconsistencies. Freebase (Bollacker et al., 2008) is a large, collaboratively authored database containing almost 40 million entities and two billion facts, covering more than 100 domains. We filter Freebase to cover the domains contained in the FQ dataset resulting in a database containing 18 million entities</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>Zelle, J. and Mooney, R. (1996). Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="7918" citStr="Zettlemoyer and Collins (2005)" startWordPosition="1207" endWordPosition="1210">cture of the sentence but do not use constants from O. For example, in Figure 1, l0 denotes the underspecified logical forms paired with each sentence x. The parser then maps this intermediate representation to a logical form that uses constants from O, such as the y seen in Figure 1. 1www.wiktionary.com 1546 Learning We assume access to data containing question-answer pairs {(xZ, az) : i = 1... n} and a corresponding knowledge base K. The learning algorithm (Section 7.1) estimates the parameters of a linear model for ranking the possible entires in GEN(x, O). Unlike much previous work (e.g., Zettlemoyer and Collins (2005)), we do not induce a CCG lexicon. The lexicon is open domain, using no symbols from the ontology O for K. This allows us to write a single set of lexical templates that are reused in every domain (Section 5.1). The burden of learning word meaning is shifted to the second, ontology matching, stage of parsing (Section 5.2), and modeled with a number of new features (Section 7.2) as part of the joint model. Evaluation We evaluate on held out questionanswer pairs in two benchmark domains, Freebase and GeoQuery. Following Cai and Yates (2013a), we also report a cross-domain evaluation where the Fr</context>
<context position="10222" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1574" endWordPosition="1577"> meaning representations that scales to large databases like Freebase. Cai and Yates (2013a) present the most closely related work. They applied schema matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in gen</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, L. and Collins, M. (2005). Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, L. and Collins, M. (2007). Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="10374" citStr="Zettlemoyer and Collins, 2009" startWordPosition="1598" endWordPosition="1601"> matching techniques to expand a CCG lexicon learned with the UBL algorithm (Kwiatkowski et al., 2010). This approach was one of the first to scale to Freebase, but required labeled logical forms and did not jointly model semantic parsing and ontological reasoning. This method serves as the state of the art for our comparison in Section 9. We build on a number of existing algorithmic ideas, including using CCGs to build meaning representations (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2011), building derivations to transform the output of the CCG parser based on context (Zettlemoyer and Collins, 2009), and using weakly supervised margin-sensitive parameter updates (Artzi and Zettlemoyer, 2011, 2013). However, we introduce the idea of learning an open-domain CCG semantic parser; all previous methods suffered, to various degrees, from the ontological mismatch problem that motivates our work. The challenge of ontological mismatch has been previously recognized in many settings. Hobbs (1985) describes the need for ontological promiscuity in general language understanding. Many previous hand-engineered natural language understanding systems (Grosz et al., 1987; Alshawi, 1992; Bos, 2008) are des</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Zettlemoyer, L. and Collins, M. (2009). Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhang</author>
<author>R Hoffmann</author>
<author>D S Weld</author>
</authors>
<title>Ontological smoothing for relation extraction with minimal supervision.</title>
<date>2012</date>
<booktitle>In Proceeds of the Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="11682" citStr="Zhang et al., 2012" startWordPosition="1795" endWordPosition="1798">cent efforts to build natural language interfaces to large databases, for example DBpedia (Yahya et al., 2012; Unger et al., 2012), have also used handengineered ontology matching techniques. Fader et al. (2013) recently presented a scalable approach to learning an open domain QA system, where ontological mismatches are resolved with learned paraphrases. Finally, the databases research community has a long history of developing schema matching techniques (Doan et al., 2004; Euzenat et al., 2007), which has inspired more recent work on distant supervision for relation extraction with Freebase (Zhang et al., 2012). 4 Background Semantic Modeling We use the typed lambda calculus to build logical forms that represent the meanings of words, phrases and sentences. Logical forms contain constants, variables, lambda abstractions, and literals. In this paper, we use the term literal to refer to the application of a constant to a sequence of 1547 library of new york N N\N NP NP Ax.library(x) AyAfAx.f(x) n loc(x, y) NYC N\N Af.Ax.f(x) n loc(x, NYC) N Ax.library(x) n loc(x, NYC) Figure 2: A sample CCG parse. arguments. We include types for entities e, truth values t, numbers i, events ev, and higher-order functi</context>
</contexts>
<marker>Zhang, Hoffmann, Weld, 2012</marker>
<rawString>Zhang, C., Hoffmann, R., and Weld, D. S. (2012). Ontological smoothing for relation extraction with minimal supervision. In Proceeds of the Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
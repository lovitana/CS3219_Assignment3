<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000225">
<title confidence="0.987569">
Open-Domain Fine-Grained Class Extraction from Web Search Queries
</title>
<author confidence="0.955083">
Marius Pas¸ca
</author>
<affiliation confidence="0.91305">
Google Inc.
</affiliation>
<address confidence="0.747734">
1600 Amphitheatre Parkway
Mountain View, California 94043
</address>
<email confidence="0.999284">
mars@google.com
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998962444444444">
This paper introduces a method for extract-
ing fine-grained class labels (“countries with
double taxation agreements with india”) from
Web search queries. The class labels are more
numerous and more diverse than those pro-
duced by current extraction methods. Also
extracted are representative sets of instances
(singapore, united kingdom) for the class la-
bels.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999304875000001">
Motivation: As more semantic constraints are
added, concepts like companies become more spe-
cific, e.g., companies that are in the software busi-
ness, and have been started in a garage. The
sets of instances associated with the classes become
smaller; the class labels used to concisely describe
the meaning of more specific concepts tend to be-
come longer. In fact, fine-grained class labels such
as “software companies started in a garage” are of-
ten complex noun phrases, since they must somehow
summarize multiple semantic constraints. Although
Web users are interested in both coarse (e.g., “com-
panies”) and fine-grained (e.g., “software compa-
nies started in a garage”) class labels, virtually all
class labels acquired from text by previous extrac-
tion methods (Etzioni et al., 2005; Van Durme and
Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et
al., 2006) exhibit little syntactic diversity. Indeed,
instances and class labels that are relatively com-
plex nouns are known to be difficult to detect and
pick out precisely from surrounding text (Downey
et al., 2007). This and other challenges associated
with large-scale extraction from Web text (Etzioni
et al., 2011) cause the extracted class labels to usu-
ally follow a rigid modifiers-plus-nouns format. The
format covers nouns (“companies”) possibly pre-
ceded by one or many modifiers (“software com-
panies”, “computer security software companies”).
Examples of actual extractions include “european
cities” (Etzioni et al., 2005), “strong acids” (Pan-
tel and Pennacchiotti, 2006), “prestigious private
schools” (Van Durme and Pas¸ca, 2008), “aquatic
birds” (Kozareva and Hovy, 2010).
As an alternative to extracting class labels from
text, some methods simply import them from
human-curated resources, for example from the set
of categories encoded in Wikipedia (Remy, 2002).
As a result, class labels potentially exhibit higher
syntactic diversity. The modifiers-plus-nouns for-
mat (“computer security software companies”) is
usually still the norm. But other formats are possi-
ble: “software companies based in london”, “soft-
ware companies of the united kingdom”. Vocab-
ulary coverage gaps remain a problem, with many
relevant class labels (“software companies of texas”
“software companies started in a garage”, “soft-
ware companies that give sap training”) still miss-
ing. There is a need for methods that more ag-
gressively identify fine-grained class labels, beyond
those extracted by previous methods or encoded in
existing, manually-created resources. Such class la-
bels increase coverage, for example in scenarios that
enrich Web search results with instances available
for the class labels specified in the queries.
Contributions: The contributions of this paper are
twofold. First, it proposes a weakly-supervised
</bodyText>
<page confidence="0.990028">
403
</page>
<note confidence="0.7335575">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 403–414,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999537538461539">
method to assemble a large vocabulary of class la-
bels from queries. The class labels include fine-
grained class labels (“countries with double taxa-
tion agreements with india”, “no front license plate
states”) that are difficult to extract from text by
previous methods for open-domain information ex-
traction. Second, the method acquires representa-
tive instances (singapore, united kingdom; arizona,
new mexico) that belong to fine-grained class labels
(“countries with double taxation agreements with
india”, “no front license plate states”). Both class
labels and their instances are extracted from Web
search queries.
</bodyText>
<sectionHeader confidence="0.980173" genericHeader="method">
2 Extraction from Queries
</sectionHeader>
<subsectionHeader confidence="0.990465">
2.1 Extraction of Class Labels
</subsectionHeader>
<bodyText confidence="0.99989336923077">
Overview: Given a set of arbitrary Web search
queries as input, our method produces a vocabulary
of fine-grained class labels. For this purpose, it: a)
selects an initial vocabulary of class labels, as a sub-
set of input queries that are likely to correspond to
search requests for classes; b) expands the vocabu-
lary, by generating a large, noisy set of other pos-
sible class labels, through replacements of ngrams
within initial class labels with their similar phrases;
c) restricts the generated class labels to those that
match the syntactic structure of class labels within
the initial vocabulary; and d) further restricts the
generated class labels to those that appear within the
larger set of arbitrary Web search queries.
Initial Vocabulary of Class Labels: Out of a set
of arbitrary search queries available as input, the
queries in the format “list of ..” are selected as the
initial vocabulary of class labels. The prefix “list
of” is discarded from each query. Thus, the query
“list ofsoftware companies that use linux” gives the
class label “software companies that use linux”.
Generation via Phrase Similarities: As a prerequi-
site to generating class labels, distributionally simi-
lar phrases (Lin and Pantel, 2002; Lin and Wu, 2009;
Pantel et al., 2009) and their scores are collected in
advance. A phrase is represented as a vector of its
contextual features. A feature is a word, collected
from windows of three words centered around the
occurrences of the phrase in sentences across Web
documents (Lin and Wu, 2009). In the contextual
vector of a phrase, the weight of a feature is the
pointwise-mutual information (Lin and Wu, 2009)
between the phrase P and the feature F. The dis-
tributional similarity score between two phrases is
the cosine similarity between the contextual vectors
of the two phrases. The lists of most distribution-
ally similar phrases of a phrase P are thus compiled
offline, by ranking the similar phrases of P in de-
creasing order of their similarity score relative to P.
Each class label from the initial vocabulary is ex-
panded into a set of generated, candidate class la-
bels. To this effect, every ngram P within a given
class label is replaced with each of the distribution-
ally similar phrases, if any, available for the ngram.
As shown later in the experimental section, the ex-
pansion can increase the vocabulary by a factor of
100.
Approximate Syntactic Filtering: The set of gen-
erated class labels is noisy. The set is filtered, by
retaining only class labels whose syntactic structure
matches the syntactic structure of some class label(s)
from the initial vocabulary. The syntactic structure
is loosely approximated at surface rather than syn-
tactic level. A generated class label is retained, if
its sequence of part of speech tags matches the se-
quence of part of speech tags of one of the class la-
bels from the initial vocabulary. As an additional
constraint, the sequence must contain one tag cor-
responding to a common noun in plural form, i.e.,
NNS. Otherwise, the class label is discarded.
Query Filtering: Generated class labels that pass
previous filters are further restricted. They are inter-
sected with the set of arbitrary Web search queries
available as input. Generated class labels that are
not full queries are discarded.
</bodyText>
<subsectionHeader confidence="0.999969">
2.2 Extraction of Instances
</subsectionHeader>
<bodyText confidence="0.9998133">
Overview: Our method mines instances of fine-
grained class labels from queries. In a nutshell, it
identifies queries containing two types of informa-
tion simultaneously. First, the queries contain an in-
stance (marvin gaye) of the more general class labels
(“musicians”) from which the fine-grained class la-
bels (“musicians who have been shot”) can be ob-
tained. Second, the queries contain the constraints
added by the fine-grained class labels (“... shot”) on
top of the more general class labels.
</bodyText>
<subsectionHeader confidence="0.568728">
Instances of General Class Labels: Follow-
</subsectionHeader>
<bodyText confidence="0.9356065">
ing (Ponzetto and Strube, 2007), the Wikipedia cate-
gory network is refined into a hierarchy that discards
</bodyText>
<page confidence="0.998459">
404
</page>
<bodyText confidence="0.9996653">
non-IsA (thematic) edges, and retains only IsA (sub-
sumption) edges from the network (Ponzetto and
Strube, 2007). Instances, i.e., titles of Wikipedia
articles, are propagated upwards to all their ances-
tor categories. The class label “musicians” would
be mapped into madonna, marvin gaye, jon bon jovi
etc. The mappings from each ancestor category, to
all its descendant instances in the Wikipedia hierar-
chy, represent our mappings from more general class
labels to instances.
</bodyText>
<subsectionHeader confidence="0.684443">
Decomposition of Fine-Grained Class Labels: A
</subsectionHeader>
<bodyText confidence="0.999862793103448">
fine-grained class label (e.g., “musicians who have
been shot”) is effectively decomposed into pairs of
two pieces of information. The first piece is a more
general class label (“musicians”), if any occurs in
it. The second piece is a bag of words, collected
from the remainder of the fine-grained class label
after discarding stop words. Note that the standard
set of stop words is augmented with auxiliary verbs
(e.g., does, has, is, would), determiners, conjunc-
tions, prepositions, and question wh-words (Radev
et al., 2005) (e.g., where, how). In the first piece
of each pair, the general class label is then replaced
with each of its instances. This produces multiple
pairs of a candidate instance and a bag of words, for
each fine-grained class label. As an illustration, the
class labels “musicians who have been shot” and
“automobiles with remote start” are decomposed
into pairs like &lt;madonna, {shot}&gt;, &lt;marvin gaye,
{shot}&gt;; and &lt;buick lacrosse, {remote, start}&gt;,
&lt;nissan versa, {remote, start}&gt;, respectively.
Matching of Candidate Instances: A decomposed
class label is retained, if there are matching queries
that contain the candidate instance, the bag of words,
and optionally stop words. Otherwise, the decom-
posed class label is discarded. The word matching is
performed after word stemming (Porter, 1980). The
aggregated frequency of the matching queries is as-
signed as the score of the candidate instance for the
fine-grained class label:
</bodyText>
<equation confidence="0.794881">
(Freq(Q)JMatch(Q, &lt; I, C &gt;)) (1)
</equation>
<bodyText confidence="0.999851666666667">
For example, the score of the candidate instance
marvin gaye for the class label “musicians who have
been shot”, is the sum of the frequencies of the
matching queries “marvin gaye is shot”, “when was
marvin gaye shot”, “why marvin gaye was shot”
etc. Similarly, the score of buick lacrosse for “au-
tomobiles with remote start” is given by the aggre-
gated frequencies of the queries “buick lacrosse re-
mote start”, “how to remote start buick lacrosse”,
“remote start for buick lacrosse”. Candidate in-
stances of a class label are ranked in decreasing or-
der of their scores.
</bodyText>
<sectionHeader confidence="0.993476" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<bodyText confidence="0.98817825">
Web Textual Data: The experiments rely on a sam-
ple of 1 billion queries in English submitted by users
of a Web search engine. Each query is accompa-
nied by its frequency of occurrence. Also available
is a sample of around 200 million Web documents
in English.
Phrase Similarities: Web documents are used in
the experiments only to construct a phrase similar-
ity repository following (Lin and Wu, 2009; Pantel
et al., 2009). The repository contains ranked lists
of the top 1000 phrases, computed to be the most
distributionally similar to each of around 16 million
phrases.
Text Pre-Processing: The TnT tagger (Brants,
2000) assigns part of speech tags to words in class
labels.
Instances: To collect mappings from Wikipedia cat-
egories (as more general class labels) to titles of de-
scendant Wikipedia articles (as instances), a snap-
shot of Wikipedia articles was intersected with the
Wikipedia category hierarchy from (Ponzetto and
Strube, 2007). The mappings connect a total of
1,535,083 instances to a total of 108,756 class la-
bels.
</bodyText>
<sectionHeader confidence="0.839479" genericHeader="method">
4 Evaluation of Class Labels
</sectionHeader>
<subsectionHeader confidence="0.988292">
4.1 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.999670916666667">
Experimental Runs: Human-compiled information
available within Wikipedia serves as the source of
data for two baseline runs. The set of all categories,
listed in Wikipedia for any of its articles, corre-
sponds to the set of class labels “acquired” in run
R, Categories used for internal Wikipedia book-
keeping (Ponzetto and Strube, 2007) are discarded.
Their names contain one of the words article(s), cat-
egory(ies), indices, pages, redirects, stubs, or tem-
plates. Similarly, the titles of Wikipedia articles with
the prefix “List of..” (e.g., “List of automobile man-
ufacturers of Germany”) form the set of class labels
</bodyText>
<equation confidence="0.956673333333333">
�
Score(I, C) �
Q
</equation>
<page confidence="0.982097">
405
</page>
<bodyText confidence="0.99929885">
“acquired” in run R,,,l. The prefix “List of” is dis-
carded.
For completeness, a third baseline run, Rd,, cor-
responds to class labels extracted from Web docu-
ments. The class labels are noun phrases C that fill
extraction patterns equivalent to “C such as I”. The
patterns are matched to document sentences. The
boundaries of the class labels C are approximated
from part of speech tags of sentence words (Van
Durme and Pas¸ca, 2008). The patterns were pro-
posed in (Hearst, 1992). They were employed
widely in subsequent methods (Etzioni et al., 2005;
Kozareva et al., 2008; Wu et al., 2012), which ex-
tract class labels precisely from the set of class la-
bels C produced by the extraction patterns. Even
methods using queries as a textual data source still
extract class labels from documents using the same
extraction patterns (Pas¸ca, 2010). Therefore, from
the point of view of evaluating class labels, run Rd,
is a valid representative of previous extraction meth-
ods, including (Etzioni et al., 2005; Kozareva et al.,
2008; Van Durme and Pas¸ca, 2008; Pas¸ca, 2010; Wu
et al., 2012).
Besides the baseline runs, three experimental runs
are considered. In run Ryl, the queries starting with
the prefix “list of” form the set of class labels. The
prefix “list of” is discarded from each query. In run
Ryy, the class labels are generated via phrase sim-
ilarities, starting from Ryl as an initial set of class
labels. Run Rya represents an ablation experiment.
It is created from Ryy, by limiting the expansion of
a given class label via distributional similarities to
only one, rather than multiple, phrases within the
class label. Note that, by design, none of the class
labels that appear in Ryl also appear in runs Rya or
Ryy. Therefore, the intersection between Ryl, on one
hand, and Rya and Ryy, on the other hand, is the
empty set.
All data, including the class labels extracted in all
experimental runs, is converted to lower case.
</bodyText>
<subsectionHeader confidence="0.971711">
4.2 Relative Coverage of Class Labels
</subsectionHeader>
<bodyText confidence="0.998878833333333">
Coverage Over Entire Sets: Table 1 illustrates the
overall coverage of the various experimental runs.
The table takes all class labels into account, relative
to the Wikipedia-based runs as reference sets: R,,,,
(Wikipedia categories), in the upper part of the table;
and R,,,l (Wikipedia List-Of categories), in the lower
</bodyText>
<table confidence="0.982127625">
vs. Wikipedia categories:
Rwc Rdc 295,587 2,884,390 15,011 0.051
Rql 295,587 1,649,261 21,979 0.074
Rqa 295,587 33,073,741 33,502 0.113
Rqg 295,587 134,235,151 43,935 0.148
RqlURqg 295,587 135,884,412 65,914 0.222
vs. Wikipedia categories that are queries:
RwcnQ Rdc 126,318 2,884,390 14,840 0.117
Rql 126,318 1,649,261 21,979 0.173
Rqa 126,318 33,073,741 33,502 0.265
Rqg 126,318 134,235,151 43,935 0.347
RqlURqg 126,318 135,884,412 65,914 0.521
vs. Wikipedia List-Of categories:
Rwl Rdc 134,840 2,884,390 8,099 0.060
Rql 134,840 1,649,261 26,446 0.196
Rqa 134,840 33,073,741 16,204 0.120
Rqg 134,840 134,235,151 20,021 0.148
RqlURqg 134,840 135,884,412 46,467 0.344
vs. Wikipedia List-Of categories that are queries:
RwlnQ Rdc 47,442 2,884,390 7,985 0.168
Rql 47,442 1,649,261 24,821 0.523
Rqa 47,442 33,073,741 16,204 0.341
Rqg 47,442 134,235,151 20,021 0.422
RqlURqg 47,442 135,884,412 44,842 0.945
</table>
<tableCaption confidence="0.999134">
Table 1: Coverage of class labels extracted by various
</tableCaption>
<bodyText confidence="0.999082318181818">
experimental runs, relative to class labels available in
Wikipedia before and after intersecting them with a large
set of arbitrary queries (A = reference set, relative to
which coverage is computed; B = measured set, for which
coverage is computed relative to the reference set; JAJ =
size of set A; Q = set of input queries)
part of the table. Note that the number of class labels
extracted by the individual run shown in the second
column (B) is shown in the fourth column (JBJ). In
particular, there are around 1.6 million unique “list
of ..” queries, from which class labels are collected
in run Ryl.
During the computation of coverage, the refer-
ence set, and the set for which coverage is being
computed, are intersected. Intersection relies on
strict string matching. All words, including punc-
tuation, must match exactly in order for a class la-
bel to be part of the intersection. The reference
sets are intersected with the set of all Web search
queries Q used in the experiments. Coverage is com-
puted both before and after intersection. Less than
half (126,318 of 295,587) of the class labels, for
</bodyText>
<figure confidence="0.997845555555556">
Counts
Cvg
A
B
JAJ
JBJ
JAnBJ
|AnB|
|A|
</figure>
<page confidence="0.997336">
406
</page>
<bodyText confidence="0.996190862068966">
the reference set R,,,,; and about a third (47,442 of
134,840) for R,,,l; appear in the set Q of all queries.
Three conclusions can be drawn from the re-
sults. First, query-based runs vastly outperform
Wikipedia-based runs in terms of absolute coverage.
Run Rql contains around 5 and 12 times more class
labels, than R,,,, and R,,,l respectively. On top of
that, generating class labels via phrase similarities
further increases the class label count by about 20
times for Rqa, and 80 times for Rqy. Second, query-
based runs Rqa and Rqy surpass the document-based
run Rd,. Third, higher class label counts translate
into higher relative coverage. In the upper part of
the table, run R,,,l contains 3.9% (relative to R,,,,)
and 7.1% (relative to R,,,,f1Q) of the reference set.
But the relative coverage doubles for Rql at 7.4%
(relative to R,,,,) and 17.3% (relative to R,,,,f1Q).
Coverage again doubles for Rqy at 14.8% (relative
to R,,,,) and 34.7% (relative to R,,,,f1Q). The union
of query-based initial and generated class labels is
RqlURqy. The union contains about a quarter (i.e.,
22.2%) or half (52.1%) of the reference set R,,,,, de-
pending on whether the reference set is intersected
with the set of all queries or not. In the lower part of
the table, more than 90% of the queries in the refer-
ence set R,,,l that are also queries are found among
the class labels collectively extracted in the query-
based runs. Note that, since Rql is disjoint from Rqa
and Rqy, none of the class labels already in Rql can
be “re-discovered” (generated) again in Rqa or Rqy.
Therefore, by experimental design, relative coverage
scores of Rql may be relatively difficult to surpass by
Rqa or Rqy taken individually.
Diversity: Class labels restricted to those that have
the format “.. that/which/who ..” are relatively more
specific, e.g., “grocery stores that double coupons in
omaha”, “airlines which fly from santa barbara”,
“writers who were doctors”. The most frequent
head phrases of such restricted class labels offer an
idea about how diverse the class labels are. The
counts of class labels for the most frequent head
phrases are in the order of 10’s in the case of R,,,l vs.
10,000’s for Rqy. In comparison, none of the class
labels of run Rd, have this format. The lack of such
class labels in run Rd,, and their smaller proportion
in run R,,,l vs. Rqy, suggest that class labels extracted
by the proposed method exhibit higher lexical and
syntactic diversity than previous methods do.
Tag (Value): Examples of Class Labels
correct (1.0): angioplasty specialists in kolkata, good
things pancho villa did, eating disorders inpatient units
in the uk nhs specialist services
questionable (0.5): picture framers adelaide cbd, side
effects bicalutamide, different eating disorders, private
hospitals treat kidney stones uk
incorrect (0.0): al hirschfield theatre hours, value of
berkshire hathaway shares, remove spaces in cobol,
dogs with loss of appetite, 1999 majorca open
</bodyText>
<tableCaption confidence="0.976435">
Table 2: Correctness tags manually assigned to class la-
bels containing one of the (underlined) target phrases, ex-
tracted by various runs
</tableCaption>
<subsectionHeader confidence="0.999398">
4.3 Precision of Class Labels
</subsectionHeader>
<bodyText confidence="0.999307151515152">
Evaluation Metric: Class labels being evaluated are
manually assigned a correctness tag. A class label is
deemed correct, if it is grammatically well-formed
and describes a relevant concept that embodies some
(unspecified) set of instances that share similar prop-
erties; questionable, if it is relevant but not well-
formed; or incorrect. A questionable class label is
not well-formed because it lacks necessary linking
particles (e.g., the prepositions of or for in “side ef-
fects bicalutamide”), or contains undesirable mod-
ifiers (“different eating disorders”). Examples of
correct and incorrect class labels are “angioplasty
specialists in kolkata” and “al hirschfield theatre
hours” respectively.
To compute the precision score, the correctness
tags are converted to numeric values, as shown in
Table 2: correct to 1; questionable to 0.5; and in-
correct to 0. Precision over a list of class labels is
measured as the sum of the correctness values of the
class labels in the list, divided by the size of the list.
Precision Relative to Target Phrases: The preci-
sion of the class labels in each run is determined sim-
ilarly to how relative coverage was computed ear-
lier. More precisely, the precision is computed over
the class labels whose names contain each phrase
from the set of 75 target phrases from (Alfonseca
et al., 2010). For each phrase, and for each run,
a random sample of at most 50 of the class labels
that match the phrase is selected for evaluation. The
samples taken for each run, corresponding to the
same phrase, are combined into a merged list. This
produces one merged list for each phrase, for a total
of 75 merged lists. The precision score over a target
</bodyText>
<page confidence="0.991447">
407
</page>
<bodyText confidence="0.999941020833333">
phrase is the precision score over its sample of class
labels.
The last two columns of Table 3 capture the pre-
cision scores for the class labels. The scores are
computed in two ways: averaged over the (variable)
subsets of target phrases for which some matching
class label(s) exist, in the last but one column, e.g.,
over 19 of the 75 target phrases for R,,,,; and aver-
aged over the entire set of 75 target phrases, in the
last column. The former does not penalize a run
for not being able to extract any class labels con-
taining a particular target phrase, whereas the latter
does penalize. Naturally, precision scores over the
entire set of target phrases decrease when coverage
is lower, for runs R,,,,, R,,,l and, to a lesser extent,
Rd, and RQl. But even after ignoring target phrases
with no matching class labels, precision scores in
the last but one column in Table 3 reveal important
properties of the experimental runs. First, between
the two Wikipedia-based runs, R,,,l has perfect class
labels, whereas as many as 1 in 4 class labels of
run R,,,, are marked as incorrect during the evalu-
ation. Second, the class labels collected from “list
of..” queries in run RQl correspond to relevant, well-
formed concepts in 80% of the cases. Third, the gen-
eration of class labels via phrase similarities (RQy)
greatly increases coverage as shown earlier. The in-
crease comes at the expense of lowering precision
from 80% to 72%. However, the phrases from ini-
tial queries that are expanded via distributional sim-
ilarities can be limited from multiple to only one, by
switching from RQy to RQ,,. This gives higher preci-
sion for RQ,, than for RQy.
As a complement to Table 3, the graphs in Fig-
ure 1 offer a more detailed view into the precision
of class labels. The figure covers a Wikipedia-based
run (R,,,,) and two query-based runs (RQl, RQy). The
graphs show the precision scores, over each of the
75 target phrases. Among target phrases for which
some matching class labels exist in the respective
run, the target phrases with the lowest precision
scores are robotics (score of 0.15) and karlsruhe
(0.33), for R,,,,; carotid arteries and kidney stones,
both with a score of 0.00 because their matching
class labels are all incorrect, for Rd,; african pop-
ulation and chester arthur, both with a score of 0.00
because their matching class labels are all incorrect,
for RQl; and arlene martel (0.00) and right to vote
</bodyText>
<table confidence="0.999214272727273">
Precision
Precision
Run Target Phrases Precision of Class Labels
Over Target Phrases
All Matched Cvg Over Matched Over All
R,,,, 75 19 0.253 0.756 0.191
R,,,l 75 15 0.200 1.000 0.200
Rd, 75 35 0.467 0.834 0.389
RQl 75 48 0.640 0.800 0.512
RQa 75 70 0.933 0.868 0.810
RQy 75 73 0.973 0.724 0.705
</table>
<tableCaption confidence="0.9636526">
Table 3: Precision of class labels that match (i.e., whose
names contain) each target phrase, computed as an av-
erage over (variable) subsets of target phrases for which
some matching class label(s) exist, and as an average over
the entire set of 75 target phrases
</tableCaption>
<figure confidence="0.99457">
Per-Phrase Precision for Run Rwc Per-Phrase Precision for Run Rql
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.999371333333333">
Figure 1: Precision scores for runs R,,,,, RQl, Rd, and
RQy, over class labels that match (i.e., contain) each of
the 75 target phrases
</figureCaption>
<bodyText confidence="0.996644866666667">
(0.25), for RQy.
Precision over Samples of Class Labels: The pre-
cision is separately computed over a random sample
of 400 class labels per experimental run. The sam-
ples are selected from the set of all class labels ex-
tracted by the respective run. The precision scores
are: 0.759 for R,,,,; 1.000 for R,,,l; 0.806 for Rd,;
0.811 for RQl; 0.856 for RQ,,; and 0.711 for RQy. The
scores are in line with scores computed earlier over
the target phrases, in the fourth column of Table 3.
Discussion: As noted in (Ponzetto and Strube,
2007), Wikipedia organizes its articles and cate-
gories into a category network that mixes IsA (sub-
sumption) edges with non-IsA (thematic) edges.
Whenever an edge in Wikipedia is not IsA, the par-
</bodyText>
<figure confidence="0.901536792207792">
aaa 1
adelaide cbd 5
american fascism 10
antarctic region 15
baquba 20
boulder colorado 25
chester arthur 30
contemporary art 35
eating disorders 40
halogens 45
juan carlos 50
lucky ali 55
phosphorus 60
rouen 65
u.s. 70
wlan 75
Phrase
aaa 1
adelaide cbd 5
american fascism 10
antarctic region 15
baquba 20
boulder colorado 25
chester arthur 30
contemporary art 35
eating disorders 40
halogens 45
juan carlos 50
lucky ali 55
phosphorus 60
rouen 65
u.s. 70
wlan 75
Phrase
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
er-Phrase Precision for Run Rdc
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Per-Phrase Precision for Run Rqg
aaa 1 aaa 1
adelaide cbd 5 adelaide cbd 5
american fascism 10 american fascism 10
antarctic region 15 antarctic region 15
baquba 20 baquba 20
boulder colorado 25 boulder colorado 25
chester arthur 30 chester arthur 30
contemporary art 35 contemporary art 35
eating disorders 40 eating disorders 40
halogens 45 halogens 45
juan carlos 50 juan carlos 50
lucky ali 55 lucky ali 55
phosphorus 60 phosphorus 60
rouen 65 rouen 65
u.s. 70 u.s. 70
wlan 75 wlan 75
Phrase Phrase
</figure>
<page confidence="0.973085">
408
</page>
<subsectionHeader confidence="0.726282">
Longest Class Labels
</subsectionHeader>
<bodyText confidence="0.824853857142857">
Rwl: [japanese army and navy members in military or
politic services in proper japan korea manchuria occu-
pied china and nearest areas in previous times and pa-
cific war epoch(1930-40s), mental disorders as defined
by the diagnostic and statistical manual of mental dis-
orders and the international statistical classification of
diseases and related health problems,..]
</bodyText>
<tableCaption confidence="0.688168777777778">
R99: [differences between transformational lead-
ership and transactional leadership, things to do in
llanfairpwllgwyngyllgogerychwyrndrobwllllantysilio-
gogogoch, philosophical differences between thomas
jefferson and alexander hamilton, musculoskeletal
manifestations of human immunodeficiency virus
infection,..]
77Table 4: Longest class labels extracted by runs Rwl and
��
</tableCaption>
<page confidence="0.532207">
��99
</page>
<bodyText confidence="0.999300935483871">
ent category may not be a relevant concept that de-
scribes some set of instances that share similar prop-
erties. Such categories are not good class labels,
and therefore are marked as incorrect. Examples in-
clude the class labels “austrian contemporary art”,
“1999 majorca open” and “u.s. route 30”, listed in
Wikipedia as categories of the instances vienna bien-
nale,1999 majorca open and squirrel hill tunnel re-
spectively. This affects the precision scores for Rwc
in Table 3. It also affects the coverage values rela-
tive to Rwc in Table 1. Ideally, high-precision exper-
imental runs would not extract any incorrect class la-
bels that happen to appear in Rwc, for example “aus-
trian contemporary art”. But the coverage relative
to Rwc would artificially penalize such runs, for not
extracting the incorrect class labels from Rwc.
As a proxy for estimating class label complexity,
Table 4 shows the longest class labels derived from
Wikipedia (Rwl) vs. generated from queries (Rqg).
Class labels derived from Web search queries may
be semantically overlapping. Examples are “writers
who killed themselves” vs. “writers who committed
suicide”. The overlap is desirable, since different
Web users may request the same information via dif-
ferent queries. The same phenomenon has been ob-
served in other information extraction tasks. It also
affects manually-created resources like Wikipedia.
The continuous manual refinements to Wikipedia
content still cannot prevent the occurrence of du-
plicate class labels among Wikipedia List-Of cate-
gories. The duplicates are present in run Rwl. Exam-
</bodyText>
<subsectionHeader confidence="0.95308">
Target Class Labels
</subsectionHeader>
<bodyText confidence="0.995399766666667">
007 movie actors, .308 weapons, actors with obsessive
compulsive disorder, antibiotics for multiple sclerosis,
astronauts in space station, automobiles with remote
start, beatles songs of love, beetles that bite, compa-
nies with sustainable competitive advantage, countries
with double taxation agreements with india, criminals
who have been executed, daft punk live albums, dal-
las medical companies, direct democracy states, elec-
tronic companies in electronic city bangalore, expen-
sive brands of shoes, eye diseases in cats, f1 car com-
panies, fwd sports cars, garden landscaping maga-
zines, heliskiing resorts, hell in a cell wrestlers, hol-
idays celebrated in sydney, ibf weight classes, ibiza
2011 djs, immunology scientists, jewelry manufactur-
ing companies, kanye west songs on youtube, kingston
upon thames supermarkets, latin military ranks, lud-
hiana newspapers, maastricht treaty countries, mu-
sicians who have been shot, no front license plate
states, non-profit organizations in nashville tennessee,
organic chocolate companies, plants which are used in
homeopathy, programming languages for server side
programming, qatar chemical companies, qld private
schools, real estate companies in virginia beach vir-
ginia, respiratory infection antibiotics, serial killers
with antisocial personality disorder, singers with curly
hair, telecommunications companies in the philip-
pines, trains from la to san diego, visual basic database
management systems, warmblood colors, washington
university basketball players, world heritage sites in
northern ireland
</bodyText>
<tableCaption confidence="0.9839595">
Table 5: Set of 50 class labels, used in the evaluation of
extracted instances
</tableCaption>
<bodyText confidence="0.9994213">
ples are “formula one drivers that never qualifiedfor
a race” vs. “formula one drivers who never quali-
fied for a race”; or “goaltenders who have scored
a goal in a nhl game” vs. “goaltenders who have
scored a goal in an nhl game”. Some of the lexi-
cal differences among class labels are due to unde-
sirable misspellings. Again, similar problems occa-
sionally affect existing Wikipedia categories: “no-
bel laureates who endorse barack obama” vs. “no-
bel laureates who endorse barrack obama”.
</bodyText>
<sectionHeader confidence="0.881679" genericHeader="evaluation">
5 Evaluation of Instances
</sectionHeader>
<subsectionHeader confidence="0.937246">
5.1 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.998382666666667">
Target Set of Class Labels: The target set for evalu-
ation is shown in Table 5. Initially, a random sample
of 100 class labels is selected from all class labels in
</bodyText>
<page confidence="0.997983">
409
</page>
<table confidence="0.999108833333333">
Tag (Value): Examples of Instances
correct (1.0): countries with double taxation agree-
ments with india: thailand; hell in a cell wrestlers:
brock lesnar; ibiza 2011 djs: dimitri from paris; he-
liskiing resorts: valle nevado
questionable (0.5): 007 movie actors: david niven;
kanye west songs on youtube: the good life; holidays
celebrated in sydney: waitangi day
incorrect (0.0): electronic companies in electronic
city bangalore: bank of baroda; garden landscaping
magazines: marquis; immunology scientists: rosalind
franklin
</table>
<tableCaption confidence="0.965783">
Table 6: Correctness tags manually assigned to instances
extracted from queries for various class labels
</tableCaption>
<bodyText confidence="0.999652962962963">
run RQy. Class labels deemed incorrect, as well as
class labels for which no instances are extracted, are
manually removed from the sample. Out of the re-
maining class labels, a smaller random sample of 50
of the remaining class labels is retained, for the pur-
pose of evaluating the quality of instances extracted
for various class labels.
Evaluation Metric: The evaluation computes the
precision of the ranked list of instances extracted for
each target class label. To remove any undesirable
bias towards higher-ranked instances, the ranked list
is sorted alphabetically, then each instance is as-
signed one of the correctness tags from Table 6.
Instances are deemed questionable, if they would
be correct for a rather obscure interpretation of the
class label. For example, david niven is an actor in
one of the spoofs rather than main releases of the
007 movie. Instances that would be correct if a few
words were dropped or added are also deemed ques-
tionable: the good life is not one of the “kanye west
songs on youtube” but good life is.
To compute the precision score over a ranked list
of instances, the correctness tags are converted to
numeric values. Precision at some rank N in the list
is measured as the sum of the correctness values of
the instances extracted up to rank N, divided by the
number of instances extracted up to rank N.
</bodyText>
<subsectionHeader confidence="0.999937">
5.2 Precision of Instances
</subsectionHeader>
<bodyText confidence="0.9982724">
Precision: Precision scores in Table 7 vary across
target class labels. For some class labels, the ex-
tracted instances are noisy enough that scores are
below 0.50 at ranks 10 and higher. This is the case
for “electronic companies in electronic city banga-
</bodyText>
<table confidence="0.998635741935484">
Target Class Label Precision of Instances
@1 @5 @10 @50
007 movie actors 1.00 1.00 0.85 0.85
actors with obsessive compul- 0.00 0.60 0.70 0.70
sive disorder
antibiotics for multiple sclerosis 0.50 0.60 0.55 0.58
astronauts in space station 1.00 0.70 0.85 0.83
automobiles with remote start 1.00 1.00 0.75 0.75
beatles songs of love 0.00 0.50 0.65 0.52
beetles that bite 1.00 0.80 0.50 0.56
companies with sustainable 1.00 1.00 0.80 0.88
competitive advantage
countries with double taxation 1.00 1.00 1.00 0.90
agreements with india
criminals who have been exe- 1.00 1.00 0.90 0.82
cuted
daft punk live albums 0.50 0.40 0.35 0.35
dallas medical companies 0.00 0.70 0.65 0.54
direct democracy states 1.00 1.00 0.90 0.86
electronic companies in elec- 1.00 0.40 0.40 0.42
tronic city bangalore
expensive brands of shoes 1.00 1.00 0.90 0.92
eye diseases in cats 0.50 0.50 0.35 0.35
f1 car companies 1.00 1.00 0.80 0.30
fwd sports cars 1.00 1.00 1.00 1.00
garden landscaping magazines 0.00 0.10 0.15 0.06
heliskiing resorts 1.00 1.00 1.00 1.00
hell in a cell wrestlers 1.00 1.00 1.00 0.92
holidays celebrated in sydney 1.00 0.70 0.75 0.75
... ... ... ... ...
Average over 50 class labels 0.80 0.80 0.76 0.71
</table>
<tableCaption confidence="0.98982525">
Table 7: Precision at various ranks in the ranked lists of
instances extracted from queries, for various target class
labels and as an average over the entire set of 50 target
class labels
</tableCaption>
<bodyText confidence="0.999721357142857">
lore” and “daft punk live albums”, and especially
for “garden landscaping magazines” which has the
worst precision. On the other hand, instances ex-
tracted for “companies with sustainable competitive
advantage” or “criminals who have been executed”
have high precision across all ranks. As an aver-
age over all target class labels, precision is 0.76 at
rank 10, and 0.71 at rank 50. Although there is room
for improvement, we find these accuracy levels to be
encouragingly good, especially at rank 50. As a re-
minder, instances are extracted from noisy queries,
and for class labels as fine-grained as those acquired
and used in our experiments. Some of the extracted
ranked lists of instances are shown in Table 8.
</bodyText>
<page confidence="0.982175">
410
</page>
<table confidence="0.449088105263158">
Target Class Label Extracted Instances
countries with [singapore, malaysia, mauritius,
double taxation kenya, australia, united king-
agreements with dom, cyprus, turkey, thailand, ger-
india many,..]
direct democracy [california, oregon, nevada, wis-
states consin, louisiana, arizona, ver-
mont, alaska, illinois, michigan,..]
fwd sports cars [scion tc, ford probe, honda pre-
lude, nissan 200sx, lotus elan, mit-
subishi fto, dodge srt-4, mitsubishi
gto, volvo c30, toyota celica,..]
garden landscap- [front, contemporary, gallery,
ing magazines edge, view, chelsea, wallpaper,
expo, wizard, sunset,..]
holidays cele- [halloween, australia day, anzac
brated in sydney day, independence day, waitangi
day, melbourne cup, hogmanay,
rotuma day, solstice, yule,..]
</table>
<tableCaption confidence="0.931834">
Table 8: Ranked lists of instances extracted for a sample
of class labels
</tableCaption>
<bodyText confidence="0.999983755102041">
In additional experiments, the same evaluation
procedure is applied to output from two previous ex-
traction methods. The first method starts by inter-
nally generating a small set of seed instances for a
class label given as input (Wang and Cohen, 2009).
A set expansion module then expands the seed set
into a longer, ranked list of instances. The instances
are extracted from unstructured and semi-structured
text within Web documents. The documents are ac-
cessed via the search interface of a general-purpose
Web search engine (cf. (Wang and Cohen, 2009)
for more details). The second method extracts in-
stances of class labels using the extraction patterns
proposed in (Hearst, 1992). As such, it is similar
to (Kozareva et al., 2008; Van Durme and Pas¸ca,
2008; Wu et al., 2012). The method corresponds
to the run Rd, described earlier, where the rela-
tive ranking of instances and class labels uses the
co-occurrence of instances and class labels within
queries (Pas¸ca, 2010). For the purpose of the eval-
uation, when no instances are available for a target
class label, the class label is generalized into iter-
atively shorter phrases containing fewer modifiers,
until some instances are available for the shorter
phrase. For example, target class labels like actors
with obsessive compulsive disorder, beatles songs of
love, garden landscaping magazines do not have any
instances extracted by the second method. There-
fore, the instances evaluated for the second method
for these target class labels are collected from the
instances of the more general actors, beatles songs,
landscaping magazines. Without the generalization,
the target class label would receive no credit dur-
ing the evaluation, and the two previous methods
would have lower precision scores. Over the 50 tar-
get class labels, the precision of the two methods is
0.11 and 0.27 at rank 5; 0.06 and 0.25 at rank 10;
0.05 and 0.22 at rank 20; and 0.05 and 0.20 at rank
50. The results confirm that, as explained earlier,
previous methods for open-domain information ex-
traction have limited ability to extract instances of
fine-grained class labels.
Discussion: Earlier errors in the acquisition of the
class label affect the usefulness of any instances that
may be subsequently extracted for them. The ex-
periments require candidate instances to appear in
Wikipedia. This may improve precision, at the ex-
pense of not extracting instances that are not yet in
Wikipedia (Lin et al., 2012).
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999956083333333">
Previous methods for extracting classes of instances
from text acquire sets of instances that are each
either unlabeled (Pennacchiotti and Pantel, 2009;
Jain and Pennacchiotti, 2010; Shi et al., 2010),
or associated with a class label (Banko et al.,
2007; Wang and Cohen, 2009). The sets of in-
stances and/or class labels may be organized as
flat sets or hierarchically, relative to inferred hier-
archies (Kozareva and Hovy, 2010) or existing hier-
archies such as WordNet (Snow et al., 2006; Davi-
dov and Rappoport, 2009) or the category network
within Wikipedia (Wu and Weld, 2008; Ponzetto
and Navigli, 2009). Semi-structured text from Web
documents is a complementary resource to unstruc-
tured text, for the purpose of extracting relations in
general (Cafarella et al., 2008), and classes and in-
stances in particular (Talukdar et al., 2008; Dalvi et
al., 2012).
With previous methods, the vocabulary of class
labels potentially produced for any instance is con-
fined to a closed set provided manually as in-
put (Wang and Cohen, 2009; Carlson et al., 2010).
The closed set is often derived from resources like
Wikipedia (Talukdar and Pereira, 2010; Lin et al.,
</bodyText>
<page confidence="0.996064">
411
</page>
<bodyText confidence="0.999892666666667">
2012; Hoffart et al., 2013) or Freebase (Pantel et
al., 2012). Alternatively, the vocabulary is not a
closed set, but instead is acquired along with the
instances (Pantel and Pennacchiotti, 2006; Snow
et al., 2006; Banko et al., 2007; Van Durme and
Pas¸ca, 2008; Kozareva and Hovy, 2010). In the lat-
ter case, the extracted class labels take the form of
head nouns preceded by modifiers. Examples are
“cities”, “european cities” (Etzioni et al., 2005);
“artists”, “strong acids” (Pantel and Pennacchiotti,
2006); “outdoor activities”, “prestigious private
schools” (Van Durme and Pas¸ca, 2008); “methate-
rians”, “aquatic birds” (Kozareva and Hovy, 2010).
In contrast, the class labels extracted in our method
exhibit greater syntactic diversity and are finer-
grained. In addition, they are not constrained to a
particular set of categories available in resources like
Wikipedia.
Fine-grained class labels roughly correspond to
queries submitted in typed search (Demartini et al.,
2009) or entity search (Balog et al., 2010) or list-
seeking questions (“name the circuit judges in the
cayman islands that are british”). But our focus is
on generating, rather than answering such queries
or, more generally, attempting to deeply understand
their semantics (Li, 2010). Phrase similarities can
be derived with any methods, using documents (Lin
and Wu, 2009) or search queries (Jain and Pennac-
chiotti, 2010).
Whether Web search queries are a useful textual
data source for open-domain information extraction
has been investigated in several tasks. Examples are
collecting unlabeled sets of similar instances (Jain
and Pennacchiotti, 2010), ranking of class labels
already extracted from text (Pas¸ca, 2010), extract-
ing attributes of instances (Alfonseca et al., 2010)
and identifying the occurrences in queries of in-
stances of several types, where the types are de-
fined in a manually-created resource (Pantel et al.,
2012). Comparatively, we show that queries are use-
ful in identifying possible class labels, not only re-
ranking them; and even in populating the class labels
with relevant, albeit small, sets of corresponding in-
stances.
As automatically-extracted class labels become
finer-grained, they more clearly illustrate a phe-
nomenon that received little attention. Namely, class
labels of an instance, on one hand, and relations link-
ing the instance with other instances and classes, on
the other hand, are not mutually exclusive pieces
of knowledge. Their extraction does not necessar-
ily require different, dedicated techniques. Quite
the opposite, class labels serve in text as nothing
more than convenient lexical representations, or lex-
ical shorthands, of relations linking instances with
other instances. The class labels “no front license
plate states” and “states with no front license plate
requirement” are applicable to arizona. If so, it is
because arizona is a state, and states require the in-
stallation of license plates on vehicles, and the re-
quirement does not apply to the front of vehicles
in the case of arizona. The connection between
class labels and relations has been judiciously ex-
ploited in (Nastase and Strube, 2008). In that study,
relations encoded implicitly within Wikipedia cat-
egories are transformed into explicit relations. As
an example, the explicit relation that deconstruct-
ing harry is directed by woody allen is obtained
from the fact that deconstructing harry is listed un-
der “movies directed by woody allen” in Wikipedia.
Ours is the first approach to examine the potential
for extracting relations from search queries, where
relations are compactly and loosely folded into the
respective class labels. A variety of methods address
the more general task of acquisition of open-domain
relations from documents, e.g., (Zhu et al., 2009;
Carlson et al., 2010; Fader et al., 2011; Lao et al.,
2011).
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999951133333333">
The approach introduced in this paper exploits
knowledge loosely encoded within Web search
queries. It acquires a vocabulary of class labels that
are finer grained than in previous literature. The
class labels have precision comparable to that of
class labels derived from human-created knowledge
repositories. Furthermore, representative instances
are extracted from queries for the fine-grained class
labels, at encouraging levels of accuracy. Current
work explores the use of noisy syntactic features to
increase the accuracy of extracted class labels; the
extraction of instances from evidence in multiple,
rather than single queries; the expansion of extracted
instances into larger sets; and the conversion of fine-
grained class labels into relations among classes.
</bodyText>
<page confidence="0.997871">
412
</page>
<sectionHeader confidence="0.9903" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999083141509434">
E. Alfonseca, M. Pas¸ca, and E. Robledo-Arnuncio. 2010.
Acquisition of instance attributes via labeled and re-
lated instances. In Proceedings of the 33rd Interna-
tional Conference on Research and Development in In-
formation Retrieval (SIGIR-10), pages 58–65, Geneva,
Switzerland.
K. Balog, M. Bron, and M. de Rijke. 2010. Category-
based query modeling for entity search. In Proceed-
ings of the 32nd European Conference on Information
Retrieval (ECIR-10), pages 319–331, Milton Keynes,
United Kingdom.
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670–2676, Hyderabad, India.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), pages 224–231,
Seattle, Washington.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on
the Web. In Proceedings of the 34th Conference on
Very Large Data Bases (VLDB-08), pages 538–549,
Auckland, New Zealand.
A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and
T. Mitchell. 2010. Coupled semi-supervised learn-
ing for information extraction. In Proceedings of the
3rd ACM Conference on Web Search and Data Mining
(WSDM-10), pages 101–110, New York.
B. Dalvi, W. Cohen, and J. Callan. 2012. Websets: Ex-
tracting sets of entities from the Web using unsuper-
vised information extraction. In Proceedings of the
5th ACM Conference on Web Search and Data Mining
(WSDM-12), pages 243–252, Seattle, Washington.
D. Davidov and A. Rappoport. 2009. Enhancement
of lexical concepts using cross-lingual Web mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
09), pages 852–861, Singapore.
G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview
of the INEX 2009 Entity Ranking track. In INitiative
for the Evaluation of XML Retrieval Workshop, pages
254–264, Brisbane, Australia.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in Web text. In Proceed-
ings of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI-07), pages 2733–2739, Hy-
derabad, India.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165(1):91–134.
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and
Mausam. 2011. Open information extraction: The
second generation. In Proceedings of the 22nd In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-11), pages 3–10, Barcelona, Spain.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP-11), pages
1535–1545, Edinburgh, Scotland.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539–545, Nantes, France.
J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum.
2013. YAGO2: a spatially and temporally enhanced
knowledge base from Wikipedia. Artificial Intelli-
gence, 194:28–61.
A. Jain and M. Pennacchiotti. 2010. Open entity ex-
traction from Web search query logs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING-10), pages 510–518,
Beijing, China.
Z. Kozareva and E. Hovy. 2010. A semi-supervised
method to learn and construct taxonomies using the
web. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-10), pages 1110–1118, Cambridge, Mas-
sachusetts.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern
linkage graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-08), pages 1048–1056, Columbus, Ohio.
N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk
inference and learning in a large scale knowledge base.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
11), pages 529–539, Edinburgh, Scotland.
X. Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 1337–1345, Up-
psala, Sweden.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1–
7, Taipei, Taiwan.
D. Lin and X. Wu. 2009. Phrase clustering for discrim-
inative learning. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-IJCNLP-09), pages 1030–1038, Singapore.
</reference>
<page confidence="0.99125">
413
</page>
<reference confidence="0.999799019417476">
T. Lin, Mausam, and O. Etzioni. 2012. No noun phrase
left behind: Detecting and typing unlinkable enti-
ties. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL-12), pages 893–903, Jeju Island, Korea.
V. Nastase and M. Strube. 2008. Decoding Wikipedia
categories for knowledge acquisition. In Proceedings
of the 23rd National Conference on Artificial Intelli-
gence (AAAI-08), pages 1219–1224, Chicago, Illinois.
M. Pas¸ca. 2010. The role of queries in ranking la-
beled instances extracted from text. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING-10), pages 955–962, Bei-
jing, China.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL-06), pages 113–120,
Sydney, Australia.
P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-09), pages 938–947, Singapore.
P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity
types from query logs via user intent modeling. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-12), pages
563–571, Jeju Island, Korea.
M. Pennacchiotti and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP-09), pages 238–247,
Singapore.
S. Ponzetto and R. Navigli. 2009. Large-scale taxonomy
mapping for restructuring and integrating Wikipedia.
In Proceedings of the 21st International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), pages 2083–
2088, Pasadena, California.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings ofthe 22nd
National Conference on Artificial Intelligence (AAAI-
07), pages 1440–1447, Vancouver, British Columbia.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3):130–137.
D. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. 2005.
Probabilistic question answering on the Web. Journal
of the American Society for Information Science and
Technology, 56(3):571–583.
M. Remy. 2002. Wikipedia: The free encyclopedia. On-
line Information Review, 26(6):434.
S. Shi, H. Zhang, X. Yuan, and J. Wen. 2010.
Corpus-based semantic class mining: Distributional
vs. pattern-based approaches. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING-10), pages 993–1001, Beijing,
China.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801–808, Sydney, Australia.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL-10), pages 1473–1481, Uppsala,
Sweden.
P. Talukdar, J. Reisinger, M. Pas¸ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph ran-
dom walks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-08), pages 582–590, Honolulu, Hawaii.
B. Van Durme and M. Pas¸ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243–
1248, Chicago, Illinois.
R. Wang and W. Cohen. 2009. Automatic set instance
extraction using the Web. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-IJCNLP-09), pages 441–449, Singa-
pore.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635–644, Beijing, China.
W. Wu, , H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
Proceedings of the 2012 International Conference on
Management of Data (SIGMOD-12), pages 481–492,
Scottsdale, Arizona.
J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. Stat-
Snowball: a statistical approach to extracting entity re-
lationships. In Proceedings of the 18th World Wide
Web Conference (WWW-09), pages 101–110, Madrid,
Spain.
</reference>
<page confidence="0.998115">
414
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327084">
<title confidence="0.7556505">Open-Domain Fine-Grained Class Extraction from Web Search Queries Google</title>
<address confidence="0.865876">1600 Amphitheatre Mountain View, California</address>
<email confidence="0.999793">mars@google.com</email>
<abstract confidence="0.9844554">This paper introduces a method for extractfine-grained class labels with taxation agreements with from Web search queries. The class labels are more numerous and more diverse than those produced by current extraction methods. Also extracted are representative sets of instances for the class labels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Alfonseca</author>
<author>M Pas¸ca</author>
<author>E Robledo-Arnuncio</author>
</authors>
<title>Acquisition of instance attributes via labeled and related instances.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd International Conference on Research and Development in Information Retrieval (SIGIR-10),</booktitle>
<pages>58--65</pages>
<location>Geneva, Switzerland.</location>
<marker>Alfonseca, Pas¸ca, Robledo-Arnuncio, 2010</marker>
<rawString>E. Alfonseca, M. Pas¸ca, and E. Robledo-Arnuncio. 2010. Acquisition of instance attributes via labeled and related instances. In Proceedings of the 33rd International Conference on Research and Development in Information Retrieval (SIGIR-10), pages 58–65, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>M Bron</author>
<author>M de Rijke</author>
</authors>
<title>Categorybased query modeling for entity search.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd European Conference on Information Retrieval (ECIR-10),</booktitle>
<pages>319--331</pages>
<location>Milton Keynes, United Kingdom.</location>
<marker>Balog, Bron, de Rijke, 2010</marker>
<rawString>K. Balog, M. Bron, and M. de Rijke. 2010. Categorybased query modeling for entity search. In Proceedings of the 32nd European Conference on Information Retrieval (ECIR-10), pages 319–331, Milton Keynes, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>Michael J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2670--2676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="39402" citStr="Banko et al., 2007" startWordPosition="6360" endWordPosition="6363">ss labels. Discussion: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, Michael J Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670–2676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - a statistical part of speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00),</booktitle>
<pages>224--231</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="11376" citStr="Brants, 2000" startWordPosition="1789" endWordPosition="1790">ual Data: The experiments rely on a sample of 1 billion queries in English submitted by users of a Web search engine. Each query is accompanied by its frequency of occurrence. Also available is a sample of around 200 million Web documents in English. Phrase Similarities: Web documents are used in the experiments only to construct a phrase similarity repository following (Lin and Wu, 2009; Pantel et al., 2009). The repository contains ranked lists of the top 1000 phrases, computed to be the most distributionally similar to each of around 16 million phrases. Text Pre-Processing: The TnT tagger (Brants, 2000) assigns part of speech tags to words in class labels. Instances: To collect mappings from Wikipedia categories (as more general class labels) to titles of descendant Wikipedia articles (as instances), a snapshot of Wikipedia articles was intersected with the Wikipedia category hierarchy from (Ponzetto and Strube, 2007). The mappings connect a total of 1,535,083 instances to a total of 108,756 class labels. 4 Evaluation of Class Labels 4.1 Evaluation Procedure Experimental Runs: Human-compiled information available within Wikipedia serves as the source of data for two baseline runs. The set of</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT - a statistical part of speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), pages 224–231, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>A Halevy</author>
<author>D Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>WebTables: Exploring the power of tables on the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08),</booktitle>
<pages>538--549</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="39921" citStr="Cafarella et al., 2008" startWordPosition="6444" endWordPosition="6447">9; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. WebTables: Exploring the power of tables on the Web. In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08), pages 538–549, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>R Wang</author>
<author>E Hruschka</author>
<author>T Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM Conference on Web Search and Data Mining (WSDM-10),</booktitle>
<pages>101--110</pages>
<location>New York.</location>
<contexts>
<context position="40199" citStr="Carlson et al., 2010" startWordPosition="6492" endWordPosition="6495">r existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and P</context>
<context position="44099" citStr="Carlson et al., 2010" startWordPosition="7095" endWordPosition="7098">d implicitly within Wikipedia categories are transformed into explicit relations. As an example, the explicit relation that deconstructing harry is directed by woody allen is obtained from the fact that deconstructing harry is listed under “movies directed by woody allen” in Wikipedia. Ours is the first approach to examine the potential for extracting relations from search queries, where relations are compactly and loosely folded into the respective class labels. A variety of methods address the more general task of acquisition of open-domain relations from documents, e.g., (Zhu et al., 2009; Carlson et al., 2010; Fader et al., 2011; Lao et al., 2011). 7 Conclusion The approach introduced in this paper exploits knowledge loosely encoded within Web search queries. It acquires a vocabulary of class labels that are finer grained than in previous literature. The class labels have precision comparable to that of class labels derived from human-created knowledge repositories. Furthermore, representative instances are extracted from queries for the fine-grained class labels, at encouraging levels of accuracy. Current work explores the use of noisy syntactic features to increase the accuracy of extracted clas</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, R. Wang, E. Hruschka, and T. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the 3rd ACM Conference on Web Search and Data Mining (WSDM-10), pages 101–110, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dalvi</author>
<author>W Cohen</author>
<author>J Callan</author>
</authors>
<title>Websets: Extracting sets of entities from the Web using unsupervised information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 5th ACM Conference on Web Search and Data Mining (WSDM-12),</booktitle>
<pages>243--252</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="40006" citStr="Dalvi et al., 2012" startWordPosition="6459" endWordPosition="6462">ko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the la</context>
</contexts>
<marker>Dalvi, Cohen, Callan, 2012</marker>
<rawString>B. Dalvi, W. Cohen, and J. Callan. 2012. Websets: Extracting sets of entities from the Web using unsupervised information extraction. In Proceedings of the 5th ACM Conference on Web Search and Data Mining (WSDM-12), pages 243–252, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Enhancement of lexical concepts using cross-lingual Web mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP09),</booktitle>
<pages>852--861</pages>
<contexts>
<context position="39665" citStr="Davidov and Rappoport, 2009" startWordPosition="6404" endWordPosition="6408">n, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (T</context>
</contexts>
<marker>Davidov, Rappoport, 2009</marker>
<rawString>D. Davidov and A. Rappoport. 2009. Enhancement of lexical concepts using cross-lingual Web mining. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP09), pages 852–861, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Demartini</author>
<author>T Iofciu</author>
<author>A de Vries</author>
</authors>
<title>Entity Ranking track.</title>
<date>2009</date>
<journal>Overview of the INEX</journal>
<booktitle>In INitiative for the Evaluation of XML Retrieval Workshop,</booktitle>
<pages>254--264</pages>
<location>Brisbane, Australia.</location>
<marker>Demartini, Iofciu, de Vries, 2009</marker>
<rawString>G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview of the INEX 2009 Entity Ranking track. In INitiative for the Evaluation of XML Retrieval Workshop, pages 254–264, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Locating complex named entities in Web text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2733--2739</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1613" citStr="Downey et al., 2007" startWordPosition="243" endWordPosition="246">ften complex noun phrases, since they must somehow summarize multiple semantic constraints. Although Web users are interested in both coarse (e.g., “companies”) and fine-grained (e.g., “software companies started in a garage”) class labels, virtually all class labels acquired from text by previous extraction methods (Etzioni et al., 2005; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et al., 2006) exhibit little syntactic diversity. Indeed, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions include “european cities” (Etzioni et al., 2005), “strong acids” (Pantel and Pennacchiotti, 2006), “prestigious private schools” (Van Durme and Pas¸ca, 2008), “aquatic birds” (Kozareva and Hovy, 2010). As an alternative to extractin</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>D. Downey, M. Broadhead, and O. Etzioni. 2007. Locating complex named entities in Web text. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2733–2739, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1332" citStr="Etzioni et al., 2005" startWordPosition="197" endWordPosition="200">started in a garage. The sets of instances associated with the classes become smaller; the class labels used to concisely describe the meaning of more specific concepts tend to become longer. In fact, fine-grained class labels such as “software companies started in a garage” are often complex noun phrases, since they must somehow summarize multiple semantic constraints. Although Web users are interested in both coarse (e.g., “companies”) and fine-grained (e.g., “software companies started in a garage”) class labels, virtually all class labels acquired from text by previous extraction methods (Etzioni et al., 2005; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et al., 2006) exhibit little syntactic diversity. Indeed, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security soft</context>
<context position="13029" citStr="Etzioni et al., 2005" startWordPosition="2057" endWordPosition="2060">cturers of Germany”) form the set of class labels � Score(I, C) � Q 405 “acquired” in run R,,,l. The prefix “List of” is discarded. For completeness, a third baseline run, Rd,, corresponds to class labels extracted from Web documents. The class labels are noun phrases C that fill extraction patterns equivalent to “C such as I”. The patterns are matched to document sentences. The boundaries of the class labels C are approximated from part of speech tags of sentence words (Van Durme and Pas¸ca, 2008). The patterns were proposed in (Hearst, 1992). They were employed widely in subsequent methods (Etzioni et al., 2005; Kozareva et al., 2008; Wu et al., 2012), which extract class labels precisely from the set of class labels C produced by the extraction patterns. Even methods using queries as a textual data source still extract class labels from documents using the same extraction patterns (Pas¸ca, 2010). Therefore, from the point of view of evaluating class labels, run Rd, is a valid representative of previous extraction methods, including (Etzioni et al., 2005; Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Pas¸ca, 2010; Wu et al., 2012). Besides the baseline runs, three experimental runs are consider</context>
<context position="40758" citStr="Etzioni et al., 2005" startWordPosition="6585" endWordPosition="6588">manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas¸ca, 2008); “methaterians”, “aquatic birds” (Kozareva and Hovy, 2010). In contrast, the class labels extracted in our method exhibit greater syntactic diversity and are finergrained. In addition, they are not constrained to a particular set of categories available in resources like Wikipedia. Fine-grained class labels roughly correspond to queries submitted in typed search (Demartini et al., 2009) or entity search (Balog et al., 2010) or listseeking questions (“na</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the Web: an experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>A Fader</author>
<author>J Christensen</author>
<author>S Soderland</author>
<author>Mausam</author>
</authors>
<title>Open information extraction: The second generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI-11),</booktitle>
<pages>3--10</pages>
<location>Barcelona,</location>
<contexts>
<context position="1716" citStr="Etzioni et al., 2011" startWordPosition="258" endWordPosition="261">eb users are interested in both coarse (e.g., “companies”) and fine-grained (e.g., “software companies started in a garage”) class labels, virtually all class labels acquired from text by previous extraction methods (Etzioni et al., 2005; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et al., 2006) exhibit little syntactic diversity. Indeed, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions include “european cities” (Etzioni et al., 2005), “strong acids” (Pantel and Pennacchiotti, 2006), “prestigious private schools” (Van Durme and Pas¸ca, 2008), “aquatic birds” (Kozareva and Hovy, 2010). As an alternative to extracting class labels from text, some methods simply import them from human-curated resources, for example fro</context>
</contexts>
<marker>Etzioni, Fader, Christensen, Soderland, Mausam, 2011</marker>
<rawString>O. Etzioni, A. Fader, J. Christensen, S. Soderland, and Mausam. 2011. Open information extraction: The second generation. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI-11), pages 3–10, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-11),</booktitle>
<pages>1535--1545</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="44119" citStr="Fader et al., 2011" startWordPosition="7099" endWordPosition="7102">kipedia categories are transformed into explicit relations. As an example, the explicit relation that deconstructing harry is directed by woody allen is obtained from the fact that deconstructing harry is listed under “movies directed by woody allen” in Wikipedia. Ours is the first approach to examine the potential for extracting relations from search queries, where relations are compactly and loosely folded into the respective class labels. A variety of methods address the more general task of acquisition of open-domain relations from documents, e.g., (Zhu et al., 2009; Carlson et al., 2010; Fader et al., 2011; Lao et al., 2011). 7 Conclusion The approach introduced in this paper exploits knowledge loosely encoded within Web search queries. It acquires a vocabulary of class labels that are finer grained than in previous literature. The class labels have precision comparable to that of class labels derived from human-created knowledge repositories. Furthermore, representative instances are extracted from queries for the fine-grained class labels, at encouraging levels of accuracy. Current work explores the use of noisy syntactic features to increase the accuracy of extracted class labels; the extrac</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-11), pages 1535–1545, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="12958" citStr="Hearst, 1992" startWordPosition="2048" endWordPosition="2049">es with the prefix “List of..” (e.g., “List of automobile manufacturers of Germany”) form the set of class labels � Score(I, C) � Q 405 “acquired” in run R,,,l. The prefix “List of” is discarded. For completeness, a third baseline run, Rd,, corresponds to class labels extracted from Web documents. The class labels are noun phrases C that fill extraction patterns equivalent to “C such as I”. The patterns are matched to document sentences. The boundaries of the class labels C are approximated from part of speech tags of sentence words (Van Durme and Pas¸ca, 2008). The patterns were proposed in (Hearst, 1992). They were employed widely in subsequent methods (Etzioni et al., 2005; Kozareva et al., 2008; Wu et al., 2012), which extract class labels precisely from the set of class labels C produced by the extraction patterns. Even methods using queries as a textual data source still extract class labels from documents using the same extraction patterns (Pas¸ca, 2010). Therefore, from the point of view of evaluating class labels, run Rd, is a valid representative of previous extraction methods, including (Etzioni et al., 2005; Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Pas¸ca, 2010; Wu et al.,</context>
<context position="37364" citStr="Hearst, 1992" startWordPosition="6031" endWordPosition="6032">o output from two previous extraction methods. The first method starts by internally generating a small set of seed instances for a class label given as input (Wang and Cohen, 2009). A set expansion module then expands the seed set into a longer, ranked list of instances. The instances are extracted from unstructured and semi-structured text within Web documents. The documents are accessed via the search interface of a general-purpose Web search engine (cf. (Wang and Cohen, 2009) for more details). The second method extracts instances of class labels using the extraction patterns proposed in (Hearst, 1992). As such, it is similar to (Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Wu et al., 2012). The method corresponds to the run Rd, described earlier, where the relative ranking of instances and class labels uses the co-occurrence of instances and class labels within queries (Pas¸ca, 2010). For the purpose of the evaluation, when no instances are available for a target class label, the class label is generalized into iteratively shorter phrases containing fewer modifiers, until some instances are available for the shorter phrase. For example, target class labels like actors with obsessive </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>F Suchanek</author>
<author>K Berberich</author>
<author>G Weikum</author>
</authors>
<title>YAGO2: a spatially and temporally enhanced knowledge base from Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--28</pages>
<contexts>
<context position="40335" citStr="Hoffart et al., 2013" startWordPosition="6515" endWordPosition="6518"> Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas¸ca, 2008); “methaterians”, “aquatic birds” (</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. 2013. YAGO2: a spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence, 194:28–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>M Pennacchiotti</author>
</authors>
<title>Open entity extraction from Web search query logs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>510--518</pages>
<location>Beijing, China.</location>
<contexts>
<context position="39329" citStr="Jain and Pennacchiotti, 2010" startWordPosition="6346" endWordPosition="6349">nformation extraction have limited ability to extract instances of fine-grained class labels. Discussion: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and cl</context>
<context position="41705" citStr="Jain and Pennacchiotti, 2010" startWordPosition="6724" endWordPosition="6728">addition, they are not constrained to a particular set of categories available in resources like Wikipedia. Fine-grained class labels roughly correspond to queries submitted in typed search (Demartini et al., 2009) or entity search (Balog et al., 2010) or listseeking questions (“name the circuit judges in the cayman islands that are british”). But our focus is on generating, rather than answering such queries or, more generally, attempting to deeply understand their semantics (Li, 2010). Phrase similarities can be derived with any methods, using documents (Lin and Wu, 2009) or search queries (Jain and Pennacchiotti, 2010). Whether Web search queries are a useful textual data source for open-domain information extraction has been investigated in several tasks. Examples are collecting unlabeled sets of similar instances (Jain and Pennacchiotti, 2010), ranking of class labels already extracted from text (Pas¸ca, 2010), extracting attributes of instances (Alfonseca et al., 2010) and identifying the occurrences in queries of instances of several types, where the types are defined in a manually-created resource (Pantel et al., 2012). Comparatively, we show that queries are useful in identifying possible class labels</context>
</contexts>
<marker>Jain, Pennacchiotti, 2010</marker>
<rawString>A. Jain and M. Pennacchiotti. 2010. Open entity extraction from Web search query logs. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 510–518, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Hovy</author>
</authors>
<title>A semi-supervised method to learn and construct taxonomies using the web.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<pages>1110--1118</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1385" citStr="Kozareva and Hovy, 2010" startWordPosition="206" endWordPosition="209">ted with the classes become smaller; the class labels used to concisely describe the meaning of more specific concepts tend to become longer. In fact, fine-grained class labels such as “software companies started in a garage” are often complex noun phrases, since they must somehow summarize multiple semantic constraints. Although Web users are interested in both coarse (e.g., “companies”) and fine-grained (e.g., “software companies started in a garage”) class labels, virtually all class labels acquired from text by previous extraction methods (Etzioni et al., 2005; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et al., 2006) exhibit little syntactic diversity. Indeed, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions incl</context>
<context position="39576" citStr="Kozareva and Hovy, 2010" startWordPosition="6389" endWordPosition="6392">riments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009</context>
<context position="40959" citStr="Kozareva and Hovy, 2010" startWordPosition="6610" endWordPosition="6613"> or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas¸ca, 2008); “methaterians”, “aquatic birds” (Kozareva and Hovy, 2010). In contrast, the class labels extracted in our method exhibit greater syntactic diversity and are finergrained. In addition, they are not constrained to a particular set of categories available in resources like Wikipedia. Fine-grained class labels roughly correspond to queries submitted in typed search (Demartini et al., 2009) or entity search (Balog et al., 2010) or listseeking questions (“name the circuit judges in the cayman islands that are british”). But our focus is on generating, rather than answering such queries or, more generally, attempting to deeply understand their semantics (L</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Z. Kozareva and E. Hovy. 2010. A semi-supervised method to learn and construct taxonomies using the web. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-10), pages 1110–1118, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic class learning from the Web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>1048--1056</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="13052" citStr="Kozareva et al., 2008" startWordPosition="2061" endWordPosition="2064">orm the set of class labels � Score(I, C) � Q 405 “acquired” in run R,,,l. The prefix “List of” is discarded. For completeness, a third baseline run, Rd,, corresponds to class labels extracted from Web documents. The class labels are noun phrases C that fill extraction patterns equivalent to “C such as I”. The patterns are matched to document sentences. The boundaries of the class labels C are approximated from part of speech tags of sentence words (Van Durme and Pas¸ca, 2008). The patterns were proposed in (Hearst, 1992). They were employed widely in subsequent methods (Etzioni et al., 2005; Kozareva et al., 2008; Wu et al., 2012), which extract class labels precisely from the set of class labels C produced by the extraction patterns. Even methods using queries as a textual data source still extract class labels from documents using the same extraction patterns (Pas¸ca, 2010). Therefore, from the point of view of evaluating class labels, run Rd, is a valid representative of previous extraction methods, including (Etzioni et al., 2005; Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Pas¸ca, 2010; Wu et al., 2012). Besides the baseline runs, three experimental runs are considered. In run Ryl, the que</context>
<context position="37414" citStr="Kozareva et al., 2008" startWordPosition="6039" endWordPosition="6042">ods. The first method starts by internally generating a small set of seed instances for a class label given as input (Wang and Cohen, 2009). A set expansion module then expands the seed set into a longer, ranked list of instances. The instances are extracted from unstructured and semi-structured text within Web documents. The documents are accessed via the search interface of a general-purpose Web search engine (cf. (Wang and Cohen, 2009) for more details). The second method extracts instances of class labels using the extraction patterns proposed in (Hearst, 1992). As such, it is similar to (Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Wu et al., 2012). The method corresponds to the run Rd, described earlier, where the relative ranking of instances and class labels uses the co-occurrence of instances and class labels within queries (Pas¸ca, 2010). For the purpose of the evaluation, when no instances are available for a target class label, the class label is generalized into iteratively shorter phrases containing fewer modifiers, until some instances are available for the shorter phrase. For example, target class labels like actors with obsessive compulsive disorder, beatles songs of love, garden</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the Web with hyponym pattern linkage graphs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 1048–1056, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lao</author>
<author>T Mitchell</author>
<author>W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP11),</booktitle>
<pages>529--539</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="44138" citStr="Lao et al., 2011" startWordPosition="7103" endWordPosition="7106">re transformed into explicit relations. As an example, the explicit relation that deconstructing harry is directed by woody allen is obtained from the fact that deconstructing harry is listed under “movies directed by woody allen” in Wikipedia. Ours is the first approach to examine the potential for extracting relations from search queries, where relations are compactly and loosely folded into the respective class labels. A variety of methods address the more general task of acquisition of open-domain relations from documents, e.g., (Zhu et al., 2009; Carlson et al., 2010; Fader et al., 2011; Lao et al., 2011). 7 Conclusion The approach introduced in this paper exploits knowledge loosely encoded within Web search queries. It acquires a vocabulary of class labels that are finer grained than in previous literature. The class labels have precision comparable to that of class labels derived from human-created knowledge repositories. Furthermore, representative instances are extracted from queries for the fine-grained class labels, at encouraging levels of accuracy. Current work explores the use of noisy syntactic features to increase the accuracy of extracted class labels; the extraction of instances f</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>N. Lao, T. Mitchell, and W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP11), pages 529–539, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
</authors>
<title>Understanding the semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1337--1345</pages>
<location>Uppsala,</location>
<contexts>
<context position="41567" citStr="Li, 2010" startWordPosition="6705" endWordPosition="6706">). In contrast, the class labels extracted in our method exhibit greater syntactic diversity and are finergrained. In addition, they are not constrained to a particular set of categories available in resources like Wikipedia. Fine-grained class labels roughly correspond to queries submitted in typed search (Demartini et al., 2009) or entity search (Balog et al., 2010) or listseeking questions (“name the circuit judges in the cayman islands that are british”). But our focus is on generating, rather than answering such queries or, more generally, attempting to deeply understand their semantics (Li, 2010). Phrase similarities can be derived with any methods, using documents (Lin and Wu, 2009) or search queries (Jain and Pennacchiotti, 2010). Whether Web search queries are a useful textual data source for open-domain information extraction has been investigated in several tasks. Examples are collecting unlabeled sets of similar instances (Jain and Pennacchiotti, 2010), ranking of class labels already extracted from text (Pas¸ca, 2010), extracting attributes of instances (Alfonseca et al., 2010) and identifying the occurrences in queries of instances of several types, where the types are defined</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>X. Li. 2010. Understanding the semantic structure of noun phrase queries. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1337–1345, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics (COLING-02),</booktitle>
<volume>1</volume>
<pages>pages</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5420" citStr="Lin and Pantel, 2002" startWordPosition="815" endWordPosition="818">) further restricts the generated class labels to those that appear within the larger set of arbitrary Web search queries. Initial Vocabulary of Class Labels: Out of a set of arbitrary search queries available as input, the queries in the format “list of ..” are selected as the initial vocabulary of class labels. The prefix “list of” is discarded from each query. Thus, the query “list ofsoftware companies that use linux” gives the class label “software companies that use linux”. Generation via Phrase Similarities: As a prerequisite to generating class labels, distributionally similar phrases (Lin and Pantel, 2002; Lin and Wu, 2009; Pantel et al., 2009) and their scores are collected in advance. A phrase is represented as a vector of its contextual features. A feature is a word, collected from windows of three words centered around the occurrences of the phrase in sentences across Web documents (Lin and Wu, 2009). In the contextual vector of a phrase, the weight of a feature is the pointwise-mutual information (Lin and Wu, 2009) between the phrase P and the feature F. The distributional similarity score between two phrases is the cosine similarity between the contextual vectors of the two phrases. The </context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002. Concept discovery from text. In Proceedings of the 19th International Conference on Computational linguistics (COLING-02), pages 1– 7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>X Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP-09),</booktitle>
<pages>1030--1038</pages>
<contexts>
<context position="5438" citStr="Lin and Wu, 2009" startWordPosition="819" endWordPosition="822">e generated class labels to those that appear within the larger set of arbitrary Web search queries. Initial Vocabulary of Class Labels: Out of a set of arbitrary search queries available as input, the queries in the format “list of ..” are selected as the initial vocabulary of class labels. The prefix “list of” is discarded from each query. Thus, the query “list ofsoftware companies that use linux” gives the class label “software companies that use linux”. Generation via Phrase Similarities: As a prerequisite to generating class labels, distributionally similar phrases (Lin and Pantel, 2002; Lin and Wu, 2009; Pantel et al., 2009) and their scores are collected in advance. A phrase is represented as a vector of its contextual features. A feature is a word, collected from windows of three words centered around the occurrences of the phrase in sentences across Web documents (Lin and Wu, 2009). In the contextual vector of a phrase, the weight of a feature is the pointwise-mutual information (Lin and Wu, 2009) between the phrase P and the feature F. The distributional similarity score between two phrases is the cosine similarity between the contextual vectors of the two phrases. The lists of most dist</context>
<context position="11153" citStr="Lin and Wu, 2009" startWordPosition="1752" endWordPosition="1755">eries “buick lacrosse remote start”, “how to remote start buick lacrosse”, “remote start for buick lacrosse”. Candidate instances of a class label are ranked in decreasing order of their scores. 3 Experimental Setting Web Textual Data: The experiments rely on a sample of 1 billion queries in English submitted by users of a Web search engine. Each query is accompanied by its frequency of occurrence. Also available is a sample of around 200 million Web documents in English. Phrase Similarities: Web documents are used in the experiments only to construct a phrase similarity repository following (Lin and Wu, 2009; Pantel et al., 2009). The repository contains ranked lists of the top 1000 phrases, computed to be the most distributionally similar to each of around 16 million phrases. Text Pre-Processing: The TnT tagger (Brants, 2000) assigns part of speech tags to words in class labels. Instances: To collect mappings from Wikipedia categories (as more general class labels) to titles of descendant Wikipedia articles (as instances), a snapshot of Wikipedia articles was intersected with the Wikipedia category hierarchy from (Ponzetto and Strube, 2007). The mappings connect a total of 1,535,083 instances to</context>
<context position="41656" citStr="Lin and Wu, 2009" startWordPosition="6717" endWordPosition="6720">c diversity and are finergrained. In addition, they are not constrained to a particular set of categories available in resources like Wikipedia. Fine-grained class labels roughly correspond to queries submitted in typed search (Demartini et al., 2009) or entity search (Balog et al., 2010) or listseeking questions (“name the circuit judges in the cayman islands that are british”). But our focus is on generating, rather than answering such queries or, more generally, attempting to deeply understand their semantics (Li, 2010). Phrase similarities can be derived with any methods, using documents (Lin and Wu, 2009) or search queries (Jain and Pennacchiotti, 2010). Whether Web search queries are a useful textual data source for open-domain information extraction has been investigated in several tasks. Examples are collecting unlabeled sets of similar instances (Jain and Pennacchiotti, 2010), ranking of class labels already extracted from text (Pas¸ca, 2010), extracting attributes of instances (Alfonseca et al., 2010) and identifying the occurrences in queries of instances of several types, where the types are defined in a manually-created resource (Pantel et al., 2012). Comparatively, we show that querie</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>D. Lin and X. Wu. 2009. Phrase clustering for discriminative learning. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP-09), pages 1030–1038, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lin</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>No noun phrase left behind: Detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL-12),</booktitle>
<pages>893--903</pages>
<location>Jeju Island,</location>
<contexts>
<context position="39131" citStr="Lin et al., 2012" startWordPosition="6317" endWordPosition="6320">11 and 0.27 at rank 5; 0.06 and 0.25 at rank 10; 0.05 and 0.22 at rank 20; and 0.05 and 0.20 at rank 50. The results confirm that, as explained earlier, previous methods for open-domain information extraction have limited ability to extract instances of fine-grained class labels. Discussion: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponz</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>T. Lin, Mausam, and O. Etzioni. 2012. No noun phrase left behind: Detecting and typing unlinkable entities. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL-12), pages 893–903, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>M Strube</author>
</authors>
<title>Decoding Wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1219--1224</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="43446" citStr="Nastase and Strube, 2008" startWordPosition="6994" endWordPosition="6997">d techniques. Quite the opposite, class labels serve in text as nothing more than convenient lexical representations, or lexical shorthands, of relations linking instances with other instances. The class labels “no front license plate states” and “states with no front license plate requirement” are applicable to arizona. If so, it is because arizona is a state, and states require the installation of license plates on vehicles, and the requirement does not apply to the front of vehicles in the case of arizona. The connection between class labels and relations has been judiciously exploited in (Nastase and Strube, 2008). In that study, relations encoded implicitly within Wikipedia categories are transformed into explicit relations. As an example, the explicit relation that deconstructing harry is directed by woody allen is obtained from the fact that deconstructing harry is listed under “movies directed by woody allen” in Wikipedia. Ours is the first approach to examine the potential for extracting relations from search queries, where relations are compactly and loosely folded into the respective class labels. A variety of methods address the more general task of acquisition of open-domain relations from doc</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>V. Nastase and M. Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1219–1224, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>The role of queries in ranking labeled instances extracted from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>955--962</pages>
<location>Beijing, China.</location>
<marker>Pas¸ca, 2010</marker>
<rawString>M. Pas¸ca. 2010. The role of queries in ranking labeled instances extracted from text. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 955–962, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06),</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2078" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="308" endWordPosition="312">d, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions include “european cities” (Etzioni et al., 2005), “strong acids” (Pantel and Pennacchiotti, 2006), “prestigious private schools” (Van Durme and Pas¸ca, 2008), “aquatic birds” (Kozareva and Hovy, 2010). As an alternative to extracting class labels from text, some methods simply import them from human-curated resources, for example from the set of categories encoded in Wikipedia (Remy, 2002). As a result, class labels potentially exhibit higher syntactic diversity. The modifiers-plus-nouns format (“computer security software companies”) is usually still the norm. But other formats are possible: “software companies based in london”, “software companies of the united kingdom”. Vocabulary cove</context>
<context position="40502" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="6541" endWordPosition="6544">ing relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas¸ca, 2008); “methaterians”, “aquatic birds” (Kozareva and Hovy, 2010). In contrast, the class labels extracted in our method exhibit greater syntactic diversity and are finergrained. In addition, they are not con</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06), pages 113–120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>E Crestan</author>
<author>A Borkovsky</author>
<author>A Popescu</author>
<author>V Vyas</author>
</authors>
<title>Web-scale distributional similarity and entity set expansion.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09),</booktitle>
<pages>938--947</pages>
<contexts>
<context position="5460" citStr="Pantel et al., 2009" startWordPosition="823" endWordPosition="826">labels to those that appear within the larger set of arbitrary Web search queries. Initial Vocabulary of Class Labels: Out of a set of arbitrary search queries available as input, the queries in the format “list of ..” are selected as the initial vocabulary of class labels. The prefix “list of” is discarded from each query. Thus, the query “list ofsoftware companies that use linux” gives the class label “software companies that use linux”. Generation via Phrase Similarities: As a prerequisite to generating class labels, distributionally similar phrases (Lin and Pantel, 2002; Lin and Wu, 2009; Pantel et al., 2009) and their scores are collected in advance. A phrase is represented as a vector of its contextual features. A feature is a word, collected from windows of three words centered around the occurrences of the phrase in sentences across Web documents (Lin and Wu, 2009). In the contextual vector of a phrase, the weight of a feature is the pointwise-mutual information (Lin and Wu, 2009) between the phrase P and the feature F. The distributional similarity score between two phrases is the cosine similarity between the contextual vectors of the two phrases. The lists of most distributionally similar p</context>
<context position="11175" citStr="Pantel et al., 2009" startWordPosition="1756" endWordPosition="1759">sse remote start”, “how to remote start buick lacrosse”, “remote start for buick lacrosse”. Candidate instances of a class label are ranked in decreasing order of their scores. 3 Experimental Setting Web Textual Data: The experiments rely on a sample of 1 billion queries in English submitted by users of a Web search engine. Each query is accompanied by its frequency of occurrence. Also available is a sample of around 200 million Web documents in English. Phrase Similarities: Web documents are used in the experiments only to construct a phrase similarity repository following (Lin and Wu, 2009; Pantel et al., 2009). The repository contains ranked lists of the top 1000 phrases, computed to be the most distributionally similar to each of around 16 million phrases. Text Pre-Processing: The TnT tagger (Brants, 2000) assigns part of speech tags to words in class labels. Instances: To collect mappings from Wikipedia categories (as more general class labels) to titles of descendant Wikipedia articles (as instances), a snapshot of Wikipedia articles was intersected with the Wikipedia category hierarchy from (Ponzetto and Strube, 2007). The mappings connect a total of 1,535,083 instances to a total of 108,756 cl</context>
</contexts>
<marker>Pantel, Crestan, Borkovsky, Popescu, Vyas, 2009</marker>
<rawString>P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and V. Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 938–947, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>T Lin</author>
<author>M Gamon</author>
</authors>
<title>Mining entity types from query logs via user intent modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-12),</booktitle>
<pages>563--571</pages>
<location>Jeju Island,</location>
<contexts>
<context position="40369" citStr="Pantel et al., 2012" startWordPosition="6521" endWordPosition="6524">2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas¸ca, 2008); “methaterians”, “aquatic birds” (Kozareva and Hovy, 2010). In contr</context>
<context position="42220" citStr="Pantel et al., 2012" startWordPosition="6802" endWordPosition="6805">ved with any methods, using documents (Lin and Wu, 2009) or search queries (Jain and Pennacchiotti, 2010). Whether Web search queries are a useful textual data source for open-domain information extraction has been investigated in several tasks. Examples are collecting unlabeled sets of similar instances (Jain and Pennacchiotti, 2010), ranking of class labels already extracted from text (Pas¸ca, 2010), extracting attributes of instances (Alfonseca et al., 2010) and identifying the occurrences in queries of instances of several types, where the types are defined in a manually-created resource (Pantel et al., 2012). Comparatively, we show that queries are useful in identifying possible class labels, not only reranking them; and even in populating the class labels with relevant, albeit small, sets of corresponding instances. As automatically-extracted class labels become finer-grained, they more clearly illustrate a phenomenon that received little attention. Namely, class labels of an instance, on one hand, and relations linking the instance with other instances and classes, on the other hand, are not mutually exclusive pieces of knowledge. Their extraction does not necessarily require different, dedicat</context>
</contexts>
<marker>Pantel, Lin, Gamon, 2012</marker>
<rawString>P. Pantel, T. Lin, and M. Gamon. 2012. Mining entity types from query logs via user intent modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-12), pages 563–571, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>P Pantel</author>
</authors>
<title>Entity extraction via ensemble semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09),</booktitle>
<pages>238--247</pages>
<contexts>
<context position="39299" citStr="Pennacchiotti and Pantel, 2009" startWordPosition="6342" endWordPosition="6345">evious methods for open-domain information extraction have limited ability to extract instances of fine-grained class labels. Discussion: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (C</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>M. Pennacchiotti and P. Pantel. 2009. Entity extraction via ensemble semantics. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 238–247, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>R Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09),</booktitle>
<pages>pages</pages>
<location>Pasadena, California.</location>
<contexts>
<context position="39754" citStr="Ponzetto and Navigli, 2009" startWordPosition="6419" endWordPosition="6422">012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pante</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>S. Ponzetto and R. Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating Wikipedia. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09), pages 2083– 2088, Pasadena, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 22nd National Conference on Artificial Intelligence (AAAI07),</booktitle>
<pages>1440--1447</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="8072" citStr="Ponzetto and Strube, 2007" startWordPosition="1256" endWordPosition="1259">t full queries are discarded. 2.2 Extraction of Instances Overview: Our method mines instances of finegrained class labels from queries. In a nutshell, it identifies queries containing two types of information simultaneously. First, the queries contain an instance (marvin gaye) of the more general class labels (“musicians”) from which the fine-grained class labels (“musicians who have been shot”) can be obtained. Second, the queries contain the constraints added by the fine-grained class labels (“... shot”) on top of the more general class labels. Instances of General Class Labels: Following (Ponzetto and Strube, 2007), the Wikipedia category network is refined into a hierarchy that discards 404 non-IsA (thematic) edges, and retains only IsA (subsumption) edges from the network (Ponzetto and Strube, 2007). Instances, i.e., titles of Wikipedia articles, are propagated upwards to all their ancestor categories. The class label “musicians” would be mapped into madonna, marvin gaye, jon bon jovi etc. The mappings from each ancestor category, to all its descendant instances in the Wikipedia hierarchy, represent our mappings from more general class labels to instances. Decomposition of Fine-Grained Class Labels: A</context>
<context position="11697" citStr="Ponzetto and Strube, 2007" startWordPosition="1837" endWordPosition="1840">ments only to construct a phrase similarity repository following (Lin and Wu, 2009; Pantel et al., 2009). The repository contains ranked lists of the top 1000 phrases, computed to be the most distributionally similar to each of around 16 million phrases. Text Pre-Processing: The TnT tagger (Brants, 2000) assigns part of speech tags to words in class labels. Instances: To collect mappings from Wikipedia categories (as more general class labels) to titles of descendant Wikipedia articles (as instances), a snapshot of Wikipedia articles was intersected with the Wikipedia category hierarchy from (Ponzetto and Strube, 2007). The mappings connect a total of 1,535,083 instances to a total of 108,756 class labels. 4 Evaluation of Class Labels 4.1 Evaluation Procedure Experimental Runs: Human-compiled information available within Wikipedia serves as the source of data for two baseline runs. The set of all categories, listed in Wikipedia for any of its articles, corresponds to the set of class labels “acquired” in run R, Categories used for internal Wikipedia bookkeeping (Ponzetto and Strube, 2007) are discarded. Their names contain one of the words article(s), category(ies), indices, pages, redirects, stubs, or temp</context>
<context position="25492" citStr="Ponzetto and Strube, 2007" startWordPosition="4148" endWordPosition="4151">Ql, Rd, and RQy, over class labels that match (i.e., contain) each of the 75 target phrases (0.25), for RQy. Precision over Samples of Class Labels: The precision is separately computed over a random sample of 400 class labels per experimental run. The samples are selected from the set of all class labels extracted by the respective run. The precision scores are: 0.759 for R,,,,; 1.000 for R,,,l; 0.806 for Rd,; 0.811 for RQl; 0.856 for RQ,,; and 0.711 for RQy. The scores are in line with scores computed earlier over the target phrases, in the fourth column of Table 3. Discussion: As noted in (Ponzetto and Strube, 2007), Wikipedia organizes its articles and categories into a category network that mixes IsA (subsumption) edges with non-IsA (thematic) edges. Whenever an edge in Wikipedia is not IsA, the paraaa 1 adelaide cbd 5 american fascism 10 antarctic region 15 baquba 20 boulder colorado 25 chester arthur 30 contemporary art 35 eating disorders 40 halogens 45 juan carlos 50 lucky ali 55 phosphorus 60 rouen 65 u.s. 70 wlan 75 Phrase aaa 1 adelaide cbd 5 american fascism 10 antarctic region 15 baquba 20 boulder colorado 25 chester arthur 30 contemporary art 35 eating disorders 40 halogens 45 juan carlos 50 </context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proceedings ofthe 22nd National Conference on Artificial Intelligence (AAAI07), pages 1440–1447, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="9994" citStr="Porter, 1980" startWordPosition="1556" endWordPosition="1557"> bag of words, for each fine-grained class label. As an illustration, the class labels “musicians who have been shot” and “automobiles with remote start” are decomposed into pairs like &lt;madonna, {shot}&gt;, &lt;marvin gaye, {shot}&gt;; and &lt;buick lacrosse, {remote, start}&gt;, &lt;nissan versa, {remote, start}&gt;, respectively. Matching of Candidate Instances: A decomposed class label is retained, if there are matching queries that contain the candidate instance, the bag of words, and optionally stop words. Otherwise, the decomposed class label is discarded. The word matching is performed after word stemming (Porter, 1980). The aggregated frequency of the matching queries is assigned as the score of the candidate instance for the fine-grained class label: (Freq(Q)JMatch(Q, &lt; I, C &gt;)) (1) For example, the score of the candidate instance marvin gaye for the class label “musicians who have been shot”, is the sum of the frequencies of the matching queries “marvin gaye is shot”, “when was marvin gaye shot”, “why marvin gaye was shot” etc. Similarly, the score of buick lacrosse for “automobiles with remote start” is given by the aggregated frequencies of the queries “buick lacrosse remote start”, “how to remote start</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>W Fan</author>
<author>H Qi</author>
<author>H Wu</author>
<author>A Grewal</author>
</authors>
<title>Probabilistic question answering on the Web.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="9200" citStr="Radev et al., 2005" startWordPosition="1432" endWordPosition="1435"> from more general class labels to instances. Decomposition of Fine-Grained Class Labels: A fine-grained class label (e.g., “musicians who have been shot”) is effectively decomposed into pairs of two pieces of information. The first piece is a more general class label (“musicians”), if any occurs in it. The second piece is a bag of words, collected from the remainder of the fine-grained class label after discarding stop words. Note that the standard set of stop words is augmented with auxiliary verbs (e.g., does, has, is, would), determiners, conjunctions, prepositions, and question wh-words (Radev et al., 2005) (e.g., where, how). In the first piece of each pair, the general class label is then replaced with each of its instances. This produces multiple pairs of a candidate instance and a bag of words, for each fine-grained class label. As an illustration, the class labels “musicians who have been shot” and “automobiles with remote start” are decomposed into pairs like &lt;madonna, {shot}&gt;, &lt;marvin gaye, {shot}&gt;; and &lt;buick lacrosse, {remote, start}&gt;, &lt;nissan versa, {remote, start}&gt;, respectively. Matching of Candidate Instances: A decomposed class label is retained, if there are matching queries that </context>
</contexts>
<marker>Radev, Fan, Qi, Wu, Grewal, 2005</marker>
<rawString>D. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. 2005. Probabilistic question answering on the Web. Journal of the American Society for Information Science and Technology, 56(3):571–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Remy</author>
</authors>
<title>Wikipedia: The free encyclopedia.</title>
<date>2002</date>
<journal>Online Information Review,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="2373" citStr="Remy, 2002" startWordPosition="354" endWordPosition="355">ollow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions include “european cities” (Etzioni et al., 2005), “strong acids” (Pantel and Pennacchiotti, 2006), “prestigious private schools” (Van Durme and Pas¸ca, 2008), “aquatic birds” (Kozareva and Hovy, 2010). As an alternative to extracting class labels from text, some methods simply import them from human-curated resources, for example from the set of categories encoded in Wikipedia (Remy, 2002). As a result, class labels potentially exhibit higher syntactic diversity. The modifiers-plus-nouns format (“computer security software companies”) is usually still the norm. But other formats are possible: “software companies based in london”, “software companies of the united kingdom”. Vocabulary coverage gaps remain a problem, with many relevant class labels (“software companies of texas” “software companies started in a garage”, “software companies that give sap training”) still missing. There is a need for methods that more aggressively identify fine-grained class labels, beyond those ex</context>
</contexts>
<marker>Remy, 2002</marker>
<rawString>M. Remy. 2002. Wikipedia: The free encyclopedia. Online Information Review, 26(6):434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shi</author>
<author>H Zhang</author>
<author>X Yuan</author>
<author>J Wen</author>
</authors>
<title>Corpus-based semantic class mining: Distributional vs. pattern-based approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>993--1001</pages>
<location>Beijing, China.</location>
<contexts>
<context position="39348" citStr="Shi et al., 2010" startWordPosition="6350" endWordPosition="6353">ited ability to extract instances of fine-grained class labels. Discussion: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances</context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>S. Shi, H. Zhang, X. Yuan, and J. Wen. 2010. Corpus-based semantic class mining: Distributional vs. pattern-based approaches. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 993–1001, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL-06),</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1405" citStr="Snow et al., 2006" startWordPosition="210" endWordPosition="213">me smaller; the class labels used to concisely describe the meaning of more specific concepts tend to become longer. In fact, fine-grained class labels such as “software companies started in a garage” are often complex noun phrases, since they must somehow summarize multiple semantic constraints. Although Web users are interested in both coarse (e.g., “companies”) and fine-grained (e.g., “software companies started in a garage”) class labels, virtually all class labels acquired from text by previous extraction methods (Etzioni et al., 2005; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010; Snow et al., 2006) exhibit little syntactic diversity. Indeed, instances and class labels that are relatively complex nouns are known to be difficult to detect and pick out precisely from surrounding text (Downey et al., 2007). This and other challenges associated with large-scale extraction from Web text (Etzioni et al., 2011) cause the extracted class labels to usually follow a rigid modifiers-plus-nouns format. The format covers nouns (“companies”) possibly preceded by one or many modifiers (“software companies”, “computer security software companies”). Examples of actual extractions include “european cities</context>
<context position="39635" citStr="Snow et al., 2006" startWordPosition="6400" endWordPosition="6403">ay improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived fr</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL-06), pages 801–808, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
<author>F Pereira</author>
</authors>
<title>Experiments in graphbased semi-supervised learning methods for classinstance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1473--1481</pages>
<location>Uppsala,</location>
<contexts>
<context position="40290" citStr="Talukdar and Pereira, 2010" startWordPosition="6506" endWordPosition="6509">) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et al., 2013) or Freebase (Pantel et al., 2012). Alternatively, the vocabulary is not a closed set, but instead is acquired along with the instances (Pantel and Pennacchiotti, 2006; Snow et al., 2006; Banko et al., 2007; Van Durme and Pas¸ca, 2008; Kozareva and Hovy, 2010). In the latter case, the extracted class labels take the form of head nouns preceded by modifiers. Examples are “cities”, “european cities” (Etzioni et al., 2005); “artists”, “strong acids” (Pantel and Pennacchiotti, 2006); “outdoor activities”, “prestigious private schools” (Van Durme and Pas</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>P. Talukdar and F. Pereira. 2010. Experiments in graphbased semi-supervised learning methods for classinstance acquisition. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1473–1481, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
<author>J Reisinger</author>
<author>M Pas¸ca</author>
<author>D Ravichandran</author>
<author>R Bhagat</author>
<author>F Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<pages>582--590</pages>
<location>Honolulu, Hawaii.</location>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>P. Talukdar, J. Reisinger, M. Pas¸ca, D. Ravichandran, R. Bhagat, and F. Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), pages 582–590, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>M Pas¸ca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1243--1248</pages>
<location>Chicago, Illinois.</location>
<marker>Van Durme, Pas¸ca, 2008</marker>
<rawString>B. Van Durme and M. Pas¸ca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1243– 1248, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wang</author>
<author>W Cohen</author>
</authors>
<title>Automatic set instance extraction using the Web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP-09),</booktitle>
<pages>441--449</pages>
<contexts>
<context position="36932" citStr="Wang and Cohen, 2009" startWordPosition="5961" endWordPosition="5964">, toyota celica,..] garden landscap- [front, contemporary, gallery, ing magazines edge, view, chelsea, wallpaper, expo, wizard, sunset,..] holidays cele- [halloween, australia day, anzac brated in sydney day, independence day, waitangi day, melbourne cup, hogmanay, rotuma day, solstice, yule,..] Table 8: Ranked lists of instances extracted for a sample of class labels In additional experiments, the same evaluation procedure is applied to output from two previous extraction methods. The first method starts by internally generating a small set of seed instances for a class label given as input (Wang and Cohen, 2009). A set expansion module then expands the seed set into a longer, ranked list of instances. The instances are extracted from unstructured and semi-structured text within Web documents. The documents are accessed via the search interface of a general-purpose Web search engine (cf. (Wang and Cohen, 2009) for more details). The second method extracts instances of class labels using the extraction patterns proposed in (Hearst, 1992). As such, it is similar to (Kozareva et al., 2008; Van Durme and Pas¸ca, 2008; Wu et al., 2012). The method corresponds to the run Rd, described earlier, where the rel</context>
<context position="39425" citStr="Wang and Cohen, 2009" startWordPosition="6364" endWordPosition="6367">n: Earlier errors in the acquisition of the class label affect the usefulness of any instances that may be subsequently extracted for them. The experiments require candidate instances to appear in Wikipedia. This may improve precision, at the expense of not extracting instances that are not yet in Wikipedia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous met</context>
</contexts>
<marker>Wang, Cohen, 2009</marker>
<rawString>R. Wang and W. Cohen. 2009. Automatic set instance extraction using the Web. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP-09), pages 441–449, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D Weld</author>
</authors>
<title>Automatically refining the Wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th World Wide Web Conference (WWW-08),</booktitle>
<pages>635--644</pages>
<location>Beijing, China.</location>
<contexts>
<context position="39725" citStr="Wu and Weld, 2008" startWordPosition="6415" endWordPosition="6418">edia (Lin et al., 2012). 6 Related Work Previous methods for extracting classes of instances from text acquire sets of instances that are each either unlabeled (Pennacchiotti and Pantel, 2009; Jain and Pennacchiotti, 2010; Shi et al., 2010), or associated with a class label (Banko et al., 2007; Wang and Cohen, 2009). The sets of instances and/or class labels may be organized as flat sets or hierarchically, relative to inferred hierarchies (Kozareva and Hovy, 2010) or existing hierarchies such as WordNet (Snow et al., 2006; Davidov and Rappoport, 2009) or the category network within Wikipedia (Wu and Weld, 2008; Ponzetto and Navigli, 2009). Semi-structured text from Web documents is a complementary resource to unstructured text, for the purpose of extracting relations in general (Cafarella et al., 2008), and classes and instances in particular (Talukdar et al., 2008; Dalvi et al., 2012). With previous methods, the vocabulary of class labels potentially produced for any instance is confined to a closed set provided manually as input (Wang and Cohen, 2009; Carlson et al., 2010). The closed set is often derived from resources like Wikipedia (Talukdar and Pereira, 2010; Lin et al., 411 2012; Hoffart et </context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>F. Wu and D. Weld. 2008. Automatically refining the Wikipedia infobox ontology. In Proceedings of the 17th World Wide Web Conference (WWW-08), pages 635–644, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>H Wang</author>
<author>K Zhu</author>
</authors>
<title>Probase: a probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on Management of Data (SIGMOD-12),</booktitle>
<pages>481--492</pages>
<location>Scottsdale, Arizona.</location>
<marker>Li, Wang, Zhu, 2012</marker>
<rawString>W. Wu, , H. Li, H. Wang, and K. Zhu. 2012. Probase: a probabilistic taxonomy for text understanding. In Proceedings of the 2012 International Conference on Management of Data (SIGMOD-12), pages 481–492, Scottsdale, Arizona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>Z Nie</author>
<author>X Liu</author>
<author>B Zhang</author>
<author>J Wen</author>
</authors>
<title>StatSnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th World Wide Web Conference (WWW-09),</booktitle>
<pages>101--110</pages>
<location>Madrid,</location>
<contexts>
<context position="44077" citStr="Zhu et al., 2009" startWordPosition="7091" endWordPosition="7094">, relations encoded implicitly within Wikipedia categories are transformed into explicit relations. As an example, the explicit relation that deconstructing harry is directed by woody allen is obtained from the fact that deconstructing harry is listed under “movies directed by woody allen” in Wikipedia. Ours is the first approach to examine the potential for extracting relations from search queries, where relations are compactly and loosely folded into the respective class labels. A variety of methods address the more general task of acquisition of open-domain relations from documents, e.g., (Zhu et al., 2009; Carlson et al., 2010; Fader et al., 2011; Lao et al., 2011). 7 Conclusion The approach introduced in this paper exploits knowledge loosely encoded within Web search queries. It acquires a vocabulary of class labels that are finer grained than in previous literature. The class labels have precision comparable to that of class labels derived from human-created knowledge repositories. Furthermore, representative instances are extracted from queries for the fine-grained class labels, at encouraging levels of accuracy. Current work explores the use of noisy syntactic features to increase the accu</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>J. Zhu, Z. Nie, X. Liu, B. Zhang, and J. Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of the 18th World Wide Web Conference (WWW-09), pages 101–110, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
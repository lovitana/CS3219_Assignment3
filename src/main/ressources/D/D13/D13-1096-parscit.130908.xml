<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997943">
A Dataset for Research on Short-Text Conversation *
</title>
<author confidence="0.952124">
Hao Wang§ Zhengdong Lu‡ Hang Li‡ Enhong Chen§
</author>
<email confidence="0.8008245">
§xdwangh@mail.ustc.edu.cn ‡lu.zhengdong@huawei.com
‡hangli.hl@huawei.com §cheneh@ustc.edu.cn
</email>
<note confidence="0.323551">
§Univ. of Sci &amp; Tech of China, China ‡Noah’s Ark Lab, Huawei Technologies, Hong Kong
</note>
<sectionHeader confidence="0.981115" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930916666667">
Natural language conversation is widely re-
garded as a highly difficult problem, which
is usually attacked with either rule-based or
learning-based models. In this paper we
propose a retrieval-based automatic response
model for short-text conversation, to exploit
the vast amount of short conversation in-
stances available on social media. For this
purpose we introduce a dataset of short-text
conversation based on the real-world instances
from Sina Weibo (a popular Chinese mi-
croblog service), which will be soon released
to public. This dataset provides rich collec-
tion of instances for the research on finding
natural and relevant short responses to a given
short text, and useful for both training and test-
ing of conversation models. This dataset con-
sists of both naturally formed conversation-
s, manually labeled data, and a large repos-
itory of candidate responses. Our prelimi-
nary experiments demonstrate that the simple
retrieval-based conversation model performs
reasonably well when combined with the rich
instances in our dataset.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.940878">
Natural language conversation is one of the holy
grail of artificial intelligence, and has been taken as
the original form of the celebrated Turing test. Pre-
vious effort in this direction has largely focused on
analyzing the text and modeling the state of the con-
versation through dialogue models, while in this pa-
The work is done when the first author worked as intern at
Noah’s Ark Lab, Huawei Techologies.
per we take one step back and focus on a much easi-
er task of finding the response for a given short text.
This task is in clear contrast with previous effort in
dialogue modeling in the following two aspects
</bodyText>
<listItem confidence="0.831268857142857">
• we do not consider the context or history of
conversations, and assume that the given short
text is self-contained;
• we only require the response to be natural, rel-
evant, and human-like, and do not require it to
contain particular opinion, content, or to be of
particular style.
</listItem>
<bodyText confidence="0.99984125">
This task is much simpler than modeling a complete
dialogue session (e.g., as proposed in Turing test),
and probably not enough for real conversation sce-
nario which requires often several rounds of interac-
tions (e.g., automatic question answering system as
in (Litman et al., 2000)). However it can shed impor-
tant light on understanding the complicated mecha-
nism of the interaction between an utterance and it-
s response. The research in this direction will not
only instantly help the applications of short session
dialogue such as automatic message replying on mo-
bile phone and the chatbot employed in voice assis-
tant like Siri1, but also it will eventually benefit the
modeling of dialogues in a more general setting.
Previous effort in modeling lengthy dialogues fo-
cused either on rule-based or learning-based models
(Carpenter, 1997; Litman et al., 2000; Williams and
Young, 2007; Schatzmann et al., 2006; Misu et al.,
2012). This category of approaches require relative-
ly less data (e.g. reinforcement learning based) for
</bodyText>
<footnote confidence="0.982445">
1http://en.wikipedia.org/wiki/Siri
</footnote>
<page confidence="0.934623">
935
</page>
<note confidence="0.7366745">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943619047619">
training or no training at all, but much manual ef-
fort in designing the rules or the particular learning
algorithms. In this paper, we propose to attack this
problem using an alternative approach, by leverag-
ing the vast amount of training data available from
the social media. Similar ideas have appeared in (Ja-
farpour and Burges, 2010; Leuski and Traum, 2011)
as an initial step for training a chatbot.
With the emergence of social media, especially
microblogs such as Twitter, in the past decade, they
have become an important form of communication
for many people. As the result, it has collected con-
versation history with volume previously unthink-
able, which brings opportunity for attacking the con-
versation problem from a whole new angle. More
specifically, instead of generating a response to an
utterance, we pick a massive suitable one from the
candidate set. The hope is, with a reasonable re-
trieval model and a large enough candidate set, the
system can produce fairly natural and appropriate re-
sponses.
This retrieval-based model is somewhat like non-
parametric model in machine learning communities,
which performs well only when we have abundan-
t data. In our model, it needs only a relatively s-
mall labeled dataset for training the retrieval model,
but requires a rather large unlabeled set (e.g., one
million instances) for candidate responses. To fur-
ther promote the research in similar direction, we
create a dataset for training and testing the retrieval
model, with a candidate responses set of reason-
able size. Sina Weibo is the most popular Twitter-
like microblog service in China, hosting over 500
million registered users and generating over 100
million messages per day 2. As almost all mi-
croblog services, Sina Weibo allows users to com-
ment on a published post3, which forms a natural
one-round conversation. Due to the great abundance
of those (post, response) pairs, it provides an ideal
data source and test bed for one-round conversation.
We will make this dataset publicly available in the
near future.
</bodyText>
<footnote confidence="0.991954333333333">
2http://en.wikipedia.org/wiki/Sina_Weibo
3Actually it also allows users to comment on other users’
comments, but we will not consider that in the dataset.
</footnote>
<sectionHeader confidence="0.493677" genericHeader="introduction">
2 The Dialogues on Sina Weibo
</sectionHeader>
<bodyText confidence="0.9996064">
Sina Weibo is a Twitter-like microblog service, on
which a user can publish short messages (will be re-
ferred to as post in the remainder of the paper) visi-
ble to public or a group specified by the user. Simi-
lar to Twitter, Sina Weibo has the word limit of 140
Chinese characters. Other users can comment on a
published post, with the same length limit, as shown
in the real example given in Figure 6 (in Chinese).
Those comments will be referred to as responses in
the remainder of the paper.
</bodyText>
<figureCaption confidence="0.997394">
Figure 1: An example of Sina Weibo post and the com-
ments it received.
</figureCaption>
<bodyText confidence="0.99969495">
We argue that the (post, response) pairs on Sina
Weibo provide rather valuable resource for studying
one round dialogue between users. The comments
to a post can be of rather flexible forms and diverse
topics, as illustrated in the example in Table 1. With
a post stating the user’s status (traveling to Hawaii),
the comments can be of quite different styles and
contents, but apparently all appropriate.
In many cases, the (post, response) pair is self-
contained, which means one does not need any back-
ground and contextual information to get the main
point of the conversation (Examples of that include
the responses from B, D, G and H). In some cas-
es, one may need extra knowledge to understand the
conversation. For example, the response from user
E will be fairly elusive if taken out of the context
that A’s Hawaii trip is for an international confer-
ence and he is going to give a talk there. We argue
that the number of self-contained (post, response)
pairs is vast, and therefore the extracted (post, re-
</bodyText>
<page confidence="0.997886">
936
</page>
<table confidence="0.999292727272727">
Post
User A: The first day at Hawaii. Watching sunset at the balcony with a big glass of wine in hand.
Responses
User B: Enjoy it &amp; don’t forget to share your photos!
User C: Please take me with you next time!
User D: How long are you going to stay there?
User E: When will be your talk?
User F: Haha, I am doing the same thing right now. Which hotel are you staying in?
User G: Stop showing-off, buddy. We are still coding crazily right now in the lab.
User H: Lucky you! Our flight to Honolulu is delayed and I am stuck in the airport. Chewing French
fries in MacDonald’s right now.
</table>
<tableCaption confidence="0.9975995">
Table 1: A typical example of Sina Weibo post and the comments it received. The original text is in Chinese, and we
translated it into English for easy access of readers. We did the same thing for all the examples throughout this paper.
</tableCaption>
<bodyText confidence="0.986644">
sponse) pairs can serve as a rich resource for ex-
ploring rather sophisticated patterns and structures
in natural language conversation.
</bodyText>
<sectionHeader confidence="0.584023" genericHeader="method">
3 Content of the Dataset
</sectionHeader>
<figureCaption confidence="0.910343777777778">
The dataset consists of three parts, as illustrated in
Figure 2. Part 1 contains the original (post, re-
sponse) pairs, indicated by the dark-grey section in
Figure 2. Part 2, indicated by the light-gray section
in Figure 2, consists labeled (post, response) pairs
for some Weibo posts, including positive and nega-
tive examples. Part 3 collects all the responses, in-
cluding but not limited to the responses in Part 1 and
2. Some of the basic statistics are summarized in
</figureCaption>
<tableCaption confidence="0.929733">
Table 2.
</tableCaption>
<table confidence="0.990696">
# posts # responses vocab. # labeled pairs
4,6345 1,534,874 105,732 12,427
</table>
<tableCaption confidence="0.999619">
Table 2: Some statistics of the dataset
</tableCaption>
<bodyText confidence="0.991515258064516">
Original (Post, Response) Pairs This part of
dataset gives (post, response) pairs naturally pre-
sented in the microblog service. In other words,
we create a (post, response) pair there when the re-
sponse is actually given to the post in Sina Weibo.
The part of data is noisy since the responses given
to a Weibo post could still be inappropriate for d-
ifferent reasons, for example, they could be spams
or targeting some responses given earlier. We have
628,833 pairs.
Labeled Pairs This part of data contains the (post,
response) pairs that are labeled by human. Note that
1) the labeling is only on a small subset of posts,
and 2) for each selected post, the labeled responses
are not originally given to it. The labeling is done
in an active manner (see Section 4 for more detail-
s), so the obtained labels are much more informative
than the those on randomly selected pairs (over 98%
of which are negative). This part of data can be di-
rectly used for training and testing of retrieval-based
response models. We have labeled 422 posts and for
each of them, about 30 candidate responses.
Responses This part of dataset contains only re-
sponses, but they are not necessarily for a certain
post. These extra responses are mainly filtered out
by our data cleaning strategy (see Section 4.2) for
original (post, response) pairs, including those from
filtered-out Weibo posts and those addressing oth-
er responses. Nevertheless, those responses are still
valid candidate for responses. We have about 1.5
million responses in the dataset.
</bodyText>
<subsectionHeader confidence="0.9996075">
3.1 Using the Dataset for Retrieval-based
Response Models
</subsectionHeader>
<bodyText confidence="0.974251444444444">
Our data can be used for training and testing of
retrieval-based response model, or just as a bank of
responses. More specifically, it can be used in at
least the following three ways.
Training Low-level Matching Features The
rather abundant original (post, response) pairs pro-
vide rather rich supervision signal for learning dif-
ferent matching patterns between a post and a re-
sponse. These matching patterns could be of dif-
</bodyText>
<page confidence="0.993484">
937
</page>
<figureCaption confidence="0.999819">
Figure 2: Content of the dataset.
</figureCaption>
<bodyText confidence="0.990662488372093">
ferent levels. For example, one may discover from
the data that when the word “Hawaii” occurs in the
post, the response are more likely to contain word-
s like “trip”, “flight”, or “Honolulu”. On a slight-
ly more abstract level, one may learn that when an
entity name is mentioned in the post, it tends to be
mentioned again in the response. More complicated
matching pattern could also be learned. For exam-
ple, the response to a post asking “how to” is statisti-
cally longer than average responses. As a particular
case, Ritter et al. (2011) applied translation model
(Brown et al., 1993) on similar parallel data extract-
ed from Twitter in order to extract the word-to-word
correlation. Please note that with more sophisticat-
ed natural language processing, we can go beyond
bag-of-words for more complicated correspondence
between post and response.
Training Automatic Response Models Although
the original (post, response) pairs are rather abun-
dant, they are not enough for discriminative training
and testing of retrieval models, for the following rea-
sons. In the labeled pairs, both positive and negative
ones are ranked high by some baseline models, and
hence more difficult to tell apart. This supervision
will naturally tune the model parameters to find the
real good responses from the seemingly good ones.
Please note that without the labeled negative pairs,
we need to generate negative pairs with randomly
chosen responses, which in most of the cases are too
easy to differentiate by the ranking model and can-
not fully tune the model parameters. This intuition
has been empirically verified by our experiments.
Testing Automatic Response Models In testing a
retrieval-based system, although we can simply use
the original responses associated with the query post
as positive and treat all the others as negative, this
strategy suffers from the problem of spurious neg-
ative examples. In other words, with a reasonably
good model, the retrieved responses are often good
even if they are not the original ones, which brings
significant bias to the evaluation. With the labeled
pairs, this problem can be solved if we limit the test-
ing only in the small pool of labeled responses.
</bodyText>
<subsectionHeader confidence="0.999925">
3.2 Using the Dataset for Other Purposes
</subsectionHeader>
<bodyText confidence="0.999989916666667">
Our dataset can also be used for other researches re-
lated to short-text conversations, namely anaphora
resolution, sentiment analysis, and speech act anal-
ysis, based on the large collection of original (post,
response) pairs. For example, to determine the sen-
timent of a response, one needs to consider both
the original post as well as the observed interaction
between the two. In Figure 3, if we want to un-
derstand user’s sentiment towards the “invited talk”
mentioned in the post, the two responses should be
taken as positive, although the sentiment in the mere
responses is either negative or neutral.
</bodyText>
<sectionHeader confidence="0.790141" genericHeader="method">
4 Creation of the Dataset
</sectionHeader>
<bodyText confidence="0.999966714285714">
The (post, comment) pairs are sampled from the
Sina Weibo posts published by users in a loosely
connected community and the comments they re-
ceived (may not be from this community). This
community is mainly posed of professors, re-
searchers, and students of natural language process-
ing (NLP) and related areas in China, and the users
</bodyText>
<page confidence="0.993483">
938
</page>
<figureCaption confidence="0.997476666666667">
Figure 3: An example (original Chinese and the English
translation) on the difficulty of sentiment analysis on re-
sponses.
</figureCaption>
<bodyText confidence="0.996768">
commonly followed them.
The creation process of the dataset, as illustrated
in Figure 4, consists of three consecutive steps: 1)
crawling the community of users, 2) crawling their
Weibo posts and their responses, 3) cleaning the da-
ta, with more details described in the remainder of
this section.
</bodyText>
<subsectionHeader confidence="0.999497">
4.1 Sampling Strategy
</subsectionHeader>
<bodyText confidence="0.98405375">
We take the following sampling strategy for collect-
ing the (post, response) pairs to make the topic rel-
atively focused. We first locate 3,200 users from a
loosely connected community of Natural Language
Processing (NLP) and Machine Learning (ML) in
China. This is done through crawling followees4 of
ten manually selected seed users who are NLP re-
searchers active on Sina Weibo (with no less than 2
posts per day on average) and popular enough (with
no less than 100 followers).
We crawl the posts and the responses they re-
ceived (not necessarily from the crawled communi-
ty) for two months (from April 5th, 2013, to June
5th, 2013). The topics are relatively limited due to
our choice of the users, with the most saliently ones
being:
</bodyText>
<listItem confidence="0.984796727272727">
• Research: discussion on research ideas, paper-
s, books, tutorials, conferences, and researchers
in NLP and machine learning, etc;
• General Arts and Science: mathematics,
physics, biology, music, painting, etc;
4When user A follows user B, A is called B’s follower, and
B is called A’s followee.
• IT Technology: Mobile phones, IT companies,
jobs opportunities, etc;
• Life: traveling (both touring or conference trip-
s), food, photography, etc.
</listItem>
<subsectionHeader confidence="0.986578">
4.2 Processing, Filtering, and Data Cleaning
</subsectionHeader>
<bodyText confidence="0.871259">
On the crawled posts and responses, we first perform
a four-step filtering on the post and responses
• We first remove the Weibo posts and their re-
sponses if the length of post is less than 10 Chi-
nese characters or the length of the response is
less than 5 characters. The reason for that is
two-fold: 1) if the text is too short, it can bare-
ly contain information that can be reliably cap-
tured, e.g. the following example
P: Three down, two to go.
and 2) some of the posts or responses are too
general to be interesting for other cases, e.g. the
response in the example below,
</bodyText>
<table confidence="0.549342">
P: Nice restaurant. I’d strong recommend it.
Everything here is good except the long
waiting line
R: wow.
</table>
<listItem confidence="0.960840384615385">
• In the remained posts, we only keep the first
100 responses in the original (post, response)
pairs, since we observe that after the first 100
responses there will be a non-negligible propor-
tion of responses addressing things other than
the original Weibo post (e.g., the responses giv-
en earlier). We however will still keep the re-
sponses in the bank of responses.
• The last step is to filter out the potential adver-
tisements. We will find the long responses that
have been posted more than twice on different
posts and scrub them out of both original (post,
response) pairs and the response repository.
</listItem>
<bodyText confidence="0.6951245">
For the remained posts and responses, we remove
the punctuation marks and emoticons, and use ICT-
CLAS (Zhang et al., 2003) for Chinese word seg-
mentation.
</bodyText>
<page confidence="0.995651">
939
</page>
<figureCaption confidence="0.999827">
Figure 4: Diagram of the process for creating the dataset.
</figureCaption>
<subsectionHeader confidence="0.99688">
4.3 Labeling
</subsectionHeader>
<bodyText confidence="0.984069131147541">
We employ a pooling strategy widely used in in-
formation retrieval for getting the instance to label
(Voorhees, 2002). More specifically, for a given
post, we use three baseline retrieval models to each
select 10 responses (see Section 5 for the descrip-
tion of the baselines), and merge them to form a
much reduced candidate set with size &lt; 30. Then
we label the reduced candidate set into “suitable”
and “unsuitable” categories. Basically we consider
a response suitable for a given post if we cannot tell
whether it is an original response. More specifically
the suitability of a response is judged based on the
following three criteria5:
Semantic Relevance: This requires the content of
the response to be semantically relevant to the post.
As shown in the example right below, the post P is
about soccer, and so is response R1 (hence seman-
tically relevant), whereas response R2 is about food
(hence semantically irrelevant).
P: There are always 8 English players in their
own penalty area. Unbelievable!
Haha, it is still 0:0, no goal so far.
The food in England is horrible.
Another important aspect of semantic relevance is
the entity association. This requires the entities in
the response to be correctly aligned with those in
the post. In other words, if the post is about entity
5Note that although our criteria in general favor short and
general answers like “Well said!” or “Nice”, most of these gen-
eral answers have already been filtered out due to their length
(see Section 4.2).
A, while the response is about entity B, they are very
likely to be mismatched. As shown in the following
example, where the original post is about Paris, and
the response R2 talks about London:
P: It is my last day in Paris. So hard to say
goodbye.
Enjoy your time in Paris.
Man, I wish I am in London right now.
This is however not absolute, since a response con-
taining a different entity could still be sound, as
demonstrated by the following two responses to the
post above
Enjoy your time in France.
The fall of London is nice too.
Logic Consistency: This requires the content of
the response to be logically consistent with the post.
For example, in the table right below, post P states
that the Huawei mobile phone “Honor” is already in
the market of mainland China. Response R1 talk-
s about a personal preference over the same phone
model (hence logically consistent), whereas R2 asks
the question the answer to which is already clear
from P (hence logically inconsistent).
P: HUAWEI’s mobile phone, Honor, sells
well in Chinese Mainland.
HUAWEI Honor is my favorite phone
When will HUAWEI Honor get to the
market in mainland China?
Speech Act Alignment: Another important factor
in determining the suitability of a response is the
</bodyText>
<page confidence="0.992989">
940
</page>
<bodyText confidence="0.999924">
speech act. For example, when a question is posed in
the Weibo post, a certain act (e.g., answering or for-
warding it) is expected. In the example below, post
P asks a special question about location. Response
R1 and R2 either forwards or answers the question,
whereas R3 is a negative sentence and therefore does
not align well in speech act.
</bodyText>
<table confidence="0.5347042">
P: Any one knows where KDD will be held the
year after next?
co-ask. Hopefully Europe
New York, as I heard
No, it is still in New York City
</table>
<sectionHeader confidence="0.996035" genericHeader="method">
5 Retrieval-based Response Model
</sectionHeader>
<bodyText confidence="0.99993225">
In a retrieval-based response model, for a given post
x we pick from the candidate set the response with
the highest ranking score, where the score is the en-
semble of several individual matching features
</bodyText>
<equation confidence="0.9949915">
score(x,y) = � wi4&apos;i(x, y). (1)
i∈Q
</equation>
<bodyText confidence="0.98003695">
with y stands for a candidate response.
We perform a two-stage retrieval to handle the s-
calability associated with the massive candidate set,
as illustrated in Figure 5. In Stage I, the system em-
ploys several fast baseline matching models to re-
trieve a number of candidate responses for the giv-
en post x, forming a much reduced candidate set
C(reduced) .In Stage II, the system uses a ranking
function with more and sophisticated features to fur-
ther evaluate all the responses in C(reduced), return-
ing a matching score for each response. Our re-
sponse model then decides whether to respond and
which candidate response to choose.
In Stage II, we use the linear score function de-
fined in Equation 1 with 15 features, trained with
RankSVM (Joachims, 2002). The training and test-
ing are both performed on the 422 labeled posts,
with about 12,000 labeled (post, response) pairs. We
use a 5-fold cross validation with a fixed penalty pa-
rameter for slack variable. 6
</bodyText>
<subsectionHeader confidence="0.970773">
5.1 Baseline Matching Models
</subsectionHeader>
<bodyText confidence="0.9994945">
We use the following matching models as the base-
line model for Stage I fast retrieval. Moreover, the
</bodyText>
<footnote confidence="0.956458">
6The performance is fairly insensitive to the choice of the
penalty, so we only report the result with a typical choice of it.
</footnote>
<bodyText confidence="0.955562636363636">
matching features used in the ranking function in
Stage II are generated, directly or indirectly, from
the those matching models:
POST-RESPONSE SEMANTIC MATCHING:
This particular matching function relies on a learned
mapping from the original sparse representation for
text to a low-dimensional but dense representation
for both Weibo posts and responses. The level of
matching score between a post and a response can
be measured as the inner product between their
images in the low-dimensional space
</bodyText>
<equation confidence="0.928843">
SemMatch(x, y) = x&gt;LXL&gt;Yy. (2)
</equation>
<bodyText confidence="0.999605">
where x and y are respectively the 1-in-N represen-
tations of x and y. This is to capture the seman-
tic matching between a Weibo post and a response,
which may not be well captured by a word-by-word
matching. More specifically, we find LX and LY
through a large margin variant of (Wu et al., 2013)
</bodyText>
<equation confidence="0.943896833333333">
�arg minLX ,LY �max(1 − x&gt;i LXL&gt; Yyi, 0)
i i
s.t. kLn,X k1 ≤ µ1, n = 1, 2, ··· , Nx
kLm,Yk1 ≤ µ1, m = 1, 2, ··· , Ny
kLn,X k2 = µ2, n = 1, 2, ··· , Nx
kLm,Yk2 = µ2 m = 1, 2, ··· , Ny.
</equation>
<bodyText confidence="0.999116230769231">
where i indices the original (post, response) pairs.
Our experiments (Section 6) indicate that this sim-
ple linear model can learn meaningful patterns, due
to the massive training set. For example, the im-
age of the word “Italy” in the post in the latent s-
pace matches well word “Sicily”, “Mediterranean
sea” and “travel”. Once the mapping LX and LY
are learned, the semantic matching score x&gt;LXL&gt;Yy
will be treated as a feature for modeling the overall
suitability of y as a response to post x.
POST-RESPONSE SIMILARITY: Here we use a
simple vector-space model for measuring the simi-
larity between a post and a response
</bodyText>
<equation confidence="0.6704305">
x&gt;y
simPR(x, y) =
</equation>
<bodyText confidence="0.993515">
Although it is not necessarily true that a good re-
sponse has many common words as the post, but this
measurement is often helpful in finding relevant re-
sponses. For example, when the post and response
</bodyText>
<equation confidence="0.9123325">
kxkkyk
. (3)
</equation>
<page confidence="0.991237">
941
</page>
<figureCaption confidence="0.999901">
Figure 5: Diagram of the retrieval-based automatic response system.
</figureCaption>
<bodyText confidence="0.985924727272727">
both have “National Palace Museum in Taipei”, it
is a strong signal that they are about similar topic-
s. Unlike the semantic matching feature, this simple
similarity requires no learning and works on infre-
quent words. Our empirical results show that it can
often capture the Post-Response relation failed with
semantic matching feature.
POST-POST SIMILARITY: The basic idea here is
to find posts similar to x and use their responses as
the candidates. Again we use the vector space model
for measuring the post-post similarity
</bodyText>
<equation confidence="0.999484">
T &apos;
simPP(x,x&apos;) = 11x��1x .(4)
</equation>
<bodyText confidence="0.9999738">
The intuition here is that if a post x&apos; is similar to x its
responses might be appropriate for x. It however of-
ten fails, especially when a response to x&apos; addresses
parts of x not contained by x, which fortunately can
be alleviated when combined with other measures.
</bodyText>
<subsectionHeader confidence="0.99993">
5.2 Learning to Rank with Labeled Data
</subsectionHeader>
<bodyText confidence="0.999710666666667">
With all the matching features, we can learn a rank-
ing model with the labeled (post, response) pairs,
e.g., through off-the-shelf ranking algorithms. From
the labeled data, we can extract triples (x, y+, y−)
to ensure that score(x, y+) &gt; score(x, y−). Appar-
ently y+ can be selected from labeled positive re-
sponse of x, while y− can be sampled either from
labeled negative negative or randomly selected ones.
Since the manually labeled negative instances are
top-ranked candidates according to some individual
retrieval model (see Section 5.1) and therefore gen-
erally yield slightly better results.
The matching features are mostly constructed by
combining the individual matching models, for ex-
ample the following two
</bodyText>
<listItem confidence="0.99916">
• 4)7(x, y): this feature measures the length of
the longest common string in the post and the
response;
• 4&apos;12(x, y): this feature considers both seman-
tic matching score between query post x and
candidate response y, as well as the similarity
between x and y’s original post x&apos;:
</listItem>
<equation confidence="0.89328">
012(x, y) = SemMatch(x, y)simPP (x, x&apos;).
</equation>
<bodyText confidence="0.999891666666667">
In addition to the matching features, we also have
simple features describing responses only, such as
the length of it.
</bodyText>
<sectionHeader confidence="0.994807" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999356333333333">
We perform experiments on the proposed dataset to
test our retrieval-based model as an algorithm for au-
tomatically generating response.
</bodyText>
<subsectionHeader confidence="0.999882">
6.1 Performance of Models
</subsectionHeader>
<bodyText confidence="0.827981625">
We evaluate the retrieved models based on the fol-
lowing two metrics:
MAP This one measures the mean average preci-
sion (MAP)(Manning et al., 2008) associated
with the ranked list on C(reduced)
� .
P@1 This one simply measures the precision of the
top one response in the ranked list:
</bodyText>
<equation confidence="0.45145">
P@1 = #good top-1 responses
#posts
</equation>
<page confidence="0.993383">
942
</page>
<bodyText confidence="0.99968">
We perform a 5-fold cross-validation on the 422 la-
beled posts, with the results reported in Table 1. As
it shows, the semantic matching helps slightly im-
prove the overall performance on P@1.
</bodyText>
<table confidence="0.9954346">
Model MAP P@1
P2R 0.565 0.489
P2R + P2P 0.621 0.567
P2R + MATCH 0.575 0.513
P2R + P2P + MATCH 0.621 0.574
</table>
<tableCaption confidence="0.999651">
Table 3: Comparison of different choices of features,
</tableCaption>
<bodyText confidence="0.995266461538462">
where P2R stands for the features based on post-response
similarity, P2P stands for the features based on post-post
similarity, and MATCH stands for the semantic match fea-
ture.
To mimic a more realistic scenario on automatic
response model on Sina Weibo, we allow the system
to choose which post to respond to. Here we simply
set the response algorithm to respond only when the
highest score of the candidate response passes a cer-
tain threshold. Our experiments show that when we
choose to respond only to 50% of the posts, the P@1
increases to 0.76, while if the system only respond
to 25% of the posts, P@1 keeps increasing to 81%.
</bodyText>
<subsectionHeader confidence="0.999702">
6.2 Case Study
</subsectionHeader>
<bodyText confidence="0.9991994">
Although our preliminary retrieval model does not
consider more complicated syntax, it is still able to
capture some useful coupling structure between the
appropriate (post, response) pairs, as well as the sim-
ilar (post, post) pairs.
</bodyText>
<figureCaption confidence="0.994965333333333">
Figure 6: An actual instance (the original Chinese text
and its English translation) of response returned by our
retrieval-based system.
</figureCaption>
<bodyText confidence="0.998837285714286">
Case study shows that our retrieval is fairly ef-
fective at capturing the semantic relevance (Section
6.2.1), but relative weak on modeling the logic con-
sistency (Section 6.2.2). Also it is clear that the se-
mantic matching feature (described in Section 5.1)
helps find matched responses that do not share any
words with the post (Section 6.2.3).
</bodyText>
<subsectionHeader confidence="0.856713">
6.2.1 On Semantic Relevance
</subsectionHeader>
<bodyText confidence="0.9908645">
The features employed in our retrieval model are
mostly vector-space based, which are fairly good at
capturing the semantic relevance, as illustrated by
Example 1 &amp; 2.
</bodyText>
<sectionHeader confidence="0.809689" genericHeader="method">
EXAMPLE 1:
</sectionHeader>
<bodyText confidence="0.965505">
P: It is a small town on an Spanish with 500
population, and guess what, they even
have a casino!
R: If you travel to Spain, you need to spend
some time there.
</bodyText>
<sectionHeader confidence="0.854081" genericHeader="method">
EXAMPLE 2:
</sectionHeader>
<bodyText confidence="0.999150538461539">
P: One quote from Benjamin Franklin: “We
are all born ignorant, but one must
work hard to remain stupid.”
R: Benjamin Franklin is a wise man, and
one of the founding fathers of USA.
However our retrieval model also makes bad
choice, especially when either the query post or the
response is long, as shown in Example 3. Here the
response is picked up because 1) the correspondence
between the word “IT” in the post and the word
“mobile phone” in the candidate, and 2) the Chinese
word for “lay off” in the post and the word for “out-
dated” in the response are the same.
</bodyText>
<sectionHeader confidence="0.804412" genericHeader="method">
EXAMPLE 3:
</sectionHeader>
<bodyText confidence="0.9958005">
P: As to the laying-off, I haven’t heard anything
about it. ”Elimination of the least competent”
is kind-off conventional in IT, but the ratio is
actually quite small.
R: Please don’t speak that way, otherwise you can
get outdated. Mobile phones are very expensive
when they were just out, but now they are fairly
cheap. Look forward, or you will be outdated.
The entity association is only partially addressed
with features like post-response cosine similarity,
treating entity name just as a word, which is appar-
ently not enough for preventing the following type
</bodyText>
<page confidence="0.996741">
943
</page>
<bodyText confidence="0.987784">
of mistakes (see Example 4 &amp; 5) when the post and
response match well on other parts
</bodyText>
<table confidence="0.904821181818182">
EXAMPLE 4:
P: Professor Wang will give a curse on natural
language processing, starting next semester.
R: Jealous.. I wish I can attend Prof. Li’s
course too some time in the future.
EXAMPLE 5:
P: The fine China from Exhibition at the National
Palace Museum in Taipei
R: This drawing looks so nice. National Palace
Museum in Taipei is full of national treasures
6.2.2 On Logic Consistency
</table>
<bodyText confidence="0.996198857142857">
Our current model does not explicitly maintain
the logic consistency between the response and the
post, since Logic consistency requires a deeper anal-
ysis of the text, and therefore hard to capture with
just a vector space model. Below are two examples
which are semantically relevant, and correct with re-
spect to speech act, but logically inappropriate.
</bodyText>
<sectionHeader confidence="0.811947" genericHeader="method">
EXAMPLE 1:
</sectionHeader>
<bodyText confidence="0.9952545">
P: I checked. Wang Fengyi is not my great grand-
father, although they’ve done similar deeds
and both were called “Wang the Well-doer”.
R: wow, Wang Fengyi is your great grand-father
</bodyText>
<sectionHeader confidence="0.893356" genericHeader="method">
EXAMPLE 2:
</sectionHeader>
<bodyText confidence="0.9995534">
P: We are looking for summer interns. We provide
books and lunch. If you are in Wu Han and
interested, drop us an email. Sorry we don’t
take any students outside Wu Han.
R: Are you looking for summer intern?
</bodyText>
<subsectionHeader confidence="0.643564">
6.2.3 The Effect of Semantic Matching
</subsectionHeader>
<bodyText confidence="0.989808">
The experiments also show that we may find inter-
esting and appropriate responses that have no com-
mon words as the post, as shown in the example be-
low. Our bi-linear semantic matching model how-
ever performs relatively poorly on long posts, where
the topics of the sentence cannot be well captured
by the sum of the latent vectors associated with each
word.
P: Eight England players stand in the penalty
area.
What a classic match
Haha, it is still 0:0, no goal so far
</bodyText>
<sectionHeader confidence="0.99658" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.9993222">
In this paper we propose a retrieval-based response
model for short-text based conversation, to leverage
the massive instances collected from social media.
For research in similar directions, we create a dataset
based on the posts and comments from Sina Weibo.
Our preliminary experiments show that our retrieval-
based response model, when combined with a large
candidate set, can achieve fairly good performance.
This dataset will be valuable for both training and
testing automatic response models for short texts.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999520066666667">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematic-
s of statistical machine translation: parameter estima-
tion. Comput. Linguist., 19(2).
Rollo Carpenter. 1997. Cleverbot.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’02, pages 133–
142, New York, NY, USA. ACM.
Anton Leuski and David R. Traum. 2011. Npceditor:
Creating virtual human dialogue using information re-
trieval techniques. AI Magazine, 32(2):42–56.
Diane Litman, Satinder Singh, Michael Kearns, and Mar-
ilyn Walker. 2000. Njfun: a reinforcement learning
spoken dialogue system. In Proceedings of the 2000
ANLP/NAACL Workshop on Conversational systems -
Volume 3, ANLP/NAACL-ConvSyst ’00, pages 17–
20, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual muse-
um guides. In Proceedings of the 13th Annual Meeting
</reference>
<page confidence="0.986788">
944
</page>
<reference confidence="0.999703703703704">
of the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ’12, pages 84–93.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 583–593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowl. Eng. Rev., pages
97–126.
Ellen M Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355–
370. Springer.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Comput. Speech Lang., 21(2):393–422.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning
bilinear model for matching queries and documents.
Journal of Machine Learning Research (2013 to ap-
pear).
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. SIGHAN ’03.
</reference>
<page confidence="0.998863">
945
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635355">
<title confidence="0.964557">Dataset for Research on Short-Text Conversation</title>
<author confidence="0.651623">of Sci</author>
<author confidence="0.651623">Tech of China</author>
<author confidence="0.651623">China Ark Lab</author>
<author confidence="0.651623">Huawei Technologies</author>
<author confidence="0.651623">Hong Kong</author>
<abstract confidence="0.999159">Natural language conversation is widely regarded as a highly difficult problem, which is usually attacked with either rule-based or learning-based models. In this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo (a popular Chinese microblog service), which will be soon released to public. This dataset provides rich collection of instances for the research on finding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. This dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11505" citStr="Brown et al., 1993" startWordPosition="1917" endWordPosition="1920">Figure 2: Content of the dataset. ferent levels. For example, one may discover from the data that when the word “Hawaii” occurs in the post, the response are more likely to contain words like “trip”, “flight”, or “Honolulu”. On a slightly more abstract level, one may learn that when an entity name is mentioned in the post, it tends to be mentioned again in the response. More complicated matching pattern could also be learned. For example, the response to a post asking “how to” is statistically longer than average responses. As a particular case, Ritter et al. (2011) applied translation model (Brown et al., 1993) on similar parallel data extracted from Twitter in order to extract the word-to-word correlation. Please note that with more sophisticated natural language processing, we can go beyond bag-of-words for more complicated correspondence between post and response. Training Automatic Response Models Although the original (post, response) pairs are rather abundant, they are not enough for discriminative training and testing of retrieval models, for the following reasons. In the labeled pairs, both positive and negative ones are ranked high by some baseline models, and hence more difficult to tell a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rollo Carpenter</author>
</authors>
<date>1997</date>
<publisher>Cleverbot.</publisher>
<contexts>
<context position="3074" citStr="Carpenter, 1997" startWordPosition="487" endWordPosition="488">omatic question answering system as in (Litman et al., 2000)). However it can shed important light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1, but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012). This category of approaches require relatively less data (e.g. reinforcement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using </context>
</contexts>
<marker>Carpenter, 1997</marker>
<rawString>Rollo Carpenter. 1997. Cleverbot.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina Jafarpour</author>
<author>Christopher J C Burges</author>
</authors>
<title>Filter, rank, and transfer the knowledge: Learning to chat.</title>
<date>2010</date>
<contexts>
<context position="3837" citStr="Jafarpour and Burges, 2010" startWordPosition="598" endWordPosition="602">ely less data (e.g. reinforcement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using an alternative approach, by leveraging the vast amount of training data available from the social media. Similar ideas have appeared in (Jafarpour and Burges, 2010; Leuski and Traum, 2011) as an initial step for training a chatbot. With the emergence of social media, especially microblogs such as Twitter, in the past decade, they have become an important form of communication for many people. As the result, it has collected conversation history with volume previously unthinkable, which brings opportunity for attacking the conversation problem from a whole new angle. More specifically, instead of generating a response to an utterance, we pick a massive suitable one from the candidate set. The hope is, with a reasonable retrieval model and a large enough </context>
</contexts>
<marker>Jafarpour, Burges, 2010</marker>
<rawString>Sina Jafarpour and Christopher J. C. Burges. 2010. Filter, rank, and transfer the knowledge: Learning to chat.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>133--142</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="21544" citStr="Joachims, 2002" startWordPosition="3622" endWordPosition="3623">set, as illustrated in Figure 5. In Stage I, the system employs several fast baseline matching models to retrieve a number of candidate responses for the given post x, forming a much reduced candidate set C(reduced) .In Stage II, the system uses a ranking function with more and sophisticated features to further evaluate all the responses in C(reduced), returning a matching score for each response. Our response model then decides whether to respond and which candidate response to choose. In Stage II, we use the linear score function defined in Equation 1 with 15 features, trained with RankSVM (Joachims, 2002). The training and testing are both performed on the 422 labeled posts, with about 12,000 labeled (post, response) pairs. We use a 5-fold cross validation with a fixed penalty parameter for slack variable. 6 5.1 Baseline Matching Models We use the following matching models as the baseline model for Stage I fast retrieval. Moreover, the 6The performance is fairly insensitive to the choice of the penalty, so we only report the result with a typical choice of it. matching features used in the ranking function in Stage II are generated, directly or indirectly, from the those matching models: POST-</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 133– 142, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>David R Traum</author>
</authors>
<title>Npceditor: Creating virtual human dialogue using information retrieval techniques.</title>
<date>2011</date>
<journal>AI Magazine,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="3862" citStr="Leuski and Traum, 2011" startWordPosition="603" endWordPosition="606">ement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using an alternative approach, by leveraging the vast amount of training data available from the social media. Similar ideas have appeared in (Jafarpour and Burges, 2010; Leuski and Traum, 2011) as an initial step for training a chatbot. With the emergence of social media, especially microblogs such as Twitter, in the past decade, they have become an important form of communication for many people. As the result, it has collected conversation history with volume previously unthinkable, which brings opportunity for attacking the conversation problem from a whole new angle. More specifically, instead of generating a response to an utterance, we pick a massive suitable one from the candidate set. The hope is, with a reasonable retrieval model and a large enough candidate set, the system</context>
</contexts>
<marker>Leuski, Traum, 2011</marker>
<rawString>Anton Leuski and David R. Traum. 2011. Npceditor: Creating virtual human dialogue using information retrieval techniques. AI Magazine, 32(2):42–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Satinder Singh</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Njfun: a reinforcement learning spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems -Volume 3, ANLP/NAACL-ConvSyst ’00,</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2519" citStr="Litman et al., 2000" startWordPosition="396" endWordPosition="399">effort in dialogue modeling in the following two aspects • we do not consider the context or history of conversations, and assume that the given short text is self-contained; • we only require the response to be natural, relevant, and human-like, and do not require it to contain particular opinion, content, or to be of particular style. This task is much simpler than modeling a complete dialogue session (e.g., as proposed in Turing test), and probably not enough for real conversation scenario which requires often several rounds of interactions (e.g., automatic question answering system as in (Litman et al., 2000)). However it can shed important light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1, but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 20</context>
</contexts>
<marker>Litman, Singh, Kearns, Walker, 2000</marker>
<rawString>Diane Litman, Satinder Singh, Michael Kearns, and Marilyn Walker. 2000. Njfun: a reinforcement learning spoken dialogue system. In Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems -Volume 3, ANLP/NAACL-ConvSyst ’00, pages 17– 20, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Kallirroi Georgila</author>
<author>Anton Leuski</author>
<author>David Traum</author>
</authors>
<title>Reinforcement learning of question-answering dialogue policies for virtual museum guides.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>84--93</pages>
<contexts>
<context position="3166" citStr="Misu et al., 2012" startWordPosition="501" endWordPosition="504">ant light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1, but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012). This category of approaches require relatively less data (e.g. reinforcement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using an alternative approach, by leveraging the vast amount of training data available from the s</context>
</contexts>
<marker>Misu, Georgila, Leuski, Traum, 2012</marker>
<rawString>Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-answering dialogue policies for virtual museum guides. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 84–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>William B Dolan</author>
</authors>
<title>Data-driven response generation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>583--593</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11458" citStr="Ritter et al. (2011)" startWordPosition="1910" endWordPosition="1913">nse. These matching patterns could be of dif937 Figure 2: Content of the dataset. ferent levels. For example, one may discover from the data that when the word “Hawaii” occurs in the post, the response are more likely to contain words like “trip”, “flight”, or “Honolulu”. On a slightly more abstract level, one may learn that when an entity name is mentioned in the post, it tends to be mentioned again in the response. More complicated matching pattern could also be learned. For example, the response to a post asking “how to” is statistically longer than average responses. As a particular case, Ritter et al. (2011) applied translation model (Brown et al., 1993) on similar parallel data extracted from Twitter in order to extract the word-to-word correlation. Please note that with more sophisticated natural language processing, we can go beyond bag-of-words for more complicated correspondence between post and response. Training Automatic Response Models Although the original (post, response) pairs are rather abundant, they are not enough for discriminative training and testing of retrieval models, for the following reasons. In the labeled pairs, both positive and negative ones are ranked high by some base</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2011</marker>
<rawString>Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 583–593, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Karl Weilhammer</author>
<author>Matt Stuttle</author>
<author>Steve Young</author>
</authors>
<title>A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies.</title>
<date>2006</date>
<journal>Knowl. Eng. Rev.,</journal>
<pages>97--126</pages>
<contexts>
<context position="3146" citStr="Schatzmann et al., 2006" startWordPosition="497" endWordPosition="500">owever it can shed important light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1, but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012). This category of approaches require relatively less data (e.g. reinforcement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using an alternative approach, by leveraging the vast amount of training data </context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. 2006. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. Knowl. Eng. Rev., pages 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The philosophy of information retrieval evaluation. In Evaluation of crosslanguage information retrieval systems,</title>
<date>2002</date>
<pages>355--370</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17416" citStr="Voorhees, 2002" startWordPosition="2906" endWordPosition="2907">ses in the bank of responses. • The last step is to filter out the potential advertisements. We will find the long responses that have been posted more than twice on different posts and scrub them out of both original (post, response) pairs and the response repository. For the remained posts and responses, we remove the punctuation marks and emoticons, and use ICTCLAS (Zhang et al., 2003) for Chinese word segmentation. 939 Figure 4: Diagram of the process for creating the dataset. 4.3 Labeling We employ a pooling strategy widely used in information retrieval for getting the instance to label (Voorhees, 2002). More specifically, for a given post, we use three baseline retrieval models to each select 10 responses (see Section 5 for the description of the baselines), and merge them to form a much reduced candidate set with size &lt; 30. Then we label the reduced candidate set into “suitable” and “unsuitable” categories. Basically we consider a response suitable for a given post if we cannot tell whether it is an original response. More specifically the suitability of a response is judged based on the following three criteria5: Semantic Relevance: This requires the content of the response to be semantic</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Ellen M Voorhees. 2002. The philosophy of information retrieval evaluation. In Evaluation of crosslanguage information retrieval systems, pages 355– 370. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Comput. Speech Lang.,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3121" citStr="Williams and Young, 2007" startWordPosition="493" endWordPosition="496"> (Litman et al., 2000)). However it can shed important light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1, but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012). This category of approaches require relatively less data (e.g. reinforcement learning based) for 1http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem using an alternative approach, by leveraging the vast</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Comput. Speech Lang., 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>Learning bilinear model for matching queries and documents.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research</journal>
<note>to appear).</note>
<contexts>
<context position="22836" citStr="Wu et al., 2013" startWordPosition="3838" endWordPosition="3841">arned mapping from the original sparse representation for text to a low-dimensional but dense representation for both Weibo posts and responses. The level of matching score between a post and a response can be measured as the inner product between their images in the low-dimensional space SemMatch(x, y) = x&gt;LXL&gt;Yy. (2) where x and y are respectively the 1-in-N representations of x and y. This is to capture the semantic matching between a Weibo post and a response, which may not be well captured by a word-by-word matching. More specifically, we find LX and LY through a large margin variant of (Wu et al., 2013) �arg minLX ,LY �max(1 − x&gt;i LXL&gt; Yyi, 0) i i s.t. kLn,X k1 ≤ µ1, n = 1, 2, ··· , Nx kLm,Yk1 ≤ µ1, m = 1, 2, ··· , Ny kLn,X k2 = µ2, n = 1, 2, ··· , Nx kLm,Yk2 = µ2 m = 1, 2, ··· , Ny. where i indices the original (post, response) pairs. Our experiments (Section 6) indicate that this simple linear model can learn meaningful patterns, due to the massive training set. For example, the image of the word “Italy” in the post in the latent space matches well word “Sicily”, “Mediterranean sea” and “travel”. Once the mapping LX and LY are learned, the semantic matching score x&gt;LXL&gt;Yy will be treated a</context>
</contexts>
<marker>Wu, Lu, Li, 2013</marker>
<rawString>Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning bilinear model for matching queries and documents. Journal of Machine Learning Research (2013 to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua-Ping Zhang</author>
<author>Hong-Kui Yu</author>
<author>De-Yi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Hhmm-based chinese lexical analyzer ictclas.</title>
<date>2003</date>
<journal>SIGHAN</journal>
<volume>03</volume>
<contexts>
<context position="17192" citStr="Zhang et al., 2003" startWordPosition="2867" endWordPosition="2870">we observe that after the first 100 responses there will be a non-negligible proportion of responses addressing things other than the original Weibo post (e.g., the responses given earlier). We however will still keep the responses in the bank of responses. • The last step is to filter out the potential advertisements. We will find the long responses that have been posted more than twice on different posts and scrub them out of both original (post, response) pairs and the response repository. For the remained posts and responses, we remove the punctuation marks and emoticons, and use ICTCLAS (Zhang et al., 2003) for Chinese word segmentation. 939 Figure 4: Diagram of the process for creating the dataset. 4.3 Labeling We employ a pooling strategy widely used in information retrieval for getting the instance to label (Voorhees, 2002). More specifically, for a given post, we use three baseline retrieval models to each select 10 responses (see Section 5 for the description of the baselines), and merge them to form a much reduced candidate set with size &lt; 30. Then we label the reduced candidate set into “suitable” and “unsuitable” categories. Basically we consider a response suitable for a given post if w</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun Liu. 2003. Hhmm-based chinese lexical analyzer ictclas. SIGHAN ’03.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
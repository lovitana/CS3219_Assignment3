<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.99409">
Interactive Machine Translation using Hierarchical Translation Models
</title>
<author confidence="0.965509">
Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, Jos´e-Miguel Benedi, Francisco Casacuberta
</author>
<affiliation confidence="0.587549">
D. de Sistemas Inform´aticos y Computaci´on
Universitat Polit`ecnica de Val`encia
Camino de Vera s/n, 46021 Valencia (Spain)
</affiliation>
<email confidence="0.995707">
{jegonzalez,dortiz,jbenedi,fcn}@dsic.upv.es
</email>
<sectionHeader confidence="0.997335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997975">
Current automatic machine translation sys-
tems are not able to generate error-free trans-
lations and human intervention is often re-
quired to correct their output. Alternatively,
an interactive framework that integrates the
human knowledge into the translation pro-
cess has been presented in previous works.
Here, we describe a new interactive ma-
chine translation approach that is able to work
with phrase-based and hierarchical translation
models, and integrates error-correction all in
a unified statistical framework. In our experi-
ments, our approach outperforms previous in-
teractive translation systems, and achieves es-
timated effort reductions of as much as 48%
relative over a traditional post-edition system.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985836734694">
Research in the field of machine translation (MT)
aims to develop computer systems which are able
to translate between languages automatically, with-
out human intervention. However, the quality of
the translations produced by any automatic MT sys-
tem still remain below than that of human transla-
tion. Typical solutions to reach human-level quality
require a subsequent manual post-editing process.
Such decoupled post-edition solution is rather inef-
ficient and tedious for the human translator. More-
over, it prevents the MT system from taking advan-
tage of the knowledge of the human translator and,
reciprocal, the human translator cannot take advan-
tage of the adapting ability of MT technology.
An alternative way to take advantage of the exist-
ing MT technology is to use them in collaboration
with human translators within a computer-assisted
translation (CAT) or interactive framework (Isabelle
and Church, 1998). The TransType and TransType2
projects (Foster et al., 1998; Langlais and Lapalme,
2002; Barrachina et al., 2009) entailed an interesting
focus shift in CAT technology by aiming interaction
directly at the production of the target text. These
research projects proposed to embed an MT system
within an interactive translation environment. This
way, the human translator can ensure a high-quality
output while the MT system ensures a significant
gain of productivity. Particularly interesting is the
interactive machine translation (IMT) approach pro-
posed in (Barrachina et al., 2009). In this scenario,
a statistical MT (SMT) system uses the source sen-
tence and a previously validated part (prefix1) of its
translation to propose a suitable continuation. Then
the user finds and corrects the next system error,
thereby providing a longer prefix which the system
uses to suggests a new, hopefully better continua-
tion. The reported results showed that IMT can save
a significant amount of human effort.
Barrachina et al,. (2009) provide a thorough de-
scription of the IMT approach and describe algo-
rithms for its practical implementation. Neverthe-
less, we identify two basic problems for which we
think there is room for improvement. The first prob-
lem arises when the system cannot generate the pre-
fix validated by the user. To solve this problem,
the authors simply provide an ad-hoc heuristic error-
correction technique. The second problem is how
the system deals with word reordering. Particularly,
the models used by the system were either mono-
</bodyText>
<footnote confidence="0.9013215">
1We use the terms prefix and suffix to denote any sub-string
at the beginning and end respectively of a string of characters
(including spaces and punctuation). These terms do not imply
any morphological significance as they usually do in linguistics.
</footnote>
<page confidence="0.9384">
244
</page>
<note confidence="0.7540575">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 244–254,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.994881416666667">
tonic by nature or non-monotonic but heuristically
defined (not estimated from training data).
We work on the foundations of Barrachina et
al., (2009) and provide formal solutions to these
two challenges. On the one hand, we adopt the
statistical formalization of the IMT framework de-
scribed in (Ortiz-Martinez, 2011), which includes
a stochastic error-correction model in its formaliza-
tion to address prefix coverage problems. Moreover,
we refine this formalization proposing an alternative
error-correction formalization for the IMT frame-
work (Section 2). Additionally, we also propose a
specific error-correction model based on a statisti-
cal interpretation of the Levenshtein distance (Lev-
enshtein, 1966). These formalizations provide a
unified statistical framework for the IMT model in
comparison to the ad-hoc heuristic error-correction
methods previously used.
In order to address the problem of properly deal
with reordering in IMT, we introduce the use of hi-
erarchical MT models (Chiang, 2005; Zollmann and
Venugopal, 2006). These methods provide a natural
approach to handle long range dependencies and al-
low the incorporation of reordering information into
a consistent statistical framework. Here, we also de-
scribe how state-of-the-art hierarchical MT models
can be extended to handle IMT (Sections 3 and 4).
We evaluate the proposed IMT approach on two
different translation task. The comparative results
against the IMT approach described by Barrachina
et al., (2009) and a conventional post-edition ap-
proach show that our IMT formalization for hier-
archical SMT models indeed outperform other ap-
proaches (Sections 5 and 6). Moreover, it leads to
large reductions in the human effort required to gen-
erate error-free translations.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="method">
2 Statistical Framework
</sectionHeader>
<subsectionHeader confidence="0.996582">
2.1 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.99992925">
Assuming that we are given a sentence s in a source
language, the translation problem can be stated as
finding its translation t in a target language of max-
imum probability (Brown et al., 1993):
</bodyText>
<equation confidence="0.996838">
Pr(t  |s) (1)
Pr(t) · Pr(s  |t) (2)
</equation>
<note confidence="0.6506925">
source (s): Para ver la lista de recursos
desired translation (t): To view a listing of resources
</note>
<figure confidence="0.727869">
To view the resources list
p To view
k
ts
p To view a listing
k o
f resources
END p To view a listing of resources
</figure>
<figureCaption confidence="0.981270222222222">
Figure 1: IMT session to translate a Spanish sentence into
English. The desired translation is the translation the hu-
man user wants to obtain. At IT-0, the system suggests
a translation (ts). At IT-1, the user moves the mouse to
accept the first eight characters “To view ” and presses the
a key (k), then the system suggests completing the sen-
tence with “list of resources” (a new ts). Iterations 2 and
3 are similar. In the final iteration, the user accepts the
current translation.
</figureCaption>
<bodyText confidence="0.987387555555556">
The terms in the latter equation are the lan-
guage model probability Pr(t) that represents the
well-formedness of t (n-gram models are usu-
ally adopted), and the (inverted) translation model
Pr(s  |t) that represents the relationship between the
source sentence and its translation.
In practice all of these models (and possibly oth-
ers) are often combined into a log-linear model for
Pr(t  |s) (Och and Ney, 2002):
</bodyText>
<equation confidence="0.999886">
t� = arg max
t � N
n=1 λn · log(fn(t, s)) (3)
</equation>
<bodyText confidence="0.99994025">
where fn(t, s) can be any model that represents an
important feature for the translation, N is the num-
ber of models (or features), and λn are the weights
of the log-linear combination.
</bodyText>
<subsectionHeader confidence="0.999453">
2.2 Statistical Interactive Machine Translation
</subsectionHeader>
<bodyText confidence="0.99997">
Unfortunately, current MT technology is still far
from perfect. This implies that, in order to achieve
good translations, manual post-editing is needed.
An alternative to this decoupled approach (first
MT, then manual correction) is given by the IMT
</bodyText>
<equation confidence="0.90964125">
k
ts
IT-2
p To view a list
i
ng resources
t = arg max
t
= arg max
t
IT-0 p
ts
</equation>
<figure confidence="0.9290578">
IT-1
a
list of resources
IT-3
ts
</figure>
<page confidence="0.997674">
245
</page>
<bodyText confidence="0.999904481481482">
paradigm (Barrachina et al., 2009). Under this
paradigm, translation is considered as an iterative
left-to-right process where the human and the com-
puter collaborate to generate the final translation.
Figure 1 shows an example of the IMT approach.
There, a source Spanish sentence s =”Para ver la
lista de recursos” is to be translated into a target En-
glish sentence t. Initially, with no user feedback, the
system suggests a complete translation ts =”To view
the resources list”. From this translation, the user
marks a prefix p =”To view” as correct and begins
to type the rest of the target sentence. Depending on
the system or the user’s preferences, the user might
type the full next word, or only some letters of it (in
our example, the user types the single next charac-
ter “a”). Then, the MT system suggests a new suffix
ts =“list of resources” that completes the validated
prefix and the input the user has just typed (p =”To
view a”). The interaction continues with a new pre-
fix validation followed, if necessary, by new input
from the user, and so on, until the user considers the
translation to be complete and satisfactory.
The crucial step of the process is the production
of the suffix. Again decision theory tells us to max-
imize the probability of the suffix given the avail-
able information. Formally, the best suffix of a given
length will be:
</bodyText>
<equation confidence="0.998056142857143">
is = arg max Pr(ts  |s, p) (4)
t3
which can be straightforwardly rewritten as:
�ts = arg max Pr(p, ts  |s) (5)
t3
= arg max Pr(p, ts) · Pr(s  |p, ts) (6)
t3
</equation>
<bodyText confidence="0.999961461538462">
Note that, since pts = t, this equation is very
similar to Equation (2). The main difference is that
now the search process is restricted to those target
sentences t that contains p as prefix. This implies
that we can use the same MT models (including
the log-linear approach) if the search procedures are
adequately modified (Och et al., 2003). Finally, it
should be noted that the statistical models are usu-
ally defined at word level, while the IMT process
described in this section works at character level. To
deal with this problem, during the search process it
is necessary to verify the compatibility between t
and p at character level.
</bodyText>
<subsectionHeader confidence="0.998423">
2.3 IMT with Stochastic Error-Correction
</subsectionHeader>
<bodyText confidence="0.999801428571429">
A common problem in IMT arises when the user sets
a prefix which cannot be explained by the statistical
models. To solve this problem, IMT systems typi-
cally include ad-hoc error-correction techniques to
guarantee that the suffixes can be generated (Bar-
rachina et al., 2009). As an alternative to this heuris-
tic approach, Ortiz-Martinez (2011) proposed a for-
malization of the IMT framework that does include
stochastic error-correction models in its statistical
formalization. The starting point of this alternative
IMT formalization accounts for the problem of find-
ing the translation t that, at the same time, better
explains the source sentence s and the prefix given
by the user p:
</bodyText>
<equation confidence="0.9983875">
Pr(t  |s, p) (7)
Pr(t) · Pr(s, p  |t) (8)
</equation>
<bodyText confidence="0.9945835">
The following naive Bayes’ assumption is now
made: the source sentence s and the user prefix p are
statistically independent variables given the transla-
tion t, obtaining:
</bodyText>
<equation confidence="0.997991">
t = arg max
t
</equation>
<bodyText confidence="0.993700882352941">
where Pr(t) can be approximated with a language
model, Pr(s  |t) can be approximated with a trans-
lation model, and Pr(p  |t) can be approximated
by an error correction model that measures the com-
patibility between the user-defined prefix p and the
hypothesized translation t.
Note that the translation result, t, given by Equa-
tion (9) may not contain p as prefix because every
translation is compatible with p with a certain prob-
ability. Thus, despite being close, Equation (9) is not
equivalent to the IMT formalization in Equation (6).
To solve this problem, we define an alignment,
a, between the user-defined prefix p and the hy-
pothesized translation t, so that the unaligned words
of t, in an appropriate order, constitute the suffix
searched in IMT. This allows us to rewrite the error
correction probability as follows:
</bodyText>
<equation confidence="0.935573">
1:
Pr(p  |t) = Pr(p, a  |t) (10)
a
</equation>
<bodyText confidence="0.999914666666667">
To simplify things, we assume that p is mono-
tonically aligned to t, leaving the potential word-
reordering to the language and translation models.
</bodyText>
<equation confidence="0.9998864">
t� = arg max
t
= arg max
t
Pr(t) · Pr(s  |t) · Pr(p  |t) (9)
</equation>
<page confidence="0.990109">
246
</page>
<bodyText confidence="0.9999782">
Under this assumption, a determines an alignment
for t, such that t = tpts, where tp is fully-aligned to
p and ts remains unaligned. Taking all these things
into consideration, and following a maximum ap-
proximation, we finally arrive to the expression:
</bodyText>
<equation confidence="0.563514">
(�t, �a) =arg max Pr(t)·Pr(s  |t)·Pr(p,a  |t) (11)
t,a
</equation>
<bodyText confidence="0.9998322">
where the suffix required in IMT is obtained as the
portion of t� that is not aligned with the user prefix.
In practice, we combine the models in Equa-
tion (11) in a log-linear fashion as it is typically done
in SMT (see Equation (3)).
</bodyText>
<subsectionHeader confidence="0.9780785">
2.4 Alternative Formalization for IMT with
Stochastic Error-Correction
</subsectionHeader>
<bodyText confidence="0.999828">
Alternatively to Equation (11), we can operate from
Equation (9) and reach a different formalization for
IMT with error-correction. We can re-write the first
and last terms of Equation (9) as:
</bodyText>
<equation confidence="0.995476">
Pr(t) · Pr(p  |t) = Pr(p) · Pr(t  |p) (12)
</equation>
<bodyText confidence="0.9973615">
As in the previous section, we introduce an align-
ment variable, a, between t and p, giving:
</bodyText>
<equation confidence="0.935190666666667">
1:
Pr(t  |p) = Pr(t, a  |p) (13)
a
</equation>
<bodyText confidence="0.99999485">
where Pr(p) and Pr(a|p) have been dropped down
because the former does not participate in the maxi-
mization and the latter is assumed uniform.
The terms in this equation can be interpreted sim-
ilarly as those in Equation (9): Pr(s  |t) is the trans-
lation model, Pr(tp  |p, a) is the error-correction
probability that measures the compatibility between
the prefix tp of the hypothesized translation and the
user-defined prefix p, and Pr(ts  |p, a) is the lan-
guage model for the corresponding suffix ts condi-
tioned by the user-defined prefix. Again, in the ex-
periments we combine the different models in a log-
linear fashion.
The main difference between the two alternative
IMT formalization (Equations (11) and (17)) is that
in the latter the suffix to be returned is conditioned
by the user-validated prefix p. Thus, in the fol-
lowing we will refer to Equation (11) as indepen-
dent suffix formalization while we will denote Equa-
tion (17) by conditioned suffix formalization.
</bodyText>
<sectionHeader confidence="0.994187" genericHeader="method">
3 Statistical Models
</sectionHeader>
<bodyText confidence="0.9999836">
We now present the statistical models used to esti-
mate the probability distributions described in the
previous section. Section 3.1 describes the error-
correction model, while Section 3.2 describes the
models for the conditional translation probability.
</bodyText>
<equation confidence="0.7086475">
1: = Pr(a  |p) · Pr(t  |p, a) (14) 3.1 Statistical Error-Correction Model
a
</equation>
<bodyText confidence="0.9999328">
If we consider monotonic alignments, a defines
again an alignment between a prefix of the system
translation (tp) and the user prefix, producing the
suffix required in IMT (ts) as the unaligned part.
Thus, we can re-write Pr(t  |p, a) as:
</bodyText>
<equation confidence="0.995727">
Pr(t  |p, a) = Pr(tp, ts  |p, a) (15)
Pz Pr(tp  |p, a) · Pr(ts  |p, a) (16)
</equation>
<bodyText confidence="0.999969666666667">
where Equation (16) has been obtained following a
naive Bayes’ decomposition.
Combining equations (12), (14), and (16) into
Equation (9), and following a maximum approxima-
tion for the summation of the alignment variable a,
we arrive to the following expression:
</bodyText>
<equation confidence="0.699395">
(�t, �a) =arg max Pr(s|t)·Pr(tp |p, a)·Pr(ts |p, a) (17)
t,a
</equation>
<bodyText confidence="0.999990666666667">
Following the vast majority of IMT systems de-
scribed in the literature, we implement an error-
correction model based on the concept of edit dis-
tance (Levenshtein, 1966). Typically, IMT systems
use non-probabilistic error correction models. The
first stochastic error correction model for IMT was
proposed in (Ortiz-Mart´ınez, 2011) and it is based
on probabilistic finite state machines. Here, we pro-
pose a simpler approach which can be seen as a
particular case of the previous one. Specifically,
the proposed approach models the edit distance as a
Bernoulli process where each character of the candi-
date string has a probability pe of being erroneous.
Under this interpretation, the number of characters
that need to be edited E in a sentence of length n
is a random variable that follows a binomial distri-
bution, E — B(n, pe), with parameters n and pe.
The probability of performing exactly k edits in a
</bodyText>
<page confidence="0.983993">
247
</page>
<bodyText confidence="0.9700615">
sentence of n characters is given by the following
probability mass function:
</bodyText>
<equation confidence="0.9953825">
n!
f(k;n,pe) = k!(n − k)! pke(1 − pe)n−k (18)
Pr(s  |t) ≈ λ1 ·|t |+ λ2 · K+
K
E [λ3 · lo9(P(�sk  |tk)) + λ4 · d(j)] (20)
k�1
</equation>
<bodyText confidence="0.999962727272727">
Note that this error-correction model penalizes
equally all edit operations. Alternatively, we can
model the distance with a multinomial distribution
and assign different probabilities to different types
of edit operations. Nevertheless, we adhere to the
binomial approximation due to its simplicity.
Finally, we compute the error-correction proba-
bility between two strings from the total number of
edits required to transform the candidate translation
into the reference translation. Specifically, we define
the error-correction distribution in Equation (11) as:
</bodyText>
<equation confidence="0.997479">
Pr(p, a  |t) ≈ |p|! pe (1 _ pe) |P |−k (19)
!(|p |− k)
</equation>
<bodyText confidence="0.999963444444444">
where k = Lev(p, ta) is the character-level Lev-
enshtein distance between the user defined prefix p
and the prefix ta of the hypothesized translation t
defined by alignment a. The error-correction prob-
ability Pr(tp  |p, a) in Equation (17) is computed
analogously.
The probability of edition pe is the single free pa-
rameter of this formulation. We will use a separate
development corpus to find an adequate value for it.
</bodyText>
<subsectionHeader confidence="0.999274">
3.2 Statistical Machine Translation Models
</subsectionHeader>
<bodyText confidence="0.99993175">
Next sections briefly describe the statistical transla-
tion models used to estimate the conditional proba-
bility distribution Pr(s  |t). A detailed description
of each model can be found in the provided citations.
</bodyText>
<subsectionHeader confidence="0.649909">
3.2.1 Phrase-Based Translation Models
</subsectionHeader>
<bodyText confidence="0.999949666666667">
Phrase-based translation models (Koehn et al.,
2003) are an instance of the noisy-channel approach
in Equation (2). The translation of a source sentence
s is obtained through a generative process composed
of three steps: first, the s is divided into K segments
(phrases), next, each source phrase, s, is translated
into a target phrase t, and finally the target phrases
are reordered to compose the final translation.
The usual phrase-based implementation of the
translation probability takes a log-linear form:
where P(s  |t) is the translation probability between
source phrase s� and target phrase t, and d(j) is a
function (distortion model) that returns the score of
translating the k-th source phrase given that it is sep-
arated j words from the (k−1)-th phrase. Weights λ1
and λ2 play a special role since they are used to con-
trol the number of words and the number of phrases
of the target sentence to be generated, respectively.
</bodyText>
<subsectionHeader confidence="0.575272">
3.2.2 Hierarchical Translation Models
</subsectionHeader>
<bodyText confidence="0.99573565">
Phrase-based models have shown a very strong
performance when translating between languages
that have similar word orders. However, they are not
able to adequately capture the complex relationships
that exist between the word orders of languages of
different families such as English and Chinese. Hi-
erarchical translation models provide a solution to
this challenge by allowing gaps in the phrases (Chi-
ang, 2005):
yu X1 you X2 → have X2 with X1
where subscripts denote placeholders for sub-
phrases. Since these rules generalize over possi-
ble phrases, they act as discontinuous phrase pairs
and may also act as phrase-reordering rules. Hence,
they are not only considerably more powerful than
conventional phrase pairs, but they also integrate re-
ordering information into a consistent framework.
These hierarchical phrase pairs are formalized as
rewrite rules of a synchronous context-free grammar
(CFG) (Aho and Ullman, 1969):
</bodyText>
<equation confidence="0.993386">
X →&lt; -y, α, ∼&gt; (21)
</equation>
<bodyText confidence="0.997916714285714">
where X is a non-terminal, -y and α are both strings
of terminals (words) and non-terminals , and ∼ is
a one-to-one correspondence between non-terminal
occurrences in -y and α. Given the example above,
-y ≡“yu X1 you X2”, α ≡“have X2 with X1”, and ∼
is indicated by the subscript numbers.
Additionally, two glue rules are also defined:
</bodyText>
<equation confidence="0.498321">
S →&lt;S1X2 , S1X2&gt; S →&lt;X1 , X1&gt;
</equation>
<page confidence="0.981831">
248
</page>
<bodyText confidence="0.9999395">
These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a phrase-based model.
The typical implementation of the hierarchical
translation model also takes the form of a log-linear
model. Let sδ and tδ be the source and target strings
generated by a derivation δ of the grammar. Then,
the conditional translation probability is given by:
</bodyText>
<equation confidence="0.984191333333333">
Pr(sS  |tS) � A1 · |tS |+ A2 · |δ |+ A3 · #g(δ)+
11 [A4 · w(r)] (22)
,ES
</equation>
<bodyText confidence="0.999952285714286">
where |δ |denotes the total number of rules used
in δ, #g(δ) returns the number of applications of
the glue rules, r E δ are the rules in δ, and w(r)
is the weight of rule r. Weights A1 and A2 have
a similar interpretation as for phrase-based models,
they respectively give some control over the total
number of words and rules that conform the trans-
lation. Additionally, A3 controls the model’s prefer-
ence for hierarchical phrases over serial combination
of phrases. Note that no distortion model is included
in the previous equation. Here, reordering is defined
at rule level by the one-to-one non-terminal corre-
spondence. In other words, reordering is a property
inherent to each rule and it is the individual score of
each rule what defines, at each step of the derivation,
the importance of reordering.
It should be noted that the IMT formalizations
presented in Section 2 can be applied to other hier-
archical or syntax-based SMT models such as those
described in (Zollmann and Venugopal, 2006; Shen
et al., 2010).
</bodyText>
<sectionHeader confidence="0.996315" genericHeader="method">
4 Search
</sectionHeader>
<bodyText confidence="0.9999603">
In offline MT, the generation of the best trans-
lation for a given source sentence is carried out
by incrementally generating the target sentence2.
This process fits nicely into a dynamic program-
ming (DP) (Bellman, 1957) framework, as hypothe-
ses which are indistinguishable by the models can
be recombined. Since the DP search space grows
exponentially with the size of the input, standard DP
search is prohibitive, and search algorithms usually
resort to a beam-search heuristic (Jelinek, 1997).
</bodyText>
<footnote confidence="0.565751">
2Phrase-based systems follow a left-to-right generation or-
der while hierarchical systems rely on a CYK-like order.
</footnote>
<figureCaption confidence="0.9669">
Figure 2: Example of a hypergraph encoding two differ-
ent translations (one solid and one dotted) for the Spanish
sentence “Vi a un hombre con un telescopio”.
</figureCaption>
<bodyText confidence="0.999948866666667">
Due to the demanding temporal constraints inher-
ent to any interactive environment, performing a full
search each time the user validates a new prefix is
unfeasible. The usual approach is to rely on a certain
representation of the search space that includes the
most probable translations of the source sentence.
The computational cost of this approach is much
lower, as the whole search for the translation must
be carried out only once, and the generated represen-
tation can be reused for further completion requests.
Next, we introduce hypergraphs, the formalism
chosen to represent the search space of both phrase-
based and hierarchical systems (Section 4.1). Then,
we describe the algorithms implemented to search
for suffix completions in them (Section 4.2).
</bodyText>
<subsectionHeader confidence="0.975116">
4.1 Hypergraphs
</subsectionHeader>
<bodyText confidence="0.999968882352941">
A hypergraph is a generalization of the concept
of graph where the edges (now called hyperedges)
may connect several nodes (hypernodes) at the same
time. Formally, a hypergraph is a weighted acyclic
graph represented by a pair &lt; V, £ &gt;, where V is a
set of hypernodes and £ is a set of hyperedges. Each
hyperedge e E £ connects a head hypernode and a
set of tail hypernodes. The number of tail nodes is
called the arity of the hyperedge and the arity of a
hypergraph is the maximum arity of its hyperedges.
We can use hypergraphs to represent the deriva-
tions for a given CFG. Each hypernode represents
a partial translation generated during the decoding
process. Each ingoing hyperedge represents the rule
with which the corresponding non-terminal was sub-
stituted. Moreover, hypergraphs can represent a
whole set of possible translations. An example is
</bodyText>
<figure confidence="0.995539375">
I saw a man with a telescope
I saw with a telescope a man
6
4
5
1
2 3
I saw a man with a telescope
</figure>
<page confidence="0.996696">
249
</page>
<bodyText confidence="0.999990714285714">
shown in Figure 2. Two alternative translations are
constructed from the leave nodes (1, 2 and 3) up to
the root node (6) of the hypergraph. Additionally,
hypernodes and hyperedges may be shared among
different derivations if they represent the same in-
formation. Thus, we can achieve a compact repre-
sentation of the translation space that allows us to
derive efficient search algorithms.
Note that word-graphs (Ueffing et al., 2002),
which are used to represent the search space for
phrase-based models, are a special case of hyper-
graphs in which the maximum arity is one. Thus,
hypergraphs allow us to represent both phrase-based
and hierarchical systems in a unified framework.
</bodyText>
<subsectionHeader confidence="0.968151">
4.2 Suffix Search on Hypergraphs
</subsectionHeader>
<bodyText confidence="0.99998025">
Now, we describe a unified search process to obtain
the suffix is that completes a prefix p given by the
user according to the two IMT formulations (Equa-
tion (11) and Equation (17)) described in Section 2.
Given an hypergraph, certain hypernodes define a
possible solution to the maximization defined in the
two IMT formulations. Specifically, only those hy-
pernodes that generate a prefix of a potential trans-
lation are to be taken into account3. The prob-
ability of the solution defined by each hypernode
has two components, namely the probability of the
SMT model (given by the language and translation
models) and the probability of the error-correction
model. On the one hand, the SMT model probabil-
ity is given by the translation of maximum probabil-
ity through the hypernode. On the other hand, the
error-correction probability is computed between p
and the partial translation of maximum probability
actually covered by the hypernode. Among all the
solutions defined by the hypernodes, we finally se-
lect that of maximum probability.
Once the best-scoring hypernode is identified, the
rest of the translation not covered by it is returned as
the suffix completion required in IMT.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="method">
5 Experimental Framework
</sectionHeader>
<bodyText confidence="0.9990725">
The models and search procedure introduced in the
previous sections were assessed through a series of
</bodyText>
<footnote confidence="0.960022">
3For example, in Figure 2 the hypernodes that generate pre-
fixes are those labeled with numbers 1 (“I saw”), 4 (“I saw with
a telescope) and 6 (“I saw a man with a telescope” and “I saw
with a telescope a man”).
</footnote>
<table confidence="0.9880957">
EU (Es/En)
Train Development Test
Sentences 214K 400 800
Token 5.9M / 5.2M 12K / 10K 23K / 20K
Vocabulary 97K / 84K 3K / 3K 5K / 4K
TED (Zh/En)
Train Development Test
Sentences 107K 934 1664
Token 2M / 2M 22K / 20K 33K / 32K
Vocabulary 42K / 52K 4K / 3K 4K / 4K
</table>
<tableCaption confidence="0.966053">
Table 1: Main figures of the processed EU and TED cor-
pora. K and M stand for thousands and millions of ele-
ments respectively.
</tableCaption>
<bodyText confidence="0.993326">
IMT experiments with different corpora. These cor-
pora, the experimental methodology, and the evalu-
ation measures are presented in this section.
</bodyText>
<subsectionHeader confidence="0.914139">
5.1 EU and TED corpora
</subsectionHeader>
<bodyText confidence="0.999886307692308">
We tested the proposed methods in two different
translation tasks each one involving a different lan-
guage pair: Spanish-to-English (Es–En) for the EU
(Bulletin of the European Union) task, and Chinese-
to-English (Zh–En) for the TED (TED4 talks) task.
The EU corpora were extracted from the Bul-
letin of the European Union, which exists in all of-
ficial languages of the European Union and is pub-
licly available on the Internet. Particularly, the cho-
sen Es–En corpus was part of the evaluation of the
TransType2 project (Barrachina et al., 2009). The
TED talks is a collection of recordings of public
speeches covering a variety of topics, and for which
high quality transcriptions and translations into sev-
eral languages are available. The Zh–En corpus
used in the experiments was part of the MT track
in the 2011 evaluation campaign of the workshop on
spoken language translation (Federico et al., 2011).
Specifically, we used the dev2010 partition for de-
velopment and the test2010 partition for test.
We process the Spanish and English parts of the
EU corpus to separate words and punctuation marks
keeping sentences truecase. Regarding the TED cor-
pus, we tokenized and lowercased the English part
(Chinese has no case information), and split Chi-
nese sentences into words with the Stanford word
</bodyText>
<footnote confidence="0.994659">
4www.ted.com
</footnote>
<page confidence="0.980828">
250
</page>
<bodyText confidence="0.4982525">
segmenter (Tseng et al., 2005). Table 1 shows the
main figures of the processed EU and TED corpora.
</bodyText>
<subsectionHeader confidence="0.915191">
5.2 Model Estimation and User Simulation
</subsectionHeader>
<bodyText confidence="0.999939575757576">
Key-stroke and mouse-action ratio (KSMR):
number of key strokes plus mouse movements per-
formed by the user, divided by the total number of
characters in the reference.
We used the standard configuration of the Moses
toolkit (Koehn et al., 2007) to estimate one phrase-
based and one hierarchical model for each cor-
pus; log-linear weights were optimized by minimum
error-rate training (Och, 2003) with the development
partitions. Then, the optimized models were used to
generate the word-graphs and hypergraphs with the
translations of the development and test partitions.
A direct evaluation of the proposed IMT proce-
dures involving human users would have been slow
and expensive. Thus, following previous works in
the literature (Barrachina et al., 2009; Gonz´alez-
Rubio et al., 2010), we used the references in the
corpora to simulate the translations that a human
user would want to obtain. Each time the system
suggested a new translation, it was compared to
the reference and the longest common prefix (LCP)
was obtained. Then, the first non-matching charac-
ter was replaced by the corresponding character in
the reference and a new system suggestion was pro-
duced. This process is iterated until a full match with
the reference was obtained.
Finally, we used this user simulation to optimize
the value for the probability of edition pe in the
error-correction model (Section 3.1), and for the log-
linear weights in the proposed IMT formulations. In
this case, these values were chosen so that they min-
imize the estimated user effort required to interac-
tively translate the development partitions.
</bodyText>
<subsectionHeader confidence="0.997006">
5.3 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999496235294118">
Different measures have been adopted to evaluate
the proposed IMT approach. On the one hand, dif-
ferent IMT systems can be compared according to
the effort needed by a human user to generate the de-
sired translations. This effort is usually estimated as
the number of actions performed by the user while
interacting with the system. In the user simulation
described above these actions are: looking for the
next error and moving the mouse pointer to that po-
sition (LCP computation), and correcting errors with
some key strokes. Hence, we implement the follow-
ing IMT effort measure (Barrachina et al., 2009):
On the other hand, we also want to compare the
proposed IMT approach against a conventional CAT
approach without interactivity, such as a decoupled
post-edition system. For such systems, character-
level user effort is usually measured by the Charac-
ter Error Rate (CER). However, it is clear that CER
is at a disadvantage due to the auto-completion ap-
proach of IMT. To perform a fairer comparison be-
tween post-edition and IMT, we implement a post-
editing system with autocompletion. Here, when
the user enters a character to correct some incor-
rect word, the system automatically completes the
word with the most probable word in the task vo-
cabulary. To evaluate the effort of a user using such
a system, we implement the following measure pro-
posed in (Romero et al., 2010):
Post-editing key stroke ratio (PKSR): using
a post-edition system with word-autocompleting,
number of user key strokes divided by the total num-
ber of reference characters.
The counterpart of PKSR in an IMT scenario
is (Barrachina et al., 2009):
Key-stroke ratio (KSR): number of key strokes,
divided by the number of reference characters.
PKSR and KSR are fairly comparable and the rel-
ative difference between them gives us a good es-
timate of the reduction in human effort that can be
achieved by using IMT instead of a conventional
post-edition system.
We also evaluate the quality of the automatic
translations generated by the MT models with the
widespread BLEU score (Papineni et al., 2002).
Finally, we provide both confidence intervals for
the results and statistical significance of the ob-
served differences in performance. Confidence in-
tervals were computed by pair-wise re-sampling as
in (Zhang and Vogel, 2004) while statistical signifi-
cance was computed using the Tukey’s HSD (honest
significance difference) test (Hsu, 1996).
</bodyText>
<page confidence="0.994811">
251
</page>
<table confidence="0.9297165">
EU TED
WG HG WG HG
1-best BLEU [%] 45.0 45.1 11.0 11.2
1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0
</table>
<tableCaption confidence="0.936519">
Table 2: BLEU score of the word-graphs (WG) and hy-
pergraphs (HG) used to implement the IMT procedures.
</tableCaption>
<table confidence="0.9995375">
IMT EU TED
Setup PB HT PB HT
ISF 27.4±.5 26.5±.5* 53.0±.4 52.3±.4*
CSF 26.6±.5* 25.1±.5* 52.2±.4* 50.8±.4*
</table>
<tableCaption confidence="0.998805">
Table 3: IMT results (KSMR [%]) for the EU and
</tableCaption>
<bodyText confidence="0.860788375">
TED tasks using the independent suffix formalization
(ISF) and the conditioned suffix formalization (CSF). PB
stands for phrase-based model and HT stands for hierar-
chical translation model. For each task, the best result
is displayed boldface, an asterisk * denotes a statistically
significant better result (99% confidence) with respect to
ISF with PB, and a star * denotes a statistically signifi-
cant difference with respect to all the other systems.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99997564">
We start by reporting conventional MT quality re-
sults to test if the generated word-graphs and hyper-
graphs encode translations of similar quality. Ta-
ble 2 displays the quality (BLEU (Papineni et al.,
2002)) of the automatic translations generated for
the test partitions. The lower 1-best BLEU results
obtained for TED show that this is a much more dif-
ficult task than EU. Additionally, the similar aver-
age BLEU results obtained for the 1000-best transla-
tions indicate that word-graphs and hypergraphs en-
code translations of similar quality. Thus, the IMT
systems that use these word-graphs and hypergraphs
can be compared in a fair way.
Then, we evaluated different setups of the pro-
posed IMT approach. Table 3 displays the IMT re-
sults obtained for the EU and TED tasks. We report
KSMR (as a percentage) for the independent suffix
formalization (ISF) and the conditioned suffix for-
malization (CSF) using both phase-based (PB) and
hierarchical (HT) translation models. The KSMR
result of ISF using a phrase-based model can be con-
sidered our baseline since this setup is comparable
to that used in (Barrachina et al., 2009). Results for
HT consistently outperformed the corresponding re-
sults for PB. Similarly, results for CSF were con-
</bodyText>
<table confidence="0.7655555">
EU TED
PE IMT PE IMT
PKSR [%] KSR [%] PKSR [%] KSR [%]
27.1 14.1 (48%) 40.8 29.7 (27.2%)
</table>
<tableCaption confidence="0.9030345">
Table 4: Estimation of the effort required to translate
the test partition of the EU and TED tasks using post-
editing with word-completion (PE) and IMT under the
independent suffix formalization (IMT). We used hierar-
chical MT in both approaches. In parenthesis we display
the estimated effort reduction of IMT with respect to PE.
</tableCaption>
<bodyText confidence="0.999988962962963">
sistently better than those for ISF. More specifically,
no statistically significant difference were found be-
tween ISF with HT and CSF with PB but both sta-
tistically outperformed the baseline (ISF with PB).
Finally, CSF with HT statistically outperformed the
other three configurations reducing KSMR by —2.2
points with respect to the baseline. We hypothe-
size that the better results of HT can be explained
by its more efficient representation of word reorder-
ing. Regarding the CSF, its better results are due to
the better suffixes that can be obtained by taking into
account the actual prefix validated by the user.
Finally, we compared the estimated human effort
required to translate the test partitions of the EU and
TED corpora with the best IMT configuration (inde-
pendent suffix formalization with hierarchical trans-
lation model) and a conventional post-editing (PE)
CAT system with word-completion. That is, when
the user corrects a character, the PE system auto-
matically proposes a different word that begins with
the given word prefix but, obviously, the rest of the
sentence is not changed. According to the results,
the estimated human effort to generate the error-free
translations was significantly reduced with respect
to using the conventional PE approach. IMT can
save about 48% of the overall eastimated effort for
the EU task and about 27% for the TED task.
</bodyText>
<sectionHeader confidence="0.996051" genericHeader="conclusions">
7 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999900714285714">
We have proposed a new IMT approach that uses hi-
erarchical SMT models as its underlying translation
technology. This approach is based on a statistical
formalization previously described in the literature
that includes stochastic error correction. Addition-
ally, we have proposed a refined formalization that
improves the quality of the IMT suffixes by taking
</bodyText>
<page confidence="0.988196">
252
</page>
<bodyText confidence="0.9999781">
into account the prefix validated by the user. More-
over, since word-graphs constitute a particular case
of hypergraphs, we are able to manage both phrase-
based and hierarchical translation models in a uni-
fied IMT framework.
Simulated results on two different translation
tasks showed that hierarchical translation models
outperform phrase-based models in our IMT frame-
work. Additionally, the proposed alternative IMT
formalization also allows to improve the results of
the IMT formalization previously described in the
literature. Finally, the proposed IMT system with
hierarchical SMT models largely reduces the esti-
mated user effort required to generate correct trans-
lations in comparison to that of a conventional post-
edition system. We look forward to corroborating
these result in test with human translators.
There are many ways to build on the work de-
scribed here. In the near future, we plan to explore
the following research directions:
</bodyText>
<listItem confidence="0.977932">
• Alternative IMT scenarios where the user is not
bounded to correct translation errors in a left-
to-right fashion. In such scenarios, the user will
be allowed to correct errors at any position in
the translation while the IMT system will be
required to derive translations compatible with
these isolated corrections.
• Adaptive translation engines that take advan-
</listItem>
<bodyText confidence="0.962359444444444">
tage of the user’s corrections to improve its sta-
tistical models. As the translator works and
corrects the proposed translations, the transla-
tion engine will be able to make better predic-
tions. One of the first works on this topic was
proposed in (Nepveu et al., 2004). More re-
cently, Ortiz-Martinez et al. (2010) described a
set of techniques to obtain an incrementally up-
dateable IMT system, solving technical prob-
lems encountered in previous works.
• More sophisticated measures to estimate the
human effort. Specifically, measures that esti-
mate the cognitive load involve in reading, un-
derstanding and detecting an error in a trans-
lation (Foster et al., 2002), in contrast KSMR
simply considers a constant cost. This will lead
to a more accurate estimation of the improve-
ments that may be expected by a human user.
</bodyText>
<sectionHeader confidence="0.997711" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999931833333333">
Work supported by the European Union 7th Frame-
work Program (FP7/2007-2013) under the Cas-
MaCat project (grans agreement no 287576), by
Spanish MICINN under grant TIN2012-31723, and
by the Generalitat Valenciana under grant ALMPR
(Prometeo/2009/014).
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995528878048781">
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syn-
tax directed translations and the pushdown assembler.
Journal of Computer and Systems Science, 3(1):37–
56, February.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi, An-
tonio Lagarda, Hermann Ney, Jes´us Tom´as, Enrique
Vidal, and Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Computa-
tional Linguistics, 35:3–28, March.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ, USA, 1 edi-
tion.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263–311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270.
M. Federico, L. Bentivogli, M. Paul, and S. St¨uker. 2011.
Overview of the iwslt 2011 evaluation campaign. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 11–20.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1998. Target-text mediated interactive machine trans-
lation. Machine Translation, 12(1/2):175–194, Jan-
uary.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 conference on Empirical
methods in natural language processing - Volume 10,
pages 148–155.
Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, and Fran-
cisco Casacuberta. 2010. Balancing user effort and
translation error in interactive machine translation via
confidence measures. In Proceedings of the ACL 2010
Conference Short Papers, pages 173–177.
Jason Hsu. 1996. Multiple Comparisons: Theory and
Methods. Chapman and Hall/CRC.
</reference>
<page confidence="0.981798">
253
</page>
<reference confidence="0.999844835164835">
Pierre Isabelle and Ken Church. 1998. Special issue on:
New tools for human translators, volume 12. Kluwer
Academic Publishers, January.
Frederick Jelinek. 1997. Statistical methods for speech
recognition. MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, demonstration session, June.
Philippe Langlais and Guy Lapalme. 2002. TransType:
development-evaluation cycles to boost translator’s
productivity. Machine Translation, 17(2):77–98,
September.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710, February.
Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proceedings of the conference on Empirical Methods
on Natural Language Processing, pages 190–197.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In Proceedings of the European
chapter of the Association for Computational Linguis-
tics, pages 387–393.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 160–167. Association for
Computational Linguistics.
Daniel Ortiz-Mart´ınez, Ismael Garc´ıa-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In Proceedings
of the 2010 Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 546–554.
Daniel Ortiz-Mart´ınez. 2011. Advances in Fully-
Automatic and Interactive Phrase-Based Statistical
Machine Translation. Ph.D. thesis, Universitat
Polit`ecnica de Val`encia. Advisors: Ismael Garc´ıa
Varea and Francisco Casacuberta.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318. Associa-
tion for Computational Linguistics.
Veronica Romero, Alejandro H. Toselli, and Enrique Vi-
dal. 2010. Character-level interaction in computer-
assisted transcription of text images. In Proceedings
of the 12th International Conference on Frontiers in
Handwriting Recognition, pages 539–544.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649–671, Decem-
ber.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine trans-
lation. In Proceedings of the conference on Empirical
Methods in Natural Language Processing, pages 156–
163.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of The international Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138–141.
</reference>
<page confidence="0.998642">
254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875907">
<title confidence="0.999931">Interactive Machine Translation using Hierarchical Translation Models</title>
<author confidence="0.997269">Jes´us Gonz´alez-Rubio</author>
<author confidence="0.997269">Daniel Ortiz-Martinez</author>
<author confidence="0.997269">Jos´e-Miguel Benedi</author>
<author confidence="0.997269">Francisco</author>
<affiliation confidence="0.9791605">D. de Sistemas Inform´aticos y Universitat Polit`ecnica de</affiliation>
<address confidence="0.915633">Camino de Vera s/n, 46021 Valencia</address>
<abstract confidence="0.999667352941176">Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves eseffort reductions of as much as relative over a traditional post-edition system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and Systems Science,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>56</pages>
<contexts>
<context position="19195" citStr="Aho and Ullman, 1969" startWordPosition="3135" endWordPosition="3138">chical translation models provide a solution to this challenge by allowing gaps in the phrases (Chiang, 2005): yu X1 you X2 → have X2 with X1 where subscripts denote placeholders for subphrases. Since these rules generalize over possible phrases, they act as discontinuous phrase pairs and may also act as phrase-reordering rules. Hence, they are not only considerably more powerful than conventional phrase pairs, but they also integrate reordering information into a consistent framework. These hierarchical phrase pairs are formalized as rewrite rules of a synchronous context-free grammar (CFG) (Aho and Ullman, 1969): X →&lt; -y, α, ∼&gt; (21) where X is a non-terminal, -y and α are both strings of terminals (words) and non-terminals , and ∼ is a one-to-one correspondence between non-terminal occurrences in -y and α. Given the example above, -y ≡“yu X1 you X2”, α ≡“have X2 with X1”, and ∼ is indicated by the subscript numbers. Additionally, two glue rules are also defined: S →&lt;S1X2 , S1X2&gt; S →&lt;X1 , X1&gt; 248 These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a phrase-based model. The typical implementation of the hierarchical transla</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Computer and Systems Science, 3(1):37– 56, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Barrachina</author>
<author>Oliver Bender</author>
<author>Francisco Casacuberta</author>
<author>Jorge Civera</author>
<author>Elsa Cubel</author>
<author>Shahram Khadivi</author>
<author>Antonio Lagarda</author>
<author>Hermann Ney</author>
<author>Jes´us Tom´as</author>
<author>Enrique Vidal</author>
<author>Juan-Miguel Vilar</author>
</authors>
<title>Statistical approaches to computer-assisted translation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<pages>35--3</pages>
<marker>Barrachina, Bender, Casacuberta, Civera, Cubel, Khadivi, Lagarda, Ney, Tom´as, Vidal, Vilar, 2009</marker>
<rawString>Sergio Barrachina, Oliver Bender, Francisco Casacuberta, Jorge Civera, Elsa Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jes´us Tom´as, Enrique Vidal, and Juan-Miguel Vilar. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics, 35:3–28, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<volume>1</volume>
<pages>edition.</pages>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ, USA,</location>
<contexts>
<context position="21319" citStr="Bellman, 1957" startWordPosition="3510" endWordPosition="3511">words, reordering is a property inherent to each rule and it is the individual score of each rule what defines, at each step of the derivation, the importance of reordering. It should be noted that the IMT formalizations presented in Section 2 can be applied to other hierarchical or syntax-based SMT models such as those described in (Zollmann and Venugopal, 2006; Shen et al., 2010). 4 Search In offline MT, the generation of the best translation for a given source sentence is carried out by incrementally generating the target sentence2. This process fits nicely into a dynamic programming (DP) (Bellman, 1957) framework, as hypotheses which are indistinguishable by the models can be recombined. Since the DP search space grows exponentially with the size of the input, standard DP search is prohibitive, and search algorithms usually resort to a beam-search heuristic (Jelinek, 1997). 2Phrase-based systems follow a left-to-right generation order while hierarchical systems rely on a CYK-like order. Figure 2: Example of a hypergraph encoding two different translations (one solid and one dotted) for the Spanish sentence “Vi a un hombre con un telescopio”. Due to the demanding temporal constraints inherent</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>Richard Bellman. 1957. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5974" citStr="Brown et al., 1993" startWordPosition="887" endWordPosition="890">ranslation task. The comparative results against the IMT approach described by Barrachina et al., (2009) and a conventional post-edition approach show that our IMT formalization for hierarchical SMT models indeed outperform other approaches (Sections 5 and 6). Moreover, it leads to large reductions in the human effort required to generate error-free translations. 2 Statistical Framework 2.1 Statistical Machine Translation Assuming that we are given a sentence s in a source language, the translation problem can be stated as finding its translation t in a target language of maximum probability (Brown et al., 1993): Pr(t |s) (1) Pr(t) · Pr(s |t) (2) source (s): Para ver la lista de recursos desired translation (t): To view a listing of resources To view the resources list p To view k ts p To view a listing k o f resources END p To view a listing of resources Figure 1: IMT session to translate a Spanish sentence into English. The desired translation is the translation the human user wants to obtain. At IT-0, the system suggests a translation (ts). At IT-1, the user moves the mouse to accept the first eight characters “To view ” and presses the a key (k), then the system suggests completing the sentence w</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4979" citStr="Chiang, 2005" startWordPosition="734" endWordPosition="735">n to address prefix coverage problems. Moreover, we refine this formalization proposing an alternative error-correction formalization for the IMT framework (Section 2). Additionally, we also propose a specific error-correction model based on a statistical interpretation of the Levenshtein distance (Levenshtein, 1966). These formalizations provide a unified statistical framework for the IMT model in comparison to the ad-hoc heuristic error-correction methods previously used. In order to address the problem of properly deal with reordering in IMT, we introduce the use of hierarchical MT models (Chiang, 2005; Zollmann and Venugopal, 2006). These methods provide a natural approach to handle long range dependencies and allow the incorporation of reordering information into a consistent statistical framework. Here, we also describe how state-of-the-art hierarchical MT models can be extended to handle IMT (Sections 3 and 4). We evaluate the proposed IMT approach on two different translation task. The comparative results against the IMT approach described by Barrachina et al., (2009) and a conventional post-edition approach show that our IMT formalization for hierarchical SMT models indeed outperform </context>
<context position="18683" citStr="Chiang, 2005" startWordPosition="3057" endWordPosition="3059">s λ1 and λ2 play a special role since they are used to control the number of words and the number of phrases of the target sentence to be generated, respectively. 3.2.2 Hierarchical Translation Models Phrase-based models have shown a very strong performance when translating between languages that have similar word orders. However, they are not able to adequately capture the complex relationships that exist between the word orders of languages of different families such as English and Chinese. Hierarchical translation models provide a solution to this challenge by allowing gaps in the phrases (Chiang, 2005): yu X1 you X2 → have X2 with X1 where subscripts denote placeholders for subphrases. Since these rules generalize over possible phrases, they act as discontinuous phrase pairs and may also act as phrase-reordering rules. Hence, they are not only considerably more powerful than conventional phrase pairs, but they also integrate reordering information into a consistent framework. These hierarchical phrase pairs are formalized as rewrite rules of a synchronous context-free grammar (CFG) (Aho and Ullman, 1969): X →&lt; -y, α, ∼&gt; (21) where X is a non-terminal, -y and α are both strings of terminals </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>L Bentivogli</author>
<author>M Paul</author>
<author>S St¨uker</author>
</authors>
<title>Overview of the iwslt 2011 evaluation campaign.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>11--20</pages>
<marker>Federico, Bentivogli, Paul, St¨uker, 2011</marker>
<rawString>M. Federico, L. Bentivogli, M. Paul, and S. St¨uker. 2011. Overview of the iwslt 2011 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<title>Target-text mediated interactive machine translation.</title>
<date>1998</date>
<journal>Machine Translation,</journal>
<pages>12--1</pages>
<contexts>
<context position="2039" citStr="Foster et al., 1998" startWordPosition="285" endWordPosition="288">a subsequent manual post-editing process. Such decoupled post-edition solution is rather inefficient and tedious for the human translator. Moreover, it prevents the MT system from taking advantage of the knowledge of the human translator and, reciprocal, the human translator cannot take advantage of the adapting ability of MT technology. An alternative way to take advantage of the existing MT technology is to use them in collaboration with human translators within a computer-assisted translation (CAT) or interactive framework (Isabelle and Church, 1998). The TransType and TransType2 projects (Foster et al., 1998; Langlais and Lapalme, 2002; Barrachina et al., 2009) entailed an interesting focus shift in CAT technology by aiming interaction directly at the production of the target text. These research projects proposed to embed an MT system within an interactive translation environment. This way, the human translator can ensure a high-quality output while the MT system ensures a significant gain of productivity. Particularly interesting is the interactive machine translation (IMT) approach proposed in (Barrachina et al., 2009). In this scenario, a statistical MT (SMT) system uses the source sentence a</context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1998</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1998. Target-text mediated interactive machine translation. Machine Translation, 12(1/2):175–194, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>User-friendly text prediction for translators.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 conference on Empirical methods in natural language processing - Volume</booktitle>
<volume>10</volume>
<pages>148--155</pages>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>George Foster, Philippe Langlais, and Guy Lapalme. 2002. User-friendly text prediction for translators. In Proceedings of the 2002 conference on Empirical methods in natural language processing - Volume 10, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Daniel Ortiz-Martinez</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Balancing user effort and translation error in interactive machine translation via confidence measures.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>173--177</pages>
<marker>Gonz´alez-Rubio, Ortiz-Martinez, Casacuberta, 2010</marker>
<rawString>Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, and Francisco Casacuberta. 2010. Balancing user effort and translation error in interactive machine translation via confidence measures. In Proceedings of the ACL 2010 Conference Short Papers, pages 173–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Hsu</author>
</authors>
<title>Multiple Comparisons: Theory and Methods. Chapman and Hall/CRC.</title>
<date>1996</date>
<contexts>
<context position="31880" citStr="Hsu, 1996" startWordPosition="5259" endWordPosition="5260">good estimate of the reduction in human effort that can be achieved by using IMT instead of a conventional post-edition system. We also evaluate the quality of the automatic translations generated by the MT models with the widespread BLEU score (Papineni et al., 2002). Finally, we provide both confidence intervals for the results and statistical significance of the observed differences in performance. Confidence intervals were computed by pair-wise re-sampling as in (Zhang and Vogel, 2004) while statistical significance was computed using the Tukey’s HSD (honest significance difference) test (Hsu, 1996). 251 EU TED WG HG WG HG 1-best BLEU [%] 45.0 45.1 11.0 11.2 1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0 Table 2: BLEU score of the word-graphs (WG) and hypergraphs (HG) used to implement the IMT procedures. IMT EU TED Setup PB HT PB HT ISF 27.4±.5 26.5±.5* 53.0±.4 52.3±.4* CSF 26.6±.5* 25.1±.5* 52.2±.4* 50.8±.4* Table 3: IMT results (KSMR [%]) for the EU and TED tasks using the independent suffix formalization (ISF) and the conditioned suffix formalization (CSF). PB stands for phrase-based model and HT stands for hierarchical translation model. For each task, the best result is displayed bold</context>
</contexts>
<marker>Hsu, 1996</marker>
<rawString>Jason Hsu. 1996. Multiple Comparisons: Theory and Methods. Chapman and Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Isabelle</author>
<author>Ken Church</author>
</authors>
<title>Special issue on: New tools for human translators, volume 12.</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="1979" citStr="Isabelle and Church, 1998" startWordPosition="276" endWordPosition="279">ranslation. Typical solutions to reach human-level quality require a subsequent manual post-editing process. Such decoupled post-edition solution is rather inefficient and tedious for the human translator. Moreover, it prevents the MT system from taking advantage of the knowledge of the human translator and, reciprocal, the human translator cannot take advantage of the adapting ability of MT technology. An alternative way to take advantage of the existing MT technology is to use them in collaboration with human translators within a computer-assisted translation (CAT) or interactive framework (Isabelle and Church, 1998). The TransType and TransType2 projects (Foster et al., 1998; Langlais and Lapalme, 2002; Barrachina et al., 2009) entailed an interesting focus shift in CAT technology by aiming interaction directly at the production of the target text. These research projects proposed to embed an MT system within an interactive translation environment. This way, the human translator can ensure a high-quality output while the MT system ensures a significant gain of productivity. Particularly interesting is the interactive machine translation (IMT) approach proposed in (Barrachina et al., 2009). In this scenar</context>
</contexts>
<marker>Isabelle, Church, 1998</marker>
<rawString>Pierre Isabelle and Ken Church. 1998. Special issue on: New tools for human translators, volume 12. Kluwer Academic Publishers, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21594" citStr="Jelinek, 1997" startWordPosition="3552" endWordPosition="3553">hical or syntax-based SMT models such as those described in (Zollmann and Venugopal, 2006; Shen et al., 2010). 4 Search In offline MT, the generation of the best translation for a given source sentence is carried out by incrementally generating the target sentence2. This process fits nicely into a dynamic programming (DP) (Bellman, 1957) framework, as hypotheses which are indistinguishable by the models can be recombined. Since the DP search space grows exponentially with the size of the input, standard DP search is prohibitive, and search algorithms usually resort to a beam-search heuristic (Jelinek, 1997). 2Phrase-based systems follow a left-to-right generation order while hierarchical systems rely on a CYK-like order. Figure 2: Example of a hypergraph encoding two different translations (one solid and one dotted) for the Spanish sentence “Vi a un hombre con un telescopio”. Due to the demanding temporal constraints inherent to any interactive environment, performing a full search each time the user validates a new prefix is unfeasible. The usual approach is to rely on a certain representation of the search space that includes the most probable translations of the source sentence. The computati</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical methods for speech recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<contexts>
<context position="17349" citStr="Koehn et al., 2003" startWordPosition="2841" endWordPosition="2844">tion t defined by alignment a. The error-correction probability Pr(tp |p, a) in Equation (17) is computed analogously. The probability of edition pe is the single free parameter of this formulation. We will use a separate development corpus to find an adequate value for it. 3.2 Statistical Machine Translation Models Next sections briefly describe the statistical translation models used to estimate the conditional probability distribution Pr(s |t). A detailed description of each model can be found in the provided citations. 3.2.1 Phrase-Based Translation Models Phrase-based translation models (Koehn et al., 2003) are an instance of the noisy-channel approach in Equation (2). The translation of a source sentence s is obtained through a generative process composed of three steps: first, the s is divided into K segments (phrases), next, each source phrase, s, is translated into a target phrase t, and finally the target phrases are reordered to compose the final translation. The usual phrase-based implementation of the translation probability takes a log-linear form: where P(s |t) is the translation probability between source phrase s� and target phrase t, and d(j) is a function (distortion model) that re</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics, demonstration session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="28090" citStr="Koehn et al., 2007" startWordPosition="4644" endWordPosition="4647">nd punctuation marks keeping sentences truecase. Regarding the TED corpus, we tokenized and lowercased the English part (Chinese has no case information), and split Chinese sentences into words with the Stanford word 4www.ted.com 250 segmenter (Tseng et al., 2005). Table 1 shows the main figures of the processed EU and TED corpora. 5.2 Model Estimation and User Simulation Key-stroke and mouse-action ratio (KSMR): number of key strokes plus mouse movements performed by the user, divided by the total number of characters in the reference. We used the standard configuration of the Moses toolkit (Koehn et al., 2007) to estimate one phrasebased and one hierarchical model for each corpus; log-linear weights were optimized by minimum error-rate training (Och, 2003) with the development partitions. Then, the optimized models were used to generate the word-graphs and hypergraphs with the translations of the development and test partitions. A direct evaluation of the proposed IMT procedures involving human users would have been slow and expensive. Thus, following previous works in the literature (Barrachina et al., 2009; Gonz´alezRubio et al., 2010), we used the references in the corpora to simulate the transl</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, demonstration session, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>TransType: development-evaluation cycles to boost translator’s productivity.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="2067" citStr="Langlais and Lapalme, 2002" startWordPosition="289" endWordPosition="292">ost-editing process. Such decoupled post-edition solution is rather inefficient and tedious for the human translator. Moreover, it prevents the MT system from taking advantage of the knowledge of the human translator and, reciprocal, the human translator cannot take advantage of the adapting ability of MT technology. An alternative way to take advantage of the existing MT technology is to use them in collaboration with human translators within a computer-assisted translation (CAT) or interactive framework (Isabelle and Church, 1998). The TransType and TransType2 projects (Foster et al., 1998; Langlais and Lapalme, 2002; Barrachina et al., 2009) entailed an interesting focus shift in CAT technology by aiming interaction directly at the production of the target text. These research projects proposed to embed an MT system within an interactive translation environment. This way, the human translator can ensure a high-quality output while the MT system ensures a significant gain of productivity. Particularly interesting is the interactive machine translation (IMT) approach proposed in (Barrachina et al., 2009). In this scenario, a statistical MT (SMT) system uses the source sentence and a previously validated pa</context>
</contexts>
<marker>Langlais, Lapalme, 2002</marker>
<rawString>Philippe Langlais and Guy Lapalme. 2002. TransType: development-evaluation cycles to boost translator’s productivity. Machine Translation, 17(2):77–98, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="4685" citStr="Levenshtein, 1966" startWordPosition="688" endWordPosition="690">ata). We work on the foundations of Barrachina et al., (2009) and provide formal solutions to these two challenges. On the one hand, we adopt the statistical formalization of the IMT framework described in (Ortiz-Martinez, 2011), which includes a stochastic error-correction model in its formalization to address prefix coverage problems. Moreover, we refine this formalization proposing an alternative error-correction formalization for the IMT framework (Section 2). Additionally, we also propose a specific error-correction model based on a statistical interpretation of the Levenshtein distance (Levenshtein, 1966). These formalizations provide a unified statistical framework for the IMT model in comparison to the ad-hoc heuristic error-correction methods previously used. In order to address the problem of properly deal with reordering in IMT, we introduce the use of hierarchical MT models (Chiang, 2005; Zollmann and Venugopal, 2006). These methods provide a natural approach to handle long range dependencies and allow the incorporation of reordering information into a consistent statistical framework. Here, we also describe how state-of-the-art hierarchical MT models can be extended to handle IMT (Secti</context>
<context position="15024" citStr="Levenshtein, 1966" startWordPosition="2465" endWordPosition="2466">e unaligned part. Thus, we can re-write Pr(t |p, a) as: Pr(t |p, a) = Pr(tp, ts |p, a) (15) Pz Pr(tp |p, a) · Pr(ts |p, a) (16) where Equation (16) has been obtained following a naive Bayes’ decomposition. Combining equations (12), (14), and (16) into Equation (9), and following a maximum approximation for the summation of the alignment variable a, we arrive to the following expression: (�t, �a) =arg max Pr(s|t)·Pr(tp |p, a)·Pr(ts |p, a) (17) t,a Following the vast majority of IMT systems described in the literature, we implement an errorcorrection model based on the concept of edit distance (Levenshtein, 1966). Typically, IMT systems use non-probabilistic error correction models. The first stochastic error correction model for IMT was proposed in (Ortiz-Mart´ınez, 2011) and it is based on probabilistic finite state machines. Here, we propose a simpler approach which can be seen as a particular case of the previous one. Specifically, the proposed approach models the edit distance as a Bernoulli process where each character of the candidate string has a probability pe of being erroneous. Under this interpretation, the number of characters that need to be edited E in a sentence of length n is a random</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>Guy Lapalme</author>
<author>Philippe Langlais</author>
<author>George Foster</author>
</authors>
<title>Adaptive language and translation models for interactive machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>190--197</pages>
<contexts>
<context position="37699" citStr="Nepveu et al., 2004" startWordPosition="6205" endWordPosition="6208">ve IMT scenarios where the user is not bounded to correct translation errors in a leftto-right fashion. In such scenarios, the user will be allowed to correct errors at any position in the translation while the IMT system will be required to derive translations compatible with these isolated corrections. • Adaptive translation engines that take advantage of the user’s corrections to improve its statistical models. As the translator works and corrects the proposed translations, the translation engine will be able to make better predictions. One of the first works on this topic was proposed in (Nepveu et al., 2004). More recently, Ortiz-Martinez et al. (2010) described a set of techniques to obtain an incrementally updateable IMT system, solving technical problems encountered in previous works. • More sophisticated measures to estimate the human effort. Specifically, measures that estimate the cognitive load involve in reading, understanding and detecting an error in a translation (Foster et al., 2002), in contrast KSMR simply considers a constant cost. This will lead to a more accurate estimation of the improvements that may be expected by a human user. Acknowledgments Work supported by the European Un</context>
</contexts>
<marker>Nepveu, Lapalme, Langlais, Foster, 2004</marker>
<rawString>Laurent Nepveu, Guy Lapalme, Philippe Langlais, and George Foster. 2004. Adaptive language and translation models for interactive machine translation. In Proceedings of the conference on Empirical Methods on Natural Language Processing, pages 190–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="7117" citStr="Och and Ney, 2002" startWordPosition="1091" endWordPosition="1094">d presses the a key (k), then the system suggests completing the sentence with “list of resources” (a new ts). Iterations 2 and 3 are similar. In the final iteration, the user accepts the current translation. The terms in the latter equation are the language model probability Pr(t) that represents the well-formedness of t (n-gram models are usually adopted), and the (inverted) translation model Pr(s |t) that represents the relationship between the source sentence and its translation. In practice all of these models (and possibly others) are often combined into a log-linear model for Pr(t |s) (Och and Ney, 2002): t� = arg max t � N n=1 λn · log(fn(t, s)) (3) where fn(t, s) can be any model that represents an important feature for the translation, N is the number of models (or features), and λn are the weights of the log-linear combination. 2.2 Statistical Interactive Machine Translation Unfortunately, current MT technology is still far from perfect. This implies that, in order to achieve good translations, manual post-editing is needed. An alternative to this decoupled approach (first MT, then manual correction) is given by the IMT k ts IT-2 p To view a list i ng resources t = arg max t = arg max t I</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Efficient search for interactive statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the European chapter of the Association for Computational Linguistics,</booktitle>
<pages>387--393</pages>
<contexts>
<context position="9621" citStr="Och et al., 2003" startWordPosition="1542" endWordPosition="1545">ze the probability of the suffix given the available information. Formally, the best suffix of a given length will be: is = arg max Pr(ts |s, p) (4) t3 which can be straightforwardly rewritten as: �ts = arg max Pr(p, ts |s) (5) t3 = arg max Pr(p, ts) · Pr(s |p, ts) (6) t3 Note that, since pts = t, this equation is very similar to Equation (2). The main difference is that now the search process is restricted to those target sentences t that contains p as prefix. This implies that we can use the same MT models (including the log-linear approach) if the search procedures are adequately modified (Och et al., 2003). Finally, it should be noted that the statistical models are usually defined at word level, while the IMT process described in this section works at character level. To deal with this problem, during the search process it is necessary to verify the compatibility between t and p at character level. 2.3 IMT with Stochastic Error-Correction A common problem in IMT arises when the user sets a prefix which cannot be explained by the statistical models. To solve this problem, IMT systems typically include ad-hoc error-correction techniques to guarantee that the suffixes can be generated (Barrachina</context>
</contexts>
<marker>Och, Zens, Ney, 2003</marker>
<rawString>Franz Josef Och, Richard Zens, and Hermann Ney. 2003. Efficient search for interactive statistical machine translation. In Proceedings of the European chapter of the Association for Computational Linguistics, pages 387–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28239" citStr="Och, 2003" startWordPosition="4669" endWordPosition="4670"> split Chinese sentences into words with the Stanford word 4www.ted.com 250 segmenter (Tseng et al., 2005). Table 1 shows the main figures of the processed EU and TED corpora. 5.2 Model Estimation and User Simulation Key-stroke and mouse-action ratio (KSMR): number of key strokes plus mouse movements performed by the user, divided by the total number of characters in the reference. We used the standard configuration of the Moses toolkit (Koehn et al., 2007) to estimate one phrasebased and one hierarchical model for each corpus; log-linear weights were optimized by minimum error-rate training (Och, 2003) with the development partitions. Then, the optimized models were used to generate the word-graphs and hypergraphs with the translations of the development and test partitions. A direct evaluation of the proposed IMT procedures involving human users would have been slow and expensive. Thus, following previous works in the literature (Barrachina et al., 2009; Gonz´alezRubio et al., 2010), we used the references in the corpora to simulate the translations that a human user would want to obtain. Each time the system suggested a new translation, it was compared to the reference and the longest com</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Mart´ınez</author>
<author>Ismael Garc´ıa-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning for interactive statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>546--554</pages>
<marker>Ortiz-Mart´ınez, Garc´ıa-Varea, Casacuberta, 2010</marker>
<rawString>Daniel Ortiz-Mart´ınez, Ismael Garc´ıa-Varea, and Francisco Casacuberta. 2010. Online learning for interactive statistical machine translation. In Proceedings of the 2010 Conference of the North American Chapter of the Association for Computational Linguistics, pages 546–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Mart´ınez</author>
</authors>
<title>Advances in FullyAutomatic and Interactive Phrase-Based Statistical Machine Translation.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica</institution>
<marker>Ortiz-Mart´ınez, 2011</marker>
<rawString>Daniel Ortiz-Mart´ınez. 2011. Advances in FullyAutomatic and Interactive Phrase-Based Statistical Machine Translation. Ph.D. thesis, Universitat Polit`ecnica de Val`encia. Advisors: Ismael Garc´ıa Varea and Francisco Casacuberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31538" citStr="Papineni et al., 2002" startWordPosition="5208" endWordPosition="5211">ord-autocompleting, number of user key strokes divided by the total number of reference characters. The counterpart of PKSR in an IMT scenario is (Barrachina et al., 2009): Key-stroke ratio (KSR): number of key strokes, divided by the number of reference characters. PKSR and KSR are fairly comparable and the relative difference between them gives us a good estimate of the reduction in human effort that can be achieved by using IMT instead of a conventional post-edition system. We also evaluate the quality of the automatic translations generated by the MT models with the widespread BLEU score (Papineni et al., 2002). Finally, we provide both confidence intervals for the results and statistical significance of the observed differences in performance. Confidence intervals were computed by pair-wise re-sampling as in (Zhang and Vogel, 2004) while statistical significance was computed using the Tukey’s HSD (honest significance difference) test (Hsu, 1996). 251 EU TED WG HG WG HG 1-best BLEU [%] 45.0 45.1 11.0 11.2 1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0 Table 2: BLEU score of the word-graphs (WG) and hypergraphs (HG) used to implement the IMT procedures. IMT EU TED Setup PB HT PB HT ISF 27.4±.5 26.5±.5* </context>
<context position="32910" citStr="Papineni et al., 2002" startWordPosition="5431" endWordPosition="5434">zation (ISF) and the conditioned suffix formalization (CSF). PB stands for phrase-based model and HT stands for hierarchical translation model. For each task, the best result is displayed boldface, an asterisk * denotes a statistically significant better result (99% confidence) with respect to ISF with PB, and a star * denotes a statistically significant difference with respect to all the other systems. 6 Results We start by reporting conventional MT quality results to test if the generated word-graphs and hypergraphs encode translations of similar quality. Table 2 displays the quality (BLEU (Papineni et al., 2002)) of the automatic translations generated for the test partitions. The lower 1-best BLEU results obtained for TED show that this is a much more difficult task than EU. Additionally, the similar average BLEU results obtained for the 1000-best translations indicate that word-graphs and hypergraphs encode translations of similar quality. Thus, the IMT systems that use these word-graphs and hypergraphs can be compared in a fair way. Then, we evaluated different setups of the proposed IMT approach. Table 3 displays the IMT results obtained for the EU and TED tasks. We report KSMR (as a percentage) </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronica Romero</author>
<author>Alejandro H Toselli</author>
<author>Enrique Vidal</author>
</authors>
<title>Character-level interaction in computerassisted transcription of text images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th International Conference on Frontiers in Handwriting Recognition,</booktitle>
<pages>539--544</pages>
<contexts>
<context position="30842" citStr="Romero et al., 2010" startWordPosition="5095" endWordPosition="5098">ost-edition system. For such systems, characterlevel user effort is usually measured by the Character Error Rate (CER). However, it is clear that CER is at a disadvantage due to the auto-completion approach of IMT. To perform a fairer comparison between post-edition and IMT, we implement a postediting system with autocompletion. Here, when the user enters a character to correct some incorrect word, the system automatically completes the word with the most probable word in the task vocabulary. To evaluate the effort of a user using such a system, we implement the following measure proposed in (Romero et al., 2010): Post-editing key stroke ratio (PKSR): using a post-edition system with word-autocompleting, number of user key strokes divided by the total number of reference characters. The counterpart of PKSR in an IMT scenario is (Barrachina et al., 2009): Key-stroke ratio (KSR): number of key strokes, divided by the number of reference characters. PKSR and KSR are fairly comparable and the relative difference between them gives us a good estimate of the reduction in human effort that can be achieved by using IMT instead of a conventional post-edition system. We also evaluate the quality of the automati</context>
</contexts>
<marker>Romero, Toselli, Vidal, 2010</marker>
<rawString>Veronica Romero, Alejandro H. Toselli, and Enrique Vidal. 2010. Character-level interaction in computerassisted transcription of text images. In Proceedings of the 12th International Conference on Frontiers in Handwriting Recognition, pages 539–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="21089" citStr="Shen et al., 2010" startWordPosition="3470" endWordPosition="3473">erence for hierarchical phrases over serial combination of phrases. Note that no distortion model is included in the previous equation. Here, reordering is defined at rule level by the one-to-one non-terminal correspondence. In other words, reordering is a property inherent to each rule and it is the individual score of each rule what defines, at each step of the derivation, the importance of reordering. It should be noted that the IMT formalizations presented in Section 2 can be applied to other hierarchical or syntax-based SMT models such as those described in (Zollmann and Venugopal, 2006; Shen et al., 2010). 4 Search In offline MT, the generation of the best translation for a given source sentence is carried out by incrementally generating the target sentence2. This process fits nicely into a dynamic programming (DP) (Bellman, 1957) framework, as hypotheses which are indistinguishable by the models can be recombined. Since the DP search space grows exponentially with the size of the input, standard DP search is prohibitive, and search algorithms usually resort to a beam-search heuristic (Jelinek, 1997). 2Phrase-based systems follow a left-to-right generation order while hierarchical systems rely</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="27735" citStr="Tseng et al., 2005" startWordPosition="4585" endWordPosition="4588">available. The Zh–En corpus used in the experiments was part of the MT track in the 2011 evaluation campaign of the workshop on spoken language translation (Federico et al., 2011). Specifically, we used the dev2010 partition for development and the test2010 partition for test. We process the Spanish and English parts of the EU corpus to separate words and punctuation marks keeping sentences truecase. Regarding the TED corpus, we tokenized and lowercased the English part (Chinese has no case information), and split Chinese sentences into words with the Stanford word 4www.ted.com 250 segmenter (Tseng et al., 2005). Table 1 shows the main figures of the processed EU and TED corpora. 5.2 Model Estimation and User Simulation Key-stroke and mouse-action ratio (KSMR): number of key strokes plus mouse movements performed by the user, divided by the total number of characters in the reference. We used the standard configuration of the Moses toolkit (Koehn et al., 2007) to estimate one phrasebased and one hierarchical model for each corpus; log-linear weights were optimized by minimum error-rate training (Och, 2003) with the development partitions. Then, the optimized models were used to generate the word-grap</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>156--163</pages>
<contexts>
<context position="24035" citStr="Ueffing et al., 2002" startWordPosition="3959" endWordPosition="3962">reover, hypergraphs can represent a whole set of possible translations. An example is I saw a man with a telescope I saw with a telescope a man 6 4 5 1 2 3 I saw a man with a telescope 249 shown in Figure 2. Two alternative translations are constructed from the leave nodes (1, 2 and 3) up to the root node (6) of the hypergraph. Additionally, hypernodes and hyperedges may be shared among different derivations if they represent the same information. Thus, we can achieve a compact representation of the translation space that allows us to derive efficient search algorithms. Note that word-graphs (Ueffing et al., 2002), which are used to represent the search space for phrase-based models, are a special case of hypergraphs in which the maximum arity is one. Thus, hypergraphs allow us to represent both phrase-based and hierarchical systems in a unified framework. 4.2 Suffix Search on Hypergraphs Now, we describe a unified search process to obtain the suffix is that completes a prefix p given by the user according to the two IMT formulations (Equation (11) and Equation (17)) described in Section 2. Given an hypergraph, certain hypernodes define a possible solution to the maximization defined in the two IMT for</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002. Generation of word graphs in statistical machine translation. In Proceedings of the conference on Empirical Methods in Natural Language Processing, pages 156– 163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics.</title>
<date>2004</date>
<booktitle>In Proceedings of The international Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="31764" citStr="Zhang and Vogel, 2004" startWordPosition="5241" endWordPosition="5244">d by the number of reference characters. PKSR and KSR are fairly comparable and the relative difference between them gives us a good estimate of the reduction in human effort that can be achieved by using IMT instead of a conventional post-edition system. We also evaluate the quality of the automatic translations generated by the MT models with the widespread BLEU score (Papineni et al., 2002). Finally, we provide both confidence intervals for the results and statistical significance of the observed differences in performance. Confidence intervals were computed by pair-wise re-sampling as in (Zhang and Vogel, 2004) while statistical significance was computed using the Tukey’s HSD (honest significance difference) test (Hsu, 1996). 251 EU TED WG HG WG HG 1-best BLEU [%] 45.0 45.1 11.0 11.2 1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0 Table 2: BLEU score of the word-graphs (WG) and hypergraphs (HG) used to implement the IMT procedures. IMT EU TED Setup PB HT PB HT ISF 27.4±.5 26.5±.5* 53.0±.4 52.3±.4* CSF 26.6±.5* 25.1±.5* 52.2±.4* 50.8±.4* Table 3: IMT results (KSMR [%]) for the EU and TED tasks using the independent suffix formalization (ISF) and the conditioned suffix formalization (CSF). PB stands for p</context>
</contexts>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. In Proceedings of The international Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="5010" citStr="Zollmann and Venugopal, 2006" startWordPosition="736" endWordPosition="739">refix coverage problems. Moreover, we refine this formalization proposing an alternative error-correction formalization for the IMT framework (Section 2). Additionally, we also propose a specific error-correction model based on a statistical interpretation of the Levenshtein distance (Levenshtein, 1966). These formalizations provide a unified statistical framework for the IMT model in comparison to the ad-hoc heuristic error-correction methods previously used. In order to address the problem of properly deal with reordering in IMT, we introduce the use of hierarchical MT models (Chiang, 2005; Zollmann and Venugopal, 2006). These methods provide a natural approach to handle long range dependencies and allow the incorporation of reordering information into a consistent statistical framework. Here, we also describe how state-of-the-art hierarchical MT models can be extended to handle IMT (Sections 3 and 4). We evaluate the proposed IMT approach on two different translation task. The comparative results against the IMT approach described by Barrachina et al., (2009) and a conventional post-edition approach show that our IMT formalization for hierarchical SMT models indeed outperform other approaches (Sections 5 an</context>
<context position="21069" citStr="Zollmann and Venugopal, 2006" startWordPosition="3466" endWordPosition="3469">, A3 controls the model’s preference for hierarchical phrases over serial combination of phrases. Note that no distortion model is included in the previous equation. Here, reordering is defined at rule level by the one-to-one non-terminal correspondence. In other words, reordering is a property inherent to each rule and it is the individual score of each rule what defines, at each step of the derivation, the importance of reordering. It should be noted that the IMT formalizations presented in Section 2 can be applied to other hierarchical or syntax-based SMT models such as those described in (Zollmann and Venugopal, 2006; Shen et al., 2010). 4 Search In offline MT, the generation of the best translation for a given source sentence is carried out by incrementally generating the target sentence2. This process fits nicely into a dynamic programming (DP) (Bellman, 1957) framework, as hypotheses which are indistinguishable by the models can be recombined. Since the DP search space grows exponentially with the size of the input, standard DP search is prohibitive, and search algorithms usually resort to a beam-search heuristic (Jelinek, 1997). 2Phrase-based systems follow a left-to-right generation order while hiera</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
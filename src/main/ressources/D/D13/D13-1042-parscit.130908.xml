<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.945427">
Joint Bootstrapping of Corpus Annotations and Entity Types
</title>
<author confidence="0.967024">
Hrushikesh Mohapatra Siddhanth Jain Soumen Chakrabarti*
</author>
<affiliation confidence="0.879369">
IIT Bombay
</affiliation>
<sectionHeader confidence="0.984377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914730769231">
Web search can be enhanced in powerful ways if to-
ken spans in Web text are annotated with disambiguated
entities from large catalogs like Freebase. Entity anno-
tators need to be trained on sample mention snippets.
Wikipedia entities and annotated pages offer high-quality
labeled data for training and evaluation. Unfortunately,
Wikipedia features only one-ninth the number of enti-
ties as Freebase, and these are a highly biased sample
of well-connected, frequently mentioned “head” entities.
To bring hope to “tail” entities, we broaden our goal to a
second task: assigning types to entities in Freebase but
not Wikipedia. The two tasks are synergistic: know-
ing the types of unfamiliar entities helps disambiguate
mentions, and words in mention contexts help assign
types to entities. We present TMI, a bipartite graphical
model for joint type-mention inference. TMI attempts
no schema integration or entity resolution, but exploits
the above-mentioned synergy. In experiments involving
780,000 people in Wikipedia, 2.3 million people in Free-
base, 700 million Web pages, and over 20 professional
editors, TMI shows considerable annotation accuracy im-
provement (e.g., 70%) compared to baselines (e.g., 46%),
especially for “tail” and emerging entities. We also com-
pare with Google’s recent annotations of the same corpus
with Freebase entities, and report considerable improve-
ments within the people domain.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986580423076924">
Thanks to automatic information extraction and se-
mantic Web efforts, keyword search over unstruc-
tured Web text is rapidly evolving toward entity-
and type-oriented queries (Guo et al., 2009; Pan-
tel et al., 2012) over semi-structured databases such
as Wikipedia, Freebase, and other forms of Linked
Data.
A key enabling component for such enhanced
search capability is a type and entity catalog. This
includes a directed acyclic graph of types under the
subTypeOf relation between types, and entities at-
tached to one or more types via instanceOf edges.
∗soumen@cse.iitb.ac.in
YAGO (Suchanek et al., 2007) provides such a cat-
alog by unifying Wikipedia and WordNet, followed
by some cleanup.
Another enabling component is an annotated cor-
pus in which token spans (e.g., the word “Albert”)
are identified as a mention of an entity (e.g., the
Physicist Einstein). Equipped with suitable indices,
a catalog and an annotated corpus let us find “sci-
entists who played some musical instrument”, and
answer many other powerful classes of queries (Li
et al., 2010; Sawant and Chakrabarti, 2013).
Consequently, accurate corpus annotation has
been intensely investigated (Mihalcea and Csomai,
2007; Cucerzan, 2007; Milne and Witten, 2008;
Kulkarni et al., 2009; Han et al., 2011; Ratinov et
al., 2011; Hoffart et al., 2011). With two exceptions
(Zheng et al., 2012; Gabrilovich et al., 2013) that we
discuss later, public-domain corpus annotation work
has almost exclusively used Wikipedia and deriva-
tives, partly because Wikipedia provides not only a
standardized space of entities, but also reliably la-
beled mention text within its own documents, which
can be used to train machine learning algorithms for
entity disambiguation.
However, the high quality of Wikipedia comes
at the cost of low entity coverage (4.2 million)
and bias toward often-mentioned, richly-connected
“head” entities. Hereafter, Wikipedia entities are
called W. Freebase has fewer editorial controls, but
has at least nine times as many entities. This is par-
ticularly perceptible for people entities: one needs
to be relatively famous to be featured on Wikipedia,
but Freebase is less selective. Hereafter, Freebase
entities are called F.
As in any heavy-tailed distribution, even rela-
tively obsecure entities from F \ W are collectively
mentioned a great many times on the Web, and in-
cluding them in Web annotation is critical, if entity-
oriented search is to impact the vast number of tail
</bodyText>
<page confidence="0.98795">
436
</page>
<note confidence="0.763774666666667">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 436–446,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
queries submitted to Web search engines.
</note>
<bodyText confidence="0.988655902439024">
Primary goal — corpus annotation: We have
thus established a pressing need to bootstrap from a
small entity catalog W (such as Wikipedia entities),
and a small reference corpus CW (e.g., Wikipedia
text) reliably annotated with entities from W, to a
much larger catalog F (e.g., Freebase), and an open-
domain large payload corpus C (e.g., the Web).
We can and will use entities in F n W 1 in the
bootstrapping process, but the real challenge is to
annotate C with mentions m of entities in F \ W.
Unlike for F n W, we have no training mentions for
F \ W. Therefore, the main disambiguation signal
is from the immediate entity neighborhood N(e) of
the candidate entity e in the Freebase graph. I.e., if
m also reliably mentions some entity in N(e), then
e becomes a stronger candidate. Unfortunately, for
many “tail” entities e E F \ W, N(e) is sparse. Is
there hope for annotating the Web with tail entities?
Here, we achieve enhanced accuracy for the primary
annotation goal by extending it with a related sec-
ondary goal.
Secondary goal — entity typing: If we had avail-
able a suitable type catalog T with associated enti-
ties in W, which in turn have known textual men-
tions, we can build models of contexts referring to
types like chemists, sports people and politicans.
When faced with people called John Williams in
F \W, we may first try to associate them with types.
This can then help disambiguate mentions to specific
instances of John Williams in F \ W. In principle,
useful information may also flow in the reverse di-
rection: words in mention contexts may help assign
types to entities in F \ W. For reasons to be made
clear, we choose YAGO (Suchanek et al., 2007) as
the type catalog T accompanying entities in W.
Our contributions: We present TMI, a bootstrap-
ping system for solving the two tasks jointly. Apart
from matches between the context of m and entity
names in N(e), TMI combines and balances evi-
dence from two other sources to decide if e is men-
tioned at token span m, and has type t:
</bodyText>
<listItem confidence="0.9750255">
• a language model for the context in which enti-
ties of type t are usually mentioned
1With F=Freebase and W=Wikipedia, F nW Pz� W but not
quite; W \ F is small but non-empty.
• correlations between t and certain path features
generated from N(e).
</listItem>
<bodyText confidence="0.999210322580645">
TMI uses a novel probabilistic graphical model for-
mulation to integrate these signals. We give a de-
tailed account of our design of node and edge po-
tentials, and a natural reject option (recall/precision
tradeoff).
We report on extensive experiments using YAGO
types, Wikipedia entities and text, Freebase en-
tities, and text from ClueWeb122, a 700-million-
page Web corpus. We focus on all people enti-
ties in Wikipedia and Freebase, and provide three
kinds of evaluation. First, we evaluate TMI on over
1100 entities in F n W and 5500 snippets from
Wikipedia text, where it visibly improves upon base-
lines and a recently proposed alternative method
(Zheng et al., 2012). Second, we resort to exten-
sive manual evaluation of annotation on ClueWeb12
Web text with Freebase entities, by professional ed-
itors at a commercial search company. TMI again
clearly outperforms strong baselines, doing partic-
ularly well for nascent or tail entities. TMI im-
proves per-snippet accuracy, for some classes of
entities, from 46% to 70%, and pooled F1 score
from 66% to 73%. Third, we compare TMI an-
notations with Google’s FACC1 (Gabrilovich et al.,
2013) annotations restricted to people; TMI is sig-
nificantly better. Our annotations and related data
can be downloaded from http://www.cse.iit
b.ac.in/˜soumen/doc/CSAW/. Toourknowl-
edge, this is among the first reports on extensive hu-
man evaluation of machine annotation for F \ W on
a large Web corpus.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.996453090909091">
The vast majority of entity annotation work (Mi-
halcea and Csomai, 2007; Cucerzan, 2007; Milne
and Witten, 2008; Kulkarni et al., 2009; Han et al.,
2011; Ratinov et al., 2011; Hoffart et al., 2011) use
Wikipedia or derivative knowledge bases. (Ritter et
al., 2011) and (Zheng et al., 2012) are notable ex-
ceptions. (Ritter et al., 2011) use entity names for
distant supervision in POS tagging, chunking and
broad named entity typing in short tweets, which are
different from our goals.
Recently, others have investigated inferring types
</bodyText>
<footnote confidence="0.979458">
2http://lemurproject.org/clueweb12/
</footnote>
<page confidence="0.99698">
437
</page>
<figure confidence="0.999074923076923">
ChemSpider
academic
founder
type
chemist
type
Antony John
Williams
place of birth
St Aspah
nationality
UK
0ggbn2k
Athlete
American
football
player
type
type
education
Wisconsin
Madison
John
Williams
place of birth
Muskegon
03nmyfz
Harvard University
Eunice
Kanenstenhawi
Williams
children
John
Williams
0bhbqmm
???
type
education
Mention contexts: (a) (b) (c)
</figure>
<footnote confidence="0.550317863636364">
... Antony John Williams, VP for
Strategic Development and Head of
the Cheminformatics group for the
Royal Society of Chemistry has
been honoured by Microsoft
Research for his ...
ChemSpider, Chemistry
... to a 20-0 lead by the second
quarter with running back John
Williams’s 1 yard touchdown
quarterback Neil O Donnell’s ...
Steelers, Packers, touchdown,
quarterback, Colts, quarter, yard,
goal, Indianapolis, field, receiver
... Massachusetts, on 17 September
1696, the daughter of Puritan
minister Rev. John Williams and his
wife Eunice Mather Williams ...
Massachusetts, Eunice Williams,
minister, Puritan, kanenstenhawi,
Rev.
Salient words from page containing contexts:
</footnote>
<figureCaption confidence="0.994048">
Fig. 1: Signal synergies. Three of the many people mentioned as “John Williams” on the Web are shown, with
Freebase MIDs. (a) Easy case where Freebase neighbors of 0ggbn2k match snippet and salient text, and type links are
also available. (b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippet
to 03nmvfz. (c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text,
which also match neighbors of 0bhbqmm.
</figureCaption>
<bodyText confidence="0.999750948717948">
of emerging entities (related to our secondary goal).
In concurrent work, (Nakashole et al., 2013) pro-
pose integer linear program formulations for infer-
ring types of emerging entities from the way their
mentions are embedded in curated relation-revealing
phrases. (Lin et al., 2012) earlier approached the
problem using weight propagation in a bipartite
graph connecting unknown to known entities via
textual relation patterns. Both note that this can
boost mention disambiguation accuracy.
The closest work to ours is by (Zheng et al.,
2012): they use semisupervised learning to anno-
tate a corpus with Freebase entities. Like (Milne
and Witten, 2008), they depend on unambiguous
entity-mention pairs to bootstrap a classifier, then
apply it to unlabeled, ambiguous mentions, creating
more training data. They use a per-type language
model like us (§3.3), but this is used as a secondary
cause for (word) feature generation, supplementing
and smoothing entity-specific language models. In
contrast, we use a rigorous graphical model to com-
bine new signals, not depending on naturally unam-
biguous mentions. Finally, in the interest of fully
automated evaluation, they limit their experiments
to F fl W and Wikipedia corpus, thus differing crit-
ically from our human evaluation on F and a Web
corpus.
(Gabrilovich et al., 2013) have recently released
FACC1: annotations of ClueWeb09 and ClueWeb12
with Freebase entities. Their algorithm is not yet
public. They report: “Due to the sheer size of the
data, it was not possible to verify all the automatic
annotations manually. Based on a small-scale hu-
man evaluation, the precision ... is believed to be
around 80–85%. Estimating the recall is of course
difficult; however, it is believed to be around 70-
85%.” In §5, we will see that, for people entities,
TMI greatly increases recall beyond FACC1, keep-
ing precision unimpaired.
</bodyText>
<sectionHeader confidence="0.943126" genericHeader="method">
3 The three signals
</sectionHeader>
<bodyText confidence="0.999664363636364">
Fig. 1 shows three Freebase entities mentioned as
“John Williams” in Web text, represented as nodes
with Freebase “MID”s e = 0ggbn2k, 03nmvfz,
and 0bhbqmm, embedded in their Freebase graph
neighborhoods. Owing to larger size and higher
flux, Freebase shows less editorial uniformity than
Wikipedia. This shows up in missing or non-
standard relation edges. Unlike YAGO, where
each entity is attached to one or more types, e =
0bhbqmm does not have a type link. Many people
have a link labeled profession, which is a second
</bodyText>
<page confidence="0.995151">
438
</page>
<bodyText confidence="0.999937129032258">
kind of type link. Entities like e = 0bhbqmm also
have small, uninformative graph neighborhoods.
Also shown are three mention contexts, each rep-
resented by the snippet immediately surrounding the
mention, and salient words from the documents con-
taining each snippet. (Salient words may be ex-
tracted as words that contribute the largest compo-
nents to the document represented as a TFIDF vec-
tor.) (a) shows a favorable but relatively rare case
where e = 0ggbn2k has reliable type links. We can-
not assume there will be a 1-to-1 correspondence be-
tween Freebase and YAGO types, but in §3.2 we will
describe how to learn associations between Freebase
paths around entities and their YAGO types. The
snippet and salient words show reasonable overlap
with N(e). In §3.4 we will describe features that
characterize such overlap. In (b), e = 03nmvfz
is reliably typed, but there is no direct match be-
tween N(e) and the snippet. Nevertheless, the snip-
pet can be reliably annotated with 03nmvfz if we
can learn associations between types American foot-
ball player and Athlete (or their approximate YAGO
target types) and several context/salient words (see
§3.3). In (c), e = 0bhbqmm is not reliably typed.
However, there are matches between N(e) and con-
text/salient words. Once the snippet-to-entity asso-
ciation is established, it is easier to assign 0bhbqmm
to suitable types in YAGO.
In this section we will first describe our design of
the target type space, and then the tree signals that
will be used in our joint inference.
</bodyText>
<subsectionHeader confidence="0.99962">
3.1 Designing the target type space
</subsectionHeader>
<bodyText confidence="0.99728">
By typing two people called John Williams as ac-
tor and footballer, we may also disambiguate their
mentions accurately. Therefore, we need a well-
organized type space where the types
</bodyText>
<listItem confidence="0.99342075">
• collectively cover most entities of interest,
• offer reasonable type prediction accuracy, and
• can be selected algorithmically, for any do-
main.
</listItem>
<bodyText confidence="0.9987355">
Wikipedia and Freebase have many obscure types
like “people born in 1937” or “artists from On-
tario” which satisfy none of the above requirements.
YAGO, on the other hand, has a clean hierarchy of
over 200,000 types with broad coverage and fine dif-
ferentiation. Most entities in F \ W can be accu-
</bodyText>
<figure confidence="0.905028125">
if t has &lt; Nlow = 5000 member entities then
reject t from our type space
return
if t has &gt; Nhigh = 25000 member entities then
for each immediate child t&apos; C t do
call ChooseTypes(t&apos;)
else
accept t into our type space (but do not recurse)
</figure>
<figureCaption confidence="0.997966">
Fig. 2: Procedure ChooseTypes(t).
</figureCaption>
<bodyText confidence="0.99778075">
rately attached to one or more YAGO types.
YAGO lists around 37,000 subtypes of person.
To satisfy the three requirements above, we called
ChooseTypes(person) (Fig. 2); this resulted in
130 suitable types being selected. These directly
covered 80% of Freebase people; the rest could
mostly be attached to slightly over-generic types
within our selection.
</bodyText>
<subsectionHeader confidence="0.999832">
3.2 Predicting types from entity neighborhood
</subsectionHeader>
<bodyText confidence="0.999927583333334">
There will generally not be a simple mapping be-
tween Freebase and target types. E.g., entity e
may be known as a Mayor in Freebase, but the
closest YAGO type may be Politician. Edge and
node labels from the Freebase graph neighborhood
N(e) can embed many clues for assigning e a target
type. E.g., e = 03nmvfz may have an edge labeled
playedFor to a node representing the Wikipedia en-
tity Pittsburgh Steelers, which has a type
link to NFL team. This two-hop link label sequence
would repeat for a large number of players, and can
be used as a feature in a classifier.
</bodyText>
<figureCaption confidence="0.8856626">
Fig. 3: Freebase has small diameter despite graph thin-
ning. Prune1 removes paths from the whole Free-
base graph that pass through nodes /user/root and /com-
mon/topic, Prune2 also removes node /people/person,
and Prune3 removes several other high degree hubs.
</figureCaption>
<bodyText confidence="0.99934775">
Two further refinements are needed to make this
work. First, we have to collect path labels around
negative instances we well, and submit positive and
negative path labels to a binary classifier to can-
</bodyText>
<figure confidence="0.9970025">
Reachable
0.8
0.6
0.4
0.2
0
1 2 3 Hops
1
Whole
Prune1
Prune2
Prune3
</figure>
<page confidence="0.99771">
439
</page>
<bodyText confidence="0.999813769230769">
cel the effect of frequent but non-informative path
types. Second, indiscriminate expansion around e is
infeasible because the Freebase graph has very small
diameter. Even after substantial pruning, paths of
length 3 and 4 reach over 40% and 96% of all nodes
(Fig. 3). This increases computational burden and
floods us with noisy and spurious paths. We rem-
edy the problem using an idea from PathRank (Lao
and Cohen, 2010). Instead of trying to explore all
paths originating (or terminating) at e, where e may
or may not belong to a target type, we focus on paths
between e and other known members of the target
type.
</bodyText>
<subsectionHeader confidence="0.989218">
3.3 Type “language model”
</subsectionHeader>
<bodyText confidence="0.99997336">
To exploit the second signal, shown in Fig. 1(b), we
need to model the association between target YAGO
types and the mention contexts of Wikipedia entities
known to belong to those types. This model compo-
nent is in the same spirit as (Zheng et al., 2012).
For each target YAGO type t, we sample positive
entities e E F n W, and for each e, we collect, from
Wikipedia annotated text, a corpus of snippets men-
tioning e. We remove the mention words and retain
the rest. We also collect salient words from the en-
tire Wikipedia document containing the snippet, as
shown in Fig. 1.
At this point each target type is associated with
a “corpus” of contexts, each represented by snippet
words. We compute the IDF of all words in this
corpus3, and then represent each type as a TFIDF
vector (Salton and McGill, 1983). A test context is
turned into a similar vector, and its score with re-
spect to t is the cosine between these two vectors.
This simple approach was found superior to building
a more traditional smoothed multinomial unigram
model (Zhai, 2008) for each type. Given the output
of this component feeds into an outer discriminative
inference mechanism, a strict probabilistic model is
not necessary.
</bodyText>
<subsectionHeader confidence="0.977203">
3.4 Entity neighborhood match with snippet
</subsectionHeader>
<bodyText confidence="0.9999806">
The third signal is a staple of any disambiguation
work: match the occurrence context against the
neighborhood in the structured representation. In
word sense disambiguation (WSD), support for as-
signing a word in context to a synset comes from
</bodyText>
<footnote confidence="0.561896">
3Generic IDF from Wiki text does not work.
</footnote>
<bodyText confidence="0.997285714285714">
matches between, say, other words in the context and
the WordNet neighborhood of the proposed synset.
As in WSD, many approaches to Wikification mea-
sure some local consistency between a mention m
and the neighborhood N(e) of a candidate entity e.
N(e) is again limited by a maximum path length E.
From snippet m we extract all phrases P(m) exclud-
ing the mention words. For each phrase p E P(m),
if p occurs at least once4 in any node of N(e), then
we accumulate a credit of |p |E.,P IDF(w), where
w ranges over words in p, IDF(w) is its inverse
document frequency (Salton and McGill, 1983) in
Wikipedia, and |p |is the length of the phrase. This
rewards exact phrase matches.
</bodyText>
<figure confidence="0.9001995">
Candidate types
Mentions in context
</figure>
<figureCaption confidence="0.990529">
Fig. 4: Tripartite assignment problem.
</figureCaption>
<sectionHeader confidence="0.940764" genericHeader="method">
4 Unified model
</sectionHeader>
<bodyText confidence="0.9783383">
Figure 4 abstracts out the signals shown in Figure 1
into a tripartite assignment problem. Each mention
like m1 has to choose at most one entity from among
candidate aliasing entities like e and e&apos;. Each entity
e E F \W has to choose one type (for simplicity we
ignore zero or more than one types as possibilities)
from candidates like t and t&apos;.
The configuration of thick (green) edges should
be preferred to alternative dotted (red) edges under
these considerations:
</bodyText>
<listItem confidence="0.9984838">
• There is high local affinity or compatibility be-
tween e and t, based on associations between t
and N(e) as discussed in §3.2.
• There are better textual matches between N(e)
and m1, as compared to N(e&apos;) and m1.
• In aggregate, the non-mention tokens in the
context of m1, m2 (shown as gray horizontal
lines) match well the language model associ-
ated with mentions of entities of type t (rather
than t&apos;).
</listItem>
<footnote confidence="0.687164">
4Incorporating term frequency often polluted the score.
</footnote>
<figure confidence="0.954353">
r r’
Candidate
entities
e’
e
M1 M2
440
Potential of edge
connecting snippet
node to entity node
</figure>
<figureCaption confidence="0.8694375">
Fig. 5: Illustration of the proposed bipartite graphical model, with tables for node and edge potentials and synthetic
L-entity nodes to implement the reject option.
</figureCaption>
<figure confidence="0.994386465116279">
e0 t0 0
... ... ...
e1 t0 0
t0
... ... ...
Potential of edge
connecting snippet
to dummy entity
node
...
t0
t1
0
0
0
Snippet node
potential
Uniform node potential
for dummy entity
e0
e1
0 (e0)
0 (e1)
...
e0 t0
... ... ...
e1 t0 0
... ... 0
t0
t1
t2
0 (t0)
0 (t1)
...
Entity node
potential
e5
e1
0 (e5)
0 (e1)
...
Snippet node
potential
</figure>
<bodyText confidence="0.9999711875">
We now present a unified model that combines the
signals and solves the two proposed tasks jointly.
We model two kinds of decision variables, which
will be represented by nodes in a graphical model.
Associated with each entity e E F \ W there is a
hidden type variable (node) Te, which can take on a
value from (some subset of) the type catalog T . As-
sociated with each mention m (along with its snip-
pet context and all its observable features) there is
a hidden entity variable Em, which can take on val-
ues from some subset of entities. (For simplicity, we
assume that entities in F n W have already been an-
notated in the corpus, and no m of interest mentions
such entities.)
We will model the probability of a joint assign-
ment of values to Te, Em as
</bodyText>
<equation confidence="0.8564554">
�log Pr(~t,~e) = α φe(te) + β � φm(em)
e m
� ψe,m(te, em) — const., (1)
+
e,m
</equation>
<bodyText confidence="0.999711428571429">
where node log potentials are called φe, φm, edge
log potentials are called ψe,m, and α, β are tuned
constants. The log partition function, written
“const.” above, will not be of interest in infer-
ence, where we will seek to choose ~t,~e to maximize
Pr(~t,~e). In this section, we will design the node and
edge log potentials.
</bodyText>
<subsectionHeader confidence="0.992635">
4.1 Node log potentials
</subsectionHeader>
<bodyText confidence="0.998718125">
Each node Te is associated with a node potential ta-
ble, mapping from possible types in Te to nonneg-
ative potentials. The potential values are supplied
from §3.2 as the classifier output scores.
Each node Em is associated with a node poten-
tial table, mapping from possible entities in £m to
nonnegative potentials. The potential values are sup-
plied from §3.4.
</bodyText>
<subsectionHeader confidence="0.995146">
4.2 Edge log potentials
</subsectionHeader>
<bodyText confidence="0.999981">
Suppose we assign Em = e, and Te = t. Then
we would like the non-mention context words of
m to be highly compatible with the type “language
model” developed in §3.3.
If e is among the set of values £m that Em can
take, then nodes Te and Em are joined by an edge.
This edge is associated with an edge potential table
ψe,m : Te x£m — R+. ψe ,m (·, e&apos;) will be set to zero
(cells shaded gray in Fig. 5) when e =� e&apos;. ψe,m(t, e)
is set to the cosine match score described in §3.3.
</bodyText>
<subsectionHeader confidence="0.991496">
4.3 The reject (a.k.a. null, nil, NA, L) option
</subsectionHeader>
<bodyText confidence="0.997778857142857">
An algorithm may reject many snippets, i.e., refrain
from annotating them. This could be because the
snippet mentions an entity outside F (and outside
W), or the system wishes to ensure high precision at
some cost to recall.
Rejection is modeled by adding, for each snippet
m, a pseudo or “null” entity Lm (also called “no an-
</bodyText>
<page confidence="0.997612">
441
</page>
<bodyText confidence="0.999960566666667">
notation” NA, null or nil in the TAC-KBP5 commu-
nity). For simplicity, we assume that ⊥m and ⊥m,
are incomparable or distinct for m =6 m&apos;. I.e., we
do not offer to cluster mentions of unknown enti-
ties. These remain separate, unconnected nodes in
the (augmented) Freebase graph.
If we choose Em = ⊥m, we get zero credit for
matching non-mention text in m to N(⊥m), because
N(⊥m) = 0 and §3.4 has no information to con-
tribute. I.e., we set φm(⊥m) = 0. In general,
φm(·) ≥ 0, so ⊥m gets the lowest possible credit
(but this will be modified shortly).
There is also a type variable TL.. To what type
should we assign “entity” ⊥m? Because ⊥m has no
connections in the Freebase graph, no hint can come
from §3.2. Put differently, φL.(t) will be constant
(say, zero) for all snippets m and types t.
Even if we do not know the entity mentioned in
m, the non-mention text in m will have differential
affinity to different types, obtained from §3.3. This
means that, if Em = ⊥m is chosen, TL. will be
argmaxt ψL.,m(t,⊥m), which explains the non-
mention words in m using the best available lan-
guage model associated with some type. For a dif-
ferent entity Em = eo and type Tep = t assign-
ment to win, αφm(e0)+βφe0(t)+ψe0,m(t, eo) must
exceed the null score above. This provides a us-
able recall/precision handle: we modify φm(⊥m) to
a tuned number; making it smaller generally gives
higher recall and lower precision.
</bodyText>
<subsectionHeader confidence="0.905621">
4.4 Inference and training
</subsectionHeader>
<bodyText confidence="0.999344384615385">
The goal of collective inference will be to assign a
type value to each Te and an entity value to each
Em. We seek the maximum a-posteriori (MAP) la-
bel, for which we use tree-reweighted message pass-
ing (TRW-S) (Kolmogorov, 2006). Our graph has
plenty of bipartite cycles, so inference is approxi-
mate. Given the sparsity of data, we preferred to
delexicalize our objective (1), i.e., avoid word-level
features and pre-aggregate their signals via time-
tested aggregators (such as TFIDF cosine). As a re-
sult we have only two free parameters α, β in (1),
which we tune via grid search. A more principled
training regimen is left for future work.
</bodyText>
<footnote confidence="0.824497">
5http://www.nist.gov/tac/2013/KBP/
</footnote>
<sectionHeader confidence="0.998494" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999837384615385">
We focus our experiments on one broad type of en-
tities, people, that is more challenging for disam-
biguators than typical showcase examples of distin-
guishing (Steve) Jobs from employment and Apple
Inc. from fruit.
We report on three sets of experiments. In §5.1,
we restrict to entities from F ∩ W and Wikipedia
text, for which ground truth annotation is avail-
able. In §5.2, we evaluate TMI and baselines on
ClueWeb12 and entities from Freebase, not lim-
ited to Wikipedia. In §5.3, we compare TMI
with Google’s recently published FACC1 annota-
tions (Gabrilovich et al., 2013).
</bodyText>
<subsectionHeader confidence="0.635526">
5.1 Reference corpus CW with F ∩ W entities
</subsectionHeader>
<bodyText confidence="0.937045125">
Limited to people, |F |= 2323792, |W |= 807381,
|F \ W |= 1544942, and |F ∩ W |= 778850. It
is easiest to evaluate TMI and others on Wikipedia
entities. They have known YAGO types. Wikipedia
text has explicit (disambiguated) entity annotations.
For these reasons, the few known systems for
Freebase-based annotation (Zheng et al., 2012) are
evaluated exclusively on F ∩ W.
</bodyText>
<subsectionHeader confidence="0.511194">
5.1.1 Seeding and setup
</subsectionHeader>
<bodyText confidence="0.999885047619048">
People in F ∩ W are known by one or more men-
tion words/phrases. From these, we collect mention
phrases along with the candidate entity set for each
phrase. The number of candidates is the phrase’s de-
gree of ambiguity. We sort phrases by ambiguity and
draw a sample over the ambiguity range. This gives
us seed phrases with representative ambiguity. Then
we collect all entities mentioned by these phrases.
Overall we collect about 1100 entities and 5500 dis-
tinct mentions.
Contrast this with (Zheng et al., 2012), who sam-
ple entities from much fewer than 130, and largely
well-separated types: professional athletes, aca-
demics, actors, films, books, hotels, and tourist at-
tractions. If there were only two namesakes, an ac-
tor and a politician, the politician disappears, leav-
ing a naturally unambiguous alias. I.e., (Zheng et
al., 2012) did not “complete” their entity sets with
aliased entities. For all these reasons, Z0 numbers
here are not directly comparable to those in their pa-
per.
</bodyText>
<page confidence="0.996309">
442
</page>
<subsubsectionHeader confidence="0.829536">
5.1.2 Tasks and baselines
</subsubsectionHeader>
<bodyText confidence="0.999511869565217">
The structure of TMI suggests two natural base-
lines to compare against it. TMI solves two tasks si-
multaneously: assign types to entities and entities to
snippets. So the first baseline, T0, is one that solves
the typing task separately, and the second, A0, does
snippet annotation separately. A third baseline, Z0,
from (Zheng et al., 2012) does only snippet annota-
tion; they do not consider typing entities.
While evaluating types output by TMI and T0
against ground truth, we may wish to assign partial
credit for overlapping types, e.g., athlete vs. soccer
player, because our types form an incomplete hierar-
chy. We use the standard “M&amp;W” score of semantic
similarity between types (Milne and Witten, 2008)
for this.
As regards snippet annotation, Z0 (Zheng et al.,
2012) does not specify any mechanism for han-
dling ⊥. Therefore we run two sets of experiments.
In one we eliminate all snippets with ground truth
⊥. A0, and TMI are also debarred from returning ⊥
for any snippet. In the other, snippets marked ⊥ in
ground truth are included. A0 and TMI are enabled
to return ⊥, but Z0 cannot.
</bodyText>
<figure confidence="0.4749125">
TMI A0 Z0
0/1 snippet accuracy 0.827 0.699 0.627
</figure>
<figureCaption confidence="0.820776">
Fig. 6: Snippet annotation on CW corpus, F ∩W entities,
⊥ not allowed.
</figureCaption>
<table confidence="0.9953846">
TMI A0 Z0
0/1 snippet accuracy 0.7307 0.651 0.622
Snippet precision 0.858 0.843 0.622
Snippet recall 0.777 0.692 0.639
Snippet F1 0.815 0.760 0.630
</table>
<figureCaption confidence="0.7558845">
Fig. 7: Snippet annotation on CW corpus, F ∩W entities,
⊥ allowed.
</figureCaption>
<subsubsectionHeader confidence="0.69995">
5.1.3 Snippet annotation results
</subsubsectionHeader>
<bodyText confidence="0.999084222222222">
Fig. 6 shows snippet annotation accuracy (frac-
tion of snippets labeled with the correct entity) when
⊥ is not allowed as an entity. As two uninformed
refernces, uniform random choice gives an accuracy
of 0.423 and choosing the entity with the largest
prior gives an accuracy of 0.767. TMI is consid-
erably better than A0, which is better than Z0 and
the uninformed references. This is despite training
Z0’s per-type topic models not only on unambiguous
</bodyText>
<figureCaption confidence="0.832896">
Fig. 8: Bucketed comparison between TMI and baselines,
F ∩ W, ⊥ allowed.
</figureCaption>
<figure confidence="0.916936">
TMI T0
0/1 type accuracy
M&amp;W type accuracy
</figure>
<figureCaption confidence="0.69167425">
Fig. 9: Type inference, CW corpus, F ∩ W entities.
snippets, but also on a disjoint fraction of F∩W, as a
surrogate for Wikipedia’s containment in Freebase.
Fig. 7 repeats the experiment while allowing ⊥.
</figureCaption>
<bodyText confidence="0.9127676">
Here, in 0/1 accuracy, ⊥ is regarded as just another
entity. Again, we see that TMI has a clear advantage.
Z0’s performance here is worse than in (Zheng et
al., 2012). This is explained by our much larger and
difficult-to-separate type system.
We disaggregate the summary results into buck-
ets, shown in Fig. 8. Each bucket covers a range of
degrees of entity nodes in Freebase, while roughly
balacing the number of snippets in each bucket. TMI
generally shows larger gains for low-degree buckets.
</bodyText>
<subsubsectionHeader confidence="0.568235">
5.1.4 Type prediction results
</subsubsectionHeader>
<bodyText confidence="0.999799727272727">
We also compared the type inference accuracy of
TMI and T0; (Zheng et al., 2012) do not infer types.
The summary is in Fig. 9. Two uninformed baselines
are worth mentioning. Uniform random choice over
130 types gave only 2% accuracy. Chossing the type
with largest prior probability gave 28.2% accuracy.
TMI is much better, but offers no significant ben-
efit (or degradation) compared to T0. We verified,
partly by way of debugging, that there do exist enti-
ties e with small degree but a modest number of as-
signed snippets, for which snippet-to-N(e) matches
</bodyText>
<figure confidence="0.999711891304348">
0...19 20...39 &gt;=40
Degree--&gt;
0...19 20...39 &gt;=40
Degree--&gt;
9 3
TMI
1 A0 Z0
c
r
c
0.85
0.85
0.81
0.73
0.
0.75
0.74
0.8
Accuracy--&gt;
0.52
0.6
0.42
0.4
0.2
0
Pooled F1--&gt;
0.8
0.6
0.4
0.2
TMI A0 Z0
1
0.67
0.62
0.53
0.93
0.84
0.93
0.73
0.74
0.57
0
0.80
0.82
0.81
0.83
</figure>
<page confidence="0.999157">
443
</page>
<bodyText confidence="0.964895583333333">
angus mcdonald, chris robinson, christopher henry, elizabeth
cameron, george woods , henry barnes, jack scott, jeremy
robert, john sherman, leonard thomas, marc anthony, mitchell
donald, morrison mark, parker edward, richard andrew , simon
scott, stephen ross, stuart baron, tom clark, whitney john, austin
scott, barbara johnson, brian peterson, carlos rivero, david
berman, david johns, donald fraser, george davies, george fisher,
graham smith, john pepper, jonathan edwards, kevin brown,
kevin hughes, matt johnson, michael davidson, nancy johnson,
paul holmes, pedro martins, peter frank, peter mitchell, peter
mullen, robert stern, roger edwards, stuart walker, terry evans,
tony angelo, tony ward, william jarvis, william sampson
</bodyText>
<figure confidence="0.976834666666667">
Mentions
01
0
</figure>
<figureCaption confidence="0.9992185">
Fig. 10: Seed mentions for confusion clusters for Web
corpus C and entities in F.
</figureCaption>
<bodyText confidence="0.9999688">
provide a boost to type prediction accuracy (about
50%), as compared to T0 (about 20% for these in-
stances). Therefore, the flow of information between
type and entity assignments is, in principle, bidirec-
tional, in the regime of such entities.
</bodyText>
<subsectionHeader confidence="0.997145">
5.2 Payload corpus C with entities in F
</subsectionHeader>
<bodyText confidence="0.9999645">
Recall our main goal is to annotate payload cor-
pus C with entities in all of F. Experience with
F fl W and CW may not be representative of F and
C. Entities in F \ W may not come with reference
mentions, and their type and entity neighborhoods
may be sparse. Furthermore, compared to the closed
world of F fl W, evaluating TMI and baselines over
C and F \ W is challenging. Entities in F \ W
do not have ground truth types in the type catalog
T (here, YAGO), nor snippets labeled by humans as
mentioning them. Therefore, we need human edi-
torial judgment, which is scarce. Even though TMI
can be applied at Web scale, the scale if evaluation
is limited by editorial input.
</bodyText>
<subsectionHeader confidence="0.978705">
5.2.1 Seeding and data setup
</subsectionHeader>
<bodyText confidence="0.999415384615384">
There are about 2.3 million Freebase entities con-
nected to /people/person via type links. Similar to
§5.1.1, we chose phrases (Fig. 10) with diverse de-
gree of ambiguity (Fig. 11), to seed confusion clus-
ters. Then we completed the clusters by including
aliased entities, as before, so as not to artifically re-
duce the degree of ambiguity. Note that entities in
W can and do contend with entities in F \ W. The
cluster size distribution is shown in Fig. 12. Limited
by editorial budget, we finished with 634 entities,
238 distinct aliases and 4,500 snippets.
We used the 700-million-page ClueWeb12 Web
corpus. All phrases in the expanded clusters are
</bodyText>
<figure confidence="0.408323">
Mentions
</figure>
<figureCaption confidence="0.9996556">
Fig. 11: Ambiguity distribution for Web corpus C. Un-
ambiguous names are usually fully expanded and very
rare, if at all present, in evaluated snippets.
Fig. 12: Confusion cluster size distribution for Web cor-
pus C.
</figureCaption>
<bodyText confidence="0.999960722222222">
loaded into a trie used by a map-reduce job to ex-
tract documents, then snippets, from the corpus.
Some phrases in Figs. 10 and 11 have overwhelm-
ing numbers of pages with matches. In produc-
tion, we naturally want all of them to be annotated.
But human editorial judgment being the bottleneck,
we sampled 50% or 50,000 snippets, whichever was
smaller. Starting with about 752,450 pages, we ran
the Stanford NER (Finkel et al., 2005) to mark per-
son spans. Pages with fewer than five non-person to-
kens per person were discarded; this effectively dis-
carded long list pages without any informative text
to disambiguate anyone, and left us with 574,135
pages. From these we collected 304,309 snippets
where the mention phrase is marked by the NER as
a person. Each seed phrase leads to one cluster on
which TMI and A0 are run. Note that L must be
allowed on the open Web.
</bodyText>
<subsubsectionHeader confidence="0.712834">
5.2.2 Editorial judgment
</subsubsectionHeader>
<bodyText confidence="0.9999025">
Finally, for each algorithm, about 634 entity-type
and about 4500 snippet-entity assignments are ran-
domly sampled and sent to 20 editors in a commer-
cial search engine company, who judged each as-
signment as correct or incorrect, without knowing
which algorithm produced the annotation, to avoid
</bodyText>
<figure confidence="0.995912387096774">
#Snippets
Ambiguity
1 6 11 16 21 26 31 36 41 46
#Entities--&gt;
40
30
20
10
0
Rank of cluster--&gt;
40
30
20
10
0
Ambiguity--&gt;
george fisher
kevin brown
tom clark
clark tom
whitney john
peter mitchell
thomas leonard
chris robinson
edward parker
scott austin
ward tony
michael davidson
evans terry
roger edwards
carlos rivero
</figure>
<figureCaption confidence="0.98593303125">
thomas c leonard
edward harper parker
austin wakeman scott
vivian leonard thomas
stuart h walker
screamin scott simon
robert arthur morton
peter d mitchell
nancy johnson srebro
marinda nancy johnson
marc anthony danza
kevin j brown
john william sherman
john sherman jr
john henry barnes
john e pepper
jeremy robert johnson
jack denton scott
henry christopher
graham e smith
george wescott fisher
george lemuel woods
george d woods
george baby woods
frank peter lehmann
edward john clifton
donald m fraser
donald d mitchell
darien graham smith
charles william jarvis
barbara l johnson
arthur william sampson
</figureCaption>
<figure confidence="0.951360666666667">
40
30
20
10
&gt;
0
0
#Snippets (1000x)--&gt;
444
eEF\W(0/1)
e E W (0/1 acc.)
.75
.62
.42
.65
</figure>
<table confidence="0.969165">
TMI A0
entity
0/1 accuracy .714 .562
Pooled recall .764 .869
Precision .714 .562
F1 .738 .683 TMI
A0
</table>
<figureCaption confidence="0.838301">
Fig. 13: Snippet summary for F and payload corpus C.
</figureCaption>
<bodyText confidence="0.999968821428571">
bias. Because the editors are trained professionals
(unlike Mechanical Turks), we increased our evalu-
ation coverage by having each type or entity assign-
ment reviewed by one editor.
Pooling: Ideally, editors can be asked to find the
best type or entity for each entity or snippet, but,
given the size and diversity of Freebase, the cogni-
tive burden would be unacceptable. In the Wikipedia
corpus CW, a snippet marked ⊥ (no entity) by an al-
gorithm can be judged a loss of recall if Wikipedia
ground truth annotates it with an entity. Unfortu-
nately, this is no longer practical for Web corpus C,
because 8,217 snippets marked ⊥ would have to be
manually inspected and compared with a large num-
ber of candidate entities in Freebase. Therefore, we
adopt pooling as in TREC. (Although the pool is
small, A0 has very high recall.) Recall is evaluated
with respect to the union of snippets annoted with a
non-⊥ entity by at least one competing algorithm,
with agreement in case of more than one.
0/1 Type accuracy: Editors judged each pro-
posed type as correct or not. Unlike in §5.1, where
the true and proposed types could be compared via
M&amp;W (Milne and Witten, 2008), they could not be
asked to objectively estimate relatedness between
types. Therefore we present only their reported post-
hoc 0/1 accuracy for types: T0 and TMI have 0/1
type accuracy of 0.828 and 0.818.
</bodyText>
<subsectionHeader confidence="0.670851">
5.2.3 Snippet annotation results
</subsectionHeader>
<bodyText confidence="0.9991164">
Given the large gap between TMI and Z0 in the
easier setup in §5.1, we no longer consider Z0, and
instead focus on TMI vs. A0. The summary com-
parison of A0 vs. TMI is shown in Fig. 13. Here
TMI’s absolute gains in 0/1 accuracy and F1 are
even larger than in §5.1. To understand TMI’s per-
formance across a diversity of Freebase entity nodes
e, as a function of 1. the size and richness of N(e),
and 2. the number of snippets claimed to mention e,
we disaggregate the data of Fig. 13 into buckets of
</bodyText>
<figureCaption confidence="0.744106">
Fig. 14: 0/1 accuracy and F1 for snippets, payload corpus
C and entities in F.
</figureCaption>
<table confidence="0.998265666666666">
Snippet label judgements %
TMI ok, FACC1 ok, neither ⊥ 22
TMI ok, FACC1 wrong, neither ⊥ 6
TMI6= ⊥ ok, FACC1=⊥ wrong 40
TMI6= ⊥ wrong, FACC1=⊥ 23
TMI wrong, FACC1 wrong, neither ⊥ 2
TMI= ⊥ wrong, FACC16= ⊥ correct 4
TMI= ⊥, FACC16= ⊥, wrong 3
(TMI= ⊥, FACC1= ⊥, not judged) -
</table>
<figureCaption confidence="0.940585">
Fig. 15: TMI vs. FACC1 comparison.
</figureCaption>
<bodyText confidence="0.999949777777778">
consecutive degrees, roughly balancing the number
of snippets per bucket, as shown in Fig. 14. At the
very low end of almost disconnected entity nodes,
no algorithm does very well, because these entities
are also hardly ever mentioned. When the entity is
popular and well-connected, TMI’s benefits are rela-
tively modest. TMI’s gains are best in the mid-range
of degrees. The gap narrows for large-degree nodes,
which is expected.
</bodyText>
<subsectionHeader confidence="0.997376">
5.3 Comparison with FACC1
</subsectionHeader>
<bodyText confidence="0.999795090909091">
After collecting our pool of snippets as in §5.2.2,
we consulted FACC1 (Gabrilovich et al., 2013), and
passed on FACC1 annotations to our editors. As be-
fore, the identity of the algorithm was concealed.
Results are shown in Fig. 15. In a large 40% of
cases, TMI labels correctly while FACC1 backs off.
The converse, where FACC1 backs off and TMI
makes a mistake, is about half as frequent. These
preliminary numbers suggest that TMI is able to
push recall beyond FACC1 while also giving better
precision.
</bodyText>
<figure confidence="0.999203533333333">
Degree--&gt; 0...2 3...6 7...9 &gt;=10
Degree--&gt; 0...2 3...6 7...9 &gt;=10
Accuracy--&gt;
0.8
0.7
0.6
0.5
0.4
0.3
0.62
0.49
TMI
A0
0.75 0.70 0.72
0.53
0.46
0.64
Pooled F1--&gt;
0.8
0.7
0.6
0.5
0.620.59
TMI
A0
0.73
0.66
0.68
0.61
0.790.75
</figure>
<page confidence="0.997559">
445
</page>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999005">
We presented a formal model for bootstrapping from
YAGO types and entities annotated in Wikipedia to
two tasks, 1. annotating Web snippets with Freebase
entities, and 2. associating Freebase entities with
YAGO types. We presented TMI, a system to solve
the two tasks jointly. Experiments show that TMI’s
snippet annotation accuracy, especially for relatively
weakly-connected Freebase entities, is superior to
baselines. We aim to extend from people to all major
Freebase categories, and larger Web crawls.
Acknowledgment: We are grateful to Shrikant
Naidu, Muthusamy Chelliah, and the editors from
Yahoo! for their generous support. Shashank Gupta
helped process FACC1 data.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938402597403">
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In EMNLP Con-
ference, pages 708–716.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In ACL Conference, pages 363–370.
Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag
Subramanya. 2013. FACC1: Freebase annotation
of ClueWeb corpora. http://lemurproject.or
g/clueweb12/, June. Version 1 (Release date 2013-
06-26, Format version 1, Correction level 0).
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR Confer-
ence, pages 267–274. ACM.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in Web text: A graph-based method. In
SIGIR Conference, pages 765–774.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum.
2011. Robust disambiguation of named entities in text.
In EMNLP Conference, pages 782–792, Edinburgh,
Scotland, UK, July. SIGDAT.
Vladimir Kolmogorov. 2006. Convergent tree-
reweighted message passing for energy minimization.
IEEE PAMI, 28(10):1568–1583, October.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In SIGKDD Confer-
ence, pages 457–466.
Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine Learning, 81(1):53–67, October.
Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyEngine: Answering entity-relationship queries using
shallow semantics. In CIKM, October. (demo).
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: detecting and typing unlinkable en-
tities. In EMNLP Conference, pages 893–903.
R Mihalcea and A Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM, pages
233–242.
David Milne and Ian H Witten. 2008. Learning to link
with Wikipedia. In CIKM, pages 509–518.
Ndapandula Nakashole, Tomasz Tylenda, and Gerhard
Weikum. 2013. Fine-grained semantic typing of
emerging entities. In ACL Conference.
Patrick Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent
modeling. In ACL Conference, pages 563–571, Jeju
Island, Korea, July.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for
disambiguation to Wikipedia. In ACL Conference,
ACL/HLT, pages 1375–1384, Portland, Oregon.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In EMNLP Conference, pages 1524–
1534, Edinburgh, UK. ACL.
G Salton and M J McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing joint query interpretation and response ranking. In
WWW Conference, Brazil.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In WWW Con-
ference, pages 697–706. ACM Press.
ChengXiang Zhai. 2008. Statistical language models
for information retrieval: A critical review. Founda-
tions and Trends in Information Retrieval, 2(3):137–
213, March.
Zhicheng Zheng, Xiance Si, Fangtao Li, Edward Y.
Chang, and Xiaoyan Zhu. 2012. Entity disambigua-
tion with Freebase. In Web Intelligence Conference,
WI-IAT ’12, pages 82–89.
</reference>
<page confidence="0.999112">
446
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864116">
<title confidence="0.999075">Joint Bootstrapping of Corpus Annotations and Entity Types</title>
<author confidence="0.997518">Mohapatra Siddhanth Jain Soumen</author>
<affiliation confidence="0.93504">IIT Bombay</affiliation>
<abstract confidence="0.997098740740741">Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="2755" citStr="Cucerzan, 2007" startWordPosition="415" endWordPosition="416">nek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4</context>
<context position="8034" citStr="Cucerzan, 2007" startWordPosition="1304" endWordPosition="1305">tail entities. TMI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nationality UK 0ggbn</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In EMNLP Conference, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL Conference,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="34169" citStr="Finkel et al., 2005" startWordPosition="5775" endWordPosition="5778">bution for Web corpus C. Unambiguous names are usually fully expanded and very rare, if at all present, in evaluated snippets. Fig. 12: Confusion cluster size distribution for Web corpus C. loaded into a trie used by a map-reduce job to extract documents, then snippets, from the corpus. Some phrases in Figs. 10 and 11 have overwhelming numbers of pages with matches. In production, we naturally want all of them to be annotated. But human editorial judgment being the bottleneck, we sampled 50% or 50,000 snippets, whichever was smaller. Starting with about 752,450 pages, we ran the Stanford NER (Finkel et al., 2005) to mark person spans. Pages with fewer than five non-person tokens per person were discarded; this effectively discarded long list pages without any informative text to disambiguate anyone, and left us with 574,135 pages. From these we collected 304,309 snippets where the mention phrase is marked by the NER as a person. Each seed phrase leads to one cluster on which TMI and A0 are run. Note that L must be allowed on the open Web. 5.2.2 Editorial judgment Finally, for each algorithm, about 634 entity-type and about 4500 snippet-entity assignments are randomly sampled and sent to 20 editors in </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In ACL Conference, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Michael Ringgaard</author>
<author>Amarnag Subramanya</author>
</authors>
<title>FACC1: Freebase annotation of ClueWeb corpora. http://lemurproject.or g/clueweb12/,</title>
<date>2013</date>
<contexts>
<context position="2933" citStr="Gabrilovich et al., 2013" startWordPosition="444" endWordPosition="447"> spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias toward often-mentioned, richly-connected “head” entities. Hereafter, Wikipedia entities are called W. Freebase has fewer editorial controls, but has at least</context>
<context position="7634" citStr="Gabrilovich et al., 2013" startWordPosition="1240" endWordPosition="1243">ties in F n W and 5500 snippets from Wikipedia text, where it visibly improves upon baselines and a recently proposed alternative method (Zheng et al., 2012). Second, we resort to extensive manual evaluation of annotation on ClueWeb12 Web text with Freebase entities, by professional editors at a commercial search company. TMI again clearly outperforms strong baselines, doing particularly well for nascent or tail entities. TMI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 201</context>
<context position="11378" citStr="Gabrilovich et al., 2013" startWordPosition="1812" endWordPosition="1815">s to bootstrap a classifier, then apply it to unlabeled, ambiguous mentions, creating more training data. They use a per-type language model like us (§3.3), but this is used as a secondary cause for (word) feature generation, supplementing and smoothing entity-specific language models. In contrast, we use a rigorous graphical model to combine new signals, not depending on naturally unambiguous mentions. Finally, in the interest of fully automated evaluation, they limit their experiments to F fl W and Wikipedia corpus, thus differing critically from our human evaluation on F and a Web corpus. (Gabrilovich et al., 2013) have recently released FACC1: annotations of ClueWeb09 and ClueWeb12 with Freebase entities. Their algorithm is not yet public. They report: “Due to the sheer size of the data, it was not possible to verify all the automatic annotations manually. Based on a small-scale human evaluation, the precision ... is believed to be around 80–85%. Estimating the recall is of course difficult; however, it is believed to be around 70- 85%.” In §5, we will see that, for people entities, TMI greatly increases recall beyond FACC1, keeping precision unimpaired. 3 The three signals Fig. 1 shows three Freebase </context>
<context position="25999" citStr="Gabrilovich et al., 2013" startWordPosition="4383" endWordPosition="4386">rk. 5http://www.nist.gov/tac/2013/KBP/ 5 Experiments We focus our experiments on one broad type of entities, people, that is more challenging for disambiguators than typical showcase examples of distinguishing (Steve) Jobs from employment and Apple Inc. from fruit. We report on three sets of experiments. In §5.1, we restrict to entities from F ∩ W and Wikipedia text, for which ground truth annotation is available. In §5.2, we evaluate TMI and baselines on ClueWeb12 and entities from Freebase, not limited to Wikipedia. In §5.3, we compare TMI with Google’s recently published FACC1 annotations (Gabrilovich et al., 2013). 5.1 Reference corpus CW with F ∩ W entities Limited to people, |F |= 2323792, |W |= 807381, |F \ W |= 1544942, and |F ∩ W |= 778850. It is easiest to evaluate TMI and others on Wikipedia entities. They have known YAGO types. Wikipedia text has explicit (disambiguated) entity annotations. For these reasons, the few known systems for Freebase-based annotation (Zheng et al., 2012) are evaluated exclusively on F ∩ W. 5.1.1 Seeding and setup People in F ∩ W are known by one or more mention words/phrases. From these, we collect mention phrases along with the candidate entity set for each phrase. T</context>
<context position="38928" citStr="Gabrilovich et al., 2013" startWordPosition="6609" endWordPosition="6612">g 3 (TMI= ⊥, FACC1= ⊥, not judged) - Fig. 15: TMI vs. FACC1 comparison. consecutive degrees, roughly balancing the number of snippets per bucket, as shown in Fig. 14. At the very low end of almost disconnected entity nodes, no algorithm does very well, because these entities are also hardly ever mentioned. When the entity is popular and well-connected, TMI’s benefits are relatively modest. TMI’s gains are best in the mid-range of degrees. The gap narrows for large-degree nodes, which is expected. 5.3 Comparison with FACC1 After collecting our pool of snippets as in §5.2.2, we consulted FACC1 (Gabrilovich et al., 2013), and passed on FACC1 annotations to our editors. As before, the identity of the algorithm was concealed. Results are shown in Fig. 15. In a large 40% of cases, TMI labels correctly while FACC1 backs off. The converse, where FACC1 backs off and TMI makes a mistake, is about half as frequent. These preliminary numbers suggest that TMI is able to push recall beyond FACC1 while also giving better precision. Degree--&gt; 0...2 3...6 7...9 &gt;=10 Degree--&gt; 0...2 3...6 7...9 &gt;=10 Accuracy--&gt; 0.8 0.7 0.6 0.5 0.4 0.3 0.62 0.49 TMI A0 0.75 0.70 0.72 0.53 0.46 0.64 Pooled F1--&gt; 0.8 0.7 0.6 0.5 0.620.59 TMI A</context>
</contexts>
<marker>Gabrilovich, Ringgaard, Subramanya, 2013</marker>
<rawString>Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. http://lemurproject.or g/clueweb12/, June. Version 1 (Release date 2013-06-26, Format version 1, Correction level 0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Xueqi Cheng</author>
<author>Hang Li</author>
</authors>
<title>Named entity recognition in query.</title>
<date>2009</date>
<booktitle>In SIGIR Conference,</booktitle>
<pages>267--274</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1744" citStr="Guo et al., 2009" startWordPosition="256" endWordPosition="259">0 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain. 1 Introduction Thanks to automatic information extraction and semantic Web efforts, keyword search over unstructured Web text is rapidly evolving toward entityand type-oriented queries (Guo et al., 2009; Pantel et al., 2012) over semi-structured databases such as Wikipedia, Freebase, and other forms of Linked Data. A key enabling component for such enhanced search capability is a type and entity catalog. This includes a directed acyclic graph of types under the subTypeOf relation between types, and entities attached to one or more types via instanceOf edges. ∗soumen@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are</context>
</contexts>
<marker>Guo, Xu, Cheng, Li, 2009</marker>
<rawString>Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In SIGIR Conference, pages 267–274. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
<author>Jun Zhao</author>
</authors>
<title>Collective entity linking in Web text: A graph-based method.</title>
<date>2011</date>
<booktitle>In SIGIR Conference,</booktitle>
<pages>765--774</pages>
<marker>Han, Le Sun, Zhao, 2011</marker>
<rawString>Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in Web text: A graph-based method. In SIGIR Conference, pages 765–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>782--792</pages>
<publisher>SIGDAT.</publisher>
<location>Edinburgh, Scotland, UK,</location>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In EMNLP Conference, pages 782–792, Edinburgh, Scotland, UK, July. SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kolmogorov</author>
</authors>
<title>Convergent treereweighted message passing for energy minimization.</title>
<date>2006</date>
<journal>IEEE PAMI,</journal>
<volume>28</volume>
<issue>10</issue>
<contexts>
<context position="24965" citStr="Kolmogorov, 2006" startWordPosition="4215" endWordPosition="4216"> words in m using the best available language model associated with some type. For a different entity Em = eo and type Tep = t assignment to win, αφm(e0)+βφe0(t)+ψe0,m(t, eo) must exceed the null score above. This provides a usable recall/precision handle: we modify φm(⊥m) to a tuned number; making it smaller generally gives higher recall and lower precision. 4.4 Inference and training The goal of collective inference will be to assign a type value to each Te and an entity value to each Em. We seek the maximum a-posteriori (MAP) label, for which we use tree-reweighted message passing (TRW-S) (Kolmogorov, 2006). Our graph has plenty of bipartite cycles, so inference is approximate. Given the sparsity of data, we preferred to delexicalize our objective (1), i.e., avoid word-level features and pre-aggregate their signals via timetested aggregators (such as TFIDF cosine). As a result we have only two free parameters α, β in (1), which we tune via grid search. A more principled training regimen is left for future work. 5http://www.nist.gov/tac/2013/KBP/ 5 Experiments We focus our experiments on one broad type of entities, people, that is more challenging for disambiguators than typical showcase examples</context>
</contexts>
<marker>Kolmogorov, 2006</marker>
<rawString>Vladimir Kolmogorov. 2006. Convergent treereweighted message passing for energy minimization. IEEE PAMI, 28(10):1568–1583, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in Web text.</title>
<date>2009</date>
<booktitle>In SIGKDD Conference,</booktitle>
<pages>457--466</pages>
<contexts>
<context position="2802" citStr="Kulkarni et al., 2009" startWordPosition="421" endWordPosition="424">g by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias toward often-mentioned, ri</context>
<context position="8081" citStr="Kulkarni et al., 2009" startWordPosition="1310" endWordPosition="1313">accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nationality UK 0ggbn2k Athlete American football player type type e</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of Wikipedia entities in Web text. In SIGKDD Conference, pages 457–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<volume>81</volume>
<issue>1</issue>
<contexts>
<context position="16843" citStr="Lao and Cohen, 2010" startWordPosition="2736" endWordPosition="2739">llect path labels around negative instances we well, and submit positive and negative path labels to a binary classifier to canReachable 0.8 0.6 0.4 0.2 0 1 2 3 Hops 1 Whole Prune1 Prune2 Prune3 439 cel the effect of frequent but non-informative path types. Second, indiscriminate expansion around e is infeasible because the Freebase graph has very small diameter. Even after substantial pruning, paths of length 3 and 4 reach over 40% and 96% of all nodes (Fig. 3). This increases computational burden and floods us with noisy and spurious paths. We remedy the problem using an idea from PathRank (Lao and Cohen, 2010). Instead of trying to explore all paths originating (or terminating) at e, where e may or may not belong to a target type, we focus on paths between e and other known members of the target type. 3.3 Type “language model” To exploit the second signal, shown in Fig. 1(b), we need to model the association between target YAGO types and the mention contexts of Wikipedia entities known to belong to those types. This model component is in the same spirit as (Zheng et al., 2012). For each target YAGO type t, we sample positive entities e E F n W, and for each e, we collect, from Wikipedia annotated t</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Machine Learning, 81(1):53–67, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaonan Li</author>
<author>Chengkai Li</author>
<author>Cong Yu</author>
</authors>
<title>EntityEngine: Answering entity-relationship queries using shallow semantics.</title>
<date>2010</date>
<booktitle>In CIKM,</booktitle>
<contexts>
<context position="2607" citStr="Li et al., 2010" startWordPosition="395" endWordPosition="398">types under the subTypeOf relation between types, and entities attached to one or more types via instanceOf edges. ∗soumen@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be use</context>
</contexts>
<marker>Li, Li, Yu, 2010</marker>
<rawString>Xiaonan Li, Chengkai Li, and Cong Yu. 2010. EntityEngine: Answering entity-relationship queries using shallow semantics. In CIKM, October. (demo).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>No noun phrase left behind: detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>893--903</pages>
<contexts>
<context position="10336" citStr="Lin et al., 2012" startWordPosition="1650" endWordPosition="1653">s of 0ggbn2k match snippet and salient text, and type links are also available. (b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippet to 03nmvfz. (c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text, which also match neighbors of 0bhbqmm. of emerging entities (related to our secondary goal). In concurrent work, (Nakashole et al., 2013) propose integer linear program formulations for inferring types of emerging entities from the way their mentions are embedded in curated relation-revealing phrases. (Lin et al., 2012) earlier approached the problem using weight propagation in a bipartite graph connecting unknown to known entities via textual relation patterns. Both note that this can boost mention disambiguation accuracy. The closest work to ours is by (Zheng et al., 2012): they use semisupervised learning to annotate a corpus with Freebase entities. Like (Milne and Witten, 2008), they depend on unambiguous entity-mention pairs to bootstrap a classifier, then apply it to unlabeled, ambiguous mentions, creating more training data. They use a per-type language model like us (§3.3), but this is used as a seco</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun phrase left behind: detecting and typing unlinkable entities. In EMNLP Conference, pages 893–903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In CIKM,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="2739" citStr="Mihalcea and Csomai, 2007" startWordPosition="411" endWordPosition="414">@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low en</context>
<context position="8018" citStr="Mihalcea and Csomai, 2007" startWordPosition="1299" endWordPosition="1303">ularly well for nascent or tail entities. TMI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nati</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>R Mihalcea and A Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In CIKM, pages 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia. In</title>
<date>2008</date>
<booktitle>CIKM,</booktitle>
<pages>509--518</pages>
<contexts>
<context position="2779" citStr="Milne and Witten, 2008" startWordPosition="417" endWordPosition="420">) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias tow</context>
<context position="8058" citStr="Milne and Witten, 2008" startWordPosition="1306" endWordPosition="1309">MI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nationality UK 0ggbn2k Athlete American foot</context>
<context position="10705" citStr="Milne and Witten, 2008" startWordPosition="1707" endWordPosition="1710">d to our secondary goal). In concurrent work, (Nakashole et al., 2013) propose integer linear program formulations for inferring types of emerging entities from the way their mentions are embedded in curated relation-revealing phrases. (Lin et al., 2012) earlier approached the problem using weight propagation in a bipartite graph connecting unknown to known entities via textual relation patterns. Both note that this can boost mention disambiguation accuracy. The closest work to ours is by (Zheng et al., 2012): they use semisupervised learning to annotate a corpus with Freebase entities. Like (Milne and Witten, 2008), they depend on unambiguous entity-mention pairs to bootstrap a classifier, then apply it to unlabeled, ambiguous mentions, creating more training data. They use a per-type language model like us (§3.3), but this is used as a secondary cause for (word) feature generation, supplementing and smoothing entity-specific language models. In contrast, we use a rigorous graphical model to combine new signals, not depending on naturally unambiguous mentions. Finally, in the interest of fully automated evaluation, they limit their experiments to F fl W and Wikipedia corpus, thus differing critically fr</context>
<context position="28170" citStr="Milne and Witten, 2008" startWordPosition="4747" endWordPosition="4750">two tasks simultaneously: assign types to entities and entities to snippets. So the first baseline, T0, is one that solves the typing task separately, and the second, A0, does snippet annotation separately. A third baseline, Z0, from (Zheng et al., 2012) does only snippet annotation; they do not consider typing entities. While evaluating types output by TMI and T0 against ground truth, we may wish to assign partial credit for overlapping types, e.g., athlete vs. soccer player, because our types form an incomplete hierarchy. We use the standard “M&amp;W” score of semantic similarity between types (Milne and Witten, 2008) for this. As regards snippet annotation, Z0 (Zheng et al., 2012) does not specify any mechanism for handling ⊥. Therefore we run two sets of experiments. In one we eliminate all snippets with ground truth ⊥. A0, and TMI are also debarred from returning ⊥ for any snippet. In the other, snippets marked ⊥ in ground truth are included. A0 and TMI are enabled to return ⊥, but Z0 cannot. TMI A0 Z0 0/1 snippet accuracy 0.827 0.699 0.627 Fig. 6: Snippet annotation on CW corpus, F ∩W entities, ⊥ not allowed. TMI A0 Z0 0/1 snippet accuracy 0.7307 0.651 0.622 Snippet precision 0.858 0.843 0.622 Snippet </context>
<context position="37250" citStr="Milne and Witten, 2008" startWordPosition="6307" endWordPosition="6310">tunately, this is no longer practical for Web corpus C, because 8,217 snippets marked ⊥ would have to be manually inspected and compared with a large number of candidate entities in Freebase. Therefore, we adopt pooling as in TREC. (Although the pool is small, A0 has very high recall.) Recall is evaluated with respect to the union of snippets annoted with a non-⊥ entity by at least one competing algorithm, with agreement in case of more than one. 0/1 Type accuracy: Editors judged each proposed type as correct or not. Unlike in §5.1, where the true and proposed types could be compared via M&amp;W (Milne and Witten, 2008), they could not be asked to objectively estimate relatedness between types. Therefore we present only their reported posthoc 0/1 accuracy for types: T0 and TMI have 0/1 type accuracy of 0.828 and 0.818. 5.2.3 Snippet annotation results Given the large gap between TMI and Z0 in the easier setup in §5.1, we no longer consider Z0, and instead focus on TMI vs. A0. The summary comparison of A0 vs. TMI is shown in Fig. 13. Here TMI’s absolute gains in 0/1 accuracy and F1 are even larger than in §5.1. To understand TMI’s performance across a diversity of Freebase entity nodes e, as a function of 1. </context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H Witten. 2008. Learning to link with Wikipedia. In CIKM, pages 509–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Tomasz Tylenda</author>
<author>Gerhard Weikum</author>
</authors>
<title>Fine-grained semantic typing of emerging entities.</title>
<date>2013</date>
<booktitle>In ACL Conference.</booktitle>
<contexts>
<context position="10152" citStr="Nakashole et al., 2013" startWordPosition="1622" endWordPosition="1625">from page containing contexts: Fig. 1: Signal synergies. Three of the many people mentioned as “John Williams” on the Web are shown, with Freebase MIDs. (a) Easy case where Freebase neighbors of 0ggbn2k match snippet and salient text, and type links are also available. (b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippet to 03nmvfz. (c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text, which also match neighbors of 0bhbqmm. of emerging entities (related to our secondary goal). In concurrent work, (Nakashole et al., 2013) propose integer linear program formulations for inferring types of emerging entities from the way their mentions are embedded in curated relation-revealing phrases. (Lin et al., 2012) earlier approached the problem using weight propagation in a bipartite graph connecting unknown to known entities via textual relation patterns. Both note that this can boost mention disambiguation accuracy. The closest work to ours is by (Zheng et al., 2012): they use semisupervised learning to annotate a corpus with Freebase entities. Like (Milne and Witten, 2008), they depend on unambiguous entity-mention pai</context>
</contexts>
<marker>Nakashole, Tylenda, Weikum, 2013</marker>
<rawString>Ndapandula Nakashole, Tomasz Tylenda, and Gerhard Weikum. 2013. Fine-grained semantic typing of emerging entities. In ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Thomas Lin</author>
<author>Michael Gamon</author>
</authors>
<title>Mining entity types from query logs via user intent modeling.</title>
<date>2012</date>
<booktitle>In ACL Conference,</booktitle>
<pages>563--571</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="1766" citStr="Pantel et al., 2012" startWordPosition="260" endWordPosition="264">dia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain. 1 Introduction Thanks to automatic information extraction and semantic Web efforts, keyword search over unstructured Web text is rapidly evolving toward entityand type-oriented queries (Guo et al., 2009; Pantel et al., 2012) over semi-structured databases such as Wikipedia, Freebase, and other forms of Linked Data. A key enabling component for such enhanced search capability is a type and entity catalog. This includes a directed acyclic graph of types under the subTypeOf relation between types, and entities attached to one or more types via instanceOf edges. ∗soumen@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a menti</context>
</contexts>
<marker>Pantel, Lin, Gamon, 2012</marker>
<rawString>Patrick Pantel, Thomas Lin, and Michael Gamon. 2012. Mining entity types from query logs via user intent modeling. In ACL Conference, pages 563–571, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to Wikipedia.</title>
<date>2011</date>
<booktitle>In ACL Conference, ACL/HLT,</booktitle>
<pages>1375--1384</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="2842" citStr="Ratinov et al., 2011" startWordPosition="429" endWordPosition="432">owed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias toward often-mentioned, richly-connected “head” entities. Hereafte</context>
<context position="8121" citStr="Ratinov et al., 2011" startWordPosition="1318" endWordPosition="1321">rom 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nationality UK 0ggbn2k Athlete American football player type type education Wisconsin Madison John Williams</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In ACL Conference, ACL/HLT, pages 1375–1384, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>1524--1534</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, UK.</location>
<contexts>
<context position="8211" citStr="Ritter et al., 2011" startWordPosition="1332" endWordPosition="1335">h Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/˜soumen/doc/CSAW/. Toourknowledge, this is among the first reports on extensive human evaluation of machine annotation for F \ W on a large Web corpus. 2 Related work The vast majority of entity annotation work (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011) use Wikipedia or derivative knowledge bases. (Ritter et al., 2011) and (Zheng et al., 2012) are notable exceptions. (Ritter et al., 2011) use entity names for distant supervision in POS tagging, chunking and broad named entity typing in short tweets, which are different from our goals. Recently, others have investigated inferring types 2http://lemurproject.org/clueweb12/ 437 ChemSpider academic founder type chemist type Antony John Williams place of birth St Aspah nationality UK 0ggbn2k Athlete American football player type type education Wisconsin Madison John Williams place of birth Muskegon 03nmyfz Harvard University Eunice Kanenstenhawi Williams children</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In EMNLP Conference, pages 1524– 1534, Edinburgh, UK. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="17871" citStr="Salton and McGill, 1983" startWordPosition="2924" endWordPosition="2927">ypes. This model component is in the same spirit as (Zheng et al., 2012). For each target YAGO type t, we sample positive entities e E F n W, and for each e, we collect, from Wikipedia annotated text, a corpus of snippets mentioning e. We remove the mention words and retain the rest. We also collect salient words from the entire Wikipedia document containing the snippet, as shown in Fig. 1. At this point each target type is associated with a “corpus” of contexts, each represented by snippet words. We compute the IDF of all words in this corpus3, and then represent each type as a TFIDF vector (Salton and McGill, 1983). A test context is turned into a similar vector, and its score with respect to t is the cosine between these two vectors. This simple approach was found superior to building a more traditional smoothed multinomial unigram model (Zhai, 2008) for each type. Given the output of this component feeds into an outer discriminative inference mechanism, a strict probabilistic model is not necessary. 3.4 Entity neighborhood match with snippet The third signal is a staple of any disambiguation work: match the occurrence context against the neighborhood in the structured representation. In word sense dis</context>
<context position="19185" citStr="Salton and McGill, 1983" startWordPosition="3148" endWordPosition="3151"> IDF from Wiki text does not work. matches between, say, other words in the context and the WordNet neighborhood of the proposed synset. As in WSD, many approaches to Wikification measure some local consistency between a mention m and the neighborhood N(e) of a candidate entity e. N(e) is again limited by a maximum path length E. From snippet m we extract all phrases P(m) excluding the mention words. For each phrase p E P(m), if p occurs at least once4 in any node of N(e), then we accumulate a credit of |p |E.,P IDF(w), where w ranges over words in p, IDF(w) is its inverse document frequency (Salton and McGill, 1983) in Wikipedia, and |p |is the length of the phrase. This rewards exact phrase matches. Candidate types Mentions in context Fig. 4: Tripartite assignment problem. 4 Unified model Figure 4 abstracts out the signals shown in Figure 1 into a tripartite assignment problem. Each mention like m1 has to choose at most one entity from among candidate aliasing entities like e and e&apos;. Each entity e E F \W has to choose one type (for simplicity we ignore zero or more than one types as possibilities) from candidates like t and t&apos;. The configuration of thick (green) edges should be preferred to alternative </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G Salton and M J McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uma Sawant</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Learning joint query interpretation and response ranking.</title>
<date>2013</date>
<booktitle>In WWW Conference,</booktitle>
<contexts>
<context position="2638" citStr="Sawant and Chakrabarti, 2013" startWordPosition="399" endWordPosition="402">ubTypeOf relation between types, and entities attached to one or more types via instanceOf edges. ∗soumen@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning alg</context>
</contexts>
<marker>Sawant, Chakrabarti, 2013</marker>
<rawString>Uma Sawant and Soumen Chakrabarti. 2013. Learning joint query interpretation and response ranking. In WWW Conference, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In WWW Conference,</booktitle>
<pages>697--706</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2158" citStr="Suchanek et al., 2007" startWordPosition="321" endWordPosition="324">main. 1 Introduction Thanks to automatic information extraction and semantic Web efforts, keyword search over unstructured Web text is rapidly evolving toward entityand type-oriented queries (Guo et al., 2009; Pantel et al., 2012) over semi-structured databases such as Wikipedia, Freebase, and other forms of Linked Data. A key enabling component for such enhanced search capability is a type and entity catalog. This includes a directed acyclic graph of types under the subTypeOf relation between types, and entities attached to one or more types via instanceOf edges. ∗soumen@cse.iitb.ac.in YAGO (Suchanek et al., 2007) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup. Another enabling component is an annotated corpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; M</context>
<context position="5920" citStr="Suchanek et al., 2007" startWordPosition="945" endWordPosition="948">ing: If we had available a suitable type catalog T with associated entities in W, which in turn have known textual mentions, we can build models of contexts referring to types like chemists, sports people and politicans. When faced with people called John Williams in F \W, we may first try to associate them with types. This can then help disambiguate mentions to specific instances of John Williams in F \ W. In principle, useful information may also flow in the reverse direction: words in mention contexts may help assign types to entities in F \ W. For reasons to be made clear, we choose YAGO (Suchanek et al., 2007) as the type catalog T accompanying entities in W. Our contributions: We present TMI, a bootstrapping system for solving the two tasks jointly. Apart from matches between the context of m and entity names in N(e), TMI combines and balances evidence from two other sources to decide if e is mentioned at token span m, and has type t: • a language model for the context in which entities of type t are usually mentioned 1With F=Freebase and W=Wikipedia, F nW Pz� W but not quite; W \ F is small but non-empty. • correlations between t and certain path features generated from N(e). TMI uses a novel pro</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge unifying WordNet and Wikipedia. In WWW Conference, pages 697–706. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
</authors>
<title>Statistical language models for information retrieval: A critical review.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>2</volume>
<issue>3</issue>
<pages>213</pages>
<contexts>
<context position="18112" citStr="Zhai, 2008" startWordPosition="2967" endWordPosition="2968">ion words and retain the rest. We also collect salient words from the entire Wikipedia document containing the snippet, as shown in Fig. 1. At this point each target type is associated with a “corpus” of contexts, each represented by snippet words. We compute the IDF of all words in this corpus3, and then represent each type as a TFIDF vector (Salton and McGill, 1983). A test context is turned into a similar vector, and its score with respect to t is the cosine between these two vectors. This simple approach was found superior to building a more traditional smoothed multinomial unigram model (Zhai, 2008) for each type. Given the output of this component feeds into an outer discriminative inference mechanism, a strict probabilistic model is not necessary. 3.4 Entity neighborhood match with snippet The third signal is a staple of any disambiguation work: match the occurrence context against the neighborhood in the structured representation. In word sense disambiguation (WSD), support for assigning a word in context to a synset comes from 3Generic IDF from Wiki text does not work. matches between, say, other words in the context and the WordNet neighborhood of the proposed synset. As in WSD, man</context>
</contexts>
<marker>Zhai, 2008</marker>
<rawString>ChengXiang Zhai. 2008. Statistical language models for information retrieval: A critical review. Foundations and Trends in Information Retrieval, 2(3):137– 213, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Xiance Si</author>
<author>Fangtao Li</author>
<author>Edward Y Chang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Entity disambiguation with Freebase.</title>
<date>2012</date>
<booktitle>In Web Intelligence Conference, WI-IAT ’12,</booktitle>
<pages>82--89</pages>
<contexts>
<context position="2906" citStr="Zheng et al., 2012" startWordPosition="440" endWordPosition="443">orpus in which token spans (e.g., the word “Albert”) are identified as a mention of an entity (e.g., the Physicist Einstein). Equipped with suitable indices, a catalog and an annotated corpus let us find “scientists who played some musical instrument”, and answer many other powerful classes of queries (Li et al., 2010; Sawant and Chakrabarti, 2013). Consequently, accurate corpus annotation has been intensely investigated (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). With two exceptions (Zheng et al., 2012; Gabrilovich et al., 2013) that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation. However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias toward often-mentioned, richly-connected “head” entities. Hereafter, Wikipedia entities are called W. Freebase has fewer editorial</context>
<context position="7166" citStr="Zheng et al., 2012" startWordPosition="1164" endWordPosition="1167">formulation to integrate these signals. We give a detailed account of our design of node and edge potentials, and a natural reject option (recall/precision tradeoff). We report on extensive experiments using YAGO types, Wikipedia entities and text, Freebase entities, and text from ClueWeb122, a 700-millionpage Web corpus. We focus on all people entities in Wikipedia and Freebase, and provide three kinds of evaluation. First, we evaluate TMI on over 1100 entities in F n W and 5500 snippets from Wikipedia text, where it visibly improves upon baselines and a recently proposed alternative method (Zheng et al., 2012). Second, we resort to extensive manual evaluation of annotation on ClueWeb12 Web text with Freebase entities, by professional editors at a commercial search company. TMI again clearly outperforms strong baselines, doing particularly well for nascent or tail entities. TMI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%. Third, we compare TMI annotations with Google’s FACC1 (Gabrilovich et al., 2013) annotations restricted to people; TMI is significantly better. Our annotations and related data can be downloaded from http://www.c</context>
<context position="10596" citStr="Zheng et al., 2012" startWordPosition="1690" endWordPosition="1693">GO based on snippet and salient text, which also match neighbors of 0bhbqmm. of emerging entities (related to our secondary goal). In concurrent work, (Nakashole et al., 2013) propose integer linear program formulations for inferring types of emerging entities from the way their mentions are embedded in curated relation-revealing phrases. (Lin et al., 2012) earlier approached the problem using weight propagation in a bipartite graph connecting unknown to known entities via textual relation patterns. Both note that this can boost mention disambiguation accuracy. The closest work to ours is by (Zheng et al., 2012): they use semisupervised learning to annotate a corpus with Freebase entities. Like (Milne and Witten, 2008), they depend on unambiguous entity-mention pairs to bootstrap a classifier, then apply it to unlabeled, ambiguous mentions, creating more training data. They use a per-type language model like us (§3.3), but this is used as a secondary cause for (word) feature generation, supplementing and smoothing entity-specific language models. In contrast, we use a rigorous graphical model to combine new signals, not depending on naturally unambiguous mentions. Finally, in the interest of fully au</context>
<context position="17319" citStr="Zheng et al., 2012" startWordPosition="2823" endWordPosition="2826">eases computational burden and floods us with noisy and spurious paths. We remedy the problem using an idea from PathRank (Lao and Cohen, 2010). Instead of trying to explore all paths originating (or terminating) at e, where e may or may not belong to a target type, we focus on paths between e and other known members of the target type. 3.3 Type “language model” To exploit the second signal, shown in Fig. 1(b), we need to model the association between target YAGO types and the mention contexts of Wikipedia entities known to belong to those types. This model component is in the same spirit as (Zheng et al., 2012). For each target YAGO type t, we sample positive entities e E F n W, and for each e, we collect, from Wikipedia annotated text, a corpus of snippets mentioning e. We remove the mention words and retain the rest. We also collect salient words from the entire Wikipedia document containing the snippet, as shown in Fig. 1. At this point each target type is associated with a “corpus” of contexts, each represented by snippet words. We compute the IDF of all words in this corpus3, and then represent each type as a TFIDF vector (Salton and McGill, 1983). A test context is turned into a similar vector</context>
<context position="26381" citStr="Zheng et al., 2012" startWordPosition="4449" endWordPosition="4452">ruth annotation is available. In §5.2, we evaluate TMI and baselines on ClueWeb12 and entities from Freebase, not limited to Wikipedia. In §5.3, we compare TMI with Google’s recently published FACC1 annotations (Gabrilovich et al., 2013). 5.1 Reference corpus CW with F ∩ W entities Limited to people, |F |= 2323792, |W |= 807381, |F \ W |= 1544942, and |F ∩ W |= 778850. It is easiest to evaluate TMI and others on Wikipedia entities. They have known YAGO types. Wikipedia text has explicit (disambiguated) entity annotations. For these reasons, the few known systems for Freebase-based annotation (Zheng et al., 2012) are evaluated exclusively on F ∩ W. 5.1.1 Seeding and setup People in F ∩ W are known by one or more mention words/phrases. From these, we collect mention phrases along with the candidate entity set for each phrase. The number of candidates is the phrase’s degree of ambiguity. We sort phrases by ambiguity and draw a sample over the ambiguity range. This gives us seed phrases with representative ambiguity. Then we collect all entities mentioned by these phrases. Overall we collect about 1100 entities and 5500 distinct mentions. Contrast this with (Zheng et al., 2012), who sample entities from </context>
<context position="27801" citStr="Zheng et al., 2012" startWordPosition="4687" endWordPosition="4690">an, the politician disappears, leaving a naturally unambiguous alias. I.e., (Zheng et al., 2012) did not “complete” their entity sets with aliased entities. For all these reasons, Z0 numbers here are not directly comparable to those in their paper. 442 5.1.2 Tasks and baselines The structure of TMI suggests two natural baselines to compare against it. TMI solves two tasks simultaneously: assign types to entities and entities to snippets. So the first baseline, T0, is one that solves the typing task separately, and the second, A0, does snippet annotation separately. A third baseline, Z0, from (Zheng et al., 2012) does only snippet annotation; they do not consider typing entities. While evaluating types output by TMI and T0 against ground truth, we may wish to assign partial credit for overlapping types, e.g., athlete vs. soccer player, because our types form an incomplete hierarchy. We use the standard “M&amp;W” score of semantic similarity between types (Milne and Witten, 2008) for this. As regards snippet annotation, Z0 (Zheng et al., 2012) does not specify any mechanism for handling ⊥. Therefore we run two sets of experiments. In one we eliminate all snippets with ground truth ⊥. A0, and TMI are also d</context>
<context position="29863" citStr="Zheng et al., 2012" startWordPosition="5043" endWordPosition="5046"> which is better than Z0 and the uninformed references. This is despite training Z0’s per-type topic models not only on unambiguous Fig. 8: Bucketed comparison between TMI and baselines, F ∩ W, ⊥ allowed. TMI T0 0/1 type accuracy M&amp;W type accuracy Fig. 9: Type inference, CW corpus, F ∩ W entities. snippets, but also on a disjoint fraction of F∩W, as a surrogate for Wikipedia’s containment in Freebase. Fig. 7 repeats the experiment while allowing ⊥. Here, in 0/1 accuracy, ⊥ is regarded as just another entity. Again, we see that TMI has a clear advantage. Z0’s performance here is worse than in (Zheng et al., 2012). This is explained by our much larger and difficult-to-separate type system. We disaggregate the summary results into buckets, shown in Fig. 8. Each bucket covers a range of degrees of entity nodes in Freebase, while roughly balacing the number of snippets in each bucket. TMI generally shows larger gains for low-degree buckets. 5.1.4 Type prediction results We also compared the type inference accuracy of TMI and T0; (Zheng et al., 2012) do not infer types. The summary is in Fig. 9. Two uninformed baselines are worth mentioning. Uniform random choice over 130 types gave only 2% accuracy. Choss</context>
</contexts>
<marker>Zheng, Si, Li, Chang, Zhu, 2012</marker>
<rawString>Zhicheng Zheng, Xiance Si, Fangtao Li, Edward Y. Chang, and Xiaoyan Zhu. 2012. Entity disambiguation with Freebase. In Web Intelligence Conference, WI-IAT ’12, pages 82–89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.993524">
Identifying Web Search Query Reformulation using Concept based
Matching
</title>
<author confidence="0.978527">
Ahmed Hassan
</author>
<affiliation confidence="0.947824">
Microsoft Research
</affiliation>
<address confidence="0.94735">
One Microsoft Way
Redmond, WA 98053, USA
</address>
<email confidence="0.999268">
hassanam@microsoft.com
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803909090909">
Web search users frequently modify their
queries in hope of receiving better results.
This process is referred to as “Query Refor-
mulation”. Previous research has mainly fo-
cused on proposing query reformulations in
the form of suggested queries for users. Some
research has studied the problem of predicting
whether the current query is a reformulation
of the previous query or not. However, this
work has been limited to bag-of-words models
where the main signals being used are word
overlap, character level edit distance and word
level edit distance. In this work, we show
that relying solely on surface level text sim-
ilarity results in many false positives where
queries with different intents yet similar top-
ics are mistakenly predicted as query reformu-
lations. We propose a new representation for
Web search queries based on identifying the
concepts in queries and show that we can sig-
nificantly improve query reformulation perfor-
mance using features of query concepts.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885844444445">
Web search is a process of querying, learning and
reformulating queries to satisfy certain information
needs. When a user submits a search query, the
search engine attempts to return the best results to
that query. Oftentimes, users modify their search
queries in hope of getting better results. Typical
search users have low tolerance to viewing lowly
ranked search results and they prefer to reformu-
late the query rather than wade through result list-
ings (Jansen and Spink, 2006). Previous stud-
ies have also shown that 37% of search queries
are reformulations to previous queries (Jansen et
al., 2007) and that 52% of users reformulate their
queries (Jansen et al., 2005).
Understanding query reformulation behavior and
being able to accurately identify reformulation
queries have several benefits. One of these benefits
is learning from user behavior to better suggest au-
tomatic query refinements or query alterations. An-
other benefit is using query reformulation predic-
tion to identify boundaries between search tasks and
hence segmenting user activities into topically co-
herent units. Also, if we are able to accurately iden-
tify query reformulations, then we will be in a bet-
ter position to evaluate the satisfaction of users with
query results. For example, search satisfaction is
typically evaluated using clickthrough information
by assuming that if a user clicks on a result, and
possibly dwells for a certain amount of time, then
the user is satisfied. Identifying query reformulation
can be very useful for finding cases where the users
are not satisfied even after a click on a result that
may have seemed relevant given its title and sum-
mary but then turned out to be not relevant to the
user’s information need.
Previous work on query reformulation has either
focused on automatic query refinement by the search
system, e.g. (Jones et al., 2006; Boldi et al.,
2008) or on defining taxonomies for query refor-
mulation strategies, e.g. (Lau and Horvitz, 1999;
Anick, 2003). Other work has proposed solutions
for the query reformulation prediction problem or
for the similar problem of task boundary identifi-
cation (Radlinski and Joachims, 2005; Jones and
Klinkner, 2008). These solutions have adopted the
</bodyText>
<page confidence="0.866212">
1000
</page>
<note confidence="0.728692">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999945967741935">
bag-of-words approach for representing queries and
mostly used features of word overlap or character
and word level edit distances. Take the queries “ho-
tels in New York City” and “weather in New York
City” as an example. The two queries are very likely
to have been issued by a user who is planning to
travel to New York City. The two queries have 5
words each, 4 of them are shared by the two queries.
Hence, most of the solutions proposed in previous
work for this problem will incorrectly assume that
the second query is a reformulation of the first due
to the high word overlap ratio and the small edit dis-
tance. In this work, we propose a method that goes
beyond the bag-of-words method by identifying the
concepts underlying these queries. In the previous
example, we would like our method to realize that
in the first query, the user is searching for “hotels”
while for in the second query, she is searching for the
“weather” in New York City. Hence, despite similar
in terms of shared terms, the two queries have differ-
ent intents and are not reformulations of one another.
To this end, we conducted a study where we col-
lected thousands of consecutive queries and trained
judges to label them as either reformulations or
not. We then built a classifier to identify query
reformulation pairs and showed that the proposed
classifier outperforms the state-of-the-art methods
on identifying query reformulations. The proposed
method significantly reduces false positives (non-
reformulation pairs incorrectly classified as refor-
mulation) while achieving high recall and precision.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999906">
There are three areas of work related to the research
presented in this paper: (i) query reformulation tax-
onomies, (ii) automatic query refinement, and (iii)
search tasks boundary identification. We cover each
of these areas in turn.
</bodyText>
<subsectionHeader confidence="0.973828">
2.1 Query Reformulation Taxonomies
</subsectionHeader>
<bodyText confidence="0.999973864864865">
Existing research has studied how web search en-
gines can propose reformulations, but has given less
attention to how people perform query reformula-
tions. Most of the research on manual query re-
formulation has focused on building taxonomies of
query reformulation. These taxonomies are gener-
ally constructed by examining a small set of query
logs. Anick (2003) classified a random sample of
100 reformulations by hand into eleven categories.
Jensen et al. (2007) identified 6 different kinds
of reformulation states (New, Assistance, Content
Change, Generalization, Reformulation, and Spe-
cialization) and provided heuristics for identifying
them. They also used them to predict when a user
is most receptive to automatic query suggestions.
The same categories were used in several other stud-
ies (Guo et al., 2008; Lau and Horvitz, 1999).
Huang and Efthimis (2010) proposed another re-
formulation taxonomy. Their taxonomy was lexi-
cal in nature (e.g., word reorder, adding words, re-
moving words, etc.). They also proposed the use of
regular expressions to identify them. While study-
ing re-finding behavior, Teevan et al. (2006) con-
structed a taxonomy of query re-finding by manu-
ally examining query logs, and implemented algo-
rithms to identify repeat queries, equal click queries
and overlapping click queries. None of this work
has built an automatic classifier distinguishing refor-
mulation queries from other queries. Heuristics and
regular expressions have been used in (Huang et al.,
2010) and (Jansen et al., 2007) to identify different
types of reformulations. This line of work is relevant
to our work because it studies query reformulation
strategies. Our work is different because we build a
machine-learned predictive model to identify query
reformulation while this line of work mainly focuses
on defining taxonomies for reformulation strategies.
</bodyText>
<subsectionHeader confidence="0.997467">
2.2 Automatic Query Refinement
</subsectionHeader>
<bodyText confidence="0.9998835625">
A close problem that has received most of the re-
search attention in this area is the problem of auto-
matically generating query refinements. These re-
finements are typically offered as query suggestions
to the users or used to alter the user query before
submitting it to the search engine.
Boldi et al. (2008) introduced the concept of
the query-flow graph where every query is repre-
sented by a node and edges connect queries if it is
likely for users to move from one query to another.
Mei et al. (2008) used random walks over a bipar-
tite graph of queries and URLs to find query refine-
ments. Query logs were used to suggest query re-
finements in (Baeza-Yates et al., 2005). Hierarchi-
cal agglomerative clustering was used to group sim-
ilar queries that can be used as suggestions for one
</bodyText>
<page confidence="0.984204">
1001
</page>
<bodyText confidence="0.999096571428571">
another. Other research has adopted methods based
on query expansion (Mitra et al., 1998) or query
substitution (Jones et al., 2006). This line of work
is different from our work because it focuses on au-
tomatically generating query refinements while this
work focuses on identifying cases of manual query
reformulations.
</bodyText>
<subsectionHeader confidence="0.99976">
2.3 Search Task Boundary Identification
</subsectionHeader>
<bodyText confidence="0.999836729166667">
The problem of classifying the boundaries of the
user search tasks within sessions in web search logs
has been widely addressed before. This problem is
closely related to the problem of identifying query
reformulation. A search task has been defined in
(Jones and Klinkner, 2008) as a single information
need that may result in one or more queries. Sim-
ilarly , Jansen et al. (2007) defined a session as
a series of interactions by the user toward address-
ing a single information need. On the other hand, a
query reformulation is intended to modify a previ-
ous query in hope of getting better results to satisfy
the same information need. From these definitions,
it is clear how query reformulation and task bound-
ary detection are two sides of the same problem.
Boldi et al. (2008) presented the concept of the
query-flow graph. A query-flow graph represents
chains of related queries in query logs. They use
this model for finding logical session boundaries and
query recommendation. Ozmutlu (2006) proposed
a method for identifying new topics in search logs.
He demonstrated that time interval, search pattern
and position of a query in a user session, are ef-
fective for shifting to a new topic. Radlinski and
Joachims (2005) study sequences of related queries
(query chains). They used that to generate new types
of preference judgments from search engine logs to
learn better ranked retrieval functions.
Arlitt (2000) found session boundaries using a
calculated timeout threshold. Murray et al. (2006)
extended this work by using hierarchical clustering
to find better timeout values to detect session bound-
aries. Jones and Klinkner (2008) also addressed
the problem of classifying the boundaries of the
goals and missions in search logs. They showed
that using features like edit distance and common
words achieves considerably better results compared
to timeouts. Lucchese et al. (Lucchese et al., 20011)
uses a similar set of features as (Jones and Klinkner,
2008), but uses clustering to group queries in the
same task together as opposed to identifying task
boundary as in (Jones and Klinkner, 2008). This
line of work is perhaps the closest to our work. Our
work is different because it goes beyond the bag of
words approach and tries to assess query similarity
based on the concepts represented in each query. We
compare our work to the state-of-the-art work in this
area later in this paper.
</bodyText>
<sectionHeader confidence="0.988054" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.99783915">
We start by defining some terms that will be used
throughout the paper:
Definition: Query Reformulation is the act of
submitting a query Q2 to modify a previous search
query Q1 in hope of retrieving better results to sat-
isfy the same information need.
Definition: A Search Session is group of queries
and clicks demarcated with a 30-minute inactiv-
ity timeout, such as that used in previous work
(Downey et al., 2007; Radlinski and Joachims,
2005).
Search engines receive streams of queries from
users. In response to each query, the engine returns a
set of search results. Depending on these results, the
user may decide to click on one or more results, sub-
mit another query, or end the search session. In this
work, we focus on cases where the user submits an-
other query. Our objective is to solve the following
problem: Given a query Q1, and the following query
Q2, predict whether Q2 is reformulation of Q1.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999943666666667">
In this section, we propose methods for predicting
whether the current query has been issued by the
user to reformulate the previous query.
</bodyText>
<subsectionHeader confidence="0.996664">
4.1 Query Normalization
</subsectionHeader>
<bodyText confidence="0.999959125">
We perform standard normalization where we re-
place all letters with their corresponding lower case
representation. We also replace all runs of whites-
pace characters with a single space and remove any
leading or trailing spaces. In addition to the stan-
dard normalization, we also break queries that do not
respect word boundaries into words. Word break-
ing is a well-studied topic that has proved to be
</bodyText>
<page confidence="0.997325">
1002
</page>
<tableCaption confidence="0.927473">
Table 1 : Examples of queries, the corresponding segmentation, and the concept representation.
Phrases are separated by “|” and different tokens in a keyword are separated by “ ”
</tableCaption>
<bodyText confidence="0.92578928">
Query Phrases and Keywords Concept Representation
hotels in new york city hotels in new york city Concept1 {head=“hotels”,
modifiers = “new york city”}
hyundai roadside assistance hyundai roadside assistance Concept1 {head = “roadside
phone number  |phone number assistance”, modifiers = “hyundai”},
Concept2{“phone number”}
kodak easyshare recharger chord kodak easyshare recharger cord Concept1{head =“recharger cord”,
modifiers ¯‘‘kodak easyshare”}
user reviews for apple iphone user reviews for apple iphone Concept1{head=“user reviews”
, modifiers = “apple iphone”}
user reviews for apple ipad user reviews for apple ipad Concept1{head =“user reviews”,
modifiers = “apple ipad”}
tommy bhama rug tommy bhama rug Concept1{head =“rug”,
modifiers ¯‘‘tommy bhama”}
tommy bhama perfume tommy bhama perfume Concept1{head =”perfume”,
modifiers ¯“tommy bhama”}
useful for many natural language processing appli-
cations. This becomes a frequent problems with
queries when users do not observe the correct word
boundaries (for example: “southjerseycraigslist” for
“south jersey craiglist”) or when users are searching
for a part of a URL (for example “quincycollege”
for “quincy college”). We used a freely available
word breaker Web service that has been described at
(Wang et al., 2011).
</bodyText>
<subsectionHeader confidence="0.996207">
4.2 Queries to Concepts
</subsectionHeader>
<bodyText confidence="0.999994046511628">
Lexical similarity between queries has been often
used to identify related queries (Jansen et al., 2007).
The problem with lexical similarity is that it intro-
duces many false negatives (e.g. synonyms) , but
this can be handled by other features as we will de-
scribe later. More seriously, it introduces many false
positives. Take the following query pair as an exam-
ple Q1: weather in new york city and Q2: “hotels in
new york city”. Out of 5 words, 4 words are shared
between Q1 and Q2. Hence, any lexical similarity
feature would predict that the user submitted Q2 as
a reformulation of Q1. What we would like to do
is to have a query representation that recognizes the
difference between Q1 and Q2.
If we look closely at the two queries, we will no-
tice that in the first query, the user is looking for
the “weather”, while in the second query the user
is looking for “hotels”. We would like to recog-
nize “weather”, and “hotels” as the head keywords
of Q1 and Q2 respectively, while “new york city” is
a modifier of the head keyword in both cases. To
build such a representation, we start by segmenting
each query into phrases. Query segmentation is the
process of taking a users search query and divid-
ing the tokens into individual phrases or semantic
units (Bergsma and Wang, 2007). Many approaches
to query segmentation have been presented in recent
research. Some of them pose the problem as a super-
vised learning problem (Bergsma and Wang, 2007;
Yu and Shi, 2009). Many of the supervised methods
though use expensive features that are difficult to re-
implement.
On the other hand, many unsupervised methods
for query segmentation have also been proposed
(Hagen et al., 2011; Hagen et al., 2010). Most
of these methods use only raw web n-gram fre-
quencies and are very easy to re-implement. Ad-
ditionally, Hagen et al. (2010) have shown that
these methods can achieve segmentation accuracy
comparable to current state-of-the-art techniques us-
ing supervised learning. We opt for the unsuper-
vised techniques to perform query segmentation.
More specifically, we adopt the mutual information
</bodyText>
<page confidence="0.926084">
1003
</page>
<bodyText confidence="0.9954775">
method (MI) used throughout the literature. A seg-
mentation for a query is obtained by computing the
pointwise mutual information score for each pair
of consecutive words. More formally, for a query
</bodyText>
<equation confidence="0.986535333333333">
x = {x1, x2, ..., xnI
PMI(xi,xi+1) = log p(xi, xi+1)
p(xi)p(xi+1) (1)
</equation>
<bodyText confidence="0.998910327272727">
where p(xi, xi+1) is the joint probability of occur-
rence of the bigram (xi, xi+1) and p(xi) and p(xi+1)
are the individual occurrence probabilities of the two
tokens xi and xi+1 .
A segment break is introduced whenever the point
wise mutual information between two consecutive
words drops below a certain threshold τ. The thresh-
old we used, τ = 0.895 , was selected to max-
imize the break accuracy (Jones et al., 2006) on
the Bergsma-Wang-Corpus (Bergsma and Wang,
2007). Furthermore, we do not allow a break to hap-
pen between a noun and a proposition (e.g. no break
can be introduced between “hotels” and “in” or “in”
and “new York” in the query “hotels in new york
city”). We will shorty explain how we obtained the
part-of-speech tags.
In addition to breaking the query into phrases, we
were also interested in grouping multi-word key-
words together (e.g. “new york”, “Michael Jack-
son”, etc.). The intuition behind that is that a
query containing the keyword “new york” and an-
other containing the keyword “new mexico” should
not be awarded because they share the word “new”.
We do that by adopting a hierarchical segmentation
technique where the same segmentation method de-
scribed above is reapplied to every resulting phrase
with a new threshold τs &lt; τ . We selected the
new threshold, τ = 1.91 , to maximize the break
accuracy over a set of a random sample of 10,000
Wikipedia title of persons, cities, countries and or-
ganizations and a random sample of bigrams and tri-
grams from Wikipedia text.
In our implementation, the probabilities for all
words and n-grams have been computed using the
freely available Microsoft Web N-Gram Service
(Huang et al., 2010).
Now that we have the phrases and keywords in
each query, we assume that every phrase corre-
sponds to a semantic unit. Every semantic unit
has a head and a zero or more modifiers. Depen-
dency parsing could be used to identify the head
and modifiers from every phrase. However, because
queries are typical short and not always well-formed
sentences, this may pose a challenge to the depen-
dency parser. But as we are mainly interested in
short noun phrases, we can apply a simple set of
rules to identify the head keyword of each phrase
using the part of speech tags of the words in the
phrase. A part-of-speech (POS) tagger assigns parts
of speech to each word in an input text, such as noun,
verb, adjective, etc. We used the Stanford POS tag-
ger, using Stanford CoreNLP, to assign POS tags to
queries (Toutanova et al., 2003). To identify the head
and attributes of every noun phrase, we use the fol-
lowing rules:
</bodyText>
<listItem confidence="0.922120428571429">
• For phrases with of the form: “NNX+” (i.e. one
more nouns, where NNX could be NN: noun,
singular, NNS: noun, plural, NNP: proper
noun, singular or NNPS: proper noun, plural),
the head is the last noun keyword and all other
keywords are treated as attributes/modifiers.
• For the phrases of the form “NNX+ IN NNX+”,
</listItem>
<bodyText confidence="0.985309277777778">
where IN denotes a preposition or a subordinat-
ing conjunction (e.g. “in”, “of”, etc.), the head
is the last noun keyword before the preposition.
Table 1 shows different examples of queries,
the corresponding phrases, keywords, and con-
cepts. For example the query “kodak easyshare
recharger chord” consists of a single semantic
unit (phrase) and two keywords “Kodak easyshare”
and “recharger cord”. The head of this semantic
unit is the keyword “recharger cord” and “kodak
easyshare” is regarded as an attribute/modifier. An-
other example is the two queries “tommy bhama
rug” and “tommy bhama perfume”. The head of the
former is “rug”, while the head of the latter is “per-
fume”. Both share the attribute “tommy bhama”.
This shows that the user had two different intents
even though most of the words in the two queries
are shared.
</bodyText>
<subsectionHeader confidence="0.998386">
4.3 Matching Concepts
</subsectionHeader>
<bodyText confidence="0.999432666666667">
Phrases in two concepts may have full term over-
lap, partial term overlap, or no direct overlap yet are
semantically similar. To capture concept similarity,
</bodyText>
<page confidence="0.985776">
1004
</page>
<bodyText confidence="0.999260428571429">
The word translation probabilities P(s|q) (i.e.
the model parameters 0) are optimized by max-
imizing the probability of generating document
titles from queries over the entire training cor-
pus:
we define four different ways of matching concepts
ranked from the most to the least strict:
</bodyText>
<listItem confidence="0.990663714285714">
• Exact Match: The head and the attributes of the
two concepts match exactly.
• Approximate Match: To capture spelling vari-
ants and misspelling, we allow two keywords to
match if the Levenshtein edit distance between
them is less than 2.
• Lemma Match: Lemmatization is the process
</listItem>
<bodyText confidence="0.655198">
of reducing an inflected spelling to its lexical
root or lemma form. We match two concepts if
the lemmas of their keywords can be matched.
</bodyText>
<equation confidence="0.914134">
M
0* = argmaxg P(S|Q, 0) (3)
i=1
</equation>
<table confidence="0.65184075">
where P(S|Q, 0) is defined as:
� �I J P(si|qj) (4)
P(S|Q, 0) = i=1 j=1
(J+!)I
</table>
<listItem confidence="0.746309">
• Semantic Match: We compute the concept sim-
ilarity by measuring the semantic similarity be-
</listItem>
<bodyText confidence="0.99917945">
tween the two phrases from which the concepts
where extracted. Let Q = {q1, ..., qI} be one
phrase and S = {s1, ..., sJ} be another, the
semantic similarity between these two phrases
can be measured by estimating the probabil-
ity of one of them being a translation of an-
other. The translation probabilities can be es-
timated using the IBM Model 1 (Brown et al.,
1993; Berger and Lafferty, 1999). The model
was originally proposed to model the probabil-
ity of translating from one sequence of words
in one language to another. It has been also
used in different IR applications to estimate the
probability of translating from one sequence
of words to another sequence in the same lan-
guage (e.g. (Gao et al., 2012), (Gao et al.,
2010) and (White et al., 2013)). More for-
mally, the similarity between two sequences of
words, Q = {q1, ..., qI} and S = {s1, ..., sJ},
can be defined as:
</bodyText>
<equation confidence="0.9999365">
P(S|Q) = I J P(si|qj)P(qj|Q) (2)
i=1 j=1
</equation>
<bodyText confidence="0.998608428571429">
where P(q|Q) is the unigram probability of
word q in query Q. The word translation prob-
abilities P(s|q) are estimated using the query-
title pairs derived from the clickthrough search
logs, assuming that the title terms are likely to
be the desired alternation of the paired query.
where a is a constant, I is the token length of S,
and J is the token length of Q. The query-title pairs
used for model training are sampled from one year
worth of search logs from a commercial search en-
gine. The search logs do not intersect with the search
logs where the data described in Section5.1 has been
sampled from. S and Q are considered a match if
P(S|Q, 0) &gt; 0.5.
</bodyText>
<subsectionHeader confidence="0.725732">
4.4 Features
4.4.1 Textual Features
</subsectionHeader>
<bodyText confidence="0.989746166666667">
Jones and Klinkner (2008) showed that word and
character edit features are very useful for identifying
same task queries. The intuition behind this is that
consecutive queries which have many words and/or
characters in common tend to be related. The fea-
tures they used are:
</bodyText>
<listItem confidence="0.999926916666667">
• normalized Levenshtein edit distance
• 1 if lev &gt; 2, 0 otherwise
• Number of characters in common starting from
the left
• Number of characters in common starting from
the right
• Number of words in common starting from the
left
• Number of words in common starting from the
right
• Number of words in common
• Jaccard distance between sets of words
</listItem>
<page confidence="0.986226">
1005
</page>
<subsectionHeader confidence="0.469911">
4.4.2 Concept Features
</subsectionHeader>
<bodyText confidence="0.9999711">
As we explained earlier the word and character
edit features capture similarity between many pairs
of queries. However, they also tend to mis-classify
many other pairs especially when the two queries
share many words yet have different intents. We
used the conceptual representation of queries de-
scribed in the previous subsection to compute the
following set of features, notice that every feature
has two variants one at the concept level and the
other at the keyword (head or attribute) level:
</bodyText>
<listItem confidence="0.999752466666667">
• Number of “exact match” concepts in common
• Number of “approximate match” concepts in
common
• Number of “lemma match” concepts in com-
mon
• Number of “semantic match” concepts in com-
mon
• Number of concepts in Q1
• Number of concepts in Q2
• Number of concepts in Q1 but not in Q2
• Number of concepts in Q1 but not in Q2
• 1 if Q1 contains all Q2s concepts
• 1 if Q2 contains all Q1s concepts
• all above features recomputed for keywords in-
stead of concepts
</listItem>
<subsubsectionHeader confidence="0.448692">
4.4.3 Other Features
</subsubsectionHeader>
<bodyText confidence="0.9035455">
Other features, that have been also used in (Jones
and Klinkner, 2008), include temporal features:
</bodyText>
<listItem confidence="0.943137">
• time between queries in seconds
• time between queries as a binary feature (5
mins, 10 mins, 20 mins, 30 mins, 60 mins, 120
mins)
and search results feature:
• cosine distance between vectors derived from
the first 10 search results for the query terms.
</listItem>
<subsectionHeader confidence="0.984911">
4.5 Predicting Reformulation Type
</subsectionHeader>
<bodyText confidence="0.992431333333333">
There are different strategies users use to reformu-
late a query which results in different types of query
reformulations:
</bodyText>
<listItem confidence="0.994328666666667">
• Generalization: A generalization reformula-
tion occurs when the second query is intended
to seek more general information compared to
the first query
• Specification: A specification reformulation
occurs when the second query is intended to
seek more specific information compared to the
first query
• Spelling: A spelling reformulation occurs
when the second query is intended to correct
one or more misspelled words in the first query
• Same Intent: A same intent reformulation oc-
</listItem>
<bodyText confidence="0.729804666666667">
curs when the second query is intended to ex-
press the same intent as the first query. This
can be the result of word substitution or word
reorder.
We used the following features to predict the
query reformulation type:
</bodyText>
<listItem confidence="0.999264">
• Length (num. characters and num. words) of
Q1, Q2 and difference between them
• Number of out-of-vocabulary words in Q1, Q2
and the difference between them
• num. of “exact match” concepts in common
• num. of “approximate match” concepts in com-
mon
• num. of “lemma match” concepts in common
• num. of “semantic match” concepts in common
• num. of concepts in Q1, Q2 and the difference
between them
• num. of concepts in Q1 but not in Q2
• num. of concepts in Q1 but not in Q2
• 1 if Q1 contains all Q2s concepts
• 1 if Q2 contains all Q1s concepts
• all concept features above recomputed for key-
words instead of concepts
</listItem>
<page confidence="0.995453">
1006
</page>
<sectionHeader confidence="0.99336" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.931187">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999968333333333">
Our data consists of query pairs randomly sampled
from the queries submitted to a commercial search
engine during a week in mid-2012. Every record
in our data consisted of a consecutive query pair
(Qi,Qi+1) submitted to the search engine by the
same user and in the same session (i.e. within less
than 30 minutes of idle time, the 30 minutes thresh-
old has been frequently used in previous work, e.g.
(White and Drucker, 2007)). Identical queries were
excluded from the data because they are always la-
beled as reformulation and their label is very easy to
predict. Hence, when included, they result in unre-
alistically high estimates of the performance of the
proposed methods. All data in the session to which
the sampled query pair belongs were recorded. In
addition to queries, the data contained a timestamp
for each page view, all elements shown in response
to that query (e.g. Web results, answers, etc.), and
visited Web page or clicked answers. Intranet and
secure URL visits were excluded. Any personally
identifiable information was removed from the data
prior to analysis.
Annotators were instructed to exhaustively exam-
ine each session and “re-enact” the user’s experi-
ence. The annotators inspected the entire search
results page for each of Qi and Qi+1, including
URLs, page titles, relevant snippets, and other fea-
tures. They were also shown clicks to aid them in
their judgments. Additionally, they were also shown
queries and clicks before and after the query pair of
interest. They were asked to then use their assess-
ment of the user’s objectives to determine whether
Qi+1 is a reformulation of Qi. Each query pair was
labeled by three judges and the majority vote among
judges was used. Because the number of positive
instances is much smaller than the number of neg-
ative instances, we used all positive instances and
an equal number of randomly selected negative in-
stances leaving us with approximately 6000 query
pairs.
Judges were also asked to classify reformulations
into one of four different categories: Generalization
(second query is intended to seek more general in-
formation), Specification (second query is intended
to seek more specific information), Spelling (second
</bodyText>
<figure confidence="0.962095">
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.964284">
Figure 1 Precision-Recall Curves for the Reformu-
lation Prediction Methods
</figureCaption>
<figure confidence="0.999540666666667">
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Same intent Spelling Generalization Specification
</figure>
<figureCaption confidence="0.9635055">
Figure 2 Distribution of Query Reformulation
Types
</figureCaption>
<bodyText confidence="0.990647333333333">
query is intended to correct spelling mistakes), and
Same Intent (second query is intended to express the
same intent in a different way).
</bodyText>
<subsectionHeader confidence="0.999932">
5.2 Predicting Query Reformulation
</subsectionHeader>
<bodyText confidence="0.9993938">
In this section we describe the experiments we con-
ducted to evaluate the reformulation prediction clas-
sifier. We perform experiments using the data de-
scribed in the previous section. We compare the per-
formance of four different systems:
</bodyText>
<listItem confidence="0.92249">
• The first one, Heuristic, simply computes the
similarity between two queries as the percent-
age of common words to the length of the
longer query in terms of the number of words.
</listItem>
<figure confidence="0.999203933333333">
Heurisitc
Textual
Concepts
All
Precision 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.986544">
1007
</page>
<tableCaption confidence="0.99662">
Table 2 : Heuristics vs. Textual vs. Concept Features for Reformulation Prediction
</tableCaption>
<table confidence="0.9991596">
Accuracy Reform. F1 No-Reform. F1
Heuristics 77.10% 75.60% 69.07%
Textual 82.90% 71.75% 87.75%
Concepts 87.60% 81.20% 90.78%
All 89.02% 83.63% 91.75%
</table>
<figure confidence="0.803454727272727">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Same intent Spelling Generalization Specification
</figure>
<figureCaption confidence="0.996088">
Figure 3 Precision and Recall for Query Reformu-
lation Type Prediction
</figureCaption>
<bodyText confidence="0.998927">
When finding common words, it allows two
words to be matched if their Levenshtein edit
distance is less than or equals 2. The second
query is predicted to be a reformulation of the
first if similarity &gt; Tsim and the time difference
:5 Ttime minutes. The two thresholds were set
to 0.35 and 5 minutes respectively using grid
search to maximize accuracy over the training
data.
</bodyText>
<listItem confidence="0.940797">
• The second system, Textual, uses the textual
features from previous work that have been de-
scribed in Section 4.4.1 and the temporal and
results features described in Section 4.4.3.
• The third system, Concepts, uses the concept
</listItem>
<bodyText confidence="0.991528159090909">
features that we presented in Section 4.4.2 and
the temporal and results features described in
Section 4.4.3.
• Finally, the last system, All, uses both the tex-
tual features, the conceptual features and the
temporal and results features.
For all methods, we used gradient boosted regres-
sion trees as a classifier with 10-fold cross valida-
tion. We also tried other classifiers like SVM and
logistic regression but we got the best performance
using the gradient boosted regression trees. All re-
ported differences are statistically significant at the
0.05 level according to a two-tailed student t-test.
The accuracy, positive (reformulation) F1, and
negative (non-reformulation) F1 for the four meth-
ods are shown in Table 2. The precision recall
curves for all methods are shown in Figure 1; the
heuristic method uses fixed thresholds resulting in
a single operating point. The results show that
the concept features outperform the textual features.
Combining them together results in a small gain
over using the concept features only. The concept
features were able to achieve higher precision rates
while not sacrificing recall because they were more
effective in eliminating false reformulation cases.
We examined the cases where the classifier failed
to predict the correct label to understand when the
classifier fails to work properly. We identified sev-
eral cases where this happens. For example, the
classifier failed to match some terms that have the
same semantic meaning. Many of these cases were
acronyms (e.g. “AR” and “Accelerated Reader”,
”GE” and ”General Electric”). These cases can
be handled by using a semantic matching method
that yields higher coverage especially in cases of
acronyms.
The classifier also failed in cases where the key-
word extractor and/or the POS tagger failed to cor-
rectly parse the queries (e.g. “last to know”was
not recognized as a song name). These cases can
be handled by identifying named entities as a pre-
processing step and treating them accordingly when
identifying keywords or assigning POS tags to key-
words.
</bodyText>
<table confidence="0.508972">
1
Precision Recall
</table>
<page confidence="0.970152">
1008
</page>
<bodyText confidence="0.999876375">
Another dominant class of cases where the classi-
fier failed were cases where the dependency rules
failed to correctly identify the head keyword in a
query. In many such cases, the query was a non well-
formed sequence of words (e.g. “dresses Christmas
toddler”). This is the hardest class to handle. Since
it is hard to correctly parse short text and it is even
harder when the text it is not well-formed.
</bodyText>
<subsectionHeader confidence="0.997818">
5.3 Predicting Reformulation Type
</subsectionHeader>
<bodyText confidence="0.999987411764706">
We conducted another experiment to evaluate the
performance of the reformulation type classifier. We
performed experiments using the data described ear-
lier where judges were asked to select the type of
reformulation for every reformulation query. The
distribution of reformulations across types is shown
in Figure 2. The figure shows that most popular re-
formulations types are those where users move to
a more specific intent or express the same intent in
a different way. Reformulations with spelling sug-
gestions and query generalizations are less popular.
We conducted a one-vs-all experiment using gradi-
ent boosted regression trees with 10-fold cross val-
idation. The precision and recall of every type are
shown in Figure 3. The micro-averaged and macro-
averaged accuracy was 78.13% and 72.52% respec-
tively.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999993588235294">
Identifying query reformulations is an interesting
and useful application in Information Retrieval. Re-
formulation identification is useful for automatic
query refinements, task boundary identification and
satisfaction prediction. Previous work on this prob-
lem has adopted a bag-of-words approach where
lexical similarity and word overlap are the key fea-
tures for identifying query reformulation. We pro-
posed a method for identifying concepts in search
queries and using them to identify query reformula-
tions. The proposed method outperforms previous
work because it can better represent the information
intent underlying the query and hence can better as-
sess query similarity. We showed that the proposed
method significantly outperforms the other methods.
We also showed that we can reliably predict the type
of the reformulation with high accuracy.
</bodyText>
<sectionHeader confidence="0.977437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997230169811321">
Peter Anick. 2003. Learning noun phrase query segmen-
tation. In Proceedings of the 26th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 88–95.
M. Arlitt. 2000. Characterizing web user sessions. ACM
SIGMETRICS Performance Eval Review, 28(2):50–
63.
Ricardo Baeza-Yates, Carlos Hurtado, Marcelo Men-
doza, and Georges Dupret. 2005. Modeling user
search behavior. In LA-WEB ’05: Proceedings of the
Third Latin American Web Congress, Washington, DC,
USA. IEEE Computer Society.
A. L. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. In Proceedings of the 22th
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR 1999), pages 222–229.
S. Bergsma and I. Q. Wang. 2007. Learning noun phrase
query segmentation. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 816–826.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applications.
In Proceeding of the 17th ACM conference on In-
formation and knowledge management (CIKM 2008),
pages 609–618.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
Doug Downey, Susan Dumais, and Eric Horvitz. 2007.
Models of searching and browsing: Languages, stud-
ies, and applications. Journal of the American Soci-
ety for Information Science and Technology (JASIST),
58(6):862–871.
J. Gao, X. He, and J. Nie. 2010. Clickthrough-based
translation models for web search: from word mod-
els to phrase models. In Proceeding of the ACM con-
ference on Information and knowledge management
(CIKM 2010), pages 1139–1148.
J. Gao, S. Xie, X. He, and A. Ali. 2012. Learning lexicon
models from search logs for query expansion. In Pro-
ceeding of the Conference on Emprical Methods for
Natural Language Processing (EMNLP 2012).
J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and
discriminative model for query refinement. In Pro-
ceedings of the annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 379–386.
M. Hagen, M. Potthast, B. Stein, and C. Brautigam.
2010. The power of naiv query segmentation. In Pro-
ceeding of the ACM Conference of the Special Interest
</reference>
<page confidence="0.846663">
1009
</page>
<reference confidence="0.999636112359551">
Group on Information Retrieval (SIGIR 2010), pages
797–798,.
M. Hagen, M. Potthast, B. Stein, and C. Brautigam.
2011. Query segmentation revisited. In Proceeding of
the ACM World Wide Web Conference (WWW 2011),
pages 97–106.
J. Huang, J. Gao, J. Miao, X. Li, K. Wang, and F. Behr.
2010. Exploring web scale language models for search
query processing. In Proceeding of the ACM World
Wide Web Conference (WWW 2010), pages 451–460.
Bernard J. Jansen and Amanda Spink. 2006. How are we
searching the world wide web?: a comparison of nine
search engine transaction logs. Inf. Process. Manage.,
42:248–263, January.
B. J. Jansen, A. Spink, and J. Pedersen. 2005. A tem-
poral comparison of altavista web searching. Journal
of the American Society for Information Science and
Technology, 56:559–570.
B. J. Jansen, M. Zhang, and A. Spink. 2007. Patterns and
transitions of query reformulation during web search-
ing. International Journal of Web Information Sys-
tems.
Rosie Jones and Kristina Klinkner. 2008. Beyond the
session timeout: Automatic hierarchical segmentation
of search topics in query logs. In Proceedings ofACM
17th Conference on Information and Knowledge Man-
agement (CIKM 2008).
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substititions. In Pro-
ceedings of the Fifteenth International Conference on
the World-Wide Web (WWW06), pages 387–396.
Tessa Lau and Eric Horvitz. 1999. Patterns of search:
Analyzing and modeling web query refinement. In
ACM Press, editor, Proceedings of the Seventh Inter-
national Conference on User Modeling.
C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and
G. Tolomei. 20011. Identifying task-based sessions
in search engine query logs. In Proceedings of ACM
Conference on Web Search and Data Mining(WSDM
2011).
Q. Mei, D. Zhou, and K. Church. 2008. Query sug-
gestion using hitting time. In Proceeding of the 17th
ACM conference on Information and knowledge man-
agement (CIKM 2008), pages 469–478.
M. Mitra, A. Singhal, and C. Buckley. 1998. Improv-
ing automatic query expansion. In Proceedings of
the 21th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 206–214.
G. V. Murray, J. Lin, and A. Chowdhury. 2006. Identifi-
cation of user sessions with hierarchical agglomerative
clustering. ASIST, 43(1):934–950.
Seda Ozmutlu. 2006. Automatic new topic identification
using multiple linear regression. Information Process-
ing and Management, 42(4):934–950.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: learning to rank from implicit feedback. In
Robert Grossman, Roberto Bayardo, and Kristin P.
Bennett, editors, KDD, pages 239–248. ACM.
Jamie Teevan, Eytan Adar, Rosie Jones, and Michael
Potts. 2006. History repeats itself: Repeat queries
in yahoo’s logs. In Proceedings of the 29th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 703–704.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceeding of the Hu-
man Language Technologies Conference and the An-
nual Meeting of the North American Association of
Computational Linguists (HLT-NAACL 2003), pages
252–259.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale nlp:
A case study on url word breaking. In Proceeding of
the ACM World Wide Web Conference (WWW 2011),
pages 357–366.
Ryen W. White and Steven M. Drucker. 2007. Inves-
tigating behavioral variability in web search. In Pro-
ceedings of the 16th international conference on World
Wide Web.
Ryen W. White, Wei Chu, Ahmed Hassan, Xiaodong He,
Yang Song, and Hongning Wang. 2013. Enhancing
personalized search by mining and modeling task be-
havior. In Proceedings of the 22nd international con-
ference on World Wide Web, WWW ’13, pages 1411–
1420.
X. Yu and H. Shi. 2009. Query segmentation using con-
ditional random fields. In Proceedings of the Work-
shop on Keyword Search on Structured Data (KEYS),
pages 21–26.
</reference>
<page confidence="0.99012">
1010
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699326">
<title confidence="0.998456">Identifying Web Search Query Reformulation using Concept based Matching</title>
<author confidence="0.937521">Ahmed</author>
<affiliation confidence="0.938756">Microsoft</affiliation>
<address confidence="0.886186">One Microsoft Redmond, WA 98053,</address>
<email confidence="0.999562">hassanam@microsoft.com</email>
<abstract confidence="0.999224434782609">Web search users frequently modify their queries in hope of receiving better results. process is referred to as Refor- Previous research has mainly focused on proposing query reformulations in the form of suggested queries for users. Some research has studied the problem of predicting whether the current query is a reformulation of the previous query or not. However, this work has been limited to bag-of-words models where the main signals being used are word overlap, character level edit distance and word level edit distance. In this work, we show that relying solely on surface level text similarity results in many false positives where queries with different intents yet similar topics are mistakenly predicted as query reformulations. We propose a new representation for Web search queries based on identifying the concepts in queries and show that we can significantly improve query reformulation performance using features of query concepts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Anick</author>
</authors>
<title>Learning noun phrase query segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="3150" citStr="Anick, 2003" startWordPosition="498" endWordPosition="499"> on a result, and possibly dwells for a certain amount of time, then the user is satisfied. Identifying query reformulation can be very useful for finding cases where the users are not satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representing queries and mostly used features of word overlap or character and word level edit distances. Take the queries “hotels in Ne</context>
<context position="5814" citStr="Anick (2003)" startWordPosition="923" endWordPosition="924">areas of work related to the research presented in this paper: (i) query reformulation taxonomies, (ii) automatic query refinement, and (iii) search tasks boundary identification. We cover each of these areas in turn. 2.1 Query Reformulation Taxonomies Existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. Most of the research on manual query reformulation has focused on building taxonomies of query reformulation. These taxonomies are generally constructed by examining a small set of query logs. Anick (2003) classified a random sample of 100 reformulations by hand into eleven categories. Jensen et al. (2007) identified 6 different kinds of reformulation states (New, Assistance, Content Change, Generalization, Reformulation, and Specialization) and provided heuristics for identifying them. They also used them to predict when a user is most receptive to automatic query suggestions. The same categories were used in several other studies (Guo et al., 2008; Lau and Horvitz, 1999). Huang and Efthimis (2010) proposed another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorde</context>
</contexts>
<marker>Anick, 2003</marker>
<rawString>Peter Anick. 2003. Learning noun phrase query segmentation. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arlitt</author>
</authors>
<title>Characterizing web user sessions.</title>
<date>2000</date>
<journal>ACM SIGMETRICS Performance Eval Review,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>63</pages>
<contexts>
<context position="9910" citStr="Arlitt (2000)" startWordPosition="1586" endWordPosition="1587">he query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu (2006) proposed a method for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner (2008) also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves considerably better results compared to timeouts. Lucchese et al. (Lucchese et al., 20011) uses a similar set of features as (Jones and Klinkner, 2008), but uses clustering to group queries in the same</context>
</contexts>
<marker>Arlitt, 2000</marker>
<rawString>M. Arlitt. 2000. Characterizing web user sessions. ACM SIGMETRICS Performance Eval Review, 28(2):50– 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Carlos Hurtado</author>
<author>Marcelo Mendoza</author>
<author>Georges Dupret</author>
</authors>
<title>Modeling user search behavior.</title>
<date>2005</date>
<booktitle>In LA-WEB ’05: Proceedings of the Third Latin American Web Congress,</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="8011" citStr="Baeza-Yates et al., 2005" startWordPosition="1274" endWordPosition="1277">research attention in this area is the problem of automatically generating query refinements. These refinements are typically offered as query suggestions to the users or used to alter the user query before submitting it to the search engine. Boldi et al. (2008) introduced the concept of the query-flow graph where every query is represented by a node and edges connect queries if it is likely for users to move from one query to another. Mei et al. (2008) used random walks over a bipartite graph of queries and URLs to find query refinements. Query logs were used to suggest query refinements in (Baeza-Yates et al., 2005). Hierarchical agglomerative clustering was used to group similar queries that can be used as suggestions for one 1001 another. Other research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones et al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformulations. 2.3 Search Task Boundary Identification The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely ad</context>
</contexts>
<marker>Baeza-Yates, Hurtado, Mendoza, Dupret, 2005</marker>
<rawString>Ricardo Baeza-Yates, Carlos Hurtado, Marcelo Mendoza, and Georges Dupret. 2005. Modeling user search behavior. In LA-WEB ’05: Proceedings of the Third Latin American Web Congress, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR</booktitle>
<pages>222--229</pages>
<contexts>
<context position="21455" citStr="Berger and Lafferty, 1999" startWordPosition="3521" endWordPosition="3524"> their keywords can be matched. M 0* = argmaxg P(S|Q, 0) (3) i=1 where P(S|Q, 0) is defined as: � �I J P(si|qj) (4) P(S|Q, 0) = i=1 j=1 (J+!)I • Semantic Match: We compute the concept similarity by measuring the semantic similarity between the two phrases from which the concepts where extracted. Let Q = {q1, ..., qI} be one phrase and S = {s1, ..., sJ} be another, the semantic similarity between these two phrases can be measured by estimating the probability of one of them being a translation of another. The translation probabilities can be estimated using the IBM Model 1 (Brown et al., 1993; Berger and Lafferty, 1999). The model was originally proposed to model the probability of translating from one sequence of words in one language to another. It has been also used in different IR applications to estimate the probability of translating from one sequence of words to another sequence in the same language (e.g. (Gao et al., 2012), (Gao et al., 2010) and (White et al., 2013)). More formally, the similarity between two sequences of words, Q = {q1, ..., qI} and S = {s1, ..., sJ}, can be defined as: P(S|Q) = I J P(si|qj)P(qj|Q) (2) i=1 j=1 where P(q|Q) is the unigram probability of word q in query Q. The word t</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. L. Berger and J. Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR 1999), pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>I Q Wang</author>
</authors>
<title>Learning noun phrase query segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>816--826</pages>
<contexts>
<context position="15174" citStr="Bergsma and Wang, 2007" startWordPosition="2445" endWordPosition="2448">nizes the difference between Q1 and Q2. If we look closely at the two queries, we will notice that in the first query, the user is looking for the “weather”, while in the second query the user is looking for “hotels”. We would like to recognize “weather”, and “hotels” as the head keywords of Q1 and Q2 respectively, while “new york city” is a modifier of the head keyword in both cases. To build such a representation, we start by segmenting each query into phrases. Query segmentation is the process of taking a users search query and dividing the tokens into individual phrases or semantic units (Bergsma and Wang, 2007). Many approaches to query segmentation have been presented in recent research. Some of them pose the problem as a supervised learning problem (Bergsma and Wang, 2007; Yu and Shi, 2009). Many of the supervised methods though use expensive features that are difficult to reimplement. On the other hand, many unsupervised methods for query segmentation have also been proposed (Hagen et al., 2011; Hagen et al., 2010). Most of these methods use only raw web n-gram frequencies and are very easy to re-implement. Additionally, Hagen et al. (2010) have shown that these methods can achieve segmentation a</context>
<context position="16722" citStr="Bergsma and Wang, 2007" startWordPosition="2699" endWordPosition="2702">al information score for each pair of consecutive words. More formally, for a query x = {x1, x2, ..., xnI PMI(xi,xi+1) = log p(xi, xi+1) p(xi)p(xi+1) (1) where p(xi, xi+1) is the joint probability of occurrence of the bigram (xi, xi+1) and p(xi) and p(xi+1) are the individual occurrence probabilities of the two tokens xi and xi+1 . A segment break is introduced whenever the point wise mutual information between two consecutive words drops below a certain threshold τ. The threshold we used, τ = 0.895 , was selected to maximize the break accuracy (Jones et al., 2006) on the Bergsma-Wang-Corpus (Bergsma and Wang, 2007). Furthermore, we do not allow a break to happen between a noun and a proposition (e.g. no break can be introduced between “hotels” and “in” or “in” and “new York” in the query “hotels in new york city”). We will shorty explain how we obtained the part-of-speech tags. In addition to breaking the query into phrases, we were also interested in grouping multi-word keywords together (e.g. “new york”, “Michael Jackson”, etc.). The intuition behind that is that a query containing the keyword “new york” and another containing the keyword “new mexico” should not be awarded because they share the word </context>
</contexts>
<marker>Bergsma, Wang, 2007</marker>
<rawString>S. Bergsma and I. Q. Wang. 2007. Learning noun phrase query segmentation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 816–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Boldi</author>
<author>Francesco Bonchi</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Sebastiano Vigna</author>
</authors>
<title>The query-flow graph: model and applications.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management (CIKM</booktitle>
<pages>609--618</pages>
<contexts>
<context position="3046" citStr="Boldi et al., 2008" startWordPosition="480" endWordPosition="483">le, search satisfaction is typically evaluated using clickthrough information by assuming that if a user clicks on a result, and possibly dwells for a certain amount of time, then the user is satisfied. Identifying query reformulation can be very useful for finding cases where the users are not satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representing queries and mostly </context>
<context position="7648" citStr="Boldi et al. (2008)" startWordPosition="1206" endWordPosition="1209">f work is relevant to our work because it studies query reformulation strategies. Our work is different because we build a machine-learned predictive model to identify query reformulation while this line of work mainly focuses on defining taxonomies for reformulation strategies. 2.2 Automatic Query Refinement A close problem that has received most of the research attention in this area is the problem of automatically generating query refinements. These refinements are typically offered as query suggestions to the users or used to alter the user query before submitting it to the search engine. Boldi et al. (2008) introduced the concept of the query-flow graph where every query is represented by a node and edges connect queries if it is likely for users to move from one query to another. Mei et al. (2008) used random walks over a bipartite graph of queries and URLs to find query refinements. Query logs were used to suggest query refinements in (Baeza-Yates et al., 2005). Hierarchical agglomerative clustering was used to group similar queries that can be used as suggestions for one 1001 another. Other research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones</context>
<context position="9270" citStr="Boldi et al. (2008)" startWordPosition="1484" endWordPosition="1487">related to the problem of identifying query reformulation. A search task has been defined in (Jones and Klinkner, 2008) as a single information need that may result in one or more queries. Similarly , Jansen et al. (2007) defined a session as a series of interactions by the user toward addressing a single information need. On the other hand, a query reformulation is intended to modify a previous query in hope of getting better results to satisfy the same information need. From these definitions, it is clear how query reformulation and task boundary detection are two sides of the same problem. Boldi et al. (2008) presented the concept of the query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu (2006) proposed a method for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better r</context>
</contexts>
<marker>Boldi, Bonchi, Castillo, Donato, Gionis, Vigna, 2008</marker>
<rawString>Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. The query-flow graph: model and applications. In Proceeding of the 17th ACM conference on Information and knowledge management (CIKM 2008), pages 609–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21427" citStr="Brown et al., 1993" startWordPosition="3517" endWordPosition="3520">pts if the lemmas of their keywords can be matched. M 0* = argmaxg P(S|Q, 0) (3) i=1 where P(S|Q, 0) is defined as: � �I J P(si|qj) (4) P(S|Q, 0) = i=1 j=1 (J+!)I • Semantic Match: We compute the concept similarity by measuring the semantic similarity between the two phrases from which the concepts where extracted. Let Q = {q1, ..., qI} be one phrase and S = {s1, ..., sJ} be another, the semantic similarity between these two phrases can be measured by estimating the probability of one of them being a translation of another. The translation probabilities can be estimated using the IBM Model 1 (Brown et al., 1993; Berger and Lafferty, 1999). The model was originally proposed to model the probability of translating from one sequence of words in one language to another. It has been also used in different IR applications to estimate the probability of translating from one sequence of words to another sequence in the same language (e.g. (Gao et al., 2012), (Gao et al., 2010) and (White et al., 2013)). More formally, the similarity between two sequences of words, Q = {q1, ..., qI} and S = {s1, ..., sJ}, can be defined as: P(S|Q) = I J P(si|qj)P(qj|Q) (2) i=1 j=1 where P(q|Q) is the unigram probability of w</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Susan Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>Models of searching and browsing: Languages, studies, and applications.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology (JASIST),</journal>
<volume>58</volume>
<issue>6</issue>
<contexts>
<context position="11329" citStr="Downey et al., 2007" startWordPosition="1820" endWordPosition="1823">ords approach and tries to assess query similarity based on the concepts represented in each query. We compare our work to the state-of-the-art work in this area later in this paper. 3 Problem Definition We start by defining some terms that will be used throughout the paper: Definition: Query Reformulation is the act of submitting a query Q2 to modify a previous search query Q1 in hope of retrieving better results to satisfy the same information need. Definition: A Search Session is group of queries and clicks demarcated with a 30-minute inactivity timeout, such as that used in previous work (Downey et al., 2007; Radlinski and Joachims, 2005). Search engines receive streams of queries from users. In response to each query, the engine returns a set of search results. Depending on these results, the user may decide to click on one or more results, submit another query, or end the search session. In this work, we focus on cases where the user submits another query. Our objective is to solve the following problem: Given a query Q1, and the following query Q2, predict whether Q2 is reformulation of Q1. 4 Approach In this section, we propose methods for predicting whether the current query has been issued </context>
</contexts>
<marker>Downey, Dumais, Horvitz, 2007</marker>
<rawString>Doug Downey, Susan Dumais, and Eric Horvitz. 2007. Models of searching and browsing: Languages, studies, and applications. Journal of the American Society for Information Science and Technology (JASIST), 58(6):862–871.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>X He</author>
<author>J Nie</author>
</authors>
<title>Clickthrough-based translation models for web search: from word models to phrase models.</title>
<date>2010</date>
<booktitle>In Proceeding of the ACM conference on Information and knowledge management (CIKM</booktitle>
<pages>1139--1148</pages>
<contexts>
<context position="21792" citStr="Gao et al., 2010" startWordPosition="3580" endWordPosition="3583">..., sJ} be another, the semantic similarity between these two phrases can be measured by estimating the probability of one of them being a translation of another. The translation probabilities can be estimated using the IBM Model 1 (Brown et al., 1993; Berger and Lafferty, 1999). The model was originally proposed to model the probability of translating from one sequence of words in one language to another. It has been also used in different IR applications to estimate the probability of translating from one sequence of words to another sequence in the same language (e.g. (Gao et al., 2012), (Gao et al., 2010) and (White et al., 2013)). More formally, the similarity between two sequences of words, Q = {q1, ..., qI} and S = {s1, ..., sJ}, can be defined as: P(S|Q) = I J P(si|qj)P(qj|Q) (2) i=1 j=1 where P(q|Q) is the unigram probability of word q in query Q. The word translation probabilities P(s|q) are estimated using the querytitle pairs derived from the clickthrough search logs, assuming that the title terms are likely to be the desired alternation of the paired query. where a is a constant, I is the token length of S, and J is the token length of Q. The query-title pairs used for model training </context>
</contexts>
<marker>Gao, He, Nie, 2010</marker>
<rawString>J. Gao, X. He, and J. Nie. 2010. Clickthrough-based translation models for web search: from word models to phrase models. In Proceeding of the ACM conference on Information and knowledge management (CIKM 2010), pages 1139–1148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>S Xie</author>
<author>X He</author>
<author>A Ali</author>
</authors>
<title>Learning lexicon models from search logs for query expansion.</title>
<date>2012</date>
<booktitle>In Proceeding of the Conference on Emprical Methods for Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="21772" citStr="Gao et al., 2012" startWordPosition="3576" endWordPosition="3579">phrase and S = {s1, ..., sJ} be another, the semantic similarity between these two phrases can be measured by estimating the probability of one of them being a translation of another. The translation probabilities can be estimated using the IBM Model 1 (Brown et al., 1993; Berger and Lafferty, 1999). The model was originally proposed to model the probability of translating from one sequence of words in one language to another. It has been also used in different IR applications to estimate the probability of translating from one sequence of words to another sequence in the same language (e.g. (Gao et al., 2012), (Gao et al., 2010) and (White et al., 2013)). More formally, the similarity between two sequences of words, Q = {q1, ..., qI} and S = {s1, ..., sJ}, can be defined as: P(S|Q) = I J P(si|qj)P(qj|Q) (2) i=1 j=1 where P(q|Q) is the unigram probability of word q in query Q. The word translation probabilities P(s|q) are estimated using the querytitle pairs derived from the clickthrough search logs, assuming that the title terms are likely to be the desired alternation of the paired query. where a is a constant, I is the token length of S, and J is the token length of Q. The query-title pairs used</context>
</contexts>
<marker>Gao, Xie, He, Ali, 2012</marker>
<rawString>J. Gao, S. Xie, X. He, and A. Ali. 2012. Learning lexicon models from search logs for query expansion. In Proceeding of the Conference on Emprical Methods for Natural Language Processing (EMNLP 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Guo</author>
<author>G Xu</author>
<author>H Li</author>
<author>X Cheng</author>
</authors>
<title>A unified and discriminative model for query refinement.</title>
<date>2008</date>
<booktitle>In Proceedings of the annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>379--386</pages>
<contexts>
<context position="6266" citStr="Guo et al., 2008" startWordPosition="989" endWordPosition="992">rmulation has focused on building taxonomies of query reformulation. These taxonomies are generally constructed by examining a small set of query logs. Anick (2003) classified a random sample of 100 reformulations by hand into eleven categories. Jensen et al. (2007) identified 6 different kinds of reformulation states (New, Assistance, Content Change, Generalization, Reformulation, and Specialization) and provided heuristics for identifying them. They also used them to predict when a user is most receptive to automatic query suggestions. The same categories were used in several other studies (Guo et al., 2008; Lau and Horvitz, 1999). Huang and Efthimis (2010) proposed another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying re-finding behavior, Teevan et al. (2006) constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms to identify repeat queries, equal click queries and overlapping click queries. None of this work has built an automatic classifier distinguishing reformulation queries from other quer</context>
</contexts>
<marker>Guo, Xu, Li, Cheng, 2008</marker>
<rawString>J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and discriminative model for query refinement. In Proceedings of the annual international ACM SIGIR conference on Research and development in information retrieval, pages 379–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hagen</author>
<author>M Potthast</author>
<author>B Stein</author>
<author>C Brautigam</author>
</authors>
<title>The power of naiv query segmentation.</title>
<date>2010</date>
<booktitle>In Proceeding of the ACM Conference of the Special Interest Group on Information Retrieval (SIGIR</booktitle>
<pages>797--798</pages>
<contexts>
<context position="15589" citStr="Hagen et al., 2010" startWordPosition="2513" endWordPosition="2516"> we start by segmenting each query into phrases. Query segmentation is the process of taking a users search query and dividing the tokens into individual phrases or semantic units (Bergsma and Wang, 2007). Many approaches to query segmentation have been presented in recent research. Some of them pose the problem as a supervised learning problem (Bergsma and Wang, 2007; Yu and Shi, 2009). Many of the supervised methods though use expensive features that are difficult to reimplement. On the other hand, many unsupervised methods for query segmentation have also been proposed (Hagen et al., 2011; Hagen et al., 2010). Most of these methods use only raw web n-gram frequencies and are very easy to re-implement. Additionally, Hagen et al. (2010) have shown that these methods can achieve segmentation accuracy comparable to current state-of-the-art techniques using supervised learning. We opt for the unsupervised techniques to perform query segmentation. More specifically, we adopt the mutual information 1003 method (MI) used throughout the literature. A segmentation for a query is obtained by computing the pointwise mutual information score for each pair of consecutive words. More formally, for a query x = {x</context>
</contexts>
<marker>Hagen, Potthast, Stein, Brautigam, 2010</marker>
<rawString>M. Hagen, M. Potthast, B. Stein, and C. Brautigam. 2010. The power of naiv query segmentation. In Proceeding of the ACM Conference of the Special Interest Group on Information Retrieval (SIGIR 2010), pages 797–798,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hagen</author>
<author>M Potthast</author>
<author>B Stein</author>
<author>C Brautigam</author>
</authors>
<title>Query segmentation revisited.</title>
<date>2011</date>
<booktitle>In Proceeding of the ACM World Wide Web Conference (WWW</booktitle>
<pages>97--106</pages>
<contexts>
<context position="15568" citStr="Hagen et al., 2011" startWordPosition="2509" endWordPosition="2512">ch a representation, we start by segmenting each query into phrases. Query segmentation is the process of taking a users search query and dividing the tokens into individual phrases or semantic units (Bergsma and Wang, 2007). Many approaches to query segmentation have been presented in recent research. Some of them pose the problem as a supervised learning problem (Bergsma and Wang, 2007; Yu and Shi, 2009). Many of the supervised methods though use expensive features that are difficult to reimplement. On the other hand, many unsupervised methods for query segmentation have also been proposed (Hagen et al., 2011; Hagen et al., 2010). Most of these methods use only raw web n-gram frequencies and are very easy to re-implement. Additionally, Hagen et al. (2010) have shown that these methods can achieve segmentation accuracy comparable to current state-of-the-art techniques using supervised learning. We opt for the unsupervised techniques to perform query segmentation. More specifically, we adopt the mutual information 1003 method (MI) used throughout the literature. A segmentation for a query is obtained by computing the pointwise mutual information score for each pair of consecutive words. More formall</context>
</contexts>
<marker>Hagen, Potthast, Stein, Brautigam, 2011</marker>
<rawString>M. Hagen, M. Potthast, B. Stein, and C. Brautigam. 2011. Query segmentation revisited. In Proceeding of the ACM World Wide Web Conference (WWW 2011), pages 97–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>J Gao</author>
<author>J Miao</author>
<author>X Li</author>
<author>K Wang</author>
<author>F Behr</author>
</authors>
<title>Exploring web scale language models for search query processing.</title>
<date>2010</date>
<booktitle>In Proceeding of the ACM World Wide Web Conference (WWW</booktitle>
<pages>451--460</pages>
<contexts>
<context position="6944" citStr="Huang et al., 2010" startWordPosition="1094" endWordPosition="1097">d another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying re-finding behavior, Teevan et al. (2006) constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms to identify repeat queries, equal click queries and overlapping click queries. None of this work has built an automatic classifier distinguishing reformulation queries from other queries. Heuristics and regular expressions have been used in (Huang et al., 2010) and (Jansen et al., 2007) to identify different types of reformulations. This line of work is relevant to our work because it studies query reformulation strategies. Our work is different because we build a machine-learned predictive model to identify query reformulation while this line of work mainly focuses on defining taxonomies for reformulation strategies. 2.2 Automatic Query Refinement A close problem that has received most of the research attention in this area is the problem of automatically generating query refinements. These refinements are typically offered as query suggestions to </context>
<context position="17914" citStr="Huang et al., 2010" startWordPosition="2904" endWordPosition="2907">ecause they share the word “new”. We do that by adopting a hierarchical segmentation technique where the same segmentation method described above is reapplied to every resulting phrase with a new threshold τs &lt; τ . We selected the new threshold, τ = 1.91 , to maximize the break accuracy over a set of a random sample of 10,000 Wikipedia title of persons, cities, countries and organizations and a random sample of bigrams and trigrams from Wikipedia text. In our implementation, the probabilities for all words and n-grams have been computed using the freely available Microsoft Web N-Gram Service (Huang et al., 2010). Now that we have the phrases and keywords in each query, we assume that every phrase corresponds to a semantic unit. Every semantic unit has a head and a zero or more modifiers. Dependency parsing could be used to identify the head and modifiers from every phrase. However, because queries are typical short and not always well-formed sentences, this may pose a challenge to the dependency parser. But as we are mainly interested in short noun phrases, we can apply a simple set of rules to identify the head keyword of each phrase using the part of speech tags of the words in the phrase. A part-o</context>
</contexts>
<marker>Huang, Gao, Miao, Li, Wang, Behr, 2010</marker>
<rawString>J. Huang, J. Gao, J. Miao, X. Li, K. Wang, and F. Behr. 2010. Exploring web scale language models for search query processing. In Proceeding of the ACM World Wide Web Conference (WWW 2010), pages 451–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Amanda Spink</author>
</authors>
<title>How are we searching the world wide web?: a comparison of nine search engine transaction logs.</title>
<date>2006</date>
<journal>Inf. Process. Manage.,</journal>
<pages>42--248</pages>
<contexts>
<context position="1648" citStr="Jansen and Spink, 2006" startWordPosition="254" endWordPosition="257">ncepts in queries and show that we can significantly improve query reformulation performance using features of query concepts. 1 Introduction Web search is a process of querying, learning and reformulating queries to satisfy certain information needs. When a user submits a search query, the search engine attempts to return the best results to that query. Oftentimes, users modify their search queries in hope of getting better results. Typical search users have low tolerance to viewing lowly ranked search results and they prefer to reformulate the query rather than wade through result listings (Jansen and Spink, 2006). Previous studies have also shown that 37% of search queries are reformulations to previous queries (Jansen et al., 2007) and that 52% of users reformulate their queries (Jansen et al., 2005). Understanding query reformulation behavior and being able to accurately identify reformulation queries have several benefits. One of these benefits is learning from user behavior to better suggest automatic query refinements or query alterations. Another benefit is using query reformulation prediction to identify boundaries between search tasks and hence segmenting user activities into topically coheren</context>
</contexts>
<marker>Jansen, Spink, 2006</marker>
<rawString>Bernard J. Jansen and Amanda Spink. 2006. How are we searching the world wide web?: a comparison of nine search engine transaction logs. Inf. Process. Manage., 42:248–263, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>A Spink</author>
<author>J Pedersen</author>
</authors>
<title>A temporal comparison of altavista web searching.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>56--559</pages>
<contexts>
<context position="1840" citStr="Jansen et al., 2005" startWordPosition="286" endWordPosition="289">ormulating queries to satisfy certain information needs. When a user submits a search query, the search engine attempts to return the best results to that query. Oftentimes, users modify their search queries in hope of getting better results. Typical search users have low tolerance to viewing lowly ranked search results and they prefer to reformulate the query rather than wade through result listings (Jansen and Spink, 2006). Previous studies have also shown that 37% of search queries are reformulations to previous queries (Jansen et al., 2007) and that 52% of users reformulate their queries (Jansen et al., 2005). Understanding query reformulation behavior and being able to accurately identify reformulation queries have several benefits. One of these benefits is learning from user behavior to better suggest automatic query refinements or query alterations. Another benefit is using query reformulation prediction to identify boundaries between search tasks and hence segmenting user activities into topically coherent units. Also, if we are able to accurately identify query reformulations, then we will be in a better position to evaluate the satisfaction of users with query results. For example, search sa</context>
</contexts>
<marker>Jansen, Spink, Pedersen, 2005</marker>
<rawString>B. J. Jansen, A. Spink, and J. Pedersen. 2005. A temporal comparison of altavista web searching. Journal of the American Society for Information Science and Technology, 56:559–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>M Zhang</author>
<author>A Spink</author>
</authors>
<title>Patterns and transitions of query reformulation during web searching.</title>
<date>2007</date>
<journal>International Journal of Web Information Systems.</journal>
<contexts>
<context position="1770" citStr="Jansen et al., 2007" startWordPosition="274" endWordPosition="277">. 1 Introduction Web search is a process of querying, learning and reformulating queries to satisfy certain information needs. When a user submits a search query, the search engine attempts to return the best results to that query. Oftentimes, users modify their search queries in hope of getting better results. Typical search users have low tolerance to viewing lowly ranked search results and they prefer to reformulate the query rather than wade through result listings (Jansen and Spink, 2006). Previous studies have also shown that 37% of search queries are reformulations to previous queries (Jansen et al., 2007) and that 52% of users reformulate their queries (Jansen et al., 2005). Understanding query reformulation behavior and being able to accurately identify reformulation queries have several benefits. One of these benefits is learning from user behavior to better suggest automatic query refinements or query alterations. Another benefit is using query reformulation prediction to identify boundaries between search tasks and hence segmenting user activities into topically coherent units. Also, if we are able to accurately identify query reformulations, then we will be in a better position to evaluat</context>
<context position="6970" citStr="Jansen et al., 2007" startWordPosition="1099" endWordPosition="1102">axonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying re-finding behavior, Teevan et al. (2006) constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms to identify repeat queries, equal click queries and overlapping click queries. None of this work has built an automatic classifier distinguishing reformulation queries from other queries. Heuristics and regular expressions have been used in (Huang et al., 2010) and (Jansen et al., 2007) to identify different types of reformulations. This line of work is relevant to our work because it studies query reformulation strategies. Our work is different because we build a machine-learned predictive model to identify query reformulation while this line of work mainly focuses on defining taxonomies for reformulation strategies. 2.2 Automatic Query Refinement A close problem that has received most of the research attention in this area is the problem of automatically generating query refinements. These refinements are typically offered as query suggestions to the users or used to alter</context>
<context position="8872" citStr="Jansen et al. (2007)" startWordPosition="1414" endWordPosition="1417">al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformulations. 2.3 Search Task Boundary Identification The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely addressed before. This problem is closely related to the problem of identifying query reformulation. A search task has been defined in (Jones and Klinkner, 2008) as a single information need that may result in one or more queries. Similarly , Jansen et al. (2007) defined a session as a series of interactions by the user toward addressing a single information need. On the other hand, a query reformulation is intended to modify a previous query in hope of getting better results to satisfy the same information need. From these definitions, it is clear how query reformulation and task boundary detection are two sides of the same problem. Boldi et al. (2008) presented the concept of the query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation</context>
<context position="13995" citStr="Jansen et al., 2007" startWordPosition="2231" endWordPosition="2234">bhama perfume Concept1{head =”perfume”, modifiers ¯“tommy bhama”} useful for many natural language processing applications. This becomes a frequent problems with queries when users do not observe the correct word boundaries (for example: “southjerseycraigslist” for “south jersey craiglist”) or when users are searching for a part of a URL (for example “quincycollege” for “quincy college”). We used a freely available word breaker Web service that has been described at (Wang et al., 2011). 4.2 Queries to Concepts Lexical similarity between queries has been often used to identify related queries (Jansen et al., 2007). The problem with lexical similarity is that it introduces many false negatives (e.g. synonyms) , but this can be handled by other features as we will describe later. More seriously, it introduces many false positives. Take the following query pair as an example Q1: weather in new york city and Q2: “hotels in new york city”. Out of 5 words, 4 words are shared between Q1 and Q2. Hence, any lexical similarity feature would predict that the user submitted Q2 as a reformulation of Q1. What we would like to do is to have a query representation that recognizes the difference between Q1 and Q2. If w</context>
</contexts>
<marker>Jansen, Zhang, Spink, 2007</marker>
<rawString>B. J. Jansen, M. Zhang, and A. Spink. 2007. Patterns and transitions of query reformulation during web searching. International Journal of Web Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Kristina Klinkner</author>
</authors>
<title>Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs.</title>
<date>2008</date>
<booktitle>In Proceedings ofACM 17th Conference on Information and Knowledge Management (CIKM</booktitle>
<contexts>
<context position="3348" citStr="Jones and Klinkner, 2008" startWordPosition="525" endWordPosition="528">satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representing queries and mostly used features of word overlap or character and word level edit distances. Take the queries “hotels in New York City” and “weather in New York City” as an example. The two queries are very likely to have been issued by a user who is planning to travel to New York City. The two queries have 5 words each</context>
<context position="8770" citStr="Jones and Klinkner, 2008" startWordPosition="1394" endWordPosition="1397">research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones et al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformulations. 2.3 Search Task Boundary Identification The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely addressed before. This problem is closely related to the problem of identifying query reformulation. A search task has been defined in (Jones and Klinkner, 2008) as a single information need that may result in one or more queries. Similarly , Jansen et al. (2007) defined a session as a series of interactions by the user toward addressing a single information need. On the other hand, a query reformulation is intended to modify a previous query in hope of getting better results to satisfy the same information need. From these definitions, it is clear how query reformulation and task boundary detection are two sides of the same problem. Boldi et al. (2008) presented the concept of the query-flow graph. A query-flow graph represents chains of related quer</context>
<context position="10132" citStr="Jones and Klinkner (2008)" startWordPosition="1617" endWordPosition="1620">for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner (2008) also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves considerably better results compared to timeouts. Lucchese et al. (Lucchese et al., 20011) uses a similar set of features as (Jones and Klinkner, 2008), but uses clustering to group queries in the same task together as opposed to identifying task boundary as in (Jones and Klinkner, 2008). This line of work is perhaps the closest to our work. Our work is different because it goes beyond the bag of words approach and trie</context>
<context position="22700" citStr="Jones and Klinkner (2008)" startWordPosition="3748" endWordPosition="3751">ties P(s|q) are estimated using the querytitle pairs derived from the clickthrough search logs, assuming that the title terms are likely to be the desired alternation of the paired query. where a is a constant, I is the token length of S, and J is the token length of Q. The query-title pairs used for model training are sampled from one year worth of search logs from a commercial search engine. The search logs do not intersect with the search logs where the data described in Section5.1 has been sampled from. S and Q are considered a match if P(S|Q, 0) &gt; 0.5. 4.4 Features 4.4.1 Textual Features Jones and Klinkner (2008) showed that word and character edit features are very useful for identifying same task queries. The intuition behind this is that consecutive queries which have many words and/or characters in common tend to be related. The features they used are: • normalized Levenshtein edit distance • 1 if lev &gt; 2, 0 otherwise • Number of characters in common starting from the left • Number of characters in common starting from the right • Number of words in common starting from the left • Number of words in common starting from the right • Number of words in common • Jaccard distance between sets of words</context>
<context position="24379" citStr="Jones and Klinkner, 2008" startWordPosition="4047" endWordPosition="4050">e concept level and the other at the keyword (head or attribute) level: • Number of “exact match” concepts in common • Number of “approximate match” concepts in common • Number of “lemma match” concepts in common • Number of “semantic match” concepts in common • Number of concepts in Q1 • Number of concepts in Q2 • Number of concepts in Q1 but not in Q2 • Number of concepts in Q1 but not in Q2 • 1 if Q1 contains all Q2s concepts • 1 if Q2 contains all Q1s concepts • all above features recomputed for keywords instead of concepts 4.4.3 Other Features Other features, that have been also used in (Jones and Klinkner, 2008), include temporal features: • time between queries in seconds • time between queries as a binary feature (5 mins, 10 mins, 20 mins, 30 mins, 60 mins, 120 mins) and search results feature: • cosine distance between vectors derived from the first 10 search results for the query terms. 4.5 Predicting Reformulation Type There are different strategies users use to reformulate a query which results in different types of query reformulations: • Generalization: A generalization reformulation occurs when the second query is intended to seek more general information compared to the first query • Specif</context>
</contexts>
<marker>Jones, Klinkner, 2008</marker>
<rawString>Rosie Jones and Kristina Klinkner. 2008. Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs. In Proceedings ofACM 17th Conference on Information and Knowledge Management (CIKM 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substititions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifteenth International Conference on the World-Wide Web (WWW06),</booktitle>
<pages>387--396</pages>
<contexts>
<context position="3025" citStr="Jones et al., 2006" startWordPosition="476" endWordPosition="479">y results. For example, search satisfaction is typically evaluated using clickthrough information by assuming that if a user clicks on a result, and possibly dwells for a certain amount of time, then the user is satisfied. Identifying query reformulation can be very useful for finding cases where the users are not satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representin</context>
<context position="8262" citStr="Jones et al., 2006" startWordPosition="1315" endWordPosition="1318">2008) introduced the concept of the query-flow graph where every query is represented by a node and edges connect queries if it is likely for users to move from one query to another. Mei et al. (2008) used random walks over a bipartite graph of queries and URLs to find query refinements. Query logs were used to suggest query refinements in (Baeza-Yates et al., 2005). Hierarchical agglomerative clustering was used to group similar queries that can be used as suggestions for one 1001 another. Other research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones et al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformulations. 2.3 Search Task Boundary Identification The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely addressed before. This problem is closely related to the problem of identifying query reformulation. A search task has been defined in (Jones and Klinkner, 2008) as a single information need that may result in one or more queries. Similarly , Jansen et </context>
<context position="16670" citStr="Jones et al., 2006" startWordPosition="2692" endWordPosition="2695">uery is obtained by computing the pointwise mutual information score for each pair of consecutive words. More formally, for a query x = {x1, x2, ..., xnI PMI(xi,xi+1) = log p(xi, xi+1) p(xi)p(xi+1) (1) where p(xi, xi+1) is the joint probability of occurrence of the bigram (xi, xi+1) and p(xi) and p(xi+1) are the individual occurrence probabilities of the two tokens xi and xi+1 . A segment break is introduced whenever the point wise mutual information between two consecutive words drops below a certain threshold τ. The threshold we used, τ = 0.895 , was selected to maximize the break accuracy (Jones et al., 2006) on the Bergsma-Wang-Corpus (Bergsma and Wang, 2007). Furthermore, we do not allow a break to happen between a noun and a proposition (e.g. no break can be introduced between “hotels” and “in” or “in” and “new York” in the query “hotels in new york city”). We will shorty explain how we obtained the part-of-speech tags. In addition to breaking the query into phrases, we were also interested in grouping multi-word keywords together (e.g. “new york”, “Michael Jackson”, etc.). The intuition behind that is that a query containing the keyword “new york” and another containing the keyword “new mexico</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substititions. In Proceedings of the Fifteenth International Conference on the World-Wide Web (WWW06), pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tessa Lau</author>
<author>Eric Horvitz</author>
</authors>
<title>Patterns of search: Analyzing and modeling web query refinement.</title>
<date>1999</date>
<booktitle>In ACM Press, editor, Proceedings of the Seventh International Conference on User Modeling.</booktitle>
<contexts>
<context position="3136" citStr="Lau and Horvitz, 1999" startWordPosition="494" endWordPosition="497">g that if a user clicks on a result, and possibly dwells for a certain amount of time, then the user is satisfied. Identifying query reformulation can be very useful for finding cases where the users are not satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representing queries and mostly used features of word overlap or character and word level edit distances. Take the queries</context>
<context position="6290" citStr="Lau and Horvitz, 1999" startWordPosition="993" endWordPosition="996">sed on building taxonomies of query reformulation. These taxonomies are generally constructed by examining a small set of query logs. Anick (2003) classified a random sample of 100 reformulations by hand into eleven categories. Jensen et al. (2007) identified 6 different kinds of reformulation states (New, Assistance, Content Change, Generalization, Reformulation, and Specialization) and provided heuristics for identifying them. They also used them to predict when a user is most receptive to automatic query suggestions. The same categories were used in several other studies (Guo et al., 2008; Lau and Horvitz, 1999). Huang and Efthimis (2010) proposed another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying re-finding behavior, Teevan et al. (2006) constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms to identify repeat queries, equal click queries and overlapping click queries. None of this work has built an automatic classifier distinguishing reformulation queries from other queries. Heuristics and regu</context>
</contexts>
<marker>Lau, Horvitz, 1999</marker>
<rawString>Tessa Lau and Eric Horvitz. 1999. Patterns of search: Analyzing and modeling web query refinement. In ACM Press, editor, Proceedings of the Seventh International Conference on User Modeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lucchese</author>
<author>S Orlando</author>
<author>R Perego</author>
<author>F Silvestri</author>
<author>G Tolomei</author>
</authors>
<title>Identifying task-based sessions in search engine query logs.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conference on Web Search and Data Mining(WSDM</booktitle>
<contexts>
<context position="10397" citStr="Lucchese et al., 2001" startWordPosition="1658" endWordPosition="1661">d that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner (2008) also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves considerably better results compared to timeouts. Lucchese et al. (Lucchese et al., 20011) uses a similar set of features as (Jones and Klinkner, 2008), but uses clustering to group queries in the same task together as opposed to identifying task boundary as in (Jones and Klinkner, 2008). This line of work is perhaps the closest to our work. Our work is different because it goes beyond the bag of words approach and tries to assess query similarity based on the concepts represented in each query. We compare our work to the state-of-the-art work in this area later in this paper. 3 Problem Definition We start by defining some terms that will be used throughout the paper: Definition:</context>
</contexts>
<marker>Lucchese, Orlando, Perego, Silvestri, Tolomei, 2001</marker>
<rawString>C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and G. Tolomei. 20011. Identifying task-based sessions in search engine query logs. In Proceedings of ACM Conference on Web Search and Data Mining(WSDM 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>D Zhou</author>
<author>K Church</author>
</authors>
<title>Query suggestion using hitting time.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management (CIKM</booktitle>
<pages>469--478</pages>
<contexts>
<context position="7843" citStr="Mei et al. (2008)" startWordPosition="1243" endWordPosition="1246">s line of work mainly focuses on defining taxonomies for reformulation strategies. 2.2 Automatic Query Refinement A close problem that has received most of the research attention in this area is the problem of automatically generating query refinements. These refinements are typically offered as query suggestions to the users or used to alter the user query before submitting it to the search engine. Boldi et al. (2008) introduced the concept of the query-flow graph where every query is represented by a node and edges connect queries if it is likely for users to move from one query to another. Mei et al. (2008) used random walks over a bipartite graph of queries and URLs to find query refinements. Query logs were used to suggest query refinements in (Baeza-Yates et al., 2005). Hierarchical agglomerative clustering was used to group similar queries that can be used as suggestions for one 1001 another. Other research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones et al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformul</context>
</contexts>
<marker>Mei, Zhou, Church, 2008</marker>
<rawString>Q. Mei, D. Zhou, and K. Church. 2008. Query suggestion using hitting time. In Proceeding of the 17th ACM conference on Information and knowledge management (CIKM 2008), pages 469–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mitra</author>
<author>A Singhal</author>
<author>C Buckley</author>
</authors>
<title>Improving automatic query expansion.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="8219" citStr="Mitra et al., 1998" startWordPosition="1308" endWordPosition="1311">ing it to the search engine. Boldi et al. (2008) introduced the concept of the query-flow graph where every query is represented by a node and edges connect queries if it is likely for users to move from one query to another. Mei et al. (2008) used random walks over a bipartite graph of queries and URLs to find query refinements. Query logs were used to suggest query refinements in (Baeza-Yates et al., 2005). Hierarchical agglomerative clustering was used to group similar queries that can be used as suggestions for one 1001 another. Other research has adopted methods based on query expansion (Mitra et al., 1998) or query substitution (Jones et al., 2006). This line of work is different from our work because it focuses on automatically generating query refinements while this work focuses on identifying cases of manual query reformulations. 2.3 Search Task Boundary Identification The problem of classifying the boundaries of the user search tasks within sessions in web search logs has been widely addressed before. This problem is closely related to the problem of identifying query reformulation. A search task has been defined in (Jones and Klinkner, 2008) as a single information need that may result in </context>
</contexts>
<marker>Mitra, Singhal, Buckley, 1998</marker>
<rawString>M. Mitra, A. Singhal, and C. Buckley. 1998. Improving automatic query expansion. In Proceedings of the 21th annual international ACM SIGIR conference on Research and development in information retrieval, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G V Murray</author>
<author>J Lin</author>
<author>A Chowdhury</author>
</authors>
<title>Identification of user sessions with hierarchical agglomerative clustering.</title>
<date>2006</date>
<journal>ASIST,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="9994" citStr="Murray et al. (2006)" startWordPosition="1596" endWordPosition="1599">in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu (2006) proposed a method for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner (2008) also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves considerably better results compared to timeouts. Lucchese et al. (Lucchese et al., 20011) uses a similar set of features as (Jones and Klinkner, 2008), but uses clustering to group queries in the same task together as opposed to identifying task boundary as in (Jones and Klinkner, 20</context>
</contexts>
<marker>Murray, Lin, Chowdhury, 2006</marker>
<rawString>G. V. Murray, J. Lin, and A. Chowdhury. 2006. Identification of user sessions with hierarchical agglomerative clustering. ASIST, 43(1):934–950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seda Ozmutlu</author>
</authors>
<title>Automatic new topic identification using multiple linear regression.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>42--4</pages>
<contexts>
<context position="9488" citStr="Ozmutlu (2006)" startWordPosition="1518" endWordPosition="1519">efined a session as a series of interactions by the user toward addressing a single information need. On the other hand, a query reformulation is intended to modify a previous query in hope of getting better results to satisfy the same information need. From these definitions, it is clear how query reformulation and task boundary detection are two sides of the same problem. Boldi et al. (2008) presented the concept of the query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu (2006) proposed a method for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect s</context>
</contexts>
<marker>Ozmutlu, 2006</marker>
<rawString>Seda Ozmutlu. 2006. Automatic new topic identification using multiple linear regression. Information Processing and Management, 42(4):934–950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>Query chains: learning to rank from implicit feedback.</title>
<date>2005</date>
<pages>239--248</pages>
<editor>In Robert Grossman, Roberto Bayardo, and Kristin P. Bennett, editors, KDD,</editor>
<publisher>ACM.</publisher>
<contexts>
<context position="3321" citStr="Radlinski and Joachims, 2005" startWordPosition="521" endWordPosition="524">cases where the users are not satisfied even after a click on a result that may have seemed relevant given its title and summary but then turned out to be not relevant to the user’s information need. Previous work on query reformulation has either focused on automatic query refinement by the search system, e.g. (Jones et al., 2006; Boldi et al., 2008) or on defining taxonomies for query reformulation strategies, e.g. (Lau and Horvitz, 1999; Anick, 2003). Other work has proposed solutions for the query reformulation prediction problem or for the similar problem of task boundary identification (Radlinski and Joachims, 2005; Jones and Klinkner, 2008). These solutions have adopted the 1000 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000–1010, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics bag-of-words approach for representing queries and mostly used features of word overlap or character and word level edit distances. Take the queries “hotels in New York City” and “weather in New York City” as an example. The two queries are very likely to have been issued by a user who is planning to travel to New York City. The tw</context>
<context position="9716" citStr="Radlinski and Joachims (2005)" startWordPosition="1555" endWordPosition="1558">ts to satisfy the same information need. From these definitions, it is clear how query reformulation and task boundary detection are two sides of the same problem. Boldi et al. (2008) presented the concept of the query-flow graph. A query-flow graph represents chains of related queries in query logs. They use this model for finding logical session boundaries and query recommendation. Ozmutlu (2006) proposed a method for identifying new topics in search logs. He demonstrated that time interval, search pattern and position of a query in a user session, are effective for shifting to a new topic. Radlinski and Joachims (2005) study sequences of related queries (query chains). They used that to generate new types of preference judgments from search engine logs to learn better ranked retrieval functions. Arlitt (2000) found session boundaries using a calculated timeout threshold. Murray et al. (2006) extended this work by using hierarchical clustering to find better timeout values to detect session boundaries. Jones and Klinkner (2008) also addressed the problem of classifying the boundaries of the goals and missions in search logs. They showed that using features like edit distance and common words achieves conside</context>
<context position="11360" citStr="Radlinski and Joachims, 2005" startWordPosition="1824" endWordPosition="1827">es to assess query similarity based on the concepts represented in each query. We compare our work to the state-of-the-art work in this area later in this paper. 3 Problem Definition We start by defining some terms that will be used throughout the paper: Definition: Query Reformulation is the act of submitting a query Q2 to modify a previous search query Q1 in hope of retrieving better results to satisfy the same information need. Definition: A Search Session is group of queries and clicks demarcated with a 30-minute inactivity timeout, such as that used in previous work (Downey et al., 2007; Radlinski and Joachims, 2005). Search engines receive streams of queries from users. In response to each query, the engine returns a set of search results. Depending on these results, the user may decide to click on one or more results, submit another query, or end the search session. In this work, we focus on cases where the user submits another query. Our objective is to solve the following problem: Given a query Q1, and the following query Q2, predict whether Q2 is reformulation of Q1. 4 Approach In this section, we propose methods for predicting whether the current query has been issued by the user to reformulate the </context>
</contexts>
<marker>Radlinski, Joachims, 2005</marker>
<rawString>Filip Radlinski and Thorsten Joachims. 2005. Query chains: learning to rank from implicit feedback. In Robert Grossman, Roberto Bayardo, and Kristin P. Bennett, editors, KDD, pages 239–248. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Teevan</author>
<author>Eytan Adar</author>
<author>Rosie Jones</author>
<author>Michael Potts</author>
</authors>
<title>History repeats itself: Repeat queries in yahoo’s logs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>703--704</pages>
<contexts>
<context position="6578" citStr="Teevan et al. (2006)" startWordPosition="1038" endWordPosition="1041">tion states (New, Assistance, Content Change, Generalization, Reformulation, and Specialization) and provided heuristics for identifying them. They also used them to predict when a user is most receptive to automatic query suggestions. The same categories were used in several other studies (Guo et al., 2008; Lau and Horvitz, 1999). Huang and Efthimis (2010) proposed another reformulation taxonomy. Their taxonomy was lexical in nature (e.g., word reorder, adding words, removing words, etc.). They also proposed the use of regular expressions to identify them. While studying re-finding behavior, Teevan et al. (2006) constructed a taxonomy of query re-finding by manually examining query logs, and implemented algorithms to identify repeat queries, equal click queries and overlapping click queries. None of this work has built an automatic classifier distinguishing reformulation queries from other queries. Heuristics and regular expressions have been used in (Huang et al., 2010) and (Jansen et al., 2007) to identify different types of reformulations. This line of work is relevant to our work because it studies query reformulation strategies. Our work is different because we build a machine-learned predictive</context>
</contexts>
<marker>Teevan, Adar, Jones, Potts, 2006</marker>
<rawString>Jamie Teevan, Eytan Adar, Rosie Jones, and Michael Potts. 2006. History repeats itself: Repeat queries in yahoo’s logs. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 703–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceeding of the Human Language Technologies Conference and the Annual Meeting of the North American Association of Computational Linguists (HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="18738" citStr="Toutanova et al., 2003" startWordPosition="3053" endWordPosition="3056">uld be used to identify the head and modifiers from every phrase. However, because queries are typical short and not always well-formed sentences, this may pose a challenge to the dependency parser. But as we are mainly interested in short noun phrases, we can apply a simple set of rules to identify the head keyword of each phrase using the part of speech tags of the words in the phrase. A part-of-speech (POS) tagger assigns parts of speech to each word in an input text, such as noun, verb, adjective, etc. We used the Stanford POS tagger, using Stanford CoreNLP, to assign POS tags to queries (Toutanova et al., 2003). To identify the head and attributes of every noun phrase, we use the following rules: • For phrases with of the form: “NNX+” (i.e. one more nouns, where NNX could be NN: noun, singular, NNS: noun, plural, NNP: proper noun, singular or NNPS: proper noun, plural), the head is the last noun keyword and all other keywords are treated as attributes/modifiers. • For the phrases of the form “NNX+ IN NNX+”, where IN denotes a preposition or a subordinating conjunction (e.g. “in”, “of”, etc.), the head is the last noun keyword before the preposition. Table 1 shows different examples of queries, the c</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceeding of the Human Language Technologies Conference and the Annual Meeting of the North American Association of Computational Linguists (HLT-NAACL 2003), pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wang</author>
<author>C Thrasher</author>
<author>B Hsu</author>
</authors>
<title>Web scale nlp: A case study on url word breaking.</title>
<date>2011</date>
<booktitle>In Proceeding of the ACM World Wide Web Conference (WWW</booktitle>
<pages>357--366</pages>
<contexts>
<context position="13865" citStr="Wang et al., 2011" startWordPosition="2211" endWordPosition="2214">iers = “apple ipad”} tommy bhama rug tommy bhama rug Concept1{head =“rug”, modifiers ¯‘‘tommy bhama”} tommy bhama perfume tommy bhama perfume Concept1{head =”perfume”, modifiers ¯“tommy bhama”} useful for many natural language processing applications. This becomes a frequent problems with queries when users do not observe the correct word boundaries (for example: “southjerseycraigslist” for “south jersey craiglist”) or when users are searching for a part of a URL (for example “quincycollege” for “quincy college”). We used a freely available word breaker Web service that has been described at (Wang et al., 2011). 4.2 Queries to Concepts Lexical similarity between queries has been often used to identify related queries (Jansen et al., 2007). The problem with lexical similarity is that it introduces many false negatives (e.g. synonyms) , but this can be handled by other features as we will describe later. More seriously, it introduces many false positives. Take the following query pair as an example Q1: weather in new york city and Q2: “hotels in new york city”. Out of 5 words, 4 words are shared between Q1 and Q2. Hence, any lexical similarity feature would predict that the user submitted Q2 as a refo</context>
</contexts>
<marker>Wang, Thrasher, Hsu, 2011</marker>
<rawString>K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale nlp: A case study on url word breaking. In Proceeding of the ACM World Wide Web Conference (WWW 2011), pages 357–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryen W White</author>
<author>Steven M Drucker</author>
</authors>
<title>Investigating behavioral variability in web search.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web.</booktitle>
<contexts>
<context position="26610" citStr="White and Drucker, 2007" startWordPosition="4435" endWordPosition="4438">but not in Q2 • 1 if Q1 contains all Q2s concepts • 1 if Q2 contains all Q1s concepts • all concept features above recomputed for keywords instead of concepts 1006 5 Experiments and Results 5.1 Data Our data consists of query pairs randomly sampled from the queries submitted to a commercial search engine during a week in mid-2012. Every record in our data consisted of a consecutive query pair (Qi,Qi+1) submitted to the search engine by the same user and in the same session (i.e. within less than 30 minutes of idle time, the 30 minutes threshold has been frequently used in previous work, e.g. (White and Drucker, 2007)). Identical queries were excluded from the data because they are always labeled as reformulation and their label is very easy to predict. Hence, when included, they result in unrealistically high estimates of the performance of the proposed methods. All data in the session to which the sampled query pair belongs were recorded. In addition to queries, the data contained a timestamp for each page view, all elements shown in response to that query (e.g. Web results, answers, etc.), and visited Web page or clicked answers. Intranet and secure URL visits were excluded. Any personally identifiable </context>
</contexts>
<marker>White, Drucker, 2007</marker>
<rawString>Ryen W. White and Steven M. Drucker. 2007. Investigating behavioral variability in web search. In Proceedings of the 16th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryen W White</author>
<author>Wei Chu</author>
<author>Ahmed Hassan</author>
<author>Xiaodong He</author>
<author>Yang Song</author>
<author>Hongning Wang</author>
</authors>
<title>Enhancing personalized search by mining and modeling task behavior.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web, WWW ’13,</booktitle>
<pages>1411--1420</pages>
<contexts>
<context position="21817" citStr="White et al., 2013" startWordPosition="3585" endWordPosition="3588">e semantic similarity between these two phrases can be measured by estimating the probability of one of them being a translation of another. The translation probabilities can be estimated using the IBM Model 1 (Brown et al., 1993; Berger and Lafferty, 1999). The model was originally proposed to model the probability of translating from one sequence of words in one language to another. It has been also used in different IR applications to estimate the probability of translating from one sequence of words to another sequence in the same language (e.g. (Gao et al., 2012), (Gao et al., 2010) and (White et al., 2013)). More formally, the similarity between two sequences of words, Q = {q1, ..., qI} and S = {s1, ..., sJ}, can be defined as: P(S|Q) = I J P(si|qj)P(qj|Q) (2) i=1 j=1 where P(q|Q) is the unigram probability of word q in query Q. The word translation probabilities P(s|q) are estimated using the querytitle pairs derived from the clickthrough search logs, assuming that the title terms are likely to be the desired alternation of the paired query. where a is a constant, I is the token length of S, and J is the token length of Q. The query-title pairs used for model training are sampled from one year</context>
</contexts>
<marker>White, Chu, Hassan, He, Song, Wang, 2013</marker>
<rawString>Ryen W. White, Wei Chu, Ahmed Hassan, Xiaodong He, Yang Song, and Hongning Wang. 2013. Enhancing personalized search by mining and modeling task behavior. In Proceedings of the 22nd international conference on World Wide Web, WWW ’13, pages 1411– 1420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yu</author>
<author>H Shi</author>
</authors>
<title>Query segmentation using conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Keyword Search on Structured Data (KEYS),</booktitle>
<pages>21--26</pages>
<contexts>
<context position="15359" citStr="Yu and Shi, 2009" startWordPosition="2476" endWordPosition="2479">r is looking for “hotels”. We would like to recognize “weather”, and “hotels” as the head keywords of Q1 and Q2 respectively, while “new york city” is a modifier of the head keyword in both cases. To build such a representation, we start by segmenting each query into phrases. Query segmentation is the process of taking a users search query and dividing the tokens into individual phrases or semantic units (Bergsma and Wang, 2007). Many approaches to query segmentation have been presented in recent research. Some of them pose the problem as a supervised learning problem (Bergsma and Wang, 2007; Yu and Shi, 2009). Many of the supervised methods though use expensive features that are difficult to reimplement. On the other hand, many unsupervised methods for query segmentation have also been proposed (Hagen et al., 2011; Hagen et al., 2010). Most of these methods use only raw web n-gram frequencies and are very easy to re-implement. Additionally, Hagen et al. (2010) have shown that these methods can achieve segmentation accuracy comparable to current state-of-the-art techniques using supervised learning. We opt for the unsupervised techniques to perform query segmentation. More specifically, we adopt th</context>
</contexts>
<marker>Yu, Shi, 2009</marker>
<rawString>X. Yu and H. Shi. 2009. Query segmentation using conditional random fields. In Proceedings of the Workshop on Keyword Search on Structured Data (KEYS), pages 21–26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
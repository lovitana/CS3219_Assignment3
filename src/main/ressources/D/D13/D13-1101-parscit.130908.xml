<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010883">
<title confidence="0.987637">
Automatically Detecting and Attributing Indirect Quotations
</title>
<author confidence="0.999467">
Silvia Pareti°* Tim O’Keefe†* Ioannis Konstas° James R. Curran† Irena Koprinska†
</author>
<affiliation confidence="0.9989915">
°ILCC, School of Informatics †-lab, School of IT
University of Edinburgh University of Sydney
</affiliation>
<address confidence="0.872217">
United Kingdom NSW 2006, Australia
</address>
<email confidence="0.999382">
{s.pareti,i.konstas}@sms.ed.ac.uk {tokeefe,james,irena}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.995669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9985816875">
Direct quotations are used for opinion min-
ing and information extraction as they have an
easy to extract span and they can be attributed
to a speaker with high accuracy. However,
simply focusing on direct quotations ignores
around half of all reported speech, which is
in the form of indirect or mixed speech. This
work presents the first large-scale experiments
in indirect and mixed quotation extraction and
attribution. We propose two methods of ex-
tracting all quote types from news articles and
evaluate them on two large annotated corpora,
one of which is a contribution of this work.
We further show that direct quotation attribu-
tion methods can be successfully applied to in-
direct and mixed quotation attribution.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999629615384615">
Quotations are crucial carriers of information, par-
ticularly in news texts, with up to 90% of sentences
in some articles being reported speech (Bergler
et al., 2004). Reported speech is a carrier of evi-
dence and factuality (Bergler, 1992; Sauriand Puste-
jovsky, 2009), and as such, text mining applications
use quotations to summarise, organise and validate
information. Extraction of quotations is also rele-
vant to researchers interested in media monitoring.
Most quotation attribution studies (Pouliquen
et al., 2007; Glass and Bangay, 2007; Elson and
McKeown, 2010) thus far have limited their scope
to direct quotations (Ex.1a), as they are delimited
</bodyText>
<note confidence="0.706561">
*These authors contributed equally to this work.
</note>
<bodyText confidence="0.996548727272727">
by quotation marks, which makes them easy to ex-
tract. However, annotated resources suggest that di-
rect quotations represent only a limited portion of all
quotations, i.e., around 30% in the Penn Attribution
Relation Corpus (PARC), which covers Wall Street
Journal articles, and 52% in the Sydney Morning
Herald Corpus (SMHC), with the remainder being in-
direct (Ex.1c) or mixed (Ex.1b) quotations. Retriev-
ing only direct quotations can miss key content that
can change the interpretation of the quotation (Ex.
1b) and will entirely miss indirect quotations.
</bodyText>
<listItem confidence="0.857694333333333">
(1) a. “For 10 million, you can move $100 mil-
lion of stocks,” a specialist on the Big Board
gripes. “That gives futures traders a lot
more power.”
b. Police would only apply for the restrictions
when “we have a lot of evidence that late-
night noise... is disturbing the residents of
that neighbourhood”, Superintendent Tony
Cooke said.
c. Mr Walsh said Rio was continuing to hold
discussions with its customers to arrive at a
mutually agreed price.
</listItem>
<bodyText confidence="0.9989298">
Previous work on extracting indirect and mixed
quotations has suffered from a lack of large-scale
data, and has instead used hand-crafted lexica of re-
porting verbs with rule-based approaches. The lack
of data has also made comparing the relative merit
of these approaches difficult, as existing evaluations
are small-scale and do not compare multiple meth-
ods on the same data.
In this work we address this lack of clear, com-
parable results by evaluating two baseline meth-
</bodyText>
<page confidence="0.9734">
989
</page>
<note confidence="0.979573">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989–999,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Method Language Test Size Results
(quotations) P R
Krestel et al. (2008) hand-built grammar English 133 74% 99%
Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1
Fernandes et al. (2011) ML and regex Portuguese 205 64%2 67%2
de La Clergerie et al. (2011) patterns over parse French 40 87% 70%
Schneider et al. (2010) hand-built grammar English N/D 56%2 52%2
</note>
<tableCaption confidence="0.937169">
Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable
as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the
authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly.
</tableCaption>
<bodyText confidence="0.999820846153846">
ods against both a token-based approach that uses a
Conditional Random Field (CRF) to predict IOB la-
bels, and a maximum entropy classifier that predicts
whether parse nodes are quotations or not. We eval-
uate these approaches on two large-scale corpora
from the news domain that together include over
18,000 quotations. One of these corpora (SMHC) is a
a contribution of this work, while our results are the
first presented on the other corpus (PARC). Instead
of relying on a lexicon of reporting verbs, we de-
velop a classifier to detect verbs introducing a quo-
tation. To inform future research we present results
for direct, indirect, and mixed quotations, as well as
overall results.
Finally, we use the direct quotation attribution
methods described in O’Keefe et al. (2012) and
show that they can be successfully applied to indi-
rect and mixed quotations, albeit with lower accu-
racy. This leads us to conclude that attributing indi-
rect and mixed quotations to speakers is harder than
attributing direct quotations.
With this work, we set a new state of the art in
quotation extraction. We expect that the main con-
tribution of this work will be that future methods can
be evaluated in a comparable way, so that the relative
merit of various approaches can be determined.
</bodyText>
<sectionHeader confidence="0.989119" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9963186">
Pareti (2012) defines an attribution as having a
source span, a cue span, and a content span:
Source is the span of text that indicates who the
content is attributed to, e.g. ‘president Obama’,
‘analysts’, ‘China’, ‘she’.
Cue is the lexical anchor of the attribution relation,
usually a verb, e.g. ‘say’, ‘add’, ‘quip’.
Content is the span of text that is attributed.
Based on the type of attitude the source expresses
towards a proposition or eventuality, attributions are
subcategorised (Prasad et al., 2006) into assertions
(Ex.2a) and beliefs (Ex.2b), which imply different
degrees of commitment, facts (Ex.2c), expressing
evaluation or knowledge, and eventualities (Ex.2d),
expressing intention or attitude.
</bodyText>
<listItem confidence="0.98930925">
(2) a. Mr Abbott said that he will win the election.
b. Mr Abbott thinks he will win the election.
c. Mr Abbott knew that Gillard was in Sydney.
d. Mr Abbott agreed to the public sector cuts.
</listItem>
<bodyText confidence="0.9990646">
Only assertion attributions necessarily imply a
speech act. Their content corresponds to a quotation
span and their source is generally referred to in the
literature as the speaker. Direct, indirect and mixed
quotations differ in the degree of factuality they en-
tail, since the former are by convention interpreted
as a verbatim transcription of an utterance whereas
indirect and the non-quoted portion of mixed quota-
tions can be paraphrased forms of the original word-
ing, and are thus filtered by the writer’s perspective.
The first speaker attribution systems (Zhang et al.,
2003; Mamede and Chaleira, 2004; Glass and Ban-
gay, 2007) originate from the narrative domain and
were concerned with the identification of different
characters for speech synthesis applications. Direct
quotation attribution, with direct quotations being
given or extracted heuristically, has been the focus
of further studies in both the narrative (Elson and
McKeown, 2010) and news (Pouliquen et al., 2007;
Liang et al., 2010) domains. The few studies that
</bodyText>
<page confidence="0.993482">
990
</page>
<bodyText confidence="0.99992075">
have addressed the extraction and attribution of in-
direct and mixed quotations are discussed below.
Krestel et al. (2008) developed a quotation ex-
traction and attribution system that combines a lexi-
con of 53 common reporting verbs and a hand-built
grammar to detect constructions that match 6 gen-
eral lexical patterns. They evaluate their work on 7
articles from the Wall Street Journal, which contain
133 quotations, achieving macro-averaged Precision
(P) of 99% and Recall (R) of 74% for quotation
span detection. PICTOR (Schneider et al., 2010) re-
lies instead on a context-free grammar for the extrac-
tion and attribution of quotations. PICTOR yielded
75% P and 86% R in terms of words correctly as-
cribed to a quotation or speaker, while it achieved
56% P and 52% R when measured in terms of com-
pletely correct quotation-speaker pairs.
SAPIENS (de La Clergerie et al., 2011) extracts
quotations from French news, by using a lexicon
of reporting verbs and syntactic patterns to extract
the complement of a reporting verb as the quota-
tion span and its subject as the source. They eval-
uated 40 randomly sampled quotations and found
that their system made 32 predictions and correctly
identified the span in 28 of the 40 cases. Verba-
tim (Sarmento and Nunes, 2009) extracts quotations
from Portuguese news feeds by first finding one of
35 speech verbs and then matching the sentence to
one of 19 patterns. Their manual evaluation shows
that 11.9% of the quotations Verbatim finds are er-
rors and that the system identifies approximately one
distinct quotation for every 46 news articles.
The system presented by Fernandes et al. (2011)
also works over Portuguese news. Their work is the
closest to ours as they partially apply supervised ma-
chine learning to quotation extraction. Their work
introduces GloboQuotes, a corpus of 685 news items
containing 1,007 quotations of which 802 were used
to train an Entropy Guided Transformation Learn-
ing (ETL) algorithm (dos Santos and Milidi´u, 2009).
They treat quotation extraction as an IOB labelling
task, where they use ETL with POS and NE features
to identify the beginning of a quotation, while the
inside and outside labels are found using regular ex-
pressions. Finally they use ETL to attribute quota-
tions to their source. The overall system achieves
64% P and 67% R.
We have summarised these approaches in Table 1,
</bodyText>
<table confidence="0.998086625">
SMHC PARC
Corpus Doc Corpus Doc
Docs 965 - 2,280 -
Tokens 601k 623.3 1,139k 499.9
Quotations 7,991 8.3 10,526 4.6
Direct 4,204 4.4 3,262 1.4
Indirect 2,930 3.0 5,715 2.5
Mixed 857 0.9 1,549 0.6
</table>
<tableCaption confidence="0.927272333333333">
Table 2: Comparison of the SMHC and PARC corpora, re-
porting their document and token size and per-type occur-
rence of quotations overall and per document (average).
</tableCaption>
<bodyText confidence="0.9997575">
which shows that the majority of evaluations thus far
have been small-scale. Furthermore, the published
results do not include any comparisons with previ-
ous work, which prevents a quantitative comparison
of the approaches, and they do not include results
broken down by whether the quotation is direct, in-
direct, or mixed. It is these issues that motivate our
work.
</bodyText>
<sectionHeader confidence="0.996888" genericHeader="method">
3 Corpora
</sectionHeader>
<bodyText confidence="0.999866">
We perform our experiments over two large corpora
from the news domain.
</bodyText>
<subsectionHeader confidence="0.998976">
3.1 Penn Attribution Relations Corpus (PARC)
</subsectionHeader>
<bodyText confidence="0.999991263157895">
Our first corpus (Pareti, 2012), which we will re-
fer to as PARC, is a semi-automatically built ex-
tension to the attribution annotations included in
the PDTB (Prasad et al., 2008). The corpus covers
2,280 Wall Street Journal articles and contains an-
notations of assertions, beliefs, facts, and eventual-
ities, which are altogether referred to as attribution
relations (ARs). For this work we use only the asser-
tions, as they correspond to quotations (direct, indi-
rect and mixed). The drawback of this corpus is that
it is not yet fully annotated, i.e., it comprises positive
and unlabelled data.
The corpus includes a test set of 14 articles that
are fully annotated, which enables us to properly
evaluate our work and estimate that a proportion of
30-50% of ARs are unlabelled in the rest of the cor-
pus. The test set was manually annotated by two ex-
pert annotators. The annotators identified 491 ARs,
of which 22% were nested within another AR, with
</bodyText>
<page confidence="0.990173">
991
</page>
<bodyText confidence="0.999978833333333">
an agreement score of 87%1. The agreement for the
selection of the content and source spans of com-
monly annotated ARs was 95% and 94% respec-
tively. In this work we address only non-embedded
assertions, so the final test-set includes 267 quotes,
totalling 321 non-discontinuous gold spans.
</bodyText>
<subsectionHeader confidence="0.999817">
3.2 Sydney Morning Herald Corpus (SMHC)
</subsectionHeader>
<bodyText confidence="0.999998733333333">
We based our second corpus on the existing anno-
tations of direct quotations within Sydney Morning
Herald articles presented in O’Keefe et al. (2012).
In that work we defined direct quotations as any
text between quotation marks, which included the
directly-quoted portion of mixed quotations, as well
as scare quotes. Under that definition direct quo-
tations could be automatically extracted with very
high accuracy, so annotations in that work were
over the automatically extracted direct quotations.
As part of this work one annotator removed scare
quotes, updated mixed quotations to include both
the directly and indirectly quoted portions, and
added whole new indirect quotations. The anno-
tation scheme was developed to be comparable to
the scheme used in the PARC corpus (Pareti, 2012),
although the SMHC corpus only includes assertions
and does not annotate the lexical cue.
The resulting corpus contains 7,991 quotations
taken from 965 articles from the 2009 Sydney Morn-
ing Herald (we refer to this corpus as SMHC). The
annotations in this corpus also include the speakers
of the quotations, as well as gold standard Named
Entities (NEs). We use 60% of this corpus as train-
ing data (4,872 quotations), 10% as development
data (759 quotations), and 30% as test data (2,360
quotations). Early experiments were conducted over
the development data, while the final results were
trained on both the training and development sets
and were tested on the unseen test data.
</bodyText>
<subsectionHeader confidence="0.997919">
3.3 Comparison
</subsectionHeader>
<bodyText confidence="0.98895775">
Table 2 shows a comparison of the two corpora and
the quotations annotated within them. SMHC has a
higher density of quotations per document, 8.3 vs.
4.6 in PARC, since articles are fully annotated and
</bodyText>
<footnote confidence="0.8645825">
1The agreement was calculated using the agr metric de-
scribed in Wiebe and Riloff (2005) as the proportion of com-
monly annotated ARs with respect to the ARs identified overall
by Annotator A and Annotator B respectively
</footnote>
<table confidence="0.999794">
P R F
Bsay 94.4 43.5 59.5
Blist 75.4 71.1 73.2
k-NN 88.9 72.6 79.9
</table>
<tableCaption confidence="0.993694">
Table 3: Results for the k-NN verb-cue classifier. Bsay
</tableCaption>
<bodyText confidence="0.961964090909091">
classifies as verb-cue all instances of say while Blist
marks as verb-cues all verbs from a pre-compiled list in
Krestel et al. (2008).
were selected to contain at least one quotation. PARC
is instead only partially annotated and comprises ar-
ticles with no quotations. Excluding null-quotation
articles from PARC, the average incidence of anno-
tated quotations per article raises to 7.1. The corpora
also differ in quotation type distribution, with di-
rect quotations being largely predominant in SMHC
while indirect are more common in PARC.
</bodyText>
<sectionHeader confidence="0.99791" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997878">
4.1 Quotation Extraction
</subsectionHeader>
<bodyText confidence="0.999944636363636">
Quotation extraction is the task of extracting the
content span of all of the direct, indirect, and mixed
quotations within a given document. More pre-
cisely, we consider quotations to be acts of com-
munication, which correspond to assertions in Pareti
(2012). Some quotations have content spans that are
split into separate, non-adjacent spans, as in exam-
ple (1a). Ideally the latter span should be marked as
a continuation of a quotation, however we consider
this to be out of scope for this work, so we treat each
span as a separate quotation.
</bodyText>
<subsectionHeader confidence="0.995436">
4.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999966888888889">
As a pre-processing step, both corpora were to-
kenised and POS tagged, and the potential speak-
ers anonymised to prevent over-fitting. We used the
Stanford factored parser (Klein and Manning, 2002)
to retrieve both the Stanford dependencies and the
phrase structure parse. Quotation marks were nor-
malised to a single character, as the quotation di-
rection is often incorrect for multi-paragraph quo-
tations.
</bodyText>
<subsectionHeader confidence="0.997606">
4.3 Verb-cue Classifier
</subsectionHeader>
<bodyText confidence="0.998403">
Verbs are by far the most common introducer of a
quotation. In PARC verbs account for 96% of all
</bodyText>
<page confidence="0.988199">
992
</page>
<bodyText confidence="0.999081947368421">
cues, the prepositional phrase according to for 3%,
with the remaining 1% being nouns, adverbials and
prepositional groups. Attributional verbs are not a
closed set, they can vary across styles and genres,
and their attributional use is highly dependent on the
context in which they occur. It is therefore not possi-
ble to simply rely on a pre-compiled list of common
speech verbs. Quotations in PARC are introduced by
232 verb types, 87 of which are unique occurrences.
Not all of the verbs are speech verbs, for example
add, which is the second most frequent after say, or
the manner verb gripe (Ex.1a).
We used the attributional cues in the PARC cor-
pus to develop a separate component of our system
to identify attribution verb-cues. The classifier pre-
dicts whether the head of each verb group is a verb-
cue using the k-nearest neighbour (k-NN) algorithm,
with k equal to 3. The classifier uses 20 feature
types, including:
</bodyText>
<listItem confidence="0.999828">
• Lexical (e.g. token, lemma, adjacent tokens)
• VerbNet classes membership
• Syntactic (e.g. node-depth in the sentence, par-
ent and sibling nodes)
• Sentence features (e.g. distance from sentence
</listItem>
<bodyText confidence="0.991452458333333">
start/end, within quotation markers).
We compared the system to one baseline, Bso,y,
that marks every instance of say as a verb-cue, and
another, Blit, that marks every instance of a verb
that is on the list of 53 verbs presented in Krestel
et al. (2008). We tested the system on the test set for
PARC, which contains 1809 potential verb-cues, of
which 354 are positive and 1455 are negative.
The results in Table 3 show that the verb-cue
classifier can outperform expert-derived knowledge.
The classifier was able to identify verb-cues with P
of 88.9% and R of 72.6%. While frequently oc-
curring verbs are highly predictive, the inclusion of
VerbNet classes (Schuler, 2005) and contextual fea-
tures allows for a more accurate classification of pol-
ysemous and unseen verbs.
Since PARC contains labelled and unlabelled attri-
butions, which is detrimental for training, we used
the verb-cue classifier to identify in the corpus sen-
tences that we suspected contained an unlabelled at-
tribution. Sentences containing a verb classified as a
cue that do not contain a quotation were removed
from the training set for the quotation extraction
model.
</bodyText>
<subsectionHeader confidence="0.971593">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9998008125">
We use two metrics, listed below, for evaluating the
quotation spans predicted by our model against the
gold spans from the annotation.
Strict The first is a strict metric where a predicted
span is only considered to be correct if it exactly
matches a span from the gold standard. The stan-
dard precision, recall, and F-score can be calculated
using this definition of correctness. The drawback of
this strict score is that if a prediction is incorrect by
as little as one token it will be considered completely
incorrect.
Partial We also consider an overlap metric
(Hollingsworth and Teufel, 2005), which allows
partially correct predictions to be proportionally
counted. Precision (P), recall (R), and F-score for
this method are:
</bodyText>
<equation confidence="0.999458571428572">
P _ EgEgold EpEpred overlap(g, p) 1
|pred |( )
R _ EgEgold EpEpred overlap(p, g) 2
|gold |( )
F
_ 2PR (3)
(P + R)
</equation>
<bodyText confidence="0.999871">
Where overlap(x, y) returns the proportion of to-
kens of y that are overlapped by x. For each of these
metrics we report the micro-average, as the number
of quotations in each document varies significantly.
When reporting P for the typewise results we re-
strict the set of predicted quotations to only those
with the requisite type, while still considering the
full set of gold quotations. Similarly, when calculat-
ing R we restrict the set of gold quotations to only
those with the required type.
</bodyText>
<subsectionHeader confidence="0.969504">
4.5 Baselines
</subsectionHeader>
<bodyText confidence="0.99874125">
We have developed two baselines inspired by the
current lexical/syntactic pattern-based approaches
in the literature, which combine speech verbs and
hand-crafted rules.
</bodyText>
<page confidence="0.995067">
993
</page>
<bodyText confidence="0.9997278">
Ble� Lexical: cue verb + the longest of the spans be-
fore or after it until the sentence boundary.
Bsy,,, Syntactic: cue verb + verb syntactic object.
Bsy,,, is close to the model in de La Clergerie
et al. (2011).
Instead of relying on a lexicon of verbs, our base-
lines use those identified by the verb-cue classifier.
As direct quotations are not always explicitly intro-
duced by a cue-verb, we defined a separate baseline
with a rule-based approach (Brule) that returns text
between quotation marks that has at least 3 tokens,
and where the non-stopword and non-proper noun
tokens are not all title cased. In our full results we
apply each method along with Brule and greedily
take the longest predicted spans that do not overlap.
</bodyText>
<sectionHeader confidence="0.996458" genericHeader="method">
5 Supervised Approaches
</sectionHeader>
<bodyText confidence="0.996155888888889">
We present two supervised approaches to quotation
extraction, which operate over the tokens and the
phrase-structure parse nodes respectively. Despite
the difference in the item being classified, these ap-
proaches have some common features:
Lexical: unigram and bigram versions of the token,
lemma, and POS tags within a window of 5 to-
kens either side of the target, all indexed by po-
sition.
Sentence: features indicating whether the sentence
contains a quotation mark, a NE, a verb-cue, a
pronoun, or any combination of these. There is
also a sentence length feature.
Dependency: relation with parent, relations with
any dependants, as well as versions of these
that include the head and dependent tokens.
External knowledge: position-indexed features for
whether any of the tokens in the sentence match
a known role, organisation, or title. The titles
come from a small hand-built list, while the
role and organisation lists were built by recur-
sively following the WordNet (Fellbaum, 1998)
hyponyms of person and organization respec-
tively.
Other: features for whether the target is within quo-
tation marks, and whether there is a verb-cue
near the end of the sentence.
</bodyText>
<table confidence="0.999409333333333">
Strict Partial
P R F P R F
PARC Brule 75 94 83 96 94 95
Token 97 91 94 98 97 97
SMHC Brule 87 93 90 98 94 96
Token 94 90 92 99 97 98
</table>
<tableCaption confidence="0.991677">
Table 4: PARC and SMHC results on direct quotations.
The token based approach is trained and tested on all quo-
tations.
</tableCaption>
<subsectionHeader confidence="0.972749">
5.1 Token-based Approach
</subsectionHeader>
<bodyText confidence="0.999854322580646">
The token-based approach treats quotation extrac-
tion as analogous to NE tagging, where there are a
sequence of tokens that need to be individually la-
belled. Each token is given either an I, an O, or a B
label, where B denotes the first token in a quotation,
I denotes the token is inside a quotation, and O indi-
cates that the token is not part of a quotation. For NE
tagging it is common to use a sentence as a single
sequence, as NEs do not cross sentence boundaries.
This does not work for quotations, as they can cross
sentence and even paragraph boundaries. As such,
we treat the entire document as a single sequence,
which allows the predicted quotations to span both
sentence and paragraph bounds.
We use a linear chain Conditional Random Field
(CRF)2 as the learning algorithm, with the common
features listed above, as well as the following fea-
tures:
Verb: features indicating whether the current token
is a (possibly indirect) dependent of a verb-cue,
and another for whether the token is at the start
of a constituent that is a dependent of a verb-
cue.
Ancestor: the labels of all constituents that contain
the current token in their span, indexed by their
depth in the parse tree.
Syntactic: the label, depth, and token span size of
the highest constituent where the current token
is the left-most token in the constituent, as well
as its parent, and whether either of those con-
tains a verb-cue.
</bodyText>
<footnote confidence="0.970358">
2http://www.chokkan.org/software/crfsuite/
</footnote>
<page confidence="0.992274">
994
</page>
<table confidence="0.999870307692308">
Indirect Mixed All1
Strict P R F P R F P R F
Blex 34 32 33 17 26 20 46 44 45
Bsyn 78 46 58 61 40 49 80 63 70
Token 66 54 59 55 58 56 76 70 73
Constituent 61 50 55 50 38 43 70 64 67
ConstituentG 66 42 51 68 49 57 76 62 68
Partial P R F P R F P R F
Blex 56 66 61 78 79 78 73 79 76
Bsyn 89 58 70 88 75 81 92 74 82
Token 79 74 76 85 90 87 87 86 87
Constituent 78 67 72 84 82 83 86 80 83
ConstituentG 80 54 65 90 80 85 90 74 81
</table>
<tableCaption confidence="0.988432333333333">
Table 5: Results on PARC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations. ConstituentG shows the
results for the constituent model using the gold parse.
</tableCaption>
<subsectionHeader confidence="0.994251">
5.2 Constituent-based Approach
</subsectionHeader>
<bodyText confidence="0.997419038461539">
The constituent approach classifies whole phrase
structure nodes as either quotation or not a quota-
tion. Ideally each quotation would match exactly
one constituent, however this is not always the case
in our data. In cases without an exact match we la-
bel every constituent that is a subspan of the quo-
tation as a quotation as long as it has a parent that
is not a subspan of the quotation. In these cases
multiple nodes will be labelled quotation, so a post-
processing step is introduced that rebuilds quota-
tions by merging predicted spans that are adjacent or
overlapping within a sentence. Restricting the merg-
ing process this way loses the ability to predict quo-
tations that cover more than a sentence, but without
this restriction too many predicted quotations are er-
roneously merged.
This approach uses a maximum entropy classi-
fier3 with L1 regularisation. In early experiments
we found that the constituent-based approach per-
formed poorly when trained on all quotations, so for
these experiments the constituent classifier is trained
only on indirect and mixed quotations. The classifier
uses the common features listed above as well as the
following features:
Span: length of the span, features for whether there
is a verb or a NE.
</bodyText>
<footnote confidence="0.758061">
3http://scikit-learn.org/
</footnote>
<bodyText confidence="0.99185825">
Node: the label, number of descendants, number of
ancestors, and number of children of the target.
Context: dependency, node, and span features for
the parent and siblings of the target.
In addition the lexical features described earlier
are applied to both the start and end tokens of the
node’s span, as well as the highest token in the de-
pendency parse that is within the span.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="method">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999459">
6.1 Direct Quotations
</subsectionHeader>
<bodyText confidence="0.9999395625">
Table 4 shows the results for predicting direct quota-
tions on PARC and SMHC. In both corpora and with
both metrics the token-based approach outperforms
Brnle. Although direct quotations should be trivial
to extract, and a simple system that returns the con-
tent between quotation marks should be hard to beat,
there are two main factors that confound the rule-
based system.
The first is the presence of mixed quotations,
which is most clearly demonstrated in the difference
between the strict precision scores and the partial
precision scores for Brnle. Brnle will find all of
the directly-quoted portions of mixed quotes, which
do not exactly match a quotation, and so will re-
ceive a low precision score with the strict metric.
However the partial overlap score will reward these
</bodyText>
<page confidence="0.995192">
995
</page>
<table confidence="0.999897909090909">
Indirect Mixed All1
Strict P R F P R F P R F
Blex 37 42 40 15 36 21 50 50 50
Bsyn 63 49 55 67 36 47 82 72 76
Token 69 53 60 80 91 85 82 75 78
Constituent 54 49 51 64 42 51 77 72 75
Partial P R F P R F P R F
Blex 52 68 59 87 77 82 77 84 81
Bsyn 75 59 66 89 66 76 91 80 85
Token 82 67 74 88 84 86 92 86 89
Constituent 77 63 69 91 75 82 91 82 86
</table>
<tableCaption confidence="0.9570035">
Table 6: Results on SMHC. 1All reports the results over all quotations (direct, indirect and mixed). For the baselines,
this is a combination of the strategy in Blex or Bsyn with the rules for direct quotations.
</tableCaption>
<bodyText confidence="0.999950315789474">
predictions, as they do partially match a quote, so
there is a large difference in those scores. Note that
the reduced strict score does not occur for the token
method, which correctly identifies mixed quotations.
The other main issue is the presence of quotation
marks around items such as book titles and scare
quotes (i.e. text that is in quotation marks to distance
the author from a particular wording or claim). In
Section 4.5 we described the methods that we use to
avoid scare quotes and titles, which are rule-based
and imperfect. While these methods increase the
overall F-score of Brule, they do have a negative
impact on recall, which is why the recall is lower
than might be expected. These results demonstrate
that although direct quotations can be accurately ex-
tracted with rules, the accuracy will be lower than
might be anticipated and the returned spans will in-
clude a number of mixed quotations, which will be
missing some content.
</bodyText>
<subsectionHeader confidence="0.999078">
6.2 Indirect and Mixed Quotations
</subsectionHeader>
<bodyText confidence="0.975671205128205">
The token approach was also the most effective
method for extracting indirect and mixed quotations
as Tables 5 and 6 show. Indirect quotations were
extracted with strict F-scores of 59% and 60% and
partial F-scores of 76% and 74% in PARC and SMHC
respectively, while mixed quotes were found with
strict F-scores of 56% and 85% and partial F-scores
of 87% and 86%.
Although there is a strong interconnection be-
tween syntax and attribution, results for Bsyn show
that merely considering attribution as a syntactic re-
lation (Skadhauge and Hardt, 2005) has a large im-
pact on recall: only a subset of inter-sentential quo-
tations can be effectively matched by verb comple-
ment boundaries.
The constituent model yielded lower results than
the token one, and in particular it greatly lowered
the recall of mixed quotations in both corpora. Since
the model heavily relies on syntax, it is particularly
affected by errors made by the parser. The conjunc-
tion and in Example 3 is incorrectly attached by the
parser to the cue said, leading the classifier to iden-
tify two separate spans. In order to verify the impact
of incorrect parsing on the model, we ran the con-
stituent model using gold standard parses for PARC.
This resulted in an increase in strict P and increased
the F-score for mixed quotations to 57%, similarly
to the score achieved by the token model. However,
it surprisingly negatively affected R for indirect quo-
tations.
(3) Graeme Hugo, said strong links between Aus-
tralia’s 700,000 ethnic Chinese and China
could benefit both countries and were unlikely
to pose a threat.
The tables also report results for the extraction of
all quotations, irrespective of their type. For this
score, the baseline models for indirect and mixed
quotations are combined with Brule for direct quo-
tations.
</bodyText>
<subsectionHeader confidence="0.99961">
6.3 Model Comparison
</subsectionHeader>
<bodyText confidence="0.9999425">
We designed the features for the token and con-
stituent models to be largely similar. This al-
</bodyText>
<page confidence="0.99653">
996
</page>
<bodyText confidence="0.989802424242424">
lows us to conclude that the difference in perfor-
mance between the token and constituent models
is largely driven by the class labelling and learn-
ing method. Overall, the token-based approach out-
performed both the baselines and the constituent
method. Qualitatively we found that the token-based
approach was making reasonable predictions most
of the time, but would often fail when a quotation
was attributed to a speaker through a parenthetical
clause, as in Example 4.
(4) Finding lunar ice, said Tidbinbilla’s
spokesman, Glen Nagle, would give a
major boost to NASA’s hopes of returning
humans to the moon by 2020.
The token-based approach has a reasonable bal-
ance of the various label types, and benefits from a
decoding step that allows it to make trade-offs be-
tween good local decisions and a good overall so-
lution. By comparison, the constituent-based ap-
proach has a large class imbalance, as there are many
more negative (i.e. not quotation) parse nodes than
there are positive, which makes finding a good deci-
sion boundary difficult. We experimented with re-
ducing the number of negative nodes to consider,
but found that the overall F-score was equivalent or
worse, largely driven by a drop in recall. We also
found that in many cases the constituent-approach
predicted quotes that were too short, or that were
only the second half of a conjunction, without the
first half being labelled. We expect that these issues
would be corrected with the addition of a decoding
step, that forces the classifier to make a good global
decision.
</bodyText>
<sectionHeader confidence="0.990916" genericHeader="method">
7 Speaker Attribution
</sectionHeader>
<bodyText confidence="0.999984116666667">
While the focus of this paper is on extracting quota-
tions, we also present results on finding the speaker
of each quotation. As discussed in Section 2, quo-
tation attribution has been addressed in the litera-
ture before, including some work that includes large-
scale data (Elson and McKeown, 2010). However,
the large-scale evaluations that exist cover only di-
rect quotations, whereas we present results for di-
rect, indirect, and mixed quotations.
For this evaluation we use four of the methods that
were introduced in O’Keefe et al. (2012). The first
is a simple rule-based approach (Rule) that returns
the entity closest to the speech verb nearest the quo-
tation, or if there is no such speech verb then the
entity nearest the end of the quotation. The second
method uses a CRF which is able to choose between
up to 15 entities that are in the paragraph containing
the quotation or any preceding it. The third method
(No seq.) is a binary MaxEnt classifier that predicts
whether each entity is the speaker or not the speaker,
with the entity achieving the highest speaker proba-
bility predicted. In O’Keefe et al. (2012) this model
achieved the best results on the direct quotations in
SMHC, despite not using the sequence features or de-
coding methods that were available to other models.
The final method that we evaluate (Gold) is the ap-
proach that uses sequence features that use the gold-
standard labels from previous decisions. As noted
by O’Keefe et al., this method is not realisable in
practise, however we include these results so that
we can reassess the claims of O’Keefe et al. when
direct, indirect, and mixed quotations are included.
For our results to be comparable we use the list of
speech verbs that was presented in Elson and McK-
eown (2010) and used in O’Keefe et al. (2012).
Table 7 shows the accuracy of the two meth-
ods on both PARC and SMHC, broken down by the
type of the quotation. The first observation that
we make about these results in comparison to the
O’Keefe et al. results, is that the accuracy is gener-
ally lower, even for direct quotations. This discrep-
ancy is caused by differences in our data compared
to theirs, notably that the sequence of quotations is
altered in ours by the introduction of indirect quota-
tions, and that some of the direct quotations that they
evaluated would be considered mixed quotations in
our corpora. The rule based method performs par-
ticularly poorly on PARC, which is likely caused by
the relative scarcity of direct quotations and the fact
that it was designed for direct quotations only. Di-
rect quotations are much more frequent in SMHC, so
the rules that rely on the sequence of speakers would
likely perform relatively better than on PARC.
While the approach using gold-standard sequence
features unsurprisingly performed the best, the most
straightforward learned model (No seq.), trained
without any sequence information, equalled or out-
performed the two other non-gold approaches for all
quotation types on both corpora. This indicates that
the CRF model evaluated here was not able to effec-
</bodyText>
<page confidence="0.989329">
997
</page>
<table confidence="0.998711666666667">
Corpus Method Dir. Ind. Mix. All
PARC Rule 70 60 47 62
CRF 82 68 65 73
No seq. 85 74 65 77
Gold 88 79 74 82
SMHC Rule 89 76 78 84
CRF 83 72 71 78
No seq. 91 79 81 87
Gold 93 81 83 89
</table>
<tableCaption confidence="0.999079">
Table 7: Speaker attribution accuracy results for both cor-
pora over gold standard quotations.
</tableCaption>
<bodyText confidence="0.798977">
tively use the sequence information that is present.
</bodyText>
<sectionHeader confidence="0.990867" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999987043478261">
In this work we have presented the first large-scale
experiments on the entire quotation extraction and
attribution task: evaluating the extraction and at-
tribution of direct, indirect and mixed quotations
over two large news corpora. One of these corpora
(SMHC) is a novel contribution of this work, while
our results are the first presented for the other cor-
pus (PARC). This work has shown that while rule-
based approaches that return the object of a speech
verb are indeed effective, they are outperformed by
supervised systems that can take advantage of addi-
tional evidence. We also show that state-of-the-art
quotation attribution methods are less accurate on
indirect and mixed quotations than they are on di-
rect quotations.
Future work will include extending these methods
to extract all attributions, i.e. beliefs, eventualities,
and facts, as well as the source spans. We will also
evaluate the effect of adding a decoding step to the
constituent approach. This work provides an accu-
rate and complete quotation extraction and attribu-
tion system that can be used for a wide range of tasks
in information extraction and opinion mining.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999721222222222">
We would like to thank Bonnie Webber for her feed-
back and assistance. Pareti has been supported by a
Scottish Informatics &amp; Computer Science Alliance
(SICSA) studentship; O’Keefe has been supported
by a University of Sydney Merit scholarship and
a Capital Markets CRC top-up scholarship. This
work has been supported by ARC Discovery grant
DP1097291 and the Capital Markets CRC Com-
putable News project.
</bodyText>
<sectionHeader confidence="0.987048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994836048780488">
Sabine Bergler. 1992. Evidential analysis of re-
ported speech. Ph.D. thesis, Brandeis University.
Sabine Bergler, Monia Doandes, Christine Gerard,
and Ren´e Witte. 2004. Attributions. In Exploring
Attitude and Affect in Text: Theories andApplica-
tions, Technical Report SS-04-07, pages 16–19.
Papers from the 2004 AAAI Spring Symposium.
Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pas-
cal Denis, Gaelle Recource, and Victor Mignot.
2011. Extracting and visualizing quotations from
news wires. Human Language Technology. Chal-
lenges for Computer Science and Linguistics,
pages 522–532.
Cicero Nogueira dos Santos and Ruy Luiz Milidi´u.
2009. Entropy guided transformation learning. In
Foundations of Computational, Intelligence Vol-
ume 1, Studies in Computational Intelligence,
pages 159–184. Springer.
David K. Elson and Kathleen R. McKeown. 2010.
Automatic attribution of quoted speech in literary
narrative. In Proceedings of the Twenty-Fourth
Conference of the Association for the Advance-
ment of Artificial Intelligence, pages 1013–1019.
Christine Fellbaum. 1998. WordNet: An electronic
lexical database. MIT press Cambridge, MA.
William Paulo Ducca Fernandes, Eduardo Motta,
and Ruy Luiz Milidi´u. 2011. Quotation extraction
for portuguese. In Proceedings of the 8th Brazil-
ian Symposium in Information and Human Lan-
guage Technology (STIL 2011), pages 204–208.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification
in fiction books. In Proceedings of the 18th An-
nual Symposium of the Pattern Recognition Asso-
ciation of South Africa (PRASA07), pages 1–6.
Bill Hollingsworth and Simone Teufel. 2005. Hu-
man annotation of lexical chains: Coverage and
agreement measures. In ELECTRA Workshop on
Methodologies and Evaluation of Lexical Cohe-
sion Techniques in Real-world Applications (Be-
yond Bag of Words), page 26.
</reference>
<page confidence="0.981495">
998
</page>
<reference confidence="0.993421232876713">
Dan Klein and Christopher D Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in neural informa-
tion processing systems, pages 3–10.
Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008.
Minding the source: Automatic tagging of re-
ported speech in newspaper articles. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08).
Jisheng Liang, Navdeep Dhillon, and Krzysztof
Koperski. 2010. A large-scale system for an-
notating and querying quotations in news feeds.
In Proceedings of the 3rd International Semantic
Search Workshop, pages 1–5.
Nuno Mamede and Pedro Chaleira. 2004. Charac-
ter identification in children stories. Advances in
Natural Language Processing, pages 82–90.
Tim O’Keefe, Silvia Pareti, James R. Curran, Irena
Koprinska, and Matthew Honnibal. 2012. A se-
quence labelling approach to quote attribution. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 790–799.
Silvia Pareti. 2012. A database of attribution rela-
tions. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion, pages 3213–3217.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In Proceedings of Recent Advances
in Natural Language Processing, pages 487–492.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind
Joshi, and Bonnie Webber. 2006. Annotating at-
tribution in the Penn Discourse TreeBank. In Pro-
ceedings of the Workshop on Sentiment and Sub-
jectivity in Text, pages 31–38.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2008. The Penn Discourse Tree-
Bank 2.0 annotation manual. In Technical report,
University of Pennsylvania: Institute for Research
in Cognitive Science.
Luis Sarmento and Sergio Nunes. 2009. Automatic
extraction of quotes and topics from news feeds.
In 4th Doctoral Symposium on Informatics Engi-
neering.
Roser Sauriand James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. In Lan-
guage Resources and Evaluation, pages 227–268.
Nathan Schneider, Rebecca Hwa, Philip Gianfor-
toni, Dipanjan Das, Michael Heilman, Alan W.
Black, Frederik L. Crabbe, and Noah A. Smith.
2010. Visualizing topical quotations over time
to understand news discourse. Technical report,
Carnegie Mellon University.
Karin K. Schuler. 2005. Verbnet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D.
thesis, Faculties of Computer and Information
Science of the University of Pennsylvania.
Peter R. Skadhauge and Daniel Hardt. 2005. Syn-
tactic identification of attribution in the RST tree-
bank. In Proceedings of the Sixth International
Workshop on Linguistically Interpreted Corpora.
Janyce Wiebe and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. In Computational Linguistics
and Intelligent Text Processing, pages 486–497.
Springer.
Jason Zhang, Alan Black, and Richard Sproat.
2003. Identifying speakers in children’s stories
for speech synthesis. In Proceedings of EU-
ROSPEECH, pages 2041–2044.
</reference>
<page confidence="0.998736">
999
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925572">
<title confidence="0.999914">Automatically Detecting and Attributing Indirect Quotations</title>
<author confidence="0.999777">Tim Ioannis James R</author>
<affiliation confidence="0.999949">School of Informatics School of IT University of Edinburgh University of Sydney</affiliation>
<address confidence="0.968674">United Kingdom NSW 2006, Australia</address>
<abstract confidence="0.997358882352941">Direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy. However, simply focusing on direct quotations ignores around half of all reported speech, which is in the form of indirect or mixed speech. This work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution. We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Bergler</author>
</authors>
<title>Evidential analysis of reported speech.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<contexts>
<context position="1323" citStr="Bergler, 1992" startWordPosition="196" endWordPosition="197">ts in indirect and mixed quotation extraction and attribution. We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However, annotated resources suggest that direct quotations represent only a lim</context>
</contexts>
<marker>Bergler, 1992</marker>
<rawString>Sabine Bergler. 1992. Evidential analysis of reported speech. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Bergler</author>
<author>Monia Doandes</author>
<author>Christine Gerard</author>
<author>Ren´e Witte</author>
</authors>
<title>Attributions. In Exploring Attitude and Affect in Text: Theories andApplications,</title>
<date>2004</date>
<booktitle>Papers from the</booktitle>
<tech>Technical Report SS-04-07,</tech>
<pages>16--19</pages>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="1251" citStr="Bergler et al., 2004" startWordPosition="182" endWordPosition="185"> of indirect or mixed speech. This work presents the first large-scale experiments in indirect and mixed quotation extraction and attribution. We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However,</context>
</contexts>
<marker>Bergler, Doandes, Gerard, Witte, 2004</marker>
<rawString>Sabine Bergler, Monia Doandes, Christine Gerard, and Ren´e Witte. 2004. Attributions. In Exploring Attitude and Affect in Text: Theories andApplications, Technical Report SS-04-07, pages 16–19. Papers from the 2004 AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric de La Clergerie</author>
<author>Benoit Sagot</author>
<author>Rosa Stern</author>
<author>Pascal Denis</author>
<author>Gaelle Recource</author>
<author>Victor Mignot</author>
</authors>
<title>Extracting and visualizing quotations from news wires. Human Language Technology. Challenges for Computer Science and Linguistics,</title>
<date>2011</date>
<pages>522--532</pages>
<contexts>
<context position="3738" citStr="Clergerie et al. (2011)" startWordPosition="576" endWordPosition="579">e and do not compare multiple methods on the same data. In this work we address this lack of clear, comparable results by evaluating two baseline meth989 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989–999, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Method Language Test Size Results (quotations) P R Krestel et al. (2008) hand-built grammar English 133 74% 99% Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1 Fernandes et al. (2011) ML and regex Portuguese 205 64%2 67%2 de La Clergerie et al. (2011) patterns over parse French 40 87% 70% Schneider et al. (2010) hand-built grammar English N/D 56%2 52%2 Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly. ods against both a token-based approach that uses a Conditional Random Field (CRF) to predict IOB labels, and a maximum entropy classi</context>
<context position="8309" citStr="Clergerie et al., 2011" startWordPosition="1319" endWordPosition="1322">ar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detection. PICTOR (Schneider et al., 2010) relies instead on a context-free grammar for the extraction and attribution of quotations. PICTOR yielded 75% P and 86% R in terms of words correctly ascribed to a quotation or speaker, while it achieved 56% P and 52% R when measured in terms of completely correct quotation-speaker pairs. SAPIENS (de La Clergerie et al., 2011) extracts quotations from French news, by using a lexicon of reporting verbs and syntactic patterns to extract the complement of a reporting verb as the quotation span and its subject as the source. They evaluated 40 randomly sampled quotations and found that their system made 32 predictions and correctly identified the span in 28 of the 40 cases. Verbatim (Sarmento and Nunes, 2009) extracts quotations from Portuguese news feeds by first finding one of 35 speech verbs and then matching the sentence to one of 19 patterns. Their manual evaluation shows that 11.9% of the quotations Verbatim finds</context>
<context position="19651" citStr="Clergerie et al. (2011)" startWordPosition="3202" endWordPosition="3205"> predicted quotations to only those with the requisite type, while still considering the full set of gold quotations. Similarly, when calculating R we restrict the set of gold quotations to only those with the required type. 4.5 Baselines We have developed two baselines inspired by the current lexical/syntactic pattern-based approaches in the literature, which combine speech verbs and hand-crafted rules. 993 Ble� Lexical: cue verb + the longest of the spans before or after it until the sentence boundary. Bsy,,, Syntactic: cue verb + verb syntactic object. Bsy,,, is close to the model in de La Clergerie et al. (2011). Instead of relying on a lexicon of verbs, our baselines use those identified by the verb-cue classifier. As direct quotations are not always explicitly introduced by a cue-verb, we defined a separate baseline with a rule-based approach (Brule) that returns text between quotation marks that has at least 3 tokens, and where the non-stopword and non-proper noun tokens are not all title cased. In our full results we apply each method along with Brule and greedily take the longest predicted spans that do not overlap. 5 Supervised Approaches We present two supervised approaches to quotation extrac</context>
</contexts>
<marker>Clergerie, Sagot, Stern, Denis, Recource, Mignot, 2011</marker>
<rawString>Eric de La Clergerie, Benoit Sagot, Rosa Stern, Pascal Denis, Gaelle Recource, and Victor Mignot. 2011. Extracting and visualizing quotations from news wires. Human Language Technology. Challenges for Computer Science and Linguistics, pages 522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero Nogueira dos Santos</author>
<author>Ruy Luiz Milidi´u</author>
</authors>
<title>Entropy guided transformation learning.</title>
<date>2009</date>
<booktitle>In Foundations of Computational, Intelligence Volume 1, Studies in Computational Intelligence,</booktitle>
<pages>159--184</pages>
<publisher>Springer.</publisher>
<marker>Santos, Milidi´u, 2009</marker>
<rawString>Cicero Nogueira dos Santos and Ruy Luiz Milidi´u. 2009. Entropy guided transformation learning. In Foundations of Computational, Intelligence Volume 1, Studies in Computational Intelligence, pages 159–184. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Automatic attribution of quoted speech in literary narrative.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference of the Association for the Advancement of Artificial Intelligence,</booktitle>
<pages>1013--1019</pages>
<contexts>
<context position="1653" citStr="Elson and McKeown, 2010" startWordPosition="241" endWordPosition="244">indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However, annotated resources suggest that direct quotations represent only a limited portion of all quotations, i.e., around 30% in the Penn Attribution Relation Corpus (PARC), which covers Wall Street Journal articles, and 52% in the Sydney Morning Herald Corpus (SMHC), with the remainder being indirect (Ex.1c) or mixed (Ex.1b) quotations. Retrieving only direct quotations can miss key content that can cha</context>
<context position="7344" citStr="Elson and McKeown, 2010" startWordPosition="1157" endWordPosition="1160">im transcription of an utterance whereas indirect and the non-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a lexicon of 53 common reporting verbs and a hand-built grammar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detect</context>
<context position="31607" citStr="Elson and McKeown, 2010" startWordPosition="5309" endWordPosition="5312">any cases the constituent-approach predicted quotes that were too short, or that were only the second half of a conjunction, without the first half being labelled. We expect that these issues would be corrected with the addition of a decoding step, that forces the classifier to make a good global decision. 7 Speaker Attribution While the focus of this paper is on extracting quotations, we also present results on finding the speaker of each quotation. As discussed in Section 2, quotation attribution has been addressed in the literature before, including some work that includes largescale data (Elson and McKeown, 2010). However, the large-scale evaluations that exist cover only direct quotations, whereas we present results for direct, indirect, and mixed quotations. For this evaluation we use four of the methods that were introduced in O’Keefe et al. (2012). The first is a simple rule-based approach (Rule) that returns the entity closest to the speech verb nearest the quotation, or if there is no such speech verb then the entity nearest the end of the quotation. The second method uses a CRF which is able to choose between up to 15 entities that are in the paragraph containing the quotation or any preceding </context>
<context position="33068" citStr="Elson and McKeown (2010)" startWordPosition="5561" endWordPosition="5565">hieved the best results on the direct quotations in SMHC, despite not using the sequence features or decoding methods that were available to other models. The final method that we evaluate (Gold) is the approach that uses sequence features that use the goldstandard labels from previous decisions. As noted by O’Keefe et al., this method is not realisable in practise, however we include these results so that we can reassess the claims of O’Keefe et al. when direct, indirect, and mixed quotations are included. For our results to be comparable we use the list of speech verbs that was presented in Elson and McKeown (2010) and used in O’Keefe et al. (2012). Table 7 shows the accuracy of the two methods on both PARC and SMHC, broken down by the type of the quotation. The first observation that we make about these results in comparison to the O’Keefe et al. results, is that the accuracy is generally lower, even for direct quotations. This discrepancy is caused by differences in our data compared to theirs, notably that the sequence of quotations is altered in ours by the introduction of indirect quotations, and that some of the direct quotations that they evaluated would be considered mixed quotations in our corp</context>
</contexts>
<marker>Elson, McKeown, 2010</marker>
<rawString>David K. Elson and Kathleen R. McKeown. 2010. Automatic attribution of quoted speech in literary narrative. In Proceedings of the Twenty-Fourth Conference of the Association for the Advancement of Artificial Intelligence, pages 1013–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT press</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21185" citStr="Fellbaum, 1998" startWordPosition="3450" endWordPosition="3451">ed by position. Sentence: features indicating whether the sentence contains a quotation mark, a NE, a verb-cue, a pronoun, or any combination of these. There is also a sentence length feature. Dependency: relation with parent, relations with any dependants, as well as versions of these that include the head and dependent tokens. External knowledge: position-indexed features for whether any of the tokens in the sentence match a known role, organisation, or title. The titles come from a small hand-built list, while the role and organisation lists were built by recursively following the WordNet (Fellbaum, 1998) hyponyms of person and organization respectively. Other: features for whether the target is within quotation marks, and whether there is a verb-cue near the end of the sentence. Strict Partial P R F P R F PARC Brule 75 94 83 96 94 95 Token 97 91 94 98 97 97 SMHC Brule 87 93 90 98 94 96 Token 94 90 92 99 97 98 Table 4: PARC and SMHC results on direct quotations. The token based approach is trained and tested on all quotations. 5.1 Token-based Approach The token-based approach treats quotation extraction as analogous to NE tagging, where there are a sequence of tokens that need to be individual</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. WordNet: An electronic lexical database. MIT press Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Paulo Ducca Fernandes</author>
<author>Eduardo Motta</author>
<author>Ruy Luiz Milidi´u</author>
</authors>
<title>Quotation extraction for portuguese.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology (STIL</booktitle>
<pages>204--208</pages>
<marker>Fernandes, Motta, Milidi´u, 2011</marker>
<rawString>William Paulo Ducca Fernandes, Eduardo Motta, and Ruy Luiz Milidi´u. 2011. Quotation extraction for portuguese. In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology (STIL 2011), pages 204–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Glass</author>
<author>Shaun Bangay</author>
</authors>
<title>A naive salience-based method for speaker identification in fiction books.</title>
<date>2007</date>
<booktitle>In Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA07),</booktitle>
<pages>1--6</pages>
<contexts>
<context position="1627" citStr="Glass and Bangay, 2007" startWordPosition="237" endWordPosition="240">successfully applied to indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However, annotated resources suggest that direct quotations represent only a limited portion of all quotations, i.e., around 30% in the Penn Attribution Relation Corpus (PARC), which covers Wall Street Journal articles, and 52% in the Sydney Morning Herald Corpus (SMHC), with the remainder being indirect (Ex.1c) or mixed (Ex.1b) quotations. Retrieving only direct quotations can mis</context>
<context position="7027" citStr="Glass and Bangay, 2007" startWordPosition="1112" endWordPosition="1116">tion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mixed quotations differ in the degree of factuality they entail, since the former are by convention interpreted as a verbatim transcription of an utterance whereas indirect and the non-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a </context>
</contexts>
<marker>Glass, Bangay, 2007</marker>
<rawString>Kevin Glass and Shaun Bangay. 2007. A naive salience-based method for speaker identification in fiction books. In Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA07), pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Hollingsworth</author>
<author>Simone Teufel</author>
</authors>
<title>Human annotation of lexical chains: Coverage and agreement measures.</title>
<date>2005</date>
<booktitle>In ELECTRA Workshop on Methodologies and Evaluation of Lexical Cohesion Techniques in Real-world Applications (Beyond Bag of Words),</booktitle>
<pages>26</pages>
<contexts>
<context position="18509" citStr="Hollingsworth and Teufel, 2005" startWordPosition="3006" endWordPosition="3009"> extraction model. 4.4 Evaluation We use two metrics, listed below, for evaluating the quotation spans predicted by our model against the gold spans from the annotation. Strict The first is a strict metric where a predicted span is only considered to be correct if it exactly matches a span from the gold standard. The standard precision, recall, and F-score can be calculated using this definition of correctness. The drawback of this strict score is that if a prediction is incorrect by as little as one token it will be considered completely incorrect. Partial We also consider an overlap metric (Hollingsworth and Teufel, 2005), which allows partially correct predictions to be proportionally counted. Precision (P), recall (R), and F-score for this method are: P _ EgEgold EpEpred overlap(g, p) 1 |pred |( ) R _ EgEgold EpEpred overlap(p, g) 2 |gold |( ) F _ 2PR (3) (P + R) Where overlap(x, y) returns the proportion of tokens of y that are overlapped by x. For each of these metrics we report the micro-average, as the number of quotations in each document varies significantly. When reporting P for the typewise results we restrict the set of predicted quotations to only those with the requisite type, while still consider</context>
</contexts>
<marker>Hollingsworth, Teufel, 2005</marker>
<rawString>Bill Hollingsworth and Simone Teufel. 2005. Human annotation of lexical chains: Coverage and agreement measures. In ELECTRA Workshop on Methodologies and Evaluation of Lexical Cohesion Techniques in Real-world Applications (Beyond Bag of Words), page 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>3--10</pages>
<contexts>
<context position="15302" citStr="Klein and Manning, 2002" startWordPosition="2474" endWordPosition="2477">cument. More precisely, we consider quotations to be acts of communication, which correspond to assertions in Pareti (2012). Some quotations have content spans that are split into separate, non-adjacent spans, as in example (1a). Ideally the latter span should be marked as a continuation of a quotation, however we consider this to be out of scope for this work, so we treat each span as a separate quotation. 4.2 Preprocessing As a pre-processing step, both corpora were tokenised and POS tagged, and the potential speakers anonymised to prevent over-fitting. We used the Stanford factored parser (Klein and Manning, 2002) to retrieve both the Stanford dependencies and the phrase structure parse. Quotation marks were normalised to a single character, as the quotation direction is often incorrect for multi-paragraph quotations. 4.3 Verb-cue Classifier Verbs are by far the most common introducer of a quotation. In PARC verbs account for 96% of all 992 cues, the prepositional phrase according to for 3%, with the remaining 1% being nouns, adverbials and prepositional groups. Attributional verbs are not a closed set, they can vary across styles and genres, and their attributional use is highly dependent on the conte</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in neural information processing systems, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Krestel</author>
<author>Sabine Bergler</author>
<author>Ren´e Witte</author>
</authors>
<title>Minding the source: Automatic tagging of reported speech in newspaper articles.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="3539" citStr="Krestel et al. (2008)" startWordPosition="542" endWordPosition="545"> hand-crafted lexica of reporting verbs with rule-based approaches. The lack of data has also made comparing the relative merit of these approaches difficult, as existing evaluations are small-scale and do not compare multiple methods on the same data. In this work we address this lack of clear, comparable results by evaluating two baseline meth989 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989–999, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Method Language Test Size Results (quotations) P R Krestel et al. (2008) hand-built grammar English 133 74% 99% Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1 Fernandes et al. (2011) ML and regex Portuguese 205 64%2 67%2 de La Clergerie et al. (2011) patterns over parse French 40 87% 70% Schneider et al. (2010) hand-built grammar English N/D 56%2 52%2 Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the authors for extracting 570 quotations from 26k articles.</context>
<context position="7554" citStr="Krestel et al. (2008)" startWordPosition="1192" endWordPosition="1195">er attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a lexicon of 53 common reporting verbs and a hand-built grammar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detection. PICTOR (Schneider et al., 2010) relies instead on a context-free grammar for the extraction and attribution of quotations. PICTOR yielded 75% P and 86% R in terms of words correctly ascribed to a quotation</context>
<context position="14092" citStr="Krestel et al. (2008)" startWordPosition="2280" endWordPosition="2283">tations annotated within them. SMHC has a higher density of quotations per document, 8.3 vs. 4.6 in PARC, since articles are fully annotated and 1The agreement was calculated using the agr metric described in Wiebe and Riloff (2005) as the proportion of commonly annotated ARs with respect to the ARs identified overall by Annotator A and Annotator B respectively P R F Bsay 94.4 43.5 59.5 Blist 75.4 71.1 73.2 k-NN 88.9 72.6 79.9 Table 3: Results for the k-NN verb-cue classifier. Bsay classifies as verb-cue all instances of say while Blist marks as verb-cues all verbs from a pre-compiled list in Krestel et al. (2008). were selected to contain at least one quotation. PARC is instead only partially annotated and comprises articles with no quotations. Excluding null-quotation articles from PARC, the average incidence of annotated quotations per article raises to 7.1. The corpora also differ in quotation type distribution, with direct quotations being largely predominant in SMHC while indirect are more common in PARC. 4 Experimental Setup 4.1 Quotation Extraction Quotation extraction is the task of extracting the content span of all of the direct, indirect, and mixed quotations within a given document. More p</context>
<context position="17012" citStr="Krestel et al. (2008)" startWordPosition="2762" endWordPosition="2765">icts whether the head of each verb group is a verbcue using the k-nearest neighbour (k-NN) algorithm, with k equal to 3. The classifier uses 20 feature types, including: • Lexical (e.g. token, lemma, adjacent tokens) • VerbNet classes membership • Syntactic (e.g. node-depth in the sentence, parent and sibling nodes) • Sentence features (e.g. distance from sentence start/end, within quotation markers). We compared the system to one baseline, Bso,y, that marks every instance of say as a verb-cue, and another, Blit, that marks every instance of a verb that is on the list of 53 verbs presented in Krestel et al. (2008). We tested the system on the test set for PARC, which contains 1809 potential verb-cues, of which 354 are positive and 1455 are negative. The results in Table 3 show that the verb-cue classifier can outperform expert-derived knowledge. The classifier was able to identify verb-cues with P of 88.9% and R of 72.6%. While frequently occurring verbs are highly predictive, the inclusion of VerbNet classes (Schuler, 2005) and contextual features allows for a more accurate classification of polysemous and unseen verbs. Since PARC contains labelled and unlabelled attributions, which is detrimental for</context>
</contexts>
<marker>Krestel, Bergler, Witte, 2008</marker>
<rawString>Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008. Minding the source: Automatic tagging of reported speech in newspaper articles. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jisheng Liang</author>
<author>Navdeep Dhillon</author>
<author>Krzysztof Koperski</author>
</authors>
<title>A large-scale system for annotating and querying quotations in news feeds.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd International Semantic Search Workshop,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="7398" citStr="Liang et al., 2010" startWordPosition="1167" endWordPosition="1170">on-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a lexicon of 53 common reporting verbs and a hand-built grammar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detection. PICTOR (Schneider et al., 2010) relies instead on</context>
</contexts>
<marker>Liang, Dhillon, Koperski, 2010</marker>
<rawString>Jisheng Liang, Navdeep Dhillon, and Krzysztof Koperski. 2010. A large-scale system for annotating and querying quotations in news feeds. In Proceedings of the 3rd International Semantic Search Workshop, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Mamede</author>
<author>Pedro Chaleira</author>
</authors>
<title>Character identification in children stories.</title>
<date>2004</date>
<booktitle>Advances in Natural Language Processing,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="7002" citStr="Mamede and Chaleira, 2004" startWordPosition="1108" endWordPosition="1111">lic sector cuts. Only assertion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mixed quotations differ in the degree of factuality they entail, since the former are by convention interpreted as a verbatim transcription of an utterance whereas indirect and the non-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attributio</context>
</contexts>
<marker>Mamede, Chaleira, 2004</marker>
<rawString>Nuno Mamede and Pedro Chaleira. 2004. Character identification in children stories. Advances in Natural Language Processing, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Silvia Pareti</author>
<author>James R Curran</author>
<author>Irena Koprinska</author>
<author>Matthew Honnibal</author>
</authors>
<title>A sequence labelling approach to quote attribution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>790--799</pages>
<marker>O’Keefe, Pareti, Curran, Koprinska, Honnibal, 2012</marker>
<rawString>Tim O’Keefe, Silvia Pareti, James R. Curran, Irena Koprinska, and Matthew Honnibal. 2012. A sequence labelling approach to quote attribution. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 790–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Pareti</author>
</authors>
<title>A database of attribution relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation,</booktitle>
<pages>3213--3217</pages>
<contexts>
<context position="5502" citStr="Pareti (2012)" startWordPosition="870" endWordPosition="871">Finally, we use the direct quotation attribution methods described in O’Keefe et al. (2012) and show that they can be successfully applied to indirect and mixed quotations, albeit with lower accuracy. This leads us to conclude that attributing indirect and mixed quotations to speakers is harder than attributing direct quotations. With this work, we set a new state of the art in quotation extraction. We expect that the main contribution of this work will be that future methods can be evaluated in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (Ex.2c</context>
<context position="10676" citStr="Pareti, 2012" startWordPosition="1715" endWordPosition="1716">and token size and per-type occurrence of quotations overall and per document (average). which shows that the majority of evaluations thus far have been small-scale. Furthermore, the published results do not include any comparisons with previous work, which prevents a quantitative comparison of the approaches, and they do not include results broken down by whether the quotation is direct, indirect, or mixed. It is these issues that motivate our work. 3 Corpora We perform our experiments over two large corpora from the news domain. 3.1 Penn Attribution Relations Corpus (PARC) Our first corpus (Pareti, 2012), which we will refer to as PARC, is a semi-automatically built extension to the attribution annotations included in the PDTB (Prasad et al., 2008). The corpus covers 2,280 Wall Street Journal articles and contains annotations of assertions, beliefs, facts, and eventualities, which are altogether referred to as attribution relations (ARs). For this work we use only the assertions, as they correspond to quotations (direct, indirect and mixed). The drawback of this corpus is that it is not yet fully annotated, i.e., it comprises positive and unlabelled data. The corpus includes a test set of 14 </context>
<context position="12717" citStr="Pareti, 2012" startWordPosition="2049" endWordPosition="2050">ect quotations as any text between quotation marks, which included the directly-quoted portion of mixed quotations, as well as scare quotes. Under that definition direct quotations could be automatically extracted with very high accuracy, so annotations in that work were over the automatically extracted direct quotations. As part of this work one annotator removed scare quotes, updated mixed quotations to include both the directly and indirectly quoted portions, and added whole new indirect quotations. The annotation scheme was developed to be comparable to the scheme used in the PARC corpus (Pareti, 2012), although the SMHC corpus only includes assertions and does not annotate the lexical cue. The resulting corpus contains 7,991 quotations taken from 965 articles from the 2009 Sydney Morning Herald (we refer to this corpus as SMHC). The annotations in this corpus also include the speakers of the quotations, as well as gold standard Named Entities (NEs). We use 60% of this corpus as training data (4,872 quotations), 10% as development data (759 quotations), and 30% as test data (2,360 quotations). Early experiments were conducted over the development data, while the final results were trained o</context>
<context position="14801" citStr="Pareti (2012)" startWordPosition="2392" endWordPosition="2393">comprises articles with no quotations. Excluding null-quotation articles from PARC, the average incidence of annotated quotations per article raises to 7.1. The corpora also differ in quotation type distribution, with direct quotations being largely predominant in SMHC while indirect are more common in PARC. 4 Experimental Setup 4.1 Quotation Extraction Quotation extraction is the task of extracting the content span of all of the direct, indirect, and mixed quotations within a given document. More precisely, we consider quotations to be acts of communication, which correspond to assertions in Pareti (2012). Some quotations have content spans that are split into separate, non-adjacent spans, as in example (1a). Ideally the latter span should be marked as a continuation of a quotation, however we consider this to be out of scope for this work, so we treat each span as a separate quotation. 4.2 Preprocessing As a pre-processing step, both corpora were tokenised and POS tagged, and the potential speakers anonymised to prevent over-fitting. We used the Stanford factored parser (Klein and Manning, 2002) to retrieve both the Stanford dependencies and the phrase structure parse. Quotation marks were no</context>
</contexts>
<marker>Pareti, 2012</marker>
<rawString>Silvia Pareti. 2012. A database of attribution relations. In Proceedings of the Eight International Conference on Language Resources and Evaluation, pages 3213–3217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Clive Best</author>
</authors>
<title>Automatic detection of quotations in multilingual news.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>487--492</pages>
<contexts>
<context position="1603" citStr="Pouliquen et al., 2007" startWordPosition="233" endWordPosition="236">ribution methods can be successfully applied to indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However, annotated resources suggest that direct quotations represent only a limited portion of all quotations, i.e., around 30% in the Penn Attribution Relation Corpus (PARC), which covers Wall Street Journal articles, and 52% in the Sydney Morning Herald Corpus (SMHC), with the remainder being indirect (Ex.1c) or mixed (Ex.1b) quotations. Retrieving only d</context>
<context position="7377" citStr="Pouliquen et al., 2007" startWordPosition="1163" endWordPosition="1166">ereas indirect and the non-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a lexicon of 53 common reporting verbs and a hand-built grammar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detection. PICTOR (Schneider et al., 20</context>
</contexts>
<marker>Pouliquen, Steinberger, Best, 2007</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, and Clive Best. 2007. Automatic detection of quotations in multilingual news. In Proceedings of Recent Advances in Natural Language Processing, pages 487–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Annotating attribution in the Penn Discourse TreeBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="5999" citStr="Prasad et al., 2006" startWordPosition="949" endWordPosition="952">aluated in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (Ex.2c), expressing evaluation or knowledge, and eventualities (Ex.2d), expressing intention or attitude. (2) a. Mr Abbott said that he will win the election. b. Mr Abbott thinks he will win the election. c. Mr Abbott knew that Gillard was in Sydney. d. Mr Abbott agreed to the public sector cuts. Only assertion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mi</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Joshi, Webber, 2006</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi, and Bonnie Webber. 2006. Annotating attribution in the Penn Discourse TreeBank. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Eleni Miltsakaki</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
<author>Livio Robaldo</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0 annotation manual. In</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania: Institute for Research in Cognitive Science.</institution>
<contexts>
<context position="10823" citStr="Prasad et al., 2008" startWordPosition="1739" endWordPosition="1742">ar have been small-scale. Furthermore, the published results do not include any comparisons with previous work, which prevents a quantitative comparison of the approaches, and they do not include results broken down by whether the quotation is direct, indirect, or mixed. It is these issues that motivate our work. 3 Corpora We perform our experiments over two large corpora from the news domain. 3.1 Penn Attribution Relations Corpus (PARC) Our first corpus (Pareti, 2012), which we will refer to as PARC, is a semi-automatically built extension to the attribution annotations included in the PDTB (Prasad et al., 2008). The corpus covers 2,280 Wall Street Journal articles and contains annotations of assertions, beliefs, facts, and eventualities, which are altogether referred to as attribution relations (ARs). For this work we use only the assertions, as they correspond to quotations (direct, indirect and mixed). The drawback of this corpus is that it is not yet fully annotated, i.e., it comprises positive and unlabelled data. The corpus includes a test set of 14 articles that are fully annotated, which enables us to properly evaluate our work and estimate that a proportion of 30-50% of ARs are unlabelled in</context>
</contexts>
<marker>Prasad, Miltsakaki, Dinesh, Lee, Joshi, Robaldo, Webber, 2008</marker>
<rawString>Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan Lee, Aravind Joshi, Livio Robaldo, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0 annotation manual. In Technical report, University of Pennsylvania: Institute for Research in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Sergio Nunes</author>
</authors>
<title>Automatic extraction of quotes and topics from news feeds.</title>
<date>2009</date>
<booktitle>In 4th Doctoral Symposium on Informatics Engineering.</booktitle>
<contexts>
<context position="3604" citStr="Sarmento and Nunes (2009)" startWordPosition="552" endWordPosition="555">aches. The lack of data has also made comparing the relative merit of these approaches difficult, as existing evaluations are small-scale and do not compare multiple methods on the same data. In this work we address this lack of clear, comparable results by evaluating two baseline meth989 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989–999, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Method Language Test Size Results (quotations) P R Krestel et al. (2008) hand-built grammar English 133 74% 99% Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1 Fernandes et al. (2011) ML and regex Portuguese 205 64%2 67%2 de La Clergerie et al. (2011) patterns over parse French 40 87% 70% Schneider et al. (2010) hand-built grammar English N/D 56%2 52%2 Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly. </context>
<context position="8694" citStr="Sarmento and Nunes, 2009" startWordPosition="1385" endWordPosition="1388">s. PICTOR yielded 75% P and 86% R in terms of words correctly ascribed to a quotation or speaker, while it achieved 56% P and 52% R when measured in terms of completely correct quotation-speaker pairs. SAPIENS (de La Clergerie et al., 2011) extracts quotations from French news, by using a lexicon of reporting verbs and syntactic patterns to extract the complement of a reporting verb as the quotation span and its subject as the source. They evaluated 40 randomly sampled quotations and found that their system made 32 predictions and correctly identified the span in 28 of the 40 cases. Verbatim (Sarmento and Nunes, 2009) extracts quotations from Portuguese news feeds by first finding one of 35 speech verbs and then matching the sentence to one of 19 patterns. Their manual evaluation shows that 11.9% of the quotations Verbatim finds are errors and that the system identifies approximately one distinct quotation for every 46 news articles. The system presented by Fernandes et al. (2011) also works over Portuguese news. Their work is the closest to ours as they partially apply supervised machine learning to quotation extraction. Their work introduces GloboQuotes, a corpus of 685 news items containing 1,007 quotat</context>
</contexts>
<marker>Sarmento, Nunes, 2009</marker>
<rawString>Luis Sarmento and Sergio Nunes. 2009. Automatic extraction of quotes and topics from news feeds. In 4th Doctoral Symposium on Informatics Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauriand James Pustejovsky</author>
</authors>
<title>Factbank: A corpus annotated with event factuality.</title>
<date>2009</date>
<booktitle>In Language Resources and Evaluation,</booktitle>
<pages>227--268</pages>
<contexts>
<context position="1352" citStr="Pustejovsky, 2009" startWordPosition="199" endWordPosition="201"> quotation extraction and attribution. We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution. 1 Introduction Quotations are crucial carriers of information, particularly in news texts, with up to 90% of sentences in some articles being reported speech (Bergler et al., 2004). Reported speech is a carrier of evidence and factuality (Bergler, 1992; Sauriand Pustejovsky, 2009), and as such, text mining applications use quotations to summarise, organise and validate information. Extraction of quotations is also relevant to researchers interested in media monitoring. Most quotation attribution studies (Pouliquen et al., 2007; Glass and Bangay, 2007; Elson and McKeown, 2010) thus far have limited their scope to direct quotations (Ex.1a), as they are delimited *These authors contributed equally to this work. by quotation marks, which makes them easy to extract. However, annotated resources suggest that direct quotations represent only a limited portion of all quotation</context>
</contexts>
<marker>Pustejovsky, 2009</marker>
<rawString>Roser Sauriand James Pustejovsky. 2009. Factbank: A corpus annotated with event factuality. In Language Resources and Evaluation, pages 227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Rebecca Hwa</author>
<author>Philip Gianfortoni</author>
<author>Dipanjan Das</author>
<author>Michael Heilman</author>
<author>Alan W Black</author>
<author>Frederik L Crabbe</author>
<author>Noah A Smith</author>
</authors>
<title>Visualizing topical quotations over time to understand news discourse.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="3800" citStr="Schneider et al. (2010)" startWordPosition="587" endWordPosition="590">s work we address this lack of clear, comparable results by evaluating two baseline meth989 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 989–999, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Method Language Test Size Results (quotations) P R Krestel et al. (2008) hand-built grammar English 133 74% 99% Sarmento and Nunes (2009) patterns over text Portuguese 570 88% 5%1 Fernandes et al. (2011) ML and regex Portuguese 205 64%2 67%2 de La Clergerie et al. (2011) patterns over parse French 40 87% 70% Schneider et al. (2010) hand-built grammar English N/D 56%2 52%2 Table 1: Related work on direct, indirect and mixed quotation extraction. Note that they are not directly comparable as they apply to different languages and greatly differ in evaluation style and size of test set. 1 Figure estimated by the authors for extracting 570 quotations from 26k articles. 2 Results are for quotation extraction and attribution jointly. ods against both a token-based approach that uses a Conditional Random Field (CRF) to predict IOB labels, and a maximum entropy classifier that predicts whether parse nodes are quotations or not. </context>
<context position="7980" citStr="Schneider et al., 2010" startWordPosition="1260" endWordPosition="1263">uliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotation extraction and attribution system that combines a lexicon of 53 common reporting verbs and a hand-built grammar to detect constructions that match 6 general lexical patterns. They evaluate their work on 7 articles from the Wall Street Journal, which contain 133 quotations, achieving macro-averaged Precision (P) of 99% and Recall (R) of 74% for quotation span detection. PICTOR (Schneider et al., 2010) relies instead on a context-free grammar for the extraction and attribution of quotations. PICTOR yielded 75% P and 86% R in terms of words correctly ascribed to a quotation or speaker, while it achieved 56% P and 52% R when measured in terms of completely correct quotation-speaker pairs. SAPIENS (de La Clergerie et al., 2011) extracts quotations from French news, by using a lexicon of reporting verbs and syntactic patterns to extract the complement of a reporting verb as the quotation span and its subject as the source. They evaluated 40 randomly sampled quotations and found that their syste</context>
</contexts>
<marker>Schneider, Hwa, Gianfortoni, Das, Heilman, Black, Crabbe, Smith, 2010</marker>
<rawString>Nathan Schneider, Rebecca Hwa, Philip Gianfortoni, Dipanjan Das, Michael Heilman, Alan W. Black, Frederik L. Crabbe, and Noah A. Smith. 2010. Visualizing topical quotations over time to understand news discourse. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin K Schuler</author>
</authors>
<title>Verbnet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Faculties of Computer and Information Science of the University of Pennsylvania.</institution>
<contexts>
<context position="17431" citStr="Schuler, 2005" startWordPosition="2832" endWordPosition="2833">m to one baseline, Bso,y, that marks every instance of say as a verb-cue, and another, Blit, that marks every instance of a verb that is on the list of 53 verbs presented in Krestel et al. (2008). We tested the system on the test set for PARC, which contains 1809 potential verb-cues, of which 354 are positive and 1455 are negative. The results in Table 3 show that the verb-cue classifier can outperform expert-derived knowledge. The classifier was able to identify verb-cues with P of 88.9% and R of 72.6%. While frequently occurring verbs are highly predictive, the inclusion of VerbNet classes (Schuler, 2005) and contextual features allows for a more accurate classification of polysemous and unseen verbs. Since PARC contains labelled and unlabelled attributions, which is detrimental for training, we used the verb-cue classifier to identify in the corpus sentences that we suspected contained an unlabelled attribution. Sentences containing a verb classified as a cue that do not contain a quotation were removed from the training set for the quotation extraction model. 4.4 Evaluation We use two metrics, listed below, for evaluating the quotation spans predicted by our model against the gold spans from</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin K. Schuler. 2005. Verbnet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, Faculties of Computer and Information Science of the University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter R Skadhauge</author>
<author>Daniel Hardt</author>
</authors>
<title>Syntactic identification of attribution in the RST treebank.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora.</booktitle>
<contexts>
<context position="28387" citStr="Skadhauge and Hardt, 2005" startWordPosition="4770" endWordPosition="4773">ations, which will be missing some content. 6.2 Indirect and Mixed Quotations The token approach was also the most effective method for extracting indirect and mixed quotations as Tables 5 and 6 show. Indirect quotations were extracted with strict F-scores of 59% and 60% and partial F-scores of 76% and 74% in PARC and SMHC respectively, while mixed quotes were found with strict F-scores of 56% and 85% and partial F-scores of 87% and 86%. Although there is a strong interconnection between syntax and attribution, results for Bsyn show that merely considering attribution as a syntactic relation (Skadhauge and Hardt, 2005) has a large impact on recall: only a subset of inter-sentential quotations can be effectively matched by verb complement boundaries. The constituent model yielded lower results than the token one, and in particular it greatly lowered the recall of mixed quotations in both corpora. Since the model heavily relies on syntax, it is particularly affected by errors made by the parser. The conjunction and in Example 3 is incorrectly attached by the parser to the cue said, leading the classifier to identify two separate spans. In order to verify the impact of incorrect parsing on the model, we ran th</context>
</contexts>
<marker>Skadhauge, Hardt, 2005</marker>
<rawString>Peter R. Skadhauge and Daniel Hardt. 2005. Syntactic identification of attribution in the RST treebank. In Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>486--497</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13703" citStr="Wiebe and Riloff (2005)" startWordPosition="2211" endWordPosition="2214">60% of this corpus as training data (4,872 quotations), 10% as development data (759 quotations), and 30% as test data (2,360 quotations). Early experiments were conducted over the development data, while the final results were trained on both the training and development sets and were tested on the unseen test data. 3.3 Comparison Table 2 shows a comparison of the two corpora and the quotations annotated within them. SMHC has a higher density of quotations per document, 8.3 vs. 4.6 in PARC, since articles are fully annotated and 1The agreement was calculated using the agr metric described in Wiebe and Riloff (2005) as the proportion of commonly annotated ARs with respect to the ARs identified overall by Annotator A and Annotator B respectively P R F Bsay 94.4 43.5 59.5 Blist 75.4 71.1 73.2 k-NN 88.9 72.6 79.9 Table 3: Results for the k-NN verb-cue classifier. Bsay classifies as verb-cue all instances of say while Blist marks as verb-cues all verbs from a pre-compiled list in Krestel et al. (2008). were selected to contain at least one quotation. PARC is instead only partially annotated and comprises articles with no quotations. Excluding null-quotation articles from PARC, the average incidence of annota</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Computational Linguistics and Intelligent Text Processing, pages 486–497. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Zhang</author>
<author>Alan Black</author>
<author>Richard Sproat</author>
</authors>
<title>Identifying speakers in children’s stories for speech synthesis.</title>
<date>2003</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>2041--2044</pages>
<contexts>
<context position="6975" citStr="Zhang et al., 2003" startWordPosition="1104" endWordPosition="1107">tt agreed to the public sector cuts. Only assertion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mixed quotations differ in the degree of factuality they entail, since the former are by convention interpreted as a verbatim transcription of an utterance whereas indirect and the non-quoted portion of mixed quotations can be paraphrased forms of the original wording, and are thus filtered by the writer’s perspective. The first speaker attribution systems (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) originate from the narrative domain and were concerned with the identification of different characters for speech synthesis applications. Direct quotation attribution, with direct quotations being given or extracted heuristically, has been the focus of further studies in both the narrative (Elson and McKeown, 2010) and news (Pouliquen et al., 2007; Liang et al., 2010) domains. The few studies that 990 have addressed the extraction and attribution of indirect and mixed quotations are discussed below. Krestel et al. (2008) developed a quotatio</context>
</contexts>
<marker>Zhang, Black, Sproat, 2003</marker>
<rawString>Jason Zhang, Alan Black, and Richard Sproat. 2003. Identifying speakers in children’s stories for speech synthesis. In Proceedings of EUROSPEECH, pages 2041–2044.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
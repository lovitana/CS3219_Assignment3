<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.953439">
Fast Joint Compression and Summarization via Graph Cuts
</title>
<author confidence="0.974312">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.969224">
The University of Texas at Dallas
</affiliation>
<address confidence="0.848793">
800 W. Campbell Rd., Richardson, TX, USA
</address>
<email confidence="0.999314">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99949416">
Extractive summarization typically uses sen-
tences as summarization units. In contrast,
joint compression and summarization can use
smaller units such as words and phrases, re-
sulting in summaries containing more infor-
mation. The goal of compressive summariza-
tion is to find a subset of words that max-
imize the total score of concepts and cut-
ting dependency arcs under the grammar con-
straints and summary length constraint. We
propose an efficient decoding algorithm for
fast compressive summarization using graph
cuts. Our approach first relaxes the length con-
straint using Lagrangian relaxation. Then we
propose to bound the relaxed objective func-
tion by the supermodular binary quadratic pro-
gramming problem, which can be solved ef-
ficiently using graph max-flow/min-cut. S-
ince finding the tightest lower bound suffers
from local optimality, we use convex relax-
ation for initialization. Experimental results
on TAC2008 dataset demonstrate our method
achieves competitive ROUGE score and has
good readability, while is much faster than the
integer linear programming (ILP) method.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833390243902">
Automatic multi-document summarization helps
readers get the most important information from
large amounts of texts. Summarization techniques
can be roughly divided into two categories: extrac-
tive and abstractive. Extractive summarization casts
the summarization task as a sentence selection prob-
lem: identifying important summary sentences from
one or multiple documents. Many methods have
been developed in the past decades, including super-
vised approaches that use classifiers to predict sum-
mary sentences, graph based approaches to rank the
sentences, and recent global optimization methods
such as integer linear programming (Gillick et al.,
2008) (ILP) and submodular maximization methods
(Lin and Bilmes, 2011). Though extractive summa-
rization is popular because of its simplicity and high
readability, it has limitations in that it selects each
sentence as a whole, and thus may miss informative
partial sentences.
To improve the informativeness, joint com-
pression and summarization was proposed (Berg-
Kirkpatrick et al., 2011), which uses words as sum-
marization units, unlike extractive summarization
where each sentence is a basic undecomposable u-
nit. To achieve better readability, manually defined
grammar constraints or automatically learned mod-
els based on syntax trees are added during the sum-
marization process. Up to now, the state of the art
compressive systems are based on integer linear pro-
gramming (ILP). Because ILP suffers from expo-
nential complexity, word-based compression sum-
marization is an order of magnitude slower than
sentence-based extraction.
One common way to solve an ILP problem is to
use its LP relaxation and round the results. How-
ever Berg-Kirkpatrick et al. (2011) found that LP
relaxation gave poor results, finding unacceptably
suboptimal solutions. For speedup, they proposed a
two stage method where they performed some sen-
tence selection in the first step to reduce the number
of candidates. Despite their empirical success, such
</bodyText>
<page confidence="0.949801">
1492
</page>
<note confidence="0.7320195">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921372093023">
a pruning approach has its inherent problem in that
it may eliminate correct sentences in the first step.
Recently, Almeida and Martins (2013) proposed a
fast joint decoding algorithm based on dual decom-
position. For fast convergence, they added quadratic
penalty terms to alleviate the learning rate problem.
In this paper, we propose an efficient decoding al-
gorithm for fast ILP based compressive summariza-
tion using graph cuts. Our assumption is that all con-
cepts are word n-grams and non-negatively scored.
The rationale for the non-negativity assumption is s-
traightforward: the score of a concept reflects its in-
formativeness, hence should be non-negative. Given
a set of documents, each word is associated with a
binary variable, indicating whether the word is se-
lected in the summary. Our idea is to approximate
the ILP as a binary quadratic programming problem
where coefficients of all quadratic terms are non-
negative. It is well known that such binary quadrat-
ic function is supermodular, and its maximum can
be solved efficiently using graph max-flow/min-cut.
Hence the key is to find the coefficients of the super-
modular binary quadratic function (SBQF) so that
its maximum is close to the optimal ILP objective
function. Our solution consists of 3 steps. First,
we show that the subtree deletion model and gram-
mar constraints can be eliminated by adding SBQF-
s to the objective function. Second, we relax the
summary length constraint using Lagrangian relax-
ation. Third, we propose a family of SBQFs that
are lower bounds of the ILP objective function. S-
ince finding the tightest lower bound suffers from
local optimality, we choose to use convex relaxation
for initialization. To demonstrate our technique, we
conduct experiments on Text Analysis Conference
(TAC) datasets using the same train/test splits as pre-
vious work (Berg-Kirkpatrick et al., 2011). We com-
pare our approach with the state-of-the-art ILP based
approach in terms of summary quality (ROUGE s-
cores and sentence quality) and speed. Experimen-
tal results show that our proposed method achieves
competitive performance with ILP, while about 100
times faster.
</bodyText>
<sectionHeader confidence="0.971858" genericHeader="method">
2 Compressive Summarization
</sectionHeader>
<subsectionHeader confidence="0.903082">
2.1 Extractive Summarization
</subsectionHeader>
<bodyText confidence="0.999965228571429">
As our method is an approximation of ILP based
method, we first briefly review the ILP based extrac-
tive summarization and compressive summarization.
Gillick and Favre (2009) introduced the concept-
based ILP for summarization. A concept is a basic
semantic unit. They used word bigrams as such lan-
guage concepts. Their system achieved the highest
ROUGE score on the TAC 2009 evaluation. This
approach selects sentences so that the total score
of language concepts appearing in the summary is
maximized. The association between the language
concepts and sentences serves as the constraints, in
addition to the summary length constraint.
Formally, given a set of sentences S = {sn}Nn=1,
extractive summarization can be represented by a bi-
nary vector y, where yn indicates whether sentence
sn is selected. Let C = {c1, ... cJ} denote the set
of concepts in S, e.g., word bigrams (Gillick and
Favre, 2009). Each concept cj is associated with a
given score wj and a binary variable vj indicating
if cj is selected in the summary. Let njk denote the
index of the sentence containing the kth occurrence
of concept cj, and ln denote the length of sentence
sn. The ILP based extractive summarization system
can be formulated as below:
The first constraint is imposed by the relation be-
tween concept selection and sentence selection: s-
electing a sentence leads to the selection of all the
concepts it contains, and selecting a concept only
happens when it is present in at least one of the se-
lected sentences. The second constraint is the sum-
mary length constraint.
As solving an ILP problem is generally NP-hard,
pre-pruning of candidate concepts and sentences is
necessary for efficient summarization. For exam-
</bodyText>
<equation confidence="0.977028714285714">
max J wjvj
y,v j=1
�s.t. vj = ynik 1 &lt; j &lt; J (1)
k
N ynln &lt; L
Z=1
v, y are binary
</equation>
<page confidence="0.895126">
1493
</page>
<bodyText confidence="0.9998002">
ple, the ICSI system (Gillick et al., 2008) removed
the sentences that are too short or have non-overlap
with the queries, and concepts with document fre-
quency less than 3, resulting in 95.8 sentences and
about 80 concepts per topic on the TAC2009 dataset.
Therefore the actual scale of ILP is rather small after
pruning (e.g., 176 variables and 372 constraints per
topic). Empirical studies showed that such small s-
cale ILP can be solved within a few seconds (Gillick
and Favre, 2009).
</bodyText>
<subsectionHeader confidence="0.998572">
2.2 Compressive Summarization
</subsectionHeader>
<bodyText confidence="0.995888918367347">
The quality of sentence-based extractive summariza-
tion is limited by the informativeness of the orig-
inal sentences and the summary length constraint.
To remove the unimportant part from a long sen-
tence, sentence compression is proposed to generate
more informative summaries (Liu and Liu, 2009; Li
et al., 2013a). Recent studies show that joint sen-
tence compression and extraction, namely compres-
sive summarization, outperforms pipeline systems
that run extractive summarization on the compressed
sentences or compress selected summary sentences
(Martins and Smith, 2009; Berg-Kirkpatrick et al.,
2011; Chali and Hasan, 2012). In Berg-Kirkpatrick
et al. (2011), compressive summarization inte-
grates the concept model for extractive summariza-
tion (Gillick and Favre, 2009) and subtree deletion
model for sentence compression. The score of a
compressive summary consists of two parts, scores
of selected concepts, and scores of the broken arcs
in the dependency parse trees. The selected word-
s must satisfy the length constraint and grammar
constraints that include subtree constraint and some
manually defined hard constraints.
Formally, let x = x1 ... xI denote the word se-
quence of documents, where s1 = x1, ... xl1 corre-
sponds to the first sentence, s2 = xl1+1, . . . , xl1+l2
corresponds to the second sentence, and so on. A
compressive summary can be represented by a bi-
nary vector z, where zi indicates whether word xi
is selected in the summary. Let ahm denote the arc
xh → xm in the dependency parse tree of the cor-
responding sentence containing words xh and xm,
and A = {ahm} denote the set of dependency arcs.
The subtree constraint ensures that word xm is se-
lected only if its head xh is selected. In order to
guarantee the readability, grammar constraints are
added to prohibit the breaks of some specific arc-
s. For example, Clarke and Lapata (2008) nev-
er deleted an arc whose dependency label is SUB,
OBJ, PMOD, SBAR or VC. In this paper, we use
B ⊆ A to denote the set of these arcs that must
not be broken in summarization. We use ojk to de-
note the indices of words corresponding to the kth
occurrence of cj. For example, suppose the jth
concept European Union appears twice in the doc-
ument: x22x23 = x50x51 =European Union, then
oj1 = {22, 23}, oj2 = {50, 51}.
The compressive summarization model can be
formulated as an integer programming problem
</bodyText>
<equation confidence="0.990391">
�wj · vj + wahmzh(1 − zm)
ahmEA
Us.t. vj = ri zi bj
k iEojk
�
i
zh ≥ zm bahm E A (2)
zh = zm bahm E B
z, v are binary
</equation>
<bodyText confidence="0.99991995">
According to the subtree deletion model, the score
of arc ahm is included if zh = 1 and zm = 0, which
can be formulated as wahm · zh(1 − zm). The first
constraint is similar to that in extractive summariza-
tion, that is, a concept is selected if and only if any
of its occurrence is selected. The third and fourth
constraints are the subtree constraints and manual-
ly defined grammar constraints respectively. In the
rest of the paper, without loss of generality, we re-
move the fourth constraint by directly substituting
one variable for the other.
Finding the optimal summary is generally NP-
hard. Unlike extractive summarization where the s-
cale of the problem (the number of sentences and
concepts) is small, the number of variables in com-
pressive summarization is linear in the number of
words, which is usually thousands on the TAC
datasets. Hence solving such a problem using ILP
based decoding algorithms is not efficient especially
when the document set is large.
</bodyText>
<figure confidence="0.6845345">
J
max
z,v j=1
ziGL
</figure>
<page confidence="0.967409">
1494
</page>
<sectionHeader confidence="0.857697" genericHeader="method">
3 Fast Decoding via Graph Cuts
</sectionHeader>
<bodyText confidence="0.999700692307692">
In this section, we introduce our fast decoding al-
gorithm. We assume that all the concepts are word
n-grams, and their scores are non-negative. The non-
negativity assumption can reduce the computational
complexity, but is also reasonable: the score of a
concept denotes its informativeness, hence should
be non-negative. For example, Li et al. (2013b)
proposed to use the estimated normalized frequen-
cies of concepts as scores, which are essentially non-
negative. The basic idea of our method is to approx-
imate the above optimization problem (2) by the su-
permodular binary quadratic programming (SBQP)
problem:
</bodyText>
<equation confidence="0.9805558">
max
z � �βizi + αijzizj
ij
i
s.t. z is binary (3)
</equation>
<bodyText confidence="0.999979166666667">
where αij &gt; 0. It is known that such a binary
quadratic function is supermodular, and its maxi-
mum can be solved efficiently using graph max-
flow/min-cut (Billionnet and Minoux, 1985; Kol-
mogorov and Zabih, 2004). Now the problem is to
find the optimal α, β for a good approximation.
</bodyText>
<subsectionHeader confidence="0.972536">
3.1 Formulate Grammar Constraints and
Subtree Deletion Model by SBQF
</subsectionHeader>
<bodyText confidence="0.9999965">
We show that the subtree deletion model can be for-
mulated equivalently using SBQF. First, we can e-
liminate the constraint zh &gt; zm by adding a penalty
term to the objective function. That is,
</bodyText>
<equation confidence="0.984213">
max f(z)
s.t. zh &gt; zm
z is binary
is equivalent to
max f(z) − 00(1 − zh)zm
s.t. z is binary
</equation>
<bodyText confidence="0.997372">
We can see that the penalty term −00(1− zh)zm ex-
cludes zh = 0, zm = 1 from the feasible set, and
for zh &gt; zm, both problems have the same objective
function value. Hence the two problems are equiva-
lent. Notice that the coefficient of quadratic term in
−00(1 − zh)zm is positive, hence the penalty term
is supermodular.
Now we eliminate the third constraint in problem
(2) using the penalized objective function described
above. Note that the fourth constraint has been elim-
inated by variable substitution, we have
</bodyText>
<equation confidence="0.942343666666667">
�wj - vj + wahmzh(1 − zm)
ahmEA
�−00 (1 − zh)zm
ahmEA
zi bj (4)
z, v are binary
</equation>
<bodyText confidence="0.9995305">
We can see that for each arc ahm, there must be a
positive quadratic term +00zhzm in the objective
function, which guarantees the supermodularity of
the objective function, no matter what wahm is.
</bodyText>
<subsectionHeader confidence="0.9979575">
3.2 Eliminate Length Constraint Using
Lagrangian Relaxation
</subsectionHeader>
<bodyText confidence="0.999760888888889">
Problem (4) is NP-hard, because for any feasible v,
it is a SBQP with a length constraint. Since size con-
strained minimum cut problem is generally NP-hard
(Nagano et al., 2011), Problem (4) can not be cast
as a SBQP as long as P =� NP. One popular way to
deal with the size constrained optimization problem
is Lagrangian relaxation. We introduce Lagrangian
multiplier λ to the length constraint in Problem (4),
and get
</bodyText>
<equation confidence="0.951018">
�wj - vj + wahmzh(1 − zm)
ahmEA
�−00 (1 − zh)zm
ahmEA
+λ(L − � zi)
i
Us.t. vj = ri zi bj (5)
k iEojk
λ &gt; 0
z, v are binary
</equation>
<bodyText confidence="0.999784666666667">
We solve the relaxed problem iteratively. In each
iteration, we fix λ and solve the inner maximiza-
tion problem (details described below). The score of
</bodyText>
<figure confidence="0.997614307692308">
J
max
z,v j=1
Us.t. vj = ri
k iEojk
� ziGL
i
J
j=1
min
A
max
z,v
</figure>
<page confidence="0.972219">
1495
</page>
<bodyText confidence="0.999925846153846">
each word is penalized by λ — with larger λ, few-
er words are selected. Hence the summary length
can be adjusted by λ. The optimal λ can be found
using binary search. We maintain an upper bound
λmax , and a lower bound λmin, which is initially 0.
In each iteration, we choose λ = 12(λmax + λmin)
and search the optimal z. If the duality gap vanish-
es, i.e., λ(L − ∑i zi) = 0 and ∑i zi &lt; L, then we
get the global solution of Problem (4). Otherwise,
if ∑i zi &gt; L, then the current λ is too small, so we
set λmin = λ; otherwise, λ &gt; 0 and ∑i zi &lt; L,
we set λmax = λ. The search process terminates if
λmax − λmin is less than a predefined threshold.
</bodyText>
<subsectionHeader confidence="0.8605405">
3.3 Eliminate v Using Supermodular
Relaxation
</subsectionHeader>
<bodyText confidence="0.9999215">
Now we consider the inner maximization Problem
(5). It is still not a SBQP, since the objective func-
tion is not a linear function of zizj. We propose to
approximate the objective function using SBQP. Our
solution consists of two steps. First we relax the first
constraint of Problem (5) by bounding the objec-
tive function with a family of supermodular pseudo
boolean functions (Boros and Hammer, 2002). Sec-
ond we reformulate these pseudo boolean functions
equivalently as quadratic functions.
Similar to the bounding strategy in (Qian and Liu,
2013), we relax the logical disjunction by lineariza-
tion. Using the fact that for any binary vector a, we
have
</bodyText>
<equation confidence="0.99455325">
∪ ∑
ai = max
pE∆
i
</equation>
<bodyText confidence="0.945282">
where ∆ denotes the probability simplex
</bodyText>
<equation confidence="0.944963125">
∆ = tp |∑ pk = 1,pk &gt;&gt;- 0}
k
We have
∪vj = ∏ zi
k iEojk
∑ ∏
= max pjk zi
pjE∆ k iEojk
</equation>
<bodyText confidence="0.966703">
Plug the equation above into the objective function
of Problem (5), we get the following optimization
</bodyText>
<equation confidence="0.953393">
problem
 
∑ ∏
pjkwj zi 
k iEojk
+ ∑ wahmzh(1 − zm)
ahmEA
∑−00 (1 − zh)zm
ahmEA
+λ(L − ∑ zi)
i
s.t. z is binary (6)
pj E ∆ bj
Let Q(p, z) denote the objective function of Prob-
</equation>
<bodyText confidence="0.989836">
lem (6). Given p, we can see that Q is a supermod-
ular pseudo boolean function because coefficients
of all non-linear terms are non-negative. Using the
fact that for any binary vector a = [a1,... ar]&apos;,
</bodyText>
<equation confidence="0.993593117647059">
ai E t0, 1},1 &lt; i &lt; r,
i=1
∏r ai = max ∑ ai − r + 1)b
bEt0,1} ( r
i=1
(Freedman and Drineas, 2005), we get the following
equivalent optimization problem of Problem (6)
pjkwjqjk ∑ zi − |ojk |+ 1

iEojk
wahmzh(1 − zm)
∑−00 (1 − zh)zm
ahmEA
+λ(L − ∑ zi)
i
s.t. z, q are binary (7)
pj E ∆ bj
</equation>
<bodyText confidence="0.999120875">
where |ojk |is the size of ojk.
Let R(z, p, q) denote the objective function of
Problem (7), to search the optimal point, we al-
ternatively update p and z, q. First we initialize
p = p(0). In each iteration, we first fix p. It is ob-
vious that Problem (7) is a SBQP, hence the optimal
z, q can be solved efficiently using max-flow/min-
cut. Then we fix z, q, and update p using projected
</bodyText>
<figure confidence="0.942373416666667">
∑J
j=1
max
z,p
∑
k
max
z,p,q
∑J
j=1
+ ∑
piai ahmEA
</figure>
<page confidence="0.735185">
1496
</page>
<equation confidence="0.95338725">
subgradient. That is
(pj ∂R
pj ew = Po + α (8)
∂pj
</equation>
<bodyText confidence="0.998838333333333">
where α &gt; 0 is the step size in line search, and func-
tion P∆(q) denotes the projection of q onto the fea-
sible set ∆
</bodyText>
<equation confidence="0.76926075">
P∆(q) = min IIp − qII2
PE∆
which can be solved efficiently by sorting (Duchi et
al., 2008).
</equation>
<subsectionHeader confidence="0.930472">
3.4 Initialize p Using Convex Relaxation
</subsectionHeader>
<bodyText confidence="0.999971714285714">
Since R is non-concave, searching its maximum us-
ing subgradient method suffers from local optimali-
ty. Though one can use techniques such as branch-
and-bound for exact inference (Qian and Liu, 2013;
Gormley and Eisner, 2013), here for fast decoding,
we use convex relaxation to choose a good seed
p(0). Recall that pjk denotes the percentage of the
kth occurrence contributing to cj. The larger pjk is,
the more likely the kth occurrence is selected. To
estimate such likelihood, we replace the binary con-
straint in extractive summarization (Problem (1)) by
0 &lt; y, v &lt; 1, since solving a relaxed LP is much
faster than ILP. Suppose y* is the optimal solution
for such a relaxed LP problem, we initialize p by
</bodyText>
<equation confidence="0.963298">
Ek y*nik
If for all k, y* = 0, then we initialize pjk using
nik
uniform distribution
1
pjk = |oj|
</equation>
<bodyText confidence="0.956043">
where |oj |is the frequency of cj.
</bodyText>
<sectionHeader confidence="0.509631" genericHeader="method">
3.5 Summary
</sectionHeader>
<bodyText confidence="0.980299142857143">
For clarity, we summarize our decoding algorithm in
Algorithm 1. Initial Amax can be arbitrarily large. In
our experiments, we set Amax = Ej wj, which em-
pirically guarantees the summary length Ei zi &lt; L
when A = Amax. The choice of the step size for
updating p is similar to the projected subgradien-
t method in dual decomposition (Koo et al., 2010).
</bodyText>
<figure confidence="0.990373423076923">
Algorithm 1 Compressive Summarization via
Graph Cuts
Require: Scores of concepts {w�} and arcs {wa,—},
max summary length L.
Ensure: Compressive summarization z*, where zi indi-
cates whether the ith word is selected.
Solve the relaxed LP of Problem (1) (replace the binary
constraint by 0 &lt; y, v &lt; 1) to get y.
Initialize p(0) using Eq (9).
Initialize sufficient large Amax, and Amin = 0
while Amax − Amin &gt; e do
Set A = 2(Amin + Amax)
Set p = p(0).
repeat
Fix p, solve Problem (7) to get z using max-
flow/min-cut.
Update p using Eq (8).
until convergence
if Ei zi &gt; L then
Amin = A
else if Ei zi &lt; L then
Amax = A
else
break
end if
end while
</figure>
<sectionHeader confidence="0.909104" genericHeader="method">
4 Features and Hard Constraints
</sectionHeader>
<bodyText confidence="0.999905">
We choose discriminative models to learn the scores
of concepts and arcs. For concept cj, its score is
</bodyText>
<equation confidence="0.9932185">
wj = BT
conceptfconcept(cj)
</equation>
<bodyText confidence="0.999893">
where fconcept(cj) is the feature vector of cj, and
Bconcept is the corresponding weight vector of fea-
ture fconcept(cj). Similarly, score wahm is defined
as
</bodyText>
<equation confidence="0.997845">
wahm = Bar
T cfarc(ahm)
</equation>
<bodyText confidence="0.989740333333333">
Though our algorithm can handle general word n-
gram concepts, we restrict the concepts to word bi-
grams, which have been widely used recently in the
sentence-based ILP extractive summarization sys-
tems. For a concept cj, we define the following
features, some of which have been used in previous
work (Brandow et al., 1995; Aker and Gaizauskas,
2009; Edmundson, 1969; Radev, 2001; Li et al.,
2013b). All of these features are non-negative.
</bodyText>
<listItem confidence="0.9970935">
• Term frequency: the frequency of cj in the giv-
en topic.
</listItem>
<equation confidence="0.929075">
pjk =
*
yni k (9)
</equation>
<page confidence="0.965582">
1497
</page>
<listItem confidence="0.973518">
• Stop word ratio: ratio of stop words in cj. The
value can be {0, 0.5,1}.
• Similarity with topic title: the number of com-
mon words in these two strings, divided by the
length of the longer string.
• Document ratio: percentage of documents con-
taining cj.
• Sentence ratio: percentage of sentences con-
taining cj.
• Sentence-title similarity: word uni-
gram/bigrams cosine similarity between
the sentence containing cj and the topic title.
For concepts appearing in multiple sentences,
we choose the maximal similarity.
• Sentence-query similarity: word uni-
gram/bigram cosine similarity between
the sentence containing cj and the topic query
(concatenation of topic title and description).
For concepts appearing in multiple sentences,
we choose the maximal similarity.
• Sentence position: position of the sentence
containing cj in the document. For concepts
appearing in multiple sentences, we choose the
minimum.
• Sentence length: length of the sentence con-
taining cj. For concepts appearing in multiple
sentences, we choose the maximum.
• Paragraph starter: binary feature indicating
whether cj appears in the first sentence of a
paragraph.
</listItem>
<bodyText confidence="0.838183">
For subtree deletion model, we define the follow-
ing features for arc ahm.
</bodyText>
<listItem confidence="0.998564">
• POS tags of head word xh and child word xm
and their concatenations.
• Dependency label of arc ahm and its parent arc.
• Word xm if xm is a conjunction word or prepo-
sition word. Word xh if xm is a conjunction
word or preposition word.
• Binary feature indicating whether the modifier
xm is a temporal word such as Friday.
</listItem>
<bodyText confidence="0.619922333333333">
We also define some hard constraints for subtree
deletion to improve the readability of the generated
compressed sentences.
</bodyText>
<listItem confidence="0.990414538461538">
• C0 Arc ahm can be cut only if one of the two
conditions holds: (1) there is a comma, colon,
or semicolon between the head and the modifi-
er; (2) the modifier word is a preposition (POS
tag is IN) or a wh-word, such as what, who,
whose (corresponds to POS tag IN, WDT, WP,
WP$, WRB).
• C1 Arcs with dependency labels SUB, OBJ,
PRD, SBAR or VC can not be cut.
• C2 Arcs in set phrases like so far, more than,
according to can not be cut.
• C3 All arcs in coordinate structures can not be
cut, such as cats and dogs.
</listItem>
<bodyText confidence="0.9995122">
Note that compared with previous work, our com-
pression is more conservative. Constraint C0 al-
lows only a small portion of arcs to be cut. This
is based on our observation of the sentence com-
pression corpus: removing preposition phrases (PP)
or sub-clauses can greatly reduce the length of sen-
tence, while hurting the readability little. Cutting
other arcs like NMOD usually removes only one or
two words, and possibly affects the sentence’s read-
ability.
</bodyText>
<sectionHeader confidence="0.994222" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.928737">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999995571428572">
Due to the lack of training data for compressive
summarization, we learn the subtree deletion mod-
el and the concept model separately. Specifically,
the sentence compression dataset (Clarke and La-
pata, 2008) (referred as CL08) is used for subtree
deletion model training (Oarc). A sentence pair in
the corpus is kept for training the subtree deletion
model if the compressed sentence can be derived by
deleting subtrees from the parse tree of the origi-
nal sentence. There are 3,178 out of 5, 739 such
pairs. The concept model (Oconcept) is learned from
the TAC2009 dataset. We create the oracle extrac-
tive summaries with the maximal bigram recall as
the reference summary. TAC2010 data is used as
</bodyText>
<page confidence="0.979321">
1498
</page>
<table confidence="0.9990926">
Corpus Sent. Words Topics
Train TAC2009 4,216 117,304 44
CL08 3,178 52,624 N/A
Develop TAC2010 2,688 72,609 46
Test TAC2008 4,518 123,946 48
</table>
<tableCaption confidence="0.999478">
Table 1: Corpus statistics. Training data consist of
</tableCaption>
<bodyText confidence="0.9911482">
two parts, TAC2009 for learning the concept mod-
el, CL08 (Clarke and Lapata, 2008) for learning the
subtree deletion model.
development set for various parameter tuning. Table
1 has the descriptions of all the data used.
We choose averaged perceptron for fast training.
The number of iterations is tuned on the develop-
ment data. Remind that our algorithm is based on the
assumption that scores of concepts are non-negative,
bj, wj &gt;— 0. We assume that feature vector fconcept
is non-negative (e.g., term frequency, n-gram fea-
tures), then Bconcept &gt;— 0 is required to guarantee the
non-negativity of wj. Therefore, we project Bconcept
onto the non-negative space after each iteration. S-
ince training is offline, we use ILP based exact in-
ference for accurate learning. 1
To control the contributions of the concept mod-
el and the subtree deletion model, we introduce a
parameter p, and modify the original maximization
problem (Problem 2) to:
</bodyText>
<equation confidence="0.919182">
�wj &apos; vj + p X wahmzh(1 − z.)
ahmEA
</equation>
<bodyText confidence="0.999919357142857">
We tune p on TAC2010 dataset. For max-flow/min-
cut, in our experiments, we implemented the im-
proved shortest augmented path (SAP) method (Ed-
monds and Karp, 1972).
For performance measure of the summaries, we
use both ROUGE and linguistic quality. ROUGE
has been widely used for summarization perfor-
mance and can measure the informativeness of the
summaries (content match between system and ref-
erence summaries). Since joint compression and
summarization tends to pick isolated words to max-
imize the information coverage in the system gener-
ated summaries, it may have poor readability. There-
fore we conduct human evaluation for the linguis-
</bodyText>
<footnote confidence="0.991295">
1we choose the GLPK as our ILP solver, which is used in
(Berg-Kirkpatrick et al., 2011)
</footnote>
<bodyText confidence="0.998529928571429">
tic quality for various systems. The linguistic qual-
ity consists of two parts. One evaluates the gram-
mar quality within a sentence. Annotators marked
if a compressed sentence is grammatically correc-
t. Typical grammar errors include lack of verb or
subordinate clause. The other evaluates the coher-
ence between sentences, including the order of sen-
tences and irrelevant sentences. For compressive
summaries, we removed the sentences with gram-
mar errors when evaluating coherence. The overall
linguistic quality score is the combined score of the
percentage of grammatically correct sentences and
the correct ordering of the summary sentences. The
score is scaled and ranges from 1 (bad) to 10 (good).
</bodyText>
<subsectionHeader confidence="0.983756">
5.2 Results on the Development Set
</subsectionHeader>
<bodyText confidence="0.999974454545454">
We conducted a series of experiments on the de-
velopment dataset to investigate the effect of the
non-negative score assumption, SBQP approxima-
tion, and initialization. First, we build a stan-
dard ILP based compressive summarizer without the
non-negative score assumption. We varied p over
{2−4, 2−3, ... 24} and selected the optimal p = 2−2
according to both ROUGE-2 score and linguistic
quality. This interpolation weight is used in all the
other experiments.
To study the impact of the non-negative score as-
sumption, we build another summarizer by replac-
ing the concept model with the one trained under the
non-negative constraint. We also compared three d-
ifferent initialization strategies for p. The first one
is uniform initialization, i.e., pjk = 1
|;|. The second
one is a greedy approach, where extractive summa-
rization is obtained by greedy search (i.e., add the
top ranked sentence iteratively), then we use the cor-
responding y and Eq (9) to initialize p. The last one
is our convex relaxation method described in Sec-
tion 3.4.
Table 2 shows the comparison results. For com-
parison, we also include the sentence-based ILP ex-
tractive summarization results. We can see that the
impact of initial p is substantial. Using convex re-
laxation helps our method to survive from local opti-
mality. The non-negativity assumption has very lit-
tle effect on the standard compressive summariza-
tion (comparing the first two rows). This empir-
ical result demonstrates the appropriateness of the
assumption we use in our proposed method.
</bodyText>
<figure confidence="0.810082333333333">
J
max
z,v j=1
</figure>
<page confidence="0.936761">
1499
</page>
<table confidence="0.999764714285714">
System R-2 LQ
ILP (µ = 2−2) 11.22 6.3
ILP (Non Neg.) 11.18 6.4
Graph Cut (uniform) 9.54 5.9
Graph Cut (greedy) 10.13 6.2
Graph Cut (LP) 11.06 6.1
Sent Extractive 10.11 7.3
</table>
<tableCaption confidence="0.80812825">
Table 2: Experimental results on developmen-
t dataset. R-2 and LQ are short for ROUGE-2 score
multiplied by 100, and linguistic quality respective-
ly.
</tableCaption>
<subsectionHeader confidence="0.990126">
5.3 Results on Test Dataset
</subsectionHeader>
<bodyText confidence="0.999971266666667">
Table 3 shows the summarization results for various
systems on the TAC2008 data set. We show both the
summarization performance and the speed2 of the
system. The presented systems include our graph-
cut based method, the ILP based compression and
summarization, and the sentence-based extractive
summarization. ILP 2-step refers to the 2-step fast
decoding strategy proposed by (Berg-Kirkpatrick et
al., 2011).
We also list the performance of some state-of-the-
art systems, including the two ICSI systems (Gillick
et al., 2008), the compressive summarization sys-
tem of Berg-Kirkpatrick et al. (2011) (GBK’11),
the multi-aspect ILP system of Woodsend and Lapa-
ta (2012)(WL’12) and the dual decomposition based
system (Almeida and Martins, 2013) (AM’13). Note
that for these referred systems since the linguistic
quality results are not comparable due to different
judgment methods. For our graph-cut based method,
to study the tradeoff between the readability of the
summary and the ROUGE scores, we present two
versions for this method: one uses all the constraints
(C0-C3), the other does not use C0.
We can see that our proposed method balanced
speed and quality. Compared with ILP, we achieved
competitive ROUGE scores, but with about 100x
speedup. Our method is also faster than the 2-step
ILP system. We also tried another state-of-the-art
LP solver, Gurobi version 5.53, it achieves 0.17 sec-
onds per topic, much faster than GLPK, but stil-
</bodyText>
<footnote confidence="0.999773">
2For fair comparison, we only recode the running time for
decoding. Other time costs like feature extraction, IO opera-
tions are excluded.
3www.gurobi.com
</footnote>
<table confidence="0.999626416666667">
System R-2 R-SU4 LQ sec.
Graph Cut 11.74 14.54 6.5 0.056
Graph Cut w/o C0 12.05 14.71 5.4 0.053
ILP 11.86 14.62 6.6 5.5
ILP (Non Neg.) 11.82 14.60 6.6 5.2
ILP (2-step) 11.72 14.49 6.5 1.1
Sent Extractive 11.06 13.93 7.1 0.13
ICSI-1 11.0 13.4 - 0.38†
ICSI-2 11.1 14.3 - -
BGK’11 11.70 14.38 6.5† -
WL’12 11.37 14.47 - -
AM’13 12.30+ 15.18+ 4.2† 0.41†
</table>
<tableCaption confidence="0.830774">
Table 3: Experimental results on TAC2008 dataset.
Columns 2-5 are scores of ROUGE-2, ROUGE-
</tableCaption>
<bodyText confidence="0.997551928571428">
SU4, linguistic quality, and speed (seconds per top-
ic). ROUGE-2 and ROUGE-SU4 scores are multi-
plied by 100. All the experiments are conducted on
the platform Intel Core i5-2500 CPU 3.30GHz. †
numbers are not directly comparable due to differ-
ent annotations or platforms. + extra resources are
used.
l slower than ours. Regarding the grammar con-
straints used in our system, from the two result-
s for our graph-cut based method, we can see that
adding constraint C0 significantly decreases the R-2
score but improves the language quality. This shows
that word-based joint compression and summariza-
tion can improve ROUGE score; however, we need
to keep in mind about linguistic quality and find a
tradeoff between the ROUGE score and the linguis-
tic quality. Almeida and Martins (2013) trained their
model on extra corpora using multi-task learning,
and achieved better results than ours. The results
of our system and theirs showed that Lagrangian re-
laxation based method combined with combinatorial
optimization algorithms such as dynamic program-
ming or minimum cut can exploit the inner structure
of problems and achieve significant speedup over
ILP.
Four example summaries produced by our system
are shown below. Words in gray are not selected in
the summary.
</bodyText>
<page confidence="0.651559">
1500
</page>
<bodyText confidence="0.99974925974026">
India’s space agency is ready to send a man to space within sev-
en years if the government gives the nod, while preparations have
lready begun for the launch of an unmanned lunar mission, a top
official said. India will launch more missions to the moon if its
maiden unmanned spacecraft Chandrayaan-1, slated to be launched
by 2008, is successful a top space fficial said Tuesday. The Unit-
ed States, the European Space Agency, China, Japan and India are
all planning lunar missions during the ext decade.India is “a step
ahead” of China in satellite technology and can surpass Beijing
in space research by tapping the talent of its huge pool of young
scientists, India’s space research chief said Monday. The space
agencies of India and France signed an agreement on Friday to co-
operate in launching a satellite in four years that will help make
climate predictions more accurate. The Indian Space Research Or-
ganization (ISRO) has short-listed experiments from five nation-
s including the United States, Britain and Germany, for a slot on
India’s unmanned moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) reported Monday. A
three-member Afghan delegation is in Bangalore seeking help to
set up a high-tech telemedicine facility in 10 Afghan cities linked
via Indian satellites, Indo-Asian News Service reported Saturday.
A woman was killed in Mississippi when a tree crashed on her car,
becoming the 11th fatality blamed on the powerful Hurricane Kat-
rina that slammed the US Gulf coast after pounding Florida, local
TV reported Monday. The bill for the Hurricane Katrina disaster ef-
fort has so far reached 2.87 billion dollars, federal officials said on
Tuesday. The official death toll from Hurricane Katrina has risen
to 118 people in and around the swamped city of New Orleans,
officials said Thursday. The Foreign Ministry on Friday reported
the first confirmed death of a Guatemalan due to Hurricane Kat-
rina in the United States. The Ugandan government has pledged
200,000 US dollars toward relief and rebuilding efforts in the after-
math of Hurricane Katrina, local press reported on Friday. Swiss
Reinsurance Co., the world’s second largest reinsurance company
on Monday doubled to 40 billion US dollars its initial estimate of
the global insured losses caused by Hurricane Katrina in the United
States.
The A380 ’superjumbo’, which will be presented to the world in
a lavish ceremony in southern France on Tuesday, will be prof-
itable from 2008, its maker Airbus told the French financial news-
paper La Tribune. The A380 will take over from the Boeing 747
as the biggest jet in the skies. An association of residents living n-
ear Paris’s Charles-de-Gaulle airport on Wednesday denounced the
noise pollution generated by the giant Airbus A380, after the new
airliner’s maiden flight. One problem that Airbus is encountering
with its new A380 is that the craft pushes the envelope on the max-
imum size of a commercial airplane. With a whisper more than a
roar, the largest passenger airliner ever built, the Airbus 380, took
off on its maiden flight Wednesday.
“When she came in, she was in good spirits,” a prison staffer told
the New York Daily News. Martha Stewart, the American celebrity
homemaker who had her own cooking and home improvement TV
show, reported to a federal prison in Alderson, West Virginia, on
Friday to serve a five-month sentence for lying about a stock sale.
Home fashion guru Martha Stewart said on Friday that she has ad-
justed to prison life and is keeping busy behind bars since reporting
a week ago to a federal penal camp in West Virginia, where she
is serving a five-month sentence for lying about a stock sale. The
lawyer said he did not know what she is writing, but Stewart has
suggested since her conviction that she might write a book about
her recent experience with the legal system. Walter Dellinger, the
lawyer leading the appeal, said on NBC’s “Today” that Stewart is
exploring “innovative ways to do microwave cooking” The lawyer
said he did not know with her fellow inmates. As Martha Stewart
arrives at the red-brick federal prison in Alderson, W. Va., on Fri-
day to begin a five-month sentence, the company she founded is
focused both on life without her and on life once she returns.
In most cases, the removed phrases do not hurt the
readability of the summaries. The errors are mainly
caused by the break of sub-clauses or main claus-
es that are separated by commas, for example, the
fourth sentence in the last summary, The lawyer said
he did not know what she is writing. The compressed
sentence is grammatically correct, but semantically
incomplete. Other errors are due to the lack of verb,
subject, or object, or incorrect removal of PP, such
as the last sentence of the last summary.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998565">
In this paper, we propose a fast decoding algorith-
m for compressive summarization using graph cuts.
Our idea is to approximate the original ILP prob-
lem using supermodular binary quadratic program-
ming (SBQP) problem. Under the assumption that
scores of concepts are non-negative, we eliminate
subtree constraints and grammar constraints, and
relax the length constraint and non-supermodular
part of the problem step by step. Our experimen-
tal results showed that the graph cut based method
achieved competitive performance compared to ILP,
while about 100 times faster.
There are several possibilities for further research
involving our graph cut algorithms. One idea is to
apply it to the language model based compression
method (Clarke and Lapata, 2008). The other is
to adapt it to social media text summarization task,
where text is much more noisy. As graph cut is a
general method, applying it to other binary struc-
tured learning tasks is also an interesting direction.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916">
We’d like to thank three anonymous reviewers for
their valuable comments. This work is partly sup-
ported by NSF award IIS-0845484 and DARPA un-
der Contract No. FA8750-13-2-0041. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
</bodyText>
<sectionHeader confidence="0.996555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.809364666666667">
A. Aker and R. Gaizauskas. 2009. Summary generation
for toponym-referenced images using object type lan-
guage models. In Proceedings of RANLP.
</reference>
<page confidence="0.826717">
1501
</page>
<reference confidence="0.9998757">
Miguel Almeida and Andre Martins. 2013. Fast and ro-
bust compressive summarization with dual decompo-
sition and multi-task learning. In Proceedings of ACL,
pages 196–206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481–490, June.
A. Billionnet and M. Minoux. 1985. Maximizing a su-
permodular pseudoboolean function: A polynomial al-
gorithm for supermodular cubic functions. Discrete
Applied Mathematics, 12(1):1 – 11.
Endre Boros and Peter L. Hammer. 2002. Pseudo-
boolean optimization. Discrete Applied Mathematics,
123(1C3):155 – 225.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing &amp; Man-
agement, 31(5):675 – 685.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In COLING,
pages 457–474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399–429.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
L1-ball for learning in high dimensions. In Proceed-
ings of ICML, pages 272–279.
Jack Edmonds and Richard M. Karp. 1972. Theoret-
ical improvements in algorithmic efficiency for net-
work flow problems. J. ACM, 19(2):248–264, April.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264–285, April.
Daniel Freedman and Petros Drineas. 2005. Energy min-
imization via graph cuts: Settling what is possible. In
CVPR (2), pages 939–946.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18, June.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008.
The ICSI summarization system at tac 2008. In Pro-
ceedings of the Text Understanding Conference.
Matthew R. Gormley and Jason Eisner. 2013. Noncon-
vex global optimization for latent-variable models. In
Proceedings of ACL, pages 444–454, August.
Vladimir Kolmogorov and Ramin Zabih. 2004. What en-
ergy functions can be minimized via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 26:65–81.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP 2010, pages 1288–1298, Oc-
tober.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP (to appear), Oc-
tober.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using super-
vised bigram-based ILP for extractive summarization.
In Proceedings of ACL, pages 1004–1013, August.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL, pages 510–520, June.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP 2009,
pages 261–264, August.
Andr´e F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ’09, pages 1–9.
Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuk-
i Aihara. 2011. Size-constrained submodular min-
imization through minimum norm base. In ICML,
pages 977–984.
Xian Qian and Yang Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, 1:105–151.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In First
Document Understanding Conference.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL, pages
233–243, July.
</reference>
<page confidence="0.99635">
1502
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924708">
<title confidence="0.999924">Fast Joint Compression and Summarization via Graph Cuts</title>
<author confidence="0.988989">Qian</author>
<affiliation confidence="0.980365">The University of Texas at</affiliation>
<address confidence="0.964133">800 W. Campbell Rd., Richardson, TX,</address>
<abstract confidence="0.998837807692308">Extractive summarization typically uses sentences as summarization units. In contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information. The goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint. We propose an efficient decoding algorithm for fast compressive summarization using graph cuts. Our approach first relaxes the length constraint using Lagrangian relaxation. Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efficiently using graph max-flow/min-cut. Since finding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization. Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability, while is much faster than the integer linear programming (ILP) method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>R Gaizauskas</author>
</authors>
<title>Summary generation for toponym-referenced images using object type language models.</title>
<date>2009</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="19909" citStr="Aker and Gaizauskas, 2009" startWordPosition="3464" endWordPosition="3467">o learn the scores of concepts and arcs. For concept cj, its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence containing cj and the topi</context>
</contexts>
<marker>Aker, Gaizauskas, 2009</marker>
<rawString>A. Aker and R. Gaizauskas. 2009. Summary generation for toponym-referenced images using object type language models. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Almeida</author>
<author>Andre Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>196--206</pages>
<contexts>
<context position="3608" citStr="Almeida and Martins (2013)" startWordPosition="532" endWordPosition="535">. (2011) found that LP relaxation gave poor results, finding unacceptably suboptimal solutions. For speedup, they proposed a two stage method where they performed some sentence selection in the first step to reduce the number of candidates. Despite their empirical success, such 1492 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a pruning approach has its inherent problem in that it may eliminate correct sentences in the first step. Recently, Almeida and Martins (2013) proposed a fast joint decoding algorithm based on dual decomposition. For fast convergence, they added quadratic penalty terms to alleviate the learning rate problem. In this paper, we propose an efficient decoding algorithm for fast ILP based compressive summarization using graph cuts. Our assumption is that all concepts are word n-grams and non-negatively scored. The rationale for the non-negativity assumption is straightforward: the score of a concept reflects its informativeness, hence should be non-negative. Given a set of documents, each word is associated with a binary variable, indica</context>
<context position="28724" citStr="Almeida and Martins, 2013" startWordPosition="4927" endWordPosition="4930">on performance and the speed2 of the system. The presented systems include our graphcut based method, the ILP based compression and summarization, and the sentence-based extractive summarization. ILP 2-step refers to the 2-step fast decoding strategy proposed by (Berg-Kirkpatrick et al., 2011). We also list the performance of some state-of-theart systems, including the two ICSI systems (Gillick et al., 2008), the compressive summarization system of Berg-Kirkpatrick et al. (2011) (GBK’11), the multi-aspect ILP system of Woodsend and Lapata (2012)(WL’12) and the dual decomposition based system (Almeida and Martins, 2013) (AM’13). Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods. For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two versions for this method: one uses all the constraints (C0-C3), the other does not use C0. We can see that our proposed method balanced speed and quality. Compared with ILP, we achieved competitive ROUGE scores, but with about 100x speedup. Our method is also faster than the 2-step ILP system. We also tried another state-of-the-ar</context>
<context position="30799" citStr="Almeida and Martins (2013)" startWordPosition="5276" endWordPosition="5279">orm Intel Core i5-2500 CPU 3.30GHz. † numbers are not directly comparable due to different annotations or platforms. + extra resources are used. l slower than ours. Regarding the grammar constraints used in our system, from the two results for our graph-cut based method, we can see that adding constraint C0 significantly decreases the R-2 score but improves the language quality. This shows that word-based joint compression and summarization can improve ROUGE score; however, we need to keep in mind about linguistic quality and find a tradeoff between the ROUGE score and the linguistic quality. Almeida and Martins (2013) trained their model on extra corpora using multi-task learning, and achieved better results than ours. The results of our system and theirs showed that Lagrangian relaxation based method combined with combinatorial optimization algorithms such as dynamic programming or minimum cut can exploit the inner structure of problems and achieve significant speedup over ILP. Four example summaries produced by our system are shown below. Words in gray are not selected in the summary. 1500 India’s space agency is ready to send a man to space within seven years if the government gives the nod, while prepa</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel Almeida and Andre Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proceedings of ACL, pages 196–206, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>481--490</pages>
<contexts>
<context position="2990" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="442" endWordPosition="445">s, unlike extractive summarization where each sentence is a basic undecomposable unit. To achieve better readability, manually defined grammar constraints or automatically learned models based on syntax trees are added during the summarization process. Up to now, the state of the art compressive systems are based on integer linear programming (ILP). Because ILP suffers from exponential complexity, word-based compression summarization is an order of magnitude slower than sentence-based extraction. One common way to solve an ILP problem is to use its LP relaxation and round the results. However Berg-Kirkpatrick et al. (2011) found that LP relaxation gave poor results, finding unacceptably suboptimal solutions. For speedup, they proposed a two stage method where they performed some sentence selection in the first step to reduce the number of candidates. Despite their empirical success, such 1492 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a pruning approach has its inherent problem in that it may eliminate correct sentences in the first step. Recently, Almeida </context>
<context position="5332" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="808" endWordPosition="811"> solution consists of 3 steps. First, we show that the subtree deletion model and grammar constraints can be eliminated by adding SBQFs to the objective function. Second, we relax the summary length constraint using Lagrangian relaxation. Third, we propose a family of SBQFs that are lower bounds of the ILP objective function. Since finding the tightest lower bound suffers from local optimality, we choose to use convex relaxation for initialization. To demonstrate our technique, we conduct experiments on Text Analysis Conference (TAC) datasets using the same train/test splits as previous work (Berg-Kirkpatrick et al., 2011). We compare our approach with the state-of-the-art ILP based approach in terms of summary quality (ROUGE scores and sentence quality) and speed. Experimental results show that our proposed method achieves competitive performance with ILP, while about 100 times faster. 2 Compressive Summarization 2.1 Extractive Summarization As our method is an approximation of ILP based method, we first briefly review the ILP based extractive summarization and compressive summarization. Gillick and Favre (2009) introduced the conceptbased ILP for summarization. A concept is a basic semantic unit. They used wo</context>
<context position="8564" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1337" endWordPosition="1340">2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts, and scores of the broken arcs in the dependency parse trees. The selected words must satisfy the length constraint and grammar constraints that include subtree constraint and some manually defined hard constraints. Formally, let x = x1 ... xI denote the word sequence of documents, where s</context>
<context position="25356" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="4385" endWordPosition="4388">ved shortest augmented path (SAP) method (Edmonds and Karp, 1972). For performance measure of the summaries, we use both ROUGE and linguistic quality. ROUGE has been widely used for summarization performance and can measure the informativeness of the summaries (content match between system and reference summaries). Since joint compression and summarization tends to pick isolated words to maximize the information coverage in the system generated summaries, it may have poor readability. Therefore we conduct human evaluation for the linguis1we choose the GLPK as our ILP solver, which is used in (Berg-Kirkpatrick et al., 2011) tic quality for various systems. The linguistic quality consists of two parts. One evaluates the grammar quality within a sentence. Annotators marked if a compressed sentence is grammatically correct. Typical grammar errors include lack of verb or subordinate clause. The other evaluates the coherence between sentences, including the order of sentences and irrelevant sentences. For compressive summaries, we removed the sentences with grammar errors when evaluating coherence. The overall linguistic quality score is the combined score of the percentage of grammatically correct sentences and the </context>
<context position="28392" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="4877" endWordPosition="4880"> Cut (LP) 11.06 6.1 Sent Extractive 10.11 7.3 Table 2: Experimental results on development dataset. R-2 and LQ are short for ROUGE-2 score multiplied by 100, and linguistic quality respectively. 5.3 Results on Test Dataset Table 3 shows the summarization results for various systems on the TAC2008 data set. We show both the summarization performance and the speed2 of the system. The presented systems include our graphcut based method, the ILP based compression and summarization, and the sentence-based extractive summarization. ILP 2-step refers to the 2-step fast decoding strategy proposed by (Berg-Kirkpatrick et al., 2011). We also list the performance of some state-of-theart systems, including the two ICSI systems (Gillick et al., 2008), the compressive summarization system of Berg-Kirkpatrick et al. (2011) (GBK’11), the multi-aspect ILP system of Woodsend and Lapata (2012)(WL’12) and the dual decomposition based system (Almeida and Martins, 2013) (AM’13). Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods. For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two </context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of ACL-HLT, pages 481–490, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Billionnet</author>
<author>M Minoux</author>
</authors>
<title>Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions.</title>
<date>1985</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="12307" citStr="Billionnet and Minoux, 1985" startWordPosition="2003" endWordPosition="2006">, but is also reasonable: the score of a concept denotes its informativeness, hence should be non-negative. For example, Li et al. (2013b) proposed to use the estimated normalized frequencies of concepts as scores, which are essentially nonnegative. The basic idea of our method is to approximate the above optimization problem (2) by the supermodular binary quadratic programming (SBQP) problem: max z � �βizi + αijzizj ij i s.t. z is binary (3) where αij &gt; 0. It is known that such a binary quadratic function is supermodular, and its maximum can be solved efficiently using graph maxflow/min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). Now the problem is to find the optimal α, β for a good approximation. 3.1 Formulate Grammar Constraints and Subtree Deletion Model by SBQF We show that the subtree deletion model can be formulated equivalently using SBQF. First, we can eliminate the constraint zh &gt; zm by adding a penalty term to the objective function. That is, max f(z) s.t. zh &gt; zm z is binary is equivalent to max f(z) − 00(1 − zh)zm s.t. z is binary We can see that the penalty term −00(1− zh)zm excludes zh = 0, zm = 1 from the feasible set, and for zh &gt; zm, both problems have the same objective</context>
</contexts>
<marker>Billionnet, Minoux, 1985</marker>
<rawString>A. Billionnet and M. Minoux. 1985. Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions. Discrete Applied Mathematics, 12(1):1 – 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Endre Boros</author>
<author>Peter L Hammer</author>
</authors>
<title>Pseudoboolean optimization.</title>
<date>2002</date>
<journal>Discrete Applied Mathematics, 123(1C3):155 –</journal>
<pages>225</pages>
<contexts>
<context position="15466" citStr="Boros and Hammer, 2002" startWordPosition="2604" endWordPosition="2607"> current λ is too small, so we set λmin = λ; otherwise, λ &gt; 0 and ∑i zi &lt; L, we set λmax = λ. The search process terminates if λmax − λmin is less than a predefined threshold. 3.3 Eliminate v Using Supermodular Relaxation Now we consider the inner maximization Problem (5). It is still not a SBQP, since the objective function is not a linear function of zizj. We propose to approximate the objective function using SBQP. Our solution consists of two steps. First we relax the first constraint of Problem (5) by bounding the objective function with a family of supermodular pseudo boolean functions (Boros and Hammer, 2002). Second we reformulate these pseudo boolean functions equivalently as quadratic functions. Similar to the bounding strategy in (Qian and Liu, 2013), we relax the logical disjunction by linearization. Using the fact that for any binary vector a, we have ∪ ∑ ai = max pE∆ i where ∆ denotes the probability simplex ∆ = tp |∑ pk = 1,pk &gt;&gt;- 0} k We have ∪vj = ∏ zi k iEojk ∑ ∏ = max pjk zi pjE∆ k iEojk Plug the equation above into the objective function of Problem (5), we get the following optimization problem   ∑ ∏ pjkwj zi  k iEojk + ∑ wahmzh(1 − zm) ahmEA ∑−00 (1 − zh)zm ahmEA +λ(L − ∑ zi) i s</context>
</contexts>
<marker>Boros, Hammer, 2002</marker>
<rawString>Endre Boros and Peter L. Hammer. 2002. Pseudoboolean optimization. Discrete Applied Mathematics, 123(1C3):155 – 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brandow</author>
<author>Karl Mitze</author>
<author>Lisa F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<booktitle>Information Processing &amp; Management, 31(5):675 –</booktitle>
<pages>685</pages>
<contexts>
<context position="19882" citStr="Brandow et al., 1995" startWordPosition="3460" endWordPosition="3463">iscriminative models to learn the scores of concepts and arcs. For concept cj, its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing &amp; Management, 31(5):675 – 685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yllias Chali</author>
<author>Sadid A Hasan</author>
</authors>
<title>On the effectiveness of using sentence compression models for queryfocused multi-document summarization.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>457--474</pages>
<contexts>
<context position="8588" citStr="Chali and Hasan, 2012" startWordPosition="1341" endWordPosition="1344"> quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts, and scores of the broken arcs in the dependency parse trees. The selected words must satisfy the length constraint and grammar constraints that include subtree constraint and some manually defined hard constraints. Formally, let x = x1 ... xI denote the word sequence of documents, where s1 = x1, ... xl1 correspo</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Yllias Chali and Sadid A. Hasan. 2012. On the effectiveness of using sentence compression models for queryfocused multi-document summarization. In COLING, pages 457–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>31--399</pages>
<contexts>
<context position="9823" citStr="Clarke and Lapata (2008)" startWordPosition="1555" endWordPosition="1558">first sentence, s2 = xl1+1, . . . , xl1+l2 corresponds to the second sentence, and so on. A compressive summary can be represented by a binary vector z, where zi indicates whether word xi is selected in the summary. Let ahm denote the arc xh → xm in the dependency parse tree of the corresponding sentence containing words xh and xm, and A = {ahm} denote the set of dependency arcs. The subtree constraint ensures that word xm is selected only if its head xh is selected. In order to guarantee the readability, grammar constraints are added to prohibit the breaks of some specific arcs. For example, Clarke and Lapata (2008) never deleted an arc whose dependency label is SUB, OBJ, PMOD, SBAR or VC. In this paper, we use B ⊆ A to denote the set of these arcs that must not be broken in summarization. We use ojk to denote the indices of words corresponding to the kth occurrence of cj. For example, suppose the jth concept European Union appears twice in the document: x22x23 = x50x51 =European Union, then oj1 = {22, 23}, oj2 = {50, 51}. The compressive summarization model can be formulated as an integer programming problem �wj · vj + wahmzh(1 − zm) ahmEA Us.t. vj = ri zi bj k iEojk � i zh ≥ zm bahm E A (2) zh = zm bah</context>
<context position="22967" citStr="Clarke and Lapata, 2008" startWordPosition="3987" endWordPosition="3991">raint C0 allows only a small portion of arcs to be cut. This is based on our observation of the sentence compression corpus: removing preposition phrases (PP) or sub-clauses can greatly reduce the length of sentence, while hurting the readability little. Cutting other arcs like NMOD usually removes only one or two words, and possibly affects the sentence’s readability. 5 Experimental Results 5.1 Experimental Setup Due to the lack of training data for compressive summarization, we learn the subtree deletion model and the concept model separately. Specifically, the sentence compression dataset (Clarke and Lapata, 2008) (referred as CL08) is used for subtree deletion model training (Oarc). A sentence pair in the corpus is kept for training the subtree deletion model if the compressed sentence can be derived by deleting subtrees from the parse tree of the original sentence. There are 3,178 out of 5, 739 such pairs. The concept model (Oconcept) is learned from the TAC2009 dataset. We create the oracle extractive summaries with the maximal bigram recall as the reference summary. TAC2010 data is used as 1498 Corpus Sent. Words Topics Train TAC2009 4,216 117,304 44 CL08 3,178 52,624 N/A Develop TAC2010 2,688 72,6</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. J. Artif. Intell. Res. (JAIR), 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Tushar Chandra</author>
</authors>
<title>Efficient projections onto the L1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="17333" citStr="Duchi et al., 2008" startWordPosition="2998" endWordPosition="3001">f Problem (7), to search the optimal point, we alternatively update p and z, q. First we initialize p = p(0). In each iteration, we first fix p. It is obvious that Problem (7) is a SBQP, hence the optimal z, q can be solved efficiently using max-flow/mincut. Then we fix z, q, and update p using projected ∑J j=1 max z,p ∑ k max z,p,q ∑J j=1 + ∑ piai ahmEA 1496 subgradient. That is (pj ∂R pj ew = Po + α (8) ∂pj where α &gt; 0 is the step size in line search, and function P∆(q) denotes the projection of q onto the feasible set ∆ P∆(q) = min IIp − qII2 PE∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.4 Initialize p Using Convex Relaxation Since R is non-concave, searching its maximum using subgradient method suffers from local optimality. Though one can use techniques such as branchand-bound for exact inference (Qian and Liu, 2013; Gormley and Eisner, 2013), here for fast decoding, we use convex relaxation to choose a good seed p(0). Recall that pjk denotes the percentage of the kth occurrence contributing to cj. The larger pjk is, the more likely the kth occurrence is selected. To estimate such likelihood, we replace the binary constraint in extractive summarization (Problem (1)) by 0</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. 2008. Efficient projections onto the L1-ball for learning in high dimensions. In Proceedings of ICML, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Edmonds</author>
<author>Richard M Karp</author>
</authors>
<title>Theoretical improvements in algorithmic efficiency for network flow problems.</title>
<date>1972</date>
<journal>J. ACM,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24791" citStr="Edmonds and Karp, 1972" startWordPosition="4294" endWordPosition="4298">, n-gram features), then Bconcept &gt;— 0 is required to guarantee the non-negativity of wj. Therefore, we project Bconcept onto the non-negative space after each iteration. Since training is offline, we use ILP based exact inference for accurate learning. 1 To control the contributions of the concept model and the subtree deletion model, we introduce a parameter p, and modify the original maximization problem (Problem 2) to: �wj &apos; vj + p X wahmzh(1 − z.) ahmEA We tune p on TAC2010 dataset. For max-flow/mincut, in our experiments, we implemented the improved shortest augmented path (SAP) method (Edmonds and Karp, 1972). For performance measure of the summaries, we use both ROUGE and linguistic quality. ROUGE has been widely used for summarization performance and can measure the informativeness of the summaries (content match between system and reference summaries). Since joint compression and summarization tends to pick isolated words to maximize the information coverage in the system generated summaries, it may have poor readability. Therefore we conduct human evaluation for the linguis1we choose the GLPK as our ILP solver, which is used in (Berg-Kirkpatrick et al., 2011) tic quality for various systems. T</context>
</contexts>
<marker>Edmonds, Karp, 1972</marker>
<rawString>Jack Edmonds and Richard M. Karp. 1972. Theoretical improvements in algorithmic efficiency for network flow problems. J. ACM, 19(2):248–264, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>J. ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="19926" citStr="Edmundson, 1969" startWordPosition="3468" endWordPosition="3469">pts and arcs. For concept cj, its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence containing cj and the topic title. For conc</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. J. ACM, 16(2):264–285, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Freedman</author>
<author>Petros Drineas</author>
</authors>
<title>Energy minimization via graph cuts: Settling what is possible.</title>
<date>2005</date>
<booktitle>In CVPR (2),</booktitle>
<pages>939--946</pages>
<contexts>
<context position="16444" citStr="Freedman and Drineas, 2005" startWordPosition="2808" endWordPosition="2811">vj = ∏ zi k iEojk ∑ ∏ = max pjk zi pjE∆ k iEojk Plug the equation above into the objective function of Problem (5), we get the following optimization problem   ∑ ∏ pjkwj zi  k iEojk + ∑ wahmzh(1 − zm) ahmEA ∑−00 (1 − zh)zm ahmEA +λ(L − ∑ zi) i s.t. z is binary (6) pj E ∆ bj Let Q(p, z) denote the objective function of Problem (6). Given p, we can see that Q is a supermodular pseudo boolean function because coefficients of all non-linear terms are non-negative. Using the fact that for any binary vector a = [a1,... ar]&apos;, ai E t0, 1},1 &lt; i &lt; r, i=1 ∏r ai = max ∑ ai − r + 1)b bEt0,1} ( r i=1 (Freedman and Drineas, 2005), we get the following equivalent optimization problem of Problem (6) pjkwjqjk ∑ zi − |ojk |+ 1  iEojk wahmzh(1 − zm) ∑−00 (1 − zh)zm ahmEA +λ(L − ∑ zi) i s.t. z, q are binary (7) pj E ∆ bj where |ojk |is the size of ojk. Let R(z, p, q) denote the objective function of Problem (7), to search the optimal point, we alternatively update p and z, q. First we initialize p = p(0). In each iteration, we first fix p. It is obvious that Problem (7) is a SBQP, hence the optimal z, q can be solved efficiently using max-flow/mincut. Then we fix z, q, and update p using projected ∑J j=1 max z,p ∑ k max z</context>
</contexts>
<marker>Freedman, Drineas, 2005</marker>
<rawString>Daniel Freedman and Petros Drineas. 2005. Energy minimization via graph cuts: Settling what is possible. In CVPR (2), pages 939–946.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="5832" citStr="Gillick and Favre (2009)" startWordPosition="883" endWordPosition="886">on Text Analysis Conference (TAC) datasets using the same train/test splits as previous work (Berg-Kirkpatrick et al., 2011). We compare our approach with the state-of-the-art ILP based approach in terms of summary quality (ROUGE scores and sentence quality) and speed. Experimental results show that our proposed method achieves competitive performance with ILP, while about 100 times faster. 2 Compressive Summarization 2.1 Extractive Summarization As our method is an approximation of ILP based method, we first briefly review the ILP based extractive summarization and compressive summarization. Gillick and Favre (2009) introduced the conceptbased ILP for summarization. A concept is a basic semantic unit. They used word bigrams as such language concepts. Their system achieved the highest ROUGE score on the TAC 2009 evaluation. This approach selects sentences so that the total score of language concepts appearing in the summary is maximized. The association between the language concepts and sentences serves as the constraints, in addition to the summary length constraint. Formally, given a set of sentences S = {sn}Nn=1, extractive summarization can be represented by a binary vector y, where yn indicates wheth</context>
<context position="7931" citStr="Gillick and Favre, 2009" startWordPosition="1247" endWordPosition="1250">s is necessary for efficient summarization. For exammax J wjvj y,v j=1 �s.t. vj = ynik 1 &lt; j &lt; J (1) k N ynln &lt; L Z=1 v, y are binary 1493 ple, the ICSI system (Gillick et al., 2008) removed the sentences that are too short or have non-overlap with the queries, and concepts with document frequency less than 3, resulting in 95.8 sentences and about 80 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 20</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>The ICSI summarization system at tac</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Understanding Conference.</booktitle>
<contexts>
<context position="1938" citStr="Gillick et al., 2008" startWordPosition="280" endWordPosition="283">ization helps readers get the most important information from large amounts of texts. Summarization techniques can be roughly divided into two categories: extractive and abstractive. Extractive summarization casts the summarization task as a sentence selection problem: identifying important summary sentences from one or multiple documents. Many methods have been developed in the past decades, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming (Gillick et al., 2008) (ILP) and submodular maximization methods (Lin and Bilmes, 2011). Though extractive summarization is popular because of its simplicity and high readability, it has limitations in that it selects each sentence as a whole, and thus may miss informative partial sentences. To improve the informativeness, joint compression and summarization was proposed (BergKirkpatrick et al., 2011), which uses words as summarization units, unlike extractive summarization where each sentence is a basic undecomposable unit. To achieve better readability, manually defined grammar constraints or automatically learne</context>
<context position="7489" citStr="Gillick et al., 2008" startWordPosition="1172" endWordPosition="1175">ated as below: The first constraint is imposed by the relation between concept selection and sentence selection: selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept only happens when it is present in at least one of the selected sentences. The second constraint is the summary length constraint. As solving an ILP problem is generally NP-hard, pre-pruning of candidate concepts and sentences is necessary for efficient summarization. For exammax J wjvj y,v j=1 �s.t. vj = ynik 1 &lt; j &lt; J (1) k N ynln &lt; L Z=1 v, y are binary 1493 ple, the ICSI system (Gillick et al., 2008) removed the sentences that are too short or have non-overlap with the queries, and concepts with document frequency less than 3, resulting in 95.8 sentences and about 80 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the sum</context>
<context position="28509" citStr="Gillick et al., 2008" startWordPosition="4896" endWordPosition="4899">OUGE-2 score multiplied by 100, and linguistic quality respectively. 5.3 Results on Test Dataset Table 3 shows the summarization results for various systems on the TAC2008 data set. We show both the summarization performance and the speed2 of the system. The presented systems include our graphcut based method, the ILP based compression and summarization, and the sentence-based extractive summarization. ILP 2-step refers to the 2-step fast decoding strategy proposed by (Berg-Kirkpatrick et al., 2011). We also list the performance of some state-of-theart systems, including the two ICSI systems (Gillick et al., 2008), the compressive summarization system of Berg-Kirkpatrick et al. (2011) (GBK’11), the multi-aspect ILP system of Woodsend and Lapata (2012)(WL’12) and the dual decomposition based system (Almeida and Martins, 2013) (AM’13). Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods. For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two versions for this method: one uses all the constraints (C0-C3), the other does not use C0. We can see that our propos</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008. The ICSI summarization system at tac 2008. In Proceedings of the Text Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew R Gormley</author>
<author>Jason Eisner</author>
</authors>
<title>Nonconvex global optimization for latent-variable models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>444--454</pages>
<contexts>
<context position="17598" citStr="Gormley and Eisner, 2013" startWordPosition="3040" endWordPosition="3043">Then we fix z, q, and update p using projected ∑J j=1 max z,p ∑ k max z,p,q ∑J j=1 + ∑ piai ahmEA 1496 subgradient. That is (pj ∂R pj ew = Po + α (8) ∂pj where α &gt; 0 is the step size in line search, and function P∆(q) denotes the projection of q onto the feasible set ∆ P∆(q) = min IIp − qII2 PE∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.4 Initialize p Using Convex Relaxation Since R is non-concave, searching its maximum using subgradient method suffers from local optimality. Though one can use techniques such as branchand-bound for exact inference (Qian and Liu, 2013; Gormley and Eisner, 2013), here for fast decoding, we use convex relaxation to choose a good seed p(0). Recall that pjk denotes the percentage of the kth occurrence contributing to cj. The larger pjk is, the more likely the kth occurrence is selected. To estimate such likelihood, we replace the binary constraint in extractive summarization (Problem (1)) by 0 &lt; y, v &lt; 1, since solving a relaxed LP is much faster than ILP. Suppose y* is the optimal solution for such a relaxed LP problem, we initialize p by Ek y*nik If for all k, y* = 0, then we initialize pjk using nik uniform distribution 1 pjk = |oj| where |oj |is the</context>
</contexts>
<marker>Gormley, Eisner, 2013</marker>
<rawString>Matthew R. Gormley and Jason Eisner. 2013. Nonconvex global optimization for latent-variable models. In Proceedings of ACL, pages 444–454, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kolmogorov</author>
<author>Ramin Zabih</author>
</authors>
<title>What energy functions can be minimized via graph cuts.</title>
<date>2004</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>26--65</pages>
<contexts>
<context position="12336" citStr="Kolmogorov and Zabih, 2004" startWordPosition="2007" endWordPosition="2011"> score of a concept denotes its informativeness, hence should be non-negative. For example, Li et al. (2013b) proposed to use the estimated normalized frequencies of concepts as scores, which are essentially nonnegative. The basic idea of our method is to approximate the above optimization problem (2) by the supermodular binary quadratic programming (SBQP) problem: max z � �βizi + αijzizj ij i s.t. z is binary (3) where αij &gt; 0. It is known that such a binary quadratic function is supermodular, and its maximum can be solved efficiently using graph maxflow/min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). Now the problem is to find the optimal α, β for a good approximation. 3.1 Formulate Grammar Constraints and Subtree Deletion Model by SBQF We show that the subtree deletion model can be formulated equivalently using SBQF. First, we can eliminate the constraint zh &gt; zm by adding a penalty term to the objective function. That is, max f(z) s.t. zh &gt; zm z is binary is equivalent to max f(z) − 00(1 − zh)zm s.t. z is binary We can see that the penalty term −00(1− zh)zm excludes zh = 0, zm = 1 from the feasible set, and for zh &gt; zm, both problems have the same objective function value. Hence the tw</context>
</contexts>
<marker>Kolmogorov, Zabih, 2004</marker>
<rawString>Vladimir Kolmogorov and Ramin Zabih. 2004. What energy functions can be minimized via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26:65–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="18576" citStr="Koo et al., 2010" startWordPosition="3221" endWordPosition="3224"> a relaxed LP is much faster than ILP. Suppose y* is the optimal solution for such a relaxed LP problem, we initialize p by Ek y*nik If for all k, y* = 0, then we initialize pjk using nik uniform distribution 1 pjk = |oj| where |oj |is the frequency of cj. 3.5 Summary For clarity, we summarize our decoding algorithm in Algorithm 1. Initial Amax can be arbitrarily large. In our experiments, we set Amax = Ej wj, which empirically guarantees the summary length Ei zi &lt; L when A = Amax. The choice of the step size for updating p is similar to the projected subgradient method in dual decomposition (Koo et al., 2010). Algorithm 1 Compressive Summarization via Graph Cuts Require: Scores of concepts {w�} and arcs {wa,—}, max summary length L. Ensure: Compressive summarization z*, where zi indicates whether the ith word is selected. Solve the relaxed LP of Problem (1) (replace the binary constraint by 0 &lt; y, v &lt; 1) to get y. Initialize p(0) using Eq (9). Initialize sufficient large Amax, and Amin = 0 while Amax − Amin &gt; e do Set A = 2(Amin + Amax) Set p = p(0). repeat Fix p, solve Problem (7) to get z using maxflow/min-cut. Update p using Eq (8). until convergence if Ei zi &gt; L then Amin = A else if Ei zi &lt; L</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP 2010, pages 1288–1298, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Yang Liu</author>
</authors>
<title>Document summarization via guided sentence compression.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<note>(to appear),</note>
<contexts>
<context position="8273" citStr="Li et al., 2013" startWordPosition="1299" endWordPosition="1302">0 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts</context>
<context position="11816" citStr="Li et al. (2013" startWordPosition="1918" endWordPosition="1921">tion is linear in the number of words, which is usually thousands on the TAC datasets. Hence solving such a problem using ILP based decoding algorithms is not efficient especially when the document set is large. J max z,v j=1 ziGL 1494 3 Fast Decoding via Graph Cuts In this section, we introduce our fast decoding algorithm. We assume that all the concepts are word n-grams, and their scores are non-negative. The nonnegativity assumption can reduce the computational complexity, but is also reasonable: the score of a concept denotes its informativeness, hence should be non-negative. For example, Li et al. (2013b) proposed to use the estimated normalized frequencies of concepts as scores, which are essentially nonnegative. The basic idea of our method is to approximate the above optimization problem (2) by the supermodular binary quadratic programming (SBQP) problem: max z � �βizi + αijzizj ij i s.t. z is binary (3) where αij &gt; 0. It is known that such a binary quadratic function is supermodular, and its maximum can be solved efficiently using graph maxflow/min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). Now the problem is to find the optimal α, β for a good approximation. 3.1 Form</context>
<context position="19956" citStr="Li et al., 2013" startWordPosition="3472" endWordPosition="3475">its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence containing cj and the topic title. For concepts appearing in multiple sen</context>
</contexts>
<marker>Li, Liu, Weng, Liu, 2013</marker>
<rawString>Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a. Document summarization via guided sentence compression. In Proceedings of EMNLP (to appear), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Using supervised bigram-based ILP for extractive summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1004--1013</pages>
<contexts>
<context position="8273" citStr="Li et al., 2013" startWordPosition="1299" endWordPosition="1302">0 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts</context>
<context position="11816" citStr="Li et al. (2013" startWordPosition="1918" endWordPosition="1921">tion is linear in the number of words, which is usually thousands on the TAC datasets. Hence solving such a problem using ILP based decoding algorithms is not efficient especially when the document set is large. J max z,v j=1 ziGL 1494 3 Fast Decoding via Graph Cuts In this section, we introduce our fast decoding algorithm. We assume that all the concepts are word n-grams, and their scores are non-negative. The nonnegativity assumption can reduce the computational complexity, but is also reasonable: the score of a concept denotes its informativeness, hence should be non-negative. For example, Li et al. (2013b) proposed to use the estimated normalized frequencies of concepts as scores, which are essentially nonnegative. The basic idea of our method is to approximate the above optimization problem (2) by the supermodular binary quadratic programming (SBQP) problem: max z � �βizi + αijzizj ij i s.t. z is binary (3) where αij &gt; 0. It is known that such a binary quadratic function is supermodular, and its maximum can be solved efficiently using graph maxflow/min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). Now the problem is to find the optimal α, β for a good approximation. 3.1 Form</context>
<context position="19956" citStr="Li et al., 2013" startWordPosition="3472" endWordPosition="3475">its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence containing cj and the topic title. For concepts appearing in multiple sen</context>
</contexts>
<marker>Li, Qian, Liu, 2013</marker>
<rawString>Chen Li, Xian Qian, and Yang Liu. 2013b. Using supervised bigram-based ILP for extractive summarization. In Proceedings of ACL, pages 1004–1013, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>510--520</pages>
<contexts>
<context position="2003" citStr="Lin and Bilmes, 2011" startWordPosition="289" endWordPosition="292">ge amounts of texts. Summarization techniques can be roughly divided into two categories: extractive and abstractive. Extractive summarization casts the summarization task as a sentence selection problem: identifying important summary sentences from one or multiple documents. Many methods have been developed in the past decades, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming (Gillick et al., 2008) (ILP) and submodular maximization methods (Lin and Bilmes, 2011). Though extractive summarization is popular because of its simplicity and high readability, it has limitations in that it selects each sentence as a whole, and thus may miss informative partial sentences. To improve the informativeness, joint compression and summarization was proposed (BergKirkpatrick et al., 2011), which uses words as summarization units, unlike extractive summarization where each sentence is a basic undecomposable unit. To achieve better readability, manually defined grammar constraints or automatically learned models based on syntax trees are added during the summarization</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of ACL, pages 510–520, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: Can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>261--264</pages>
<contexts>
<context position="8256" citStr="Liu and Liu, 2009" startWordPosition="1295" endWordPosition="1298">ntences and about 80 concepts per topic on the TAC2009 dataset. Therefore the actual scale of ILP is rather small after pruning (e.g., 176 variables and 372 constraints per topic). Empirical studies showed that such small scale ILP can be solved within a few seconds (Gillick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of </context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: Can it be done by sentence compression? In Proceedings of ACL-IJCNLP 2009, pages 261–264, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="8533" citStr="Martins and Smith, 2009" startWordPosition="1333" endWordPosition="1336">lick and Favre, 2009). 2.2 Compressive Summarization The quality of sentence-based extractive summarization is limited by the informativeness of the original sentences and the summary length constraint. To remove the unimportant part from a long sentence, sentence compression is proposed to generate more informative summaries (Liu and Liu, 2009; Li et al., 2013a). Recent studies show that joint sentence compression and extraction, namely compressive summarization, outperforms pipeline systems that run extractive summarization on the compressed sentences or compress selected summary sentences (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012). In Berg-Kirkpatrick et al. (2011), compressive summarization integrates the concept model for extractive summarization (Gillick and Favre, 2009) and subtree deletion model for sentence compression. The score of a compressive summary consists of two parts, scores of selected concepts, and scores of the broken arcs in the dependency parse trees. The selected words must satisfy the length constraint and grammar constraints that include subtree constraint and some manually defined hard constraints. Formally, let x = x1 ... xI denote the word</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyohito Nagano</author>
<author>Yoshinobu Kawahara</author>
<author>Kazuyuki Aihara</author>
</authors>
<title>Size-constrained submodular minimization through minimum norm base.</title>
<date>2011</date>
<booktitle>In ICML,</booktitle>
<pages>977--984</pages>
<contexts>
<context position="13787" citStr="Nagano et al., 2011" startWordPosition="2270" endWordPosition="2273">ion described above. Note that the fourth constraint has been eliminated by variable substitution, we have �wj - vj + wahmzh(1 − zm) ahmEA �−00 (1 − zh)zm ahmEA zi bj (4) z, v are binary We can see that for each arc ahm, there must be a positive quadratic term +00zhzm in the objective function, which guarantees the supermodularity of the objective function, no matter what wahm is. 3.2 Eliminate Length Constraint Using Lagrangian Relaxation Problem (4) is NP-hard, because for any feasible v, it is a SBQP with a length constraint. Since size constrained minimum cut problem is generally NP-hard (Nagano et al., 2011), Problem (4) can not be cast as a SBQP as long as P =� NP. One popular way to deal with the size constrained optimization problem is Lagrangian relaxation. We introduce Lagrangian multiplier λ to the length constraint in Problem (4), and get �wj - vj + wahmzh(1 − zm) ahmEA �−00 (1 − zh)zm ahmEA +λ(L − � zi) i Us.t. vj = ri zi bj (5) k iEojk λ &gt; 0 z, v are binary We solve the relaxed problem iteratively. In each iteration, we fix λ and solve the inner maximization problem (details described below). The score of J max z,v j=1 Us.t. vj = ri k iEojk � ziGL i J j=1 min A max z,v 1495 each word is </context>
</contexts>
<marker>Nagano, Kawahara, Aihara, 2011</marker>
<rawString>Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuki Aihara. 2011. Size-constrained submodular minimization through minimum norm base. In ICML, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Branch and bound algorithm for dependency parsing with non-local features.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--105</pages>
<contexts>
<context position="15614" citStr="Qian and Liu, 2013" startWordPosition="2626" endWordPosition="2629">a predefined threshold. 3.3 Eliminate v Using Supermodular Relaxation Now we consider the inner maximization Problem (5). It is still not a SBQP, since the objective function is not a linear function of zizj. We propose to approximate the objective function using SBQP. Our solution consists of two steps. First we relax the first constraint of Problem (5) by bounding the objective function with a family of supermodular pseudo boolean functions (Boros and Hammer, 2002). Second we reformulate these pseudo boolean functions equivalently as quadratic functions. Similar to the bounding strategy in (Qian and Liu, 2013), we relax the logical disjunction by linearization. Using the fact that for any binary vector a, we have ∪ ∑ ai = max pE∆ i where ∆ denotes the probability simplex ∆ = tp |∑ pk = 1,pk &gt;&gt;- 0} k We have ∪vj = ∏ zi k iEojk ∑ ∏ = max pjk zi pjE∆ k iEojk Plug the equation above into the objective function of Problem (5), we get the following optimization problem   ∑ ∏ pjkwj zi  k iEojk + ∑ wahmzh(1 − zm) ahmEA ∑−00 (1 − zh)zm ahmEA +λ(L − ∑ zi) i s.t. z is binary (6) pj E ∆ bj Let Q(p, z) denote the objective function of Problem (6). Given p, we can see that Q is a supermodular pseudo boolean </context>
<context position="17571" citStr="Qian and Liu, 2013" startWordPosition="3036" endWordPosition="3039">ng max-flow/mincut. Then we fix z, q, and update p using projected ∑J j=1 max z,p ∑ k max z,p,q ∑J j=1 + ∑ piai ahmEA 1496 subgradient. That is (pj ∂R pj ew = Po + α (8) ∂pj where α &gt; 0 is the step size in line search, and function P∆(q) denotes the projection of q onto the feasible set ∆ P∆(q) = min IIp − qII2 PE∆ which can be solved efficiently by sorting (Duchi et al., 2008). 3.4 Initialize p Using Convex Relaxation Since R is non-concave, searching its maximum using subgradient method suffers from local optimality. Though one can use techniques such as branchand-bound for exact inference (Qian and Liu, 2013; Gormley and Eisner, 2013), here for fast decoding, we use convex relaxation to choose a good seed p(0). Recall that pjk denotes the percentage of the kth occurrence contributing to cj. The larger pjk is, the more likely the kth occurrence is selected. To estimate such likelihood, we replace the binary constraint in extractive summarization (Problem (1)) by 0 &lt; y, v &lt; 1, since solving a relaxed LP is much faster than ILP. Suppose y* is the optimal solution for such a relaxed LP problem, we initialize p by Ek y*nik If for all k, y* = 0, then we initialize pjk using nik uniform distribution 1 p</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Branch and bound algorithm for dependency parsing with non-local features. TACL, 1:105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
</authors>
<title>Experiments in single and multidocument summarization using mead. In</title>
<date>2001</date>
<booktitle>In First Document Understanding Conference.</booktitle>
<contexts>
<context position="19939" citStr="Radev, 2001" startWordPosition="3470" endWordPosition="3471"> concept cj, its score is wj = BT conceptfconcept(cj) where fconcept(cj) is the feature vector of cj, and Bconcept is the corresponding weight vector of feature fconcept(cj). Similarly, score wahm is defined as wahm = Bar T cfarc(ahm) Though our algorithm can handle general word ngram concepts, we restrict the concepts to word bigrams, which have been widely used recently in the sentence-based ILP extractive summarization systems. For a concept cj, we define the following features, some of which have been used in previous work (Brandow et al., 1995; Aker and Gaizauskas, 2009; Edmundson, 1969; Radev, 2001; Li et al., 2013b). All of these features are non-negative. • Term frequency: the frequency of cj in the given topic. pjk = * yni k (9) 1497 • Stop word ratio: ratio of stop words in cj. The value can be {0, 0.5,1}. • Similarity with topic title: the number of common words in these two strings, divided by the length of the longer string. • Document ratio: percentage of documents containing cj. • Sentence ratio: percentage of sentences containing cj. • Sentence-title similarity: word unigram/bigrams cosine similarity between the sentence containing cj and the topic title. For concepts appearin</context>
</contexts>
<marker>Radev, 2001</marker>
<rawString>Dragomir R. Radev. 2001. Experiments in single and multidocument summarization using mead. In In First Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>233--243</pages>
<contexts>
<context position="28649" citStr="Woodsend and Lapata (2012)" startWordPosition="4916" endWordPosition="4920">s for various systems on the TAC2008 data set. We show both the summarization performance and the speed2 of the system. The presented systems include our graphcut based method, the ILP based compression and summarization, and the sentence-based extractive summarization. ILP 2-step refers to the 2-step fast decoding strategy proposed by (Berg-Kirkpatrick et al., 2011). We also list the performance of some state-of-theart systems, including the two ICSI systems (Gillick et al., 2008), the compressive summarization system of Berg-Kirkpatrick et al. (2011) (GBK’11), the multi-aspect ILP system of Woodsend and Lapata (2012)(WL’12) and the dual decomposition based system (Almeida and Martins, 2013) (AM’13). Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods. For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two versions for this method: one uses all the constraints (C0-C3), the other does not use C0. We can see that our proposed method balanced speed and quality. Compared with ILP, we achieved competitive ROUGE scores, but with about 100x speedup. Our method is al</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of EMNLP-CoNLL, pages 233–243, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
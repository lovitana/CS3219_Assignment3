<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9956165">
Improving Alignment of System Combination by Using
Multi-objective Optimization
</title>
<author confidence="0.999565">
Tian Xia+, Zongcheng Ji*, Shaodan Zhai+, Yidong Chen++, Qun Liu*, Shaojun Wang+
</author>
<affiliation confidence="0.993614666666667">
++ Xiamen University, Xiamen 361005, P.R. China
+ Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA
* Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<address confidence="0.768441">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.909884">
{jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn
{xia.7, zhai.6, shaojun.wang}@wright.edu
</email>
<sectionHeader confidence="0.99656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999583206896552">
This paper proposes a multi-objective opti-
mization framework which supports heteroge-
neous information sources to improve align-
ment in machine translation system combi-
nation techniques. In this area, most of
techniques usually utilize confusion networks
(CN) as their central data structure to com-
pact an exponential number of an potential hy-
potheses, and because better hypothesis align-
ment may benefit constructing better quality
confusion networks, it is natural to add more
useful information to improve alignment re-
sults. However, these information may be het-
erogeneous, so the widely-used Viterbi algo-
rithm for searching the best alignment may
not apply here. In the multi-objective opti-
mization framework, each information source
is viewed as an independent objective, and
a new goal of improving all objectives can
be searched by mature algorithms. The so-
lutions from this framework, termed Pareto
optimal solutions, are then combined to con-
struct confusion networks. Experiments on
two Chinese-to-English translation datasets
show significant improvements, 0.97 and 1.06
BLEU points over a strong Indirected Hidden
Markov Model-based (IHMM) system, and
4.75 and 3.53 points over the best single ma-
chine translation systems.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984554947368421">
System combination (SC) techniques have the
power of boosting translation quality in BLEU by
several percent over the best among all input ma-
chine translation systems (Bangalore et al., 2001;
Matusov et al., 2006; Sim et al., 2007; Rosti et al.,
2007b; Rosti et al., 2007a; Huang and Papineni,
2007; He et al., 2008; Rosti et al., 2008; He and
Toutanova, 2009; Li et al., 2009; Feng et al., 2009;
Pauls et al., 2009). A central data structure in the
SC is the confusion network, and its quality greatly
affects the final performance. He et al. (2008) pro-
posed a new hypothesis alignment algorithm for
constructing high-quality confusion networks called
Indirect Hidden Markov Model (IHMM), which
does better in synonym matching compared with
the classic translation edit rate (TER) based algo-
rithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et
al., 2007). Now, current state-of-the-art SC systems
have been using IHMM or variants in their align-
ment algorithms more or less (Li et al., 2009; Feng
et al., 2009).
Our motivation derives from an observation that
in an ideal alignment of a pair of sentences, many-to-
many alignments often exist. For instance, “be about
to” has the same meaning with “be on the point
of”. Because Hidden Markov Model based align-
ment algorithms, e.g. IHMM for system combina-
tion, HMM in GIZA++ software for statistical ma-
chine translation (SMT) (Och and Ney, 2000; Koehn
et al., 2003), are designed for one-to-many align-
ment, and running GIZA++ from two directions to
gain better performance turns into a standard opera-
tion in SMT, therefore we are seeking a way to em-
power IHMM by introducing bi-directional informa-
tion.
However, it appears to be intractable in an IHMM
model to search the optimal solution by simply
defining a new goal as a product of probabilities
</bodyText>
<page confidence="0.976833">
535
</page>
<note confidence="0.733192">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999981815384616">
from two directions. To bypass this problem, Liang
et al. (2006) adopts a simple and effective variational
inference algorithm.
Further, different alignment algorithms capture
different information and linguistic phenomena for
a pair of sentences, hence more information would
be expected to benefit the final alignment. Liang’s
method may not be suitable for this expected out-
come.
We propose to adopt multi-objective optimiza-
tion framework to support heterogeneous informa-
tion sources which may induce difficulties in a
conventional search algorithm. In this framework,
there exist a variety of matured multi-objective op-
timization algorithms, e.g. evolutionary algorithm
(Deb et al., 2000; Deb et al., 2002), Tabu search
(Hansen, 1997), ants colony (Engelbrecht, 2005),
and simulated annealing (Serafini, 1994). In this
work, we select the multi-objective evolutionary al-
gorithm because of its public open source software
(http://www.iitk.ac.in/kangal/codes.shtml). On the
other hand, this framework is also totally unsuper-
vised. It prevents weights of a linearly combined
goal from training even if all information is homoge-
neous and applicable in a Viterbi search (Forney Jr,
1973). This framework views any useful informa-
tion benefiting alignment as an independent objec-
tive, and researchers just need to write short codes
for objective definitions. The search algorithm seeks
for potentially better solutions which are no worse
than the current solution set. The output from multi-
objective optimization algorithms includes a set of
solutions, called Pareto optimal solutions, each one
being a many-to-many alignment. We then com-
bine and normalize them into a unique one-to-one
alignment to perform confusion network construc-
tion (Section 3.3).
Our work is conducted on the classic pipeline
which has three modules, pair-wise hypothesis
alignment, confusion network construction, and
training. Now many work integrates neighboring
modules to avoid propagated errors to gain improved
performance. For example, Rosti et al. (2008), and
Li et al. (2009) combine the first and the second
module, and He and Toutanova (2009) combine all
modules into one directly. Nevertheless, the classic
structure also owns its merits. Because of the in-
dependence between modules, a system is relatively
simple to maintain, and improvements on each mod-
ule might contribute to final performance additively.
Based on our work, lattice-based minimum error
rate training (lattice-MERT) and minimum bayes
risk training techniques (Kumar et al., 2009) could
be adopted on the third module. And Feng et al.
(2009) in the second module adopts a different data
structure called lattice which could directly use our
better many-to-many alignment for construction.
Experiments on the Chinese-to-English task on
two datasets use four objectives, IHMM probabil-
ity (Section 3.2.1), and alignment probability from
GIZA++ (Section 3.2.2) from two directions. Re-
sults show multi-objective optimization framework
efficiently integrates different information to gain
approximately 1 BLEU point improvement over a
strong baseline.
</bodyText>
<sectionHeader confidence="0.985607" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999542">
We briefly give an introduction to confusion net-
works, and because the IHMM based alignment is
an important objective in our multi-objective frame-
work, here we also provide detailed definition of for-
mulas for completeness of content.
</bodyText>
<subsectionHeader confidence="0.967321">
2.1 Confusion Network
</subsectionHeader>
<bodyText confidence="0.875810142857143">
Table 1 shows hypotheses h1 and h2 are aligned to
selected backbone h0. When alignment algorithm
obtains good enough results, the expected output
“he prefers apples” is included in its corresponding
confusion network in Figure 1. This suggests de-
veloping better alignment algorithm may help creat-
ing high-quality confusion networks. This also mo-
tivates us to use the BLEU of oracle hypotheses to
approximately measure the quality of a set of CNs.
We hereafter call it an oracle BLEU of a CN. See
more in Section 5.1.
h0 :he feels like apples
h1 :he prefer a apples
h2 :him prefers to apples
</bodyText>
<tableCaption confidence="0.765852">
Table 1: A toy example of hypothesis alignment, where
</tableCaption>
<footnote confidence="0.575131">
h0 is the backbone hypothesis. h�and h2 are aligned to
the backbone separately. The resulting confusion net-
work is in Figure 1.
</footnote>
<bodyText confidence="0.93187">
A confusion network G = (V, E) is a directed
acyclic graph with a unique source and sink vertex,
</bodyText>
<page confidence="0.996508">
536
</page>
<figure confidence="0.637135">
feel like
</figure>
<figureCaption confidence="0.9857245">
Figure 1: A classic confusion network, and the bold path
the expected output.
</figureCaption>
<bodyText confidence="0.9997292">
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
Each edge is restricted to attach to a single word as
well as an associated probability. A special mark ε
is a place-holder denoting no word here.
</bodyText>
<subsectionHeader confidence="0.948637">
2.2 IHMM-based Alignment
</subsectionHeader>
<bodyText confidence="0.999764538461538">
Indirected Hidden Markov Model (IHMM) was
firstly proposed by He et. al (2008). Compared with
TER-based alignment performing literal matching,
IHMM supports synonym comparison in redefining
emission probabilities in an IHMM model.
Let fI = (f1, ... fI) be a backbone hypothesis,
and eJ = (e1,... eJ) be a hypothesis aligned to the
backbone, both being English sentences in our ex-
periments. Let aJ = {a1,... aj} be an alignment.
Suppose the ajth word in fI is aligned to jth word
in eJ, and the conditional probability that the hy-
pothesis is generated by the backbone, shown in the
upper graph of Figure 3, is given by
</bodyText>
<equation confidence="0.995839">
J
p(fI, eJ) = � H {pt(aj|aj−1, I)po(ej|faj)}
ai =1
(1)
</equation>
<bodyText confidence="0.967365333333333">
The distortion probability pt(aj|aj−1, I) from po-
sition aj−1 to aj, relies on jumped distance, which
is computed as follows:
</bodyText>
<equation confidence="0.9982075">
pt(i,|i, I) =Et (1 c (i) ) (2)
t − i
</equation>
<bodyText confidence="0.990036666666667">
The distortion parameters c(d) are grouped into
11 buckets, c(≤ −4),c(−3),c(−2)...c(5),c(≥ 6).
Because all the hypotheses in system combina-
tion are in the same language, the IHMM model
would support more monotonic alignments, and
non-monotonic alignments will be penalized.
</bodyText>
<equation confidence="0.999534">
c(d) = (1 + |d − 1|)−K, d = −4 ... 6 (3)
</equation>
<bodyText confidence="0.9905455">
where K is tuned on held-out data.
Let p0 be the probability of jumping to a null
word state, which is also tuned on held-out data, and
the accurate transition probability becomes:
</bodyText>
<equation confidence="0.9773398">
�
p0 if i, = null
pt(i,|i, I) =
(1 − p0)pt(i,|i, I) otherwise
(4)
</equation>
<bodyText confidence="0.9832748">
The output probability po(e|f) from the state
word f to the observation word e, also called trans-
lation probability, is a linear interpolation of se-
mantic similarity psem(e|f) and surface similarity
psur(e|f), and α is the interpolation factor:
</bodyText>
<equation confidence="0.999644">
po(e|f) = αpsem(e|f) + (1 − α)psur(e|f) (5)
</equation>
<bodyText confidence="0.534375666666667">
When calculating semantic similarity psem(e|f),
source sentence src is needed, and a bilingual prob-
abilistic dictionary pdic(w1|w2) is necessary.
</bodyText>
<equation confidence="0.974627">
�psem(e|f) ≈ pdic(c|f) · pdic(e|c) (6)
cEsrc
</equation>
<bodyText confidence="0.9999145">
Note that psem(e|f) has been updated with differ-
ent source sentences.
The surface similarity psur(e|f) is measured by
the literal matching rate:
</bodyText>
<equation confidence="0.779475">
psur(e, f) = exp{ρ[ LMP(f, e)
max(|f|, |e|) 1]} (7)
</equation>
<bodyText confidence="0.9998505">
where LMP(f, e) is the length of the longest
matched prefix, and ρ is a smoothing parameter.
</bodyText>
<sectionHeader confidence="0.997588" genericHeader="method">
3 Multi-objective Optimization
</sectionHeader>
<bodyText confidence="0.9999619">
Many decision making problems in the real world
consider more than one objective. One natural way
is to scalarize multiple objectives into one by assign-
ing it with a weight vector. This method allows a
simple optimization algorithm in many cases, while
in system combination, it would cause problems.
In the first module, in order to train suitable
weights of objectives, extra labeled data is needed,
besides that, the efficient Viterbi algorithm for
searching the optimal alignment would not work for
</bodyText>
<figure confidence="0.949623071428571">
N he It prefer 11 e n
him
prefers
like
7
apples
537
5
4
3
2
1
Y: Direct IHMM Probability (1e-8)
0
</figure>
<bodyText confidence="0.999799090909091">
the alignment objectives in this work. More, the pa-
rameter training in the third module relies on the
CNs constructed from the output of the first mod-
ule, which increases the instability of the whole sys-
tem. Therefore, an unsupervised multi-objective al-
gorithm may be a good choice allowing for more
alignment information.
There exist other alternative optimization algo-
rithms in the multi-objective optimization frame-
work, though the evolutionary algorithm is adopted
here, we only introduce some general concepts.
</bodyText>
<subsectionHeader confidence="0.99913">
3.1 Pareto Optimal Solutions
</subsectionHeader>
<bodyText confidence="0.9984935">
A general multi-objective optimization problem
consists of a number of objectives and is associated
with a number of constraints. Mathematically, the
problem can be written as follows (Deb, 2001)
</bodyText>
<equation confidence="0.996761666666667">
Maximize fz(x) i = 1... M
s.t. gj(x) G 0 j = 1... N
hk(x) = 0 k = 1... K
</equation>
<bodyText confidence="0.999956571428572">
where x denotes a potential solution, its structure re-
lying on different problems, and the number of con-
straints M, N, K depend on different problems. All
the functions fz, gj, hk map a solution x into a scalar.
We will explain them in terms of system combina-
tion.
In this work, we refer to x = {xz,jixz,j E {0,1}}
as a potential alignment of a pair of hypotheses,
where xz,j is a boolean value to denote whether the
ith word in the first hypothesis is aligned to the jth
word in the second hypothesis. Here the definition of
x seems different from that of a in Formula 1, and
they could convert to each other. Using a line-based
access style, a matrix can be unfolded as a vector.
We refer to f as IHMM alignment probability (He et
al., 2008) and GIZA++ alignment probability (Chen
et al., 2009), total four objectives from two direc-
tions, and the larger the objectives, the better. The
gjs and hks serve as the role of checking if x repre-
sents a legal alignment. For instance, the subscripts
of xz,j are not in bounds.
</bodyText>
<construct confidence="0.954181333333333">
Definition 1. Let x, x&apos; be two potential align-
ments. If fz(x) &gt; fz(x&apos;) holds for all i, we call
the alignment x dominates the alignment x&apos;. If there
</construct>
<figure confidence="0.898408">
0 1 2 3 4 5
X: Reversed THMM Probability (1e-8)
</figure>
<figureCaption confidence="0.89172">
Figure 2: Sample solutions with only two objectives.
Pareto Optimal Solutions p1, p3, p5, p7. Other points
p2, p4, p� are dominated by at least one point in the Pareto
optimal solutions.
</figureCaption>
<construct confidence="0.9420926">
does not exist any alignment x00 to dominate x, we
call the alignment x to be non-dominated.
Definition 2. A alignment x is said to be Pareto
optimal if there is no other alignment x&apos; found to
dominate x.
</construct>
<bodyText confidence="0.999863363636364">
In Figure 2, p1 dominates p2, and p2 dominates
p4. To summarize, a point is dominated by the ones
on its upper and right side with ties. In this example,
p1, p3, p5, p7 are Pareto optimal.
In some cases, Pareto optimal solutions can be
used for good candidate solutions. Considering
the IHMM model, maximizing Y axis, the top-4
best alignments are p1, p2, p3, p4. But from the
view of Pareto optimal, the top-4 alignments would
be p1, p3, p5, p7 without order, which considers a
greater range than a single optimization model. In
our method, we just combine these Pareto optimal
solutions equally into a unique alignment (Section
3.3).
Our adopted multi-objective optimization search-
ing algorithm is the non-dominated sorting ge-
netic algorithm II (NSGA-II) (Deb et al., 2000;
Deb et al., 2002) with an open source software
(http://www.iitk.ac.in/kangal/codes.shtml). NSGA-
II has a complexity of O(mn2), where m is the num-
ber of objectives and n is the population size in an
evolutionary algorithm.
</bodyText>
<subsectionHeader confidence="0.997669">
3.2 Objectives in Evolutionary Algorithm
</subsectionHeader>
<bodyText confidence="0.998594">
The optimization objectives in our experiments can
be categorized as an IHMM alignment probability
(He et al., 2008) and GIZA++ alignment probability
</bodyText>
<figure confidence="0.940398846153846">
⊕
p2
⊕
p4
K
p1
p3
⊕
p6
�
p5
a
p7
</figure>
<page confidence="0.913599">
538
</page>
<equation confidence="0.99301275">
f1 f2 f3
� � �
� � �
e1 e2 e3
</equation>
<figureCaption confidence="0.945563">
Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in
two IHMM models. The upper one is a typical example
in IHMM, and in the bottom one, because any word in the
observation is required not to correspond to two statuses,
it has a minor trouble. S: status sequence, O: observation
sequence.
(Chen et al., 2009), total four from two directions.
</figureCaption>
<subsubsectionHeader confidence="0.756563">
3.2.1 IHMM Probability
</subsubsectionHeader>
<bodyText confidence="0.989758739130435">
A typical IHMM alignment is demonstrated
in the upper graph of Figure 3, where a
backbone is acting the role of a status se-
quence. The unnormalized conditional align-
ment probability is [pt(1|null)] · [pt(1|1)pt(2|1)] ·
[po(e1|f1)po(e2|f1)po(e3|f2)]. However, the same
alignment (f1, e1)(f1, e2)(f2, e3), if we change the
alignment direction, the backbone being observa-
tions, would be a bit different. We offer a minor
modification to Formula 1.
Look at the bottom graph of Figure 3, the obser-
vation f1 has two statuses, e1 and e2 at the same
time, it becomes ambiguous to compute the tran-
sitional probability between pt(3|1) and pt(3|2).
This is because IHMM algorithm deals with one-
to-many alignments, and MOEA permits many-to-
many alignments.
We hence empirically modify the IHMM model
to support many-to-many alignments. A new status
is defined, rather than a single position pt(j|i), but
as a set of positions pt({j}|{i}). The positions in
one status need not to be adjacent to each other.
The redefined transitional probability
</bodyText>
<equation confidence="0.9020508">
pt({j} |{i}) = |{j}|1|{i} |�pt(j|i)
i,j
The redefined emission probability
po(j|{i}) = ri po(j|i)
i
</equation>
<bodyText confidence="0.9989754">
We need to note that there is no guarantee on
the closed property of probabilities, though these
approximations prove to be effective in a practical
sense. Straightforwardly, when there is only one po-
sition in a new status, the expanded IHMM degener-
ates to the standard IHMM.
Let us return to the second IHMM ex-
ample. The new probability becomes
[pt(1|null)pt(2|null)]·[12pt(3|1)pt(3|2)·pt(null|3)]·
[po(f1|e1)po(f1|e2)po(f2|e3)po(f3|null)].
</bodyText>
<subsectionHeader confidence="0.531082">
3.2.2 Alignment Probability
</subsectionHeader>
<bodyText confidence="0.999203571428571">
GIZA++ considers very different and more in-
formation in alignment, we attempt to utilize them.
All probabilities appearing in below formulas can be
looked up in GIZA++.
Given a pair of hypotheses fI = (f1,... fI),
eJ = (e1,... eJ), and their alignment a, the align-
ment probability could be calculated as follows
</bodyText>
<equation confidence="0.9976225">
ripGiza(eJ|fI, a) = T (ei|fI, a)
ei
T(ei|f I, a) = n(φi|ei) �j,i��at(ei|fj)a(j|i)/φi if φi =60
�n(0|ei)t(ei|null)a(0|i)
otherwise
φi = |{j|(i, j) ∈ a}|
</equation>
<bodyText confidence="0.999792894736842">
where φi is the fertility number, t(e|c) the transla-
tion probability for the word pair, z(j|i) alignment
probability to show how likely a target word at posi-
tion i could be translated into a source word at posi-
tion j, and n(φ|e) is the fertility probability to show
how likely a given target word e is translated into φ
source words.
In order to increase the coverage of words, we col-
lect all the hypothesis pairs in both the tuning set
and the test set and feed them into GIZA++. This
is an off-line operation, which makes it not suitable
for an online translation system. In some circum-
stances, users submit a pile of documents in the hope
of high-quality translations, thus more useful knowl-
edge sources would be helpful. In our experiments,
a pure GIZA++ based system combination does not
perform as well as IHMM based, but does benefit
the final translation quality if combined in our multi-
objective optimization framework.
</bodyText>
<figure confidence="0.5093602">
S:
O:
S:
O:
e1 e2 e3
� � �
� � �
f1 f2 f3
Backbone
Backbone
</figure>
<page confidence="0.968189">
539
</page>
<subsectionHeader confidence="0.9887975">
3.3 Configuration of Evolutionary Algorithm
3.3.1 Encoding
</subsectionHeader>
<bodyText confidence="0.9868164">
Given a sentence pair &lt;fI, eJ&gt;, we define a two-
dimensional matrix x = {zij|zi� ∈ {0, 1}} to en-
code a set of possible alignments. Using a line-based
access style, the matrix could be unfolded as a vector
with |I |· |J |bits of length.
</bodyText>
<subsectionHeader confidence="0.622172">
3.3.2 Initialization
</subsectionHeader>
<bodyText confidence="0.99988575">
Because in NSGA-II software the initial popu-
lation are generated at random. In order to make
NSGA-II more consistent and flexible, better initial
seeds should be fed with, thus we combine an ex-
isting word alignment results as input. Here we use
together two N-best lists generated from directional
HMM and reversed HMM respectively for initializa-
tion.
</bodyText>
<subsectionHeader confidence="0.8681815">
3.3.3 Normalization of Pareto Optimal
Solutions
</subsectionHeader>
<bodyText confidence="0.999985869565217">
Multi-objective optimization algorithms do not
pose weights on objectives, thus they output a set
of so-called Pareto optimal solutions, each of which
is a many-to-many alignment. We can understand
them as an N-best alignment list without explicit
preferences. We also empirically compare it with the
idea that directly cuts an N-best list from the IHMM
based alignment.
We describe a two-stage strategy for normaliza-
tion. Firstly, we use a simple and effective voting
strategy to combine a set of many-to-many align-
ments into a single many-to-many alignment, and
Secondly we normalize it into a one-to-one align-
ment for confusion network construction. In the first
stage, we count the number of word-to-word align-
ments on each position pair (i, j). If there is more
than a half number of alignments, then we output 1,
otherwise 0. In the second stage, if any word relates
to more than one word alignment, the one with the
highest posterior probability is selected (He et al.,
2008; Feng et al., 2009). The posterior probabili-
ties can be computed in a classic forward-backward
procedure in IHMM (He et al., 2008).
</bodyText>
<sectionHeader confidence="0.982778" genericHeader="method">
4 Training and Decoding
</sectionHeader>
<bodyText confidence="0.953450933333333">
Our work does not change the classic pipeline, thus
the model and features are nearly identical to the
ones in (Rosti et al., 2007b; He et al., 2008), which
are modeled in a log-linear fashion in Eq. 8. Trans-
lation on a CN is just a concatenation of edges tra-
versed, on which 4 categories of features are defined.
1. word posterior probabilities. In Eq. 8,
p(w|sys, span) are word confidence scores. If
the word w comes from the kth hypothesis of
thesys-th system, the raw score should be 1
k+1,
and then it would be normalized by the same
sys and span. The same word coming from
different systems owns a different score, so
there are sys system weights Asys.
</bodyText>
<listItem confidence="0.996082">
2. logarithm of language model score, L(h).
3. number of null edge, Numnnii.
4. number of words, Num,,,.
</listItem>
<equation confidence="0.940889666666667">
log(h) = Espan log(Esys Asysp(w|sys, span))
+ w0L(h) + w1Numnnii + w2Num,,,
(8)
</equation>
<bodyText confidence="0.9809534">
Decoding a confusion network is straightforward,
traversing each node from left to right, and the beam
search algorithm will retain for each node an N-
best list. The final N-best can be acquired following
(Huang and Chiang, 2005).
The training process follows minimum error rate
training (MERT) described in (Och, 2003; Koehn et
al., 2003). In each iteration, the Powell algorithm
would attempt to predict the optimal parameters on
the cumulative N-best list.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999951714285714">
We evaluate our method in two datasets in the
Chinese-to-English task. In the first one, NIST MT
2002 and 2005 are used for tuning and testing re-
spectively, and in the second, the newswire part of
MT 2006 and 2008 are for tuning and testing. A 5-
gram language model is trained on the Xinhua por-
tion of the Gigaword corpus. We report the case-
sensitive NIST-BLEU score.
Four single machine translation systems partici-
pating in the system combination consist of a BTG-
based system using a Max-Entropy based reordering
model, a hierarchical phrase-based system, a Moses
decoder and a syntax-based system. 10-best unique
hypotheses from a single system on the development
</bodyText>
<page confidence="0.993132">
540
</page>
<note confidence="0.542566">
SYSTEM MT 2005 MT 2008(news)
</note>
<table confidence="0.999501727272727">
best single 0.3207 0.3016
IHMM* 0.3585(+3.78%) 0.3263(+2.47%)
IncIHMM 0.3639(+4.32%) 0.3320(+3.04%)
0.3438( +2.31 %) 0.3166( +1.50 %)
0.3619( +4.10 %) 0.3306( +2.90 %)
0.3590( +3.83 %) 0.3270( +2.54 %)
0.3604 0.3284
0.3610 0.3290
0.3609 0.3289
0.3630 *(+4.27%) 0.3320 *(+3.04%)
0.3682 **(+4.75%) 0.3369 **(+3.53%)
</table>
<tableCaption confidence="0.420959777777778">
Table 2: PPBD is a posterior probabilistic-based decod-
ing (section 5.3). N-best IHMM simulates the Pareto op-
timal solutions in our method (section 5.3). The last five
systems adopt different objective combinations. The im-
provement percents in parentheses are compared to the
best single. dH: directed IHMM, rH: reversed IHMM,
dT: directed translation probability, rT: reversed transla-
tion probability. ** significance at 0.01 level, and * sig-
nificance at 0.05 level over the IHMM model.
</tableCaption>
<bodyText confidence="0.999557365853659">
and test sets are collected as the input of the system
combination.
Our baseline systems are described as follows.
Two main baseline systems are IHMM based and in-
cremental IHMM (Li et al., 2009). The first system
differs from our method just in hypothesis alignment
algorithm, and the second combines the first and sec-
ond module of the system combination pipeline.
Because our method utilizes bidirectional infor-
mation, we also provide another two alternative
systems for comparison, which are GIZA++ based
alignment and the posterior probability based align-
ment (Liang et al., 2006). Finally, we also provide
an N-best alignment IHMM system, which com-
bines an N-best alignment list to simulate the Pareto
optimal solutions in our method.
The method that linearly combines all objectives
is not listed as our baseline like (Duh et al., 2012)
does, because their algorithm finds the best weighted
solution in a fixed and small solution set, while
in our problem, the solution space is a trellis-style
structure consisting of an exponential number of so-
lutions, and no efficient algorithms apply here.
The IHMM based alignment utilizes typical set-
tings (He et al., 2008; Feng et al., 2009). The
smoothing factor for the surface similarity model,
and p = 3 the controlling factor for the distor-
tion model, K = 2. The bilingual probabilistic
dictionary is trained in the FBIS corpus which in-
cludes about 230k parallel sentence pairs. GIZA++
based system is to run GIZA++ from two directions
to align all the hypotheses, and make the intersec-
tion using grow-diag-final heuristics (Koehn et al.,
2003). The many-to-many alignments are normal-
ized with the same method with ours. Our system
employs NSGA-II software to realize the MOEA al-
gorithm. The main parameters, generation number,
cross probability and mutation probability, and pop-
ulation size, are empirically set as 100, 0.9, 0.001
and 40, and we examine the influence of difference
populations sizes in the full system combination.
</bodyText>
<subsectionHeader confidence="0.995084">
5.1 The Quality of Confusion Networks
</subsectionHeader>
<bodyText confidence="0.9998114">
This experiment shows the relationship between hy-
pothesis alignment and confusion network. Intu-
itively, we expect a better hypothesis alignment
would reduce the error in constructing confusion
networks, and then improve the final translation
quality.
We first use the alignment error rate (AER) (Och
and Ney, 2000), which is widely used to measure
the quality of hypothesis alignment. The smaller,
the better. For convenience, we only examine exact
literal matching. IHMM based alignment reaches
around 0.15 in AER, and our method 0.145.
As the AER may not vividly reflect the relations
between alignment and the final BLEU of systems,
and the quality of confusion network is hard to mea-
sure directly, we assume that the quality of confu-
sion networks could be measured by the oracle hy-
potheses that could be generated from them. We test
the BLEU of the oracle hypotheses.
From this angle, we demonstrate several oracle
BLEU of CNs generated from some conventional
alignment algorithms. The results are shown in Ta-
ble 3.
We find the confusion network from IHMM based
alignment (He et al., 2008) is better than that from
TER based alignment (Rosti et al., 2007b) by about
1 point in both two datasets. These quantities agree
with the final improvements in the BLEU score in
(He et al., 2008). As confusion networks from
MOEA based alignment also show superiority over
</bodyText>
<figure confidence="0.993136875">
GIZA++
PPBD
N-best IHMM
dH+rH
dH+dT
dH+rH+dT
dH+rH+rT
dH+rH+dT+rT
</figure>
<page confidence="0.91116">
541
</page>
<table confidence="0.890285166666667">
alignment MT02 MT05
GIZA++ 0.5690 0.5228
TER 0.5720 0.5270
IHMM 0.5883 0.5382
IncIHMM 0.5931 0.5453
MOEA 0.6017 0.5526
</table>
<tableCaption confidence="0.988868">
Table 3: Oracle BLEUs of CNs. GIZA++: invoking
GIZA++ software. TER: minimum translation edit rate.
IHMM: indirect hidden markov model. IncIHMM: in-
cremental indirect hidden markov model. MOEA: multi-
objective evolution algorithm.
</tableCaption>
<bodyText confidence="0.99276875">
that from IHMM based in the oracle BLEU, we ex-
pect our final translation quality would be improved.
In Table 3, GIZA++ and TER perform simi-
larly, because the former is more capable of tackling
many-to-many alignments over the latter, while lat-
ter based might obtain relatively more precise align-
ment information. Both of the two do not consider
synonym matching compared to IHMM.
Our method and IncIHMM overpass IHMM on
this metric due to different strategies. Obtaining bet-
ter hypothesis alignment or better construction of
confusion networks benefit the quality of CNs.
</bodyText>
<subsectionHeader confidence="0.998986">
5.2 Different Objective Combinations
</subsectionHeader>
<bodyText confidence="0.998256125">
As our framework is convenient to support different
alignment information, we test the influence of dif-
ferent objective combinations to the final translation
quality. We adopt four objectives to depict the can-
didate alignment, directed IHMM probability (dH),
reversed IHMM probability (rH), directed alignment
probability (dT), and reversed alignment probability
(rT). Table 2 demonstrates all the results.
We can see that the IHMM based system out-
performs the GIZA++ based system by about 1-1.5
points in BLEU, which agrees with the difference of
oracle BLEU in Table 1. From (He et al., 2008), the
IHMM based system outperforms the TER based by
1 point, which also agrees with our results in Table
1. Our system, using dH + rH + dT + rT, improves
BLEU score by about 1 points over the IHMM based
system. This comparison verifies our assumption,
improving the quality of the confusion network does
improve system performance.
The different feature combinations exhibit inter-
esting results. The system with dH + rH + dT is
0.05 point better than the system with dH + rH, and
the system dH + rH + rT is 0.3 point better than sys-
tem with dH + rH, so the contributions of feature
dT and rT are 0.05 and 0.3 respectively. While the
two features are used together in the fourth system,
the contribution is about 0.8 point, rather than 0.35.
This phenomenon also proves the correlations be-
tween different features.
Our method explores a way to integrate GIZA++
and IHMM, and is supportive of useful features.
Compared to the classic and powerful IHMM based
system, we obtained an improvement of 0.97 points
on MT 05 and 1.06 points on news of MT 2008,
and equivalently over the best single system by 4.75
points and 3.53 points respectively. More, compared
with the incremental IHMM, our system also shows
moderate improvement, though not much. We hope
these two ideas could be effectively combined in the
future work.
</bodyText>
<subsectionHeader confidence="0.9962805">
5.3 Comparison with Other Bi-directional
Alignment Methods
</subsectionHeader>
<bodyText confidence="0.999841653846154">
Our method introduces multiple alignment infor-
mation into system combination to obtain improve-
ments, thus it would be interesting to explore other
alternative methods for utilizing this information.
We provide three alternative methods similar to our
motivations, and they fall into two categories.
The first category is from the angle of bi-
directional alignment. We use GiZA++ alignment
and the posterior probability decoding-based align-
ment for comparison. The basic idea for the lat-
ter is setting a word-to-word alignment xij as 1,
if its approximate posterior marginal probability
q(xi�j, x) = pd(xiJ|x, Bd) - pr(xij|x, Br) is greater
than a threshold 6, where pd and pr are posterior
marginal probabilities from directed and reversed
IHMM models, which could be conveniently com-
puted with a forward-backward algorithm, and the 6
is tuned on a validation-set optimized data. We just
list some 6 values to examine its best performance
shown in Table 4.
The second class is because our method combines
the Pareto optimal solutions that consist of several
candidate alignments, thus for fairness we also use
a 100-best outputs from the directed IHMM model
and conduct the same normalization technique.
The general results are shown in Table 2. We can
</bodyText>
<page confidence="0.989113">
542
</page>
<table confidence="0.871656333333333">
6 MT 2005 MT 2008
IHMM 0.3585 0.3263
0.15 0.3556 0.3391
0.2 0.3619 0.3306
0.25 0.3575 0.3278
0.3 0.3608 0.3259
</table>
<tableCaption confidence="0.980930666666667">
Table 4: Posterior decoding. When threshold S are set
to suitable values, simple bi-directional alignment could
overpass the baseline.
</tableCaption>
<bodyText confidence="0.999920235294118">
see that, GIZA++ leads to the worst performance,
which can be explained as GIZA++ does not support
synonym matching like IHMM. The N-best IHMM
has a minor improvement over the IHMM method.
We found differences in the N-best list are not obvi-
ous enough. In comparison, the posterior decoding
method brings relatively significant improvements
on both datasets. However, the threshold 6 must
be selected suitably. Table 4 lists the ideal results,
which will be hampered when tuning on a validation
set.
All of the three candidate methods can not conve-
niently support extra alignment information, and a
linear model poses restrictions on features to get an
efficient decoding, the multi-objective optimization
may be a good selection as an inference algorithm in
many circumstances.
</bodyText>
<subsectionHeader confidence="0.995596">
5.4 Population Size
</subsectionHeader>
<bodyText confidence="0.9999265">
We test the influence of final translation quality and
time consumed by different population size.
</bodyText>
<table confidence="0.6197156">
population BLEU
size MT 2005
20 0.3597
40 0.3682
60 0.3655
</table>
<tableCaption confidence="0.947293">
Table 5: Big population size consumes more CPU time.
</tableCaption>
<bodyText confidence="0.9553951">
In our experiments, we use a multi-thread technique to
speed up the alignment, and choose 40 as the parameter
to leverage the time and BLEU.
We expect enlarging the population size would
improve the translation quality, but the BLEU in
population size set as 60 does not overpass when set
as 40. We conjecture that, in our code, if the N-best
size from IHMM (we set as 50-best) does not reach
the population size, we would use randomly gener-
ated seeds, which may hamper the performance of
MOEA. We also tried a larger population in MOEA,
but did not receive obvious improvement on perfor-
mance.
We exerted a hard restriction on the genes in evo-
lutionary algorithm, that is many-to-many discon-
tiguous alignment is forbidden. This trick speeds up
running by about 20 times, and does not harm sys-
tem performance. Now our method runs about 0.9
seconds to align a pair of hypotheses. In practice,
we utilize multi-thread to speed up.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969588235294">
In this paper, we explore a multi-objective frame-
work to conveniently support more useful alignment
objectives to improve the hypothesis alignment. By
a minor modification of the first module in the
classic pipeline, we successfully combine GIZA++
and IHMM to obtain significant improvement over
a powerful and state-of-the-art IHMM based sys-
tem. In comparison with another genre of improving
system combination by combing adjacent modules
of the pipeline, more powerful incremental IHMM
here, our system also show moderate improvement.
Though, our best system may not overpass He and
Toutanova (2009) who combine all the modules into
a unified training procedure, we believe our method
could boost many work on the higher modules of the
pipeline to obtain a further improvement to match
their work.
</bodyText>
<sectionHeader confidence="0.996908" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9999215">
This research is partially supported by Air Force
Office of Scientific Research under grant FA9550-
10-1-0335, the National Science Foundation under
grant IIS RI-small 1218863 and a Google research
award. We thank the anonymous reviewers for their
insightful comments.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997949166666667">
B Bangalore, German Bordel, and Giuseppe Riccardi.
2001. Computing consensus translation from multi-
ple machine translation systems. In Automatic Speech
Recognition and Understanding.
Yidong Chen, Xiaodong Shi, Changle Zhou, and
Qingyang Hong. 2009. A word alignment
</reference>
<page confidence="0.986619">
543
</page>
<reference confidence="0.999778575471698">
model based on multiobjective evolutionary algo-
rithms. Computers and Mathematics with Applica-
tions, 57.
Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and
Tanaka Meyarivan. 2000. A fast elitist non-dominated
sorting genetic algorithm for multi-objective optimiza-
tion: Nsga-ii. Lecture notes in computer science,
1917:849–858.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
TAMT Meyarivan. 2002. A fast and elitist multiob-
jective genetic algorithm: Nsga-ii. Evolutionary Com-
putation, IEEE Transactions on, 6(2):182–197.
Kalyanmoy Deb. 2001. Multi-objective optimization.
Multi-objective optimization using evolutionary algo-
rithms, pages 13–46.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Proc. of NAACL, pages 975–983.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proc. of ACL,
pages 1–10.
Andries P Engelbrecht. 2005. Fundamentals of compu-
tational swarm intelligence, volume 1. Wiley Chich-
ester.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan Lü. 2009. Lattice-based system combination for
statistical machine translation. In Proc. of EMNLP,
EMNLP ’09.
G David Forney Jr. 1973. The viterbi algorithm. Proc.
of the IEEE, 61(3):268–278.
Michael Pilegaard Hansen. 1997. Tabu search for mul-
tiobjective optimization: Mots. In Proc. of Multiple
Criteria Decision Making, pages 574–586.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. of EMNLP.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. of EMNLP.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Fei Huang and Kishore Papineni. 2007. Hierarchical
system combination for machine translation. In Proc.
of EMNLP-CoNLL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proc. of ACL: Poster,
pages 177–180.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proc. of Joint ACL and
AFNLP.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest rerank-
ing for machine translation with the perceptron algo-
rithm. GALE book chapter on MT From Text.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. of Joint ACL and AFNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of NAACL.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of EACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. Proc. of ACL-08: HLT, pages 192–
199.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440–447, October.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, pages
160–167.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proc. of EMNLP.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a. Combining outputs from multiple machine
translation systems. In Proc. of NAACL-HLT.
Antti-Veikko I Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. ofACL, vol-
ume 45.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proc. of WSMT.
Paolo Serafini. 1994. Simulated annealing for multi ob-
jective optimization problems. In Proc. of Multiple
Criteria Decision Making, pages 283–292. Springer.
Khe Chai Sim, William J Byrne, Mark JF Gales, Hichem
Sahbi, and Phil C Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proc. of ICASSP, volume 4.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proc. of NAACL: Short Papers, pages 205–208.
</reference>
<page confidence="0.998267">
544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423414">
<title confidence="0.9977565">Improving Alignment of System Combination by Multi-objective Optimization</title>
<author confidence="0.988098">Zongcheng Shaodan Yidong Qun Shaojun</author>
<affiliation confidence="0.6501285">University, Xiamen 361005, P.R. State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435,</affiliation>
<address confidence="0.9336485">of Computing Technology, Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<email confidence="0.949994">liuqun}@ict.ac.cn{xia.7,zhai.6,shaojun.wang}@wright.edu</email>
<abstract confidence="0.999717733333333">This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here. In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems. In Automatic Speech Recognition and Understanding.</title>
<date>2001</date>
<contexts>
<context position="1924" citStr="Bangalore et al., 2001" startWordPosition="274" endWordPosition="277">es can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based al</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>B Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yidong Chen</author>
</authors>
<title>Xiaodong Shi,</title>
<location>Changle Zhou, and</location>
<marker>Chen, </marker>
<rawString>Yidong Chen, Xiaodong Shi, Changle Zhou, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingyang Hong</author>
</authors>
<title>A word alignment model based on multiobjective evolutionary algorithms.</title>
<date>2009</date>
<journal>Computers and Mathematics with Applications,</journal>
<volume>57</volume>
<marker>Hong, 2009</marker>
<rawString>Qingyang Hong. 2009. A word alignment model based on multiobjective evolutionary algorithms. Computers and Mathematics with Applications, 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalyanmoy Deb</author>
<author>Samir Agrawal</author>
<author>Amrit Pratap</author>
<author>Tanaka Meyarivan</author>
</authors>
<title>A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii. Lecture notes in computer science,</title>
<date>2000</date>
<pages>1917--849</pages>
<contexts>
<context position="4425" citStr="Deb et al., 2000" startWordPosition="673" endWordPosition="676">imple and effective variational inference algorithm. Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and resea</context>
<context position="14338" citStr="Deb et al., 2000" startWordPosition="2329" endWordPosition="2332">p3, p5, p7 are Pareto optimal. In some cases, Pareto optimal solutions can be used for good candidate solutions. Considering the IHMM model, maximizing Y axis, the top-4 best alignments are p1, p2, p3, p4. But from the view of Pareto optimal, the top-4 alignments would be p1, p3, p5, p7 without order, which considers a greater range than a single optimization model. In our method, we just combine these Pareto optimal solutions equally into a unique alignment (Section 3.3). Our adopted multi-objective optimization searching algorithm is the non-dominated sorting genetic algorithm II (NSGA-II) (Deb et al., 2000; Deb et al., 2002) with an open source software (http://www.iitk.ac.in/kangal/codes.shtml). NSGAII has a complexity of O(mn2), where m is the number of objectives and n is the population size in an evolutionary algorithm. 3.2 Objectives in Evolutionary Algorithm The optimization objectives in our experiments can be categorized as an IHMM alignment probability (He et al., 2008) and GIZA++ alignment probability ⊕ p2 ⊕ p4 K p1 p3 ⊕ p6 � p5 a p7 538 f1 f2 f3 � � � � � � e1 e2 e3 Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in two IHMM models. The upper one is a typical example in IHMM, a</context>
</contexts>
<marker>Deb, Agrawal, Pratap, Meyarivan, 2000</marker>
<rawString>Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and Tanaka Meyarivan. 2000. A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii. Lecture notes in computer science, 1917:849–858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalyanmoy Deb</author>
<author>Amrit Pratap</author>
<author>Sameer Agarwal</author>
<author>TAMT Meyarivan</author>
</authors>
<title>A fast and elitist multiobjective genetic algorithm: Nsga-ii. Evolutionary Computation,</title>
<date>2002</date>
<journal>IEEE Transactions on,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="4444" citStr="Deb et al., 2002" startWordPosition="677" endWordPosition="680">e variational inference algorithm. Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and researchers just need to</context>
<context position="14357" citStr="Deb et al., 2002" startWordPosition="2333" endWordPosition="2336">eto optimal. In some cases, Pareto optimal solutions can be used for good candidate solutions. Considering the IHMM model, maximizing Y axis, the top-4 best alignments are p1, p2, p3, p4. But from the view of Pareto optimal, the top-4 alignments would be p1, p3, p5, p7 without order, which considers a greater range than a single optimization model. In our method, we just combine these Pareto optimal solutions equally into a unique alignment (Section 3.3). Our adopted multi-objective optimization searching algorithm is the non-dominated sorting genetic algorithm II (NSGA-II) (Deb et al., 2000; Deb et al., 2002) with an open source software (http://www.iitk.ac.in/kangal/codes.shtml). NSGAII has a complexity of O(mn2), where m is the number of objectives and n is the population size in an evolutionary algorithm. 3.2 Objectives in Evolutionary Algorithm The optimization objectives in our experiments can be categorized as an IHMM alignment probability (He et al., 2008) and GIZA++ alignment probability ⊕ p2 ⊕ p4 K p1 p3 ⊕ p6 � p5 a p7 538 f1 f2 f3 � � � � � � e1 e2 e3 Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in two IHMM models. The upper one is a typical example in IHMM, and in the bottom on</context>
</contexts>
<marker>Deb, Pratap, Agarwal, Meyarivan, 2002</marker>
<rawString>Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A fast and elitist multiobjective genetic algorithm: Nsga-ii. Evolutionary Computation, IEEE Transactions on, 6(2):182–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalyanmoy Deb</author>
</authors>
<title>Multi-objective optimization. Multi-objective optimization using evolutionary algorithms,</title>
<date>2001</date>
<pages>13--46</pages>
<contexts>
<context position="11880" citStr="Deb, 2001" startWordPosition="1878" endWordPosition="1879">put of the first module, which increases the instability of the whole system. Therefore, an unsupervised multi-objective algorithm may be a good choice allowing for more alignment information. There exist other alternative optimization algorithms in the multi-objective optimization framework, though the evolutionary algorithm is adopted here, we only introduce some general concepts. 3.1 Pareto Optimal Solutions A general multi-objective optimization problem consists of a number of objectives and is associated with a number of constraints. Mathematically, the problem can be written as follows (Deb, 2001) Maximize fz(x) i = 1... M s.t. gj(x) G 0 j = 1... N hk(x) = 0 k = 1... K where x denotes a potential solution, its structure relying on different problems, and the number of constraints M, N, K depend on different problems. All the functions fz, gj, hk map a solution x into a scalar. We will explain them in terms of system combination. In this work, we refer to x = {xz,jixz,j E {0,1}} as a potential alignment of a pair of hypotheses, where xz,j is a boolean value to denote whether the ith word in the first hypothesis is aligned to the jth word in the second hypothesis. Here the definition of </context>
</contexts>
<marker>Deb, 2001</marker>
<rawString>Kalyanmoy Deb. 2001. Multi-objective optimization. Multi-objective optimization using evolutionary algorithms, pages 13–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>975--983</pages>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Proc. of NAACL, pages 975–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Learning to translate with multiple objectives.</title>
<date>2012</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="23759" citStr="Duh et al., 2012" startWordPosition="3889" endWordPosition="3892">m our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline. Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment (Liang et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not listed as our baseline like (Duh et al., 2012) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2009). The smoothing factor for the surface similarity model, and p = 3 the controlling factor for the distortion model, K = 2. The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs. GIZA++ base</context>
</contexts>
<marker>Duh, Sudoh, Wu, Tsukada, Nagata, 2012</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada, and Masaaki Nagata. 2012. Learning to translate with multiple objectives. In Proc. of ACL, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andries P Engelbrecht</author>
</authors>
<title>Fundamentals of computational swarm intelligence, volume 1.</title>
<date>2005</date>
<publisher>Wiley Chichester.</publisher>
<contexts>
<context position="4505" citStr="Engelbrecht, 2005" startWordPosition="687" endWordPosition="688">ent algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objective definitions. The search algo</context>
</contexts>
<marker>Engelbrecht, 2005</marker>
<rawString>Andries P Engelbrecht. 2005. Fundamentals of computational swarm intelligence, volume 1. Wiley Chichester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Yajuan Lü</author>
</authors>
<title>Lattice-based system combination for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP, EMNLP ’09.</booktitle>
<contexts>
<context position="2129" citStr="Feng et al., 2009" startWordPosition="314" endWordPosition="317">datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; F</context>
<context position="6327" citStr="Feng et al. (2009)" startWordPosition="958" endWordPosition="961">gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment for construction. Experiments on the Chinese-to-English task on two datasets use four objectives, IHMM probability (Section 3.2.1), and alignment probability from GIZA++ (Section 3.2.2) from two directions. Results show multi-objective optimization framework efficiently integrates different information to gain approximately 1 BLEU point improvement over a strong baseline. 2 Background We briefly give an introduction to confusion networks, and because the IHMM based </context>
<context position="19962" citStr="Feng et al., 2009" startWordPosition="3269" endWordPosition="3272">a two-stage strategy for normalization. Firstly, we use a simple and effective voting strategy to combine a set of many-to-many alignments into a single many-to-many alignment, and Secondly we normalize it into a one-to-one alignment for confusion network construction. In the first stage, we count the number of word-to-word alignments on each position pair (i, j). If there is more than a half number of alignments, then we output 1, otherwise 0. In the second stage, if any word relates to more than one word alignment, the one with the highest posterior probability is selected (He et al., 2008; Feng et al., 2009). The posterior probabilities can be computed in a classic forward-backward procedure in IHMM (He et al., 2008). 4 Training and Decoding Our work does not change the classic pipeline, thus the model and features are nearly identical to the ones in (Rosti et al., 2007b; He et al., 2008), which are modeled in a log-linear fashion in Eq. 8. Translation on a CN is just a concatenation of edges traversed, on which 4 categories of features are defined. 1. word posterior probabilities. In Eq. 8, p(w|sys, span) are word confidence scores. If the word w comes from the kth hypothesis of thesys-th system</context>
<context position="24105" citStr="Feng et al., 2009" startWordPosition="3946" endWordPosition="3949"> et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not listed as our baseline like (Duh et al., 2012) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2009). The smoothing factor for the surface similarity model, and p = 3 the controlling factor for the distortion model, K = 2. The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs. GIZA++ based system is to run GIZA++ from two directions to align all the hypotheses, and make the intersection using grow-diag-final heuristics (Koehn et al., 2003). The many-to-many alignments are normalized with the same method with ours. Our system employs NSGA-II software to realize the MOEA algorithm. The main parameters, generation number, cross pr</context>
</contexts>
<marker>Feng, Liu, Mi, Liu, Lü, 2009</marker>
<rawString>Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan Lü. 2009. Lattice-based system combination for statistical machine translation. In Proc. of EMNLP, EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G David Forney Jr</author>
</authors>
<title>The viterbi algorithm.</title>
<date>1973</date>
<booktitle>Proc. of the IEEE,</booktitle>
<pages>61--3</pages>
<contexts>
<context position="4920" citStr="Jr, 1973" startWordPosition="747" endWordPosition="748"> exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objective definitions. The search algorithm seeks for potentially better solutions which are no worse than the current solution set. The output from multiobjective optimization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment. We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3). Our work is conducted on the classi</context>
</contexts>
<marker>Jr, 1973</marker>
<rawString>G David Forney Jr. 1973. The viterbi algorithm. Proc. of the IEEE, 61(3):268–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pilegaard Hansen</author>
</authors>
<title>Tabu search for multiobjective optimization: Mots.</title>
<date>1997</date>
<booktitle>In Proc. of Multiple Criteria Decision Making,</booktitle>
<pages>574--586</pages>
<contexts>
<context position="4472" citStr="Hansen, 1997" startWordPosition="683" endWordPosition="684">m. Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objec</context>
</contexts>
<marker>Hansen, 1997</marker>
<rawString>Michael Pilegaard Hansen. 1997. Tabu search for multiobjective optimization: Mots. In Proc. of Multiple Criteria Decision Making, pages 574–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Kristina Toutanova</author>
</authors>
<title>Joint optimization for machine translation system combination.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2093" citStr="He and Toutanova, 2009" startWordPosition="306" endWordPosition="309">ts on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorit</context>
<context position="5859" citStr="He and Toutanova (2009)" startWordPosition="886" endWordPosition="889">ization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment. We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3). Our work is conducted on the classic pipeline which has three modules, pair-wise hypothesis alignment, confusion network construction, and training. Now many work integrates neighboring modules to avoid propagated errors to gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment fo</context>
</contexts>
<marker>He, Toutanova, 2009</marker>
<rawString>Xiaodong He and Kristina Toutanova. 2009. Joint optimization for machine translation system combination. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2049" citStr="He et al., 2008" startWordPosition="298" endWordPosition="301">nstruct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using</context>
<context position="12697" citStr="He et al., 2008" startWordPosition="2038" endWordPosition="2041"> on different problems. All the functions fz, gj, hk map a solution x into a scalar. We will explain them in terms of system combination. In this work, we refer to x = {xz,jixz,j E {0,1}} as a potential alignment of a pair of hypotheses, where xz,j is a boolean value to denote whether the ith word in the first hypothesis is aligned to the jth word in the second hypothesis. Here the definition of x seems different from that of a in Formula 1, and they could convert to each other. Using a line-based access style, a matrix can be unfolded as a vector. We refer to f as IHMM alignment probability (He et al., 2008) and GIZA++ alignment probability (Chen et al., 2009), total four objectives from two directions, and the larger the objectives, the better. The gjs and hks serve as the role of checking if x represents a legal alignment. For instance, the subscripts of xz,j are not in bounds. Definition 1. Let x, x&apos; be two potential alignments. If fz(x) &gt; fz(x&apos;) holds for all i, we call the alignment x dominates the alignment x&apos;. If there 0 1 2 3 4 5 X: Reversed THMM Probability (1e-8) Figure 2: Sample solutions with only two objectives. Pareto Optimal Solutions p1, p3, p5, p7. Other points p2, p4, p� are dom</context>
<context position="14718" citStr="He et al., 2008" startWordPosition="2387" endWordPosition="2390">od, we just combine these Pareto optimal solutions equally into a unique alignment (Section 3.3). Our adopted multi-objective optimization searching algorithm is the non-dominated sorting genetic algorithm II (NSGA-II) (Deb et al., 2000; Deb et al., 2002) with an open source software (http://www.iitk.ac.in/kangal/codes.shtml). NSGAII has a complexity of O(mn2), where m is the number of objectives and n is the population size in an evolutionary algorithm. 3.2 Objectives in Evolutionary Algorithm The optimization objectives in our experiments can be categorized as an IHMM alignment probability (He et al., 2008) and GIZA++ alignment probability ⊕ p2 ⊕ p4 K p1 p3 ⊕ p6 � p5 a p7 538 f1 f2 f3 � � � � � � e1 e2 e3 Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in two IHMM models. The upper one is a typical example in IHMM, and in the bottom one, because any word in the observation is required not to correspond to two statuses, it has a minor trouble. S: status sequence, O: observation sequence. (Chen et al., 2009), total four from two directions. 3.2.1 IHMM Probability A typical IHMM alignment is demonstrated in the upper graph of Figure 3, where a backbone is acting the role of a status sequence.</context>
<context position="19942" citStr="He et al., 2008" startWordPosition="3265" endWordPosition="3268">ent. We describe a two-stage strategy for normalization. Firstly, we use a simple and effective voting strategy to combine a set of many-to-many alignments into a single many-to-many alignment, and Secondly we normalize it into a one-to-one alignment for confusion network construction. In the first stage, we count the number of word-to-word alignments on each position pair (i, j). If there is more than a half number of alignments, then we output 1, otherwise 0. In the second stage, if any word relates to more than one word alignment, the one with the highest posterior probability is selected (He et al., 2008; Feng et al., 2009). The posterior probabilities can be computed in a classic forward-backward procedure in IHMM (He et al., 2008). 4 Training and Decoding Our work does not change the classic pipeline, thus the model and features are nearly identical to the ones in (Rosti et al., 2007b; He et al., 2008), which are modeled in a log-linear fashion in Eq. 8. Translation on a CN is just a concatenation of edges traversed, on which 4 categories of features are defined. 1. word posterior probabilities. In Eq. 8, p(w|sys, span) are word confidence scores. If the word w comes from the kth hypothesis</context>
<context position="24085" citStr="He et al., 2008" startWordPosition="3942" endWordPosition="3945"> alignment (Liang et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not listed as our baseline like (Duh et al., 2012) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2009). The smoothing factor for the surface similarity model, and p = 3 the controlling factor for the distortion model, K = 2. The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs. GIZA++ based system is to run GIZA++ from two directions to align all the hypotheses, and make the intersection using grow-diag-final heuristics (Koehn et al., 2003). The many-to-many alignments are normalized with the same method with ours. Our system employs NSGA-II software to realize the MOEA algorithm. The main parameters, generat</context>
<context position="26033" citStr="He et al., 2008" startWordPosition="4259" endWordPosition="4262"> alignment reaches around 0.15 in AER, and our method 0.145. As the AER may not vividly reflect the relations between alignment and the final BLEU of systems, and the quality of confusion network is hard to measure directly, we assume that the quality of confusion networks could be measured by the oracle hypotheses that could be generated from them. We test the BLEU of the oracle hypotheses. From this angle, we demonstrate several oracle BLEU of CNs generated from some conventional alignment algorithms. The results are shown in Table 3. We find the confusion network from IHMM based alignment (He et al., 2008) is better than that from TER based alignment (Rosti et al., 2007b) by about 1 point in both two datasets. These quantities agree with the final improvements in the BLEU score in (He et al., 2008). As confusion networks from MOEA based alignment also show superiority over GIZA++ PPBD N-best IHMM dH+rH dH+dT dH+rH+dT dH+rH+rT dH+rH+dT+rT 541 alignment MT02 MT05 GIZA++ 0.5690 0.5228 TER 0.5720 0.5270 IHMM 0.5883 0.5382 IncIHMM 0.5931 0.5453 MOEA 0.6017 0.5526 Table 3: Oracle BLEUs of CNs. GIZA++: invoking GIZA++ software. TER: minimum translation edit rate. IHMM: indirect hidden markov model. In</context>
<context position="27927" citStr="He et al., 2008" startWordPosition="4556" endWordPosition="4559">bjective Combinations As our framework is convenient to support different alignment information, we test the influence of different objective combinations to the final translation quality. We adopt four objectives to depict the candidate alignment, directed IHMM probability (dH), reversed IHMM probability (rH), directed alignment probability (dT), and reversed alignment probability (rT). Table 2 demonstrates all the results. We can see that the IHMM based system outperforms the GIZA++ based system by about 1-1.5 points in BLEU, which agrees with the difference of oracle BLEU in Table 1. From (He et al., 2008), the IHMM based system outperforms the TER based by 1 point, which also agrees with our results in Table 1. Our system, using dH + rH + dT + rT, improves BLEU score by about 1 points over the IHMM based system. This comparison verifies our assumption, improving the quality of the confusion network does improve system performance. The different feature combinations exhibit interesting results. The system with dH + rH + dT is 0.05 point better than the system with dH + rH, and the system dH + rH + rT is 0.3 point better than system with dH + rH, so the contributions of feature dT and rT are 0.0</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="21172" citStr="Huang and Chiang, 2005" startWordPosition="3481" endWordPosition="3484">sys-th system, the raw score should be 1 k+1, and then it would be normalized by the same sys and span. The same word coming from different systems owns a different score, so there are sys system weights Asys. 2. logarithm of language model score, L(h). 3. number of null edge, Numnnii. 4. number of words, Num,,,. log(h) = Espan log(Esys Asysp(w|sys, span)) + w0L(h) + w1Numnnii + w2Num,,, (8) Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an Nbest list. The final N-best can be acquired following (Huang and Chiang, 2005). The training process follows minimum error rate training (MERT) described in (Och, 2003; Koehn et al., 2003). In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list. 5 Experiments We evaluate our method in two datasets in the Chinese-to-English task. In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing. A 5- gram language model is trained on the Xinhua portion of the Gigaword corpus. We report the casesensitive NIS</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Kishore Papineni</author>
</authors>
<title>Hierarchical system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2032" citStr="Huang and Papineni, 2007" startWordPosition="294" endWordPosition="297">s, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC system</context>
</contexts>
<marker>Huang, Papineni, 2007</marker>
<rawString>Fei Huang and Kishore Papineni. 2007. Hierarchical system combination for machine translation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3148" citStr="Koehn et al., 2003" startWordPosition="483" endWordPosition="486">hm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist. For instance, “be about to” has the same meaning with “be on the point of”. Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT) (Och and Ney, 2000; Koehn et al., 2003), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics from two direc</context>
<context position="21282" citStr="Koehn et al., 2003" startWordPosition="3498" endWordPosition="3501">ord coming from different systems owns a different score, so there are sys system weights Asys. 2. logarithm of language model score, L(h). 3. number of null edge, Numnnii. 4. number of words, Num,,,. log(h) = Espan log(Esys Asysp(w|sys, span)) + w0L(h) + w1Numnnii + w2Num,,, (8) Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an Nbest list. The final N-best can be acquired following (Huang and Chiang, 2005). The training process follows minimum error rate training (MERT) described in (Och, 2003; Koehn et al., 2003). In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list. 5 Experiments We evaluate our method in two datasets in the Chinese-to-English task. In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing. A 5- gram language model is trained on the Xinhua portion of the Gigaword corpus. We report the casesensitive NIST-BLEU score. Four single machine translation systems participating in the system combination consist of a BTG</context>
<context position="24513" citStr="Koehn et al., 2003" startWordPosition="4014" endWordPosition="4017">pace is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2009). The smoothing factor for the surface similarity model, and p = 3 the controlling factor for the distortion model, K = 2. The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs. GIZA++ based system is to run GIZA++ from two directions to align all the hypotheses, and make the intersection using grow-diag-final heuristics (Koehn et al., 2003). The many-to-many alignments are normalized with the same method with ours. Our system employs NSGA-II software to realize the MOEA algorithm. The main parameters, generation number, cross probability and mutation probability, and population size, are empirically set as 100, 0.9, 0.001 and 40, and we examine the influence of difference populations sizes in the full system combination. 5.1 The Quality of Confusion Networks This experiment shows the relationship between hypothesis alignment and confusion network. Intuitively, we expect a better hypothesis alignment would reduce the error in con</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran, Richard</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL: Poster,</booktitle>
<pages>177--180</pages>
<marker>Zens, 2007</marker>
<rawString>Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL: Poster, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. of Joint ACL and AFNLP.</booktitle>
<contexts>
<context position="6266" citStr="Kumar et al., 2009" startWordPosition="946" endWordPosition="949"> integrates neighboring modules to avoid propagated errors to gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment for construction. Experiments on the Chinese-to-English task on two datasets use four objectives, IHMM probability (Section 3.2.1), and alignment probability from GIZA++ (Section 3.2.2) from two directions. Results show multi-objective optimization framework efficiently integrates different information to gain approximately 1 BLEU point improvement over a strong baseline. 2 Background We briefly give an in</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proc. of Joint ACL and AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Forest reranking for machine translation with the perceptron algorithm.</title>
<date>2009</date>
<note>GALE book chapter on MT From Text.</note>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009. Forest reranking for machine translation with the perceptron algorithm. GALE book chapter on MT From Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Xiaodong He</author>
<author>Yupeng Liu</author>
<author>Ning Xi</author>
</authors>
<title>Incremental hmm alignment for mt system combination.</title>
<date>2009</date>
<booktitle>In Proc. of Joint ACL and AFNLP.</booktitle>
<contexts>
<context position="2110" citStr="Li et al., 2009" startWordPosition="310" endWordPosition="313">lish translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less </context>
<context position="5790" citStr="Li et al. (2009)" startWordPosition="874" endWordPosition="877">the current solution set. The output from multiobjective optimization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment. We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3). Our work is conducted on the classic pipeline which has three modules, pair-wise hypothesis alignment, confusion network construction, and training. Now many work integrates neighboring modules to avoid propagated errors to gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different data structure called </context>
<context position="23112" citStr="Li et al., 2009" startWordPosition="3787" endWordPosition="3790">n 5.3). N-best IHMM simulates the Pareto optimal solutions in our method (section 5.3). The last five systems adopt different objective combinations. The improvement percents in parentheses are compared to the best single. dH: directed IHMM, rH: reversed IHMM, dT: directed translation probability, rT: reversed translation probability. ** significance at 0.01 level, and * significance at 0.05 level over the IHMM model. and test sets are collected as the input of the system combination. Our baseline systems are described as follows. Two main baseline systems are IHMM based and incremental IHMM (Li et al., 2009). The first system differs from our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline. Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment (Liang et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not</context>
</contexts>
<marker>Li, He, Liu, Xi, 2009</marker>
<rawString>Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi. 2009. Incremental hmm alignment for mt system combination. In Proc. of Joint ACL and AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3798" citStr="Liang et al. (2006)" startWordPosition="585" endWordPosition="588">alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics from two directions. To bypass this problem, Liang et al. (2006) adopts a simple and effective variational inference algorithm. Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary </context>
<context position="23501" citStr="Liang et al., 2006" startWordPosition="3846" endWordPosition="3849"> at 0.05 level over the IHMM model. and test sets are collected as the input of the system combination. Our baseline systems are described as follows. Two main baseline systems are IHMM based and incremental IHMM (Li et al., 2009). The first system differs from our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline. Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment (Liang et al., 2006). Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method. The method that linearly combines all objectives is not listed as our baseline like (Duh et al., 2012) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here. The IHMM based alignment utilizes typical settings (He et al., 2008; Feng et al., 2</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="1946" citStr="Matusov et al., 2006" startWordPosition="278" endWordPosition="281">ture algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al.,</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>Proc. of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. Proc. of ACL-08: HLT, pages 192– 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<pages>440--447</pages>
<contexts>
<context position="3127" citStr="Och and Ney, 2000" startWordPosition="479" endWordPosition="482">(TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist. For instance, “be about to” has the same meaning with “be on the point of”. Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT) (Och and Ney, 2000; Koehn et al., 2003), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information. However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities 535 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535–544, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Lingu</context>
<context position="25254" citStr="Och and Ney, 2000" startWordPosition="4127" endWordPosition="4130"> the MOEA algorithm. The main parameters, generation number, cross probability and mutation probability, and population size, are empirically set as 100, 0.9, 0.001 and 40, and we examine the influence of difference populations sizes in the full system combination. 5.1 The Quality of Confusion Networks This experiment shows the relationship between hypothesis alignment and confusion network. Intuitively, we expect a better hypothesis alignment would reduce the error in constructing confusion networks, and then improve the final translation quality. We first use the alignment error rate (AER) (Och and Ney, 2000), which is widely used to measure the quality of hypothesis alignment. The smaller, the better. For convenience, we only examine exact literal matching. IHMM based alignment reaches around 0.15 in AER, and our method 0.145. As the AER may not vividly reflect the relations between alignment and the final BLEU of systems, and the quality of confusion network is hard to measure directly, we assume that the quality of confusion networks could be measured by the oracle hypotheses that could be generated from them. We test the BLEU of the oracle hypotheses. From this angle, we demonstrate several or</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. pages 440–447, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="21261" citStr="Och, 2003" startWordPosition="3496" endWordPosition="3497"> The same word coming from different systems owns a different score, so there are sys system weights Asys. 2. logarithm of language model score, L(h). 3. number of null edge, Numnnii. 4. number of words, Num,,,. log(h) = Espan log(Esys Asysp(w|sys, span)) + w0L(h) + w1Numnnii + w2Num,,, (8) Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an Nbest list. The final N-best can be acquired following (Huang and Chiang, 2005). The training process follows minimum error rate training (MERT) described in (Och, 2003; Koehn et al., 2003). In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list. 5 Experiments We evaluate our method in two datasets in the Chinese-to-English task. In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing. A 5- gram language model is trained on the Xinhua portion of the Gigaword corpus. We report the casesensitive NIST-BLEU score. Four single machine translation systems participating in the system combina</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Consensus training for consensus decoding in machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2150" citStr="Pauls et al., 2009" startWordPosition="318" endWordPosition="321">ficant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (Li et al., 2009; Feng et al., 2009). Ou</context>
</contexts>
<marker>Pauls, DeNero, Klein, 2009</marker>
<rawString>Adam Pauls, John DeNero, and Dan Klein. 2009. Consensus training for consensus decoding in machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="1984" citStr="Rosti et al., 2007" startWordPosition="286" endWordPosition="289"> framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al.</context>
<context position="20229" citStr="Rosti et al., 2007" startWordPosition="3315" endWordPosition="3318">n. In the first stage, we count the number of word-to-word alignments on each position pair (i, j). If there is more than a half number of alignments, then we output 1, otherwise 0. In the second stage, if any word relates to more than one word alignment, the one with the highest posterior probability is selected (He et al., 2008; Feng et al., 2009). The posterior probabilities can be computed in a classic forward-backward procedure in IHMM (He et al., 2008). 4 Training and Decoding Our work does not change the classic pipeline, thus the model and features are nearly identical to the ones in (Rosti et al., 2007b; He et al., 2008), which are modeled in a log-linear fashion in Eq. 8. Translation on a CN is just a concatenation of edges traversed, on which 4 categories of features are defined. 1. word posterior probabilities. In Eq. 8, p(w|sys, span) are word confidence scores. If the word w comes from the kth hypothesis of thesys-th system, the raw score should be 1 k+1, and then it would be normalized by the same sys and span. The same word coming from different systems owns a different score, so there are sys system weights Asys. 2. logarithm of language model score, L(h). 3. number of null edge, Nu</context>
<context position="26098" citStr="Rosti et al., 2007" startWordPosition="4271" endWordPosition="4274"> the AER may not vividly reflect the relations between alignment and the final BLEU of systems, and the quality of confusion network is hard to measure directly, we assume that the quality of confusion networks could be measured by the oracle hypotheses that could be generated from them. We test the BLEU of the oracle hypotheses. From this angle, we demonstrate several oracle BLEU of CNs generated from some conventional alignment algorithms. The results are shown in Table 3. We find the confusion network from IHMM based alignment (He et al., 2008) is better than that from TER based alignment (Rosti et al., 2007b) by about 1 point in both two datasets. These quantities agree with the final improvements in the BLEU score in (He et al., 2008). As confusion networks from MOEA based alignment also show superiority over GIZA++ PPBD N-best IHMM dH+rH dH+dT dH+rH+dT dH+rH+rT dH+rH+dT+rT 541 alignment MT02 MT05 GIZA++ 0.5690 0.5228 TER 0.5720 0.5270 IHMM 0.5883 0.5382 IncIHMM 0.5931 0.5453 MOEA 0.6017 0.5526 Table 3: Oracle BLEUs of CNs. GIZA++: invoking GIZA++ software. TER: minimum translation edit rate. IHMM: indirect hidden markov model. IncIHMM: incremental indirect hidden markov model. MOEA: multiobjec</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007a. Combining outputs from multiple machine translation systems. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL,</booktitle>
<volume>45</volume>
<contexts>
<context position="1984" citStr="Rosti et al., 2007" startWordPosition="286" endWordPosition="289"> framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al.</context>
<context position="20229" citStr="Rosti et al., 2007" startWordPosition="3315" endWordPosition="3318">n. In the first stage, we count the number of word-to-word alignments on each position pair (i, j). If there is more than a half number of alignments, then we output 1, otherwise 0. In the second stage, if any word relates to more than one word alignment, the one with the highest posterior probability is selected (He et al., 2008; Feng et al., 2009). The posterior probabilities can be computed in a classic forward-backward procedure in IHMM (He et al., 2008). 4 Training and Decoding Our work does not change the classic pipeline, thus the model and features are nearly identical to the ones in (Rosti et al., 2007b; He et al., 2008), which are modeled in a log-linear fashion in Eq. 8. Translation on a CN is just a concatenation of edges traversed, on which 4 categories of features are defined. 1. word posterior probabilities. In Eq. 8, p(w|sys, span) are word confidence scores. If the word w comes from the kth hypothesis of thesys-th system, the raw score should be 1 k+1, and then it would be normalized by the same sys and span. The same word coming from different systems owns a different score, so there are sys system weights Asys. 2. logarithm of language model score, L(h). 3. number of null edge, Nu</context>
<context position="26098" citStr="Rosti et al., 2007" startWordPosition="4271" endWordPosition="4274"> the AER may not vividly reflect the relations between alignment and the final BLEU of systems, and the quality of confusion network is hard to measure directly, we assume that the quality of confusion networks could be measured by the oracle hypotheses that could be generated from them. We test the BLEU of the oracle hypotheses. From this angle, we demonstrate several oracle BLEU of CNs generated from some conventional alignment algorithms. The results are shown in Table 3. We find the confusion network from IHMM based alignment (He et al., 2008) is better than that from TER based alignment (Rosti et al., 2007b) by about 1 point in both two datasets. These quantities agree with the final improvements in the BLEU score in (He et al., 2008). As confusion networks from MOEA based alignment also show superiority over GIZA++ PPBD N-best IHMM dH+rH dH+dT dH+rH+dT dH+rH+rT dH+rH+dT+rT 541 alignment MT02 MT05 GIZA++ 0.5690 0.5228 TER 0.5720 0.5270 IHMM 0.5883 0.5382 IncIHMM 0.5931 0.5453 MOEA 0.6017 0.5526 Table 3: Oracle BLEUs of CNs. GIZA++: invoking GIZA++ software. TER: minimum translation edit rate. IHMM: indirect hidden markov model. IncIHMM: incremental indirect hidden markov model. MOEA: multiobjec</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I Rosti, Spyros Matsoukas, and Richard Schwartz. 2007b. Improved word-level system combination for machine translation. In Proc. ofACL, volume 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proc. of WSMT.</booktitle>
<contexts>
<context position="2069" citStr="Rosti et al., 2008" startWordPosition="302" endWordPosition="305"> networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et al., 2008; Sim et al., 2007). Now, current state-of-the-art SC systems have been using IHMM or variants in</context>
<context position="5768" citStr="Rosti et al. (2008)" startWordPosition="869" endWordPosition="872"> which are no worse than the current solution set. The output from multiobjective optimization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment. We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3). Our work is conducted on the classic pipeline which has three modules, pair-wise hypothesis alignment, confusion network construction, and training. Now many work integrates neighboring modules to avoid propagated errors to gain improved performance. For example, Rosti et al. (2008), and Li et al. (2009) combine the first and the second module, and He and Toutanova (2009) combine all modules into one directly. Nevertheless, the classic structure also owns its merits. Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively. Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (Kumar et al., 2009) could be adopted on the third module. And Feng et al. (2009) in the second module adopts a different </context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proc. of WSMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Serafini</author>
</authors>
<title>Simulated annealing for multi objective optimization problems.</title>
<date>1994</date>
<booktitle>In Proc. of Multiple Criteria Decision Making,</booktitle>
<pages>283--292</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4547" citStr="Serafini, 1994" startWordPosition="692" endWordPosition="693">and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment. Liang’s method may not be suitable for this expected outcome. We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm. In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (Deb et al., 2000; Deb et al., 2002), Tabu search (Hansen, 1997), ants colony (Engelbrecht, 2005), and simulated annealing (Serafini, 1994). In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml). On the other hand, this framework is also totally unsupervised. It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney Jr, 1973). This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objective definitions. The search algorithm seeks for potentially better solutio</context>
</contexts>
<marker>Serafini, 1994</marker>
<rawString>Paolo Serafini. 1994. Simulated annealing for multi objective optimization problems. In Proc. of Multiple Criteria Decision Making, pages 283–292. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark JF Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<volume>4</volume>
<contexts>
<context position="1964" citStr="Sim et al., 2007" startWordPosition="282" endWordPosition="285">olutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 1 Introduction System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b; Rosti et al., 2007a; Huang and Papineni, 2007; He et al., 2008; Rosti et al., 2008; He and Toutanova, 2009; Li et al., 2009; Feng et al., 2009; Pauls et al., 2009). A central data structure in the SC is the confusion network, and its quality greatly affects the final performance. He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (Rosti et al., 2007b; Rosti et a</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J Byrne, Mark JF Gales, Hichem Sahbi, and Phil C Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proc. of ICASSP, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Zhao</author>
<author>Xiaodong He</author>
</authors>
<title>Using n-gram based features for machine translation system combination.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL: Short Papers,</booktitle>
<pages>205--208</pages>
<marker>Zhao, He, 2009</marker>
<rawString>Yong Zhao and Xiaodong He. 2009. Using n-gram based features for machine translation system combination. In Proc. of NAACL: Short Papers, pages 205–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
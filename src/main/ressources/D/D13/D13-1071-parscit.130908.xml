<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<author confidence="0.845846">
Optimal Incremental Parsing via Best-First Dynamic Programming*
</author>
<affiliation confidence="0.9518545">
1Graduate Center
City University of New York
</affiliation>
<address confidence="0.759781">
365 Fifth Avenue, New York, NY 10016
</address>
<email confidence="0.999451">
{kzhao,jcross}@gc.cuny.edu
</email>
<sectionHeader confidence="0.994814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966615384616">
We present the first provably optimal polyno-
mial time dynamic programming (DP) algo-
rithm for best-first shift-reduce parsing, which
applies the DP idea of Huang and Sagae
(2010) to the best-first parser of Sagae and
Lavie (2006) in a non-trivial way, reducing
the complexity of the latter from exponential
to polynomial. We prove the correctness of
our algorithm rigorously. Experiments con-
firm that DP leads to a significant speedup
on a probablistic best-first shift-reduce parser,
and makes exact search under such a model
tractable for the first time.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875111111111">
Best-first parsing, such as A* parsing, makes con-
stituent parsing efficient, especially for bottom-up
CKY style parsing (Caraballo and Charniak, 1998;
Klein and Manning, 2003; Pauls and Klein, 2009).
Traditional CKY parsing performs cubic time exact
search over an exponentially large space. Best-first
parsing significantly speeds up by always preferring
to explore states with higher probabilities.
In terms of incremental parsing, Sagae and Lavie
(2006) is the first work to extend best-first search to
shift-reduce constituent parsing. Unlike other very
fast greedy parsers that produce suboptimal results,
this best-first parser still guarantees optimality but
requires exponential time for very long sentences
in the worst case, which is intractable in practice.
Because it needs to explore an exponentially large
space in the worst case, a bounded priority queue
becomes necessary to ensure limited parsing time.
</bodyText>
<footnote confidence="0.88607425">
*This work is mainly supported by DARPA FA8750-13-2-
0041 (DEFT), a Google Faculty Research Award, and a PSC-
CUNY Award. In addition, we thank Kenji Sagae and the
anonymous reviewers for their constructive comments.
</footnote>
<author confidence="0.903368">
Liang Huang1,2
</author>
<affiliation confidence="0.947348">
2Queens College
City University of New York
</affiliation>
<address confidence="0.745422">
6530 Kissena Blvd, Queens, NY 11367
</address>
<email confidence="0.99517">
huang@cs.qc.cuny.edu
</email>
<bodyText confidence="0.999969259259259">
On the other hand, Huang and Sagae (2010) ex-
plore the idea of dynamic programming, which is
originated in bottom-up constituent parsing algo-
rithms like Earley (1970), but in a beam-based non
best-first parser. In each beam step, they enable
state merging in a style similar to the dynamic pro-
gramming in bottom-up constituent parsing, based
on an equivalence relation defined upon feature val-
ues. Although in theory they successfully reduced
the underlying deductive system to polynomial time
complexity, their merging method is limited in that
the state merging is only between two states in the
same beam step. This significantly reduces the num-
ber of possible merges, because: 1) there are only
a very limited number of states in the beam at the
same time; 2) a lot of states in the beam with differ-
ent steps cannot be merged.
We instead propose to combine the idea of dy-
namic programming with the best-first search frame-
work, and apply it in shift-reduce dependency pars-
ing. We merge states with the same features set
globally to further reduce the number of possible
states in the search graph. Thus, our DP best-first al-
gorithm is significantly faster than non-DP best-first
parsing, and, more importantly, it has a polynomial
time complexity even in the worst case.
We make the following contributions:
</bodyText>
<listItem confidence="0.8588546">
• theoretically, we formally prove that our DP
best-first parsing reaches optimality with poly-
nomial time complexity. This is the first time
that exact search under such a probabilistic
model becomes tractable.
• more interestingly, we reveal that our dynamic
programming over shift-reduce parsing is in
parallel with the bottom-up parsers, except that
we have an extra order constraint given by the
shift action to enforce left to right generation of
</listItem>
<author confidence="0.652568">
Kai Zhao1 James Cross1
</author>
<page confidence="0.952466">
758
</page>
<note confidence="0.734656">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 758–768,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<equation confidence="0.93418175">
input w0 ... w,,,_1
axiom 0 : (0, c): 0
` : (j, S) : c
` + 1 : (j + 1, S|wj) : c + scsh(j, S) j &lt; n
` : (j, S|s1|s0) : c
` + 1 : (j, S|s1&apos;s0) : c + scre,(j, S|s1|s0)
` : (j, S|s1|s0) : c
` + 1 : (j, S|s1&apos;s0) : c + scre,(j, S|s1|s0)
</equation>
<figureCaption confidence="0.9984386">
Figure 1: Deductive system of basic non-DP shift-reduce
parsing. Here ` is the step index (for beam search), S is
the stack, c is the score of the precedent, and sca(x) is
the score of action a from derivation x. See Figure 2 for
the DP version.
</figureCaption>
<bodyText confidence="0.894486111111111">
partial trees, which is analogous to Earley.
• practically, our DP best-first parser is only —2
times slower than a pure greedy parser, but is
guaranteed to reach optimality. In particular,
it is —20 times faster than a non-DP best-first
parser. With inexact search of bounded prior-
ity queue size, DP best-first search can reach
optimality with a significantly smaller priority
queue size bound, compared to non-DP best-
first parser.
Our system is based on a MaxEnt model to meet
the requirement from best-first search. We observe
that this locally trained model is not as strong as
global models like structured perceptron. With that
being said, our algorithm shows its own merits in
both theory and practice. To find a better model for
best-first search would be an interesting topic for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.744227" genericHeader="introduction">
2 Shift-Reduce and Best-First Parsing
</sectionHeader>
<bodyText confidence="0.999486666666667">
In this section we review the basics of shift-reduce
parsing, beam search, and the best-first shift-reduce
parsing algorithm of Sagae and Lavie (2006).
</bodyText>
<subsectionHeader confidence="0.999747">
2.1 Shift-Reduce Parsing and Beam Search
</subsectionHeader>
<bodyText confidence="0.9994929">
Due to space constraints we will assume some ba-
sic familiarity with shift-reduce parsing; see Nivre
(2008) for details. Basically, shift-reduce parsing
(Aho and Ullman, 1972) performs a left-to-right
scan of the input sentence, and at each step, chooses
either to shift the next word onto the stack, or to re-
duce, i.e., combine the top two trees on stack, ei-
ther with left as the root or right as the root. This
scheme is often called “arc-standard” in the litera-
ture (Nivre, 2008), and is the basis of several state-
of-the-art parsers, e.g. Huang and Sagae (2010). See
Figure 1 for the deductive system of shift-reduce de-
pendency parsing.
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search (Zhang
and Clark, 2008), where b derivations develop in
parallel. At each step we extend the derivations in
the current beam by applying each of the three ac-
tions, and then choose the best b resulting deriva-
tions for the next step.
</bodyText>
<subsectionHeader confidence="0.999511">
2.2 Best-First Shift-Reduce Parsing
</subsectionHeader>
<bodyText confidence="0.999785941176471">
Sagae and Lavie (2006) present the parsing prob-
lem as a search problem over a DAG, in which each
parser derivation is denoted as a node, and an edge
from node x to node y exists if and only if the corre-
sponding derivation y can be generated from deriva-
tion x by applying one action.
The best-first parsing algorithm is an applica-
tion of the Dijkstra algorithm over the DAG above,
where the score of each derivation is the priority.
Dijkstra algorithm requires the priority to satisfy
the superiority property, which means a descendant
derivation should never have a higher score than its
ancestors. This requirement can be easily satisfied if
we use a generative scoring model like PCFG. How-
ever, in practice we use a MaxEnt model. And we
use the negative log probability as the score to sat-
isfy the superiority:
</bodyText>
<equation confidence="0.696679">
x � y #&gt; x.score &lt; y.score,
</equation>
<bodyText confidence="0.9998575">
where the order x � y means derivation x has a
higher priority than y.1
The vanilla best-first parsing algorithm inher-
its the optimality directly from Dijkstra algorithm.
However, it explores exponentially many derivations
to reach the goal configuration in the worst case.
We propose a new method that has polynomial time
complexity even in the worst case.
</bodyText>
<footnote confidence="0.996617">
1For simplicity we ignore the case when two derivations
have the same score. In practice we can choose either one of
the two derivations when they have the same score.
</footnote>
<equation confidence="0.575954666666667">
sh
re,
re,
</equation>
<page confidence="0.99715">
759
</page>
<sectionHeader confidence="0.8738045" genericHeader="method">
3 Dynamic Programming for Best-First
Shift-Reduce Parsing
</sectionHeader>
<subsectionHeader confidence="0.999388">
3.1 Dynamic Programming Notations
</subsectionHeader>
<bodyText confidence="0.9999548">
The key innovation of this paper is to extend best-
first parsing with the “state-merging” method of dy-
namic programming described in Huang and Sagae
(2010). We start with describing a parsing configu-
ration as a non-DP derivation:
</bodyText>
<equation confidence="0.542712">
hi, j, ...s2s1s0i,
</equation>
<bodyText confidence="0.9998682">
where ...s2s1s0 is the stack of partial trees, [i..j] is
the span of the top tree s0, and s1s2... are the re-
mainder of the trees on the stack.
The notation fk(sk) is used to indicate the features
used by the parser from the tree sk on the stack. Note
that the parser only extracts features from the top
d+1 trees on the stack.
Following Huang and Sagae (2010), f(x) of a
derivation x is called atomic features, defined as the
smallest set of features s.t.
</bodyText>
<equation confidence="0.9698855">
�f(i, j, ...s2s1s0) = f(i, j, ...s�2s01s00)
⇔ fk(sk) = fk(s�k),∀k ∈ [0, d].
</equation>
<bodyText confidence="0.8116795">
The atomic feature function i(·) defines an equiv-
alence relation ∼ in the space of derivations D:
</bodyText>
<equation confidence="0.7470665">
hi, j, ...s2s1s0i ∼ hi, j, ...s�2s01s00i
⇔ f(i, j, ...s2s1s0) = �f(i, j, ...s, 2s0 1s0 0)
</equation>
<bodyText confidence="0.9998758">
This implies that any derivations with the same
atomic features are in the same equivalence class,
and their behaviors are similar in shift and reduce.
We call each equivalence class a DP state. More
formally we define the space of all states S as:
</bodyText>
<subsectionHeader confidence="0.530706">
S Δ= D/∼.
</subsectionHeader>
<bodyText confidence="0.975285193548387">
Since only the top d+1 trees on the stack are used
in atomic features, we only need to remember the
necessary information and write the state as:
hi, j, sd...s0i.
We denote a derivation x’s state as [x]—. In the rest
of this paper, we always denote derivations with let-
ters x, y, and z, and denote states with letters p, q,
and r.
The deductive system for dynamic programming
best-first parsing is adapted from Huang and Sagae
(2010). (See the left of Figure 2.) The difference is
that we do not distinguish the step index of a state.
This deductive system describes transitions be-
tween states. However, in practice we use one state’s
best derivation found so far to represent the state.
For each state p, we calculate the prefix score, p.pre,
which is the score of the derivation to reach this
state, and the inside score, p.ins, which is the score
of p’s top tree p.s0. In addition we denote the shift
score of state p as p.sh Δ= scsh(p), and the reduce
score of state p as p.re = scre(p). Similarly we
Δ
have the prefix score, inside score, shift score, and
reduce score for a derivation.
With this deductive system we extend the concept
of reducible states with the following definitions:
The set of all states with which a state p can
legally reduce from the right is denoted L(p), or left
states. (see Figure 3 (a)) We call any state q ∈ L(p)
a left state of p. Thus each element of this set would
have the following form:
</bodyText>
<equation confidence="0.9911725">
L(hi,j, sd...s0i) Δ={hh, i, s&apos;d...so)i |
fk(s�k−1)=fk(sk), ∀k ∈ [1, d]} (1)
</equation>
<bodyText confidence="0.999951333333333">
in which the span of the “left” state’s top tree ends
where that of the “right” state’s top tree begins, and
fk(sk) = fk(s�k−1) for all k ∈ [1, d].
Similarly, the set of all states with which a state p
can legally reduce from the left is denoted R(p), or
right states. (see Figure 3 (a)) For two states p, q,
</bodyText>
<equation confidence="0.394152">
p ∈ L(q) ⇔ q ∈ R(p)
</equation>
<subsectionHeader confidence="0.996276">
3.2 Algorithm 1
</subsectionHeader>
<bodyText confidence="0.997307888888889">
We constrain the searching time with a polynomial
bound by transforming the original search graph
with exponentially many derivations into a graph
with polynomial number of states.
In Algorithm 1, we maintain a chart C and a prior-
ity queue Q, both of which are based on hash tables.
Chart C can be formally defined as a function
mapping from the space of states to the space of
derivations:
</bodyText>
<equation confidence="0.84582">
C : S → D.
</equation>
<bodyText confidence="0.999959">
In practice, we use the atomic features �f(p) as the
signature of state p, since all derivations in the same
state share the same atomic features.
</bodyText>
<page confidence="0.756902">
760
</page>
<equation confidence="0.915959764705882">
re,,
sh
state p:
h , j, sd...s0i: (c, )
j &lt; n
hj,j + 1,sd−1 ... s0, wji: (c + ξ,0)
E s&apos; ...s&apos; s&apos; �s c+v+S v&apos; ) ��)
hk
j, d 1, 0 0) ( , +v+S
h , j, A → α.Bβi: (c, )
PRED B E G
hj, j, B → .γi : (c+ s, s) ( Y)
hk, i, A→α.Bβi: (c&apos;, v&apos;) hi, j, Bi: ( , v)
COMP
hk, j, A → αB.βi: (c&apos;+v, v&apos;+v)
state q: state p:
hk, i, s&apos;d...s&apos;0i: (c&apos;, v&apos;) hi, j, sd...s0i: ( , v)
</equation>
<figureCaption confidence="0.9933825">
Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting
re, case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ξ = scsh(p), δ = scsh(q) + sc, _(p),
s = sc(B → γ), G is the set of CFG rules, hi, j, Bi is a surrogate for any hi, j, B → γ.i, and is a wildcard that
matches anything.
</figureCaption>
<figure confidence="0.912213">
(a) L(p) and R(p) (b) T (p) = R(L(p))
</figure>
<figureCaption confidence="0.912886">
Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set
of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states
R(p) is the set of states that can be reduced with p so that p.s0 will be the left child of the top tree of the result state.
(b) Left corner states T (p) is the set of states that have the same reducibility as shifted state p, i.e., ∀p&apos; ∈ L(p), we
have ∀q ∈ T (p), q ∈ R(p&apos;). In both (a) and (b), thick sh arrow means shifts from multiple states; thin sh arrow means
shift from a single state.
</figureCaption>
<figure confidence="0.7511554">
p T (p)
p R(p)
L(p)
L(p)
. . .
. . .
. . .
. . .
sh sh
sh
</figure>
<bodyText confidence="0.977138595238095">
We use C [p] to retrieve the derivation in C that
is associated with state p. We sometimes abuse this
notation to say C [x] to retrieve the derivation asso-
ciated with signature _f(x) for derivation x. This is
fine since we know derivation x’s state immediately
from the signature. We say state p ∈ C if _f(p) is
associated with some derivation in C. A derivation
x ∈ C if C [x] = x. Chart C supports operation
PUSH, _denoted as C[x] ← x, which associate a sig-
nature f (x) with derivation x.
Priority queue Q is defined similarly as C, except
that it supports the operation POP that pops the high-
est priority item.
Following Stolcke (1995) and Nederhof (2003),
we use the prefix score and the inside score as the
priority in Q:
x ≺ y ⇔ x.pre &lt; y.pre or
(x.pre = y.pre and x.ins &lt; y.ins), (2)
Note that, for simplicity, we again ignore the spe-
cial case when two derivations have the same prefix
score and inside score. In practice for this case we
can pick either one of them. This will not affect the
correctness of our optimality proof in Section 5.1.
In the DP best-first parsing algorithm, once a
derivation x is popped from the priority queue Q,
as usual we try to expand it with shift and reduce.
Note that both left and right reduces are between
the derivation x of state p = [x]— and an in-chart
derivation y of left state q = [y]— ∈ L(p) (Line 10
of Algorithm 1), as shown in the deductive system
(Figure 2). We call this kind of reduction left expan-
sion.
We further expand derivation x of state p with
some in-chart derivation z of state r s.t. p ∈ L(r),
i.e., r ∈ R(p) as in Figure 3 (a). (see Line 11 of
Algorithm 1.) Derivation z is in the chart because it
is the descendant of some other derivation that has
been explored before x. We call this kind of reduc-
tion right expansion.
Our reduction with L and R is inspired by Neder-
hof (2003) and Knuth (1977) algorithm, which will
be discussed in Section 4.
</bodyText>
<page confidence="0.985003">
761
</page>
<table confidence="0.816957714285714">
Algorithm 1 Best-First DP Shift-Reduce Parsing.
Let LC(x) Δ= C[L([x]∼)] be in-chart derivations of
[x]∼’s left states
Let RC(x) = Δ C[R(p)] be in-chart derivations of
[x]∼’s right states
1: function PARSE(w0 ... w,,−1)
2: C 0 &gt; empty chart
</table>
<listItem confidence="0.780293666666667">
3: Q {INIT} &gt; initial priority queue
4: while Q =� 0 do
5: x +— POP(Q)
6: if GOAL(x) then return x &gt; found best parse
7: if [x]∼ E� C then
8: C[x] +— x &gt; add x to chart
9: SHIFT(x, Q)
10: REDUCE(LC (x), {x}, Q) &gt; left expansion
11: REDUCE({x}, RC (x), Q) &gt; right expansion
12: procedure SHIFT(x, Q)
13: TRYADD(sh(x), Q) &gt; shift
14: procedure REDUCE(A, B, Q)
15: for (x, y) E A x B do &gt; try all possible pairs
16: TRYADD(re,(x, y), Q) &gt; left reduce
17: TRYADD(rer,.(x, y), Q) &gt; right reduce
18: function TRYADD(x, Q)
19: if [x]∼ E� Q or x Q[x] then
20: Q[x] +— x &gt; insert x into Q or update Q[x]
</listItem>
<subsectionHeader confidence="0.974792">
3.3 Algorithm 2: Lazy Expansion
</subsectionHeader>
<bodyText confidence="0.999601428571429">
We further improve DP best-first parsing with lazy
expansion.
In Algorithm 2 we only show the parts that are
different from Algorithm 1.
Assume a shifted derivation x of state p is a direct
descendant from derivation x0 of state p0, then p E
R(p0), and we have:
</bodyText>
<equation confidence="0.592944">
bys.t. [y]∼ = q E REDUCE({p0}, R(p0)), x y
</equation>
<bodyText confidence="0.8629428">
which is proved in Section 5.1.
More formally, we can conclude that
bys.t. [y]∼ = q E REDUCE(L(p), T (p)), x y
where T (p) is the left corner states of shifted state
p, defined as
</bodyText>
<equation confidence="0.670611">
T((i, i+1, sd...s0)) Δ={(i, h, s0d...s00) �
fk(s0k)=fk(sk), bk E [1, d]}
</equation>
<bodyText confidence="0.9754055">
which represents the set of all states that have the
same reducibility as a shifted state p. In other words,
</bodyText>
<figure confidence="0.6876669">
T (p) = R(L(p)),
Algorithm 2 Lazy Expansion of Algorithm 1.
Let TC (x) Δ= C [T ([x]∼)] be in-chart derivations of
[x]∼’s left-corner states
1: function PARSE(w0 ... w,,−1)
2: C 0 &gt; empty chart
3: Q {INIT} &gt; initial priority queue
4: while Q =� 0 do
5: x +— POP(Q)
6: if GOAL(x) then return x &gt; found best parse
7: if [x]∼ E� C then
8: C[x] +— x &gt; add x to chart
9: SHIFT(x, Q)
10: REDUCE(x.lefts, {x}, Q) &gt; left expansion
11: else if x.action is sh then
12: REDUCE(x.lefts, TC (x), Q) &gt; right expan.
13: procedure SHIFT(x, Q)
14: y +— sh(x)
15: y.lefts +— {x} &gt; initialize lefts
16: TRYADD(y, Q)
17: function TRYADD(x, Q)
18: if [x]∼ E Q then
19: if x.action is sh then &gt; maintain lefts
20: y — Q[x]
21: if x y then Q[x] +— x
22: Q[x].lefts +— y.lefts U x.lefts
23: else if x Q[x] then
24: Q[x] x
25: else &gt; x E� Q
26: Q[x] +— x
</figure>
<bodyText confidence="0.9922206875">
which is illustrated in Figure 3 (a). Intuitively, T (p)
is the set of states that have p’s top tree, p.s0, which
contains only one node, as the left corner.
Based on this observation, we can safely delay the
REDUCE({x}, Rc(x)) operation (Line 11 in Algo-
rithm 1), until the derivation x of a shifted state is
popped out from Q. This helps us eliminate unnec-
essary right expansion.
We can delay even more derivations by extending
the concept of left corner states to reduced states.
Note that for any two states p, q, if q’s top tree q.s0
has p’s top tree p.s0 as left corner, and p, q share the
same left states, then derivations of p should always
have higher priority than derivations of q. We can
further delay the generation of q’s derivations until
p’s derivations are popped out.2
</bodyText>
<footnote confidence="0.901614">
2We did not implement this idea in experiments due to its
complexity.
</footnote>
<page confidence="0.987564">
762
</page>
<sectionHeader confidence="0.9800895" genericHeader="method">
4 Comparison with Best-First CKY and
Best-First Earley
</sectionHeader>
<subsectionHeader confidence="0.999648">
4.1 Best-First CKY and Knuth Algorithm
</subsectionHeader>
<bodyText confidence="0.999807166666667">
Vanilla CKY parsing can be viewed as searching
over a hypergraph(Klein and Manning, 2005), where
a hyperedge points from two nodes x, y to one node
z, if x, y can form a new partial tree represented by
z. Best-first CKY performs best-first search over
the hypergraph, which is a special application of the
Knuth Algorithm (Knuth, 1977).
Non-DP best-first shift-reduce parsing can be
viewed as searching over a graph. In this graph, a
node represents a derivation. A node points to all its
possible descendants generated from shift and left
and right reduces. This graph is actually a tree with
exponentially many nodes.
DP best-first parsing enables state merging on
the previous graph. Now the nodes in the hyper-
graph are not derivations, but equivalence classes of
derivations, i.e., states. The number of nodes in the
hypergraph is no longer always exponentially many,
but depends on the equivalence function, which is
the atomic feature function �f(·) in our algorithms.
DP best-first shift-reduce parsing is still a special
case of the Knuth algorithm. However, it is more dif-
ficult than best-first CKY parsing, because of the ex-
tra topological order constraints from shift actions.
</bodyText>
<subsectionHeader confidence="0.981303">
4.2 Best-First Earley
</subsectionHeader>
<bodyText confidence="0.998291090909091">
DP best-first shift-reduce parsing is analogous to
weighted Earley (Earley, 1970; Stolcke, 1995), be-
cause: 1) in Earley the PRED rule generates states
similar to shifted states in shift-reduce parsing; and,
2) a newly completed state also needs to check all
possible left expansions and right expansions, simi-
lar to a state popped from the priority queue in Al-
gorithm 1. (see Figure 2)
Our Algorithm 2 exploits lazy expansion, which
reduces unnecessary expansions, and should be
more efficient than pure Earley.
</bodyText>
<sectionHeader confidence="0.979406" genericHeader="method">
5 Optimality and Polynomial Complexity
</sectionHeader>
<subsectionHeader confidence="0.995152">
5.1 Proof of Optimality
</subsectionHeader>
<bodyText confidence="0.998878857142857">
We define a best derivation of state [x]— as a deriva-
tion x such that Vy E [x]—, x � y.
Note that each state has a unique feature signa-
ture. We want to prove that Algorithm 1 actually fills
the chart by assigning a best derivation to its state.
Without loss of generality, we assume Algorithm 1
fills C with derivations in the following order:
</bodyText>
<equation confidence="0.967776">
x0, x1, x2, ... , xm
</equation>
<bodyText confidence="0.9992782">
where x0 is the initial derivation, xm is the first goal
derivation in the sequence, and C[xz] = xZ, 0 &lt; i &lt;
m. Denote the status of chart right after xk being
filled as Ck. Specially, we define C_1 = 0
However, we do not have superiority as in non-DP
best-first parsing. Because we use a pair of prefix
score and inside score, (pre, ins), as priority (Equa-
tion 2) in the deductive system (Figure 2). We have
the following property as an alternative for superior-
ity:
</bodyText>
<construct confidence="0.50436825">
Lemma 1. After derivation xk has been filled into
chart, Vx s.t. x E Q, and x is a best derivation
of state [x]—, then x’s descendants can not have a
higher priority than xk.
</construct>
<listItem confidence="0.84105">
Proof. Note that when xk pops out, x is still in Q,
so xk � x. Assume z is x’s direct descendant.
• If z = sh(x) or z = re(x, ), based on the de-
ductive system, x --&lt; z, so xk _� x --&lt; z.
• If z = re(y, x), y E L(x), assume z --&lt; xk.
z.pre = y.pre + y.sh + x.ins + x.re
</listItem>
<bodyText confidence="0.949836">
We can construct a new derivation x&apos; ti x by
appending x’s top tree, x.s0 to y’s stack, and
</bodyText>
<equation confidence="0.652414333333333">
x&apos;.pre = y.pre + y.sh + x.ins &lt; z.pre
So x&apos; --&lt; z --&lt; xk _� x, which contradicts that x
is a best derivation of its state.
</equation>
<bodyText confidence="0.862423333333333">
With induction we can easily show that any descen-
dants of x can not have a higher priority than xk.
We can now derive:
Theorem 1 (Stepwise Completeness and Optimal-
ity). For any k, 0 &lt; k &lt; m, we have the following
two properties:
</bodyText>
<equation confidence="0.8049795">
Vx --&lt; xk, [x]— E Ck_1 (Stepwise Completeness)
Vx ti xk, xk _ x (Stepwise Optimality)
763
Proof. We prove by induction on k.
1. For k = 0, these two properties trivially hold.
k...i i...j
(h00, h0
) : (c0, v0) (h0, h
) : ( ,v)
(h00, h
</equation>
<bodyText confidence="0.47028">
2. Assume this theorem holds for k = 2, ..., i−1.
For k = i, we have:
</bodyText>
<equation confidence="0.632580666666667">
) : (c0 + v + λ, v0 + v + λ)
rem
a) [Prooffor Stepwise Completeness]
</equation>
<bodyText confidence="0.973164782608696">
(Proof by Contradiction) Assume Ix � xi
s.t. [x]∼ E� Ci−1.Without loss of generality we
take a best derivation of state [x]∼ as x. x must
be derived from other best derivations only.
Consider this derivation transition hypergraph,
which starts at initial derivation x0 E Ci−1, and
ends at x E� Ci−1.
There must be a best derivation x0 in this tran-
sition hypergraph, s.t. all best parent deriva-
tion(s) of x0 are in Ci−1, but not x0.
If x0 is a reduced derivation, assume x0’s best
parent derivations are y E Ci−1, z E Ci−1.
Because y and z are best derivations, and they
are in Ci−1, from Stepwise Optimality on k =
1, ..., i − 1,y, z E {x0, x1, ... , xi−1}. From
Line 7-11 in Algorithm 1, x0 must have been
pushed into Q when the latter of y, z is popped.
If x0 is a shifted derivation, similarly x0 must
have been pushed into Q.
As x0 E� Ci−1, x0 must still be in Q when xi is
popped. However, from Lemma 1, none of x0’s
descendants can have a higher priority than xi,
which contradicts x � xi.
</bodyText>
<equation confidence="0.306994">
b) [Prooffor Stepwise Optimality]
(Proof by Contradiction) Assume Ix — xi
s.t. x � xi. From Stepwise Completeness on
k = 1, ..., i, x E Ci−1, which means the state
</equation>
<bodyText confidence="0.99733275">
[xi]∼ has already been assigned to x, contra-
dicting the premise that xi is pushed into chart.
Both of the two properties have very intuitive
meanings. Stepwise Optimality means Algorithm 1
only fills chart with a best derivation for each state.
Stepwise Completeness means every state that has
its best derivation better than best derivation pi must
have been filled before pi, this guarantees that the
</bodyText>
<figure confidence="0.418689">
k...j
</figure>
<figureCaption confidence="0.905947333333333">
Figure 4: Example of shift-reduce with dynamic pro-
gramming: simulating an edge-factored model. GSS
is implicit here, and re, case omitted. Here λ =
scsh(h00, h0) + screr(h0, h).
global best goal derivation is captured by Algo-
rithm 1.
</figureCaption>
<bodyText confidence="0.99841175">
More formally we have:
Theorem 2 (Optimality of Algorithm 1). The irst
goal derivation popped off the priority queue is the
optimal parse.
Proof. (Proof by Contradiction.) Assume Ix, x is
the a goal derivation and x � xm. Based on Step-
wise Completeness of Theorem 1, x E Cm−1, thus x
has already been popped out, which contradicts that
xm is the first popped out goal derivation.
Furthermore, we can see our lazy expansion ver-
sion, i.e., Algorithm 2, is also optimal. The key ob-
servation is that we delay the reduction of derivation
x0 and a derivation of right states R([x0]∼) (Line 11
of Algorithm 1), until shifted derivation, x = sh(x0),
is popped out (Line 11 of Algorithm 2). However,
this delayed reduction will not generate any deriva-
tion y, s.t. y � x, because, based on our deduc-
tive system (Figure 2), for any such kind of reduced
derivations y, y.pre = x0.pre +x0.sh +y.re +y.ins,
while x.pre = x0.pre + x0.sh.
</bodyText>
<subsectionHeader confidence="0.999934">
5.2 Analysis of Time and Space Complexity
</subsectionHeader>
<bodyText confidence="0.9903304">
Following Huang and Sagae (2010) we present the
complexity analysis for our DP best-first parsing.
Theorem 3. Dynamic programming best-irst pars-
ing runs in worst-case polynomial time and space,
as long as the atomic features function satisies:
</bodyText>
<listItem confidence="0.999893666666667">
• bounded: b derivation x, I�f(x)I is bounded by
a constant.
• monotonic:
</listItem>
<page confidence="0.934488">
764
</page>
<equation confidence="0.987329857142857">
– horizontal: bk, fk(s) = fk(t) ==&gt;.
fk+1(s) = fk+1(t), for all possible trees
s, t.
– vertical: bk, fk(s&amp;quot;s&apos;) = fk(t&amp;quot;t&apos;) �
fk(s) = fk(t) and fk(s-&amp;quot;s&apos;) = fk(t&apos;t&apos;) �
fk(s&apos;) = fk(t&apos;), for all possible trees s, s&apos;,
t, t&apos;.
</equation>
<bodyText confidence="0.999979210526316">
In the above theorem, boundness means we can
only extract finite information from a derivation, so
that the atomic feature function �f(·) can only dis-
tinguish a finite number of different states. Mono-
tonicity requires the feature representation fk sub-
sumes fk+1. This is necessary because we use the
features as signature to match all possible left states
and right states (Equation 1). Note that we add the
vertical monotonicity condition following the sug-
gestion from Kuhlmann et al. (2011), which fixes
a flaw in the original theorem of Huang and Sagae
(2010).
We use the edge-factored model (Eisner, 1996;
McDonald et al., 2005) with dynamic programming
described in Figure 4 as a concrete example for com-
plexity analysis. In the edge-factored model the fea-
ture set consists of only combinations of informa-
tion from the roots of the two top trees s1, s0, and
the queue. So the atomic feature function is
</bodyText>
<equation confidence="0.879616">
f(p) = (2, j, h(p.s1), h(p.s0))
</equation>
<bodyText confidence="0.999985222222222">
where h(s) returns the head word index of tree s.
The deductive system for the edge-factored model
is in Figure 4. The time complexity for this deduc-
tive system is O(n6), because we have three head
indexes and three span indexes as free variables in
the exploration. Compared to the work of Huang
and Sagae (2010), we reduce the time complexity
from O(n7) to O(n6) because we do not need to
keep track of the number of the steps for a state.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999801125">
In experiments we compare our DP best-first parsing
with non-DP best-first parsing, pure greedy parsing,
and beam parser of Huang and Sagae (2010).
Our underlying MaxEnt model is trained on the
Penn Treebank (PTB) following the standard split:
Sections 02-21 as the training set and Section 22 as
the held-out set. We collect gold actions at differ-
ent parsing configurations as positive examples from
</bodyText>
<table confidence="0.993311">
model score accuracy # states time
greedy −1.4303 90.08% 125.8 0.0055
beam* −1.3302 90.60% 869.6 0.0331
non-DP −1.3269 90.70% 4,194.4 0.2622
DP −1.3269 90.70% 243.2 0.0132
</table>
<tableCaption confidence="0.895171333333333">
Table 1: Dynamic programming best-first parsing reach
optimality faster. *: for beam search we use beam size of
8. (All above results are averaged over the held-out set.)
</tableCaption>
<bodyText confidence="0.992856705882353">
gold parses in PTB to train the MaxEnt model. We
use the feature set of Huang and Sagae (2010).
Furthermore, we reimplemented the beam parser
with DP of Huang and Sagae (2010) for compari-
son. The result of our implementation is consistent
with theirs. We reach 92.39% accuracy with struc-
tured perceptron. However, in experiments we still
use MaxEnt to make the comparison fair.
To compare the performance we measure two sets
of criteria: 1) the internal criteria consist of the
model score of the parsing result, and the number
of states explored; 2) the external criteria consist of
the unlabeled accuracy of the parsing result, and the
parsing time.
We perform our experiments on a computer with
two 3.1GHz 8-core CPUs (16 processors in total)
and 64GB RAM. Our implementation is in Python.
</bodyText>
<subsectionHeader confidence="0.99976">
6.1 Search Quality &amp; Speed
</subsectionHeader>
<bodyText confidence="0.999964157894737">
We first compare DP best-first parsing algorithm
with pure greedy parsing and non-DP best-first pars-
ing without any extra constraints.
The results are shown in Table 1. Best-first pars-
ing reaches an accuracy of 90.70% in the held-out
set. Since that the MaxEnt model is locally trained,
this accuracy is not as high as the best shift-reduce
parsers available now. However, this is sufficient for
our comparison, because we aim at improving the
search quality and efficiency of parsing.
Compared to greedy parsing, DP best-first pars-
ing reaches a significantly higher accuracy, with —2
times more parsing time. Given the extra time in
maintaining priority queue, this is consistent with
the internal criteria: DP best-first parsing reaches a
significantly higher model score, which is actually
optimal, exploring twice as many as states.
On the other hand, non-DP best-first parsing also
achieves the optimal model score and accuracy.
</bodyText>
<page confidence="0.98791">
765
</page>
<figure confidence="0.996157">
0 10 20 30 40 50 60 70
sentence length
</figure>
<figureCaption confidence="0.958102666666667">
Figure 5: DP best-first significantly reduces parsing time.
Beam parser (beam size 8) guarantees linear parsing time.
Non-DP best-first parser is fast for short sentences, but
the time grows exponentially with sentence length. DP
best-first parser is as fast as non-DP for short sentences,
but the time grows significantly slower.
</figureCaption>
<bodyText confidence="0.9997676875">
However, it explores —17 times more states than DP,
with an unbearable average time.
Furthermore, on average our DP best-first parsing
is significantly faster than the beam parser, because
most sentences are short.
Figure 5 explains the inefficiency of non-DP best-
first parsing. As the time complexity grows expo-
nentially with the sentence length, non-DP best-first
parsing takes an extremely long time for long sen-
tences. DP best-first search has a polynomial time
bound, which grows significantly slower.
In general DP best-first parsing manages to reach
optimality in tractable time with exact search. To
further investigate the potential of this DP best-
first parsing, we perform inexact search experiments
with bounded priority queue.
</bodyText>
<subsectionHeader confidence="0.999964">
6.2 Parsing with Bounded Priority Queue
</subsectionHeader>
<bodyText confidence="0.999991166666667">
Bounded priority queue is a very practical choice
when we want to parse with only limited memory.
We bound the priority queue size at 1, 2, 5, 10,
20, 50, 100, 500, and 1000, and once the priority
queue size exceeds the bound, we discard the worst
one in the priority queue. The performances of non-
DP best-first parsing and DP best-first parsing are
illustrated in Figure 6 (a) (b).
Firstly, in Figure 6 (a), our DP best-first pars-
ing reaches the optimal model score with bound
50, while non-DP best-first parsing fails even with
bound 1000. Also, in average with bound 1000,
compared to non-DP, DP best-first only needs to ex-
plore less than half of the number of states.
Secondly, for external criteria in Figure 6 (b), both
algorithms reach accuracy of 90.70% in the end. In
speed, with bound 1000, DP best-first takes —1/3
time of non-DP to parse a sentence in average.
Lastly, we also compare to beam parser with beam
size 1, 2, 4, 8. Figure 6 (a) shows that beam parser
fails to reach the optimality, while exploring signif-
icantly more states. On the other hand, beam parser
also fails to reach an accuracy as high as best-first
parsers. (see Figure 6 (b))
</bodyText>
<subsectionHeader confidence="0.99787">
6.3 Simulating the Edge-Factored Model
</subsectionHeader>
<bodyText confidence="0.999983272727273">
We further explore the potential of DP best-first
parsing with the edge-factored model.
The simplified feature set of the edge-factored
model reduces the number of possible states, which
means more state-merging in the search graph. We
expect more significant improvement from our DP
best-first parsing in speed and number of explored
states.
Experiment results confirms this. In Figure 6 (c)
(d), curves of DP best-first diverge from non-DP
faster than standard model (Figure 6 (a) (b)).
</bodyText>
<sectionHeader confidence="0.996628" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999483533333333">
We have presented a dynamic programming algo-
rithm for best-first shift-reduce parsing which is
guaranteed to return the optimal solution in poly-
nomial time. This algorithm is related to best-first
Earley parsing, and is more sophisticated than best-
first CKY. Experiments have shown convincingly
that our algorithm leads to significant speedup over
the non-dynamic programming baseline, and makes
exact search tractable for the first-time under this
model.
For future work we would like to improve the per-
formance of the probabilistic models that is required
by the best-first search. We are also interested in
exploring A* heuristics to further speed up our DP
best-first parsing.
</bodyText>
<figure confidence="0.99391604477612">
0.12
0.08
0.06
0.04
0.02
0.1
0
non-DP
DP
beam
avg. parsing time (secs)
766
avg. model score on held-out
avg. accuracy (%) on held-out
90.8
90.6
90.4
90.2
90
0 100 200 300 400 500 600 700 800 900
# of states
0 0.01 0.02 0.03 0.04 0.05
parsing time (secs)
-1.35
-1.45
-1.3
-1.4
bound=50 bound=1000
non-DP
DP
beam
-1.3269
beam=8
bound=50
non-DP
DP
beam
90.70
beam=8
bound=1000
(a) search quality vs. # of states (b) parsing accuracy vs. time
0 100 200 300 400 500 600 700 800 900
# of states
0 0.01 0.02 0.03 0.04 0.05
parsing time (secs)
avg. model score on held-out
-1.24
-1.28
-1.32
-1.36
-1.4
bound=20 bound=500 beam=8
non-DP
DP
beam
-1.2565
avg. accuracy (%) on held-out
90.2
89.8
90
bound=20 bound=500
non-DP
DP
beam
90.25
beam=8
(c) search quality vs. # of states (edge-factored) (d) parsing accuracy vs. time (edge-factored)
</figure>
<figureCaption confidence="0.9206506">
Figure 6: Parsing performance comparison between DP and non-DP. (a) (b) Standard model with features of Huang
and Sagae (2010). (c) (d) Simulating edge-factored model with reduced feature set based on McDonald et al. (2005).
Note that to implement bounded priority queue we use two priority queues to keep track of the worst elements, which
introduces extra overhead, so that our bounded parser is slower than the unbounded version for large priority queue
size bound.
</figureCaption>
<page confidence="0.991287">
767
</page>
<sectionHeader confidence="0.993658" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999591980769231">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing of Series in Automatic Computation. Prentice
Hall, Englewood Cliffs, New Jersey.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24(2):275–298.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Dan Klein and Christopher D Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of HLT-NAACL.
Dan Klein and Christopher D Manning. 2005. Pars-
ing and hypergraphs. In New developments in parsing
technology, pages 351–372. Springer.
Donald Knuth. 1977. A generalization of Dijkstra’s al-
gorithm. Information Processing Letters, 6(1).
Marco Kuhlmann, Carlos G´omez-Rodriguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Computational Linguis-
tics, pages 135–143.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513–553.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 557–565. Association for
Computational Linguistics.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of ACL.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165–
201.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP.
</reference>
<page confidence="0.996251">
768
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579107">
<title confidence="0.596852">Incremental Parsing via Best-First Dynamic</title>
<affiliation confidence="0.989278">City University of New</affiliation>
<address confidence="0.996">365 Fifth Avenue, New York, NY</address>
<abstract confidence="0.999162785714286">We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="5676" citStr="Aho and Ullman, 1972" startWordPosition="920" endWordPosition="923">al models like structured perceptron. With that being said, our algorithm shows its own merits in both theory and practice. To find a better model for best-first search would be an interesting topic for future work. 2 Shift-Reduce and Best-First Parsing In this section we review the basics of shift-reduce parsing, beam search, and the best-first shift-reduce parsing algorithm of Sagae and Lavie (2006). 2.1 Shift-Reduce Parsing and Beam Search Due to space constraints we will assume some basic familiarity with shift-reduce parsing; see Nivre (2008) for details. Basically, shift-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reduce, i.e., combine the top two trees on stack, either with left as the root or right as the root. This scheme is often called “arc-standard” in the literature (Nivre, 2008), and is the basis of several stateof-the-art parsers, e.g. Huang and Sagae (2010). See Figure 1 for the deductive system of shift-reduce dependency parsing. To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b derivati</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation. Prentice Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="902" citStr="Caraballo and Charniak, 1998" startWordPosition="130" endWordPosition="133"> best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intra</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon A Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, 24(2):275–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="2169" citStr="Earley (1970)" startWordPosition="322" endWordPosition="323">onentially large space in the worst case, a bounded priority queue becomes necessary to ensure limited parsing time. *This work is mainly supported by DARPA FA8750-13-2- 0041 (DEFT), a Google Faculty Research Award, and a PSCCUNY Award. In addition, we thank Kenji Sagae and the anonymous reviewers for their constructive comments. Liang Huang1,2 2Queens College City University of New York 6530 Kissena Blvd, Queens, NY 11367 huang@cs.qc.cuny.edu On the other hand, Huang and Sagae (2010) explore the idea of dynamic programming, which is originated in bottom-up constituent parsing algorithms like Earley (1970), but in a beam-based non best-first parser. In each beam step, they enable state merging in a style similar to the dynamic programming in bottom-up constituent parsing, based on an equivalence relation defined upon feature values. Although in theory they successfully reduced the underlying deductive system to polynomial time complexity, their merging method is limited in that the state merging is only between two states in the same beam step. This significantly reduces the number of possible merges, because: 1) there are only a very limited number of states in the beam at the same time; 2) a </context>
<context position="19591" citStr="Earley, 1970" startWordPosition="3541" endWordPosition="3542">s graph. Now the nodes in the hypergraph are not derivations, but equivalence classes of derivations, i.e., states. The number of nodes in the hypergraph is no longer always exponentially many, but depends on the equivalence function, which is the atomic feature function �f(·) in our algorithms. DP best-first shift-reduce parsing is still a special case of the Knuth algorithm. However, it is more difficult than best-first CKY parsing, because of the extra topological order constraints from shift actions. 4.2 Best-First Earley DP best-first shift-reduce parsing is analogous to weighted Earley (Earley, 1970; Stolcke, 1995), because: 1) in Earley the PRED rule generates states similar to shifted states in shift-reduce parsing; and, 2) a newly completed state also needs to check all possible left expansions and right expansions, similar to a state popped from the priority queue in Algorithm 1. (see Figure 2) Our Algorithm 2 exploits lazy expansion, which reduces unnecessary expansions, and should be more efficient than pure Earley. 5 Optimality and Polynomial Complexity 5.1 Proof of Optimality We define a best derivation of state [x]— as a derivation x such that Vy E [x]—, x � y. Note that each st</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="26096" citStr="Eisner, 1996" startWordPosition="4759" endWordPosition="4760">t&apos;. In the above theorem, boundness means we can only extract finite information from a derivation, so that the atomic feature function �f(·) can only distinguish a finite number of different states. Monotonicity requires the feature representation fk subsumes fk+1. This is necessary because we use the features as signature to match all possible left states and right states (Equation 1). Note that we add the vertical monotonicity condition following the suggestion from Kuhlmann et al. (2011), which fixes a flaw in the original theorem of Huang and Sagae (2010). We use the edge-factored model (Eisner, 1996; McDonald et al., 2005) with dynamic programming described in Figure 4 as a concrete example for complexity analysis. In the edge-factored model the feature set consists of only combinations of information from the roots of the two top trees s1, s0, and the queue. So the atomic feature function is f(p) = (2, j, h(p.s1), h(p.s0)) where h(s) returns the head word index of tree s. The deductive system for the edge-factored model is in Figure 4. The time complexity for this deductive system is O(n6), because we have three head indexes and three span indexes as free variables in the exploration. C</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2045" citStr="Huang and Sagae (2010)" startWordPosition="301" endWordPosition="304">ires exponential time for very long sentences in the worst case, which is intractable in practice. Because it needs to explore an exponentially large space in the worst case, a bounded priority queue becomes necessary to ensure limited parsing time. *This work is mainly supported by DARPA FA8750-13-2- 0041 (DEFT), a Google Faculty Research Award, and a PSCCUNY Award. In addition, we thank Kenji Sagae and the anonymous reviewers for their constructive comments. Liang Huang1,2 2Queens College City University of New York 6530 Kissena Blvd, Queens, NY 11367 huang@cs.qc.cuny.edu On the other hand, Huang and Sagae (2010) explore the idea of dynamic programming, which is originated in bottom-up constituent parsing algorithms like Earley (1970), but in a beam-based non best-first parser. In each beam step, they enable state merging in a style similar to the dynamic programming in bottom-up constituent parsing, based on an equivalence relation defined upon feature values. Although in theory they successfully reduced the underlying deductive system to polynomial time complexity, their merging method is limited in that the state merging is only between two states in the same beam step. This significantly reduces t</context>
<context position="6065" citStr="Huang and Sagae (2010)" startWordPosition="992" endWordPosition="995">and Lavie (2006). 2.1 Shift-Reduce Parsing and Beam Search Due to space constraints we will assume some basic familiarity with shift-reduce parsing; see Nivre (2008) for details. Basically, shift-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reduce, i.e., combine the top two trees on stack, either with left as the root or right as the root. This scheme is often called “arc-standard” in the literature (Nivre, 2008), and is the basis of several stateof-the-art parsers, e.g. Huang and Sagae (2010). See Figure 1 for the deductive system of shift-reduce dependency parsing. To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b derivations develop in parallel. At each step we extend the derivations in the current beam by applying each of the three actions, and then choose the best b resulting derivations for the next step. 2.2 Best-First Shift-Reduce Parsing Sagae and Lavie (2006) present the parsing problem as a search problem over a DAG, in which each parser derivation is denoted as a node, and an edge from node x t</context>
<context position="8131" citStr="Huang and Sagae (2010)" startWordPosition="1344" endWordPosition="1347">ithm. However, it explores exponentially many derivations to reach the goal configuration in the worst case. We propose a new method that has polynomial time complexity even in the worst case. 1For simplicity we ignore the case when two derivations have the same score. In practice we can choose either one of the two derivations when they have the same score. sh re, re, 759 3 Dynamic Programming for Best-First Shift-Reduce Parsing 3.1 Dynamic Programming Notations The key innovation of this paper is to extend bestfirst parsing with the “state-merging” method of dynamic programming described in Huang and Sagae (2010). We start with describing a parsing configuration as a non-DP derivation: hi, j, ...s2s1s0i, where ...s2s1s0 is the stack of partial trees, [i..j] is the span of the top tree s0, and s1s2... are the remainder of the trees on the stack. The notation fk(sk) is used to indicate the features used by the parser from the tree sk on the stack. Note that the parser only extracts features from the top d+1 trees on the stack. Following Huang and Sagae (2010), f(x) of a derivation x is called atomic features, defined as the smallest set of features s.t. �f(i, j, ...s2s1s0) = f(i, j, ...s�2s01s00) ⇔ fk(s</context>
<context position="9637" citStr="Huang and Sagae (2010)" startWordPosition="1620" endWordPosition="1623"> the same equivalence class, and their behaviors are similar in shift and reduce. We call each equivalence class a DP state. More formally we define the space of all states S as: S Δ= D/∼. Since only the top d+1 trees on the stack are used in atomic features, we only need to remember the necessary information and write the state as: hi, j, sd...s0i. We denote a derivation x’s state as [x]—. In the rest of this paper, we always denote derivations with letters x, y, and z, and denote states with letters p, q, and r. The deductive system for dynamic programming best-first parsing is adapted from Huang and Sagae (2010). (See the left of Figure 2.) The difference is that we do not distinguish the step index of a state. This deductive system describes transitions between states. However, in practice we use one state’s best derivation found so far to represent the state. For each state p, we calculate the prefix score, p.pre, which is the score of the derivation to reach this state, and the inside score, p.ins, which is the score of p’s top tree p.s0. In addition we denote the shift score of state p as p.sh Δ= scsh(p), and the reduce score of state p as p.re = scre(p). Similarly we Δ have the prefix score, ins</context>
<context position="12059" citStr="Huang and Sagae, 2010" startWordPosition="2100" endWordPosition="2103">ations: C : S → D. In practice, we use the atomic features �f(p) as the signature of state p, since all derivations in the same state share the same atomic features. 760 re,, sh state p: h , j, sd...s0i: (c, ) j &lt; n hj,j + 1,sd−1 ... s0, wji: (c + ξ,0) E s&apos; ...s&apos; s&apos; �s c+v+S v&apos; ) ��) hk j, d 1, 0 0) ( , +v+S h , j, A → α.Bβi: (c, ) PRED B E G hj, j, B → .γi : (c+ s, s) ( Y) hk, i, A→α.Bβi: (c&apos;, v&apos;) hi, j, Bi: ( , v) COMP hk, j, A → αB.βi: (c&apos;+v, v&apos;+v) state q: state p: hk, i, s&apos;d...s&apos;0i: (c&apos;, v&apos;) hi, j, sd...s0i: ( , v) Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting re, case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ξ = scsh(p), δ = scsh(q) + sc, _(p), s = sc(B → γ), G is the set of CFG rules, hi, j, Bi is a surrogate for any hi, j, B → γ.i, and is a wildcard that matches anything. (a) L(p) and R(p) (b) T (p) = R(L(p)) Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states R(p) is the set of states that can be reduced wit</context>
<context position="24980" citStr="Huang and Sagae (2010)" startWordPosition="4569" endWordPosition="4572">ion. Furthermore, we can see our lazy expansion version, i.e., Algorithm 2, is also optimal. The key observation is that we delay the reduction of derivation x0 and a derivation of right states R([x0]∼) (Line 11 of Algorithm 1), until shifted derivation, x = sh(x0), is popped out (Line 11 of Algorithm 2). However, this delayed reduction will not generate any derivation y, s.t. y � x, because, based on our deductive system (Figure 2), for any such kind of reduced derivations y, y.pre = x0.pre +x0.sh +y.re +y.ins, while x.pre = x0.pre + x0.sh. 5.2 Analysis of Time and Space Complexity Following Huang and Sagae (2010) we present the complexity analysis for our DP best-first parsing. Theorem 3. Dynamic programming best-irst parsing runs in worst-case polynomial time and space, as long as the atomic features function satisies: • bounded: b derivation x, I�f(x)I is bounded by a constant. • monotonic: 764 – horizontal: bk, fk(s) = fk(t) ==&gt;. fk+1(s) = fk+1(t), for all possible trees s, t. – vertical: bk, fk(s&amp;quot;s&apos;) = fk(t&amp;quot;t&apos;) � fk(s) = fk(t) and fk(s-&amp;quot;s&apos;) = fk(t&apos;t&apos;) � fk(s&apos;) = fk(t&apos;), for all possible trees s, s&apos;, t, t&apos;. In the above theorem, boundness means we can only extract finite information from a derivati</context>
<context position="26741" citStr="Huang and Sagae (2010)" startWordPosition="4872" endWordPosition="4875">) with dynamic programming described in Figure 4 as a concrete example for complexity analysis. In the edge-factored model the feature set consists of only combinations of information from the roots of the two top trees s1, s0, and the queue. So the atomic feature function is f(p) = (2, j, h(p.s1), h(p.s0)) where h(s) returns the head word index of tree s. The deductive system for the edge-factored model is in Figure 4. The time complexity for this deductive system is O(n6), because we have three head indexes and three span indexes as free variables in the exploration. Compared to the work of Huang and Sagae (2010), we reduce the time complexity from O(n7) to O(n6) because we do not need to keep track of the number of the steps for a state. 6 Experiments In experiments we compare our DP best-first parsing with non-DP best-first parsing, pure greedy parsing, and beam parser of Huang and Sagae (2010). Our underlying MaxEnt model is trained on the Penn Treebank (PTB) following the standard split: Sections 02-21 as the training set and Section 22 as the held-out set. We collect gold actions at different parsing configurations as positive examples from model score accuracy # states time greedy −1.4303 90.08%</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="927" citStr="Klein and Manning, 2003" startWordPosition="134" endWordPosition="137">ng, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice. Becau</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs. In New developments in parsing technology,</title>
<date>2005</date>
<pages>351--372</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="18390" citStr="Klein and Manning, 2005" startWordPosition="3345" endWordPosition="3348">xtending the concept of left corner states to reduced states. Note that for any two states p, q, if q’s top tree q.s0 has p’s top tree p.s0 as left corner, and p, q share the same left states, then derivations of p should always have higher priority than derivations of q. We can further delay the generation of q’s derivations until p’s derivations are popped out.2 2We did not implement this idea in experiments due to its complexity. 762 4 Comparison with Best-First CKY and Best-First Earley 4.1 Best-First CKY and Knuth Algorithm Vanilla CKY parsing can be viewed as searching over a hypergraph(Klein and Manning, 2005), where a hyperedge points from two nodes x, y to one node z, if x, y can form a new partial tree represented by z. Best-first CKY performs best-first search over the hypergraph, which is a special application of the Knuth Algorithm (Knuth, 1977). Non-DP best-first shift-reduce parsing can be viewed as searching over a graph. In this graph, a node represents a derivation. A node points to all its possible descendants generated from shift and left and right reduces. This graph is actually a tree with exponentially many nodes. DP best-first parsing enables state merging on the previous graph. No</context>
</contexts>
<marker>Klein, Manning, 2005</marker>
<rawString>Dan Klein and Christopher D Manning. 2005. Parsing and hypergraphs. In New developments in parsing technology, pages 351–372. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Knuth</author>
</authors>
<title>A generalization of Dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="14929" citStr="Knuth (1977)" startWordPosition="2679" endWordPosition="2680"> derivation x of state p = [x]— and an in-chart derivation y of left state q = [y]— ∈ L(p) (Line 10 of Algorithm 1), as shown in the deductive system (Figure 2). We call this kind of reduction left expansion. We further expand derivation x of state p with some in-chart derivation z of state r s.t. p ∈ L(r), i.e., r ∈ R(p) as in Figure 3 (a). (see Line 11 of Algorithm 1.) Derivation z is in the chart because it is the descendant of some other derivation that has been explored before x. We call this kind of reduction right expansion. Our reduction with L and R is inspired by Nederhof (2003) and Knuth (1977) algorithm, which will be discussed in Section 4. 761 Algorithm 1 Best-First DP Shift-Reduce Parsing. Let LC(x) Δ= C[L([x]∼)] be in-chart derivations of [x]∼’s left states Let RC(x) = Δ C[R(p)] be in-chart derivations of [x]∼’s right states 1: function PARSE(w0 ... w,,−1) 2: C 0 &gt; empty chart 3: Q {INIT} &gt; initial priority queue 4: while Q =� 0 do 5: x +— POP(Q) 6: if GOAL(x) then return x &gt; found best parse 7: if [x]∼ E� C then 8: C[x] +— x &gt; add x to chart 9: SHIFT(x, Q) 10: REDUCE(LC (x), {x}, Q) &gt; left expansion 11: REDUCE({x}, RC (x), Q) &gt; right expansion 12: procedure SHIFT(x, Q) 13: TRY</context>
<context position="18636" citStr="Knuth, 1977" startWordPosition="3391" endWordPosition="3392">ivations of q. We can further delay the generation of q’s derivations until p’s derivations are popped out.2 2We did not implement this idea in experiments due to its complexity. 762 4 Comparison with Best-First CKY and Best-First Earley 4.1 Best-First CKY and Knuth Algorithm Vanilla CKY parsing can be viewed as searching over a hypergraph(Klein and Manning, 2005), where a hyperedge points from two nodes x, y to one node z, if x, y can form a new partial tree represented by z. Best-first CKY performs best-first search over the hypergraph, which is a special application of the Knuth Algorithm (Knuth, 1977). Non-DP best-first shift-reduce parsing can be viewed as searching over a graph. In this graph, a node represents a derivation. A node points to all its possible descendants generated from shift and left and right reduces. This graph is actually a tree with exponentially many nodes. DP best-first parsing enables state merging on the previous graph. Now the nodes in the hypergraph are not derivations, but equivalence classes of derivations, i.e., states. The number of nodes in the hypergraph is no longer always exponentially many, but depends on the equivalence function, which is the atomic fe</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>Donald Knuth. 1977. A generalization of Dijkstra’s algorithm. Information Processing Letters, 6(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Carlos G´omez-Rodriguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Kuhlmann, G´omez-Rodriguez, Satta, 2011</marker>
<rawString>Marco Kuhlmann, Carlos G´omez-Rodriguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="26120" citStr="McDonald et al., 2005" startWordPosition="4761" endWordPosition="4764">ve theorem, boundness means we can only extract finite information from a derivation, so that the atomic feature function �f(·) can only distinguish a finite number of different states. Monotonicity requires the feature representation fk subsumes fk+1. This is necessary because we use the features as signature to match all possible left states and right states (Equation 1). Note that we add the vertical monotonicity condition following the suggestion from Kuhlmann et al. (2011), which fixes a flaw in the original theorem of Huang and Sagae (2010). We use the edge-factored model (Eisner, 1996; McDonald et al., 2005) with dynamic programming described in Figure 4 as a concrete example for complexity analysis. In the edge-factored model the feature set consists of only combinations of information from the roots of the two top trees s1, s0, and the queue. So the atomic feature function is f(p) = (2, j, h(p.s1), h(p.s0)) where h(s) returns the head word index of tree s. The deductive system for the edge-factored model is in Figure 4. The time complexity for this deductive system is O(n6), because we have three head indexes and three span indexes as free variables in the exploration. Compared to the work of H</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>135--143</pages>
<contexts>
<context position="13721" citStr="Nederhof (2003)" startWordPosition="2442" endWordPosition="2443">C that is associated with state p. We sometimes abuse this notation to say C [x] to retrieve the derivation associated with signature _f(x) for derivation x. This is fine since we know derivation x’s state immediately from the signature. We say state p ∈ C if _f(p) is associated with some derivation in C. A derivation x ∈ C if C [x] = x. Chart C supports operation PUSH, _denoted as C[x] ← x, which associate a signature f (x) with derivation x. Priority queue Q is defined similarly as C, except that it supports the operation POP that pops the highest priority item. Following Stolcke (1995) and Nederhof (2003), we use the prefix score and the inside score as the priority in Q: x ≺ y ⇔ x.pre &lt; y.pre or (x.pre = y.pre and x.ins &lt; y.ins), (2) Note that, for simplicity, we again ignore the special case when two derivations have the same prefix score and inside score. In practice for this case we can pick either one of them. This will not affect the correctness of our optimality proof in Section 5.1. In the DP best-first parsing algorithm, once a derivation x is popped from the priority queue Q, as usual we try to expand it with shift and reduce. Note that both left and right reduces are between the der</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, pages 135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="5608" citStr="Nivre (2008)" startWordPosition="913" endWordPosition="914">ve that this locally trained model is not as strong as global models like structured perceptron. With that being said, our algorithm shows its own merits in both theory and practice. To find a better model for best-first search would be an interesting topic for future work. 2 Shift-Reduce and Best-First Parsing In this section we review the basics of shift-reduce parsing, beam search, and the best-first shift-reduce parsing algorithm of Sagae and Lavie (2006). 2.1 Shift-Reduce Parsing and Beam Search Due to space constraints we will assume some basic familiarity with shift-reduce parsing; see Nivre (2008) for details. Basically, shift-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reduce, i.e., combine the top two trees on stack, either with left as the root or right as the root. This scheme is often called “arc-standard” in the literature (Nivre, 2008), and is the basis of several stateof-the-art parsers, e.g. Huang and Sagae (2010). See Figure 1 for the deductive system of shift-reduce dependency parsing. To improve on strictly greedy search, shift-reduce parsing is often</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Hierarchical search for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>557--565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="951" citStr="Pauls and Klein, 2009" startWordPosition="138" endWordPosition="141">idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice. Because it needs to explore a</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. Hierarchical search for parsing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 557–565. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1209" citStr="Sagae and Lavie (2006)" startWordPosition="174" endWordPosition="177">ads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time. 1 Introduction Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009). Traditional CKY parsing performs cubic time exact search over an exponentially large space. Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities. In terms of incremental parsing, Sagae and Lavie (2006) is the first work to extend best-first search to shift-reduce constituent parsing. Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice. Because it needs to explore an exponentially large space in the worst case, a bounded priority queue becomes necessary to ensure limited parsing time. *This work is mainly supported by DARPA FA8750-13-2- 0041 (DEFT), a Google Faculty Research Award, and a PSCCUNY Award. In addition, we </context>
<context position="5459" citStr="Sagae and Lavie (2006)" startWordPosition="888" endWordPosition="891"> priority queue size bound, compared to non-DP bestfirst parser. Our system is based on a MaxEnt model to meet the requirement from best-first search. We observe that this locally trained model is not as strong as global models like structured perceptron. With that being said, our algorithm shows its own merits in both theory and practice. To find a better model for best-first search would be an interesting topic for future work. 2 Shift-Reduce and Best-First Parsing In this section we review the basics of shift-reduce parsing, beam search, and the best-first shift-reduce parsing algorithm of Sagae and Lavie (2006). 2.1 Shift-Reduce Parsing and Beam Search Due to space constraints we will assume some basic familiarity with shift-reduce parsing; see Nivre (2008) for details. Basically, shift-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reduce, i.e., combine the top two trees on stack, either with left as the root or right as the root. This scheme is often called “arc-standard” in the literature (Nivre, 2008), and is the basis of several stateof-the-art parsers, e.g. Huang and Sagae </context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>201</pages>
<contexts>
<context position="12138" citStr="Stolcke, 1995" startWordPosition="2113" endWordPosition="2114">ate p, since all derivations in the same state share the same atomic features. 760 re,, sh state p: h , j, sd...s0i: (c, ) j &lt; n hj,j + 1,sd−1 ... s0, wji: (c + ξ,0) E s&apos; ...s&apos; s&apos; �s c+v+S v&apos; ) ��) hk j, d 1, 0 0) ( , +v+S h , j, A → α.Bβi: (c, ) PRED B E G hj, j, B → .γi : (c+ s, s) ( Y) hk, i, A→α.Bβi: (c&apos;, v&apos;) hi, j, Bi: ( , v) COMP hk, j, A → αB.βi: (c&apos;+v, v&apos;+v) state q: state p: hk, i, s&apos;d...s&apos;0i: (c&apos;, v&apos;) hi, j, sd...s0i: ( , v) Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting re, case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ξ = scsh(p), δ = scsh(q) + sc, _(p), s = sc(B → γ), G is the set of CFG rules, hi, j, Bi is a surrogate for any hi, j, B → γ.i, and is a wildcard that matches anything. (a) L(p) and R(p) (b) T (p) = R(L(p)) Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states R(p) is the set of states that can be reduced with p so that p.s0 will be the left child of the top tree of the result state. (b</context>
<context position="13701" citStr="Stolcke (1995)" startWordPosition="2439" endWordPosition="2440"> the derivation in C that is associated with state p. We sometimes abuse this notation to say C [x] to retrieve the derivation associated with signature _f(x) for derivation x. This is fine since we know derivation x’s state immediately from the signature. We say state p ∈ C if _f(p) is associated with some derivation in C. A derivation x ∈ C if C [x] = x. Chart C supports operation PUSH, _denoted as C[x] ← x, which associate a signature f (x) with derivation x. Priority queue Q is defined similarly as C, except that it supports the operation POP that pops the highest priority item. Following Stolcke (1995) and Nederhof (2003), we use the prefix score and the inside score as the priority in Q: x ≺ y ⇔ x.pre &lt; y.pre or (x.pre = y.pre and x.ins &lt; y.ins), (2) Note that, for simplicity, we again ignore the special case when two derivations have the same prefix score and inside score. In practice for this case we can pick either one of them. This will not affect the correctness of our optimality proof in Section 5.1. In the DP best-first parsing algorithm, once a derivation x is popped from the priority queue Q, as usual we try to expand it with shift and reduce. Note that both left and right reduces</context>
<context position="19607" citStr="Stolcke, 1995" startWordPosition="3543" endWordPosition="3544">he nodes in the hypergraph are not derivations, but equivalence classes of derivations, i.e., states. The number of nodes in the hypergraph is no longer always exponentially many, but depends on the equivalence function, which is the atomic feature function �f(·) in our algorithms. DP best-first shift-reduce parsing is still a special case of the Knuth algorithm. However, it is more difficult than best-first CKY parsing, because of the extra topological order constraints from shift actions. 4.2 Best-First Earley DP best-first shift-reduce parsing is analogous to weighted Earley (Earley, 1970; Stolcke, 1995), because: 1) in Earley the PRED rule generates states similar to shifted states in shift-reduce parsing; and, 2) a newly completed state also needs to check all possible left expansions and right expansions, similar to a state popped from the priority queue in Algorithm 1. (see Figure 2) Our Algorithm 2 exploits lazy expansion, which reduces unnecessary expansions, and should be more efficient than pure Earley. 5 Optimality and Polynomial Complexity 5.1 Proof of Optimality We define a best derivation of state [x]— as a derivation x such that Vy E [x]—, x � y. Note that each state has a unique</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6258" citStr="Zhang and Clark, 2008" startWordPosition="1022" endWordPosition="1025">ft-reduce parsing (Aho and Ullman, 1972) performs a left-to-right scan of the input sentence, and at each step, chooses either to shift the next word onto the stack, or to reduce, i.e., combine the top two trees on stack, either with left as the root or right as the root. This scheme is often called “arc-standard” in the literature (Nivre, 2008), and is the basis of several stateof-the-art parsers, e.g. Huang and Sagae (2010). See Figure 1 for the deductive system of shift-reduce dependency parsing. To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b derivations develop in parallel. At each step we extend the derivations in the current beam by applying each of the three actions, and then choose the best b resulting derivations for the next step. 2.2 Best-First Shift-Reduce Parsing Sagae and Lavie (2006) present the parsing problem as a search problem over a DAG, in which each parser derivation is denoted as a node, and an edge from node x to node y exists if and only if the corresponding derivation y can be generated from derivation x by applying one action. The best-first parsing algorithm is an application of the Dijkstra algor</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
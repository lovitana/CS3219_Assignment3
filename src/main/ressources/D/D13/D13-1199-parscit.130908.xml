<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.982108">
Identifying Manipulated Offerings on Review Portals
</title>
<author confidence="0.998865">
Jiwei Li
</author>
<affiliation confidence="0.944666666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993915">
jiweil@cs.cmu.edu
</email>
<author confidence="0.987641">
Myle Ott Claire Cardie
</author>
<affiliation confidence="0.937588666666667">
Department of Computer Science
Cornell University
Ithaca, NY 14853, USA
</affiliation>
<email confidence="0.997489">
myleott,cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935263157895">
Recent work has developed supervised meth-
ods for detecting deceptive opinion spam—
fake reviews written to sound authentic and
deliberately mislead readers. And whereas
past work has focused on identifying individ-
ual fake reviews, this paper aims to identify
offerings (e.g., hotels) that contain fake re-
views. We introduce a semi-supervised man-
ifold ranking algorithm for this task, which
relies on a small set of labeled individual re-
views for training. Then, in the absence of
gold standard labels (at an offering level),
we introduce a novel evaluation procedure
that ranks artificial instances of real offer-
ings, where each artificial offering contains a
known number of injected deceptive reviews.
Experiments on a novel dataset of hotel re-
views show that the proposed method outper-
forms state-of-art learning baselines.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945708333334">
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sions (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, potentially
anonymously, combined with the public’s trust and
growing reliance on opinions and other information
found online, create opportunities and incentives for
unscrupulous businesses to post deceptive opinion
spam—fraudulent or fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader (Ott et al, 2011).
Unlike other kinds of spam, such as
Web (Martinez-Romo and Araujo, 2009; Castillo
et al, 2006) and e-mail spam (Chirita et al, 2005),
recent work has found that deceptive opinion spam
is neither easily ignored nor easily identified by
human readers (Ott et al, 2011). Accordingly, there
is growing interest in developing automatic, usually
learning-based, methods to help users identify
deceptive opinion spam (see Section 2). Even
in fully-supervised settings, however, automatic
methods are imperfect at identifying individual
deceptive reviews, and erroneously labeling genuine
reviews as deceptive may frustrate and alienate
honest reviewers.
An alternative approach, not yet considered in
previous work, is to instead identify those prod-
uct or service offerings where fake reviews appear
with high probability. For example, a hotel manager
may post fake positive reviews to promote their own
hotel, or fake negative reviews to demote a com-
petitor’s hotel. In both cases, rather than identify-
ing these deceptive reviews individually, it may be
preferable to identify the manipulated offering (i.e.,
the hotel) so that review portal operators, such as
TripAdvisor or Yelp, can further investigate the sit-
uation without alienating users.1
Accordingly, this paper addresses the novel task
of identifying manipulated offerings, which we
frame as a ranking problem, where the goal is to rank
offerings by the proportion of their reviews that are
believed to be deceptive. We propose a novel three-
layer graph model, based on manifold ranking (Zhou
et al, 2003a; 2003b), to jointly model deceptive lan-
guage at the offering-, review- and term-level. In
particular, rather than treating reviews within the
same offering as independent units, there is a rein-
forcing relationship between offerings and reviews.
</bodyText>
<footnote confidence="0.832814">
1Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive re-
views may be unlawful in the United States (FTC, 2009).
</footnote>
<page confidence="0.729685">
1933
</page>
<note confidence="0.8810515">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1933–1942,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9466905">
Figure 1: Mutual Reinforcement Graph Model for Hotel
Ranking using the Manifold-Ranking Method
</figureCaption>
<bodyText confidence="0.99981652631579">
Our manifold ranking approach is semi-
supervised in that it requires no supervisory
information at the offering level; rather, it requires
only a small amount of labeled data at a review
level. Intuitively, and as depicted in Figure 1 for
hotel offerings, we represent hotels, reviews and
terms as nodes in a graph, where each hotel is
connected to its reviews, and each review, in turn, is
connected to the terms used within it. The influence
of labeled data is propagated along the graph to
unlabeled data, such that a hotel is considered more
deceptive if it is heavily linked with other deceptive
reviews, and a review, in turn, is more deceptive if it
is generated by a deceptive hotel.
The success of our semi-supervised approach fur-
ther depends on the ability to learn patterns of truth-
ful and deceptive reviews that generalize across re-
views of different offerings. This is challenging, be-
cause reviews often contain offering-specific vocab-
ulary. For example, reviews of hotels in Los Angeles
are more likely to include keywords such as “beach”,
“sea”, “sunshine” or “LA”, while reviews of Juneau
hotels may contain “glacier”, “Juneau”, “bear” or
“aurora borealis.” A hotel review might also men-
tion the hotel’s restaurant or bar by name.
Unfortunately, it is unclear how important (or
detrimental) offering-specific features are when de-
ciding whether a review is fake. Accordingly, we
propose a dimensionality-reduction approach, based
on Latent Dirichlet Allocation (LDA) (Blei et al,
2003), to obtain a vector representation of reviews
for the ranking algorithm that generalizes across re-
views of different offerings. Specifically, we train
an LDA-based topic model to view each review as a
mixture of aspect-, city-, hotel- and review-specific
topics (see Section 6). We then reduce the dimen-
sionality of our data (i.e., labeled and unlabeled re-
views) by replacing each review term vector with a
vector that corresponds to its term distribution over
just its aspect-specific topics, i.e., excluding city-,
hotel- and review-specific topics. We find that, com-
pared to models trained either on the full vocabulary,
or trained on standard LDA document-topic vectors,
this representation allows our models to generalize
better across reviews of different offerings.
We evaluate our approach on the task of identi-
fying (ranking) manipulated hotels. In particular, in
the absence of gold standard offering-level labels,
we introduce a novel evaluation procedure for this
task, in which we rank numerous versions of each
hotel, where each hotel version contains a differ-
ent number of injected, known deceptive reviews.
Thus, we expect hotel versions with larger propor-
tions of deceptive reviews to be ranked higher than
those with smaller proportions.
For labeled training data, we use the Ott et
al. (2011) dataset of 800 positive (5-star) reviews of
20 Chicago hotels (400 deceptive and 400 truthful).
For evaluation, we construct a new FOUR-CITIES
dataset, containing 40 deceptive and 40 truthful re-
views for each of eight hotels in four different cities
(640 reviews total), following the procedure out-
lined in Ott et al. (2011). We find that our manifold
ranking approach outperforms several state-of-the-
art learning baselines on this task, including trans-
ductive Support Vector Regression. We addition-
ally apply our approach to a large-scale collection
of real-world reviews from TripAdvisor and explore
the resulting ranking.
In the sections below, we discuss related work
(Section 2) and describe the datasets used in this
work (Section 3), the dimensionality-reduction ap-
proach for representing reviews (Section 4), and the
semi-supervised manifold ranking approach (Sec-
tion 5). We then evaluate the methods quantitatively
(Sections 6 and 7) and qualitatively (Section 8).
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9969635">
A number of recent approaches have focused on
identifying individual fake reviews or users who post
</bodyText>
<page confidence="0.993696">
1934
</page>
<bodyText confidence="0.996762194444444">
fake reviews. For example, Jindal and Liu (2008)
train machine learning classifiers to identify dupli-
cate (or near duplicate) reviews. Yoo and Gretzel
(2009) gathered 40 truthful and 42 deceptive hotel
reviews and manually compare the psychologically
relevant linguistic differences between them. Lim et
al. (2010) propose an approach based on abnormal
user behavior to predict spam users, without using
any textual features. Ott et al. (2011) solicit decep-
tive reviews from workers on Amazon Mechanical
Turk, and built a dataset containing 400 deceptive
and 400 truthful reviews, which they use to train
and evaluate supervised SVM classifiers. Ott et al.
(2012) expand upon this work to estimate preva-
lences of deception in a review community. Mukher-
jee et al. (2012) study spam produced by groups of
fake reviewers. Li et al. (2013) use topic models
to detect differences between deceptive and truthful
topic-word distributions. In contrast, in this work we
aim to identify fake reviews at an offering level.2
LDA Topic Models. LDA topic models (Blei et
al, 2003) have been employed for many NLP tasks
in recent years. Here, we build on earlier work
that uses topic models to (a) separate background
information from information discussing the vari-
ous “aspects” of products (e.g., Chemudugunta et
al. (2007)) and (b) identify different levels of infor-
mation (e.g., user-specific, location-specific, time-
specific) (Ramage et al., 2009).
Manifold Ranking Algorithm. The manifold-
ranking method (Zhou et al, 2003a; Zhou et al,
2003b) is a mutual reinforcement ranking approach
initially proposed to rank data points along their un-
derlying manifold structure. It has been widely used
in many different ranking applications, such as sum-
marization (Wan et al, 2007; Wan and Yang, 2007).
</bodyText>
<sectionHeader confidence="0.998571" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.99801425">
In this paper, we train all of our models using the
CHICAGO dataset of Ott et al (2011), which contains
20 deceptive and 20 truthful reviews from each of 20
Chicago hotels (800 reviews total). This dataset is
</bodyText>
<tableCaption confidence="0.5301105">
2Approaches for identifying individual fake reviews may be
applied to our task, for example, by averaging the review-level
predictions for an offering. This averaging approach is one of
our baselines in Section 7.
</tableCaption>
<table confidence="0.998475428571429">
City Hotels
Chicago W Chicago, Palomar Chicago
New York Hotel Pennsylvania, Waldorf Astoria
Los Angeles Sheraton Gateway,
The Westin Los Angeles Airport
Houston Magnolia Hotel,
Crowne Plaza Houston River Oaks
</table>
<tableCaption confidence="0.999862">
Table 1: Details of our FOUR-CITIES evaluation data.
</tableCaption>
<bodyText confidence="0.999903875">
unique in that it contains known (gold standard) de-
ceptive reviews, solicited through Amazon Mechan-
ical Turk, and is publicly-available.3
Unfortunately, the CHICAGO dataset is limited,
both in size (800 reviews) and scope, in that it only
contains reviews of hotels in one city: Chicago.
Accordingly, in order to perform a more realistic
evaluation for our task, we construct a new dataset,
FOUR-CITIES, that contains 40 deceptive and 40
truthful reviews from each of eight hotels in four dif-
ferent cities (640 reviews total).
We build the FOUR-CITIES dataset using the same
procedure as Ott et al (2011), by creating and di-
viding 320 Mechanical Turk jobs, called Human-
Intelligence Tasks (HITs), evenly across eight of the
most popular hotels in our four chosen cities (see Ta-
ble 1). Each HIT presents a worker with the name of
a hotel and a link to the hotel’s website. Workers are
asked to imagine that they work for the marketing
department of the hotel and that their boss has asked
them to write a fake positive review, as if they were
a customer, to be posted on a travel review website.
Each worker is allowed to submit a single review,
and is paid $1 for an acceptable submission.
Finally, we augment our deceptive FOUR-CITIES
reviews with a matching set of truthful reviews from
TripAdvisor by randomly sampling 40 positive (5-
star) reviews for each of the eight chosen hotels.
While we cannot know for sure that the sampled re-
views are truthful, previous work has suggested that
rates of deception among popular hotels is likely to
be low (Jindal and Liu, 2008; Lim et al, 2010).
</bodyText>
<sectionHeader confidence="0.9966585" genericHeader="method">
4 Topic Models for Dimensionality
Reduction
</sectionHeader>
<bodyText confidence="0.9993875">
As mentioned in the introduction, we want to learn
patterns of truthful and deceptive reviews that apply
</bodyText>
<footnote confidence="0.8399305">
3We use the dataset available at: http://www.cs.
cornell.edu/˜myleott/op_spam.
</footnote>
<page confidence="0.988928">
1935
</page>
<figureCaption confidence="0.998284">
Figure 2: Graphical illustration of the RLDA topic model.
</figureCaption>
<bodyText confidence="0.999841615384615">
across hotels in different locations. This is challeng-
ing, however, because hotel reviews often contain
specific information about the hotel or city, and it
is unclear whether these features will generalize to
reviews of other hotels.
We therefore investigate an LDA-based
dimensionality-reduction approach (RLDA) to
derive effective vector representations of reviews.
Specifically, we model each document as a bag of
words, generated from a mixture of: (a) “aspect”
topics (that discuss various dimensions of the
offering); (b) city-specific topics; (c) hotel-specific
topics; (d) review-specific topics;4 and (e) a back-
ground topic. We use this model to reduce the
dimensionality of the review representation in our
training and test sets, by replacing each review’s
term vector with a vector corresponding to the
distribution over only the aspect-based topics, i.e.,
we exclude city, hotel and review-specific topics, as
well as the background topic.
Below we present specific details of our model
(Sections 4.1 and 4.2). The effectiveness of our
dimensionality-reduction approach will be directly
evaluated in Section 6, by comparing the perfor-
mance of various classifiers trained either on the full
vocabulary, or on our reduced feature representation.
</bodyText>
<subsectionHeader confidence="0.622728">
4.1 RLDA Model Details
</subsectionHeader>
<bodyText confidence="0.9999315">
The plate diagram and generative story for our
model are given in Figures 2 and 3, respectively.
Our model has a similar general structure to stan-
dard LDA, but with additional machinery to handle
different levels of information. In particular, in or-
der to model K aspects in a collection of R reviews,
</bodyText>
<footnote confidence="0.703909">
4These will be terms used in just a small number of reviews.
</footnote>
<listItem confidence="0.926364555555556">
• Draw 0B — Dir(λ)
• For each aspect z = 1, 2,..., K: draw 0z — Dir(λ)
• For each city c = 1, 2,..., C: draw 0c — Dir(λ)
• For each hotel h = 1, 2, ..., H: draw 0h — Dir(λ)
• For each review r:
– Draw irr — Dir(β)
– Draw 0r — Dir(λ)
– Draw Br — Dir(α)
– For each word w in d:
</listItem>
<figure confidence="0.96883625">
* Draw yw — Multi(irr)
* If yw = 0:
� Draw zw — Multi(B)
� Draw w — Multi(0z-)
* If yw = 1: draw w — Multi(0B)
* If yw = 2: draw w — Multi(0d)
* If yw = 3: draw w — Multi(0h)
* If yw = 4: draw w — Multi(0c)
</figure>
<figureCaption confidence="0.999889">
Figure 3: Generative story for the RLDA topic model.
</figureCaption>
<bodyText confidence="0.999893428571429">
of H hotels, in C cities, we first draw multinomial
word distributions corresponding to: the background
topic, OB; aspect topics, Ok for k E [1, K]; review-
specific topics, Or for r E [1, R]; hotel-specific top-
ics, Oh for h E [1, H]; and city-specific topics, O,
for c E [1, C]. Then, for each word w in review
R, we sample a switch variable, y E [0, 4], indicat-
ing whether w comes from one of the aspect topics
(y = 0), or the background topic (y = 1), review-
specific topic (y = 2), hotel-specific topic (y = 3)
or city-specific topic (y = 4). If the word comes
from one of the aspect topics, then we further sam-
ple the specific aspect topic, zw E [1, K]. Finally,
we generate the word, w, from the corresponding O.
</bodyText>
<sectionHeader confidence="0.662165" genericHeader="method">
4.2 Inference for RLDA
</sectionHeader>
<bodyText confidence="0.999817285714286">
Given the review collection, our goal is to find the
most likely assignment yw (and zw if yw = 0) for
each word, w, in each review. We perform infer-
ence using Gibbs sampling. It is relatively straight-
forward to derive Gibbs sampling equations that al-
low joint sampling of the zw and yw latent variables
for each word token w:
</bodyText>
<equation confidence="0.584033">
P(yw = 0, Zw = k) =
</equation>
<bodyText confidence="0.972402">
Note that the subscript −w indicates that the
count for word token w is excluded. Also, Nr
</bodyText>
<figure confidence="0.996573454545455">
/�
Nr,a−w + N X
Nr,−w + 50
[�Ck
,T−w + α X
Lek Ckr,−w + Kα
Ewk + A
�w Ewk + V A,
P(yw = m, m = 1, 2, 3, 4) = Nmr,−w + � Ewm + A
X Ew Ewm + V A,
Nr,−w + 50
</figure>
<page confidence="0.947782">
1936
</page>
<bodyText confidence="0.995103555555556">
denotes the number of words in review r and
Nar,−w, N1 r,−w, N2r,−w, N3r,−w, N4r,−w are the number of
words in review r assigned to the aspect, background,
review-specific, hotel-specific and city-specific topics, re-
spectively, excluding the current word. Ckr,−w denotes
the number of words in review r assigned to aspect topic
k. Ewk , Ew1 , Ew2 , Ew3 , Ew4 denote the number of times that
the word w is assigned to aspect k, the background topic,
review-specific topic r, hotel-specific topic h, and city-
specific topic c, respectively. We set hyperparameter α
to 1, β to 0.5, λ to 0.01. We run 200 iterations of Gibbs
sampling until the topic distribution stabilizes. After each
iteration in Gibbs sampling, we obtain:
Finally, at the end of Gibbs sampling, we filter out
background, document-specific, hotel-specific and
city-specific information, by replacing each docu-
ment’s term vector with a 1 × K aspect-topic vector,
~Gr = hθ1 r, θ2 r, · · · , θK r i.
</bodyText>
<sectionHeader confidence="0.977119" genericHeader="method">
5 Manifold Ranking for Hotels
</sectionHeader>
<bodyText confidence="0.999936">
In this section, we describe our ranking algorithm —
based on manifold ranking (Zhou et al, 2003a; Zhou
et al, 2003b) — that tries to jointly model deceptive
language at the hotel-, review- and term-level.
</bodyText>
<subsectionHeader confidence="0.919844">
5.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999788">
We use a three-layer (hotel layer, review layer and
term layer) mutual reinforcement model (see Fig-
ure 1). Formally, we represent our three-layer graph
as G = hVH, VR, VT, EHR, ERR, ERT, ETTi,
where VH = {Hi}i=NH
</bodyText>
<equation confidence="0.999492666666667">
i=1 , VR = {R}i=HR
i=1 and
VT = {Ti}i=V
</equation>
<bodyText confidence="0.987150555555556">
i=1 correspond to the set of hotels, re-
views and terms respectively. EHR, ERR and ERT
respectively denote the edges between hotels and re-
views, reviews and reviews and reviews and terms.
Each edge is associated with a weight that denotes
the similarity between two nodes.
Let sim(Hi, Rj), where Hi ∈ VH and Rj ∈ VR,
denote the edge weight between hotel Hi and review
Rj, calculated as follows:
</bodyText>
<equation confidence="0.982200545454545">
(
1 if Ri ∈ Hj
sim(Hi,Rj) = (2)
0 if Ri ∈6Hj
Then we get row normalized matrices DHR ∈
RNHxNR and DRH ∈ RNRxNH as follows:
sim(Hi, Rj)
DHR(i, j) =
Pi, sim(Hi,, Rj)
DRH(i, j) =
Pj, sim(Hi, Rj,)
</equation>
<bodyText confidence="0.999899666666667">
As described in Section 4.2, each review is rep-
resented with a 1 × K aspect vector Gr after fil-
tering undesired information. The edge weight be-
tween two reviews is then the cosine similarity,
sim(Ri, Rj), between two reviews and can be cal-
culated as follows:
</bodyText>
<equation confidence="0.992793333333333">
t=K · G
Gt
sim(Ri, Rj) = Pt=1 i jt qPt=K qPt=K (4)
t=1 Gt2
i · t=1 Gt2
j
</equation>
<bodyText confidence="0.992370625">
Since the normalization process will make the
review-to-review relation matrix asymmetric, we
adopt the following strategy: let P denote the sim-
ilarity matrix between reviews, where P(i, j) =
sim(Ri, Rj) and M denotes the diagonal matrix
with (i,i)-element equal to the sum of the ith row
of SIM. The normalized matrix between reviews
DRR ∈ RNRxNR is calculated as follows:
</bodyText>
<equation confidence="0.982421">
DRR = M− 2 · P · M− 2
1 1 (5)
</equation>
<bodyText confidence="0.949680666666667">
sim(Ri,wj) denotes the similarity between re-
view Ri and term wj and is the conditional prob-
ability of word wj given review Ri. If wj ∈ Rj,
sim(Ri, wj) is calculated according to Eq. (6) by
integrating out latent parameters θ and π. Else if
wj ∈6 Rj, sim(Ri, wj) = 0.
</bodyText>
<equation confidence="0.993486">
X
θz d · φ(wj)
z +
tE{B,h,c,d}
(6)
</equation>
<bodyText confidence="0.8141578">
Similar to Eq. (3), we further get the normalized ma-
trix DRT ∈ RHRxV and DTR ∈ RV xHR.
Similarity between terms sim(wi, wj) is given by
the WordNet path-similarity,5 normalized to create
the matrix DVV .
</bodyText>
<footnote confidence="0.93759425">
5Path-similarity is based on the shortest path that con-
nects the senses in the “is-a” (hypernym/hyponym) tax-
onomy. See http://nltk.googlecode.com/svn/
trunk/doc/howto/wordnet.html.
</footnote>
<equation confidence="0.998996444444444">
Ni r + β
πi r = P P
i Ni r + 5β θk r = Ck r + α
k Ckr + Kα (1)
Ew z + λ Ew m + λ
φw z = Pw Ew z + V λ φw P
m = w Ew m + V λ
(3)
sim(Hi,Rj)
sim(Ri, wj) = k=KX p(z = k|ri) × p(wj|z = k)
k=1
+ X p(wj|yi = t)p(yi = t|ri)
tE{B,h,c,d}
k=KX
k=1
= π(a)
d
π(t) φtwj)
</equation>
<page confidence="0.957505">
1937
</page>
<bodyText confidence="0.885695">
Input: The hotel set VD, review set VR, term
set VT, normalized transition probability matrix
DHR, DRR, DRH, DRT , DTT , DTR-
Output:
TR-Output: the ranking vectors SR, SH, ST.
Begin:
</bodyText>
<listItem confidence="0.9874135">
1. Initialization: set the score labeled reviews to
+1 or −1 and other unlabeled reviews 0: S0R =
[+1, ..., +1, −1, ..., −1, 0, ..., 0]. Set S0H and
S0T to 0. Normalize the score vector.
2. update SkR, SkH and SkT according to Eq. (7).
3. normalize SkR, SkH and SkT.
4. fix the score of labeled reviews to +1 and −1.
Go to step (2) until convergence.
</listItem>
<figureCaption confidence="0.999339">
Figure 4: Semi-Supervised Reinforcement Ranking.
</figureCaption>
<subsectionHeader confidence="0.9988055">
5.2 Reinforcement Ranking Based on the
Manifold Method
</subsectionHeader>
<bodyText confidence="0.999828222222222">
Based on the set of labeled reviews, nodes for truth-
ful reviews (positive) are initialized with a high
score (1) and nodes for deceptive reviews, a low
score (-1). Given the weighted graph, our task is
to assign a score to the each hotel, each term, and
the remaining unlabeled reviews. Let SH, SR and
ST denote the ranking scores of hotels, reviews and
terms, which are updated during each iteration as
follows until convergence6:
</bodyText>
<equation confidence="0.994014285714286">
Sk+1
H = DHR · SkR
Sk+1
R = c1DRR · SkR + E2DRH · SkH + E3DRT · Skt
Sk+1
T = C4DTT · SkT + c5DTR · SkR
(7)
</equation>
<bodyText confidence="0.9992945">
where e1 + 62 + 63 = 1 and 64 + 65 = 1. (The score
of labeled reviews will be fixed to +1 or −1.)
</bodyText>
<sectionHeader confidence="0.880543" genericHeader="method">
6 Learning Generalizable Classifiers
</sectionHeader>
<bodyText confidence="0.9975485">
In Section 4, we introduced RLDA to filter out
review-, hotel- and city-specific information from
our vector-based review representation. Here, we
will directly evaluate the effectiveness of RLDA
by comparing the performance of binary deceptive
vs. truthful classifiers trained on three feature sets:
(a) the full vocabulary, encoded as unigrams and
bigrams (N-GRAMS); (b) a reduced-dimensionality
feature space, based on standard LDA (Blei et
al, 2003); and (c) a reduced-dimensionality feature
</bodyText>
<footnote confidence="0.8659065">
6Convergence is achieved if the difference between ranking
scores in two consecutive iterations is less than 0.00001.
</footnote>
<bodyText confidence="0.999187133333333">
space, based on our proposed revised LDA approach
(RLDA).
We compare two kinds of classifiers, which are
trained on only the labeled CHICAGO dataset and
tested on the FOUR-CITIES dataset. First, we use
SVMhght (Joachims, 1999) to train linear SVM clas-
sifiers, which have been shown to perform well in
related work (Ott et al, 2011). Second, we train a
two-layer manifold classifier, which is a simplified
version of the model presented in Section 5. In this
model, the graph consists of only review and term
layers, and the score of a labeled review is fixed to
1 or -1 in each iteration. After convergence, reviews
with scores greater than 0 are classified as truthful,
and less than 0 as deceptive.
Results and Discussion The results are shown in
Table 2 and show the average accuracy and preci-
sion/recall w.r.t. the truthful (positive) class. We find
that SVM and MANIFOLD are comparable in all six
conditions, and not surprisingly, perform best when
evaluated on reviews from the two Chicago hotels in
our FOUR-CITIES data. However, the N-GRAM and
LDA feature sets perform much worse than RDLA
when evaluation is performed on reviews from the
other three (non-Chicago) cities. This confirms that
classifiers trained on n-gram features overfit to the
training data (CHICAGO) and do not generalize well
to reviews from other cities. In addition, the stan-
dard LDA-based method for dimensionality reduc-
tion is not sufficient for our specific task.
</bodyText>
<sectionHeader confidence="0.989134" genericHeader="method">
7 Identifying Manipulated Hotels
</sectionHeader>
<bodyText confidence="0.99996">
In this section, we evaluate the performance of our
manifold ranking approach (see Section 5) on the
task of identifying manipulated hotels.
Baselines. We consider several baseline ranking
approaches to compare to our manifold ranking ap-
proach. Like the manifold ranking approach, the
baselines also employ both the CHICAGO dataset (la-
beled) and FOUR-CITIES dataset (without labels).7
For fair comparison, we use identical processing
techniques for each approach. Topic number is set
</bodyText>
<footnote confidence="0.99071375">
7While we have not investigated the effects of unlabeled data
in detail, providing additional unlabeled data (beyond the test
set) boosts the manifold ranking performances reported below
by 1-2%.
</footnote>
<note confidence="0.294698">
I
</note>
<page confidence="0.800557">
1938
</page>
<table confidence="0.999183375">
city feature set SVM Manifold
Accuracy Precision Recall Accuracy Precision Recall
Chicago N-GRAMS 0.831 0.844 0.818 0.835 0.844 0.825
LDA 0.833 0.846 0.819 0.817 0.832 0.802
RLDA 0.830 0.838 0.822 0.841 0.819 0.863
Non-Chicago N-GRAMS 0.728 0.744 0.714 0.733 0.738 0.727
LDA 0.714 0.696 0.732 0.728 0.715 0.741
RLDA 0.791 0.799 0.780 0.801 0.787 0.815
</table>
<tableCaption confidence="0.989651333333333">
Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data. Results
correspond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIES
reviews (six hotels).
</tableCaption>
<bodyText confidence="0.997865666666667">
to five for all topic-model-based approaches. Each
baseline makes review-level predictions and then
ranks each hotel by the average of those predictions.
</bodyText>
<listItem confidence="0.961687222222222">
• Review-SVR: Uses linear Tranductive Support
Vector Regression with unigram and bigram fea-
tures, similar to Ott et al. (2011).
• Review-SVR+LDA (R): Similar to REVIEW-
SVR but uses our revised LDA (RLDA) topic
model for dimensionality reduction (R).
• Two-Layer Manifold (S): A simplified version of
our model where the hotel-level is removed from
the graph. Dimensionality reduction is performed
using standard LDA (S).
• Two-Layer Manifold (R): Similar to TWO-
LAYER MANIFOLD (S) but uses the revised LDA
(RLDA) model for dimensionality reduction.
• Three-layer Manifold (tf-idf): Our three-layer
manifold ranking model, except with each review
represented as a TF-IDF term vector. Review sim-
ilarity is calculated based on the cosine similarity
between these vectors.
</listItem>
<bodyText confidence="0.999803269230769">
Evaluation Method. To evaluate ranking perfor-
mance in the absence of a gold standard set of ma-
nipulated hotels, we rearrange the FOUR-CITIES test
set of 40 truthful and 40 deceptive reviews for each
of eight hotels: we create 41 versions of each hotel,
where each hotel version contains a different num-
ber of injected deceptive reviews, ranging from 0 to
40. For example, the first version of a hotel will have
40 truthful and 0 deceptive reviews, the second ver-
sion 39 truthful and 1 deceptive, and the 41st ver-
sion 0 truthful and 40 deceptive. In total, we gen-
erate 41 x 8 = 328 versions of hotel reviews. We
expect versions with larger proportions of deceptive
reviews to receive lower scores by the ranking mod-
els (i.e., they are ranked higher/more deceptive).
Metrics. To qualitatively evaluate the ranking re-
sults, we use the Normalized Discounted Cumula-
tive Gain (NDCG), which is commonly used to eval-
uate retrieval algorithms with respect to an ideal
relevance-based ranking. In particular, NDCG re-
wards rankings with the most relevant results at the
top positions (Liu, 2009), which is also our objec-
tive, namely, to rank versions that have higher pro-
portions of deceptive reviews nearer to the top.
Let R(m) denote the relevance score of mth
ranked hotel version. Then, NDCGN is defined as:
</bodyText>
<equation confidence="0.988411">
2R(m) − 1
loge(1 + m) (8)
</equation>
<bodyText confidence="0.999737">
where IDCGN refers to discounted cumulative gain
(DCG) of the ideal ranking of the top N results. We
define the ideal ranking according to the proportion
of deceptive reviews in different versions, and re-
port NDCG scores for the Nth ranked hotel versions
(N = 8 to 321), at intervals of 8 (to account for ties
among the eight hotels).
Results and Discussion. NDCG results are shown
in Figure 5. We observe that our approach (using
2, 5 or 10 topics) generally outperforms the other
approaches. In particular, approaches that use our
RLDA text representation (OUR APPROACH, TWO-
LAYER MANIFOLD (R), and REVIEW-SVR+LDA
(R)), which tries to remove city- and hotel-specific
information, perform better than those that use
the full vocabulary (REVIEW-SVR, TWO-LAYER
MANIFOLD (S), and THREE-LAYER MANIFOLD
(TF-IDF)). This further confirms that our RLDA
dimensionality reduction technique allows models,
</bodyText>
<equation confidence="0.834844">
1
NDCGN = IDCGN
m=NE
m=�
</equation>
<page confidence="0.949718">
1939
</page>
<figureCaption confidence="0.9974435">
Figure 5: NDCGN results for different approaches. K
indicates the number of topics.
</figureCaption>
<bodyText confidence="0.997564363636364">
trained on limited data, to generalize to reviews of
different hotels and in different locations. We also
find that approaches that model a reinforcing rela-
tionship between hotels and their reviews are bet-
ter than approaches that model reviews as inde-
pendent units, e.g., TWO-LAYER MANIFOLD (R)
vs. REVIEW-SVR+LDA and TWO-LAYER MANI-
FOLD (S) vs. REVIEW-SVR. This confirms our in-
tuition that a hotel is more deceptive if it is con-
nected with many deceptive reviews, and, in turn,
a review is more deceptive if from a deceptive hotel.
</bodyText>
<sectionHeader confidence="0.970456" genericHeader="method">
8 Qualitative Evaluation
</sectionHeader>
<bodyText confidence="0.999966">
We now present qualitative evaluations for the
RLDA topic model and the manifold ranking model.
Topic Quality. Table 3 gives the top words for
four aspect topics and four city-specific topics in the
RLDA topic model; Table 4 gives the highest and
lowest ranking term weights in our three-layer man-
ifold model. By comparing the first row of topics in
Table 3, corresponding to aspect topics, to the top
words in Table 4, we observe that the learned top-
ics relate to truthful and deceptive classes. For ex-
ample, Topics 1 and 4 share many terms with the
top truthful terms in the manifold model, e.g., spa-
tial terms, such as location, floor and block,
and punctuation, such as (, ), and $. Similarly,
Topics 2 and 7 share many terms with the top de-
ceptive terms in the manifold model, e.g., hotel,
husband, wife, amazing, experience and
recommend. This makes sense, since topic models
have been shown to produce discriminative topics on
</bodyText>
<table confidence="0.999964227272727">
Topic1 Topic2 Topic4 Topic7
location hotel ( hotel
$ stay room service
walk staff ) husband
night restaurant park amazing
block friendly bed will
floor room night weekend
quiet recommend shower friendly
nice love view travel
lobby excellent minute experience
breakfast wife pillow friend
NYC Chicago LA Houston
York Chicago los Houston
ny Michigan Angeles downtown
time mile la Texas
square tower lax cab
nyc Illinois shuttling Westside
street avenue hollywood center
empire Rogers plane Northwest
Chinatown river morning st
station Burnham California museum
Wall Goodman downtown mission
</table>
<tableCaption confidence="0.955515">
Table 3: Top words in topics extracted from RLDA topic
model (see Section 4). The top row presents topic words
from four aspect topics (K = 10) and the bottom row
presents top words from four city-specific topics.
</tableCaption>
<table confidence="0.999599">
Deceptive Truthful
term score term score
my -1.063 $ 0.964
visit -0.944 location 0.922
we -0.882 ( 0.884
hotel -0.863 ) 0.884
husband -0.828 bathroom 0.842
family -0.824 floor 0.810
amazing -0.782 breakfast 0.784
experience -0.740 bar 0.762
recommend -0.732 block 0.747
wife -0.680 small 0.721
relax -0.678 but 0.720
vacation -0.651 walk 0.707
will -0.651 lobby 0.707
friendly -0.646 quiet 0.684
</table>
<tableCaption confidence="0.999289">
Table 4: Term scores from our ranking algorithm.
</tableCaption>
<bodyText confidence="0.961665428571429">
this data in previous work (Li et al., 2013).
With respect to the second row in Table 4, con-
taining top words from city-specific topics, we ob-
serve that each topic does contain primarily city-
specific information. This helps to explain why re-
moving terms associated with these topics resulted
in a better vector representation for reviews.
</bodyText>
<page confidence="0.985552">
1940
</page>
<figureCaption confidence="0.999997">
Figure 6: Hotel Ranking Distribution on TripAdvisor
Figure 7: Proportion of Singletons vs. Hotel Ranking.
</figureCaption>
<bodyText confidence="0.999907741935484">
Real-world Evaluation. Finally, we apply our
ranking model to a large-scale collection of real-
world reviews from TripAdvisor. We crawl 878,561
reviews from 3,945 hotels in 25 US cities from Tri-
pAdvisor excluding all non-5-star reviews and re-
moving hotels with fewer than 100 reviews. In the
end, we collect 244,810 reviews from 838 hotels.
We apply our manifold ranking model and rank
all 838 hotels. First, we present a histogram of the
resulting manifold ranking scores in Figure 6. We
observe that the distribution reaches a peak around
0.04, which in our quantitative evaluation (Sec-
tion 7) corresponded to a hotel with 34 truthful and
6 deceptive reviews. These results suggest that the
majority of reviews in TripAdvisor are truthful, in
line with previous findings by Ott et al. (2011).
Next, we note that previous work has hypothe-
sized that deceptive reviews are more likely to be
posted by first-time review writers, or singleton re-
viewers (Ott et al, 2011; Wu et al, 2011). Accord-
ingly, if this hypothesis were valid, then manipu-
lated hotels would have an above-average proportion
of singleton reviews. Figure 7 shows a histogram
of the average proportion of singleton reviews, as
a function of the ranking scores produced by our
model. Noting that lower scores correspond to a
higher predicted proportion of deceptive reviews, we
observe that hotels that are ranked as being more de-
ceptive by our model have much higher proportions
of singleton reviews, on average, compared to hotels
ranked as less deceptive.
</bodyText>
<sectionHeader confidence="0.996681" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999817714285714">
We study the problem of identifying manipulated of-
ferings on review portals and propose a novel three-
layer graph model, based on manifold ranking for
ranking offerings based on the proportion of reviews
expected to be instances of deceptive opinion spam.
Experimental results illustrate the effectiveness of
our model over several learning-based baselines.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995458">
This work was supported in part by National Sci-
ence Foundation Grant BCS-0904822, a DARPA
Deft grant, as well as a gift from Google. We also
thank the EMNLP reviewers for their helpful com-
ments and advice.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997199727272727">
David Blei, Ng Andrew and Michael Jordan. Latent
Dirichlet allocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi, Massimo Santini and Sebas-
tiano Vigna. A reference collection for web spam. In
ACM Sigir Forum. 2006.
Paul-Alexandru Chirita, Jrg Diederich and Wolfgang Ne-
jdl. MailRank: using ranking for spam detection. In
Proceedings of the 14th ACM international conference
on Information and knowledge management. 2005.
Cone. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions. August.
Yajuan Duan, Zhumin Chen, Furu Wei, Ming Zhou and
Heung-Yeung Shum. Twitter Topic Summarization by
Ranking Tweets Using Social Influence and Content
Quality. In Proceedings of 24th International Confer-
ence on Computational Linguistics 2012.
Federal Trade Commission. Guides Concerning Use of
Endorsements and Testimonials in Advertising. In
FTC 16 CFR Part 255. 2009.
</reference>
<page confidence="0.871883">
1941
</page>
<reference confidence="0.999776011235955">
Socialogue: Five Stars? Thumbs Up? A+ or
Just Average? URL:http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929g
Nitin Jindal, and Bing Liu. Opinion spam and analysis. In
Proceedings of the 2008 International Conference on
Web Search and Data Mining. 2008.
Nitin Jindal, Bing Liu and Ee-Peng Lim. Finding Unusual
Review Patterns Using Unexpected Rules. In Proceed-
ings of the 19th ACM international conference on In-
formation and knowledge management.2010.
Thorsten Joachims. Making large-scale support vector
machine learning practical. In Advances in kernel
methods.1999.
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu.
Learning to identify review Spam. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence. 2011.
Jiwei Li, Claire Cardie and Sujian Li. TopicSpam: a
Topic-Model-Based Approach for Spam Detection. In
Proceedings of the 51th Annual Meeting of the Associ-
ation for Computational Linguis- tics. 2013.
Peng Li, Jing Jiang and Yinglin Wang. Generating tem-
plates of entity summaries with an entity-aspect model
and pattern mining. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics. 2010.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Review
Spammers Using Rating Behavior. In Proceedings of
the 19th ACM international conference on Information
and knowledge management. 2010.
Tieyan Liu. Learning to Rank for Information Retrieval.
In Foundations and Trends in Information Retrieval
2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spotting
Fake Reviewer Groups in Consumer Reviews. In Pro-
ceedings of the 21st international conference on World
Wide Web. 2012.
Juan Martinez-Romo and Lourdes Araujo. Web spam
identification through language model analysis. In
Proceedings of the 5th international workshop on ad-
versarial information retrieval on the web. 2009.
Myle Ott, Claire Cardie and Jeffrey Hancock. Estimating
the Prevalence of Deception in Online Review Com-
munities. In Proceedings of the 21st international con-
ference on World Wide Web. 2012.
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock.
Finding Deceptive Opinion Spam by Any Stretch of
the Imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics. 2011.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. Labeled LDA: A supervised
topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th conference
on Uncertainty in artificial intelligence.2004.
Xiaojun Wan and Jianwu Yang. Multi-Document Sum-
marization Using Cluster-Based Link Analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval. 2008.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao Manifold-
Ranking Based Topic-Focused Multi-Document Sum-
marization. In Proceedings of International Joint Con-
ferences on Artificial Intelligence,2007.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer De-
tection. In Proceedings of International Conference of
Data Mining. 2011.
Guangyu Wu, Derek Greene and, Padraig Cunningham.
Merging multiple criteria to identify suspicious re-
views. In Proceedings of the fourth ACM conference
on Recommender systems. 2011.
Kyung-Hyan Yoo and Ulrike Gretzel. Comparison of De-
ceptive and Truthful Travel Reviews. In Information
and Communication Technologies in Tourism. 2009.
Dengyong Zhou, Olivier Bousquet, Thomas Navin and
Jason Weston. Learning with local and global consis-
tency. In Proceedings of Advances in neural informa-
tion processing systems.2003.
Dengyong Zhou, Jason Weston, Arthur Gretton and
Olivier Bousquet. Ranking on data manifolds. In Pro-
ceedings ofAdvances in neural information processing
systems.2003.
</reference>
<page confidence="0.995798">
1942
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647764">
<title confidence="0.999877">Identifying Manipulated Offerings on Review Portals</title>
<author confidence="0.999869">Jiwei Li</author>
<affiliation confidence="0.9999695">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.9986">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999815">jiweil@cs.cmu.edu</email>
<author confidence="0.996266">Myle Ott Claire Cardie</author>
<affiliation confidence="0.9674755">Department of Computer Cornell</affiliation>
<address confidence="0.727479">Ithaca, NY 14853,</address>
<email confidence="0.999669">myleott,cardie@cs.cornell.edu</email>
<abstract confidence="0.99861515">Recent work has developed supervised methfor detecting opinion fake reviews written to sound authentic and deliberately mislead readers. And whereas past work has focused on identifying individual fake reviews, this paper aims to identify hotels) that contain fake reviews. We introduce a semi-supervised manifold ranking algorithm for this task, which relies on a small set of labeled individual reviews for training. Then, in the absence of gold standard labels (at an offering level), we introduce a novel evaluation procedure that ranks artificial instances of real offerings, where each artificial offering contains a known number of injected deceptive reviews. Experiments on a novel dataset of hotel reviews show that the proposed method outperforms state-of-art learning baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Ng Andrew</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="5558" citStr="Blei et al, 2003" startWordPosition="838" endWordPosition="841">hallenging, because reviews often contain offering-specific vocabulary. For example, reviews of hotels in Los Angeles are more likely to include keywords such as “beach”, “sea”, “sunshine” or “LA”, while reviews of Juneau hotels may contain “glacier”, “Juneau”, “bear” or “aurora borealis.” A hotel review might also mention the hotel’s restaurant or bar by name. Unfortunately, it is unclear how important (or detrimental) offering-specific features are when deciding whether a review is fake. Accordingly, we propose a dimensionality-reduction approach, based on Latent Dirichlet Allocation (LDA) (Blei et al, 2003), to obtain a vector representation of reviews for the ranking algorithm that generalizes across reviews of different offerings. Specifically, we train an LDA-based topic model to view each review as a mixture of aspect-, city-, hotel- and review-specific topics (see Section 6). We then reduce the dimensionality of our data (i.e., labeled and unlabeled reviews) by replacing each review term vector with a vector that corresponds to its term distribution over just its aspect-specific topics, i.e., excluding city-, hotel- and review-specific topics. We find that, compared to models trained either</context>
<context position="9031" citStr="Blei et al, 2003" startWordPosition="1380" endWordPosition="1383">it deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee et al. (2012) study spam produced by groups of fake reviewers. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects” of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., user-specific, location-specific, timespecific) (Ramage et al., 2009). Manifold Ranking Algorithm. The manifoldranking method (Zhou et al, 2003a; Zhou et al, 2003b) is a mutual reinforcement ranking approach initially proposed to rank data points along their underlying manifold structure. It has b</context>
<context position="21351" citStr="Blei et al, 2003" startWordPosition="3618" endWordPosition="3621">4DTT · SkT + c5DTR · SkR (7) where e1 + 62 + 63 = 1 and 64 + 65 = 1. (The score of labeled reviews will be fixed to +1 or −1.) 6 Learning Generalizable Classifiers In Section 4, we introduced RLDA to filter out review-, hotel- and city-specific information from our vector-based review representation. Here, we will directly evaluate the effectiveness of RLDA by comparing the performance of binary deceptive vs. truthful classifiers trained on three feature sets: (a) the full vocabulary, encoded as unigrams and bigrams (N-GRAMS); (b) a reduced-dimensionality feature space, based on standard LDA (Blei et al, 2003); and (c) a reduced-dimensionality feature 6Convergence is achieved if the difference between ranking scores in two consecutive iterations is less than 0.00001. space, based on our proposed revised LDA approach (RLDA). We compare two kinds of classifiers, which are trained on only the labeled CHICAGO dataset and tested on the FOUR-CITIES dataset. First, we use SVMhght (Joachims, 1999) to train linear SVM classifiers, which have been shown to perform well in related work (Ott et al, 2011). Second, we train a two-layer manifold classifier, which is a simplified version of the model presented in </context>
</contexts>
<marker>Blei, Andrew, Jordan, 2003</marker>
<rawString>David Blei, Ng Andrew and Michael Jordan. Latent Dirichlet allocation. 2003. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Luca Becchetti</author>
<author>Paolo Boldi</author>
<author>Stefano Leonardi</author>
<author>Massimo Santini</author>
<author>Sebastiano Vigna</author>
</authors>
<title>A reference collection for web spam.</title>
<date>2006</date>
<booktitle>In ACM Sigir Forum.</booktitle>
<contexts>
<context position="1758" citStr="Castillo et al, 2006" startWordPosition="252" endWordPosition="255">nsumers increasingly rely on user-generated online reviews when making purchase decisions (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public’s trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam—fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al, 2011). Unlike other kinds of spam, such as Web (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) and e-mail spam (Chirita et al, 2005), recent work has found that deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al, 2011). Accordingly, there is growing interest in developing automatic, usually learning-based, methods to help users identify deceptive opinion spam (see Section 2). Even in fully-supervised settings, however, automatic methods are imperfect at identifying individual deceptive reviews, and erroneously labeling genuine reviews as deceptive may frustrate and alienate honest reviewers. An alternative approach, not yet considered in </context>
</contexts>
<marker>Castillo, Donato, Becchetti, Boldi, Leonardi, Santini, Vigna, 2006</marker>
<rawString>Carlos Castillo, Debora Donato, Luca Becchetti, Paolo Boldi, Stefano Leonardi, Massimo Santini and Sebastiano Vigna. A reference collection for web spam. In ACM Sigir Forum. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul-Alexandru Chirita</author>
<author>Jrg Diederich</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>MailRank: using ranking for spam detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="1796" citStr="Chirita et al, 2005" startWordPosition="259" endWordPosition="262">ated online reviews when making purchase decisions (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public’s trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam—fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al, 2011). Unlike other kinds of spam, such as Web (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) and e-mail spam (Chirita et al, 2005), recent work has found that deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al, 2011). Accordingly, there is growing interest in developing automatic, usually learning-based, methods to help users identify deceptive opinion spam (see Section 2). Even in fully-supervised settings, however, automatic methods are imperfect at identifying individual deceptive reviews, and erroneously labeling genuine reviews as deceptive may frustrate and alienate honest reviewers. An alternative approach, not yet considered in previous work, is to instead identify </context>
</contexts>
<marker>Chirita, Diederich, Nejdl, 2005</marker>
<rawString>Paul-Alexandru Chirita, Jrg Diederich and Wolfgang Nejdl. MailRank: using ranking for spam detection. In Proceedings of the 14th ACM international conference on Information and knowledge management. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cone</author>
</authors>
<title>Online Influence Trend Tracker.</title>
<date>2011</date>
<location>http://www.coneinc.com/negative-reviews-onlinereverse-purchase-decisions.</location>
<contexts>
<context position="1238" citStr="Cone, 2011" startWordPosition="178" endWordPosition="179">rvised manifold ranking algorithm for this task, which relies on a small set of labeled individual reviews for training. Then, in the absence of gold standard labels (at an offering level), we introduce a novel evaluation procedure that ranks artificial instances of real offerings, where each artificial offering contains a known number of injected deceptive reviews. Experiments on a novel dataset of hotel reviews show that the proposed method outperforms state-of-art learning baselines. 1 Introduction Consumers increasingly rely on user-generated online reviews when making purchase decisions (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public’s trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam—fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al, 2011). Unlike other kinds of spam, such as Web (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) and e-mail spam (Chirita et al, 2005), recent work has found that deceptive opi</context>
</contexts>
<marker>Cone, 2011</marker>
<rawString>Cone. 2011 Online Influence Trend Tracker. http://www.coneinc.com/negative-reviews-onlinereverse-purchase-decisions. August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Duan</author>
<author>Zhumin Chen</author>
<author>Furu Wei</author>
</authors>
<title>Ming Zhou and Heung-Yeung Shum. Twitter Topic Summarization by Ranking Tweets Using Social Influence and Content Quality.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics</booktitle>
<marker>Duan, Chen, Wei, 2012</marker>
<rawString>Yajuan Duan, Zhumin Chen, Furu Wei, Ming Zhou and Heung-Yeung Shum. Twitter Topic Summarization by Ranking Tweets Using Social Influence and Content Quality. In Proceedings of 24th International Conference on Computational Linguistics 2012.</rawString>
</citation>
<citation valid="true">
<title>Federal Trade Commission. Guides Concerning Use of Endorsements and Testimonials in Advertising.</title>
<date>2009</date>
<journal>In FTC 16 CFR Part</journal>
<volume>255</volume>
<contexts>
<context position="8122" citStr="(2009)" startWordPosition="1237" endWordPosition="1237">w, we discuss related work (Section 2) and describe the datasets used in this work (Section 3), the dimensionality-reduction approach for representing reviews (Section 4), and the semi-supervised manifold ranking approach (Section 5). We then evaluate the methods quantitatively (Sections 6 and 7) and qualitatively (Section 8). 2 Related Work A number of recent approaches have focused on identifying individual fake reviews or users who post 1934 fake reviews. For example, Jindal and Liu (2008) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee e</context>
</contexts>
<marker>2009</marker>
<rawString>Federal Trade Commission. Guides Concerning Use of Endorsements and Testimonials in Advertising. In FTC 16 CFR Part 255. 2009.</rawString>
</citation>
<citation valid="false">
<title>Socialogue: Five Stars? Thumbs Up? A+ or Just Average? URL:http://www.ipsos-na.com/newspolls/pressrelease.aspx?id=5929g</title>
<marker></marker>
<rawString>Socialogue: Five Stars? Thumbs Up? A+ or Just Average? URL:http://www.ipsos-na.com/newspolls/pressrelease.aspx?id=5929g</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining.</booktitle>
<contexts>
<context position="8013" citStr="Jindal and Liu (2008)" startWordPosition="1218" endWordPosition="1221">h to a large-scale collection of real-world reviews from TripAdvisor and explore the resulting ranking. In the sections below, we discuss related work (Section 2) and describe the datasets used in this work (Section 3), the dimensionality-reduction approach for representing reviews (Section 4), and the semi-supervised manifold ranking approach (Section 5). We then evaluate the methods quantitatively (Sections 6 and 7) and qualitatively (Section 8). 2 Related Work A number of recent approaches have focused on identifying individual fake reviews or users who post 1934 fake reviews. For example, Jindal and Liu (2008) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ot</context>
<context position="12014" citStr="Jindal and Liu, 2008" startWordPosition="1872" endWordPosition="1875">hotel and that their boss has asked them to write a fake positive review, as if they were a customer, to be posted on a travel review website. Each worker is allowed to submit a single review, and is paid $1 for an acceptable submission. Finally, we augment our deceptive FOUR-CITIES reviews with a matching set of truthful reviews from TripAdvisor by randomly sampling 40 positive (5- star) reviews for each of the eight chosen hotels. While we cannot know for sure that the sampled reviews are truthful, previous work has suggested that rates of deception among popular hotels is likely to be low (Jindal and Liu, 2008; Lim et al, 2010). 4 Topic Models for Dimensionality Reduction As mentioned in the introduction, we want to learn patterns of truthful and deceptive reviews that apply 3We use the dataset available at: http://www.cs. cornell.edu/˜myleott/op_spam. 1935 Figure 2: Graphical illustration of the RLDA topic model. across hotels in different locations. This is challenging, however, because hotel reviews often contain specific information about the hotel or city, and it is unclear whether these features will generalize to reviews of other hotels. We therefore investigate an LDA-based dimensionality-r</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal, and Bing Liu. Opinion spam and analysis. In Proceedings of the 2008 International Conference on Web Search and Data Mining. 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nitin Jindal</author>
</authors>
<title>Bing Liu and Ee-Peng Lim. Finding Unusual Review Patterns Using Unexpected Rules.</title>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management.2010.</booktitle>
<marker>Jindal, </marker>
<rawString>Nitin Jindal, Bing Liu and Ee-Peng Lim. Finding Unusual Review Patterns Using Unexpected Rules. In Proceedings of the 19th ACM international conference on Information and knowledge management.2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<booktitle>In Advances in kernel methods.1999.</booktitle>
<marker>Joachims, </marker>
<rawString>Thorsten Joachims. Making large-scale support vector machine learning practical. In Advances in kernel methods.1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Yi Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to identify review Spam.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence.</booktitle>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu. Learning to identify review Spam. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
</authors>
<title>Claire Cardie and Sujian Li. TopicSpam: a Topic-Model-Based Approach for Spam Detection.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguis- tics.</booktitle>
<marker>Li, 2013</marker>
<rawString>Jiwei Li, Claire Cardie and Sujian Li. TopicSpam: a Topic-Model-Based Approach for Spam Detection. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguis- tics. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Jing Jiang</author>
<author>Yinglin Wang</author>
</authors>
<title>Generating templates of entity summaries with an entity-aspect model and pattern mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>Peng Li, Jing Jiang and Yinglin Wang. Generating templates of entity summaries with an entity-aspect model and pattern mining. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ee-Peng Lim</author>
<author>Viet-An Nguyen</author>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Hady Wirawan Lauw</author>
</authors>
<title>Detecting Product Review Spammers Using Rating Behavior.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="8279" citStr="Lim et al. (2010)" startWordPosition="1256" endWordPosition="1259">ting reviews (Section 4), and the semi-supervised manifold ranking approach (Section 5). We then evaluate the methods quantitatively (Sections 6 and 7) and qualitatively (Section 8). 2 Related Work A number of recent approaches have focused on identifying individual fake reviews or users who post 1934 fake reviews. For example, Jindal and Liu (2008) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee et al. (2012) study spam produced by groups of fake reviewers. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-wo</context>
<context position="12032" citStr="Lim et al, 2010" startWordPosition="1876" endWordPosition="1879">oss has asked them to write a fake positive review, as if they were a customer, to be posted on a travel review website. Each worker is allowed to submit a single review, and is paid $1 for an acceptable submission. Finally, we augment our deceptive FOUR-CITIES reviews with a matching set of truthful reviews from TripAdvisor by randomly sampling 40 positive (5- star) reviews for each of the eight chosen hotels. While we cannot know for sure that the sampled reviews are truthful, previous work has suggested that rates of deception among popular hotels is likely to be low (Jindal and Liu, 2008; Lim et al, 2010). 4 Topic Models for Dimensionality Reduction As mentioned in the introduction, we want to learn patterns of truthful and deceptive reviews that apply 3We use the dataset available at: http://www.cs. cornell.edu/˜myleott/op_spam. 1935 Figure 2: Graphical illustration of the RLDA topic model. across hotels in different locations. This is challenging, however, because hotel reviews often contain specific information about the hotel or city, and it is unclear whether these features will generalize to reviews of other hotels. We therefore investigate an LDA-based dimensionality-reduction approach </context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu, and Hady Wirawan Lauw. Detecting Product Review Spammers Using Rating Behavior. In Proceedings of the 19th ACM international conference on Information and knowledge management. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tieyan Liu</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2009</date>
<booktitle>In Foundations and Trends in Information Retrieval</booktitle>
<contexts>
<context position="26281" citStr="Liu, 2009" startWordPosition="4402" endWordPosition="4403">hful and 1 deceptive, and the 41st version 0 truthful and 40 deceptive. In total, we generate 41 x 8 = 328 versions of hotel reviews. We expect versions with larger proportions of deceptive reviews to receive lower scores by the ranking models (i.e., they are ranked higher/more deceptive). Metrics. To qualitatively evaluate the ranking results, we use the Normalized Discounted Cumulative Gain (NDCG), which is commonly used to evaluate retrieval algorithms with respect to an ideal relevance-based ranking. In particular, NDCG rewards rankings with the most relevant results at the top positions (Liu, 2009), which is also our objective, namely, to rank versions that have higher proportions of deceptive reviews nearer to the top. Let R(m) denote the relevance score of mth ranked hotel version. Then, NDCGN is defined as: 2R(m) − 1 loge(1 + m) (8) where IDCGN refers to discounted cumulative gain (DCG) of the ideal ranking of the top N results. We define the ideal ranking according to the proportion of deceptive reviews in different versions, and report NDCG scores for the Nth ranked hotel versions (N = 8 to 321), at intervals of 8 (to account for ties among the eight hotels). Results and Discussion</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tieyan Liu. Learning to Rank for Information Retrieval. In Foundations and Trends in Information Retrieval 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Natalie Glance</author>
</authors>
<title>Spotting Fake Reviewer Groups in Consumer Reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web.</booktitle>
<contexts>
<context position="8734" citStr="Mukherjee et al. (2012)" startWordPosition="1330" endWordPosition="1334">tzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee et al. (2012) study spam produced by groups of fake reviewers. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects” of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., u</context>
</contexts>
<marker>Mukherjee, Liu, Glance, 2012</marker>
<rawString>Arjun Mukherjee, Bing Liu and Natalie Glance. Spotting Fake Reviewer Groups in Consumer Reviews. In Proceedings of the 21st international conference on World Wide Web. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Martinez-Romo</author>
<author>Lourdes Araujo</author>
</authors>
<title>Web spam identification through language model analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th international workshop on adversarial information retrieval on the web.</booktitle>
<contexts>
<context position="1735" citStr="Martinez-Romo and Araujo, 2009" startWordPosition="248" endWordPosition="251">ing baselines. 1 Introduction Consumers increasingly rely on user-generated online reviews when making purchase decisions (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public’s trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam—fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al, 2011). Unlike other kinds of spam, such as Web (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) and e-mail spam (Chirita et al, 2005), recent work has found that deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al, 2011). Accordingly, there is growing interest in developing automatic, usually learning-based, methods to help users identify deceptive opinion spam (see Section 2). Even in fully-supervised settings, however, automatic methods are imperfect at identifying individual deceptive reviews, and erroneously labeling genuine reviews as deceptive may frustrate and alienate honest reviewers. An alternative approach,</context>
</contexts>
<marker>Martinez-Romo, Araujo, 2009</marker>
<rawString>Juan Martinez-Romo and Lourdes Araujo. Web spam identification through language model analysis. In Proceedings of the 5th international workshop on adversarial information retrieval on the web. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Claire Cardie</author>
<author>Jeffrey Hancock</author>
</authors>
<title>Estimating the Prevalence of Deception in Online Review Communities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web.</booktitle>
<contexts>
<context position="8628" citStr="Ott et al. (2012)" startWordPosition="1312" endWordPosition="1315">8) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee et al. (2012) study spam produced by groups of fake reviewers. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects</context>
</contexts>
<marker>Ott, Cardie, Hancock, 2012</marker>
<rawString>Myle Ott, Claire Cardie and Jeffrey Hancock. Estimating the Prevalence of Deception in Online Review Communities. In Proceedings of the 21st international conference on World Wide Web. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey Hancock</author>
</authors>
<title>Finding Deceptive Opinion Spam by Any Stretch of the Imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1662" citStr="Ott et al, 2011" startWordPosition="236" endWordPosition="239">how that the proposed method outperforms state-of-art learning baselines. 1 Introduction Consumers increasingly rely on user-generated online reviews when making purchase decisions (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public’s trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam—fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader (Ott et al, 2011). Unlike other kinds of spam, such as Web (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) and e-mail spam (Chirita et al, 2005), recent work has found that deceptive opinion spam is neither easily ignored nor easily identified by human readers (Ott et al, 2011). Accordingly, there is growing interest in developing automatic, usually learning-based, methods to help users identify deceptive opinion spam (see Section 2). Even in fully-supervised settings, however, automatic methods are imperfect at identifying individual deceptive reviews, and erroneously labeling genuine reviews as decept</context>
<context position="6870" citStr="Ott et al. (2011)" startWordPosition="1043" endWordPosition="1046"> allows our models to generalize better across reviews of different offerings. We evaluate our approach on the task of identifying (ranking) manipulated hotels. In particular, in the absence of gold standard offering-level labels, we introduce a novel evaluation procedure for this task, in which we rank numerous versions of each hotel, where each hotel version contains a different number of injected, known deceptive reviews. Thus, we expect hotel versions with larger proportions of deceptive reviews to be ranked higher than those with smaller proportions. For labeled training data, we use the Ott et al. (2011) dataset of 800 positive (5-star) reviews of 20 Chicago hotels (400 deceptive and 400 truthful). For evaluation, we construct a new FOUR-CITIES dataset, containing 40 deceptive and 40 truthful reviews for each of eight hotels in four different cities (640 reviews total), following the procedure outlined in Ott et al. (2011). We find that our manifold ranking approach outperforms several state-of-theart learning baselines on this task, including transductive Support Vector Regression. We additionally apply our approach to a large-scale collection of real-world reviews from TripAdvisor and explo</context>
<context position="8408" citStr="Ott et al. (2011)" startWordPosition="1277" endWordPosition="1280">ely (Sections 6 and 7) and qualitatively (Section 8). 2 Related Work A number of recent approaches have focused on identifying individual fake reviews or users who post 1934 fake reviews. For example, Jindal and Liu (2008) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee et al. (2012) study spam produced by groups of fake reviewers. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic mo</context>
<context position="9845" citStr="Ott et al (2011)" startWordPosition="1511" endWordPosition="1514"> of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., user-specific, location-specific, timespecific) (Ramage et al., 2009). Manifold Ranking Algorithm. The manifoldranking method (Zhou et al, 2003a; Zhou et al, 2003b) is a mutual reinforcement ranking approach initially proposed to rank data points along their underlying manifold structure. It has been widely used in many different ranking applications, such as summarization (Wan et al, 2007; Wan and Yang, 2007). 3 Dataset In this paper, we train all of our models using the CHICAGO dataset of Ott et al (2011), which contains 20 deceptive and 20 truthful reviews from each of 20 Chicago hotels (800 reviews total). This dataset is 2Approaches for identifying individual fake reviews may be applied to our task, for example, by averaging the review-level predictions for an offering. This averaging approach is one of our baselines in Section 7. City Hotels Chicago W Chicago, Palomar Chicago New York Hotel Pennsylvania, Waldorf Astoria Los Angeles Sheraton Gateway, The Westin Los Angeles Airport Houston Magnolia Hotel, Crowne Plaza Houston River Oaks Table 1: Details of our FOUR-CITIES evaluation data. un</context>
<context position="21843" citStr="Ott et al, 2011" startWordPosition="3696" endWordPosition="3699">oded as unigrams and bigrams (N-GRAMS); (b) a reduced-dimensionality feature space, based on standard LDA (Blei et al, 2003); and (c) a reduced-dimensionality feature 6Convergence is achieved if the difference between ranking scores in two consecutive iterations is less than 0.00001. space, based on our proposed revised LDA approach (RLDA). We compare two kinds of classifiers, which are trained on only the labeled CHICAGO dataset and tested on the FOUR-CITIES dataset. First, we use SVMhght (Joachims, 1999) to train linear SVM classifiers, which have been shown to perform well in related work (Ott et al, 2011). Second, we train a two-layer manifold classifier, which is a simplified version of the model presented in Section 5. In this model, the graph consists of only review and term layers, and the score of a labeled review is fixed to 1 or -1 in each iteration. After convergence, reviews with scores greater than 0 are classified as truthful, and less than 0 as deceptive. Results and Discussion The results are shown in Table 2 and show the average accuracy and precision/recall w.r.t. the truthful (positive) class. We find that SVM and MANIFOLD are comparable in all six conditions, and not surprisin</context>
<context position="24556" citStr="Ott et al. (2011)" startWordPosition="4118" endWordPosition="4121">0.738 0.727 LDA 0.714 0.696 0.732 0.728 0.715 0.741 RLDA 0.791 0.799 0.780 0.801 0.787 0.815 Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data. Results correspond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIES reviews (six hotels). to five for all topic-model-based approaches. Each baseline makes review-level predictions and then ranks each hotel by the average of those predictions. • Review-SVR: Uses linear Tranductive Support Vector Regression with unigram and bigram features, similar to Ott et al. (2011). • Review-SVR+LDA (R): Similar to REVIEWSVR but uses our revised LDA (RLDA) topic model for dimensionality reduction (R). • Two-Layer Manifold (S): A simplified version of our model where the hotel-level is removed from the graph. Dimensionality reduction is performed using standard LDA (S). • Two-Layer Manifold (R): Similar to TWOLAYER MANIFOLD (S) but uses the revised LDA (RLDA) model for dimensionality reduction. • Three-layer Manifold (tf-idf): Our three-layer manifold ranking model, except with each review represented as a TF-IDF term vector. Review similarity is calculated based on the </context>
<context position="31514" citStr="Ott et al. (2011)" startWordPosition="5260" endWordPosition="5263">US cities from TripAdvisor excluding all non-5-star reviews and removing hotels with fewer than 100 reviews. In the end, we collect 244,810 reviews from 838 hotels. We apply our manifold ranking model and rank all 838 hotels. First, we present a histogram of the resulting manifold ranking scores in Figure 6. We observe that the distribution reaches a peak around 0.04, which in our quantitative evaluation (Section 7) corresponded to a hotel with 34 truthful and 6 deceptive reviews. These results suggest that the majority of reviews in TripAdvisor are truthful, in line with previous findings by Ott et al. (2011). Next, we note that previous work has hypothesized that deceptive reviews are more likely to be posted by first-time review writers, or singleton reviewers (Ott et al, 2011; Wu et al, 2011). Accordingly, if this hypothesis were valid, then manipulated hotels would have an above-average proportion of singleton reviews. Figure 7 shows a histogram of the average proportion of singleton reviews, as a function of the ranking scores produced by our model. Noting that lower scores correspond to a higher predicted proportion of deceptive reviews, we observe that hotels that are ranked as being more d</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9402" citStr="Ramage et al., 2009" startWordPosition="1436" endWordPosition="1439">. Li et al. (2013) use topic models to detect differences between deceptive and truthful topic-word distributions. In contrast, in this work we aim to identify fake reviews at an offering level.2 LDA Topic Models. LDA topic models (Blei et al, 2003) have been employed for many NLP tasks in recent years. Here, we build on earlier work that uses topic models to (a) separate background information from information discussing the various “aspects” of products (e.g., Chemudugunta et al. (2007)) and (b) identify different levels of information (e.g., user-specific, location-specific, timespecific) (Ramage et al., 2009). Manifold Ranking Algorithm. The manifoldranking method (Zhou et al, 2003a; Zhou et al, 2003b) is a mutual reinforcement ranking approach initially proposed to rank data points along their underlying manifold structure. It has been widely used in many different ranking applications, such as summarization (Wan et al, 2007; Wan and Yang, 2007). 3 Dataset In this paper, we train all of our models using the CHICAGO dataset of Ott et al (2011), which contains 20 deceptive and 20 truthful reviews from each of 20 Chicago hotels (800 reviews total). This dataset is 2Approaches for identifying individ</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati and Christopher Manning. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michal Rosen-zvi</author>
<author>Thomas Griffith</author>
</authors>
<title>Mark Steyvers and Padhraic Smyth. The author-topic model for authors and documents.</title>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence.2004.</booktitle>
<marker>Rosen-zvi, Griffith, </marker>
<rawString>Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and Padhraic Smyth. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence.2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-Document Summarization Using Cluster-Based Link Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. Multi-Document Summarization Using Cluster-Based Link Analysis. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Jianwu Yang and Jianguo Xiao ManifoldRanking Based Topic-Focused Multi-Document Summarization.</title>
<booktitle>In Proceedings of International Joint Conferences on Artificial Intelligence,2007.</booktitle>
<marker>Wan, </marker>
<rawString>Xiaojun Wan, Jianwu Yang and Jianguo Xiao ManifoldRanking Based Topic-Focused Multi-Document Summarization. In Proceedings of International Joint Conferences on Artificial Intelligence,2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guan Wang</author>
<author>Sihong Xie</author>
<author>Bing Liu</author>
<author>Philip Yu</author>
</authors>
<title>Review Graph based Online Store Review Spammer Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference of Data Mining.</booktitle>
<marker>Wang, Xie, Liu, Yu, 2011</marker>
<rawString>Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Review Graph based Online Store Review Spammer Detection. In Proceedings of International Conference of Data Mining. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyu Wu</author>
<author>Derek Greene and</author>
<author>Padraig Cunningham</author>
</authors>
<title>Merging multiple criteria to identify suspicious reviews.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM conference on Recommender systems.</booktitle>
<contexts>
<context position="31704" citStr="Wu et al, 2011" startWordPosition="5294" endWordPosition="5297">ng model and rank all 838 hotels. First, we present a histogram of the resulting manifold ranking scores in Figure 6. We observe that the distribution reaches a peak around 0.04, which in our quantitative evaluation (Section 7) corresponded to a hotel with 34 truthful and 6 deceptive reviews. These results suggest that the majority of reviews in TripAdvisor are truthful, in line with previous findings by Ott et al. (2011). Next, we note that previous work has hypothesized that deceptive reviews are more likely to be posted by first-time review writers, or singleton reviewers (Ott et al, 2011; Wu et al, 2011). Accordingly, if this hypothesis were valid, then manipulated hotels would have an above-average proportion of singleton reviews. Figure 7 shows a histogram of the average proportion of singleton reviews, as a function of the ranking scores produced by our model. Noting that lower scores correspond to a higher predicted proportion of deceptive reviews, we observe that hotels that are ranked as being more deceptive by our model have much higher proportions of singleton reviews, on average, compared to hotels ranked as less deceptive. 9 Conclusion We study the problem of identifying manipulated</context>
</contexts>
<marker>Wu, and, Cunningham, 2011</marker>
<rawString>Guangyu Wu, Derek Greene and, Padraig Cunningham. Merging multiple criteria to identify suspicious reviews. In Proceedings of the fourth ACM conference on Recommender systems. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung-Hyan Yoo</author>
<author>Ulrike Gretzel</author>
</authors>
<title>Comparison of Deceptive and Truthful Travel Reviews.</title>
<date>2009</date>
<booktitle>In Information and Communication Technologies in Tourism.</booktitle>
<contexts>
<context position="8122" citStr="Yoo and Gretzel (2009)" startWordPosition="1234" endWordPosition="1237">he sections below, we discuss related work (Section 2) and describe the datasets used in this work (Section 3), the dimensionality-reduction approach for representing reviews (Section 4), and the semi-supervised manifold ranking approach (Section 5). We then evaluate the methods quantitatively (Sections 6 and 7) and qualitatively (Section 8). 2 Related Work A number of recent approaches have focused on identifying individual fake reviews or users who post 1934 fake reviews. For example, Jindal and Liu (2008) train machine learning classifiers to identify duplicate (or near duplicate) reviews. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the psychologically relevant linguistic differences between them. Lim et al. (2010) propose an approach based on abnormal user behavior to predict spam users, without using any textual features. Ott et al. (2011) solicit deceptive reviews from workers on Amazon Mechanical Turk, and built a dataset containing 400 deceptive and 400 truthful reviews, which they use to train and evaluate supervised SVM classifiers. Ott et al. (2012) expand upon this work to estimate prevalences of deception in a review community. Mukherjee e</context>
</contexts>
<marker>Yoo, Gretzel, 2009</marker>
<rawString>Kyung-Hyan Yoo and Ulrike Gretzel. Comparison of Deceptive and Truthful Travel Reviews. In Information and Communication Technologies in Tourism. 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas Navin</author>
<author>Jason Weston</author>
</authors>
<title>Learning with local and global consistency.</title>
<booktitle>In Proceedings of Advances in neural information processing systems.2003.</booktitle>
<marker>Zhou, Bousquet, Navin, Weston, </marker>
<rawString>Dengyong Zhou, Olivier Bousquet, Thomas Navin and Jason Weston. Learning with local and global consistency. In Proceedings of Advances in neural information processing systems.2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dengyong Zhou</author>
<author>Jason Weston</author>
<author>Arthur Gretton</author>
<author>Olivier Bousquet</author>
</authors>
<title>Ranking on data manifolds.</title>
<booktitle>In Proceedings ofAdvances in neural information processing systems.2003.</booktitle>
<marker>Zhou, Weston, Gretton, Bousquet, </marker>
<rawString>Dengyong Zhou, Jason Weston, Arthur Gretton and Olivier Bousquet. Ranking on data manifolds. In Proceedings ofAdvances in neural information processing systems.2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
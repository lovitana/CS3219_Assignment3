<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004887">
<title confidence="0.990723">
Automated Essay Scoring by Maximizing Human-machine Agreement
</title>
<author confidence="0.994484">
Hongbo Chen and Ben He
</author>
<affiliation confidence="0.905467">
School of Computer and Control Engineering
University of Chinese Academy of Sciences
100190 Beijing, China
</affiliation>
<email confidence="0.992928">
chenhongbo11@mails.ucas.ac.cn, benhe@ucas.ac.cn
</email>
<sectionHeader confidence="0.995525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999610458333334">
Previous approaches for automated essay
scoring (AES) learn a rating model by min-
imizing either the classification, regression,
or pairwise classification loss, depending on
the learning algorithm used. In this paper,
we argue that the current AES systems can
be further improved by taking into account
the agreement between human and machine
raters. To this end, we propose a rank-
based approach that utilizes listwise learn-
ing to rank algorithms for learning a rating
model, where the agreement between the hu-
man and machine raters is directly incorpo-
rated into the loss function. Various linguistic
and statistical features are utilized to facilitate
the learning algorithms. Experiments on the
publicly available English essay dataset, Au-
tomated Student Assessment Prize (ASAP),
show that our proposed approach outperforms
the state-of-the-art algorithms, and achieves
performance comparable to professional hu-
man raters, which suggests the effectiveness
of our proposed method for automated essay
scoring.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999707953488372">
Automated essay scoring utilizes the NLP tech-
niques to automatically rate essays written for given
prompts, namely, essay topics, in an educational set-
ting (Dikli, 2006). Nowadays, AES systems have
been put into practical use in large-scale English
tests and play the role of one human rater. For ex-
ample, before AES systems enter the picture, essays
in the writing assessment of Graduate Record Ex-
amination (GRE) are rated by two human raters. A
third human rater is needed when the difference of
the scores given by the two human raters is larger
than one in the 6-point scale. Currently, GRE essays
are rated by one human rater and one AES system. A
second human rater is required only when there ex-
ists a non-negligible disagreement between the first
human rater and the machine rater. With the help of
an AES system that highly agrees with human raters,
the human workload can be reduced by half at most.
Therefore, the agreement between the AES system
and the human rater is an important indicator of an
AES system’s effectiveness.
There have been efforts in developing AES meth-
ods since the 1960s. Various kinds of algorithms
and models based on NLP and machine learning
techniques have been proposed to implement AES
systems. Existing approaches consider essay rating
as a classification (Larkey, 1998), regression (Attali
and Burstein, 2006) or preference ranking problem
(Yannakoudakis et al., 2011), where the loss func-
tion is the regression loss, classification loss and
pairwise classification loss, respectively. In this pa-
per, we argue that the purpose of AES is to predict
the essay’s rating that human raters would give. If
an AES system frequently disagrees with the first
human rater, a second human rater will be needed in
most cases. Thus, the introduction of the AES sys-
tem does not bring much benefit in reducing the hu-
man workload. It is therefore desirable to minimize
the disagreement between the machine and human
raters. However, this disagreement is not explicitly,
if any, addressed in the current AES methods.
To this end, we propose a rank-based approach
in this paper that utilizes a listwise learning to rank
</bodyText>
<page confidence="0.932751">
1741
</page>
<note confidence="0.742354">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1741–1752,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.997323916666667">
algorithm to address automated essay scoring in
the view of directly optimizing the agreement be-
tween human raters and the AES system. Different
from the preference ranking-based approach (Yan-
nakoudakis et al., 2011) that maximizes the pairwise
classification precision (Liu, 2009), our rank-based
approach follows the listwise learning paradigm and
the agreement between the machine and human
raters is directly integrated into the loss function that
is optimized by gradient boost regression trees.
To the best of our knowledge, this work is the first
to apply listwise learning to rank approach for AES,
which aims at the optimization of the agreement be-
tween the human and machine raters. Experimental
results on the publicly available dataset ASAP indi-
cate that our proposed method achieves high agree-
ment with human raters, that is about 0.80, mea-
sured by quadratic weighted Kappa (Brenner and
Kliebsch, 1996). Our proposed method also out-
performs the previous classification, regression and
preference ranking based approaches. As it is widely
accepted that the agreement between human raters,
measured by either quadratic weighted Kappa or
Pearson’s correlation coefficient, ranges from 0.70
to 0.80 (Powers et al., 2000) (Williamson, 2009), our
proposed approach therefore performs as well as hu-
man raters.
The rest of this paper is organized as follows.
In section 2, we introduce the research background
of automated essay scoring and give a brief intro-
duction to learning to rank. In section 3, a de-
tailed description of our listwise learning to rank
approach for automated essay scoring is presented.
Section 4 explains the experimental setup and sec-
tion 5 presents the experimental results. Finally, in
section 6 we conclude this research.
</bodyText>
<sectionHeader confidence="0.999588" genericHeader="related work">
2 Related Work and Background
</sectionHeader>
<bodyText confidence="0.999948333333333">
Firstly, we give a brief description of existing ap-
proaches for AES in section 2.1. Then, an introduc-
tion to learning to rank is presented in section 2.2.
</bodyText>
<subsectionHeader confidence="0.961367">
2.1 Existing AES Methods
</subsectionHeader>
<bodyText confidence="0.999971692307692">
In general, existing solutions consider AES as a
learning problem. Based on a large number of
predefined objectively measurable features, various
learning techniques, including classification, regres-
sion and preference ranking, are applied (Larkey,
1998) (Yannakoudakis et al., 2011).
Regression-based approach treats feature values
and essay score as independent variables and de-
pendent variable, respectively, and then learns a re-
gression equation by classical regression algorithms,
such as support vector regression (Vapnik et al.,
1996). In 1966, the first AES system, Project Essay
Grader, was developed by Ellis Page upon the re-
quest of the American College Board. The PEG sys-
tem defines a large set of surface text features from
essays, e.g. fourth root of essay length, and uses
regression-based approach to predict the score that
human raters will give. E-rater, developed by Edu-
cational Testing Services (ETS) in America, in late
1990s, is a commercial AES system which has been
put into practical use in the Graduate Record Exam-
ination (GRE) and the Test of English as a Foreign
Language (TOEFL). The E-rater system uses natu-
ral language processing techniques to extract various
kinds of linguistic features of essays, such as lexical,
syntactic, grammar, etc.. Then it predicts the final
score by the stepwise regression method (Attali and
Burstein, 2006).
The classification-based approach sees essay
scores as in-discriminative class labels and uses clas-
sical classification algorithms, e.g. the K-nearest
neighbor (KNN) and the naive Bayesian model, to
predict to which class an essay belongs, where a
class is associated to a numeric rating. Intelligent
Essay Assessor (IEA) (Foltz et al., 1999), developed
also in late 1990s, evaluates essay by measuring se-
mantic features. Each ungraded essay, represented
by a semantic vector generated by Latent Seman-
tic Analysis (LSA) (Dumais, 2005), is rated accord-
ing to the similarity degree with semantic vectors of
graded essays. Bayesian Essay Test Scoring sYs-
tem, developed by Larkey in 2003, is based on naive
Bayesian model. It is the only open-source AES sys-
tem, but has not been put into practical use yet.
Besides classification and regression-based ap-
proaches, (Yannakoudakis et al., 2011) proposed a
preference ranking based approach for learning a
rating model, where a ranking function or model
is learned to construct a global ordering of essays
based on writing quality. It is also the first study
of rank-based approach in automated essay scor-
ing. Although “learning to rank” is not mentioned in
</bodyText>
<page confidence="0.987616">
1742
</page>
<bodyText confidence="0.999956083333333">
their paper, the algorithm they used, Ranking SVM
(svm-light package with “-z p“ option), is actually a
pairwise approach. We will give a brief introduction
to learning to rank in section 2.2.
The AES systems can be deployed in two differ-
ent manners, namely prompt-specific and generic. A
prompt-specific rating model is built for a specific
prompt and designed to be the best rating model
for the particular prompt (Williamson, 2009). For
different prompts, the features used, their weights,
and scoring criteria, may be different. It usually re-
quires several hundreds of graded essays for train-
ing, which is time-consuming and usually imprac-
tical in a classroom environment. Generic rating
model is trained from essays across a group of
prompts and designed to be the best fit for pre-
dicting human scores for all prompts. It usually
does not consider prompt-specific features and just
takes writing quality into account. Generic rating
model evaluates essays across all prompts with the
same scoring criteria, which is more consistent with
the human rubric that is usually the same for all
prompts, and therefore has validity-related advan-
tages (Attali et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999874">
2.2 Learning to Rank
</subsectionHeader>
<bodyText confidence="0.999935580645161">
Learning to rank, also called machine-learned rank-
ing, was originally proposed to settle the ranking
problem in information retrieval (IR) (Liu, 2009). It
is a type of supervised or semi-supervised machine
learning algorithm that automatically construct a
ranking model or function from training data.
Current learning to rank algorithms fall into three
categories, that is, the pointwise, pairwise, listwise
approaches. Pointwise approach takes individual
documents as training examples for learning a scor-
ing function. In fact, both multiple linear regres-
sion and support vector regression (Vapnik et al.,
1996), which have been widely used in automated
essay scoring (Shermis and Burstein, 2002), can be
seen as pointwise approaches. Pairwise approaches
process a pair of documents each time and usu-
ally model ranking as a pairwise classification prob-
lem. Thus, the loss function is always a classifi-
cation loss. Representative algorithms are ranking
SVM (Joachims, 2006), RankNet (Li et al., 2007),
etc.. (Yannakoudakis et al., 2011) apply pairwise
approach, ranking SVM, to automated essay scoring
and achieve better performance than support vec-
tor regression. In listwise approaches, ranking al-
gorithms process a list of documents each time and
the loss function aims at measuring the accordance
between predicted ranking list and the ground truth
label. Representative algorithms are LambdaMart
(Wu et al., 2008), RankCosine (Qin et al., 2008),
etc.. Listwise approach has not yet been used in au-
tomated essay scoring.
</bodyText>
<sectionHeader confidence="0.997608" genericHeader="method">
3 Automated Essay Scoring by
</sectionHeader>
<subsectionHeader confidence="0.990859">
Maximizing Human-machine Agreement
</subsectionHeader>
<bodyText confidence="0.9999920625">
The main work-flow of our proposed approach is
as follows. Firstly, a set of essays rated by profes-
sional human raters are gathered for the training. A
listwise learning to rank algorithm learns a ranking
model or function using this set of human rated es-
says represented by vectors of the pre-defined fea-
tures. Then the learned ranking model or function
outputs a model score for each essay, including both
rated and unrated essays, from which a global order-
ing of essays is constructed. Finally, the model score
is mapped to a predefined scale of valid ratings, such
as an integer from 1 to 6 in a 6-point scale
In this section, we give a detailed description of
our listwise learning to rank approach for AES in
section 3.1. And features used in our approach are
presented in section 3.2.
</bodyText>
<subsectionHeader confidence="0.999254">
3.1 Listwise Learning to Rank for AES
</subsectionHeader>
<bodyText confidence="0.999954352941177">
Our choice of the listwise learning to rank algorithm
is due to the fact that it takes the entire set of la-
beled essays associated to a given prompt, instead
of the individual essays or essay pairs as in (Yan-
nakoudakis et al., 2011), as training examples. This
brings us the convenience of easily embedding the
inter-rater agreement into the loss function for the
learning.
In this paper, we deploy LambdaMART (Wu et
al., 2008), a listwise learning to rank algorithm and
then use Random Forests (RF) (Breiman, 2001) for
the bagging of LambdaMART learners. Having
been widely used in information retrieval applica-
tions, LambdaMART is one of the most effective
learning to rank algorithms. For instance, it achieves
the top results in the 2010 Yahoo! Learning to Rank
challenge (Burges, 2010). Random Forests is an
</bodyText>
<page confidence="0.89999">
1743
</page>
<bodyText confidence="0.97764537704918">
ensemble learning method for classification and re-
gression.
Previously, the loss function of LambdaMART is
defined as the gradient loss of the retrieval effec-
tiveness, measured by IR evaluation criteria such as
Normalized Discounted Cumulative Gain (nDCG)
(Wu et al., 2008). More specifically, it is a heuristic
method that directly defines A, the gradient of nDCG
with respect to the model score of each document,
and has been shown to work empirically for partic-
ular loss functions NDCG (Yue and Burges, 2007).
Then, Multiple Additive Regression Trees (MART)
(Friedman, 2000), also called Gradient Boosting De-
cision Tree (GBDT)l, are used to “learn” these gra-
dients iteratively. MART is a class of boosting al-
gorithms that performs gradient descent in function
space, using regression trees. Its output F(x) can be
written as F(x) = &amp; αifi(x), i = 1, 2, ....N. Each
fi(x) is a function modeled by a single regression
tree and the αi is the corresponding weight. Given
that n trees have been trained, the (n+1)th regression
tree, fi+1(x), models the derivative of the cost with
respect to the current model score at each training
point. Thus, what remains is to compute the deriva-
tive.
As for the automated essay scoring, Lamb-
daMART is not readily available since its loss func-
tion is defined as a function of the gradient of IR
evaluation measures. While such measures focus on
the top-ranked documents that are of great impor-
tance to the IR applications, they are not suitable to
our study. This is because for AES, the rating pre-
diction of all essays equally matters, no matter what
ratings they receive.
It is therefore necessary to re-define the A. Specif-
ically, we need to define the gradient of the evalua-
tion criteria in AES, e.g. quadratic weighted Kappa
(Brenner and Kliebsch, 1996) and Pearson’s corre-
lation coefficient, with respect to the model score of
each essay. In this paper, we use quadratic weighted
Kappa as the evaluation metric. Kappa (Cohen
and others, 1960) is a statistical metric which is
used to measure inter-rater agreement. Quadratic
weighted Kappa takes the degree of disagreement
between raters into account. This measuring method
1For space reason, we refer the readers to (Friedman, 2000),
(Breiman, 2001) for details of MART, GBDT and Random
Forests.
is widely accepted as a primary evaluation metric for
the AES tasks. For instance, it is the official evalu-
ation metric in the Automated Student Assessment
Prize sponsored by Hewlett Foundation2. We denote
our modified LambdaMART as K-LambdaMART in
which K stands for the Kappa-based gradient func-
tion. Specific steps include the following:
To begin with, we re-define the Ai,j for each pair
of essays. For a pair of essays, essay i and essay
j, Ai,j is defined as the derivative of RankNet (Li et
al., 2007) loss function multiplied by the Quadratic
weighted Kappa gain after exchanging the two es-
says’ ratings.
</bodyText>
<equation confidence="0.992159">
−�
Ai,j = 1 + eδ(sz−sj) |OKappa |(1)
</equation>
<bodyText confidence="0.9993655">
si and sj are the model scores for essay i and es-
say j, respectively. 6 is a parameter which deter-
mines the shape of the sigmoid. Quadratic weighted
Kappa are calculated as follows:
</bodyText>
<equation confidence="0.936182">
�i,j wi,jOi,j
� = 1 −
Ei wi,j E&apos; i,j (2 )
j
</equation>
<bodyText confidence="0.998678913043478">
In matrix O, Oi,j corresponds to the number of
essays that received a score i by human rater and a
score j by the AES system. In matrix w, wi,j is the
difference between raters scores (i−j)2
(N−1)2 , where N is
the number of possible ratings. Matrix E is calcu-
lated as the outer product between the two raters vec-
tors of scores, normalized such that E and O have
the same sum.
It is necessary to define the quadratic weighted
Kappa gain, namely OKappa, in an explicit manner.
In each iteration, every essay is ranked by its model
score and then rated according to its ranking posi-
tion. For example, for five essays e1, e2, e3, e4, e5
with actual ratings 5, 4, 3, 2, 1, if the ranking (by
model score) is e3, e4, e1, e5, e2, we assume that
e3, e4, e1, e5, e2 will get ratings of 5, 4, 3, 2, 1, over
which quadratic weighted kappa gain can be calcu-
lated.
After the definition of Ai,j for each pair of essays,
it is time to re-define the A, the gradient for each
essay. Let I denote the set of pairs of indices ⟨i, j⟩,
in which essay i receive a higher rating than essay j.
</bodyText>
<footnote confidence="0.931854">
2http://www.kaggle.com/c/asap-sas
</footnote>
<page confidence="0.990187">
1744
</page>
<bodyText confidence="0.998647">
Set I must include each pair just once. Then, the A
gradient for each essay, e.g. essay i, is defined as,
</bodyText>
<equation confidence="0.9951775">
Ai = � Ai,j − � Ai,j; (3)
j:(i,j)EI j:(j,i)EI
</equation>
<bodyText confidence="0.9997721">
The rational behind the above formulae is as fol-
lows. For each of the essays in the whole essay col-
lection associated with the same prompt, e.g. essay
i, the gradient Ai is incremented by a positive value
Ai,j when coming across another essay j that has a
lower rating. The value of Ai,j is weighted by the
quadratic weighted Kappa gain after exchanging the
two essays’ ratings. On the contrary, the gradient Ai
will be incremented by a negative value −Ai,j when
the another essay has a higher rating. As a result, af-
ter each iteration of MART, essays with higher rat-
ing tend to receive a higher model score while essays
with lower rating tend to get a lower model score.
After the training process, the ranking model out-
puts an unscaled model score for each ungraded es-
say. To determine the final rating of each given un-
rated essay, we have to map this unscaled model
score to the predefined scale, such as an integer from
1 to 6 in a 6 point scale. The mapping process
is as follows. To begin with, the learned ranking
model also computes an unscaled model score for
each essay in the training set. As the model is trained
by learning to rank algorithms, essays with higher
model scores tend to get higher actual ratings. In
other words, essays with close model scores tend to
get the same rating. Therefore, we select the k es-
says whose model scores are closest to the given es-
say. We then remove the essays with the very high-
est and lowest model scores within the k. The final
rating is the mean of the remaining k − 2 essays’
ratings. In this paper, k is empirically set to 5, ob-
tained in our preliminary experiments on the ASAP
validation set.
Finally, the Random Forests algorithm is used to
bag K-LambdaMART learners. During the training
process, both features and samples are randomly se-
lected for each K-LambdaMART learner. In the test-
ing phase, it outputs a score for each testing sam-
ple that is the mode of the scores output by each K-
LambdaMART learner.
</bodyText>
<subsectionHeader confidence="0.996081">
3.2 Pre-defined Features
</subsectionHeader>
<bodyText confidence="0.929775739130435">
We pre-define four types of features that indicate the
essay quality, including lexical, syntactical, gram-
mar and fluency, content and prompt-specific fea-
tures. A brief description of these four classes of
features is given below.
Lexical features: We define 4 subsets of lexical fea-
tures. Each subset of features consists of one or sev-
eral sub features.
– Statistics of word length: The number of words
with length in characters larger than 4, 6, 8, 10, 12,
respectively. The mean and variance of word length
in characters.
– word level: All words in Webster dictionary 3
are divided into 8 levels according to the College
Board Vocabulary Study (Breland et al., 1994). The
higher level a word belongs to, the more sophisti-
cated vocabulary usage it indicates. For example,
words like thoroughfare, percolate are in level 8,
while words with the same meanings, street, filter,
belong to level 1. We count the number of words that
belong to each level and calculate the mean word
level of a given essay.
– Unique words: The number of unique words ap-
peared in each essay, normalized by the essay length
in words.
– Spelling errors: The number of spelling er-
rors detected by the spelling check API provided by
Google 4.
Syntactical features: There are 4 subsets of syntac-
tical features.
– Statistics of sentence length: The number of
sentences with length in words larger than 10, 18,
25, respectively. The mean and variance of sentence
length in words.
– Subclauses: The mean number of subclauses
in each sentence, normalized by sentence length in
words. The mean subclause length in words. Sub-
clauses are labeled as “SBAR” in the parser tree gen-
erated by a commonly used NLP tool, Stanford Core
NLP (Klein and Manning, 2003), which is an inte-
grated suite of natural language processing tools for
English in Java5, including part-of-speech tagging,
parsing, co-reference, etc..
– Sentence level: The sum of the depth of all
nodes in a parser tree generated by Stanford Core
NLP. The height of the parser tree is also incorpo-
</bodyText>
<page confidence="0.945426">
1745
</page>
<bodyText confidence="0.975667061538462">
rated into the feature set.
– Mode, preposition, comma: The number of
modes, prepositions and commas in each sen-
tence respectively, normalized by sentence length in
words. Part of speech (POS) is detected by Stanford
Core NLP (Toutanova et al., 2003). The POS tags
of modal verb and preposition are “MD” and “IN”,
respectively.
Grammar and fluency features: There are two
subsets of grammar and fluency features.
– Word bigram and trigram: We evaluate the
grammar and fluency of an essay by calculating
mean tf/TF of word bigrams and trigrams (Briscoe et
al., 2010) (tf is the term frequency in a single essay
and TF is the term frequency in the whole essay col-
lection). We assume a bigram or trigram with high
tf/TF as a grammar error because high tf/TF means
that this kind of bigram or trigram is not commonly
used in the whole essay collection but appears in the
specific essay.
– POS bigram and trigram: Mean tf/TF of POS
bigrams and trigrams. The reason is the same with
word bigrams and trigrams.
Content and prompt-specific features: We define
four subsets of content and prompt-specific features.
– Essay length: Essay length in characters and
words, respectively. The fourth root of essay length
in words is proved to be highly correlated with the
essay score (Shermis and Burstein, 2002).
– Word vector similarity: Mean cosine similarity
of word vectors, in which the element is the term fre-
quency multiplied by inverse document frequency
(tf-idf) (Salton, 1971) of each word. It is calculated
as the weighted mean of all cosine similarities and
the weight is set as the corresponding essay score.
– Semantic vector similarity: Semantic vectors
are generated by Latent Semantic Analysis (Dumais,
2005). The calculation of mean cosine similarity of
semantic vectors is the same with word vector simi-
larity.
– Text coherence: Coherence in writing means
that all the ideas in a paragraph flow smoothly from
one sentence to the next. We only consider nouns
and pronouns in each sentence as they convey more
information. The relevance degree between one sen-
tence and its next in the same paragraph is calcu-
lated as the sum of the similarity degrees between
nouns and pronouns appeared in the two sentences,
normalized by the sum of the two sentences’ length
in words. The similarity degree between words is
set to 1 if coreference exists, indicated by Stanford
Core NLP (Lee et al., 2013). Otherwise, it is mea-
sured by WordNet similarity package (Pedersen et
al., 2004). Finally, text coherence is computed as the
average relevance degree of all pairs of neighbored
sentences.
The rating model is learned off-line using a set of
training essays. For a given target essay, it is the
feature extraction that mainly accounts for the over-
head. In our experiments, it usually costs in average
no more than 10 seconds on a desktop PC with an In-
tel i5-2410M CPU running at 2.3GHZ to extract the
pre-defined features and predict a rating for a given
essay, which is affordable, compared to the cost of a
human rater.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999596">
This section presents our the experimental design,
including the test dataset used, configuration of test-
ing algorithms, feature selection and the evaluation
methodology.
</bodyText>
<subsectionHeader confidence="0.999314">
4.1 Test Dataset
</subsectionHeader>
<bodyText confidence="0.9999943">
The dataset used in our experiments comes from
the Automated Student Assessment Prize (ASAP)1,
which is sponsored by the William and Flora
Hewlett Foundation. Dataset in this competition6
consists of eight essay sets. Each essay set was gen-
erated from a single prompt. The number of es-
says associated with each prompt ranges from 900
to 1800 and the average length of essays in word
in each essay set ranges from 150 to 650. All es-
says were written by students in different grades and
received a resolved score, namely the actual rating,
from professional human raters. Moreover, ASAP
comes with a validation set that can be used for pa-
rameter training. There is no overlap between this
validation set and the test set used in our evaluation.
In AES, the agreement between human-machine
rater is the most important measurement of success.
We use quadratic weighted Kappa to evaluate the
agreement between the ratings given by the AES al-
gorithm and the actual ratings. It is widely accepted
</bodyText>
<footnote confidence="0.999003333333333">
3http://www.merriam-webster.com/
4http://code.google.com/p/google-api-spelling-java/
5http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.994312">
1746
</page>
<bodyText confidence="0.999992545454546">
as a reasonable evaluation measure for AES systems
(Williamson, 2009), and is also the official evalu-
ation measure in the ASAP AES competition. It is
calculated on all essay topics. If there are essays that
come from n essay topics, we calculate the agree-
ment degree on each essay topic first and then com-
pute the overall agreement degree in the z-space. In
addition, analysis of variance (ANOVA) (Scheffe,
1999) is conducted to test whether significant differ-
ence exists between the two groups of scores given
by human and machine raters.
</bodyText>
<subsectionHeader confidence="0.998538">
4.2 Configuration of Testing Algorithms
</subsectionHeader>
<bodyText confidence="0.99989684375">
Random Forests bagging K-LambdaMart We denote
our proposed method K-LambdaMART where K
stands for the Kappa-based gradient. Our implemen-
tation of RF bagging K-LambdaMART is based on
the open-source RankLib toolkit7, a library of learn-
ing to rank algorithms, in which many popular learn-
ing to rank algorithms have been implemented, e.g.
LambdaMART and RankNet (Li et al., 2007). Em-
pirical settings of parameters obtained by prelimi-
nary experiments on the ASAP validation set are as
follows. For bagging: the number of bags is set to
300, subsampling rate is 0.80 and feature sampling
rate is 0.50. For LambdaMART in each bag: the
number of trees is set to 1, the number of tree leaves
is 100 and other parameters are set to default.
Baseline algorithms We use classical machine
learning algorithms, support vector machine (SVM)
for classification, regression (Vapnik et al., 1996)
and preference ranking (Joachims, 2006), respec-
tively, as the baselines. These three algorithms have
been used for AES in the literature (Briscoe et al.,
2010) (Yannakoudakis et al., 2011). Especially, the
state-of-the-art AES approach proposed by (Yan-
nakoudakis et al., 2011) utilizes the SVM for prefer-
ence ranking, a pairwise learning to rank algorithm,
for training a rating model. The linear kernel is used
in the experiments. The parameter C, which controls
the trade-off between empirical loss and regularizer,
is set by grid search on the ASAP validation set.
The original LambdaMART is not included in the
baseline algorithms as it has been shown that the
performance of LambdaMART is inferior to ranking
</bodyText>
<footnote confidence="0.932576">
6http://www.kaggle.com/c/asap-sas/data
</footnote>
<note confidence="0.709431">
SVM on the same dataset (Chen et al., 2012).
</note>
<subsectionHeader confidence="0.974929">
4.3 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999897255813954">
Although machine learning approaches usually use
the all features available for training, we try to obtain
an carefully selected feature set that can withstand
the scrutiny of construct validity in assessment de-
velopment (Chen and Zechner, 2011). Specific steps
of feature selection conducted on individual features
are as follows:
To begin with, the importance of the features is
determined by computing each features Pearson cor-
relation coefficient with human raters scores based
on the training set (Chen and Zechner, 2011). Fea-
tures whose absolute Pearson correlation coefficient
with human scores are lower than 0.20 are removed
from the feature set.
Next, we calculate the inter-correlation degrees
between these features. For each pair of features
whose Pearson correlation coefficient larger than
0.90, one of them should be removed. The criteria
for feature removing is as follows. Firstly, at least
one feature in each subset of features should be re-
mained. Satisfying the first prerequisite condition,
the removed one should be linguistically less mean-
ingful than the remaining one.
For prompt-specific rating model, feature selec-
tion is conducted on the essays associated with the
same prompt. For generic rating model, the final
feature set used for training is the intersection of the
8 feature sets for prompt-specific rating model.
For space reason, here we briefly summarize the
feature selection results. Among the lexical features,
word length in characters larger than 8 and 10, num-
ber of words in each of the levels from 3 to 6, num-
ber of unique words, and number of spelling errors
are mostly selected. As for the syntactical features,
sentence length in words larger than 18 and 25, num-
ber of commas, mean clause length and the mean
depth of parser tree are usually selected. Among
the grammar and fluency features, mean tf/TF of
word bigrams and mean tf/TF of POS trigrams are
always selected. For content and prompt-specific
features, essay length in words, word vector and se-
mantic vector similarity with high rated essays, text
coherence are usually selected for training a prompt-
</bodyText>
<footnote confidence="0.996454">
7http://people.cs.umass.edu/ vdang/ranklib.html
</footnote>
<page confidence="0.995824">
1747
</page>
<tableCaption confidence="0.999865">
Table 1: Cross-validation on ASAP dataset measured by quadratic weighted Kappa.
</tableCaption>
<table confidence="0.9980588">
Algorithm Prompt-specific ANOVA Generic ANOVA
SVMc (baseline) 0.7302(9.75%) Significant 0.6319(23.93%) Significant
SVMr (baseline) 0.7861(1.95%) Significant 0.7022(11.52%) Significant
SVMp (baseline) 0.7876(1.75%) Significant 0.7669(2.11%) Not significant
RF bagging K-LambdaMART 0.8014 Not significant 0.7831 Not significant
</table>
<bodyText confidence="0.9912465">
specific rating model. When it comes to the generic
rating model, the prompt-specific features like word
vector similarity and semantic vector similarity, are
removed.
</bodyText>
<subsectionHeader confidence="0.998955">
4.4 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999979444444445">
We conduct three sets of experiments to evaluate
the effectiveness of our listwise learning to rank ap-
proach for automated essay scoring.
The first set of experiments evaluates our pro-
posed approach under a prompt-specific setting. We
conduct 5-fold cross-validation, where the essays of
each prompt are randomly partitioned into 5 sub-
sets. In each fold, 4 subsets are used for training,
and one is used for testing. To avoid bias introduced
by the random partition, we repeat the 5-fold cross-
validation for 5 times on 5 different random parti-
tions. The overall quadratic weighted Kappa is av-
eraged on all 25 test subsets.
It should be noticed that in random partition of the
whole dataset, the overlap between any two parti-
tions should be kept below 1.5*1/(#folds)*100%.
For example, in five-fold cross validation, the over-
lap should be kept below 30%. This is because: ac-
cording to the Dirichlet principle (Courant, 2005),
each subset in one partition overlaps more than 20%
with at least one subset in another partition in five-
fold cross-validation. The tolerance boundary pa-
rameter is then set to 1.5.
The objective of the second set of experiments is
to test the performance of our listwise learning to
rank approach for generic rating models. We also
conduct 5 times 5-fold cross-validation like the first
experiment. In 5-fold cross-validation, essays as-
sociated with the same prompt are randomly parti-
tioned into 5 subsets. In this way, each fold con-
sists of essays across all prompts. The overall per-
formance is averaged on all 25 test subsets.
In the third set of experiments, we evaluate the
quality of the features used in our rating model by
feature ablation test and feature unique test. In abla-
tion test, we evaluate our essay rating model’s per-
formance before and after the removal of a subset
of features from the whole feature set. The per-
formance difference indicates the removed features’
contribution to the rating model’s overall perfor-
mance. In unique test, only a subset of features are
used in the rating model construction and all other
features are removed. The learned rating model’s
performance indicates to which extent the features
are correlated with the actual essay ratings.
</bodyText>
<sectionHeader confidence="0.990693" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.997537">
5.1 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999962384615384">
Table 1 presents the first set of experimental re-
sults obtained on the ASAP dataset, measured by
quadratic weighted Kappa. In Table 1, RF stands
for random forests. SVMc, SVMr, SVMp are SVM
for classification, regression and preference ranking,
respectively. ANOVA stands for variance analysis,
which aims to test whether significant difference ex-
ists between the scores given by human and ma-
chine raters. The improvement of our RF bagging
K-LambdaMART over each baseline in percentage
is also given.
For prompt-specific rating model, all of these al-
gorithms achieve good performance comparable to
human raters as literatures have revealed that the
agreement between two professional human raters
(measured by statistics for correlation analysis, e.g.
quadratic weighted Kappa) is around 0.70 to 0.80
(Williamson, 2009) (Williamson, 2009). It is clear
that our listwise learning to rank approach, Random
Forests bagging K-LambdaMART, gives the best
performance on the ASAP dataset. The variance
analysis result on the six groups of scores (scores
given by five times of five-fold cross-validation and
the scores provided by human rater), no signifi-
cant difference, suggests the robustness of our pro-
posed approach. On the contrary, although pref-
</bodyText>
<page confidence="0.975626">
1748
</page>
<bodyText confidence="0.99994482051282">
erence ranking based approach, SVM for ranking,
and regression based approach, SVM for regression,
give very good result in human-machine agreement,
their variance analysis results indicate that there ex-
ists significant difference between the scores given
by human and machine raters. The result of the first
set of experiments suggests the effectiveness and ro-
bustness of our listwise learning to rank approach in
the building of prompt-specific rating model.
For generic rating model, one can conclude from
Table 1 that RF bagging LambdaMART performs
better than SVM for classification, regression and
preference ranking on the ASAP dataset. The
dataset used in our experiment consists of essays
generated by 8 prompts and each prompt has its own
features. With such a training set, both classifica-
tion and regression based approaches produce not
good results, as it is commonly accepted that rat-
ing model whose performance measured by inter-
rater agreement lower than 0.70 is not applicable
(Williamson, 2009). And the variance analysis re-
sults also reveal that there exists statistically sig-
nificant difference between the scores given by hu-
man and machine raters, indicating a low robustness
of these two baselines. The performance compar-
ison of the generic rating models suggest that the
rank based approaches, SVMp and RF bagging K-
LambdaMART, are more effective than the classifi-
cation based SVMc and the regression based SVMr,
while our proposed RF bagging K-LambdaMART
outperforms the state-of-the-art SVMp. Moreover,
we find that there is no obvious performance dif-
ference when our proposed method is applied to
prompt-specific and generic rating models. Consid-
ering the advantages generic rating models have, the
result of the second set of experiments suggests the
feasibility of building a rating model which is gen-
eralizable across different prompts while performs
slightly inferior to the prompt-specific rating model.
</bodyText>
<subsectionHeader confidence="0.998974">
5.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999992387755102">
Table 2 gives the results of feature ablation and
unique test. In the table, “All features” stands for
the use of all the features available, apart from the
prompt-specific features that are not applicable to
learning a generic model. In other rows, the feature
subset name stands for the feature subset to be ab-
lated in ablation test and the feature subset to be used
in unique test. Note that we ablate (as in the ablation
test) or use (as in the unique test) a subset of features
such as the different statistics of word length as a
whole since features belonging to the same subset
are usually highly correlated.
Among the lexical features, the two feature sub-
sets, word level and statistics of word length, are
highly correlated with essay score in both prompt-
specific and generic rating models. This observation
was expected since word usage is an important no-
tion of writing quality, regardless of essay topics.
In the syntactical features, the feature subset, sen-
tence level, measured by the height and depth of
the parser tree, correlates the most with essay score.
One can infer that long sentences with nested sub-
clauses tend to improve the final ratings.
All grammar and fluency features achieve perfor-
mance around 0.60 in feature unique test for prompt-
specific rating model. What is more, during fea-
ture selection, we find that the Pearson’s correlation
coefficient between the feature values and the final
ratings in each essay prompt ranges from -0.20 to -
0.60, which suggests that our method to estimate the
number of grammar errors is applicable because it
is widely accepted that in the evaluation of student
essays, essays with more grammar errors tend to re-
ceive lower ratings.
Among the content and prompt-specific features,
essay length and word vector similarity features give
good results in feature unique test. The fourth root of
essay length in words has been proved to be a highly
correlated feature by many works on AES (Shermis
and Burstein, 2002). Word vector similarity feature
measures prompt-specific vocabulary usage, which
is also important to essay evaluation.
In ablation test, there is no significant perfor-
mance decrease no matter what feature subset is re-
moved. It seems that each feature subset contributes
little to the overall performance and therefore can be
removed. However, the result of feature unique test
suggests that most features used in our rating model
are in fact highly correlated with the writing quality.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999809333333333">
We have proposed a listwise learning to rank ap-
proach to automated essay scoring (AES) by di-
rectly incorporating the human-machine agreement
</bodyText>
<page confidence="0.99814">
1749
</page>
<tableCaption confidence="0.999242">
Table 2: Results of feature ablation and unique test
</tableCaption>
<table confidence="0.999771952380952">
Feature subset Prompt-specific Generic
All Features 0.8014 0.7831
Ablation Unique Ablation Unique
Lexical features
Statistics of word length 0.7763 0.7512 0.7801 0.7350
Word level 0.7834 0.7582 0.7779 0.7306
Unique words 0.7766 0.6737 0.7692 0.6786
Spelling errors 0.7724 0.6863 0.7730 0.6742
Syntactical features
Statistics of sentence length 0.7856 0.6410 0.7684 0.7025
Subclauses 0.7862 0.5473 0.7813 0.5050
Sentence level 0.7749 0.7046 0.7796 0.6955
Mode, preposition, comma 0.7847 0.5860 0.7807 0.5606
Grammar and fluency features
Word bigrams and trigrams 0.7813 0.6017 0.7824 0.4395
POS bigrams and trigrams 0.7844 0.6410 0.7786 0.6022
Content and prompt-specific features
Essay length 0.7930 0.7502 0.7736 0.7390
Word vector similarity 0.7658 0.7001 – –
Semantic vector similarity 0.7924 0.5683 – –
Text coherence 0.7863 0.6947 0.7798 0.6367
</table>
<bodyText confidence="0.999904413793103">
into the loss function. Experiments on the public En-
glish dataset ASAP show that our approach outper-
forms the state-of-the-art algorithms in both prompt-
specific and generic rating settings. Moreover, it is
widely accepted that the agreement between profes-
sional human raters ranges from 0.70 to 0.80, mea-
sured by quadratic weighted Kappa or Pearson’s cor-
relation (Powers et al., 2000) (Williamson, 2009). In
the experiments, our approach achieves a quadratic
weighted Kappa around 0.80 for prompt-specific rat-
ing and around 0.78 for generic rating, suggesting its
potential in automated essay scoring.
Most existing research on AES focus on train-
ing a prompt-specific rating model. While such ap-
proaches have the advantage of providing a satisfac-
tory rating accuracy for essays written for a specific
topic, they also suffer from validity and feasibility
problem as a significant amount of training data,
namely essays with human ratings, are required for
every given essay topic (Attali et al., 2010). It
is therefore appealing to develop an approach that
learns a generic model with acceptable rating accu-
racy, since it has both validity-related and logistical
advantages. In our future work, we plan to continue
the research on generic rating model. Because of
the diversification of writing features of essays asso-
ciated with different prompts, a viable approach is to
explore more generic writing features that can well
reflect the writing quality.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999811">
This work is supported in part by the Na-
tional Natural Science Foundation of China
(61103131/F020511), the President Fund of
UCAS (Y15101FY00/Y25102HN00), and the
National Key Technology R&amp;D Program of China
(2012BAH23B03).
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967385">
Y. Attali and J. Burstein. 2006. Automated essay scoring
with e-rater⃝R v. 2. The Journal of Technology, Learn-
ing and Assessment, 4(3).
Yigal Attali, Brent Bridgeman, and Catherine Trapani.
2010. Performance of a generic approach in auto-
mated essay scoring. The Journal of Technology,
Learning and Assessment, 10(3).
</reference>
<page confidence="0.888368">
1750
</page>
<reference confidence="0.999801803738318">
L. Breiman. 2001. Random forests. Machine learning,
45(1):5–32.
H.M. Breland, R.J. Jones, and L. Jenkins. 1994. The
college board vocabulary study. College Entrance Ex-
amination Board.
Hermann Brenner and Ulrike Kliebsch. 1996. Depen-
dence of weighted kappa coefficients on the number
of categories. Epidemiology, pages 199–202.
T. Briscoe, B. Medlock, and Ø. Andersen. 2010. Au-
tomated assessment of esol free text examinations.
Technical report, University of Cambridge Computer
Laboratory Technical Reports, UCAM-CL-TR-790.
C. Burges. 2010. From ranknet to lambdarank to lamb-
damart: An overview. Learning, 11:23–581.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 722–731.
Hongbo Chen, Ben He, Tiejian Luo, and Baobin Li.
2012. A ranked-based learning approach to automated
essay scoring. In Cloud and Green Computing (CGC),
2012 Second International Conference on, pages 448–
455. IEEE.
Jacob Cohen et al. 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement, 20(1):37–46.
Richard Courant. 2005. Dirichlet’s principle, conformal
mapping, and minimal surfaces. Courier Dover Publi-
cations.
S. Dikli. 2006. An overview of automated scoring of
essays. The Journal of Technology, Learning and As-
sessment, 5(1).
S.T. Dumais. 2005. Latent semantic analysis. An-
nual Review of Information Science and Technology,
38(1):188–230.
Peter W Foltz, Darrell Laham, and Thomas K Landauer.
1999. Automated essay scoring: Applications to edu-
cational technology. In World Conference on Educa-
tional Multimedia, Hypermedia and Telecommunica-
tions, volume 1999, pages 939–944.
Jerome H. Friedman. 2000. Greedy function approxima-
tion: A gradient boosting machine. Annals of Statis-
tics, 29:1189–1232.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217–226. ACM.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Associ-
ation for Computational Linguistics.
L.S. Larkey. 1998. Automatic essay grading using
text categorization techniques. In Proceedings of the
21st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 90–95. ACM.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
P. Li, C. Burges, and Q. Wu. 2007. Learning to rank us-
ing classification and gradient boosting. In Proceed-
ings of the International Conference on Advances in
Neural Information Processing Systems (NIPS).
T.Y. Liu. 2009. Learning to rank for information re-
trieval. Foundations and Trends in Information Re-
trieval, 3(3):225–331.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL 2004, pages 38–41, Boston, Massachusetts,
2-7 May. Association for Computational Linguistics,
Stroudsburg, PA, USA.
Donald E Powers, Jill C Burstein, Martin Chodorow,
Mary E Fowles, Karen Kukich, and Graduate
Record Examinations Board. 2000. Comparing the
validity of automated and human essay scoring. RE-
SEARCH REPORT-EDUCATIONAL TESTING SER-
VICE PRINCETON RR, (10).
Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng
Wang, Tie-Yan Liu, and Hang Li. 2008. Query-level
loss functions for information retrieval. Inf. Process.
Manage., 44(2):838–855, mar.
G. Salton. 1971. The SMART Retrieval System-
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Henry Scheffe. 1999. The analysis of variance, vol-
ume 72. Wiley. com.
M.D. Shermis and J.C. Burstein. 2002. Automated essay
scoring: A cross-disciplinary perspective. Lawrence
Erlbaum.
Kristina Toutanova, Dan Klein, Christopher D Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 173–180. Association for Computational Lin-
guistics.
Vladimir Vapnik, Steven E. Golowich, and Alex Smola.
1996. Support vector method for function approxima-
tion, regression estimation, and signal processing. In
Advances in Neural Information Processing Systems 9,
pages 281–287. MIT Press.
</reference>
<page confidence="0.810503">
1751
</page>
<reference confidence="0.999750684210526">
D.M. Williamson. 2009. A framework for implement-
ing automated scoring. In Annual Meeting of the
American Educational Research Association and the
National Council on Measurement in Education, San
Diego, CA.
Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2008.
Ranking, boosting, and model adaptation. Technical
report.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
esol texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, volume 1, pages 180–
189.
Yisong Yue and C Burges. 2007. On using simultane-
ous perturbation stochastic approximation for learn-
ing to rank, and the empirical optimality of lamb-
darank. Technical report, Technical Report MSR-TR-
2007-115, Microsoft Research.
</reference>
<page confidence="0.9941">
1752
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805873">
<title confidence="0.999536">Automated Essay Scoring by Maximizing Human-machine Agreement</title>
<author confidence="0.99634">Hongbo Chen</author>
<author confidence="0.99634">Ben</author>
<affiliation confidence="0.9960215">School of Computer and Control University of Chinese Academy of</affiliation>
<address confidence="0.977589">100190 Beijing,</address>
<email confidence="0.886024">chenhongbo11@mails.ucas.ac.cn,benhe@ucas.ac.cn</email>
<abstract confidence="0.9974278">Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Attali</author>
<author>J Burstein</author>
</authors>
<title>Automated essay scoring with e-rater⃝R v. 2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="2624" citStr="Attali and Burstein, 2006" startWordPosition="402" endWordPosition="405">disagreement between the first human rater and the machine rater. With the help of an AES system that highly agrees with human raters, the human workload can be reduced by half at most. Therefore, the agreement between the AES system and the human rater is an important indicator of an AES system’s effectiveness. There have been efforts in developing AES methods since the 1960s. Various kinds of algorithms and models based on NLP and machine learning techniques have been proposed to implement AES systems. Existing approaches consider essay rating as a classification (Larkey, 1998), regression (Attali and Burstein, 2006) or preference ranking problem (Yannakoudakis et al., 2011), where the loss function is the regression loss, classification loss and pairwise classification loss, respectively. In this paper, we argue that the purpose of AES is to predict the essay’s rating that human raters would give. If an AES system frequently disagrees with the first human rater, a second human rater will be needed in most cases. Thus, the introduction of the AES system does not bring much benefit in reducing the human workload. It is therefore desirable to minimize the disagreement between the machine and human raters. H</context>
<context position="6954" citStr="Attali and Burstein, 2006" startWordPosition="1082" endWordPosition="1085">s, e.g. fourth root of essay length, and uses regression-based approach to predict the score that human raters will give. E-rater, developed by Educational Testing Services (ETS) in America, in late 1990s, is a commercial AES system which has been put into practical use in the Graduate Record Examination (GRE) and the Test of English as a Foreign Language (TOEFL). The E-rater system uses natural language processing techniques to extract various kinds of linguistic features of essays, such as lexical, syntactic, grammar, etc.. Then it predicts the final score by the stepwise regression method (Attali and Burstein, 2006). The classification-based approach sees essay scores as in-discriminative class labels and uses classical classification algorithms, e.g. the K-nearest neighbor (KNN) and the naive Bayesian model, to predict to which class an essay belongs, where a class is associated to a numeric rating. Intelligent Essay Assessor (IEA) (Foltz et al., 1999), developed also in late 1990s, evaluates essay by measuring semantic features. Each ungraded essay, represented by a semantic vector generated by Latent Semantic Analysis (LSA) (Dumais, 2005), is rated according to the similarity degree with semantic vect</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Y. Attali and J. Burstein. 2006. Automated essay scoring with e-rater⃝R v. 2. The Journal of Technology, Learning and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Brent Bridgeman</author>
<author>Catherine Trapani</author>
</authors>
<title>Performance of a generic approach in automated essay scoring.</title>
<date>2010</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="9329" citStr="Attali et al., 2010" startWordPosition="1461" endWordPosition="1464">ires several hundreds of graded essays for training, which is time-consuming and usually impractical in a classroom environment. Generic rating model is trained from essays across a group of prompts and designed to be the best fit for predicting human scores for all prompts. It usually does not consider prompt-specific features and just takes writing quality into account. Generic rating model evaluates essays across all prompts with the same scoring criteria, which is more consistent with the human rubric that is usually the same for all prompts, and therefore has validity-related advantages (Attali et al., 2010). 2.2 Learning to Rank Learning to rank, also called machine-learned ranking, was originally proposed to settle the ranking problem in information retrieval (IR) (Liu, 2009). It is a type of supervised or semi-supervised machine learning algorithm that automatically construct a ranking model or function from training data. Current learning to rank algorithms fall into three categories, that is, the pointwise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vec</context>
<context position="40432" citStr="Attali et al., 2010" startWordPosition="6551" endWordPosition="6554">00) (Williamson, 2009). In the experiments, our approach achieves a quadratic weighted Kappa around 0.80 for prompt-specific rating and around 0.78 for generic rating, suggesting its potential in automated essay scoring. Most existing research on AES focus on training a prompt-specific rating model. While such approaches have the advantage of providing a satisfactory rating accuracy for essays written for a specific topic, they also suffer from validity and feasibility problem as a significant amount of training data, namely essays with human ratings, are required for every given essay topic (Attali et al., 2010). It is therefore appealing to develop an approach that learns a generic model with acceptable rating accuracy, since it has both validity-related and logistical advantages. In our future work, we plan to continue the research on generic rating model. Because of the diversification of writing features of essays associated with different prompts, a viable approach is to explore more generic writing features that can well reflect the writing quality. Acknowledgements This work is supported in part by the National Natural Science Foundation of China (61103131/F020511), the President Fund of UCAS </context>
</contexts>
<marker>Attali, Bridgeman, Trapani, 2010</marker>
<rawString>Yigal Attali, Brent Bridgeman, and Catherine Trapani. 2010. Performance of a generic approach in automated essay scoring. The Journal of Technology, Learning and Assessment, 10(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="12287" citStr="Breiman, 2001" startWordPosition="1941" endWordPosition="1942"> features used in our approach are presented in section 3.2. 3.1 Listwise Learning to Rank for AES Our choice of the listwise learning to rank algorithm is due to the fact that it takes the entire set of labeled essays associated to a given prompt, instead of the individual essays or essay pairs as in (Yannakoudakis et al., 2011), as training examples. This brings us the convenience of easily embedding the inter-rater agreement into the loss function for the learning. In this paper, we deploy LambdaMART (Wu et al., 2008), a listwise learning to rank algorithm and then use Random Forests (RF) (Breiman, 2001) for the bagging of LambdaMART learners. Having been widely used in information retrieval applications, LambdaMART is one of the most effective learning to rank algorithms. For instance, it achieves the top results in the 2010 Yahoo! Learning to Rank challenge (Burges, 2010). Random Forests is an 1743 ensemble learning method for classification and regression. Previously, the loss function of LambdaMART is defined as the gradient loss of the retrieval effectiveness, measured by IR evaluation criteria such as Normalized Discounted Cumulative Gain (nDCG) (Wu et al., 2008). More specifically, it </context>
<context position="14831" citStr="Breiman, 2001" startWordPosition="2359" endWordPosition="2360">cessary to re-define the A. Specifically, we need to define the gradient of the evaluation criteria in AES, e.g. quadratic weighted Kappa (Brenner and Kliebsch, 1996) and Pearson’s correlation coefficient, with respect to the model score of each essay. In this paper, we use quadratic weighted Kappa as the evaluation metric. Kappa (Cohen and others, 1960) is a statistical metric which is used to measure inter-rater agreement. Quadratic weighted Kappa takes the degree of disagreement between raters into account. This measuring method 1For space reason, we refer the readers to (Friedman, 2000), (Breiman, 2001) for details of MART, GBDT and Random Forests. is widely accepted as a primary evaluation metric for the AES tasks. For instance, it is the official evaluation metric in the Automated Student Assessment Prize sponsored by Hewlett Foundation2. We denote our modified LambdaMART as K-LambdaMART in which K stands for the Kappa-based gradient function. Specific steps include the following: To begin with, we re-define the Ai,j for each pair of essays. For a pair of essays, essay i and essay j, Ai,j is defined as the derivative of RankNet (Li et al., 2007) loss function multiplied by the Quadratic we</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>L. Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Breland</author>
<author>R J Jones</author>
<author>L Jenkins</author>
</authors>
<title>The college board vocabulary study. College Entrance Examination Board.</title>
<date>1994</date>
<contexts>
<context position="19676" citStr="Breland et al., 1994" startWordPosition="3242" endWordPosition="3245">te the essay quality, including lexical, syntactical, grammar and fluency, content and prompt-specific features. A brief description of these four classes of features is given below. Lexical features: We define 4 subsets of lexical features. Each subset of features consists of one or several sub features. – Statistics of word length: The number of words with length in characters larger than 4, 6, 8, 10, 12, respectively. The mean and variance of word length in characters. – word level: All words in Webster dictionary 3 are divided into 8 levels according to the College Board Vocabulary Study (Breland et al., 1994). The higher level a word belongs to, the more sophisticated vocabulary usage it indicates. For example, words like thoroughfare, percolate are in level 8, while words with the same meanings, street, filter, belong to level 1. We count the number of words that belong to each level and calculate the mean word level of a given essay. – Unique words: The number of unique words appeared in each essay, normalized by the essay length in words. – Spelling errors: The number of spelling errors detected by the spelling check API provided by Google 4. Syntactical features: There are 4 subsets of syntact</context>
</contexts>
<marker>Breland, Jones, Jenkins, 1994</marker>
<rawString>H.M. Breland, R.J. Jones, and L. Jenkins. 1994. The college board vocabulary study. College Entrance Examination Board.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Brenner</author>
<author>Ulrike Kliebsch</author>
</authors>
<title>Dependence of weighted kappa coefficients on the number of categories. Epidemiology,</title>
<date>1996</date>
<pages>199--202</pages>
<contexts>
<context position="4537" citStr="Brenner and Kliebsch, 1996" startWordPosition="703" endWordPosition="706">ank-based approach follows the listwise learning paradigm and the agreement between the machine and human raters is directly integrated into the loss function that is optimized by gradient boost regression trees. To the best of our knowledge, this work is the first to apply listwise learning to rank approach for AES, which aims at the optimization of the agreement between the human and machine raters. Experimental results on the publicly available dataset ASAP indicate that our proposed method achieves high agreement with human raters, that is about 0.80, measured by quadratic weighted Kappa (Brenner and Kliebsch, 1996). Our proposed method also outperforms the previous classification, regression and preference ranking based approaches. As it is widely accepted that the agreement between human raters, measured by either quadratic weighted Kappa or Pearson’s correlation coefficient, ranges from 0.70 to 0.80 (Powers et al., 2000) (Williamson, 2009), our proposed approach therefore performs as well as human raters. The rest of this paper is organized as follows. In section 2, we introduce the research background of automated essay scoring and give a brief introduction to learning to rank. In section 3, a detail</context>
<context position="14383" citStr="Brenner and Kliebsch, 1996" startWordPosition="2288" endWordPosition="2291">pute the derivative. As for the automated essay scoring, LambdaMART is not readily available since its loss function is defined as a function of the gradient of IR evaluation measures. While such measures focus on the top-ranked documents that are of great importance to the IR applications, they are not suitable to our study. This is because for AES, the rating prediction of all essays equally matters, no matter what ratings they receive. It is therefore necessary to re-define the A. Specifically, we need to define the gradient of the evaluation criteria in AES, e.g. quadratic weighted Kappa (Brenner and Kliebsch, 1996) and Pearson’s correlation coefficient, with respect to the model score of each essay. In this paper, we use quadratic weighted Kappa as the evaluation metric. Kappa (Cohen and others, 1960) is a statistical metric which is used to measure inter-rater agreement. Quadratic weighted Kappa takes the degree of disagreement between raters into account. This measuring method 1For space reason, we refer the readers to (Friedman, 2000), (Breiman, 2001) for details of MART, GBDT and Random Forests. is widely accepted as a primary evaluation metric for the AES tasks. For instance, it is the official eva</context>
</contexts>
<marker>Brenner, Kliebsch, 1996</marker>
<rawString>Hermann Brenner and Ulrike Kliebsch. 1996. Dependence of weighted kappa coefficients on the number of categories. Epidemiology, pages 199–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>B Medlock</author>
<author>Ø Andersen</author>
</authors>
<title>Automated assessment of esol free text examinations.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>University of Cambridge Computer Laboratory</institution>
<contexts>
<context position="21599" citStr="Briscoe et al., 2010" startWordPosition="3569" endWordPosition="3572"> NLP. The height of the parser tree is also incorpo1745 rated into the feature set. – Mode, preposition, comma: The number of modes, prepositions and commas in each sentence respectively, normalized by sentence length in words. Part of speech (POS) is detected by Stanford Core NLP (Toutanova et al., 2003). The POS tags of modal verb and preposition are “MD” and “IN”, respectively. Grammar and fluency features: There are two subsets of grammar and fluency features. – Word bigram and trigram: We evaluate the grammar and fluency of an essay by calculating mean tf/TF of word bigrams and trigrams (Briscoe et al., 2010) (tf is the term frequency in a single essay and TF is the term frequency in the whole essay collection). We assume a bigram or trigram with high tf/TF as a grammar error because high tf/TF means that this kind of bigram or trigram is not commonly used in the whole essay collection but appears in the specific essay. – POS bigram and trigram: Mean tf/TF of POS bigrams and trigrams. The reason is the same with word bigrams and trigrams. Content and prompt-specific features: We define four subsets of content and prompt-specific features. – Essay length: Essay length in characters and words, respe</context>
<context position="27005" citStr="Briscoe et al., 2010" startWordPosition="4452" endWordPosition="4455">d by preliminary experiments on the ASAP validation set are as follows. For bagging: the number of bags is set to 300, subsampling rate is 0.80 and feature sampling rate is 0.50. For LambdaMART in each bag: the number of trees is set to 1, the number of tree leaves is 100 and other parameters are set to default. Baseline algorithms We use classical machine learning algorithms, support vector machine (SVM) for classification, regression (Vapnik et al., 1996) and preference ranking (Joachims, 2006), respectively, as the baselines. These three algorithms have been used for AES in the literature (Briscoe et al., 2010) (Yannakoudakis et al., 2011). Especially, the state-of-the-art AES approach proposed by (Yannakoudakis et al., 2011) utilizes the SVM for preference ranking, a pairwise learning to rank algorithm, for training a rating model. The linear kernel is used in the experiments. The parameter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not included in the baseline algorithms as it has been shown that the performance of LambdaMART is inferior to ranking 6http://www.kaggle.com/c/asap-sas/data SVM on</context>
</contexts>
<marker>Briscoe, Medlock, Andersen, 2010</marker>
<rawString>T. Briscoe, B. Medlock, and Ø. Andersen. 2010. Automated assessment of esol free text examinations. Technical report, University of Cambridge Computer Laboratory Technical Reports, UCAM-CL-TR-790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burges</author>
</authors>
<title>From ranknet to lambdarank to lambdamart: An overview.</title>
<date>2010</date>
<booktitle>Learning,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="12562" citStr="Burges, 2010" startWordPosition="1984" endWordPosition="1985">ssays or essay pairs as in (Yannakoudakis et al., 2011), as training examples. This brings us the convenience of easily embedding the inter-rater agreement into the loss function for the learning. In this paper, we deploy LambdaMART (Wu et al., 2008), a listwise learning to rank algorithm and then use Random Forests (RF) (Breiman, 2001) for the bagging of LambdaMART learners. Having been widely used in information retrieval applications, LambdaMART is one of the most effective learning to rank algorithms. For instance, it achieves the top results in the 2010 Yahoo! Learning to Rank challenge (Burges, 2010). Random Forests is an 1743 ensemble learning method for classification and regression. Previously, the loss function of LambdaMART is defined as the gradient loss of the retrieval effectiveness, measured by IR evaluation criteria such as Normalized Discounted Cumulative Gain (nDCG) (Wu et al., 2008). More specifically, it is a heuristic method that directly defines A, the gradient of nDCG with respect to the model score of each document, and has been shown to work empirically for particular loss functions NDCG (Yue and Burges, 2007). Then, Multiple Additive Regression Trees (MART) (Friedman, </context>
</contexts>
<marker>Burges, 2010</marker>
<rawString>C. Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning, 11:23–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>722--731</pages>
<contexts>
<context position="27911" citStr="Chen and Zechner, 2011" startWordPosition="4590" endWordPosition="4593">ter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not included in the baseline algorithms as it has been shown that the performance of LambdaMART is inferior to ranking 6http://www.kaggle.com/c/asap-sas/data SVM on the same dataset (Chen et al., 2012). 4.3 Feature Selection Although machine learning approaches usually use the all features available for training, we try to obtain an carefully selected feature set that can withstand the scrutiny of construct validity in assessment development (Chen and Zechner, 2011). Specific steps of feature selection conducted on individual features are as follows: To begin with, the importance of the features is determined by computing each features Pearson correlation coefficient with human raters scores based on the training set (Chen and Zechner, 2011). Features whose absolute Pearson correlation coefficient with human scores are lower than 0.20 are removed from the feature set. Next, we calculate the inter-correlation degrees between these features. For each pair of features whose Pearson correlation coefficient larger than 0.90, one of them should be removed. The</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongbo Chen</author>
<author>Ben He</author>
<author>Tiejian Luo</author>
<author>Baobin Li</author>
</authors>
<title>A ranked-based learning approach to automated essay scoring.</title>
<date>2012</date>
<booktitle>In Cloud and Green Computing (CGC), 2012 Second International Conference on,</booktitle>
<pages>448--455</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="27642" citStr="Chen et al., 2012" startWordPosition="4550" endWordPosition="4553">al., 2011). Especially, the state-of-the-art AES approach proposed by (Yannakoudakis et al., 2011) utilizes the SVM for preference ranking, a pairwise learning to rank algorithm, for training a rating model. The linear kernel is used in the experiments. The parameter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not included in the baseline algorithms as it has been shown that the performance of LambdaMART is inferior to ranking 6http://www.kaggle.com/c/asap-sas/data SVM on the same dataset (Chen et al., 2012). 4.3 Feature Selection Although machine learning approaches usually use the all features available for training, we try to obtain an carefully selected feature set that can withstand the scrutiny of construct validity in assessment development (Chen and Zechner, 2011). Specific steps of feature selection conducted on individual features are as follows: To begin with, the importance of the features is determined by computing each features Pearson correlation coefficient with human raters scores based on the training set (Chen and Zechner, 2011). Features whose absolute Pearson correlation coef</context>
</contexts>
<marker>Chen, He, Luo, Li, 2012</marker>
<rawString>Hongbo Chen, Ben He, Tiejian Luo, and Baobin Li. 2012. A ranked-based learning approach to automated essay scoring. In Cloud and Green Computing (CGC), 2012 Second International Conference on, pages 448– 455. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and psychological measurement,</booktitle>
<pages>20--1</pages>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen et al. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Courant</author>
</authors>
<title>Dirichlet’s principle, conformal mapping, and minimal surfaces.</title>
<date>2005</date>
<publisher>Courier Dover Publications.</publisher>
<contexts>
<context position="31350" citStr="Courant, 2005" startWordPosition="5119" endWordPosition="5120">mly partitioned into 5 subsets. In each fold, 4 subsets are used for training, and one is used for testing. To avoid bias introduced by the random partition, we repeat the 5-fold crossvalidation for 5 times on 5 different random partitions. The overall quadratic weighted Kappa is averaged on all 25 test subsets. It should be noticed that in random partition of the whole dataset, the overlap between any two partitions should be kept below 1.5*1/(#folds)*100%. For example, in five-fold cross validation, the overlap should be kept below 30%. This is because: according to the Dirichlet principle (Courant, 2005), each subset in one partition overlaps more than 20% with at least one subset in another partition in fivefold cross-validation. The tolerance boundary parameter is then set to 1.5. The objective of the second set of experiments is to test the performance of our listwise learning to rank approach for generic rating models. We also conduct 5 times 5-fold cross-validation like the first experiment. In 5-fold cross-validation, essays associated with the same prompt are randomly partitioned into 5 subsets. In this way, each fold consists of essays across all prompts. The overall performance is av</context>
</contexts>
<marker>Courant, 2005</marker>
<rawString>Richard Courant. 2005. Dirichlet’s principle, conformal mapping, and minimal surfaces. Courier Dover Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dikli</author>
</authors>
<title>An overview of automated scoring of essays.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1443" citStr="Dikli, 2006" startWordPosition="205" endWordPosition="206">Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring. 1 Introduction Automated essay scoring utilizes the NLP techniques to automatically rate essays written for given prompts, namely, essay topics, in an educational setting (Dikli, 2006). Nowadays, AES systems have been put into practical use in large-scale English tests and play the role of one human rater. For example, before AES systems enter the picture, essays in the writing assessment of Graduate Record Examination (GRE) are rated by two human raters. A third human rater is needed when the difference of the scores given by the two human raters is larger than one in the 6-point scale. Currently, GRE essays are rated by one human rater and one AES system. A second human rater is required only when there exists a non-negligible disagreement between the first human rater an</context>
</contexts>
<marker>Dikli, 2006</marker>
<rawString>S. Dikli. 2006. An overview of automated scoring of essays. The Journal of Technology, Learning and Assessment, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
</authors>
<title>Latent semantic analysis.</title>
<date>2005</date>
<journal>Annual Review of Information Science and Technology,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="7490" citStr="Dumais, 2005" startWordPosition="1164" endWordPosition="1165">the final score by the stepwise regression method (Attali and Burstein, 2006). The classification-based approach sees essay scores as in-discriminative class labels and uses classical classification algorithms, e.g. the K-nearest neighbor (KNN) and the naive Bayesian model, to predict to which class an essay belongs, where a class is associated to a numeric rating. Intelligent Essay Assessor (IEA) (Foltz et al., 1999), developed also in late 1990s, evaluates essay by measuring semantic features. Each ungraded essay, represented by a semantic vector generated by Latent Semantic Analysis (LSA) (Dumais, 2005), is rated according to the similarity degree with semantic vectors of graded essays. Bayesian Essay Test Scoring sYstem, developed by Larkey in 2003, is based on naive Bayesian model. It is the only open-source AES system, but has not been put into practical use yet. Besides classification and regression-based approaches, (Yannakoudakis et al., 2011) proposed a preference ranking based approach for learning a rating model, where a ranking function or model is learned to construct a global ordering of essays based on writing quality. It is also the first study of rank-based approach in automat</context>
<context position="22746" citStr="Dumais, 2005" startWordPosition="3763" endWordPosition="3764">tures. – Essay length: Essay length in characters and words, respectively. The fourth root of essay length in words is proved to be highly correlated with the essay score (Shermis and Burstein, 2002). – Word vector similarity: Mean cosine similarity of word vectors, in which the element is the term frequency multiplied by inverse document frequency (tf-idf) (Salton, 1971) of each word. It is calculated as the weighted mean of all cosine similarities and the weight is set as the corresponding essay score. – Semantic vector similarity: Semantic vectors are generated by Latent Semantic Analysis (Dumais, 2005). The calculation of mean cosine similarity of semantic vectors is the same with word vector similarity. – Text coherence: Coherence in writing means that all the ideas in a paragraph flow smoothly from one sentence to the next. We only consider nouns and pronouns in each sentence as they convey more information. The relevance degree between one sentence and its next in the same paragraph is calculated as the sum of the similarity degrees between nouns and pronouns appeared in the two sentences, normalized by the sum of the two sentences’ length in words. The similarity degree between words is</context>
</contexts>
<marker>Dumais, 2005</marker>
<rawString>S.T. Dumais. 2005. Latent semantic analysis. Annual Review of Information Science and Technology, 38(1):188–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automated essay scoring: Applications to educational technology.</title>
<date>1999</date>
<booktitle>In World Conference on Educational Multimedia, Hypermedia and Telecommunications,</booktitle>
<volume>volume</volume>
<pages>939--944</pages>
<contexts>
<context position="7298" citStr="Foltz et al., 1999" startWordPosition="1133" endWordPosition="1136">nguage (TOEFL). The E-rater system uses natural language processing techniques to extract various kinds of linguistic features of essays, such as lexical, syntactic, grammar, etc.. Then it predicts the final score by the stepwise regression method (Attali and Burstein, 2006). The classification-based approach sees essay scores as in-discriminative class labels and uses classical classification algorithms, e.g. the K-nearest neighbor (KNN) and the naive Bayesian model, to predict to which class an essay belongs, where a class is associated to a numeric rating. Intelligent Essay Assessor (IEA) (Foltz et al., 1999), developed also in late 1990s, evaluates essay by measuring semantic features. Each ungraded essay, represented by a semantic vector generated by Latent Semantic Analysis (LSA) (Dumais, 2005), is rated according to the similarity degree with semantic vectors of graded essays. Bayesian Essay Test Scoring sYstem, developed by Larkey in 2003, is based on naive Bayesian model. It is the only open-source AES system, but has not been put into practical use yet. Besides classification and regression-based approaches, (Yannakoudakis et al., 2011) proposed a preference ranking based approach for learn</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>Peter W Foltz, Darrell Laham, and Thomas K Landauer. 1999. Automated essay scoring: Applications to educational technology. In World Conference on Educational Multimedia, Hypermedia and Telecommunications, volume 1999, pages 939–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<pages>29--1189</pages>
<contexts>
<context position="13167" citStr="Friedman, 2000" startWordPosition="2079" endWordPosition="2080">ges, 2010). Random Forests is an 1743 ensemble learning method for classification and regression. Previously, the loss function of LambdaMART is defined as the gradient loss of the retrieval effectiveness, measured by IR evaluation criteria such as Normalized Discounted Cumulative Gain (nDCG) (Wu et al., 2008). More specifically, it is a heuristic method that directly defines A, the gradient of nDCG with respect to the model score of each document, and has been shown to work empirically for particular loss functions NDCG (Yue and Burges, 2007). Then, Multiple Additive Regression Trees (MART) (Friedman, 2000), also called Gradient Boosting Decision Tree (GBDT)l, are used to “learn” these gradients iteratively. MART is a class of boosting algorithms that performs gradient descent in function space, using regression trees. Its output F(x) can be written as F(x) = &amp; αifi(x), i = 1, 2, ....N. Each fi(x) is a function modeled by a single regression tree and the αi is the corresponding weight. Given that n trees have been trained, the (n+1)th regression tree, fi+1(x), models the derivative of the cost with respect to the current model score at each training point. Thus, what remains is to compute the de</context>
<context position="14814" citStr="Friedman, 2000" startWordPosition="2357" endWordPosition="2358">It is therefore necessary to re-define the A. Specifically, we need to define the gradient of the evaluation criteria in AES, e.g. quadratic weighted Kappa (Brenner and Kliebsch, 1996) and Pearson’s correlation coefficient, with respect to the model score of each essay. In this paper, we use quadratic weighted Kappa as the evaluation metric. Kappa (Cohen and others, 1960) is a statistical metric which is used to measure inter-rater agreement. Quadratic weighted Kappa takes the degree of disagreement between raters into account. This measuring method 1For space reason, we refer the readers to (Friedman, 2000), (Breiman, 2001) for details of MART, GBDT and Random Forests. is widely accepted as a primary evaluation metric for the AES tasks. For instance, it is the official evaluation metric in the Automated Student Assessment Prize sponsored by Hewlett Foundation2. We denote our modified LambdaMART as K-LambdaMART in which K stands for the Kappa-based gradient function. Specific steps include the following: To begin with, we re-define the Ai,j for each pair of essays. For a pair of essays, essay i and essay j, Ai,j is defined as the derivative of RankNet (Li et al., 2007) loss function multiplied by</context>
</contexts>
<marker>Friedman, 2000</marker>
<rawString>Jerome H. Friedman. 2000. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10326" citStr="Joachims, 2006" startWordPosition="1612" endWordPosition="1613">ories, that is, the pointwise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise approaches process a pair of documents each time and usually model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms process a list of documents each time and the loss function aims at measuring the accordance between predicted ranking list and the ground truth label. Representative algorithms are LambdaMart (Wu et al., 2008), RankCosine (Qin et al., 2008), etc.. Listwise approach has not yet been used in automated essay scoring. 3 Automated Essay Scoring by Maximizing Human-ma</context>
<context position="26885" citStr="Joachims, 2006" startWordPosition="4434" endWordPosition="4435">hms have been implemented, e.g. LambdaMART and RankNet (Li et al., 2007). Empirical settings of parameters obtained by preliminary experiments on the ASAP validation set are as follows. For bagging: the number of bags is set to 300, subsampling rate is 0.80 and feature sampling rate is 0.50. For LambdaMART in each bag: the number of trees is set to 1, the number of tree leaves is 100 and other parameters are set to default. Baseline algorithms We use classical machine learning algorithms, support vector machine (SVM) for classification, regression (Vapnik et al., 1996) and preference ranking (Joachims, 2006), respectively, as the baselines. These three algorithms have been used for AES in the literature (Briscoe et al., 2010) (Yannakoudakis et al., 2011). Especially, the state-of-the-art AES approach proposed by (Yannakoudakis et al., 2011) utilizes the SVM for preference ranking, a pairwise learning to rank algorithm, for training a rating model. The linear kernel is used in the experiments. The parameter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not included in the baseline algorithms as i</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20730" citStr="Klein and Manning, 2003" startWordPosition="3424" endWordPosition="3427">h in words. – Spelling errors: The number of spelling errors detected by the spelling check API provided by Google 4. Syntactical features: There are 4 subsets of syntactical features. – Statistics of sentence length: The number of sentences with length in words larger than 10, 18, 25, respectively. The mean and variance of sentence length in words. – Subclauses: The mean number of subclauses in each sentence, normalized by sentence length in words. The mean subclause length in words. Subclauses are labeled as “SBAR” in the parser tree generated by a commonly used NLP tool, Stanford Core NLP (Klein and Manning, 2003), which is an integrated suite of natural language processing tools for English in Java5, including part-of-speech tagging, parsing, co-reference, etc.. – Sentence level: The sum of the depth of all nodes in a parser tree generated by Stanford Core NLP. The height of the parser tree is also incorpo1745 rated into the feature set. – Mode, preposition, comma: The number of modes, prepositions and commas in each sentence respectively, normalized by sentence length in words. Part of speech (POS) is detected by Stanford Core NLP (Toutanova et al., 2003). The POS tags of modal verb and preposition a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Larkey</author>
</authors>
<title>Automatic essay grading using text categorization techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>90--95</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2584" citStr="Larkey, 1998" startWordPosition="399" endWordPosition="400">re exists a non-negligible disagreement between the first human rater and the machine rater. With the help of an AES system that highly agrees with human raters, the human workload can be reduced by half at most. Therefore, the agreement between the AES system and the human rater is an important indicator of an AES system’s effectiveness. There have been efforts in developing AES methods since the 1960s. Various kinds of algorithms and models based on NLP and machine learning techniques have been proposed to implement AES systems. Existing approaches consider essay rating as a classification (Larkey, 1998), regression (Attali and Burstein, 2006) or preference ranking problem (Yannakoudakis et al., 2011), where the loss function is the regression loss, classification loss and pairwise classification loss, respectively. In this paper, we argue that the purpose of AES is to predict the essay’s rating that human raters would give. If an AES system frequently disagrees with the first human rater, a second human rater will be needed in most cases. Thus, the introduction of the AES system does not bring much benefit in reducing the human workload. It is therefore desirable to minimize the disagreement</context>
<context position="5840" citStr="Larkey, 1998" startWordPosition="907" endWordPosition="908">esented. Section 4 explains the experimental setup and section 5 presents the experimental results. Finally, in section 6 we conclude this research. 2 Related Work and Background Firstly, we give a brief description of existing approaches for AES in section 2.1. Then, an introduction to learning to rank is presented in section 2.2. 2.1 Existing AES Methods In general, existing solutions consider AES as a learning problem. Based on a large number of predefined objectively measurable features, various learning techniques, including classification, regression and preference ranking, are applied (Larkey, 1998) (Yannakoudakis et al., 2011). Regression-based approach treats feature values and essay score as independent variables and dependent variable, respectively, and then learns a regression equation by classical regression algorithms, such as support vector regression (Vapnik et al., 1996). In 1966, the first AES system, Project Essay Grader, was developed by Ellis Page upon the request of the American College Board. The PEG system defines a large set of surface text features from essays, e.g. fourth root of essay length, and uses regression-based approach to predict the score that human raters w</context>
</contexts>
<marker>Larkey, 1998</marker>
<rawString>L.S. Larkey. 1998. Automatic essay grading using text categorization techniques. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 90–95. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entitycentric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="23428" citStr="Lee et al., 2013" startWordPosition="3879" endWordPosition="3882">s the same with word vector similarity. – Text coherence: Coherence in writing means that all the ideas in a paragraph flow smoothly from one sentence to the next. We only consider nouns and pronouns in each sentence as they convey more information. The relevance degree between one sentence and its next in the same paragraph is calculated as the sum of the similarity degrees between nouns and pronouns appeared in the two sentences, normalized by the sum of the two sentences’ length in words. The similarity degree between words is set to 1 if coreference exists, indicated by Stanford Core NLP (Lee et al., 2013). Otherwise, it is measured by WordNet similarity package (Pedersen et al., 2004). Finally, text coherence is computed as the average relevance degree of all pairs of neighbored sentences. The rating model is learned off-line using a set of training essays. For a given target essay, it is the feature extraction that mainly accounts for the overhead. In our experiments, it usually costs in average no more than 10 seconds on a desktop PC with an Intel i5-2410M CPU running at 2.3GHZ to extract the pre-defined features and predict a rating for a given essay, which is affordable, compared to the co</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Li</author>
<author>C Burges</author>
<author>Q Wu</author>
</authors>
<title>Learning to rank using classification and gradient boosting.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="10353" citStr="Li et al., 2007" startWordPosition="1615" endWordPosition="1618">ise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise approaches process a pair of documents each time and usually model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms process a list of documents each time and the loss function aims at measuring the accordance between predicted ranking list and the ground truth label. Representative algorithms are LambdaMart (Wu et al., 2008), RankCosine (Qin et al., 2008), etc.. Listwise approach has not yet been used in automated essay scoring. 3 Automated Essay Scoring by Maximizing Human-machine Agreement The main wo</context>
<context position="15386" citStr="Li et al., 2007" startWordPosition="2452" endWordPosition="2455">on, we refer the readers to (Friedman, 2000), (Breiman, 2001) for details of MART, GBDT and Random Forests. is widely accepted as a primary evaluation metric for the AES tasks. For instance, it is the official evaluation metric in the Automated Student Assessment Prize sponsored by Hewlett Foundation2. We denote our modified LambdaMART as K-LambdaMART in which K stands for the Kappa-based gradient function. Specific steps include the following: To begin with, we re-define the Ai,j for each pair of essays. For a pair of essays, essay i and essay j, Ai,j is defined as the derivative of RankNet (Li et al., 2007) loss function multiplied by the Quadratic weighted Kappa gain after exchanging the two essays’ ratings. −� Ai,j = 1 + eδ(sz−sj) |OKappa |(1) si and sj are the model scores for essay i and essay j, respectively. 6 is a parameter which determines the shape of the sigmoid. Quadratic weighted Kappa are calculated as follows: �i,j wi,jOi,j � = 1 − Ei wi,j E&apos; i,j (2 ) j In matrix O, Oi,j corresponds to the number of essays that received a score i by human rater and a score j by the AES system. In matrix w, wi,j is the difference between raters scores (i−j)2 (N−1)2 , where N is the number of possibl</context>
<context position="26342" citStr="Li et al., 2007" startWordPosition="4343" endWordPosition="4346">n the z-space. In addition, analysis of variance (ANOVA) (Scheffe, 1999) is conducted to test whether significant difference exists between the two groups of scores given by human and machine raters. 4.2 Configuration of Testing Algorithms Random Forests bagging K-LambdaMart We denote our proposed method K-LambdaMART where K stands for the Kappa-based gradient. Our implementation of RF bagging K-LambdaMART is based on the open-source RankLib toolkit7, a library of learning to rank algorithms, in which many popular learning to rank algorithms have been implemented, e.g. LambdaMART and RankNet (Li et al., 2007). Empirical settings of parameters obtained by preliminary experiments on the ASAP validation set are as follows. For bagging: the number of bags is set to 300, subsampling rate is 0.80 and feature sampling rate is 0.50. For LambdaMART in each bag: the number of trees is set to 1, the number of tree leaves is 100 and other parameters are set to default. Baseline algorithms We use classical machine learning algorithms, support vector machine (SVM) for classification, regression (Vapnik et al., 1996) and preference ranking (Joachims, 2006), respectively, as the baselines. These three algorithms </context>
</contexts>
<marker>Li, Burges, Wu, 2007</marker>
<rawString>P. Li, C. Burges, and Q. Wu. 2007. Learning to rank using classification and gradient boosting. In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Y Liu</author>
</authors>
<title>Learning to rank for information retrieval.</title>
<date>2009</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="3903" citStr="Liu, 2009" startWordPosition="603" endWordPosition="604"> current AES methods. To this end, we propose a rank-based approach in this paper that utilizes a listwise learning to rank 1741 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1741–1752, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics algorithm to address automated essay scoring in the view of directly optimizing the agreement between human raters and the AES system. Different from the preference ranking-based approach (Yannakoudakis et al., 2011) that maximizes the pairwise classification precision (Liu, 2009), our rank-based approach follows the listwise learning paradigm and the agreement between the machine and human raters is directly integrated into the loss function that is optimized by gradient boost regression trees. To the best of our knowledge, this work is the first to apply listwise learning to rank approach for AES, which aims at the optimization of the agreement between the human and machine raters. Experimental results on the publicly available dataset ASAP indicate that our proposed method achieves high agreement with human raters, that is about 0.80, measured by quadratic weighted </context>
<context position="9502" citStr="Liu, 2009" startWordPosition="1489" endWordPosition="1490">roup of prompts and designed to be the best fit for predicting human scores for all prompts. It usually does not consider prompt-specific features and just takes writing quality into account. Generic rating model evaluates essays across all prompts with the same scoring criteria, which is more consistent with the human rubric that is usually the same for all prompts, and therefore has validity-related advantages (Attali et al., 2010). 2.2 Learning to Rank Learning to rank, also called machine-learned ranking, was originally proposed to settle the ranking problem in information retrieval (IR) (Liu, 2009). It is a type of supervised or semi-supervised machine learning algorithm that automatically construct a ranking model or function from training data. Current learning to rank algorithms fall into three categories, that is, the pointwise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise appr</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>T.Y. Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLTNAACL</title>
<date>2004</date>
<pages>38--41</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="23509" citStr="Pedersen et al., 2004" startWordPosition="3892" endWordPosition="3895">ng means that all the ideas in a paragraph flow smoothly from one sentence to the next. We only consider nouns and pronouns in each sentence as they convey more information. The relevance degree between one sentence and its next in the same paragraph is calculated as the sum of the similarity degrees between nouns and pronouns appeared in the two sentences, normalized by the sum of the two sentences’ length in words. The similarity degree between words is set to 1 if coreference exists, indicated by Stanford Core NLP (Lee et al., 2013). Otherwise, it is measured by WordNet similarity package (Pedersen et al., 2004). Finally, text coherence is computed as the average relevance degree of all pairs of neighbored sentences. The rating model is learned off-line using a set of training essays. For a given target essay, it is the feature extraction that mainly accounts for the overhead. In our experiments, it usually costs in average no more than 10 seconds on a desktop PC with an Intel i5-2410M CPU running at 2.3GHZ to extract the pre-defined features and predict a rating for a given essay, which is affordable, compared to the cost of a human rater. 4 Experimental Setup This section presents our the experimen</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLTNAACL 2004, pages 38–41, Boston, Massachusetts, 2-7 May. Association for Computational Linguistics, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Powers</author>
<author>Jill C Burstein</author>
<author>Martin Chodorow</author>
<author>Mary E Fowles</author>
</authors>
<title>Karen Kukich, and Graduate Record Examinations Board.</title>
<date>2000</date>
<journal>RESEARCH REPORT-EDUCATIONAL TESTING SERVICE PRINCETON RR,</journal>
<volume>10</volume>
<contexts>
<context position="4851" citStr="Powers et al., 2000" startWordPosition="748" endWordPosition="751">ich aims at the optimization of the agreement between the human and machine raters. Experimental results on the publicly available dataset ASAP indicate that our proposed method achieves high agreement with human raters, that is about 0.80, measured by quadratic weighted Kappa (Brenner and Kliebsch, 1996). Our proposed method also outperforms the previous classification, regression and preference ranking based approaches. As it is widely accepted that the agreement between human raters, measured by either quadratic weighted Kappa or Pearson’s correlation coefficient, ranges from 0.70 to 0.80 (Powers et al., 2000) (Williamson, 2009), our proposed approach therefore performs as well as human raters. The rest of this paper is organized as follows. In section 2, we introduce the research background of automated essay scoring and give a brief introduction to learning to rank. In section 3, a detailed description of our listwise learning to rank approach for automated essay scoring is presented. Section 4 explains the experimental setup and section 5 presents the experimental results. Finally, in section 6 we conclude this research. 2 Related Work and Background Firstly, we give a brief description of exist</context>
<context position="39815" citStr="Powers et al., 2000" startWordPosition="6455" endWordPosition="6458"> 0.7844 0.6410 0.7786 0.6022 Content and prompt-specific features Essay length 0.7930 0.7502 0.7736 0.7390 Word vector similarity 0.7658 0.7001 – – Semantic vector similarity 0.7924 0.5683 – – Text coherence 0.7863 0.6947 0.7798 0.6367 into the loss function. Experiments on the public English dataset ASAP show that our approach outperforms the state-of-the-art algorithms in both promptspecific and generic rating settings. Moreover, it is widely accepted that the agreement between professional human raters ranges from 0.70 to 0.80, measured by quadratic weighted Kappa or Pearson’s correlation (Powers et al., 2000) (Williamson, 2009). In the experiments, our approach achieves a quadratic weighted Kappa around 0.80 for prompt-specific rating and around 0.78 for generic rating, suggesting its potential in automated essay scoring. Most existing research on AES focus on training a prompt-specific rating model. While such approaches have the advantage of providing a satisfactory rating accuracy for essays written for a specific topic, they also suffer from validity and feasibility problem as a significant amount of training data, namely essays with human ratings, are required for every given essay topic (Att</context>
</contexts>
<marker>Powers, Burstein, Chodorow, Fowles, 2000</marker>
<rawString>Donald E Powers, Jill C Burstein, Martin Chodorow, Mary E Fowles, Karen Kukich, and Graduate Record Examinations Board. 2000. Comparing the validity of automated and human essay scoring. RESEARCH REPORT-EDUCATIONAL TESTING SERVICE PRINCETON RR, (10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Qin</author>
</authors>
<title>Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li.</title>
<date>2008</date>
<journal>Inf. Process. Manage.,</journal>
<volume>44</volume>
<issue>2</issue>
<marker>Qin, 2008</marker>
<rawString>Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li. 2008. Query-level loss functions for information retrieval. Inf. Process. Manage., 44(2):838–855, mar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<date>1971</date>
<booktitle>The SMART Retrieval SystemExperiments in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall, Inc.,</publisher>
<location>Upper Saddle River, NJ, USA.</location>
<contexts>
<context position="22507" citStr="Salton, 1971" startWordPosition="3725" endWordPosition="3726">in the specific essay. – POS bigram and trigram: Mean tf/TF of POS bigrams and trigrams. The reason is the same with word bigrams and trigrams. Content and prompt-specific features: We define four subsets of content and prompt-specific features. – Essay length: Essay length in characters and words, respectively. The fourth root of essay length in words is proved to be highly correlated with the essay score (Shermis and Burstein, 2002). – Word vector similarity: Mean cosine similarity of word vectors, in which the element is the term frequency multiplied by inverse document frequency (tf-idf) (Salton, 1971) of each word. It is calculated as the weighted mean of all cosine similarities and the weight is set as the corresponding essay score. – Semantic vector similarity: Semantic vectors are generated by Latent Semantic Analysis (Dumais, 2005). The calculation of mean cosine similarity of semantic vectors is the same with word vector similarity. – Text coherence: Coherence in writing means that all the ideas in a paragraph flow smoothly from one sentence to the next. We only consider nouns and pronouns in each sentence as they convey more information. The relevance degree between one sentence and </context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton. 1971. The SMART Retrieval SystemExperiments in Automatic Document Processing. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Scheffe</author>
</authors>
<title>The analysis of variance,</title>
<date>1999</date>
<volume>72</volume>
<publisher>Wiley. com.</publisher>
<contexts>
<context position="25798" citStr="Scheffe, 1999" startWordPosition="4260" endWordPosition="4261">the AES algorithm and the actual ratings. It is widely accepted 3http://www.merriam-webster.com/ 4http://code.google.com/p/google-api-spelling-java/ 5http://nlp.stanford.edu/software/corenlp.shtml 1746 as a reasonable evaluation measure for AES systems (Williamson, 2009), and is also the official evaluation measure in the ASAP AES competition. It is calculated on all essay topics. If there are essays that come from n essay topics, we calculate the agreement degree on each essay topic first and then compute the overall agreement degree in the z-space. In addition, analysis of variance (ANOVA) (Scheffe, 1999) is conducted to test whether significant difference exists between the two groups of scores given by human and machine raters. 4.2 Configuration of Testing Algorithms Random Forests bagging K-LambdaMart We denote our proposed method K-LambdaMART where K stands for the Kappa-based gradient. Our implementation of RF bagging K-LambdaMART is based on the open-source RankLib toolkit7, a library of learning to rank algorithms, in which many popular learning to rank algorithms have been implemented, e.g. LambdaMART and RankNet (Li et al., 2007). Empirical settings of parameters obtained by prelimina</context>
</contexts>
<marker>Scheffe, 1999</marker>
<rawString>Henry Scheffe. 1999. The analysis of variance, volume 72. Wiley. com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Shermis</author>
<author>J C Burstein</author>
</authors>
<title>Automated essay scoring: A cross-disciplinary perspective. Lawrence Erlbaum.</title>
<date>2002</date>
<contexts>
<context position="10050" citStr="Shermis and Burstein, 2002" startWordPosition="1567" endWordPosition="1570">roposed to settle the ranking problem in information retrieval (IR) (Liu, 2009). It is a type of supervised or semi-supervised machine learning algorithm that automatically construct a ranking model or function from training data. Current learning to rank algorithms fall into three categories, that is, the pointwise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise approaches process a pair of documents each time and usually model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms process a list of documents each time and the loss function aims at measuring the accorda</context>
<context position="22332" citStr="Shermis and Burstein, 2002" startWordPosition="3696" endWordPosition="3699">We assume a bigram or trigram with high tf/TF as a grammar error because high tf/TF means that this kind of bigram or trigram is not commonly used in the whole essay collection but appears in the specific essay. – POS bigram and trigram: Mean tf/TF of POS bigrams and trigrams. The reason is the same with word bigrams and trigrams. Content and prompt-specific features: We define four subsets of content and prompt-specific features. – Essay length: Essay length in characters and words, respectively. The fourth root of essay length in words is proved to be highly correlated with the essay score (Shermis and Burstein, 2002). – Word vector similarity: Mean cosine similarity of word vectors, in which the element is the term frequency multiplied by inverse document frequency (tf-idf) (Salton, 1971) of each word. It is calculated as the weighted mean of all cosine similarities and the weight is set as the corresponding essay score. – Semantic vector similarity: Semantic vectors are generated by Latent Semantic Analysis (Dumais, 2005). The calculation of mean cosine similarity of semantic vectors is the same with word vector similarity. – Text coherence: Coherence in writing means that all the ideas in a paragraph fl</context>
<context position="37864" citStr="Shermis and Burstein, 2002" startWordPosition="6164" endWordPosition="6167">n’s correlation coefficient between the feature values and the final ratings in each essay prompt ranges from -0.20 to - 0.60, which suggests that our method to estimate the number of grammar errors is applicable because it is widely accepted that in the evaluation of student essays, essays with more grammar errors tend to receive lower ratings. Among the content and prompt-specific features, essay length and word vector similarity features give good results in feature unique test. The fourth root of essay length in words has been proved to be a highly correlated feature by many works on AES (Shermis and Burstein, 2002). Word vector similarity feature measures prompt-specific vocabulary usage, which is also important to essay evaluation. In ablation test, there is no significant performance decrease no matter what feature subset is removed. It seems that each feature subset contributes little to the overall performance and therefore can be removed. However, the result of feature unique test suggests that most features used in our rating model are in fact highly correlated with the writing quality. 6 Conclusions and Future Work We have proposed a listwise learning to rank approach to automated essay scoring (</context>
</contexts>
<marker>Shermis, Burstein, 2002</marker>
<rawString>M.D. Shermis and J.C. Burstein. 2002. Automated essay scoring: A cross-disciplinary perspective. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21284" citStr="Toutanova et al., 2003" startWordPosition="3516" endWordPosition="3519">a commonly used NLP tool, Stanford Core NLP (Klein and Manning, 2003), which is an integrated suite of natural language processing tools for English in Java5, including part-of-speech tagging, parsing, co-reference, etc.. – Sentence level: The sum of the depth of all nodes in a parser tree generated by Stanford Core NLP. The height of the parser tree is also incorpo1745 rated into the feature set. – Mode, preposition, comma: The number of modes, prepositions and commas in each sentence respectively, normalized by sentence length in words. Part of speech (POS) is detected by Stanford Core NLP (Toutanova et al., 2003). The POS tags of modal verb and preposition are “MD” and “IN”, respectively. Grammar and fluency features: There are two subsets of grammar and fluency features. – Word bigram and trigram: We evaluate the grammar and fluency of an essay by calculating mean tf/TF of word bigrams and trigrams (Briscoe et al., 2010) (tf is the term frequency in a single essay and TF is the term frequency in the whole essay collection). We assume a bigram or trigram with high tf/TF as a grammar error because high tf/TF means that this kind of bigram or trigram is not commonly used in the whole essay collection bu</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
<author>Steven E Golowich</author>
<author>Alex Smola</author>
</authors>
<title>Support vector method for function approximation, regression estimation, and signal processing.</title>
<date>1996</date>
<booktitle>In Advances in Neural Information Processing Systems 9,</booktitle>
<pages>281--287</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6127" citStr="Vapnik et al., 1996" startWordPosition="945" endWordPosition="948">tion to learning to rank is presented in section 2.2. 2.1 Existing AES Methods In general, existing solutions consider AES as a learning problem. Based on a large number of predefined objectively measurable features, various learning techniques, including classification, regression and preference ranking, are applied (Larkey, 1998) (Yannakoudakis et al., 2011). Regression-based approach treats feature values and essay score as independent variables and dependent variable, respectively, and then learns a regression equation by classical regression algorithms, such as support vector regression (Vapnik et al., 1996). In 1966, the first AES system, Project Essay Grader, was developed by Ellis Page upon the request of the American College Board. The PEG system defines a large set of surface text features from essays, e.g. fourth root of essay length, and uses regression-based approach to predict the score that human raters will give. E-rater, developed by Educational Testing Services (ETS) in America, in late 1990s, is a commercial AES system which has been put into practical use in the Graduate Record Examination (GRE) and the Test of English as a Foreign Language (TOEFL). The E-rater system uses natural </context>
<context position="9965" citStr="Vapnik et al., 1996" startWordPosition="1554" endWordPosition="1557">o Rank Learning to rank, also called machine-learned ranking, was originally proposed to settle the ranking problem in information retrieval (IR) (Liu, 2009). It is a type of supervised or semi-supervised machine learning algorithm that automatically construct a ranking model or function from training data. Current learning to rank algorithms fall into three categories, that is, the pointwise, pairwise, listwise approaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise approaches process a pair of documents each time and usually model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms proc</context>
<context position="26845" citStr="Vapnik et al., 1996" startWordPosition="4427" endWordPosition="4430">n which many popular learning to rank algorithms have been implemented, e.g. LambdaMART and RankNet (Li et al., 2007). Empirical settings of parameters obtained by preliminary experiments on the ASAP validation set are as follows. For bagging: the number of bags is set to 300, subsampling rate is 0.80 and feature sampling rate is 0.50. For LambdaMART in each bag: the number of trees is set to 1, the number of tree leaves is 100 and other parameters are set to default. Baseline algorithms We use classical machine learning algorithms, support vector machine (SVM) for classification, regression (Vapnik et al., 1996) and preference ranking (Joachims, 2006), respectively, as the baselines. These three algorithms have been used for AES in the literature (Briscoe et al., 2010) (Yannakoudakis et al., 2011). Especially, the state-of-the-art AES approach proposed by (Yannakoudakis et al., 2011) utilizes the SVM for preference ranking, a pairwise learning to rank algorithm, for training a rating model. The linear kernel is used in the experiments. The parameter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not </context>
</contexts>
<marker>Vapnik, Golowich, Smola, 1996</marker>
<rawString>Vladimir Vapnik, Steven E. Golowich, and Alex Smola. 1996. Support vector method for function approximation, regression estimation, and signal processing. In Advances in Neural Information Processing Systems 9, pages 281–287. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Williamson</author>
</authors>
<title>A framework for implementing automated scoring.</title>
<date>2009</date>
<booktitle>In Annual Meeting of the American Educational Research Association and the National Council on Measurement in Education,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="4870" citStr="Williamson, 2009" startWordPosition="752" endWordPosition="753">zation of the agreement between the human and machine raters. Experimental results on the publicly available dataset ASAP indicate that our proposed method achieves high agreement with human raters, that is about 0.80, measured by quadratic weighted Kappa (Brenner and Kliebsch, 1996). Our proposed method also outperforms the previous classification, regression and preference ranking based approaches. As it is widely accepted that the agreement between human raters, measured by either quadratic weighted Kappa or Pearson’s correlation coefficient, ranges from 0.70 to 0.80 (Powers et al., 2000) (Williamson, 2009), our proposed approach therefore performs as well as human raters. The rest of this paper is organized as follows. In section 2, we introduce the research background of automated essay scoring and give a brief introduction to learning to rank. In section 3, a detailed description of our listwise learning to rank approach for automated essay scoring is presented. Section 4 explains the experimental setup and section 5 presents the experimental results. Finally, in section 6 we conclude this research. 2 Related Work and Background Firstly, we give a brief description of existing approaches for </context>
<context position="8595" citStr="Williamson, 2009" startWordPosition="1346" endWordPosition="1347">global ordering of essays based on writing quality. It is also the first study of rank-based approach in automated essay scoring. Although “learning to rank” is not mentioned in 1742 their paper, the algorithm they used, Ranking SVM (svm-light package with “-z p“ option), is actually a pairwise approach. We will give a brief introduction to learning to rank in section 2.2. The AES systems can be deployed in two different manners, namely prompt-specific and generic. A prompt-specific rating model is built for a specific prompt and designed to be the best rating model for the particular prompt (Williamson, 2009). For different prompts, the features used, their weights, and scoring criteria, may be different. It usually requires several hundreds of graded essays for training, which is time-consuming and usually impractical in a classroom environment. Generic rating model is trained from essays across a group of prompts and designed to be the best fit for predicting human scores for all prompts. It usually does not consider prompt-specific features and just takes writing quality into account. Generic rating model evaluates essays across all prompts with the same scoring criteria, which is more consiste</context>
<context position="25455" citStr="Williamson, 2009" startWordPosition="4200" endWordPosition="4201">ASAP comes with a validation set that can be used for parameter training. There is no overlap between this validation set and the test set used in our evaluation. In AES, the agreement between human-machine rater is the most important measurement of success. We use quadratic weighted Kappa to evaluate the agreement between the ratings given by the AES algorithm and the actual ratings. It is widely accepted 3http://www.merriam-webster.com/ 4http://code.google.com/p/google-api-spelling-java/ 5http://nlp.stanford.edu/software/corenlp.shtml 1746 as a reasonable evaluation measure for AES systems (Williamson, 2009), and is also the official evaluation measure in the ASAP AES competition. It is calculated on all essay topics. If there are essays that come from n essay topics, we calculate the agreement degree on each essay topic first and then compute the overall agreement degree in the z-space. In addition, analysis of variance (ANOVA) (Scheffe, 1999) is conducted to test whether significant difference exists between the two groups of scores given by human and machine raters. 4.2 Configuration of Testing Algorithms Random Forests bagging K-LambdaMart We denote our proposed method K-LambdaMART where K st</context>
<context position="33501" citStr="Williamson, 2009" startWordPosition="5463" endWordPosition="5464">cation, regression and preference ranking, respectively. ANOVA stands for variance analysis, which aims to test whether significant difference exists between the scores given by human and machine raters. The improvement of our RF bagging K-LambdaMART over each baseline in percentage is also given. For prompt-specific rating model, all of these algorithms achieve good performance comparable to human raters as literatures have revealed that the agreement between two professional human raters (measured by statistics for correlation analysis, e.g. quadratic weighted Kappa) is around 0.70 to 0.80 (Williamson, 2009) (Williamson, 2009). It is clear that our listwise learning to rank approach, Random Forests bagging K-LambdaMART, gives the best performance on the ASAP dataset. The variance analysis result on the six groups of scores (scores given by five times of five-fold cross-validation and the scores provided by human rater), no significant difference, suggests the robustness of our proposed approach. On the contrary, although pref1748 erence ranking based approach, SVM for ranking, and regression based approach, SVM for regression, give very good result in human-machine agreement, their variance analy</context>
<context position="34943" citStr="Williamson, 2009" startWordPosition="5687" endWordPosition="5688"> to rank approach in the building of prompt-specific rating model. For generic rating model, one can conclude from Table 1 that RF bagging LambdaMART performs better than SVM for classification, regression and preference ranking on the ASAP dataset. The dataset used in our experiment consists of essays generated by 8 prompts and each prompt has its own features. With such a training set, both classification and regression based approaches produce not good results, as it is commonly accepted that rating model whose performance measured by interrater agreement lower than 0.70 is not applicable (Williamson, 2009). And the variance analysis results also reveal that there exists statistically significant difference between the scores given by human and machine raters, indicating a low robustness of these two baselines. The performance comparison of the generic rating models suggest that the rank based approaches, SVMp and RF bagging KLambdaMART, are more effective than the classification based SVMc and the regression based SVMr, while our proposed RF bagging K-LambdaMART outperforms the state-of-the-art SVMp. Moreover, we find that there is no obvious performance difference when our proposed method is a</context>
<context position="39834" citStr="Williamson, 2009" startWordPosition="6459" endWordPosition="6460">0.6022 Content and prompt-specific features Essay length 0.7930 0.7502 0.7736 0.7390 Word vector similarity 0.7658 0.7001 – – Semantic vector similarity 0.7924 0.5683 – – Text coherence 0.7863 0.6947 0.7798 0.6367 into the loss function. Experiments on the public English dataset ASAP show that our approach outperforms the state-of-the-art algorithms in both promptspecific and generic rating settings. Moreover, it is widely accepted that the agreement between professional human raters ranges from 0.70 to 0.80, measured by quadratic weighted Kappa or Pearson’s correlation (Powers et al., 2000) (Williamson, 2009). In the experiments, our approach achieves a quadratic weighted Kappa around 0.80 for prompt-specific rating and around 0.78 for generic rating, suggesting its potential in automated essay scoring. Most existing research on AES focus on training a prompt-specific rating model. While such approaches have the advantage of providing a satisfactory rating accuracy for essays written for a specific topic, they also suffer from validity and feasibility problem as a significant amount of training data, namely essays with human ratings, are required for every given essay topic (Attali et al., 2010). </context>
</contexts>
<marker>Williamson, 2009</marker>
<rawString>D.M. Williamson. 2009. A framework for implementing automated scoring. In Annual Meeting of the American Educational Research Association and the National Council on Measurement in Education, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wu</author>
<author>C J C Burges</author>
<author>K M Svore</author>
<author>J Gao</author>
</authors>
<title>Ranking, boosting, and model adaptation.</title>
<date>2008</date>
<tech>Technical report.</tech>
<contexts>
<context position="10771" citStr="Wu et al., 2008" startWordPosition="1677" endWordPosition="1680">lly model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms process a list of documents each time and the loss function aims at measuring the accordance between predicted ranking list and the ground truth label. Representative algorithms are LambdaMart (Wu et al., 2008), RankCosine (Qin et al., 2008), etc.. Listwise approach has not yet been used in automated essay scoring. 3 Automated Essay Scoring by Maximizing Human-machine Agreement The main work-flow of our proposed approach is as follows. Firstly, a set of essays rated by professional human raters are gathered for the training. A listwise learning to rank algorithm learns a ranking model or function using this set of human rated essays represented by vectors of the pre-defined features. Then the learned ranking model or function outputs a model score for each essay, including both rated and unrated ess</context>
<context position="12199" citStr="Wu et al., 2008" startWordPosition="1925" endWordPosition="1928">detailed description of our listwise learning to rank approach for AES in section 3.1. And features used in our approach are presented in section 3.2. 3.1 Listwise Learning to Rank for AES Our choice of the listwise learning to rank algorithm is due to the fact that it takes the entire set of labeled essays associated to a given prompt, instead of the individual essays or essay pairs as in (Yannakoudakis et al., 2011), as training examples. This brings us the convenience of easily embedding the inter-rater agreement into the loss function for the learning. In this paper, we deploy LambdaMART (Wu et al., 2008), a listwise learning to rank algorithm and then use Random Forests (RF) (Breiman, 2001) for the bagging of LambdaMART learners. Having been widely used in information retrieval applications, LambdaMART is one of the most effective learning to rank algorithms. For instance, it achieves the top results in the 2010 Yahoo! Learning to Rank challenge (Burges, 2010). Random Forests is an 1743 ensemble learning method for classification and regression. Previously, the loss function of LambdaMART is defined as the gradient loss of the retrieval effectiveness, measured by IR evaluation criteria such a</context>
</contexts>
<marker>Wu, Burges, Svore, Gao, 2008</marker>
<rawString>Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2008. Ranking, boosting, and model adaptation. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yannakoudakis</author>
<author>T Briscoe</author>
<author>B Medlock</author>
</authors>
<title>A new dataset and method for automatically grading esol texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>180--189</pages>
<contexts>
<context position="2683" citStr="Yannakoudakis et al., 2011" startWordPosition="410" endWordPosition="413"> rater. With the help of an AES system that highly agrees with human raters, the human workload can be reduced by half at most. Therefore, the agreement between the AES system and the human rater is an important indicator of an AES system’s effectiveness. There have been efforts in developing AES methods since the 1960s. Various kinds of algorithms and models based on NLP and machine learning techniques have been proposed to implement AES systems. Existing approaches consider essay rating as a classification (Larkey, 1998), regression (Attali and Burstein, 2006) or preference ranking problem (Yannakoudakis et al., 2011), where the loss function is the regression loss, classification loss and pairwise classification loss, respectively. In this paper, we argue that the purpose of AES is to predict the essay’s rating that human raters would give. If an AES system frequently disagrees with the first human rater, a second human rater will be needed in most cases. Thus, the introduction of the AES system does not bring much benefit in reducing the human workload. It is therefore desirable to minimize the disagreement between the machine and human raters. However, this disagreement is not explicitly, if any, addres</context>
<context position="5869" citStr="Yannakoudakis et al., 2011" startWordPosition="909" endWordPosition="912">n 4 explains the experimental setup and section 5 presents the experimental results. Finally, in section 6 we conclude this research. 2 Related Work and Background Firstly, we give a brief description of existing approaches for AES in section 2.1. Then, an introduction to learning to rank is presented in section 2.2. 2.1 Existing AES Methods In general, existing solutions consider AES as a learning problem. Based on a large number of predefined objectively measurable features, various learning techniques, including classification, regression and preference ranking, are applied (Larkey, 1998) (Yannakoudakis et al., 2011). Regression-based approach treats feature values and essay score as independent variables and dependent variable, respectively, and then learns a regression equation by classical regression algorithms, such as support vector regression (Vapnik et al., 1996). In 1966, the first AES system, Project Essay Grader, was developed by Ellis Page upon the request of the American College Board. The PEG system defines a large set of surface text features from essays, e.g. fourth root of essay length, and uses regression-based approach to predict the score that human raters will give. E-rater, developed </context>
<context position="7843" citStr="Yannakoudakis et al., 2011" startWordPosition="1220" endWordPosition="1223">ociated to a numeric rating. Intelligent Essay Assessor (IEA) (Foltz et al., 1999), developed also in late 1990s, evaluates essay by measuring semantic features. Each ungraded essay, represented by a semantic vector generated by Latent Semantic Analysis (LSA) (Dumais, 2005), is rated according to the similarity degree with semantic vectors of graded essays. Bayesian Essay Test Scoring sYstem, developed by Larkey in 2003, is based on naive Bayesian model. It is the only open-source AES system, but has not been put into practical use yet. Besides classification and regression-based approaches, (Yannakoudakis et al., 2011) proposed a preference ranking based approach for learning a rating model, where a ranking function or model is learned to construct a global ordering of essays based on writing quality. It is also the first study of rank-based approach in automated essay scoring. Although “learning to rank” is not mentioned in 1742 their paper, the algorithm they used, Ranking SVM (svm-light package with “-z p“ option), is actually a pairwise approach. We will give a brief introduction to learning to rank in section 2.2. The AES systems can be deployed in two different manners, namely prompt-specific and gene</context>
<context position="10389" citStr="Yannakoudakis et al., 2011" startWordPosition="1620" endWordPosition="1623">pproaches. Pointwise approach takes individual documents as training examples for learning a scoring function. In fact, both multiple linear regression and support vector regression (Vapnik et al., 1996), which have been widely used in automated essay scoring (Shermis and Burstein, 2002), can be seen as pointwise approaches. Pairwise approaches process a pair of documents each time and usually model ranking as a pairwise classification problem. Thus, the loss function is always a classification loss. Representative algorithms are ranking SVM (Joachims, 2006), RankNet (Li et al., 2007), etc.. (Yannakoudakis et al., 2011) apply pairwise approach, ranking SVM, to automated essay scoring and achieve better performance than support vector regression. In listwise approaches, ranking algorithms process a list of documents each time and the loss function aims at measuring the accordance between predicted ranking list and the ground truth label. Representative algorithms are LambdaMart (Wu et al., 2008), RankCosine (Qin et al., 2008), etc.. Listwise approach has not yet been used in automated essay scoring. 3 Automated Essay Scoring by Maximizing Human-machine Agreement The main work-flow of our proposed approach is </context>
<context position="12004" citStr="Yannakoudakis et al., 2011" startWordPosition="1893" endWordPosition="1897">rom which a global ordering of essays is constructed. Finally, the model score is mapped to a predefined scale of valid ratings, such as an integer from 1 to 6 in a 6-point scale In this section, we give a detailed description of our listwise learning to rank approach for AES in section 3.1. And features used in our approach are presented in section 3.2. 3.1 Listwise Learning to Rank for AES Our choice of the listwise learning to rank algorithm is due to the fact that it takes the entire set of labeled essays associated to a given prompt, instead of the individual essays or essay pairs as in (Yannakoudakis et al., 2011), as training examples. This brings us the convenience of easily embedding the inter-rater agreement into the loss function for the learning. In this paper, we deploy LambdaMART (Wu et al., 2008), a listwise learning to rank algorithm and then use Random Forests (RF) (Breiman, 2001) for the bagging of LambdaMART learners. Having been widely used in information retrieval applications, LambdaMART is one of the most effective learning to rank algorithms. For instance, it achieves the top results in the 2010 Yahoo! Learning to Rank challenge (Burges, 2010). Random Forests is an 1743 ensemble learn</context>
<context position="27034" citStr="Yannakoudakis et al., 2011" startWordPosition="4456" endWordPosition="4459">ments on the ASAP validation set are as follows. For bagging: the number of bags is set to 300, subsampling rate is 0.80 and feature sampling rate is 0.50. For LambdaMART in each bag: the number of trees is set to 1, the number of tree leaves is 100 and other parameters are set to default. Baseline algorithms We use classical machine learning algorithms, support vector machine (SVM) for classification, regression (Vapnik et al., 1996) and preference ranking (Joachims, 2006), respectively, as the baselines. These three algorithms have been used for AES in the literature (Briscoe et al., 2010) (Yannakoudakis et al., 2011). Especially, the state-of-the-art AES approach proposed by (Yannakoudakis et al., 2011) utilizes the SVM for preference ranking, a pairwise learning to rank algorithm, for training a rating model. The linear kernel is used in the experiments. The parameter C, which controls the trade-off between empirical loss and regularizer, is set by grid search on the ASAP validation set. The original LambdaMART is not included in the baseline algorithms as it has been shown that the performance of LambdaMART is inferior to ranking 6http://www.kaggle.com/c/asap-sas/data SVM on the same dataset (Chen et al</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011. A new dataset and method for automatically grading esol texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 180– 189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yisong Yue</author>
<author>C Burges</author>
</authors>
<title>On using simultaneous perturbation stochastic approximation for learning to rank, and the empirical optimality of lambdarank.</title>
<date>2007</date>
<tech>Technical report, Technical Report MSR-TR2007-115, Microsoft Research.</tech>
<contexts>
<context position="13101" citStr="Yue and Burges, 2007" startWordPosition="2069" endWordPosition="2072">ieves the top results in the 2010 Yahoo! Learning to Rank challenge (Burges, 2010). Random Forests is an 1743 ensemble learning method for classification and regression. Previously, the loss function of LambdaMART is defined as the gradient loss of the retrieval effectiveness, measured by IR evaluation criteria such as Normalized Discounted Cumulative Gain (nDCG) (Wu et al., 2008). More specifically, it is a heuristic method that directly defines A, the gradient of nDCG with respect to the model score of each document, and has been shown to work empirically for particular loss functions NDCG (Yue and Burges, 2007). Then, Multiple Additive Regression Trees (MART) (Friedman, 2000), also called Gradient Boosting Decision Tree (GBDT)l, are used to “learn” these gradients iteratively. MART is a class of boosting algorithms that performs gradient descent in function space, using regression trees. Its output F(x) can be written as F(x) = &amp; αifi(x), i = 1, 2, ....N. Each fi(x) is a function modeled by a single regression tree and the αi is the corresponding weight. Given that n trees have been trained, the (n+1)th regression tree, fi+1(x), models the derivative of the cost with respect to the current model sco</context>
</contexts>
<marker>Yue, Burges, 2007</marker>
<rawString>Yisong Yue and C Burges. 2007. On using simultaneous perturbation stochastic approximation for learning to rank, and the empirical optimality of lambdarank. Technical report, Technical Report MSR-TR2007-115, Microsoft Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000253">
<title confidence="0.99695">
A Semantically Enhanced Approach to Determine Textual Similarity
</title>
<author confidence="0.986167">
Eduardo Blanco and Dan Moldovan
</author>
<affiliation confidence="0.962321">
Lymba Corporation
</affiliation>
<address confidence="0.799219">
Richardson, TX 75080 USA
</address>
<email confidence="0.998258">
{eduardo,moldovan}@lymba.com
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995512">
This paper presents a novel approach to deter-
mine textual similarity. A layered methodol-
ogy to transform text into logic forms is pro-
posed, and semantic features are derived from
a logic prover. Experimental results show that
incorporating the semantic structure of sen-
tences is beneficial. When training data is
unavailable, scores obtained from the logic
prover in an unsupervised manner outperform
supervised methods.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998055428571429">
The task of Semantic Textual Similarity (Agirre et
al., 2012) measures the degree of semantic equiv-
alence between two sentences. Unlike textual en-
tailment (Giampiccolo et al., 2007), textual similar-
ity is symmetric, and unlike both textual entailment
and paraphrasing (Dolan and Brockett, 2005), tex-
tual similarity is modeled using a graded score rather
</bodyText>
<listItem confidence="0.7829022">
than a binary decision. For example, sentence pair
(1) below is very similar [5 out of 5], (2) is some-
what similar [3 out of 5] and (3) is not similar at all
[0 out of 5]:
1. Someone is removing the scales from the fish.
A person is descaling a fish.
2. A woman is chopping an herb.
A man is finely chopping a green substance.
3. A cat is playing with a watermelon on a floor.
A man is pouring oil into a pan.
</listItem>
<bodyText confidence="0.9994204">
State-of-the-art systems to determine textual sim-
ilarity (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea
et al., 2012) do not account for the semantic struc-
ture of sentences, and mostly rely on word pair-
ings and knowledge derived from large corpora, e.g.,
</bodyText>
<figureCaption confidence="0.977106">
Figure 1: Semantic representation of 1(a) A man is hold-
ing a leaf and 1(b) A monkey is fighting a man.
</figureCaption>
<bodyText confidence="0.999892896551724">
Wikipedia. Regardless of details, each word in sent�
is paired with the word in sent2 that is most simi-
lar according to some similarity measure. Then, all
similarities are added and normalized by the length
of sent� to obtain the similarity score from sent� to
sent2. The process is repeated to obtain the simi-
larity score from sent2 to sent�, and both scores are
then averaged to determine the overall textual sim-
ilarity. Several word-to-word similarity measures
are often combined with other shallow features, e.g.,
n-gram overlap, syntactic dependencies, to obtain
the final similarity score.
Consider sentences 1(a) A man is holding a leaf
and 1(b) A monkey is fighting a man. These two
sentences are very dissimilar, the only commonal-
ity is the concept ‘man’. Any approach that blindly
searches for the word in 1(b) that is the most similar
to word ‘man’ in 1(a) will find ‘man’ from 1(b) to
be a perfect match. One of three content words is a
match and thus the estimated similarity will be much
higher than it actually is.
Consider now the semantic representations for
sentences 1(a) and 1(b) in Figure 1. ‘man’ plays the
role of AGENT in 1(a), and THEME in 1(b). While
in both sentences the word ‘man’ encodes the same
concept, their semantic functions with respect to
other concepts are different. Intuitively, it seems rea-
sonable to penalize the similarity score based on the
role discrepancy.
</bodyText>
<figure confidence="0.985486142857143">
AGENT THEME
==leaf
man�� holding
monkey fighting
Tan
AGENT
THEME
</figure>
<page confidence="0.915833">
1235
</page>
<note confidence="0.780986">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1235–1245,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.997413385964912">
••
VALUE
88
man used
AGENT
PURPOSE
&amp;&amp;
sword slice
AA ]]
[[ }} plastic bottle
man sliced
\\
INSTRUMENT
VALUE
••
plastic bottle
77
%%
sword
AGENT
THEME
INSTRUMENT
THEME
(a)
AGENT THEME
(b)
PART
LOCATION
(( %%
cosmetics face
;;
THEME
woman putting
__
AGENT
woman
``
AGENT
applying
makeup
==
THEME
(c)
(d)
woman dancing
__
AGENT LOCATION
@@rain
woman dances
__
AGENT
LOCATION
AArain
LOCATION
outside
@@
(e) (f)
</figure>
<figureCaption confidence="0.9650905">
Figure 2: Semantic representations of 2(a) The man used a sword to slice a plastic bottle, 2(b) A man sliced a plastic
bottle with a sword, 2(c) A woman is applying cosmetics to her face, 2(d) A woman is putting on makeup, 2(e) A
woman is dancing in the rain, and 2(f) A woman dances in the rain outside. Pairs (a, b), (c, d) and (e, f) are highly
similar even though concepts and relations only match partially.
</figureCaption>
<bodyText confidence="0.9993195">
This paper proposes a novel approach to deter-
mine textual similarity. Semantic representations
of sentences are exploited, syntactic features omit-
ted and the only external resource used in WordNet
(Miller, 1995). The main novelties of our approach
are: it (1) derives semantic features from a logic
prover to be used in a machine learning framework;
(2) uses three logic form transformations capturing
different levels of knowledge; and (3) incorporates
semantic representations extracted automatically.
</bodyText>
<subsectionHeader confidence="0.999631">
1.1 Matching Semantic Representations and
Determining Textual Similarity
</subsectionHeader>
<bodyText confidence="0.996766189189189">
Throughout this paper, the semantic representation
of a sentence comprises the concepts in it, semantic
relations linking those concepts and named entities
qualifying them. First, we note that existing tools to
extract semantic relations and named entities are not
perfect, thus any system relying on them will suffer
from incomplete and incorrect representations. Sec-
ond, even if flawless representations were readily
available, the problem of determining textual simi-
larity cannot be reduced to matching semantic repre-
sentations: partial matches may correspond to com-
pletely similar sentences. The rest of this section
illustrates this point with the examples in Figure 2.
Our approach (Section 3) copes with the inherent er-
rors made by tools used to obtain semantic represen-
tations and learns which parts of a representation are
important to determine textual similarity.
Consider sentences 2(a) The man used a sword to
slice a plastic bottle and 2(b) A man sliced a plastic
bottle with a sword. Both sentences have high simi-
larity [5 out of 5], and yet their semantic representa-
tions only match partially. In this example, the verb
‘used’ in 2(a) and its semantic links are somewhat
semantically superfluous. Note that in other cases,
missing a semantic relation signals lower similarity,
e.g., I had fun [at the party]LOCATION and I had fun,
while similar, do not convey the same meaning.
Sentence 2(c) A woman is applying cosmetics to
her face and 2(d) A woman is putting on makeup are
highly similar even though the latter specifies neither
the LOCATION where the ‘makeup’ is applied nor
the fact that a PART of the ‘woman’ is her ‘face’.
Similarly, sentences 2(e) A woman is dancing in the
rain and 2(f) A woman dances in the rain outside
are semantically equivalent since ‘rain’ always has
LOCATION ‘outside’: missing this information does
not carry loss of meaning.
</bodyText>
<sectionHeader confidence="0.999759" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999408272727273">
Determining similarity between text snippets is rele-
vant to information retrieval (Hatzivassiloglou et al.,
1999), paraphrase recognition (Madnani and Dorr,
2010), grading answers to questions (Mohler et al.,
2011) and many others. We focus on recent work
and emphasize the differences from our approach.
The SemEval 2012 Task 6: A Pilot on Semantic
Textual Similarity (Agirre et al., 2012) brought to-
gether 35 teams that competed against each other.
The top 3 performers (B¨ar et al., 2012; ˇSari´c et
al., 2012; Banea et al., 2012), followed a ma-
</bodyText>
<page confidence="0.98088">
1236
</page>
<figure confidence="0.9991964375">
Sentences
Logic Form
Transfor-
mation
Pairwise
Similarity
Measures
logic forms
pairwise word similarity scores
Logic
Prover
LFT-based scores
features
Machine
Learning
score
</figure>
<figureCaption confidence="0.999874">
Figure 3: Main components of our system to determine textual similarity.
</figureCaption>
<bodyText confidence="0.999961222222222">
chine learning approach with features that do not
take into account the semantic structure of sen-
tences, e.g., n-grams, word overlap, evaluation mea-
sures for machine translation, pairwise word similar-
ities, syntactic dependencies. All three used Word-
Net, Wikipedia and other large corpora. In partic-
ular, Banea et al. (2012) obtained models from 6
million Wikipedia articles and more than 9.5 mil-
lion hyperlinks; B¨ar et al. (2012) used Wiktionary1,
which contains over 3 million entries; and ˇSari´c et
al. (2012) used The New York Times Annotated
Corpus (Sandhaus, 2008), which contains over 1.8
million news articles, and Google n-grams (Lin et
al., 2012), which consists of approximately 24GB
of compressed text files. Our approach only uses
WordNet, by far the smallest external resource with
less than 120,000 synsets.
Participants that incorporated information about
the semantic structure of sentences (Glinos, 2012;
Rios et al., 2012)2 did not perform at the top. Out
of 88 runs, they were ranked 16, 36 and 64. We be-
lieve this is because they use semantic relations to
calculate some ad-hoc similarity score. In contrast,
our approach derives features from semantic repre-
sentations encoded using logic, and combine these
features using machine learning. Moreover, we use
three logic form transformations capturing different
levels of knowledge, from only content words to se-
mantic structure. In turn, this allows us to boost
performance by relying on semantics when simpler
shallow methods fail.
A few logic-based approaches to recognize tex-
tual entailment are similar to the work presented
here. Bos and Markert (2006) extract semantic rep-
resentations with Boxer (Bos et al., 2004) and in-
corporate background knowledge from external re-
</bodyText>
<footnote confidence="0.971511333333333">
1http://www.wiktionary.org/
2A third team, spirin2, submitted results but a description
paper could not be found in the ACL anthology.
</footnote>
<bodyText confidence="0.99996825">
sources. They use a standard theorem prover and
extract 8 features that are later combined using ma-
chine learning. Raina et al. (2005) use a logic form
transformation derived from dependency parses and
named entities. They use abductive reasoning and
define an assumption cost model to account for par-
tial entailments. Unlike them, we define three logic
from transformations, use a modified resolution step
and extract hundreds of features from the proofs.
Tatu and Moldovan (2005) use a modified logic
prover that drops predicates when a proof cannot
be found. Unlike us, they do not drop unbound
predicates and use a single logic form transforma-
tion. Another key difference is that they assign fixed
weights to predicates a priori instead of using ma-
chine learning to determine them.
</bodyText>
<sectionHeader confidence="0.995436" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999802952380952">
Our approach to determine textual similarity (Fig-
ure 3) is grounded on using semantic features de-
rived from a logic prover that are later combined
in a standard supervised machine learning frame-
work. First, sentences are transformed into logic
forms (lft1, lft2). Then, a modified logic prover is
used to find a proof in both directions (lft1 to lft2
and lft2 to lft1). The prover yields similarity scores
based on the number of predicates dropped and fea-
tures characterizing the proofs. Additional similar-
ity scores are obtained using standard pairwise word
similarity measures. Finally, all scores and features
are combined using machine learning to yield the fi-
nal textual similarity score.
If training data is unavailable, only the LFT-based
and individual pairwise word similarity scores ap-
ply, the machine learning component is the only one
supervised. The rest of this section details each
component and exemplifies it with 2(e) A woman is
dancing in the rain and 2(f) A woman dances in the
rain outside.
</bodyText>
<page confidence="0.959295">
1237
</page>
<table confidence="0.951725454545455">
sent,: A woman is dancing in the rain.
semantic relations extracted: AGENT(dancing, woman), LOCATION(dancing, rain)
Basic woman N(x1) &amp; dance V(x2) &amp; rain N(x3)
SemRels woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2, x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3)
Full woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3)
sent2: A woman dances in the rain outside.
semantic relations extracted: AGENT(dances, woman), LOCATION(dances, rain)
Basic woman N(x1) &amp; dance V(x2) &amp; rain N(x3) &amp; outside M(x4)
SemRels woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3)
Full woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3) &amp;
outside M(x4)
</table>
<tableCaption confidence="0.999836">
Table 1: Examples of logic from transformation using modes Basic, SemRels and Full.
</tableCaption>
<subsectionHeader confidence="0.992319">
3.1 Logic Form Transformation
</subsectionHeader>
<bodyText confidence="0.999787555555555">
The logic form transformation (LFT) of a sentence
is derived from the concepts in it, the semantic
relations linking them and named entities. Un-
like other LFT proposals (Zettlemoyer and Collins,
2005; Poon and Domingos, 2009), transforming
sentences into logic forms is a straightforward step,
the quality of the logic forms is determined by the
output of standard NLP tools.
We distinguish six types of predicates:
</bodyText>
<listItem confidence="0.9997245">
• N for nouns, e.g., woman: woman N(x1).
• V for verbs, e.g., dances: dance V(x2).
• M for adjectives and adverbs, e.g., outside:
outside M(x3).
• O for concepts encoded by other POS tags.
• NE for named entities, e.g., guitar:
guitar N(x4) &amp; instrument NE(x4).
• SR for semantic relations, e.g., A woman
</listItem>
<bodyText confidence="0.962351709677419">
dances: woman N(x1) &amp; dance V(x2) &amp;
AGENT SR(x2,x1).
In order to overcome semantic relation extraction
errors, we have experimented with three logic form
transformation modes. Each mode captures different
levels of knowledge:
Basic generates predicates for all nouns, verbs,
modifiers and named entities. This logic form
is parallel to accounting for content words,
their POS tags and named entity types.
SemRels generates predicates for all semantic rela-
tions, concepts that are arguments of relations
and named entities qualifying those concepts.
This mode ignores concepts not linked to other
concepts through a relation and might miss key
concepts if some relations are missing. If no
semantic relations are found, this mode backs
off to Basic to avoid empty logic forms.
Full generates predicates for all concepts, all se-
mantic relations and all named entities. It is
equivalent to SemRels after adding predicates
for concepts that are not arguments of a seman-
tic relation.
Table 1 exemplifies the three logic form modes.
If perfect semantic relations were always available,
SemRels would be the preferred mode. However,
this is often not the case and combining the three
logic forms yields better performance (Section 4).
Note that since relation LOCATION(rain, outside) is
not extracted from sent2, predicate outside M(x4)
is not present in mode SemRels.
</bodyText>
<subsectionHeader confidence="0.999713">
3.2 Modified Logic Prover
</subsectionHeader>
<bodyText confidence="0.999972066666667">
Textual similarity is symmetric and therefore we
find proofs in both directions (from lft1 to lft2 and
from lft2 to lft1). The logic prover uses a modified
resolution procedure to calculate a similarity score
and features derived from the proof. The rest of this
section exemplifies one direction, lft1 to lft2. The
logic prover is a modification of OTTER3 (McCune
and Wos, 1997), an automated theorem prover for
first-order logic. For the textual similarity task, we
load lft1 and -&apos;lft2 to the set of support and lexical
chain axioms to the usable list. Then, the logic
prover begins its search for a proof. Two scenar-
ios are possible: (1) a contradiction is found, i.e.,
a proof is found; or (2) a contradiction cannot be
found. The modifications to the standard resolution
</bodyText>
<footnote confidence="0.960432">
3http://www.cs.unm.edu/˜mccune/otter/
</footnote>
<page confidence="0.81242">
1238
</page>
<table confidence="0.9997958">
sent,: A woman plays an electric guitar sent2: A man is cutting a potato
lft,: woman N(x1) &amp; play V(x2) &amp; AGENT SR(x2, x1) &amp; electric M(x3) &amp; guitar N(x4) &amp;
instrument NE(x4) &amp; VALUE SR(x4, x3) &amp; THEME SR(x2, x4)
lft2: man N(x1) V cut V(x2) V AGENT SR(x2,x1) V potato N(x3) V THEME SR(x2,x3)
Step Predicate dropped (regular) Score Predicate dropped (unbound) Score
1 woman N(x1) 0.875 n/a 0.875
2 play V(x2) 0.750 AGENT SR(x2,x1) 0.625
3 electric M(x3) 0.500 n/a 0.500
4 guitar N(x4) 0.375 instrument NE(x4), VALUE SR(x4, x3), 0.000
THEME SR(x2,x4)
</table>
<tableCaption confidence="0.997844">
Table 2: Example of predicate dropping step by step. Predicates A G E N T SR(x2, x1) and THEME SR(x2, x4) would not
be dropped if unbound predicates were not dropped, yielding a score of 0.250 instead of 0.000.
</tableCaption>
<bodyText confidence="0.9998658">
procedure are used in scenario (2), when a proof
cannot be found. In this case, predicates from lft1 are
dropped until a proof is found. The worst case oc-
curs when all predicates in lft1 are dropped. The goal
of the dropping mechanism is to force the prover to
always find a proof, and penalize partial proofs ac-
cordingly.
Lexical chain axioms are extracted from WordNet.
Assuming each word w in sent1 has the first sense,
axioms w —* c, where c is at most distance 2 in
the WordNet hierarchy are generated. For exam-
ple, axioms derived from woman include woman —*
female, woman —* mistress, woman —* widow and
woman —* madam. Although simple, this WordNet
expansion proved useful in our experiments.
</bodyText>
<subsectionHeader confidence="0.76533">
3.2.1 Predicate Dropping Criteria
</subsectionHeader>
<bodyText confidence="0.9999556875">
When a proof cannot be found, individual predi-
cates from lft1 not present in lft2 are dropped. A
greedy algorithm was implemented for this step: out
of all predicates from lft1 not present in lft2, drop
whichever occurs first.
Dropping a predicate is not done in isolation. Af-
ter dropping a predicate, all predicates that become
unbound are dropped as well. With our current logic
form transformation, dropping a noun, verb or modi-
fier may make a semantic relation ( SR) or named en-
tity ( NE) predicate unbound. To avoid determining
high similarity between sentences with a common
semantic structure but unrelated concepts instantiat-
ing this structure, predicates encoding semantic rela-
tions and named entities are automatically dropped
when they become unbound.
</bodyText>
<subsubsectionHeader confidence="0.581307">
3.2.2 Proof Scoring Criterion
</subsubsectionHeader>
<bodyText confidence="0.9991834375">
The score assigned to the proof from lft1 to lft2
is calculated as the ratio of number of predicates in
lft1 not dropped to find the proof over the original
number of predicates in lft1.
Note that the dropping mechanism, and in par-
ticular whether predicates that become unbound
are automatically dropped, greatly impact the proof
obtained and its score (Table 2). If predi-
cates that become unbound were not automati-
cally dropped in each step, instrument NE(x4) and
VALUE SR(x4, x3) would be dropped in steps 5 and
6, AGENT SR(x2,x1) and THEME SR(x2,x4) would not
be dropped, and the final score would be 0.250 in-
stead of 0.000. In plain English, dropping unbound
predicates avoids matching semantic structures in-
stantiated by unrelated concepts.
</bodyText>
<subsectionHeader confidence="0.770101">
3.2.3 Feature Selection
</subsectionHeader>
<bodyText confidence="0.998065928571429">
While the proof score can be used directly as an es-
timator of the similarity between lft1 and lft2, ad-
ditional features are extracted from the proof itself.
Namely, for each predicate type (N, V, M, O, SR,
NE), we count the number of predicates present in
lft1, the number of predicates dropped to find a proof
for lft2 and the ratio of the two counts. These three
counts are also calculated for each specific seman-
tic relation predicate (AGENT SR, LOCATION SR, etc.).
An example of score and feature calculation in both
directions is shown in Table 3.
The LFT-based scores and features are fed to a
machine learning algorithm. Specifically, there are
477 features derived from the logic prover:
</bodyText>
<listItem confidence="0.9889735">
• 9 LFT-based scores (3 x 3; three scores (2 di-
rections and average), three LFT modes)
</listItem>
<page confidence="0.876858">
1239
</page>
<table confidence="0.9996872">
lft1: woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp;rainN(x3) &amp; LOCATION SR(x2,x3)
lft2: woman N(x1)&amp;dance V(x2)&amp;AGENT SR(x2, x1)&amp;rain N(x3)&amp;LOCATION SR(x2, x3)&amp;outside M(x4)
lft1 to lft2 pred. dropped none
score 1
features nt nd nr vt vd vr mt md mr net ned ner srt srd srr
2 0 0 1 0 0 0 0 0 0 0 0 2 0 0
lft2 to lft1 pred. dropped outside M(x4)
score 5/6 = 0.833
features nt nd nr vt vd vr mt md mr net ned ner srt srd srr
2 0 0 1 0 0 1 1 1 0 0 0 2 0 0
</table>
<tableCaption confidence="0.8353165">
Table 3: Two logic forms and output of logic prover in both directions. For each predicate type (n, v, m, o, ne, sr)
and semantic relation type (AGENT, LOCATION, etc.) features indicate the total number of predicates, the number of
predicates dropped until a proof is found and ratio of the two counts (t, d and r respectively). We omit the features for
predicate O and individual semantic relations because of space constraints.
</tableCaption>
<listItem confidence="0.998568571428572">
• 108 features for predicates (3 x 6 x 3 x 2 =
108; three features for each of the six predicate
types, three LFT modes, two directions)
• 360 features specific to a semantic relation (3x
20 x 3 x 2 = 360; three features for each of the
20 semantic relations types, three LFT modes,
two directions)
</listItem>
<subsectionHeader confidence="0.997445">
3.3 Pairwise Word Similarities
</subsectionHeader>
<bodyText confidence="0.998593666666667">
Pairwise word similarity measures between con-
cepts have been long studied, and they have been
used for the task of textual similarity before (Mihal-
cea et al., 2006). We incorporate scores derived us-
ing these measures for comparison purposes and to
improve robustness in our approach.
Basically, each open-class word in sent1 is paired
with the open-class word in sent2 that is most sim-
ilar according to some similarity measure. All these
individual similarities are summed and normalized
by the length of sent1 to find the similarity be-
tween sent1 and sent2. The process is repeated
from sent2 to sent1 to obtain the similarity between
sent2 and sent1, and both overall similarities are av-
eraged to determine the final similarity score.
We have experimented with measures Path
(distance in a taxonomy), LCH (Leacock and
Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu
and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin,
1998) and JCN (Jiang and Conrath, 1997), and use
the WordNet::Similarity package4.
</bodyText>
<footnote confidence="0.942839">
4http://wn-similarity.sourceforge.net/
</footnote>
<subsectionHeader confidence="0.955152">
3.4 Machine Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.99998">
We follow a standard supervised machine learning
framework. Instances from the training split are
used to create a model that is later tested with test
instances not seen during training. The model was
tuned using 10-fold cross-validation over the train-
ing instances. As a learning algorithm, we use bag-
ging with M5P decision trees (Quinlan, 1992; Wang
and Witten, 1997) as implemented in the Weka soft-
ware package (Hall et al., 2009).
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9998809">
Logic forms are derived from the output of state-
of-the-art NLP tools developed previously and not
tuned in any way to the current task or corpora. Our
approach is not tied to any tool, set of named enti-
ties or relations. Any other semantic representation
could be used; the only required modification would
be the LFT component (Figure 3) so that it accounts
for the subtleties of the representation of choice.
The named entity recognizer extracts 35 fine-
grained types organized in a taxonomy (date, lan-
guage, city, instrument, etc.) and was first developed
for a question answering system (Moldovan et al.,
2002). The implementation uses publicly available
gazetteers as well as machine learning.
Semantic relations are extracted with Polaris
(Moldovan and Blanco, 2012), a semantic parser
that given text extracts semantic relations. Polaris
is trained using FrameNet (Baker et al., 1998), Prop-
Bank (Palmer et al., 2005), NomBank (Meyers et al.,
2004), several SemEval corpora (Girju et al., 2007;
</bodyText>
<page confidence="0.964315">
1240
</page>
<table confidence="0.997689153846154">
Score Sentence Pair Notes
MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to
(36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details
The unions also staged a five-day strike in March; are missing in one sentence
strikes have preceded eight of the last 10 contracts. but the pair is similar
MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to
(13/13), [750/750] A man is slicing some carrots. parse
SMTeuroparl 4.250 Then perhaps we could have avoided a catastrophe. One sentence often
(56/21), [734/459] We would perhaps then able prevent a disaster. ungrammatical (SMT)
surprise.OnWN 1.500 the alleviation of distress WN glosses, difficult to
(–/16), [–/750] a change for the better. parse with standard tools
surprise.SMTnews 3.000 He did, but the initiative did not get very far One sentence often
(–/24), [–/399] What he has done without the initiative goes too far. ungrammatical (SMT)
</table>
<tableCaption confidence="0.9911915">
Table 4: Examples of sentence pairs belonging to the five sources. The numbers between round (square) parenthesis
indicate the average number of tokens per sentence pair (number of instances) in the train and test splits.
</tableCaption>
<bodyText confidence="0.6219855">
Pustejovsky and Verhagen, 2009; Hendrickx et al.,
2010) and in-house annotations.
</bodyText>
<subsectionHeader confidence="0.944519">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.990301714285714">
We use the corpora released by SemEval 2012
Task 06: A Pilot on Semantic Textual Similarity5
(Agirre et al., 2012). These corpora consist of pairs
of sentences labeled with their semantic similar-
ity score, ranging from 0.0 to 5.0. Sentence pairs
come from five sources: (1) MSRpar, a corpus of
paraphrases; (2) MSRvid, short video descriptions;
(3) SMTeuroparl, output of machine translation sys-
tems and reference translations; (4) surprise.OnWN,
OntoNotes (Hovy et al., 2006) and WordNet (Miller,
1995) glosses; and (5) surprise.SMTnews, output of
machine translation systems in the news domain and
gold translations. Examples can be found in Table 4,
for more details refer to the aforementioned citation.
</bodyText>
<subsectionHeader confidence="0.750014">
4.2 Results and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999359">
Results are reported using the same train and test
splits provided by the organization of SemEval 2012
Task 6. For surprise.OnWn and surprise.SMTnews,
only test data is available and supervised machine
learning is not an option.
Table 5 shows results obtained with the test split
not dropping and dropping unbound predicates. For
comparison purposes, results of the top-3 perform-
ers and participants using the semantic structure of
sentences are also shown. LFT-score systems output
</bodyText>
<footnote confidence="0.98381">
5http://www.cs.york.ac.uk/semeval-2012/
task6/
</footnote>
<bodyText confidence="0.999798096774194">
the score (average of both directions) obtained with
the corresponding logic form transformation (Basic,
SemRels or Full) and are unsupervised: training data
with textual similarity scores is not used. The other
three systems presented are supervised. LFT-scores
+ features combines the 9 LFT-scores and 468 fea-
tures derived from the logic proof. WN-scores uses
as features the 7 scores derived using pairwise word
similarity measures. Finally, All combines the full
set of 484 features. We indicate that the performance
of one of our systems with respect to LFT score Ba-
sic not dropping unbound predicates is significant
with * (confidence 99%) and † (confidence 95%).
Overall, systems that drop unbound predicates
perform better than systems that do not drop them.
The only noticeable exception is LFT-score with
sentences from SMTeuroparl. However, best results
for SMTeuroparl are obtained dropping unbound
predicates and using All features. Henceforth, we
comment on results dropping unbound predicates as
they are higher.
Regarding logic form transformations, one can
see a trend depending on the source of sen-
tences. Polaris, the semantic parser, and the syn-
tactic parser Polaris relies on are mostly trained in
the news domain, and thus semantic representations
have higher quality in that domain. For SMTeu-
roparl and SMTnews, the two corpora closest to the
news domain, Full obtains better results than Ba-
sic and SemRels. The difference is most noticeable
in SMTnews, where Basic yields 0.4616, SemRels
</bodyText>
<page confidence="0.966707">
1241
</page>
<table confidence="0.999927464285714">
System MSRpar MSRvid SMTeuroparl OnWN SMTnews
not noML Basic 0.4963 0.8198 0.5101 0.6103 0.4588
dropping
unbound
predicates
SemRels ∗0.3952 LFT score 0.4920 ∗0.5055 0.4477
∗0.6753
Full 0.4525 ∗0.7024 0.5183 0.5895 0.4956
ML LFT scores + features ∗0.5750 0.8466 0.4725 n/a n/a
WN scores 0.4978 0.8495 0.5217 n/a n/a
All ∗0.5992 †0.8660 0.5194 n/a n/a
dropping noML LFT score Basic †0.5552 0.8234 0.4994 0.6120 0.4616
unbound
predicates
SemRels 0.4556 ∗0.7388 0.4871 ∗0.5113 0.4796
Full 0.5250 ∗0.7672 0.5130 0.5895 †0.5291
ML LFT scores + features ∗0.5770 0.8440 0.5277 n/a n/a
WN scores 0.4977 0.8495 0.5217 n/a n/a
All ∗0.6157 ∗0.8709 †0.5745 n/a n/a
Top (B¨ar et al., 2012) 0.6830 0.8739 0.5280 0.6641 0.4937
performer
(ˇSari´c et al., 2012) 0.6985 0.8620 0.3612 0.7049 0.4683
(Banea et al., 2012) 0.5353 0.8750 0.4203 0.6715 0.4033
Team w/ spirin2 0.5769 0.8203 0.4667 0.5835 0.4945
semantic
structure
(Rios et al., 2012) 0.3628 0.6426 0.3074 0.2806 0.2082
(Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426
</table>
<tableCaption confidence="0.996305">
Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates),
</tableCaption>
<bodyText confidence="0.974976957446809">
and results obtained by the top-3 performers and teams that included in their models features derived from the semantic
structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic
not dropping unbound predicates are indicated with * (confidence 99%) and † (confidence 95%).
0.4796 (+0.0180) and Full 0.5291 (+0.0675 and
+0.0495 respectively).
Outside the news domain (MSRpar, MSRvid,
OnWN), Basic performs better than SemRels and
Full, and Full performs better than SemRels. This
leads to the conclusion that several semantic rela-
tions are often missing, and thus considering con-
cepts even if they are not linked to other concepts
via a semantic relation (Full) is more sound than ig-
noring them (SemRels).
When training data is available (MSRpar,
MSRvid, SMTeuroparl), LFT-scores + features al-
ways outperforms the scores obtained with a single
logic form transformation in an unsupervised man-
ner. In other words, combining the scores obtained
with the three logic form transformations and in-
corporating the additional features derived from the
proofs improves performance. These results demon-
strate that while a shallow logic form transforma-
tion (Basic) offers a strong baseline, it can be suc-
cessfully complemented with logic form transfor-
mations that consider the semantic structure of sen-
tences (SemRels, Full) and additional features char-
acterizing the proofs. The improvements LFT-scores
+ features brings over the LFT-score obtained with
Basic are substantial: 0.0218 (3.9%) for MSRpar,
0.0206 (2.5%) for MSRvid and 0.0283 (5.7%) for
SMTeuroparl.
WN scores, which only uses as features the
scores derived from pairwise word similarity mea-
sures, performs astonishingly well for some cor-
pora. Namely, the differences in performance be-
tween LFT scores + features and WN scores in
MSRvid and SMTeuroparl are minimal (−0.0055
and +0.0060). We believe this is due to the charac-
teristics of these two corpora. Sentence pairs from
MSRvid are very short with 13 tokens on average
(Table 4), i.e., 6.5 tokens per sentence, and SMTeu-
roparl pairs are hard to parse: at least one comes
from a machine translation system and is often un-
grammatical.
Finally, dropping unbound predicates and using
All features outperforms any other system. While
both LFT scores + features and WN scores yield
</bodyText>
<page confidence="0.98635">
1242
</page>
<bodyText confidence="0.996293333333333">
good performance, the combination of the two out-
performs them. Features extracted successfully
complement each other for all corpora.
</bodyText>
<subsectionHeader confidence="0.687528">
4.2.1 A Look at the ML Model
</subsectionHeader>
<bodyText confidence="0.999861588235294">
A benefit of decision trees is that one can inspect
them. This section briefly gives insight about the
most predictive features for All system.
The best features, i.e., features used in decisions
closer to the root, are the LFT-scores calculated us-
ing Basic and Full. The LFT-score obtained us-
ing SemRels is used only when the other two can-
not discriminate. Sorted by impact, the features ex-
tracted for verbs, nouns, semantic relations, named
entities and modifiers follow. Towards the bottom
of the tree, features for specific semantic relations
(AGENT $R, LOCATION $R, etc.) are used. All three
sources (MSRpar, MSRvid and SMTeuroparl) use
features for THEME, LOCATION, AGENT and QUAN-
TIFICATION. MSRpar also benefits from features for
TIME and only SMTeuroparl benefits from TOPIC
and MANNER.
</bodyText>
<subsectionHeader confidence="0.879697">
4.2.2 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999829066666667">
The semantic logic-based approach presented in this
paper either outperforms other systems or performs
in the top-3 (Table 5). Moreover, it clearly outper-
forms any other proposal that takes into account the
semantic structure of sentences. These results lead
to the conclusion that the semantic structure of sen-
tences is worth considering and more effort should
be devoted to deeper approaches.
When using sentences in the news domain (SM-
Teuroparl and SMTnews), i.e., when text is closer
to the domain in which the NLP tools are trained,
our semantic approach yields the best results known
to date. For MSRvid, the system presented here
performs as well as systems that use external
knowledge (Section 2), the differences are mini-
mal (+0.0030, −0.0089, +0.0041) and not statis-
tically significant (confidence 99%). For MSRpar,
the system performs amongst the top-3 even though
two of these systems clearly obtained better results
(+0.0673, +0.0828); both differences are statisti-
cally significant (confidence 99%).
Performance using surprise.OnWN deserves spe-
cial comment. This corpus contains definitions, not
sentences (Table 4). Lin’s similarity measure alone
yields a correlation of 0.6787, beating all systems in
Table 5 except one of the top-3 performers (ˇSari´c et
al., 2012). Our semantic approach is not success-
ful because we cannot extract valid representations,
glosses are rarely a full sentence and are hard to
parse with generic NLP tools like the ones we use.
</bodyText>
<sectionHeader confidence="0.995959" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999994416666667">
This paper presents a novel approach to determine
textual similarity that employs a logic prover to ex-
tract semantic features. A layered methodology to
transform text into logic forms using three logic
form transformations modes is presented. Each
mode captures different levels of knowledge, from
only content words to semantic representations auto-
matically extracted. Best results are obtained when
features derived from the logic prover are comple-
mented with simpler pairwise word similarity mea-
sures. Features that account for the semantic struc-
ture of sentences are incorporated when needed, as
the results obtained with systems All, LFT scores
and WN scores show.
Our approach is heavily dependent on the qual-
ity of semantic representations, and unlike current
top performers, does not require knowledge derived
from Wikipedia or other large corpora. State-of-
the-art NLP tools to extract semantic representations
from text, which are far from perfect, yield promis-
ing results. Indeed, the approach outperforms previ-
ous work when the source text is relatively familiar
to the tools, i.e., within the news domain, and per-
forms in the top-3 otherwise.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994464071428571">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 385–393, Montr´eal, Canada,
7-8 June.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional Linguistics, Montreal, Canada.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised syner-
gistic approach to semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
</reference>
<page confidence="0.751217">
1243
</page>
<reference confidence="0.999358196261683">
mantic Evaluation (SemEval 2012), pages 635–642,
Montr´eal, Canada, 7-8 June.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
435–440, Montr´eal, Canada, 7-8 June.
Johan Bos and Katja Markert. 2006. Recognising tex-
tual entailment with robust logical inference. In Pro-
ceedings of the First international conference on Ma-
chine Learning Challenges: evaluating Predictive Un-
certainty Visual Object Classification, and Recogniz-
ing Textual Entailment, MLCW’05, pages 404–426,
Berlin, Heidelberg. Springer-Verlag.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a ccg parser. In Pro-
ceedings of Coling 2004, pages 1240–1246, Geneva,
Switzerland, Aug 23–Aug 27. COLING.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005). Association for Compu-
tational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1–9, Prague, June. Association for
Computational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13–18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Demetrios Glinos. 2012. Ata-sem: Chunk-based deter-
mination of semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 547–551, Montr´eal,
Canada, 7-8 June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDDExplor. Newsl., 11(1):10–18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: exploring linguistic feature combina-
tions via machine learning. In In Proceedings of the
1999 Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 203–212.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classifica-
tion of semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 33–38, Uppsala, Sweden,
July. Association for Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57–60, Morristown, NJ, USA. Association for
Computational Linguistics.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the Int’l. Conf. on Research in Computational
Linguistics.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305–332. In C. Fellbaum (Ed.), MIT
Press.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ’86, pages 24–26, New York, NY,
USA. ACM.
Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram cor-
pus. In Proceedings of the ACL 2012 System Demon-
strations, pages 169–174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, ICML
’98, pages 296–304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36(3):341–387,
September.
William McCune and Larry Wos. 1997. Otter: The cade-
13 competition incarnations. Journal of Automated
Reasoning, 18:211–220.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. Annotating noun argument struc-
ture for nombank. In LREC. European Language Re-
sources Association.
</reference>
<page confidence="0.840908">
1244
</page>
<reference confidence="0.999897626262626">
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the 21st national conference on Artificial intelligence,
AAAI’06, pages 775–780. AAAI Press.
George A. Miller. 1995. WordNet: A Lexical Database
for English. In Communications of the ACM, vol-
ume 38, pages 39–41.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752–762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Dan Moldovan and Eduardo Blanco. 2012. Po-
laris: Lymba’s semantic parser. In Nicoletta Calzo-
lari, Khalid Choukri, Thierry Declerck, Mehmet U˘gur
Do˘gan, Bente Maegaard, Joseph Mariani, and
Jan Odijk a nd Stelios Piperidis, editors, Proceedings
of the Eighth International Conference on Language
Resources and Evaluation (LREC-2012), pages 66–72,
Istanbul, Turkey, May. European Language Resources
Association (ELRA). ACL Anthology Identifier: L12-
1040.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
F. Lacatusu, A. Novischi, A. Badulescu, and O. Bolo-
han. 2002. Lcc tools for question answering. In
Voorhees and Buckland, editors, Proceedings of the
11th Text REtrieval Conference (TREC-2002), NIST,
Gaithersburg.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised Semantic Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10, Singapore, August. As-
sociation for Computational Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 Task 13: Evaluating Events, Time Expressions,
and Temporal Relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
centAchievements and Future Directions (SEW-2009),
pages 112–116, Boulder, Colorado, June. Association
for Computational Linguistics.
Ross J. Quinlan. 1992. Learning with continuous
classes. In 5th Australian Joint Conference on Arti-
ficial Intelligence, pages 343–348, Singapore. World
Scientific.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of the 20th na-
tional conference on Artificial intelligence - Volume 3,
AAAI’05, pages 1099–1105. AAAI Press.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th international joint conference on Artificial
intelligence - Volume 1, IJCAI’95, pages 448–453, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2012.
Uow: Semantically informed text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 673–678,
Montr´eal, Canada, 7-8 June.
Evan Sandhaus. 2008. The new york times annotated
corpus. In Linguistic Data Consortium, Philadelphia,
PA.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, HLT ’05, pages 371–378, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montr´eal, Canada, 7-8 June.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’94, pages 133–138, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the Proceedings of the Twenty-First
Conference Annual Conference on Uncertainty in Ar-
tificial Intelligence (UAI-05), pages 658–666, Arling-
ton, Virginia. AUAI Press.
</reference>
<page confidence="0.990933">
1245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872028">
<title confidence="0.995474">A Semantically Enhanced Approach to Determine Textual Similarity</title>
<author confidence="0.923022">Blanco</author>
<affiliation confidence="0.998442">Lymba Corporation</affiliation>
<address confidence="0.973816">Richardson, TX 75080 USA</address>
<email confidence="0.999853">eduardo@lymba.com</email>
<email confidence="0.999853">moldovan@lymba.com</email>
<abstract confidence="0.997294">This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<location>Montr´eal,</location>
<contexts>
<context position="675" citStr="Agirre et al., 2012" startWordPosition="93" endWordPosition="96">imilarity Eduardo Blanco and Dan Moldovan Lymba Corporation Richardson, TX 75080 USA {eduardo,moldovan}@lymba.com Abstract This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping</context>
<context position="7089" citStr="Agirre et al., 2012" startWordPosition="1160" endWordPosition="1163">ences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. The top 3 performers (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012), followed a ma1236 Sentences Logic Form Transformation Pairwise Similarity Measures logic forms pairwise word similarity scores Logic Prover LFT-based scores features Machine Learning score Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word </context>
<context position="23951" citStr="Agirre et al., 2012" startWordPosition="3940" endWordPosition="3943">ard tools surprise.SMTnews 3.000 He did, but the initiative did not get very far One sentence often (–/24), [–/399] What he has done without the initiative goes too far. ungrammatical (SMT) Table 4: Examples of sentence pairs belonging to the five sources. The numbers between round (square) parenthesis indicate the average number of tokens per sentence pair (number of instances) in the train and test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnews, output of machine translation systems in the news domain and gold translations. Examples can be found in Table 4, for more details refer to the aforementioned citation. 4.2 Re</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational Linguistics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="22408" citStr="Baker et al., 1998" startWordPosition="3694" endWordPosition="3697">he only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details The unions also staged a five-day strike in March; are missing in one sentence strikes have preceded eight of the last 10 contracts. but the pair is similar MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to (13/13), [750/750] A man is slicing </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th international conference on Computational Linguistics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt: A supervised synergistic approach to semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>635--642</pages>
<location>Montr´eal,</location>
<contexts>
<context position="1496" citStr="Banea et al., 2012" startWordPosition="242" endWordPosition="245">sing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art systems to determine textual similarity (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012) do not account for the semantic structure of sentences, and mostly rely on word pairings and knowledge derived from large corpora, e.g., Figure 1: Semantic representation of 1(a) A man is holding a leaf and 1(b) A monkey is fighting a man. Wikipedia. Regardless of details, each word in sent� is paired with the word in sent2 that is most similar according to some similarity measure. Then, all similarities are added and normalized by the length of sent� to obtain the similarity score from sent� to sent2. The process is repeated to obtain the similarity score from sent2 to sent�, and both scores</context>
<context position="7232" citStr="Banea et al., 2012" startWordPosition="1186" endWordPosition="1189">ON ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. The top 3 performers (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012), followed a ma1236 Sentences Logic Form Transformation Pairwise Similarity Measures logic forms pairwise word similarity scores Logic Prover LFT-based scores features Machine Learning score Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained mo</context>
<context position="27422" citStr="Banea et al., 2012" startWordPosition="4471" endWordPosition="4474">Full 0.4525 ∗0.7024 0.5183 0.5895 0.4956 ML LFT scores + features ∗0.5750 0.8466 0.4725 n/a n/a WN scores 0.4978 0.8495 0.5217 n/a n/a All ∗0.5992 †0.8660 0.5194 n/a n/a dropping noML LFT score Basic †0.5552 0.8234 0.4994 0.6120 0.4616 unbound predicates SemRels 0.4556 ∗0.7388 0.4871 ∗0.5113 0.4796 Full 0.5250 ∗0.7672 0.5130 0.5895 †0.5291 ML LFT scores + features ∗0.5770 0.8440 0.5277 n/a n/a WN scores 0.4977 0.8495 0.5217 n/a n/a All ∗0.6157 ∗0.8709 †0.5745 n/a n/a Top (B¨ar et al., 2012) 0.6830 0.8739 0.5280 0.6641 0.4937 performer (ˇSari´c et al., 2012) 0.6985 0.8620 0.3612 0.7049 0.4683 (Banea et al., 2012) 0.5353 0.8750 0.4203 0.6715 0.4033 Team w/ spirin2 0.5769 0.8203 0.4667 0.5835 0.4945 semantic structure (Rios et al., 2012) 0.3628 0.6426 0.3074 0.2806 0.2082 (Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426 Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates), and results obtained by the top-3 performers and teams that included in their models features derived from the semantic structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic not dropping unbound predicates ar</context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler, and Rada Mihalcea. 2012. Unt: A supervised synergistic approach to semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 635–642, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>435--440</pages>
<location>Montr´eal,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 435–440, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with robust logical inference.</title>
<date>2006</date>
<booktitle>In Proceedings of the First international conference on Machine Learning Challenges: evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW’05,</booktitle>
<pages>404--426</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="9125" citStr="Bos and Markert (2006)" startWordPosition="1478" endWordPosition="1481">6, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recognize textual entailment are similar to the work presented here. Bos and Markert (2006) extract semantic representations with Boxer (Bos et al., 2004) and incorporate background knowledge from external re1http://www.wiktionary.org/ 2A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three </context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katja Markert. 2006. Recognising textual entailment with robust logical inference. In Proceedings of the First international conference on Machine Learning Challenges: evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW’05, pages 404–426, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a ccg parser.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1240--1246</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="9188" citStr="Bos et al., 2004" startWordPosition="1488" endWordPosition="1491"> to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recognize textual entailment are similar to the work presented here. Bos and Markert (2006) extract semantic representations with Boxer (Bos et al., 2004) and incorporate background knowledge from external re1http://www.wiktionary.org/ 2A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step and </context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a ccg parser. In Proceedings of Coling 2004, pages 1240–1246, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="908" citStr="Dolan and Brockett, 2005" startWordPosition="127" endWordPosition="130">text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art systems to determine textual similarity (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012) do not acco</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague,</location>
<contexts>
<context position="795" citStr="Giampiccolo et al., 2007" startWordPosition="111" endWordPosition="114">bstract This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art </context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>SemEval-2007 Task 04: Classification of Semantic Relations between Nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22516" citStr="Girju et al., 2007" startWordPosition="3712" endWordPosition="3715"> the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details The unions also staged a five-day strike in March; are missing in one sentence strikes have preceded eight of the last 10 contracts. but the pair is similar MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to (13/13), [750/750] A man is slicing some carrots. parse SMTeuroparl 4.250 Then perhaps we could have avoided a catastrophe. One sentence often (</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 13–18, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Demetrios Glinos</author>
</authors>
<title>Ata-sem: Chunk-based determination of semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>547--551</pages>
<location>Montr´eal,</location>
<contexts>
<context position="8419" citStr="Glinos, 2012" startWordPosition="1366" endWordPosition="1367">et al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1, which contains over 3 million entries; and ˇSari´c et al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based</context>
<context position="27597" citStr="Glinos, 2012" startWordPosition="4499" endWordPosition="4500">ML LFT score Basic †0.5552 0.8234 0.4994 0.6120 0.4616 unbound predicates SemRels 0.4556 ∗0.7388 0.4871 ∗0.5113 0.4796 Full 0.5250 ∗0.7672 0.5130 0.5895 †0.5291 ML LFT scores + features ∗0.5770 0.8440 0.5277 n/a n/a WN scores 0.4977 0.8495 0.5217 n/a n/a All ∗0.6157 ∗0.8709 †0.5745 n/a n/a Top (B¨ar et al., 2012) 0.6830 0.8739 0.5280 0.6641 0.4937 performer (ˇSari´c et al., 2012) 0.6985 0.8620 0.3612 0.7049 0.4683 (Banea et al., 2012) 0.5353 0.8750 0.4203 0.6715 0.4033 Team w/ spirin2 0.5769 0.8203 0.4667 0.5835 0.4945 semantic structure (Rios et al., 2012) 0.3628 0.6426 0.3074 0.2806 0.2082 (Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426 Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates), and results obtained by the top-3 performers and teams that included in their models features derived from the semantic structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic not dropping unbound predicates are indicated with * (confidence 99%) and † (confidence 95%). 0.4796 (+0.0180) and Full 0.5291 (+0.0675 and +0.0495 respectively). Outside the news domain (MSRpar, MSRvid, OnWN)</context>
</contexts>
<marker>Glinos, 2012</marker>
<rawString>Demetrios Glinos. 2012. Ata-sem: Chunk-based determination of semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 547–551, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="21490" citStr="Hall et al., 2009" startWordPosition="3548" endWordPosition="3551">snik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009). 4 Experiments and Results Logic forms are derived from the output of stateof-the-art NLP tools developed previously and not tuned in any way to the current task or corpora. Our approach is not tied to any tool, set of named entities or relations. Any other semantic representation could be used; the only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting text similarity over short passages: exploring linguistic feature combinations via machine learning. In</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>203--212</pages>
<contexts>
<context position="6812" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1116" endWordPosition="1119">y the same meaning. Sentence 2(c) A woman is applying cosmetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. The top 3 performers (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012), followed a ma1236 Sentences Logic Form Transformation Pairwise Similarity Measures logic forms pairwise word similarity scores Logic Prover LFT-based scores features Machine Lear</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. Detecting text similarity over short passages: exploring linguistic feature combinations via machine learning. In In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 203–212.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>33--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% Solution. In</title>
<date>2006</date>
<booktitle>NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="24313" citStr="Hovy et al., 2006" startWordPosition="3994" endWordPosition="3997">ber of instances) in the train and test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnews, output of machine translation systems in the news domain and gold translations. Examples can be found in Table 4, for more details refer to the aforementioned citation. 4.2 Results and Error Analysis Results are reported using the same train and test splits provided by the organization of SemEval 2012 Task 6. For surprise.OnWn and surprise.SMTnews, only test data is available and supervised machine learning is not an option. Table 5 shows results obtained with the test split not dropping and dropping unbound predicates. For compari</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% Solution. In NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX, pages 57–60, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="20942" citStr="Jiang and Conrath, 1997" startWordPosition="3466" endWordPosition="3469">red with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009). 4 Experiments and Results Logic forms are derived </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification,</title>
<date>1998</date>
<pages>305--332</pages>
<editor>In C. Fellbaum (Ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20822" citStr="Leacock and Chodorow, 1998" startWordPosition="3446" endWordPosition="3449">measures for comparison purposes and to improve robustness in our approach. Basically, each open-class word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow, 1998. Combining local context and WordNet similarity for word sense identification, pages 305–332. In C. Fellbaum (Ed.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86,</booktitle>
<pages>24--26</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20841" citStr="Lesk, 1986" startWordPosition="3451" endWordPosition="3452">d to improve robustness in our approach. Basically, each open-class word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, SIGDOC ’86, pages 24–26, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Aiden Lieberman</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the google books ngram corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>169--174</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="8154" citStr="Lin et al., 2012" startWordPosition="1327" endWordPosition="1330">t do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1, which contains over 3 million entries; and ˇSari´c et al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine lear</context>
</contexts>
<marker>Lin, Michel, Lieberman, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Aiden Lieberman, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the google books ngram corpus. In Proceedings of the ACL 2012 System Demonstrations, pages 169–174, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="20908" citStr="Lin, 1998" startWordPosition="3462" endWordPosition="3463">word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009). 4 Experiments an</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pages 296–304, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="6861" citStr="Madnani and Dorr, 2010" startWordPosition="1122" endWordPosition="1125">smetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. The top 3 performers (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012), followed a ma1236 Sentences Logic Form Transformation Pairwise Similarity Measures logic forms pairwise word similarity scores Logic Prover LFT-based scores features Machine Learning score Figure 3: Main components of our syste</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Comput. Linguist., 36(3):341–387, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William McCune</author>
<author>Larry Wos</author>
</authors>
<title>Otter: The cade13 competition incarnations.</title>
<date>1997</date>
<journal>Journal of Automated Reasoning,</journal>
<pages>18--211</pages>
<contexts>
<context position="14488" citStr="McCune and Wos, 1997" startWordPosition="2343" endWordPosition="2346">e case and combining the three logic forms yields better performance (Section 4). Note that since relation LOCATION(rain, outside) is not extracted from sent2, predicate outside M(x4) is not present in mode SemRels. 3.2 Modified Logic Prover Textual similarity is symmetric and therefore we find proofs in both directions (from lft1 to lft2 and from lft2 to lft1). The logic prover uses a modified resolution procedure to calculate a similarity score and features derived from the proof. The rest of this section exemplifies one direction, lft1 to lft2. The logic prover is a modification of OTTER3 (McCune and Wos, 1997), an automated theorem prover for first-order logic. For the textual similarity task, we load lft1 and -&apos;lft2 to the set of support and lexical chain axioms to the usable list. Then, the logic prover begins its search for a proof. Two scenarios are possible: (1) a contradiction is found, i.e., a proof is found; or (2) a contradiction cannot be found. The modifications to the standard resolution 3http://www.cs.unm.edu/˜mccune/otter/ 1238 sent,: A woman plays an electric guitar sent2: A man is cutting a potato lft,: woman N(x1) &amp; play V(x2) &amp; AGENT SR(x2, x1) &amp; electric M(x3) &amp; guitar N(x4) &amp; in</context>
</contexts>
<marker>McCune, Wos, 1997</marker>
<rawString>William McCune and Larry Wos. 1997. Otter: The cade13 competition incarnations. Journal of Automated Reasoning, 18:211–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>Annotating noun argument structure for nombank. In LREC. European Language Resources Association.</title>
<date>2004</date>
<contexts>
<context position="22471" citStr="Meyers et al., 2004" startWordPosition="3705" endWordPosition="3708">re 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details The unions also staged a five-day strike in March; are missing in one sentence strikes have preceded eight of the last 10 contracts. but the pair is similar MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to (13/13), [750/750] A man is slicing some carrots. parse SMTeuroparl 4.250 Then perhaps we could hav</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. Annotating noun argument structure for nombank. In LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence, AAAI’06,</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="20151" citStr="Mihalcea et al., 2006" startWordPosition="3337" endWordPosition="3341">(t, d and r respectively). We omit the features for predicate O and individual semantic relations because of space constraints. • 108 features for predicates (3 x 6 x 3 x 2 = 108; three features for each of the six predicate types, three LFT modes, two directions) • 360 features specific to a semantic relation (3x 20 x 3 x 2 = 360; three features for each of the 20 semantic relations types, three LFT modes, two directions) 3.3 Pairwise Word Similarities Pairwise word similarity measures between concepts have been long studied, and they have been used for the task of textual similarity before (Mihalcea et al., 2006). We incorporate scores derived using these measures for comparison purposes and to improve robustness in our approach. Basically, each open-class word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with m</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence, AAAI’06, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>In Communications of the ACM,</journal>
<volume>38</volume>
<pages>39--41</pages>
<contexts>
<context position="4442" citStr="Miller, 1995" startWordPosition="745" endWordPosition="746">emantic representations of 2(a) The man used a sword to slice a plastic bottle, 2(b) A man sliced a plastic bottle with a sword, 2(c) A woman is applying cosmetics to her face, 2(d) A woman is putting on makeup, 2(e) A woman is dancing in the rain, and 2(f) A woman dances in the rain outside. Pairs (a, b), (c, d) and (e, f) are highly similar even though concepts and relations only match partially. This paper proposes a novel approach to determine textual similarity. Semantic representations of sentences are exploited, syntactic features omitted and the only external resource used in WordNet (Miller, 1995). The main novelties of our approach are: it (1) derives semantic features from a logic prover to be used in a machine learning framework; (2) uses three logic form transformations capturing different levels of knowledge; and (3) incorporates semantic representations extracted automatically. 1.1 Matching Semantic Representations and Determining Textual Similarity Throughout this paper, the semantic representation of a sentence comprises the concepts in it, semantic relations linking those concepts and named entities qualifying them. First, we note that existing tools to extract semantic relati</context>
<context position="24340" citStr="Miller, 1995" startWordPosition="4000" endWordPosition="4001">nd test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnews, output of machine translation systems in the news domain and gold translations. Examples can be found in Table 4, for more details refer to the aforementioned citation. 4.2 Results and Error Analysis Results are reported using the same train and test splits provided by the organization of SemEval 2012 Task 6. For surprise.OnWn and surprise.SMTnews, only test data is available and supervised machine learning is not an option. Table 5 shows results obtained with the test split not dropping and dropping unbound predicates. For comparison purposes, results of th</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. In Communications of the ACM, volume 38, pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to grade short answer questions using semantic similarity measures and dependency graph alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>752--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6913" citStr="Mohler et al., 2011" startWordPosition="1130" endWordPosition="1133">up are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. The top 3 performers (B¨ar et al., 2012; ˇSari´c et al., 2012; Banea et al., 2012), followed a ma1236 Sentences Logic Form Transformation Pairwise Similarity Measures logic forms pairwise word similarity scores Logic Prover LFT-based scores features Machine Learning score Figure 3: Main components of our system to determine textual similarity. chine learning ap</context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011. Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 752–762, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dan Moldovan</author>
<author>Eduardo Blanco</author>
</authors>
<title>Polaris: Lymba’s semantic parser.</title>
<date>2012</date>
<booktitle>Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>66--72</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, and Jan</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="22289" citStr="Moldovan and Blanco, 2012" startWordPosition="3676" endWordPosition="3679">. Our approach is not tied to any tool, set of named entities or relations. Any other semantic representation could be used; the only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details The unions also staged a five-day strike in March; are missing in one sentence strikes have preceded eight of the last 10 contracts. but the pai</context>
</contexts>
<marker>Moldovan, Blanco, 2012</marker>
<rawString>Dan Moldovan and Eduardo Blanco. 2012. Polaris: Lymba’s semantic parser. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, and Jan Odijk a nd Stelios Piperidis, editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 66–72, Istanbul, Turkey, May. European Language Resources Association (ELRA). ACL Anthology Identifier: L12-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>R Girju</author>
<author>P Morarescu</author>
<author>F Lacatusu</author>
<author>A Novischi</author>
<author>A Badulescu</author>
<author>O Bolohan</author>
</authors>
<title>Lcc tools for question answering.</title>
<date>2002</date>
<booktitle>Proceedings of the 11th Text REtrieval Conference (TREC-2002),</booktitle>
<editor>In Voorhees and Buckland, editors,</editor>
<location>NIST, Gaithersburg.</location>
<contexts>
<context position="22131" citStr="Moldovan et al., 2002" startWordPosition="3655" endWordPosition="3658"> Results Logic forms are derived from the output of stateof-the-art NLP tools developed previously and not tuned in any way to the current task or corpora. Our approach is not tied to any tool, set of named entities or relations. Any other semantic representation could be used; the only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often se</context>
</contexts>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badulescu, Bolohan, 2002</marker>
<rawString>D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Lacatusu, A. Novischi, A. Badulescu, and O. Bolohan. 2002. Lcc tools for question answering. In Voorhees and Buckland, editors, Proceedings of the 11th Text REtrieval Conference (TREC-2002), NIST, Gaithersburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="22440" citStr="Palmer et al., 2005" startWordPosition="3700" endWordPosition="3703">ould be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; 1240 Score Sentence Pair Notes MSRpar 2.600 The unions also staged a five-day strike in March Long sentences, difficult to (36/35) [750/750] that forced all but one of Yale’s dining halls to close. parse; often several details The unions also staged a five-day strike in March; are missing in one sentence strikes have preceded eight of the last 10 contracts. but the pair is similar MSRvid 0.000 A woman is swimming underwater. Short sentences, easy to (13/13), [750/750] A man is slicing some carrots. parse SMTeuroparl </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised Semantic Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12225" citStr="Poon and Domingos, 2009" startWordPosition="1979" endWordPosition="1982">dances, woman), LOCATION(dances, rain) Basic woman N(x1) &amp; dance V(x2) &amp; rain N(x3) &amp; outside M(x4) SemRels woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3) Full woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3) &amp; outside M(x4) Table 1: Examples of logic from transformation using modes Basic, SemRels and Full. 3.1 Logic Form Transformation The logic form transformation (LFT) of a sentence is derived from the concepts in it, the semantic relations linking them and named entities. Unlike other LFT proposals (Zettlemoyer and Collins, 2005; Poon and Domingos, 2009), transforming sentences into logic forms is a straightforward step, the quality of the logic forms is determined by the output of standard NLP tools. We distinguish six types of predicates: • N for nouns, e.g., woman: woman N(x1). • V for verbs, e.g., dances: dance V(x2). • M for adjectives and adverbs, e.g., outside: outside M(x3). • O for concepts encoded by other POS tags. • NE for named entities, e.g., guitar: guitar N(x4) &amp; instrument NE(x4). • SR for semantic relations, e.g., A woman dances: woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1). In order to overcome semantic relation extraction e</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised Semantic Parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Marc Verhagen</author>
</authors>
<title>SemEval2010 Task 13: Evaluating Events, Time Expressions, and Temporal Relations (TempEval-2).</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-2009),</booktitle>
<pages>112--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="23773" citStr="Pustejovsky and Verhagen, 2009" startWordPosition="3911" endWordPosition="3914">perhaps then able prevent a disaster. ungrammatical (SMT) surprise.OnWN 1.500 the alleviation of distress WN glosses, difficult to (–/16), [–/750] a change for the better. parse with standard tools surprise.SMTnews 3.000 He did, but the initiative did not get very far One sentence often (–/24), [–/399] What he has done without the initiative goes too far. ungrammatical (SMT) Table 4: Examples of sentence pairs belonging to the five sources. The numbers between round (square) parenthesis indicate the average number of tokens per sentence pair (number of instances) in the train and test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnew</context>
</contexts>
<marker>Pustejovsky, Verhagen, 2009</marker>
<rawString>James Pustejovsky and Marc Verhagen. 2009. SemEval2010 Task 13: Evaluating Events, Time Expressions, and Temporal Relations (TempEval-2). In Proceedings of the Workshop on Semantic Evaluations: RecentAchievements and Future Directions (SEW-2009), pages 112–116, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross J Quinlan</author>
</authors>
<title>Learning with continuous classes.</title>
<date>1992</date>
<booktitle>In 5th Australian Joint Conference on Artificial Intelligence,</booktitle>
<pages>343--348</pages>
<publisher>World Scientific.</publisher>
<contexts>
<context position="21402" citStr="Quinlan, 1992" startWordPosition="3534" endWordPosition="3535"> LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009). 4 Experiments and Results Logic forms are derived from the output of stateof-the-art NLP tools developed previously and not tuned in any way to the current task or corpora. Our approach is not tied to any tool, set of named entities or relations. Any other semantic representation could be used; the only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a </context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>Ross J. Quinlan. 1992. Learning with continuous classes. In 5th Australian Joint Conference on Artificial Intelligence, pages 343–348, Singapore. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th national conference on Artificial intelligence - Volume 3, AAAI’05,</booktitle>
<pages>1099--1105</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="9511" citStr="Raina et al. (2005)" startWordPosition="1538" endWordPosition="1541">ture. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recognize textual entailment are similar to the work presented here. Bos and Markert (2006) extract semantic representations with Boxer (Bos et al., 2004) and incorporate background knowledge from external re1http://www.wiktionary.org/ 2A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step and extract hundreds of features from the proofs. Tatu and Moldovan (2005) use a modified logic prover that drops predicates when a proof cannot be found. Unlike us, they do not drop unbound predicates and use a single logic form transformation. Another key difference is that they assign fixed weights to predicates a priori i</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of the 20th national conference on Artificial intelligence - Volume 3, AAAI’05, pages 1099–1105. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="20891" citStr="Resnik, 1995" startWordPosition="3459" endWordPosition="3460">ly, each open-class word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009).</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>Uow: Semantically informed text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>673--678</pages>
<location>Montr´eal,</location>
<contexts>
<context position="8439" citStr="Rios et al., 2012" startWordPosition="1368" endWordPosition="1371">obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1, which contains over 3 million entries; and ˇSari´c et al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recog</context>
<context position="27547" citStr="Rios et al., 2012" startWordPosition="4490" endWordPosition="4493"> n/a n/a All ∗0.5992 †0.8660 0.5194 n/a n/a dropping noML LFT score Basic †0.5552 0.8234 0.4994 0.6120 0.4616 unbound predicates SemRels 0.4556 ∗0.7388 0.4871 ∗0.5113 0.4796 Full 0.5250 ∗0.7672 0.5130 0.5895 †0.5291 ML LFT scores + features ∗0.5770 0.8440 0.5277 n/a n/a WN scores 0.4977 0.8495 0.5217 n/a n/a All ∗0.6157 ∗0.8709 †0.5745 n/a n/a Top (B¨ar et al., 2012) 0.6830 0.8739 0.5280 0.6641 0.4937 performer (ˇSari´c et al., 2012) 0.6985 0.8620 0.3612 0.7049 0.4683 (Banea et al., 2012) 0.5353 0.8750 0.4203 0.6715 0.4033 Team w/ spirin2 0.5769 0.8203 0.4667 0.5835 0.4945 semantic structure (Rios et al., 2012) 0.3628 0.6426 0.3074 0.2806 0.2082 (Glinos, 2012) 0.2312 0.6595 0.1504 0.2735 0.1426 Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates), and results obtained by the top-3 performers and teams that included in their models features derived from the semantic structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic not dropping unbound predicates are indicated with * (confidence 99%) and † (confidence 95%). 0.4796 (+0.0180) and Full 0.5291 (+0.0675 and +0.0495 respectivel</context>
</contexts>
<marker>Rios, Aziz, Specia, 2012</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. 2012. Uow: Semantically informed text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 673–678, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus.</title>
<date>2008</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8068" citStr="Sandhaus, 2008" startWordPosition="1315" endWordPosition="1316">ur system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1, which contains over 3 million entries; and ˇSari´c et al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from seman</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus. In Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>371--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9858" citStr="Tatu and Moldovan (2005)" startWordPosition="1591" endWordPosition="1594">ernal re1http://www.wiktionary.org/ 2A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step and extract hundreds of features from the proofs. Tatu and Moldovan (2005) use a modified logic prover that drops predicates when a proof cannot be found. Unlike us, they do not drop unbound predicates and use a single logic form transformation. Another key difference is that they assign fixed weights to predicates a priori instead of using machine learning to determine them. 3 Approach Our approach to determine textual similarity (Figure 3) is grounded on using semantic features derived from a logic prover that are later combined in a standard supervised machine learning framework. First, sentences are transformed into logic forms (lft1, lft2). Then, a modified log</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>Marta Tatu and Dan Moldovan. 2005. A semantic approach to recognizing textual entailment. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 371–378, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<location>Montr´eal,</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>I H Witten</author>
</authors>
<title>Induction of model trees for predicting continuous classes.</title>
<date>1997</date>
<booktitle>In Poster papers of the 9th European Conference on Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="21426" citStr="Wang and Witten, 1997" startWordPosition="3536" endWordPosition="3539">nd Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software package (Hall et al., 2009). 4 Experiments and Results Logic forms are derived from the output of stateof-the-art NLP tools developed previously and not tuned in any way to the current task or corpora. Our approach is not tied to any tool, set of named entities or relations. Any other semantic representation could be used; the only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language</context>
</contexts>
<marker>Wang, Witten, 1997</marker>
<rawString>Y. Wang and I. H. Witten. 1997. Induction of model trees for predicting continuous classes. In Poster papers of the 9th European Conference on Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20868" citStr="Wu and Palmer, 1994" startWordPosition="3454" endWordPosition="3457">tness in our approach. Basically, each open-class word in sent1 is paired with the open-class word in sent2 that is most similar according to some similarity measure. All these individual similarities are summed and normalized by the length of sent1 to find the similarity between sent1 and sent2. The process is repeated from sent2 to sent1 to obtain the similarity between sent2 and sent1, and both overall similarities are averaged to determine the final similarity score. We have experimented with measures Path (distance in a taxonomy), LCH (Leacock and Chodorow, 1998), Lesk (Lesk, 1986), WUP (Wu and Palmer, 1994), Resnik (Resnik, 1995), Lin (Lin, 1998) and JCN (Jiang and Conrath, 1997), and use the WordNet::Similarity package4. 4http://wn-similarity.sourceforge.net/ 3.4 Machine Learning Algorithm We follow a standard supervised machine learning framework. Instances from the training split are used to create a model that is later tested with test instances not seen during training. The model was tuned using 10-fold cross-validation over the training instances. As a learning algorithm, we use bagging with M5P decision trees (Quinlan, 1992; Wang and Witten, 1997) as implemented in the Weka software packa</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05),</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia.</location>
<contexts>
<context position="12199" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1975" endWordPosition="1978">tic relations extracted: AGENT(dances, woman), LOCATION(dances, rain) Basic woman N(x1) &amp; dance V(x2) &amp; rain N(x3) &amp; outside M(x4) SemRels woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3) Full woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1) &amp; rain N(x3) &amp; LOCATION SR(x2,x3) &amp; outside M(x4) Table 1: Examples of logic from transformation using modes Basic, SemRels and Full. 3.1 Logic Form Transformation The logic form transformation (LFT) of a sentence is derived from the concepts in it, the semantic relations linking them and named entities. Unlike other LFT proposals (Zettlemoyer and Collins, 2005; Poon and Domingos, 2009), transforming sentences into logic forms is a straightforward step, the quality of the logic forms is determined by the output of standard NLP tools. We distinguish six types of predicates: • N for nouns, e.g., woman: woman N(x1). • V for verbs, e.g., dances: dance V(x2). • M for adjectives and adverbs, e.g., outside: outside M(x3). • O for concepts encoded by other POS tags. • NE for named entities, e.g., guitar: guitar N(x4) &amp; instrument NE(x4). • SR for semantic relations, e.g., A woman dances: woman N(x1) &amp; dance V(x2) &amp; AGENT SR(x2,x1). In order to overcome sema</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 658–666, Arlington, Virginia. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
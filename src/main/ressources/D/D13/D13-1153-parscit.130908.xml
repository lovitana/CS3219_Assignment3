<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000704">
<title confidence="0.9994635">
Semi-Supervised Representation Learning for
Cross-Lingual Text Classification
</title>
<author confidence="0.994534">
Min Xiao and Yuhong Guo
</author>
<affiliation confidence="0.9982845">
Department of Computer and Information Sciences
Temple University
</affiliation>
<address confidence="0.619139">
Philadelphia, PA 19122, USA
</address>
<email confidence="0.999206">
{minxiao,yuhong}@temple.edu
</email>
<sectionHeader confidence="0.996666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999644142857143">
Cross-lingual adaptation aims to learn a pre-
diction model in a label-scarce target lan-
guage by exploiting labeled data from a label-
rich source language. An effective cross-
lingual adaptation system can substantially re-
duce the manual annotation effort required in
many natural language processing tasks. In
this paper, we propose a new cross-lingual
adaptation approach for document classifica-
tion based on learning cross-lingual discrim-
inative distributed representations of words.
Specifically, we propose to maximize the log-
likelihood of the documents from both lan-
guage domains under a cross-lingual log-
bilinear document model, whileminimizing
the prediction log-losses of labeled docu-
ments. We conduct extensive experiments on
cross-lingual sentiment classification tasks of
Amazon product reviews. Our experimental
results demonstrate the efficacy of the pro-
posed cross-lingual adaptation approach.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958022222222">
With the rapid development of linguistic resources
in different languages, developing cross-lingual nat-
ural language processing (NLP) systems becomes
increasingly important (Bel et al., 2003; Shanahan
et al., 2004). Recently, cross-lingual adaptation
methods have been studied to exploit labeled infor-
mation from an existing source language domain
where labeled training data is abundant for use in
a target language domain where annotated training
data is scarce (Prettenhofer and Stein, 2010). Pre-
vious work has shown that cross-lingual adaptation
can greatly reduce labeling effort for a variety of
cross language NLP tasks such as document catego-
rization (Bel et al., 2003; Amini et al., 2009), genre
classification (Petrenz and Webber, 2012), and sen-
timent classification (Shanahan et al., 2004; Wei and
Pal, 2010; Prettenhofer and Stein, 2010).
The fundamental challenge of cross-lingual adap-
tation stems from a lack of overlap between the fea-
ture space of the source language data and that of
the target language data. To address this challenge,
previous work in the literature mainly relies on au-
tomatic machine translation tools. They first trans-
late all the text data from one language domain into
the other and then apply techniques such as domain
adaptation (Wan et al., 2011; Rigutini and Maggini,
2005; Ling et al., 2008) and multi-view learning
(Amini et al., 2009; Guo and Xiao, 2012b; Wan,
2009) to achieve cross-lingual adaptation. However,
machine translation tools may not be freely available
for all languages. Moreover, translating all the text
data in one language into the other language is too
time-consuming in reality. As an economic alter-
native solution, cross-lingual representation learn-
ing has recently been used in the literature to learn
language-independent representations of the data for
cross language text classification (Prettenhofer and
Stein, 2010; Petrenz and Webber, 2012).
In this paper, we propose to tackle cross language
text classification by inducing cross-lingual predic-
tive data representations with both labeled and un-
labeled documents from the two language domains.
Specifically, we propose a cross-lingual log-bilinear
document model to learn distributed representations
of words, which can capture both the semantic sim-
</bodyText>
<page confidence="0.928778">
1465
</page>
<note confidence="0.7301535">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1465–1475,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998570625">
ilarities of words across languages and the predic-
tive information with respect to the target classifi-
cation task. We conduct the representation learn-
ing by maximizing the log-likelihood of all docu-
ments from both language domains under the cross-
lingual log-bilinear document model and minimiz-
ing the prediction log-losses of labeled documents.
We formulate the learning problem as a joint non-
convex minimization problem and solve it using a
local optimization algorithm. To evaluate the effec-
tiveness of the proposed approach, we conduct ex-
periments on the task of cross language sentiment
classification of Amazon product reviews. The em-
pirical results show the proposed approach is very
effective for cross-lingual document classification,
and outperforms other comparison methods.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955701298701">
Much work in the literature proposes to construct
cross-lingual representations by using aligned paral-
lel data. Basically, they first employ machine trans-
lation tools to translate documents from one lan-
guage domain to the other one and then induce low
dimensional latent representations as interlingual
representations (Littman et al., 1998; Vinokourov
et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo
and Xiao, 2012a). Littman et al. (1998) proposed
a cross-language latent semantic indexing method
to induce interlingual representations by perform-
ing latent semantic indexing over a dual-language
document-term matrix, where each dual-language
document contains its original words and the corre-
sponding translation text. Vinokourov et al. (2002)
proposed a cross-lingual kernel canonical corre-
lation analysis method, which learns two projec-
tions (one for each language) by conducting kernel
canonical correlation analysis over a paired bilin-
gual corpus and then uses the two projections to
project documents from language-specific feature
spaces to the shared multilingual semantic feature
space. Platt et al. (2010) employed oriented prin-
cipal component analysis (Diamantaras and Kung,
1996) over concatenated parallel documents, which
learns a multilingual projection by simultaneously
minimizing the projected distance between paral-
lel documents and maximizing the projected covari-
ance of documents across different languages. Pan
et al. (2011) proposed a bi-view non-negative matrix
tri-factorization method for cross-lingual sentiment
classification on the parallel training and test data.
Guo and Xiao (2012a) developed a transductive
subspace representation learning method for cross-
lingual text classification based on non-negative ma-
trix factorization. Some other works exploited par-
allel data by using multilingual topic models to ex-
tract cross-language latent topics as interlingual rep-
resentations (Mimno et al., 2009; Ni et al., 2011;
Platt et al., 2010; Smet et al., 2011) and using neu-
ral probabilistic language modes to learn word em-
beddings as cross-lingual distributed representations
(Klementiev et al., 2012). Most of them were de-
veloped by applying the latent Dirichlet allocation
(LDA) model (Blei et al., 2003) in a multilingual set-
ting, including the polylingual topic model (Mimno
et al., 2009), the bilingual LDA model (Smet et al.,
2011), and the multilingual LDA model (Ni et al.,
2011). Platt et al. (2010) extended the probabilis-
tic latent semantic analysis (PLSA) model (Hof-
mann, 1999) and presented two variants of multilin-
gual topic models: the joint PLSA model and the
coupled PLSA model. Recently, Klementiev et al.
(2012) extended the neural probabilistic language
model (Bengio et al., 2000) to induce cross-lingual
word distributed representations on a set of word-
level aligned parallel sentences. The applicability
of these approaches however is limited by the avail-
ability of parallel corpus. Translating the whole set
of documents to produce parallel corpus is too time-
consuming, expensive and even practically impossi-
ble for some language pairs. We thus do not evaluate
those approaches in our empirical study.
Another group of works propose to use bilin-
gual dictionaries to learn interlingual representa-
tions (Gliozzo, 2006; Prettenhofer and Stein, 2010).
Gliozzo (2006) first translated each term from one
language to the other using a bilingual dictionary
and used the translated terms to augment origi-
nal documents. Then they conducted latent se-
mantic analysis (LSA) over the document-term ma-
trix with concatenated vocabularies to obtain in-
terlingual representations. Prettenhofer and Stein
(2010) proposed a cross-language structural cor-
respondence learning (CL-SCL) method to induce
language-independent features by using word trans-
lation oracles. They first selected a subset of source
</bodyText>
<page confidence="0.986934">
1466
</page>
<bodyText confidence="0.999981864864865">
language features, which have the highest mutual in-
formation with respect to the class labels in the la-
beled documents from the source language domain,
to translate them into the target language domain,
and then used these pivot pairs to induce cross-
lingual representations by modeling the correlations
between pivot features and non-pivot features. Our
proposed approach shares a similarity with the CL-
SCL method in (Prettenhofer and Stein, 2010) on
only requiring a small amount of word translations.
But our approach performs representation learning
in a semi-supervised manner by directly incorporat-
ing discriminative information with respect to the
target prediction task, while CL-SCL only exploits
labels when selecting pivot features and the struc-
tural correspondence learning process is conducted
in a fully unsupervised fashion.
Some other bilingual resources, such as multilin-
gual WordNet (Fellbaum, 1998) and universal part-
of-speech (POS) tags (Petrov et al., 2012), have also
been exploited in the literature for interlingual learn-
ing. Gliozzo (2006) proposed to use MultiWordNet
to map words from different languages to a common
synset-id as language-sharing terms. A similar work
was proposed in A.R. et al. (2012), which trans-
formed words from different languages to WordNet
synset identifiers as interlingual sense-based rep-
resentations. However, multilingual WordNet re-
sources are not always available for different lan-
guage pairs. Recently, Petrenz and Webber (2012)
used language-specific POS taggers to tag each word
and then mapped those language-specific POS tags
to twelve universal POS tags as interlingual features
for cross language fine-grained genre classification.
This approach requires a POS tagger for each lan-
guage and it may be adversely affected by the POS
tagging accuracy.
</bodyText>
<sectionHeader confidence="0.981958" genericHeader="method">
3 Semi-Supervised Representation
</sectionHeader>
<subsectionHeader confidence="0.758904">
Learning for Cross-Lingual Text
Classification
</subsectionHeader>
<bodyText confidence="0.987559563636364">
In this section, we introduce a semi-supervised
cross-lingual representation learning method and
then use it for cross language text classification.
Assume we have ℓs labeled and us unlabeled doc-
uments in the source language domain S and ℓt la-
beled and ut unlabeled documents in the target lan-
guage domain T . We assume all the documents are
independent and identically distributed in each lan-
guage domain, and each document xi is represented
as a bag of words, xi = {wi1, wit, . . . , wiNi}. We
use (xℓi, yi) to denote the i-th labeled document and
its label, and consider exploiting the labeled docu-
ments in the source domain S for learning classifiers
in the target domain T .
To build connections between the two language
domains, we first construct a set of critical bilingual
word pairs M = {(wsi , wtj)}mi&amp;quot;1, where wsi is a crit-
ical word in the source language domain, wt j is its
translation in the target language domain, and m is
the number of word pairs. Here being critical means
the word should be discriminative for the prediction
task and occur frequently in both language domains.
Following the work (Prettenhofer and Stein, 2010),
we select bilingual word pairs in a heuristic way.
First we select a subset of words from the source lan-
guage domain, which have the highest mutual infor-
mation with the class labels in labeled source docu-
ments. The mutual information is computed based
on the empirical distributions of words and labels
in the labeled source documents. Then we translate
the selected words into the target language using a
translation tool to produce word pairs. Finally we
produce the M set by eliminating any candidate pair
(ws, wt), if either ws occurs less than a predefined
threshold value φ in all source language documents
or wt occurs less than φ in all target language docu-
ments. Given the constructed bilingual word pair set
M, the words appearing in the source language doc-
uments but not in M can be put together to form a
source specific vocabulary set Vs = {ws1, ... , wsvs}.
Similarly, the words appearing in the target language
documents but not in M can be put together to form
a target specific vocabulary set Vt = {wt 1, ... , wt }.
vt
An overall cross-lingual vocabulary set can then be
constructed as V = Vs U Vt U M, which has a total
of v = vs + vt + m entries. This cross-lingual vo-
cabulary set covers all words appearing in both do-
mains, while mapping each bilingual pair in M into
the same entry.
To tackle cross language text classification, we
then propose a cross-lingual log-bilinear document
model to learn a predictive cross-lingual represen-
tation of words, which maps each entry in the vo-
cabulary set V to one row vector in a word embed-
</bodyText>
<page confidence="0.967721">
1467
</page>
<bodyText confidence="0.99996">
ding matrix R E Rvxk. Similar to the log-bilinear
language model (Mnih and Hinton, 2007) and the
log-bilinear document model (Maas et al., 2011),
our proposed model learns a dense feature vector for
each word to capture semantic similarities between
the vocabulary entries. But unlike the previous two
models which only work with a monolingual lan-
guage, our model also captures semantic similarities
across different languages. Moreover, we explicitly
incorporate the label information into our proposed
approach, rendering the induced word embeddings
more discriminative to the target prediction task.
</bodyText>
<subsectionHeader confidence="0.999576">
3.1 Cross-Lingual Word Embeddings
</subsectionHeader>
<bodyText confidence="0.99999175">
As mentioned above, we assume a unified embed-
ding matrix R which contains the distributed vec-
tor representations of words in the two language
domains. However, even in a unified representa-
tion space, the distribution of words in the two do-
mains will be different. To capture the distribution
divergence of the two domains and facilitate cross-
lingual learning, we split the word embedding ma-
trix into three parts: source language specific part
Rs E Rvxks, common part Rc E Rvxkc and tar-
get language specific part Rt E Rvxkt, such that
k = ks + kc + kt. Intuitively, we assume that source
language words contain no target language specific
representations and target language words contain
no source language specific representations. Thus
for words in the two language domains, we retrieve
their distributed vector representations from the em-
bedding matrix R using two mapping functions, 4&apos;S
and 4&apos;T , one for each language domain. The two
mapping functions are defined as
</bodyText>
<equation confidence="0.9998465">
4&apos;S(w) =[Rs(w), Rc(w), 0t]T (1)
4&apos;T (w) =[0s, Rc(w), Rt(w)]T (2)
</equation>
<bodyText confidence="0.999966368421053">
where 0t is a kt-dimensional row vector of zeros,
0s is a ks-dimensional row vector of zeros, Rs(w)
denotes the row vector of Rs matrix corresponding
to the word w, Rc(w) denotes the row vector of Rc
matrix corresponding to the word w, and Rt(w) de-
notes the row vector of Rt matrix corresponding to
the word w. It is easy to see that each pair of words
in M will share the same vector from Rc. To encode
more information into the common part of represen-
tation for better knowledge transfer from the source
language domain to the target language domain, we
assume kc &gt; ks and kc &gt; kt. The form of three part
feature representations has been exploited in previ-
ous work of domain adaptation with heterogeneous
feature spaces (Duan et al., 2012). However, their
approach simply duplicates the original features as
language-specific representations, while we will au-
tomatically learn those three part latent representa-
tions in our approach.
</bodyText>
<subsectionHeader confidence="0.999605">
3.2 Semi-Supervised Cross-Lingual
Representation Learning
</subsectionHeader>
<bodyText confidence="0.9999964">
Given the word representation scheme above, we
conduct cross-lingual representation learning by si-
multaneously maximizing the log-likelihood of all
documents and the conditional likelihood of labeled
documents from the two language domains
</bodyText>
<equation confidence="0.987878333333333">
log PL(wij|0)+
Xα X log PL(yi|xℓi, 0) (3)
LE{S,T} xℓiEL
</equation>
<bodyText confidence="0.999969428571429">
where 0 denotes the model parameters and α is a
trade-off parameter. The first part of the objective
function captures the likelihood of the documents
being generated with the learned representation R.
PL(wij|0) is the probability of word wij appearing
in the document xi from the language domain L, and
is defined as
</bodyText>
<equation confidence="0.916243833333333">
exp (−EL(wij, 0))
PL(wij|0) = (4)
Pw′EV exp (−EL(w′, 0))
The term EL(wij, 0) is a log-bilinear energy func-
tion, defined as
EL(wij, 0) = −dTi 4&apos;L(wij) − bwij (5)
</equation>
<bodyText confidence="0.999953875">
where di is a k-dimensional weight vector for docu-
ment xi and bwij is the bias for word wij. Below we
will use b to denote a v-dimensional vector contain-
ing all words’ biases.
The second part of the objective function in (3)
takes the label information into account and aims
to render the latent word representations more task-
predictive. We use a logistic regression model to
</bodyText>
<equation confidence="0.748206666666667">
X
max
θ
LE{S,T}
Ni
X
j=1
X
xiEL
</equation>
<page confidence="0.939997">
1468
</page>
<bodyText confidence="0.999243666666667">
compute the conditional probability of the class la-
bel given the document with the induced word rep-
resentations, such that
</bodyText>
<equation confidence="0.9967785">
1
PL(yi|xIi,θ) = 1 + exp (−yi (wT,&amp;L(xIi) + q))
</equation>
<bodyText confidence="0.999955">
vector representations of the documents in both lan-
guage domains, we perform cross-lingual document
classification by training a supervised classification
model using labeled data from both language do-
mains and then applying it to classify test documents
in the target language domain.
</bodyText>
<equation confidence="0.62425">
(6)
</equation>
<bodyText confidence="0.9998462">
where w, q are model parameters of the logistic re-
gression model, IFL(xi) is the k-dimensional vector
representation of the document xi in the language
domain L. We compute IFL(xi) by taking average
over all words in the document xi such as
</bodyText>
<equation confidence="0.997816666666667">
1 Ni -bL(wij) (7)
IFL(xi) =
Ni j=1
</equation>
<bodyText confidence="0.998603">
By summing over all descriptions above, we can
see that the proposed semi-supervised representa-
tion learning has a set of model parameters, θ =
{R, {di}, b, w, q}. In order to avoid overfitting, we
add regularization terms for the parameters R, {di}
and w, which leads to the final optimization problem
below
</bodyText>
<equation confidence="0.7670205">
L L Ni )
max xiEL ( L log PL(wij|θ) − γkdik2 2
B j=1
LE{S,T}
L+ α L log PL(yi|xIi, θ)
LE{S,T } xℓiEL
− βkRk2F − ηkwk2 (8)
2
</equation>
<bodyText confidence="0.9999482">
where β, γ, η are trade-off parameters, k · kF denote
the Frobenius norm and k · k2 denote the Euclidean-
norm. This objective function is not jointly convex
in all model parameters. We develop a gradient-
based iterative optimization procedure to seek a lo-
cal optimal solution. We first randomly initialize the
model parameters {di}, R, w and set b and q to ze-
ros. Then we iteratively make gradient-based up-
dates over the model parameters until reach a local
optimal solution.
</bodyText>
<subsectionHeader confidence="0.996641">
3.3 Cross-Lingual Document Classification
</subsectionHeader>
<bodyText confidence="0.99992025">
After solving (8), we obtain a word embedding ma-
trix R. The distributed vector representation of any
given document can then be computed using Eq. (7)
based on Eq. (1) or Eq. (2). Under the distributed
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99996325">
We empirically evaluate the proposed approach us-
ing the cross language sentiment classification tasks
of Amazon product reviews in four languages. In
this section, we report our experimental results.
</bodyText>
<subsectionHeader confidence="0.844628">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999970576923077">
We used the multilingual sentiment classification
dataset1 provided by Prettenhofer and Stein (2010),
which contains Amazon product reviews in four dif-
ferent languages, English (E), French (F), German
(G) and Japanese (J). The English product reviews
were sampled from previous cross-domain senti-
ment classification datasets (Blitzer et al., 2007),
while the other three language product reviews were
crawled from Amazon by the authors in November
2009. In the dataset, each language contains three
categories of product reviews, Books (B), DVD (D)
and Music (M). Each language-category pair con-
tains a balanced training set and test set, each of
which consists of 1000 positive reviews and 1000
negative reviews. Each review is represented as
a unigram bag-of-word feature vector with term-
frequency values. Following the work (Prettenhofer
and Stein, 2010), we used the original English re-
views as the source language while treating the other
three languages as target languages. Thus, we con-
struct nine cross language sentiment classification
tasks (GB, GD, GM, FB, FD, FM, JB, JD, JM), one
for each target language-category pair. For example,
the task GB means that the target language is Ger-
man and the training and test data are samples from
Books reviews.
</bodyText>
<subsectionHeader confidence="0.922012">
4.2 Approaches
</subsectionHeader>
<bodyText confidence="0.999666">
We compare our proposed semi-supervised cross-
lingual representation learning (CL-RL) approach
to the following approaches for cross-lingual doc-
ument classification.
</bodyText>
<footnote confidence="0.991925">
1http://www.webis.de/research/corpora/
</footnote>
<page confidence="0.995752">
1469
</page>
<tableCaption confidence="0.999911333333333">
Table 1: Average classification accuracies and standard deviations for the 9 cross-lingual sentiment classification tasks.
The bold format indicates that the difference between the results of CL-RL and MT is significant with p &lt; 0.05 under
a McNemar paired test for labeling disagreements.
</tableCaption>
<table confidence="0.999882">
Task TB CL-Dict CLD-LSA CL-SCL MT CL-RL
GB 66.25±0.64 69.40±0.61 70.30±0.44 73.78±0.32 78.05±0.64 79.89±0.30
GD 63.16±0.66 66.37±0.63 66.85±0.46 71.99±0.25 75.75±0.58 77.14±0.16
GM 65.42±0.77 68.81±0.51 68.93±0.58 71.58±0.35 74.85±0.62 77.27±0.16
FB 65.98±0.51 69.35±0.48 69.98±0.51 73.89±0.16 78.00±0.49 78.25±0.32
FD 63.76±0.37 67.96±0.60 68.88±0.43 73.79±0.28 75.75±0.71 74.83±0.30
FM 65.94±0.56 67.98±0.69 68.42±0.60 71.20±0.28 74.85±0.49 78.71±0.32
JB 63.86±0.80 59.40±0.29 62.62±0.62 62.49±0.23 67.20±0.80 71.11±0.21
JD 63.59±0.74 62.13±0.26 63.87±0.72 65.54±0.29 67.70±0.57 73.12±0.23
JM 65.84±0.90 63.01±0.46 65.67±0.72 65.49±0.36 68.30±0.61 74.38±0.40
</table>
<listItem confidence="0.865760391304348">
• TB: This is a target baseline method, which
trains a supervised monolingual classifier on
the labeled training data from the target lan-
guage domain without representation learning.
• CL-Dict: This is a simple baseline compar-
ison method, which uses the bilingual word
pairs directly to align features from different
language domains into a unified feature dictio-
nary and then trains a supervised classifier on
this aligned feature space with labeled training
data from both language domains.
• CLD-LSA: This is the cross-lingual represen-
tation learning method developed in (Gliozzo,
2006), which first translates each document
from one language into the other language via
a bilingual dictionary to produce augmenting
features, and then performs latent semantic
analysis (LSA) over the augmented bilingual
document-term matrix.
• CL-SCL: This is the cross language structural
correspondence learning method developed in
(Prettenhofer and Stein, 2010).
• MT: This is a machine translation based com-
</listItem>
<bodyText confidence="0.9931511">
parison method, which first uses an existing
machine translation tool (google translation) to
translate the target language documents into the
source language and then trains a monolingual
classifier with labeled training data from both
domains in the source language.
In all experiments, we used a linear support vec-
tor machine (SVM) for sentiment classification. For
implementation, we used the liblinear package (Fan
et al., 2008) with all of its default parameters. For
the CL-SCL method, we used the same parame-
ter setting as suggested in the paper (Prettenhofer
and Stein, 2010): the number of pivot features is
set as 450, the threshold value for selecting pivot
features is 30, and the reduced dimensionality af-
ter singular value decomposition is 100. For the
CLD-LSA method, we set the dimensionality of la-
tent representation as 1000. Similarly, for our pro-
posed approach, we built the cross-lingual vocabu-
lary M by setting m = 450 and φ = 30. For
our representation learning, we set α = 1, β =
γ = η = 1e−4, and set ks, kc, kt to be 25, 50,
25, respectively. The values of α, β, γ and η are
selected using the first cross language classifica-
tion task GB. We selected the α value from the
set {0.01, 0.1,1,10,100} and selected β, γ,η values
from the set {1e−5, 1e−4, 1e−3, 1e−2, 1e−5} by re-
peating the experiment three times with random data
partitions and choosing the parameter values that led
to the best average classification accuracy.
</bodyText>
<subsectionHeader confidence="0.99913">
4.3 Classification Accuracy
</subsectionHeader>
<bodyText confidence="0.9998222">
For each of the nine cross language sentiment classi-
fication tasks with different target language-category
pairs, we used the training set in the source language
domain (English) as labeled data while treating the
test set in the source language domain as unlabeled.
</bodyText>
<page confidence="0.992716">
1470
</page>
<figureCaption confidence="0.9990995">
Figure 1: Average classification accuracies and standard deviations for 10 runs with respect to different numbers of
labeled training documents in the target language domain.
</figureCaption>
<figure confidence="0.990675645669291">
GB
GD
GM
80
75
75
75
Accuracy
Accuracy
Accuracy
70
70
70
65
65
65
60
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
100 200 300 400 500
#Labeled target instances
FB
100 200 300 400 500
#Labeled target instances
FD
100 200 300 400 500
#Labeled target instances
FM
80
80
80
75
75
Accuracy
Accuracy
75
Accuracy
70
70
70
65
65
65
60
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
100 200 300 400 500
#Labeled target instances
JB
75
100 200 300 400 500
#Labeled target instances
JD
75
100 200 300 400 500
#Labeled target instances
JM
75
70
70
70
65
65
65
60
60
60
55
Accuracy
Accuracy
Accuracy
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
TB
CL−Dict
CLD−LSA
CL−SCL
MT
CL−RL
100 200 300 400 500
#Labeled target instances
100 200 300 400 500
#Labeled target instances
100 200 300 400 500
#Labeled target instances
</figure>
<bodyText confidence="0.99967875">
For target language domain, we used the test set as
test data while randomly selecting 100 documents
from the training set as labeled data and treating the
rest as unlabeled data. Thus, for each task, we have
2000 labeled documents and 2000 unlabeled docu-
ments from the source language domain, and 100
labeled and 1900 unlabeled documents from the tar-
get language domain for training. We have 2000 test
documents from the target language domain as test-
ing data. In each experiment, a classifier is produced
by each approach with the training data and tested
on the testing data. We repeated each experiment
10 times with different random selections of 100 la-
beled training documents from the target language
domain. The average classification accuracies and
standard deviations are reported in Table 1.
From Table 1, we can see that the proposed semi-
supervised cross-lingual representation learning ap-
proach, CL-RL, clearly outperforms all other com-
parison methods on eight out of the nine tasks. The
target baseline TB performs poorly on all the nine
tasks, which suggests that 100 labeled instances
from the target language is far from enough to ob-
tain an accurate sentiment classifier in the target lan-
</bodyText>
<page confidence="0.995091">
1471
</page>
<figureCaption confidence="0.9960715">
Figure 2: Average classification accuracy and standard deviation results for the proposed approach over 10 runs with
respect to different dimensionality for the induced cross-lingual representations.
</figureCaption>
<figure confidence="0.999414193548387">
German
French
Japanese
80
80
80
75
Accuracy
75
Accuracy
75
Accuracy
70
70
Books
DVD
Music
100 200 300 400
#Dimension
70
Books
DVD
Music
100 200 300 400
#Dimension
Books
DVD
Music
60
100 200 300 400
#Dimension
</figure>
<page confidence="0.636963">
65
65
65
</page>
<bodyText confidence="0.999940666666667">
guage domain. By exploiting the large amount of
labeled training data from the source language do-
main, even the simple cross-lingual adaptation ap-
proach, CL-Dict, produces effective improvements
over TB. However, its performance is not consis-
tent across the nine tasks. It has inferior perfor-
mance than TB on the three tasks of adapting En-
glish to the Japanese language domain. This sug-
gests the simple bilingual word-pair based feature
space unification method is far from ideal for pro-
viding effective cross-lingual representations, espe-
cially when two languages (English, Japanese) are
very different. With a better designed representa-
tion learning, CLD-LSA outperforms CL-Dict on all
the nine tasks, but the improvements are very small
on some tasks (e.g., GM). CL-SCL not only out-
performs CL-Dict on all tasks, but also performs
much better than CLD-LSA on most tasks. Its per-
formance nevertheless is inferior to the method of
MT. Though MT can greatly increase the test accu-
racies comparing to the other four methods, TB, CL-
Dict, CLD-LSA, and CL-SCL, the benefit is obtained
at the cost of whole document translations. In con-
trast, our proposed approach does not require whole
document translations, but relies on the same sim-
ple word-pair translations used in CL-Dict. It how-
ever consistently and significantly outperforms TB,
CL-Dict, CLD-LSA, and CL-SCL on all tasks, and
outperforms MT on eight out of the nine tasks.
We also conduct significance tests for our pro-
posed approach and MT using a McNemar paired
test for labeling disagreements (Gillick and Cox,
1989). The results in bold format indicate that they
are significant with p &lt; 0.05. All these results
demonstrate the efficacy of our cross-lingual repre-
sentation learning method.
</bodyText>
<subsectionHeader confidence="0.998856">
4.4 Classification Accuracy vs the Number of
Labeled Target Documents
</subsectionHeader>
<bodyText confidence="0.999990925925926">
Next, we investigated the performance of the six ap-
proaches by varying the number of labeled train-
ing documents from the target language domain.
We maintained the same experimental setting as be-
fore, but investigated a range of different values,
Et = {100, 200, 300, 400, 500}, as the number of la-
beled training documents from the target language
domain. In each experiment, for a given value Et,
we randomly selected Et documents from the train-
ing set of the target language domain as labeled data
and used the rest as unlabeled data. We still per-
formed prediction on the same 2000 test documents
in the target language domain. We repeated each
experiment 10 times based on different random se-
lections of the labeled training data from the target
language domain. The average classification accura-
cies and standard deviations across different Et val-
ues for all comparison methods on all the nine tasks
are plotted in Figure 1.
We can see when the number of labeled target
documents is small, TB performs poorly, especially
for the first six tasks (GB, GD, GM, FB, FD, FM).
By increasing the size of labeled target training data,
TB can greatly increase its prediction accuracies and
even outperform the CL-Dict method. The sim-
ple CL-Dict method has inconsistent performance
across the nine tasks. Its performance is better than
</bodyText>
<page confidence="0.98029">
1472
</page>
<bodyText confidence="0.999991185185185">
TB when the labeled training data in the target lan-
guage domain is very limited and is poor than TB
when the labeled target data reaches 300 for the six
tasks using German and French as target languages.
Moreover, when adapting a system from English to
a much more different target language (Japanese),
CL-Dict produces much lower accuracies for all the
three tasks comparing with TB. These results show
that CL-Dict has very limited capacity on transfer-
ring labeled information from a related source lan-
guage domain. Similar performance is observed for
CLD-LSA. With a more sophisticated representation
learning, the CL-SCL method consistently outper-
forms CL-Dict. However, it produces inferior per-
formance than CLD-LSA on the tasks of JB and JM.
By using more translation resources, the MT method
outperforms TB, CL-Dict, CLD-LSA, CL-SCL in all
the nine tasks across almost all scenarios. Our pro-
posed method CL-RL significantly outperforms all
the other five comparison methods across all experi-
ments except on the task of FD, where MT produces
similar performance. Moreover, it is especially im-
portant to notice that CL-RL achieves high test ac-
curacies even when the number of labeled target in-
stances is small. This is important for transferring
knowledge from a source language to reduce the la-
beling effort in the target language.
</bodyText>
<subsectionHeader confidence="0.994337">
4.5 Sensitivity Analysis
</subsectionHeader>
<bodyText confidence="0.999971052631579">
We also investigated the sensitivity of the proposed
approach over the dimensionality of the induced
cross-lingual representations. We used the same ex-
perimental setting as before, and conducted experi-
ments with a set of different dimensionality values,
k = {100, 200, 300, 400}. For each value k, we
set ks = 0.25k, kc = 0.5k, kt = 0.25k. We re-
peated each experiment for 10 times based on dif-
ferent random selections of labeled target training
data and plotted the average prediction accuracies
and standard deviations in Figure 2 for all the nine
cross-lingual sentiment classification tasks. We can
see the proposed approach produces stable accuracy
results across the range of different k values. This
suggests the proposed approach is not very sensitive
to the dimensionality of the cross-lingual embedding
features within the considered range of values, and
with a small dimensionality of 100, the induced rep-
resentation can already perform very well.
</bodyText>
<subsectionHeader confidence="0.988007">
4.6 Cross-Lingual Word Representations
</subsectionHeader>
<bodyText confidence="0.999987071428571">
Finally, we used the first task GB, which adapts the
Books reviews from English to German, to gain in-
tuitive understandings over the learned cross-lingual
word representations. Given an English word as
seed word, we find its five closest neighboring En-
glish words and German words according to the Eu-
clidean distances calculated in the induced cross-
lingual representation space. We present a few re-
sults in Table 2. From Table 2, we can see that the re-
trieved words in both language domains are seman-
tically close to the seed words, which indicates that
our proposed method can capture semantic similar-
ities of words not only in a monolingual setting but
also in a multilingual setting.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999922533333334">
In this paper, we proposed a semi-supervised cross-
lingual representation learning approach to address
cross-lingual text classification. The distributed
word representation induced by the proposed ap-
proach can capture semantic similarities of words
across languages while maintaining predictive infor-
mation with respect to the target classification tasks.
To evaluate the proposed approach, we conducted
experiments on nine cross language sentiment clas-
sification tasks constructed from the Amazon prod-
uct reviews in four languages, comparing to a num-
ber of comparison methods. The empirical results
showed that the proposed approach can produce
effective cross-lingual adaptation performance and
significantly outperform other comparison methods.
</bodyText>
<sectionHeader confidence="0.997835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994454181818182">
M. Amini, N. Usunier, and C. Goutte. Learning from
multiple partially observed views - an application
to multilingual text categorization. In Advances in
Neural Information Processing Systems (NIPS),
2009.
B. A.R., A. Joshi, and P. Bhattacharyya. Cross-
lingual sentiment analysis for indian languages
using linked wordnets. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING), 2012.
N. Bel, C. Koster, and M. Villegas. Cross-lingual
</reference>
<page confidence="0.996506">
1473
</page>
<tableCaption confidence="0.9899565">
Table 2: Examples of source seed words together with five closest English words and five closest German words
estimated using the Euclidean distance in the cross-lingual representation space on the task GB.
</tableCaption>
<table confidence="0.874264571428571">
books absolutely love
English German English German English German
books buch absolutely absolut love liebe
book b¨ucher definitely absolute loved lieben
text text completely definitiv like wie
page blatt certainly komplett fond wieder
words w¨orter totally sicher feel f¨uhlen
expensive good not
English German English German English German
expensive teuer good gut not nicht
expense h¨oher better besser no nie
overpriced h¨ochsten well nett cannot nein
costly hoch nice großartig non keine
price preis great gr¨oßten never keines
</table>
<reference confidence="0.995006078431373">
text categorization. In Proceedings of European
Conference on Digital Libraries (ECDL), 2003.
Y. Bengio, R. Ducharme, and P. Vincent. A neu-
ral probabilistic language model. In Advances in
Neural Information Processing Systems (NIPS),
2000.
D. Blei, A. Ng, and M. Jordan. Latent dirichlet al-
location. Journal of Machine Learning Research
(JMLR), 3:993–1022, 2003.
J. Blitzer, M. Dredze, and F. Pereira. Biographies,
bollywood, boomboxes and blenders: Domain
adaptation for sentiment classification. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2007.
K. Diamantaras and S. Kung. Principal component
neural networks: theory and applications. Wiley-
Interscience, 1996.
L. Duan, D. Xu, and I. Tsang. Learning with aug-
mented features for heterogeneous domain adap-
tation. In Proceedings of the International Con-
ference on Machine Learning (ICML), 2012.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
LIBLINEAR: A library for large linear classifi-
cation. Journal of Machine Learning Research
(JMLR), 9:1871–1874, 2008.
C. Fellbaum, editor. WordNet: an electronic lexical
database. MIT Press, 1998.
L. Gillick and S. Cox. Some statistical issues
in the comparison of speech recognition algo-
rithms. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing (ICASSP), 1989.
A. Gliozzo. Exploiting comparable corpora and
bilingual dictionaries for cross-language text cat-
egorization. In Proceedings of the International
Conference on Computational Linguistics and the
Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), 2006.
Y. Guo and M. Xiao. Transductive representation
learning for cross-lingual text classification. In
Proceedings of the IEEE International Confer-
ence on Data Mining (ICDM), 2012a.
Y. Guo and M. Xiao. Cross language text clas-
sification via subspace co-regularized multi-view
learning. In Proceedings ofthe International Con-
ference on Machine Learning (ICML), 2012b.
T. Hofmann. Probabilistic latent semantic analysis.
In Proceedings of Uncertainty in Artificial Intelli-
gence (UAI), 1999.
A. Klementiev, I. Titov, and B. Bhattarai. Inducing
crosslingual distributed representations of words.
</reference>
<page confidence="0.881965">
1474
</page>
<reference confidence="0.999601072289156">
In Proceedings ofthe International Conference on
Computational Linguistics (COLING), 2012.
X. Ling, G. Xue, W. Dai, Y. Jiang, Q. Yang, and
Y. Yu. Can chinese web pages be classified with
english data source? In Proceedings of the Inter-
national Conference on World Wide Web (WWW),
2008.
M. Littman, S. Dumais, and T. Landauer. Automatic
Cross-Language Information Retrieval using La-
tent Semantic Indexing, chapter 5, pages 51–62.
Kluwer Academic Publishers, 1998.
A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and
C. Potts. Learning word vectors for sentiment
analysis. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL), 2011.
D. Mimno, H. Wallach, J. Naradowsky, D. Smith,
and A. McCallum. Polylingual topic models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, 2009.
A. Mnih and G. Hinton. Three new graphical mod-
els for statistical language modelling. In Proceed-
ings of the International Conference on Machine
Learning (ICML), 2007.
X. Ni, J. Sun, J. Hu, and Z. Chen. Cross lingual
text classification by mining multilingual topics
from wikipedia. In Proceedings of the ACM In-
ternational Conference on Web Search and Data
Mining (WSDM), 2011.
J. Pan, G. Xue, Y. Yu, and Y. Wang. Cross-lingual
sentiment classification via bi-view non-negative
matrix tri-factorization. In Proceedings of the
Pacific-Asia conference on Advances in knowl-
edge discovery and data mining (PAKDD), 2011.
P. Petrenz and B. Webber. Label propagation for
fine-grained cross-lingual genre classification. In
Proceedings of the NIPS xLiTe workshop, 2012.
S. Petrov, D. Das, and R. McDonald. A universal
part-of-speech tagset. In Proceedings of the Inter-
national Conference on Language Resources and
Evaluation (LREC), 2012.
J. Platt, K. Toutanova, and W. Yih. Translingual doc-
ument representations from discriminative projec-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), 2010.
P. Prettenhofer and B. Stein. Cross-language
text classification using structural correspondence
learning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), 2010.
L. Rigutini and M. Maggini. An em based train-
ing algorithm for cross-language text categoriza-
tion. In Proceedings of the Web Intelligence Con-
ference, 2005.
J. Shanahan, G. Grefenstette, Y. Qu, and D. Evans.
Mining multilingual opinions through classifica-
tion and translation. In Proceedings of AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text, 2004.
W. Smet, J. Tang, and M. Moens. Knowledge trans-
fer across multilingual corpora via latent topics.
In Proceedings of the Pacific-Asia conference on
Advances in knowledge discovery and data min-
ing (PAKDD), 2011.
A. Vinokourov, J. Shawe-taylor, and N. Cristian-
ini. Inferring a semantic representation of text
via cross-language correlation analysis. In Ad-
vances in Neural Information Processing Systems
(NIPS), 2002.
C. Wan, R. Pan, and J. Li. Bi-weighting domain
adaptation for cross-language text classification.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI), 2011.
X. Wan. Co-training for cross-lingual sentiment
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), 2009.
B. Wei and C. Pal. Cross lingual adaptation: An
experiment on sentiment classifications. In Pro-
ceedings of the Annual Meeting of the Asso. for
Computational Linguistics (ACL), 2010.
</reference>
<page confidence="0.991989">
1475
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540820">
<title confidence="0.9983875">Semi-Supervised Representation Learning Cross-Lingual Text Classification</title>
<author confidence="0.99347">Xiao</author>
<affiliation confidence="0.998705">Department of Computer and Information</affiliation>
<address confidence="0.8028135">Temple Philadelphia, PA 19122,</address>
<abstract confidence="0.995139545454546">Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, whileminimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Amini</author>
<author>N Usunier</author>
<author>C Goutte</author>
</authors>
<title>Learning from multiple partially observed views - an application to multilingual text categorization.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<contexts>
<context position="1849" citStr="Amini et al., 2009" startWordPosition="255" endWordPosition="258">eloping cross-lingual natural language processing (NLP) systems becomes increasingly important (Bel et al., 2003; Shanahan et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini</context>
</contexts>
<marker>Amini, Usunier, Goutte, 2009</marker>
<rawString>M. Amini, N. Usunier, and C. Goutte. Learning from multiple partially observed views - an application to multilingual text categorization. In Advances in Neural Information Processing Systems (NIPS), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A R</author>
<author>A Joshi</author>
<author>P Bhattacharyya</author>
</authors>
<title>Crosslingual sentiment analysis for indian languages using linked wordnets.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING),</booktitle>
<marker>R, Joshi, Bhattacharyya, 2012</marker>
<rawString>B. A.R., A. Joshi, and P. Bhattacharyya. Crosslingual sentiment analysis for indian languages using linked wordnets. In Proceedings of the International Conference on Computational Linguistics (COLING), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bel</author>
<author>C Koster</author>
<author>M Villegas</author>
</authors>
<title>Cross-lingual text categorization.</title>
<date>2003</date>
<booktitle>In Proceedings of European Conference on Digital Libraries (ECDL),</booktitle>
<contexts>
<context position="1342" citStr="Bel et al., 2003" startWordPosition="176" endWordPosition="179">y, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, whileminimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach. 1 Introduction With the rapid development of linguistic resources in different languages, developing cross-lingual natural language processing (NLP) systems becomes increasingly important (Bel et al., 2003; Shanahan et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et </context>
</contexts>
<marker>Bel, Koster, Villegas, 2003</marker>
<rawString>N. Bel, C. Koster, and M. Villegas. Cross-lingual text categorization. In Proceedings of European Conference on Digital Libraries (ECDL), 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<contexts>
<context position="7168" citStr="Bengio et al., 2000" startWordPosition="1044" endWordPosition="1047">entiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model. Recently, Klementiev et al. (2012) extended the neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences. The applicability of these approaches however is limited by the availability of parallel corpus. Translating the whole set of documents to produce parallel corpus is too timeconsuming, expensive and even practically impossible for some language pairs. We thus do not evaluate those approaches in our empirical study. Another group of works propose to use bilingual dictionaries to learn interlingual representations (Gliozzo, 2006; Prettenhofer and Stein, 2010). Gliozzo (2006) first translat</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2000</marker>
<rawString>Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. In Advances in Neural Information Processing Systems (NIPS), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>3</volume>
<contexts>
<context position="6673" citStr="Blei et al., 2003" startWordPosition="964" endWordPosition="967"> developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model. Recently, Klementiev et al. (2012) extended the neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentence</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research (JMLR), 3:993–1022, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Asso. for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="19192" citStr="Blitzer et al., 2007" startWordPosition="3018" endWordPosition="3021">ed on Eq. (1) or Eq. (2). Under the distributed 4 Experiments We empirically evaluate the proposed approach using the cross language sentiment classification tasks of Amazon product reviews in four languages. In this section, we report our experimental results. 4.1 Dataset We used the multilingual sentiment classification dataset1 provided by Prettenhofer and Stein (2010), which contains Amazon product reviews in four different languages, English (E), French (F), German (G) and Japanese (J). The English product reviews were sampled from previous cross-domain sentiment classification datasets (Blitzer et al., 2007), while the other three language product reviews were crawled from Amazon by the authors in November 2009. In the dataset, each language contains three categories of product reviews, Books (B), DVD (D) and Music (M). Each language-category pair contains a balanced training set and test set, each of which consists of 1000 positive reviews and 1000 negative reviews. Each review is represented as a unigram bag-of-word feature vector with termfrequency values. Following the work (Prettenhofer and Stein, 2010), we used the original English reviews as the source language while treating the other thr</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the Annual Meeting of the Asso. for Computational Linguistics (ACL), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Diamantaras</author>
<author>S Kung</author>
</authors>
<title>Principal component neural networks: theory and applications. WileyInterscience,</title>
<date>1996</date>
<contexts>
<context position="5632" citStr="Diamantaras and Kung, 1996" startWordPosition="814" endWordPosition="817">c indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and then uses the two projections to project documents from language-specific feature spaces to the shared multilingual semantic feature space. Platt et al. (2010) employed oriented principal component analysis (Diamantaras and Kung, 1996) over concatenated parallel documents, which learns a multilingual projection by simultaneously minimizing the projected distance between parallel documents and maximizing the projected covariance of documents across different languages. Pan et al. (2011) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited par</context>
</contexts>
<marker>Diamantaras, Kung, 1996</marker>
<rawString>K. Diamantaras and S. Kung. Principal component neural networks: theory and applications. WileyInterscience, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Duan</author>
<author>D Xu</author>
<author>I Tsang</author>
</authors>
<title>Learning with augmented features for heterogeneous domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<contexts>
<context position="15290" citStr="Duan et al., 2012" startWordPosition="2373" endWordPosition="2376">of Rs matrix corresponding to the word w, Rc(w) denotes the row vector of Rc matrix corresponding to the word w, and Rt(w) denotes the row vector of Rt matrix corresponding to the word w. It is easy to see that each pair of words in M will share the same vector from Rc. To encode more information into the common part of representation for better knowledge transfer from the source language domain to the target language domain, we assume kc &gt; ks and kc &gt; kt. The form of three part feature representations has been exploited in previous work of domain adaptation with heterogeneous feature spaces (Duan et al., 2012). However, their approach simply duplicates the original features as language-specific representations, while we will automatically learn those three part latent representations in our approach. 3.2 Semi-Supervised Cross-Lingual Representation Learning Given the word representation scheme above, we conduct cross-lingual representation learning by simultaneously maximizing the log-likelihood of all documents and the conditional likelihood of labeled documents from the two language domains log PL(wij|0)+ Xα X log PL(yi|xℓi, 0) (3) LE{S,T} xℓiEL where 0 denotes the model parameters and α is a tra</context>
</contexts>
<marker>Duan, Xu, Tsang, 2012</marker>
<rawString>L. Duan, D. Xu, and I. Tsang. Learning with augmented features for heterogeneous domain adaptation. In Proceedings of the International Conference on Machine Learning (ICML), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fan</author>
<author>K Chang</author>
<author>C Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<volume>9</volume>
<contexts>
<context position="22713" citStr="Fan et al., 2008" startWordPosition="3519" endWordPosition="3522">t-term matrix. • CL-SCL: This is the cross language structural correspondence learning method developed in (Prettenhofer and Stein, 2010). • MT: This is a machine translation based comparison method, which first uses an existing machine translation tool (google translation) to translate the target language documents into the source language and then trains a monolingual classifier with labeled training data from both domains in the source language. In all experiments, we used a linear support vector machine (SVM) for sentiment classification. For implementation, we used the liblinear package (Fan et al., 2008) with all of its default parameters. For the CL-SCL method, we used the same parameter setting as suggested in the paper (Prettenhofer and Stein, 2010): the number of pivot features is set as 450, the threshold value for selecting pivot features is 30, and the reduced dimensionality after singular value decomposition is 100. For the CLD-LSA method, we set the dimensionality of latent representation as 1000. Similarly, for our proposed approach, we built the cross-lingual vocabulary M by setting m = 450 and φ = 30. For our representation learning, we set α = 1, β = γ = η = 1e−4, and set ks, kc,</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research (JMLR), 9:1871–1874, 2008.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="4879" citStr="(1998)" startWordPosition="713" endWordPosition="713">ical results show the proposed approach is very effective for cross-lingual document classification, and outperforms other comparison methods. 2 Related Work Much work in the literature proposes to construct cross-lingual representations by using aligned parallel data. Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo and Xiao, 2012a). Littman et al. (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and then uses the two projections to project documents from language-specific feature </context>
</contexts>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. WordNet: an electronic lexical database. MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>S Cox</author>
</authors>
<title>Some statistical issues in the comparison of speech recognition algorithms.</title>
<date>1989</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<contexts>
<context position="28355" citStr="Gillick and Cox, 1989" startWordPosition="4478" endWordPosition="4481">eatly increase the test accuracies comparing to the other four methods, TB, CLDict, CLD-LSA, and CL-SCL, the benefit is obtained at the cost of whole document translations. In contrast, our proposed approach does not require whole document translations, but relies on the same simple word-pair translations used in CL-Dict. It however consistently and significantly outperforms TB, CL-Dict, CLD-LSA, and CL-SCL on all tasks, and outperforms MT on eight out of the nine tasks. We also conduct significance tests for our proposed approach and MT using a McNemar paired test for labeling disagreements (Gillick and Cox, 1989). The results in bold format indicate that they are significant with p &lt; 0.05. All these results demonstrate the efficacy of our cross-lingual representation learning method. 4.4 Classification Accuracy vs the Number of Labeled Target Documents Next, we investigated the performance of the six approaches by varying the number of labeled training documents from the target language domain. We maintained the same experimental setting as before, but investigated a range of different values, Et = {100, 200, 300, 400, 500}, as the number of labeled training documents from the target language domain. </context>
</contexts>
<marker>Gillick, Cox, 1989</marker>
<rawString>L. Gillick and S. Cox. Some statistical issues in the comparison of speech recognition algorithms. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
</authors>
<title>Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (ICCL-ACL),</booktitle>
<contexts>
<context position="7706" citStr="Gliozzo, 2006" startWordPosition="1128" endWordPosition="1129">012) extended the neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences. The applicability of these approaches however is limited by the availability of parallel corpus. Translating the whole set of documents to produce parallel corpus is too timeconsuming, expensive and even practically impossible for some language pairs. We thus do not evaluate those approaches in our empirical study. Another group of works propose to use bilingual dictionaries to learn interlingual representations (Gliozzo, 2006; Prettenhofer and Stein, 2010). Gliozzo (2006) first translated each term from one language to the other using a bilingual dictionary and used the translated terms to augment original documents. Then they conducted latent semantic analysis (LSA) over the document-term matrix with concatenated vocabularies to obtain interlingual representations. Prettenhofer and Stein (2010) proposed a cross-language structural correspondence learning (CL-SCL) method to induce language-independent features by using word translation oracles. They first selected a subset of source 1466 language features, which h</context>
<context position="9343" citStr="Gliozzo (2006)" startWordPosition="1369" endWordPosition="1370">n only requiring a small amount of word translations. But our approach performs representation learning in a semi-supervised manner by directly incorporating discriminative information with respect to the target prediction task, while CL-SCL only exploits labels when selecting pivot features and the structural correspondence learning process is conducted in a fully unsupervised fashion. Some other bilingual resources, such as multilingual WordNet (Fellbaum, 1998) and universal partof-speech (POS) tags (Petrov et al., 2012), have also been exploited in the literature for interlingual learning. Gliozzo (2006) proposed to use MultiWordNet to map words from different languages to a common synset-id as language-sharing terms. A similar work was proposed in A.R. et al. (2012), which transformed words from different languages to WordNet synset identifiers as interlingual sense-based representations. However, multilingual WordNet resources are not always available for different language pairs. Recently, Petrenz and Webber (2012) used language-specific POS taggers to tag each word and then mapped those language-specific POS tags to twelve universal POS tags as interlingual features for cross language fin</context>
<context position="21871" citStr="Gliozzo, 2006" startWordPosition="3397" endWordPosition="3398">36 68.30±0.61 74.38±0.40 • TB: This is a target baseline method, which trains a supervised monolingual classifier on the labeled training data from the target language domain without representation learning. • CL-Dict: This is a simple baseline comparison method, which uses the bilingual word pairs directly to align features from different language domains into a unified feature dictionary and then trains a supervised classifier on this aligned feature space with labeled training data from both language domains. • CLD-LSA: This is the cross-lingual representation learning method developed in (Gliozzo, 2006), which first translates each document from one language into the other language via a bilingual dictionary to produce augmenting features, and then performs latent semantic analysis (LSA) over the augmented bilingual document-term matrix. • CL-SCL: This is the cross language structural correspondence learning method developed in (Prettenhofer and Stein, 2010). • MT: This is a machine translation based comparison method, which first uses an existing machine translation tool (google translation) to translate the target language documents into the source language and then trains a monolingual cl</context>
</contexts>
<marker>Gliozzo, 2006</marker>
<rawString>A. Gliozzo. Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (ICCL-ACL), 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Guo</author>
<author>M Xiao</author>
</authors>
<title>Transductive representation learning for cross-lingual text classification.</title>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM),</booktitle>
<pages>2012</pages>
<marker>Guo, Xiao, </marker>
<rawString>Y. Guo and M. Xiao. Transductive representation learning for cross-lingual text classification. In Proceedings of the IEEE International Conference on Data Mining (ICDM), 2012a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Guo</author>
<author>M Xiao</author>
</authors>
<title>Cross language text classification via subspace co-regularized multi-view learning.</title>
<booktitle>In Proceedings ofthe International Conference on Machine Learning (ICML),</booktitle>
<pages>2012</pages>
<marker>Guo, Xiao, </marker>
<rawString>Y. Guo and M. Xiao. Cross language text classification via subspace co-regularized multi-view learning. In Proceedings ofthe International Conference on Machine Learning (ICML), 2012b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence (UAI),</booktitle>
<contexts>
<context position="6956" citStr="Hofmann, 1999" startWordPosition="1012" endWordPosition="1014">tations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model. Recently, Klementiev et al. (2012) extended the neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences. The applicability of these approaches however is limited by the availability of parallel corpus. Translating the whole set of documents to produce parallel corpus is too timeconsuming, expensive and even practically impossible for some language pairs. We thus do not evaluate thos</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of Uncertainty in Artificial Intelligence (UAI), 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>I Titov</author>
<author>B Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings ofthe International Conference on Computational Linguistics (COLING),</booktitle>
<contexts>
<context position="6568" citStr="Klementiev et al., 2012" startWordPosition="946" endWordPosition="949"> method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model. Recently, Klementiev et al. (2012) extended the neural probabilistic language model (Bengio et al., 2000)</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>A. Klementiev, I. Titov, and B. Bhattarai. Inducing crosslingual distributed representations of words. In Proceedings ofthe International Conference on Computational Linguistics (COLING), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ling</author>
<author>G Xue</author>
<author>W Dai</author>
<author>Y Jiang</author>
<author>Q Yang</author>
<author>Y Yu</author>
</authors>
<title>Can chinese web pages be classified with english data source?</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on World Wide Web (WWW),</booktitle>
<contexts>
<context position="2487" citStr="Ling et al., 2008" startWordPosition="359" endWordPosition="362">on (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to achieve cross-lingual adaptation. However, machine translation tools may not be freely available for all languages. Moreover, translating all the text data in one language into the other language is too time-consuming in reality. As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 2010; Petrenz and Webber, 2012). In this paper, we </context>
</contexts>
<marker>Ling, Xue, Dai, Jiang, Yang, Yu, 2008</marker>
<rawString>X. Ling, G. Xue, W. Dai, Y. Jiang, Q. Yang, and Y. Yu. Can chinese web pages be classified with english data source? In Proceedings of the International Conference on World Wide Web (WWW), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Littman</author>
<author>S Dumais</author>
<author>T Landauer</author>
</authors>
<title>Automatic Cross-Language Information Retrieval using Latent Semantic Indexing, chapter 5,</title>
<date>1998</date>
<pages>51--62</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="4771" citStr="Littman et al., 1998" startWordPosition="690" endWordPosition="693">proach, we conduct experiments on the task of cross language sentiment classification of Amazon product reviews. The empirical results show the proposed approach is very effective for cross-lingual document classification, and outperforms other comparison methods. 2 Related Work Much work in the literature proposes to construct cross-lingual representations by using aligned parallel data. Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo and Xiao, 2012a). Littman et al. (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a pa</context>
</contexts>
<marker>Littman, Dumais, Landauer, 1998</marker>
<rawString>M. Littman, S. Dumais, and T. Landauer. Automatic Cross-Language Information Retrieval using Latent Semantic Indexing, chapter 5, pages 51–62. Kluwer Academic Publishers, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maas</author>
<author>R Daly</author>
<author>P Pham</author>
<author>D Huang</author>
<author>A Ng</author>
<author>C Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL),</booktitle>
<contexts>
<context position="13019" citStr="Maas et al., 2011" startWordPosition="1995" endWordPosition="1998">hen be constructed as V = Vs U Vt U M, which has a total of v = vs + vt + m entries. This cross-lingual vocabulary set covers all words appearing in both domains, while mapping each bilingual pair in M into the same entry. To tackle cross language text classification, we then propose a cross-lingual log-bilinear document model to learn a predictive cross-lingual representation of words, which maps each entry in the vocabulary set V to one row vector in a word embed1467 ding matrix R E Rvxk. Similar to the log-bilinear language model (Mnih and Hinton, 2007) and the log-bilinear document model (Maas et al., 2011), our proposed model learns a dense feature vector for each word to capture semantic similarities between the vocabulary entries. But unlike the previous two models which only work with a monolingual language, our model also captures semantic similarities across different languages. Moreover, we explicitly incorporate the label information into our proposed approach, rendering the induced word embeddings more discriminative to the target prediction task. 3.1 Cross-Lingual Word Embeddings As mentioned above, we assume a unified embedding matrix R which contains the distributed vector representa</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>A. Maas, R. Daly, P. Pham, D. Huang, A. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H Wallach</author>
<author>J Naradowsky</author>
<author>D Smith</author>
<author>A McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<contexts>
<context position="6369" citStr="Mimno et al., 2009" startWordPosition="914" endWordPosition="917">d distance between parallel documents and maximizing the projected covariance of documents across different languages. Pan et al. (2011) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presente</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>D. Mimno, H. Wallach, J. Naradowsky, D. Smith, and A. McCallum. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<contexts>
<context position="12963" citStr="Mnih and Hinton, 2007" startWordPosition="1986" endWordPosition="1989">... , wt }. vt An overall cross-lingual vocabulary set can then be constructed as V = Vs U Vt U M, which has a total of v = vs + vt + m entries. This cross-lingual vocabulary set covers all words appearing in both domains, while mapping each bilingual pair in M into the same entry. To tackle cross language text classification, we then propose a cross-lingual log-bilinear document model to learn a predictive cross-lingual representation of words, which maps each entry in the vocabulary set V to one row vector in a word embed1467 ding matrix R E Rvxk. Similar to the log-bilinear language model (Mnih and Hinton, 2007) and the log-bilinear document model (Maas et al., 2011), our proposed model learns a dense feature vector for each word to capture semantic similarities between the vocabulary entries. But unlike the previous two models which only work with a monolingual language, our model also captures semantic similarities across different languages. Moreover, we explicitly incorporate the label information into our proposed approach, rendering the induced word embeddings more discriminative to the target prediction task. 3.1 Cross-Lingual Word Embeddings As mentioned above, we assume a unified embedding m</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. Three new graphical models for statistical language modelling. In Proceedings of the International Conference on Machine Learning (ICML), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ni</author>
<author>J Sun</author>
<author>J Hu</author>
<author>Z Chen</author>
</authors>
<title>Cross lingual text classification by mining multilingual topics from wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<contexts>
<context position="6386" citStr="Ni et al., 2011" startWordPosition="918" endWordPosition="921">arallel documents and maximizing the projected covariance of documents across different languages. Pan et al. (2011) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2011</marker>
<rawString>X. Ni, J. Sun, J. Hu, and Z. Chen. Cross lingual text classification by mining multilingual topics from wikipedia. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pan</author>
<author>G Xue</author>
<author>Y Yu</author>
<author>Y Wang</author>
</authors>
<title>Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Pacific-Asia conference on Advances in knowledge discovery and data mining (PAKDD),</booktitle>
<contexts>
<context position="4834" citStr="Pan et al., 2011" startWordPosition="702" endWordPosition="705">ent classification of Amazon product reviews. The empirical results show the proposed approach is very effective for cross-lingual document classification, and outperforms other comparison methods. 2 Related Work Much work in the literature proposes to construct cross-lingual representations by using aligned parallel data. Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo and Xiao, 2012a). Littman et al. (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and then uses the two projections to proj</context>
</contexts>
<marker>Pan, Xue, Yu, Wang, 2011</marker>
<rawString>J. Pan, G. Xue, Y. Yu, and Y. Wang. Cross-lingual sentiment classification via bi-view non-negative matrix tri-factorization. In Proceedings of the Pacific-Asia conference on Advances in knowledge discovery and data mining (PAKDD), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Petrenz</author>
<author>B Webber</author>
</authors>
<title>Label propagation for fine-grained cross-lingual genre classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the NIPS xLiTe workshop,</booktitle>
<contexts>
<context position="1898" citStr="Petrenz and Webber, 2012" startWordPosition="261" endWordPosition="264">essing (NLP) systems becomes increasingly important (Bel et al., 2003; Shanahan et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-</context>
<context position="9765" citStr="Petrenz and Webber (2012)" startWordPosition="1429" endWordPosition="1432">resources, such as multilingual WordNet (Fellbaum, 1998) and universal partof-speech (POS) tags (Petrov et al., 2012), have also been exploited in the literature for interlingual learning. Gliozzo (2006) proposed to use MultiWordNet to map words from different languages to a common synset-id as language-sharing terms. A similar work was proposed in A.R. et al. (2012), which transformed words from different languages to WordNet synset identifiers as interlingual sense-based representations. However, multilingual WordNet resources are not always available for different language pairs. Recently, Petrenz and Webber (2012) used language-specific POS taggers to tag each word and then mapped those language-specific POS tags to twelve universal POS tags as interlingual features for cross language fine-grained genre classification. This approach requires a POS tagger for each language and it may be adversely affected by the POS tagging accuracy. 3 Semi-Supervised Representation Learning for Cross-Lingual Text Classification In this section, we introduce a semi-supervised cross-lingual representation learning method and then use it for cross language text classification. Assume we have ℓs labeled and us unlabeled do</context>
</contexts>
<marker>Petrenz, Webber, 2012</marker>
<rawString>P. Petrenz and B. Webber. Label propagation for fine-grained cross-lingual genre classification. In Proceedings of the NIPS xLiTe workshop, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="9257" citStr="Petrov et al., 2012" startWordPosition="1354" endWordPosition="1357">posed approach shares a similarity with the CLSCL method in (Prettenhofer and Stein, 2010) on only requiring a small amount of word translations. But our approach performs representation learning in a semi-supervised manner by directly incorporating discriminative information with respect to the target prediction task, while CL-SCL only exploits labels when selecting pivot features and the structural correspondence learning process is conducted in a fully unsupervised fashion. Some other bilingual resources, such as multilingual WordNet (Fellbaum, 1998) and universal partof-speech (POS) tags (Petrov et al., 2012), have also been exploited in the literature for interlingual learning. Gliozzo (2006) proposed to use MultiWordNet to map words from different languages to a common synset-id as language-sharing terms. A similar work was proposed in A.R. et al. (2012), which transformed words from different languages to WordNet synset identifiers as interlingual sense-based representations. However, multilingual WordNet resources are not always available for different language pairs. Recently, Petrenz and Webber (2012) used language-specific POS taggers to tag each word and then mapped those language-specific</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. A universal part-of-speech tagset. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
<author>K Toutanova</author>
<author>W Yih</author>
</authors>
<title>Translingual document representations from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="4816" citStr="Platt et al., 2010" startWordPosition="698" endWordPosition="701">ross language sentiment classification of Amazon product reviews. The empirical results show the proposed approach is very effective for cross-lingual document classification, and outperforms other comparison methods. 2 Related Work Much work in the literature proposes to construct cross-lingual representations by using aligned parallel data. Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo and Xiao, 2012a). Littman et al. (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and then uses the two p</context>
<context position="6406" citStr="Platt et al., 2010" startWordPosition="922" endWordPosition="925"> and maximizing the projected covariance of documents across different languages. Pan et al. (2011) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic </context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>J. Platt, K. Toutanova, and W. Yih. Translingual document representations from discriminative projections. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Prettenhofer</author>
<author>B Stein</author>
</authors>
<title>Cross-language text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="1646" citStr="Prettenhofer and Stein, 2010" startWordPosition="221" endWordPosition="224">n product reviews. Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach. 1 Introduction With the rapid development of linguistic resources in different languages, developing cross-lingual natural language processing (NLP) systems becomes increasingly important (Bel et al., 2003; Shanahan et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly re</context>
<context position="3040" citStr="Prettenhofer and Stein, 2010" startWordPosition="439" endWordPosition="442">daptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to achieve cross-lingual adaptation. However, machine translation tools may not be freely available for all languages. Moreover, translating all the text data in one language into the other language is too time-consuming in reality. As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 2010; Petrenz and Webber, 2012). In this paper, we propose to tackle cross language text classification by inducing cross-lingual predictive data representations with both labeled and unlabeled documents from the two language domains. Specifically, we propose a cross-lingual log-bilinear document model to learn distributed representations of words, which can capture both the semantic sim1465 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1465–1475, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics ilarities</context>
<context position="7737" citStr="Prettenhofer and Stein, 2010" startWordPosition="1130" endWordPosition="1133">he neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences. The applicability of these approaches however is limited by the availability of parallel corpus. Translating the whole set of documents to produce parallel corpus is too timeconsuming, expensive and even practically impossible for some language pairs. We thus do not evaluate those approaches in our empirical study. Another group of works propose to use bilingual dictionaries to learn interlingual representations (Gliozzo, 2006; Prettenhofer and Stein, 2010). Gliozzo (2006) first translated each term from one language to the other using a bilingual dictionary and used the translated terms to augment original documents. Then they conducted latent semantic analysis (LSA) over the document-term matrix with concatenated vocabularies to obtain interlingual representations. Prettenhofer and Stein (2010) proposed a cross-language structural correspondence learning (CL-SCL) method to induce language-independent features by using word translation oracles. They first selected a subset of source 1466 language features, which have the highest mutual informat</context>
<context position="11317" citStr="Prettenhofer and Stein, 2010" startWordPosition="1688" endWordPosition="1691">te the i-th labeled document and its label, and consider exploiting the labeled documents in the source domain S for learning classifiers in the target domain T . To build connections between the two language domains, we first construct a set of critical bilingual word pairs M = {(wsi , wtj)}mi&amp;quot;1, where wsi is a critical word in the source language domain, wt j is its translation in the target language domain, and m is the number of word pairs. Here being critical means the word should be discriminative for the prediction task and occur frequently in both language domains. Following the work (Prettenhofer and Stein, 2010), we select bilingual word pairs in a heuristic way. First we select a subset of words from the source language domain, which have the highest mutual information with the class labels in labeled source documents. The mutual information is computed based on the empirical distributions of words and labels in the labeled source documents. Then we translate the selected words into the target language using a translation tool to produce word pairs. Finally we produce the M set by eliminating any candidate pair (ws, wt), if either ws occurs less than a predefined threshold value φ in all source lang</context>
<context position="18945" citStr="Prettenhofer and Stein (2010)" startWordPosition="2982" endWordPosition="2985">the model parameters until reach a local optimal solution. 3.3 Cross-Lingual Document Classification After solving (8), we obtain a word embedding matrix R. The distributed vector representation of any given document can then be computed using Eq. (7) based on Eq. (1) or Eq. (2). Under the distributed 4 Experiments We empirically evaluate the proposed approach using the cross language sentiment classification tasks of Amazon product reviews in four languages. In this section, we report our experimental results. 4.1 Dataset We used the multilingual sentiment classification dataset1 provided by Prettenhofer and Stein (2010), which contains Amazon product reviews in four different languages, English (E), French (F), German (G) and Japanese (J). The English product reviews were sampled from previous cross-domain sentiment classification datasets (Blitzer et al., 2007), while the other three language product reviews were crawled from Amazon by the authors in November 2009. In the dataset, each language contains three categories of product reviews, Books (B), DVD (D) and Music (M). Each language-category pair contains a balanced training set and test set, each of which consists of 1000 positive reviews and 1000 nega</context>
<context position="22233" citStr="Prettenhofer and Stein, 2010" startWordPosition="3445" endWordPosition="3448">age domains into a unified feature dictionary and then trains a supervised classifier on this aligned feature space with labeled training data from both language domains. • CLD-LSA: This is the cross-lingual representation learning method developed in (Gliozzo, 2006), which first translates each document from one language into the other language via a bilingual dictionary to produce augmenting features, and then performs latent semantic analysis (LSA) over the augmented bilingual document-term matrix. • CL-SCL: This is the cross language structural correspondence learning method developed in (Prettenhofer and Stein, 2010). • MT: This is a machine translation based comparison method, which first uses an existing machine translation tool (google translation) to translate the target language documents into the source language and then trains a monolingual classifier with labeled training data from both domains in the source language. In all experiments, we used a linear support vector machine (SVM) for sentiment classification. For implementation, we used the liblinear package (Fan et al., 2008) with all of its default parameters. For the CL-SCL method, we used the same parameter setting as suggested in the paper</context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>P. Prettenhofer and B. Stein. Cross-language text classification using structural correspondence learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rigutini</author>
<author>M Maggini</author>
</authors>
<title>An em based training algorithm for cross-language text categorization.</title>
<date>2005</date>
<booktitle>In Proceedings of the Web Intelligence Conference,</booktitle>
<contexts>
<context position="2467" citStr="Rigutini and Maggini, 2005" startWordPosition="355" endWordPosition="358">., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to achieve cross-lingual adaptation. However, machine translation tools may not be freely available for all languages. Moreover, translating all the text data in one language into the other language is too time-consuming in reality. As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 2010; Petrenz and Webber, 2012)</context>
</contexts>
<marker>Rigutini, Maggini, 2005</marker>
<rawString>L. Rigutini and M. Maggini. An em based training algorithm for cross-language text categorization. In Proceedings of the Web Intelligence Conference, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shanahan</author>
<author>G Grefenstette</author>
<author>Y Qu</author>
<author>D Evans</author>
</authors>
<title>Mining multilingual opinions through classification and translation.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text,</booktitle>
<contexts>
<context position="1366" citStr="Shanahan et al., 2004" startWordPosition="180" endWordPosition="183">aximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, whileminimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach. 1 Introduction With the rapid development of linguistic resources in different languages, developing cross-lingual natural language processing (NLP) systems becomes increasingly important (Bel et al., 2003; Shanahan et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, </context>
</contexts>
<marker>Shanahan, Grefenstette, Qu, Evans, 2004</marker>
<rawString>J. Shanahan, G. Grefenstette, Y. Qu, and D. Evans. Mining multilingual opinions through classification and translation. In Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Smet</author>
<author>J Tang</author>
<author>M Moens</author>
</authors>
<title>Knowledge transfer across multilingual corpora via latent topics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Pacific-Asia conference on Advances in knowledge discovery and data mining (PAKDD),</booktitle>
<contexts>
<context position="6426" citStr="Smet et al., 2011" startWordPosition="926" endWordPosition="929">projected covariance of documents across different languages. Pan et al. (2011) proposed a bi-view non-negative matrix tri-factorization method for cross-lingual sentiment classification on the parallel training and test data. Guo and Xiao (2012a) developed a transductive subspace representation learning method for crosslingual text classification based on non-negative matrix factorization. Some other works exploited parallel data by using multilingual topic models to extract cross-language latent topics as interlingual representations (Mimno et al., 2009; Ni et al., 2011; Platt et al., 2010; Smet et al., 2011) and using neural probabilistic language modes to learn word embeddings as cross-lingual distributed representations (Klementiev et al., 2012). Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009), the bilingual LDA model (Smet et al., 2011), and the multilingual LDA model (Ni et al., 2011). Platt et al. (2010) extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PL</context>
</contexts>
<marker>Smet, Tang, Moens, 2011</marker>
<rawString>W. Smet, J. Tang, and M. Moens. Knowledge transfer across multilingual corpora via latent topics. In Proceedings of the Pacific-Asia conference on Advances in knowledge discovery and data mining (PAKDD), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vinokourov</author>
<author>J Shawe-taylor</author>
<author>N Cristianini</author>
</authors>
<title>Inferring a semantic representation of text via cross-language correlation analysis.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<contexts>
<context position="4796" citStr="Vinokourov et al., 2002" startWordPosition="694" endWordPosition="697">eriments on the task of cross language sentiment classification of Amazon product reviews. The empirical results show the proposed approach is very effective for cross-lingual document classification, and outperforms other comparison methods. 2 Related Work Much work in the literature proposes to construct cross-lingual representations by using aligned parallel data. Basically, they first employ machine translation tools to translate documents from one language domain to the other one and then induce low dimensional latent representations as interlingual representations (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Pan et al., 2011; Guo and Xiao, 2012a). Littman et al. (1998) proposed a cross-language latent semantic indexing method to induce interlingual representations by performing latent semantic indexing over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. Vinokourov et al. (2002) proposed a cross-lingual kernel canonical correlation analysis method, which learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and</context>
</contexts>
<marker>Vinokourov, Shawe-taylor, Cristianini, 2002</marker>
<rawString>A. Vinokourov, J. Shawe-taylor, and N. Cristianini. Inferring a semantic representation of text via cross-language correlation analysis. In Advances in Neural Information Processing Systems (NIPS), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wan</author>
<author>R Pan</author>
<author>J Li</author>
</authors>
<title>Bi-weighting domain adaptation for cross-language text classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<contexts>
<context position="2439" citStr="Wan et al., 2011" startWordPosition="351" endWordPosition="354"> 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to achieve cross-lingual adaptation. However, machine translation tools may not be freely available for all languages. Moreover, translating all the text data in one language into the other language is too time-consuming in reality. As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 201</context>
</contexts>
<marker>Wan, Pan, Li, 2011</marker>
<rawString>C. Wan, R. Pan, and J. Li. Bi-weighting domain adaptation for cross-language text classification. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="2564" citStr="Wan, 2009" startWordPosition="374" endWordPosition="375">ei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to achieve cross-lingual adaptation. However, machine translation tools may not be freely available for all languages. Moreover, translating all the text data in one language into the other language is too time-consuming in reality. As an economic alternative solution, cross-lingual representation learning has recently been used in the literature to learn language-independent representations of the data for cross language text classification (Prettenhofer and Stein, 2010; Petrenz and Webber, 2012). In this paper, we propose to tackle cross language text classification by inducing cross-lingua</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>X. Wan. Co-training for cross-lingual sentiment classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wei</author>
<author>C Pal</author>
</authors>
<title>Cross lingual adaptation: An experiment on sentiment classifications.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Asso. for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="1970" citStr="Wei and Pal, 2010" startWordPosition="273" endWordPosition="276">et al., 2004). Recently, cross-lingual adaptation methods have been studied to exploit labeled information from an existing source language domain where labeled training data is abundant for use in a target language domain where annotated training data is scarce (Prettenhofer and Stein, 2010). Previous work has shown that cross-lingual adaptation can greatly reduce labeling effort for a variety of cross language NLP tasks such as document categorization (Bel et al., 2003; Amini et al., 2009), genre classification (Petrenz and Webber, 2012), and sentiment classification (Shanahan et al., 2004; Wei and Pal, 2010; Prettenhofer and Stein, 2010). The fundamental challenge of cross-lingual adaptation stems from a lack of overlap between the feature space of the source language data and that of the target language data. To address this challenge, previous work in the literature mainly relies on automatic machine translation tools. They first translate all the text data from one language domain into the other and then apply techniques such as domain adaptation (Wan et al., 2011; Rigutini and Maggini, 2005; Ling et al., 2008) and multi-view learning (Amini et al., 2009; Guo and Xiao, 2012b; Wan, 2009) to ac</context>
</contexts>
<marker>Wei, Pal, 2010</marker>
<rawString>B. Wei and C. Pal. Cross lingual adaptation: An experiment on sentiment classifications. In Proceedings of the Annual Meeting of the Asso. for Computational Linguistics (ACL), 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
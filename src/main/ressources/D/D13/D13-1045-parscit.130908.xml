<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9986255">
Improving Web Search Ranking by Incorporating Structured
Annotation of Queries*
</title>
<author confidence="0.998595">
Xiao Ding1, Zhicheng Doul, Bing Qin1, Ting Liu1, Ji-Rong Wen3
</author>
<affiliation confidence="0.9993565">
1Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
2Microsoft Research Asia, Beijing 100190, China
3Renmin University of China, Beijing, China
</affiliation>
<email confidence="0.7654285">
1{xding, qinb, tliu}@ir.hit.edu.cn;
2zhichdou@microsoft.com; 3jirong.wen@gmail.com
</email>
<bodyText confidence="0.999454466666667">
Driving the web search evolution are the user
needs. Users usually have a template in mind when
formulating queries to search for information.
Agarwal et al., (2010) surveyed a search log of 15
million queries from a commercial search engine.
They found that 90% of queries follow certain
templates. For example, by issuing the query
“taylor swift lyrics falling in love”, the users are
actually seeking for the lyrics of the song “Mary&apos;s
Song (oh my my my)” by artist Taylor Swift. The
words “falling in love” are actually part of the
lyrics they are searching for. However, some top
search results are irrelevant to the query, although
they contain all the query terms. For example, the
first top search result shown in Figure 1(a) does
not contain the required lyrics. It just contains the
lyrics of another song of Taylor Swift, rather than
the song that users are seeking.
A possible way to solve the above ranking
problem is to understand the underlying query
structure. For example, after recognizing that
“taylor swift” is an artist name and “falling in love”
are part of the lyrics, we can improve the ranking
by comparing the structured query with the
corresponding structured data in documents
(shown in Figure 1(b)). Some previous studies
investigated how to extract structured information
from user queries, such as query segmentation
(Bergsma and Wang, 2007). The task of query
segmentation is to separate the query words into
</bodyText>
<sectionHeader confidence="0.569501" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925466666667">
Web users are increasingly looking for
structured data, such as lyrics, job, or recipes,
using unstructured queries on the web.
However, retrieving relevant results from such
data is a challenging problem due to the
unstructured language of the web queries. In
this paper, we propose a method to improve
web search ranking by detecting Structured
Annotation of queries based on top search
results. In a structured annotation, the original
query is split into different units that are
associated with semantic attributes in the
corresponding domain. We evaluate our
techniques using real world queries and achieve
significant improvement.
</bodyText>
<sectionHeader confidence="0.982383" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960040857142857">
Search engines are getting more sophisticated by
utilizing information from multiple diverse sources.
One such valuable source of information is
structured and semi-structured data, which is not
very difficult to access, owing to information
extraction (Wong et al., 2009; Etzioni et al., 2008;
Zhai and Liu 2006) and semantic web efforts.
</bodyText>
<footnote confidence="0.516628">
*Work was done when the first author was visiting Microsoft
Research Asia
</footnote>
<page confidence="0.992309">
468
</page>
<note confidence="0.9558594">
Query: taylor swift lyrics falling in love
Search Results (a)
Annotated Tokens (b)
d1 [Taylor Swift, #artist_name]
[Crazier, #song_name]
[Feel like I’m falling and ..., #lyrics]
d2 [Taylor Swift, #artist_name]
[Mary’s Song (oh my my my), #song_name]
[Growing up and falling in love..., #lyrics]
d3 [Taylor Swift, #artist_name]
[Jump Then Fall, #song_name]
[I realize your love is the best ..., #lyrics]
d4 [Taylor Swift, #artist_name]
[Mary’s Song (oh my my my), #song_name]
[Growing up and falling in love..., #lyrics]
</note>
<figure confidence="0.978848846153846">
4
&lt;[taylor swift, #artist_name] lyrics
[falling in love, #lyrics]&gt;
[Taylor Swift, #artist_name, 0.34]
[Mary’s Song (oh my my my), #song_name, 0.16]
[Crazier, #song_name, 0.1]
[Jump Then Fall, #song_name, 0.08]
[Growing up and falling in love..., #lyrics, 0.16]
[Feel like I’m falling and ..., #lyrics, 0.1]
[I realize your love is the best ..., #lyrics, 0.08]
Top Results Re-ranking (e) Query Structured Annotation Generation (d) Weighted Annotated Tokens (c)
...
...
</figure>
<figureCaption confidence="0.998156">
Figure 1. Search results of query “taylor swift lyrics falling in love” and processing pipeline
</figureCaption>
<page confidence="0.689773">
2
3
</page>
<bodyText confidence="0.999871295454546">
disjointed segments so that each segment maps to a
semantic unit (Li et al., 2011). For example, the
segmentation of the query “taylor swift lyrics
falling in love” can be “taylor swift  |lyrics  |falling
in love”. Since query segmentation cannot tell
“talylor swift” is an artist name and “falling in love”
are part of lyrics, it is still difficult for us to judge
whether each part of the query segmentations
matches the right field of the documents or not
(such as judge whether “talylor swift” matches the
artist name in the document). Recently, a lot of
work (Sarkas et al., 2010; Li et al., 2009) proposed
the task of structured annotation of queries which
aims to detect the structure of the query and assign
a specific label to it. However, to our knowledge,
the previous methods do not exploit an effective
approach for improving web search ranking by
incorporating structured annotation of queries.
In this paper, we investigate the possibility of
using structured annotation of queries to improve
web search ranking. Specifically, we propose a
greedy algorithm which uses the structured data
(named annotated tokens in Figure 1(b)) extracted
from the top search results to annotate the latent
structured semantics in web queries. We then
compute matching scores between the annotated
query and the corresponding structured
information contained in documents. The top
search results can be re-ranked according to the
matching scores. However, it is very difficult to
extract structured data from all of the search results.
Hence, we propose a relevance feedback based re-
ranking model. We use these structured documents
whose matching scores are greater than a threshold
as feedback documents, to effectively re-rank other
search results to bring more relevant and novel
information to the user.
Experiments on a large web search dataset from
a major commercial search engine show that the F-
Measure of structured annotation generated by our
approach is as high as 91%. On this dataset, our re-
ranking model using the structured annotations
significantly outperforms two baselines.
The main contributions of our work include:
</bodyText>
<listItem confidence="0.9011895">
1. We propose a novel approach to generate
structured annotation of queries based on top
search results.
2. Although structured annotation of queries has
been studied previously, to the best of our
knowledge this is the first paper that attempts
to improve web search ranking by
incorporating structured annotation of queries.
</listItem>
<bodyText confidence="0.999776111111111">
The rest of this paper is organized as follows.
We briefly introduce related work in Section 2.
Section 3 presents our method for generating
structured annotation of queries. We then propose
two novel re-ranking models based on structured
annotation in Section 4. Section 5 introduces the
data used in this paper. We report experimental
results in Section 6. Finally we conclude the work
in Section 7.
</bodyText>
<page confidence="0.999772">
469
</page>
<sectionHeader confidence="0.999711" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992875">
There is a great deal of prior research that
identifies query structured information. We
summarize this research according to their
different approaches.
</bodyText>
<subsectionHeader confidence="0.99022">
2.1 Structured Annotation of Queries
</subsectionHeader>
<bodyText confidence="0.999905709677419">
Recently, a lot of work has been done on
understanding query structure (Sarkas et al., 2010;
Li et al., 2009; Bendersky et al., 2010). One
important method is structured annotation of
queries which aims to detect the structure of the
query and assign a specific label to it. Li et al.,
(2009) proposed web query tagging and its goal is
to assign to each query term a specified category,
roughly corresponding to a list of attributes. A
semi-supervised Conditional Random Field (CRF)
is used to capture dependencies between query
words and to identify the most likely joint
assignment of words to “categories.” Comparing
with previous work, the advantages of our
approach are on the following aspects. First, we
generate structured annotation of queries based on
top search results, not some global knowledge base
or query logs. Second, they mainly focus on the
method of generating structured annotation of
queries, rather than leverage the generated query
structures to improve web search rankings. In this
paper, we not only offer a novel solution for
generating structured annotation of queries, but
also propose a re-ranking approach to improve
Web search based on structured annotation of
queries. Bendersky et al., (2011) also used top
search results to generate structured annotation of
queries. However, the annotations in their
definition are capitalization, POS tags, and
segmentation indicators, which are different from
ours.
</bodyText>
<subsectionHeader confidence="0.999542">
2.2 Query Template Generation
</subsectionHeader>
<bodyText confidence="0.9984951">
The concept of query template has been discussed
in a few recent papers (Agarwal et al., 2010; Pasca
2011; Liu et al., 2011; Szpektor et al., 2011). A
query template is a sequence of terms, where each
term could be a word or an attribute. For example,
&lt;#artist_name lyrics #lyrics&gt; is a query template,
“#artist_name” and “#lyrics” are attributes, and
“lyrics” is a word. Structured annotation of queries
is different from query template, as a query
template can instantiate multiple queries while a
</bodyText>
<tableCaption confidence="0.999366">
Table 1. Example domain schemas
</tableCaption>
<table confidence="0.9965955">
Domain Schema Example structured annotations
lyrics #artist_name &lt;lyrics of [hey jude, #song_name] [beatles,
#song_name #artist_name]&gt;
#lyrics
job #category &lt;[teacher, #category] job in [America,
#location #location]&gt;
recipe #directions &lt;[baking, # directions] [bread, #
#ingredients ingredients] recipe&gt;
</table>
<bodyText confidence="0.995321">
structured annotation only serves for a specific
query. Unlike query template, our work is ranking-
oriented. We aim to automatically annotate query
structure based on top search results, and further
use these structured annotations to re-rank top
search results for improving search performance.
</bodyText>
<subsectionHeader confidence="0.998606">
2.3 Query Segmentation
</subsectionHeader>
<bodyText confidence="0.99998575">
The task of query segmentation is to separate the
query words into disjointed segments so that each
segment maps to a semantic unit (Li et al., 2011).
Query segmentation techniques have been well
studied in recent literature (Tan and Peng, 2008;
Yu and Shi, 2009). However, structured annotation
of queries cannot only separate the query words
into disjoint segments but can also assign each
segment a semantic label which can help the search
engine to judge whether each part of query
segmentation matches the right field of the
documents or not.
</bodyText>
<subsectionHeader confidence="0.975589">
2.4 Entity Search
</subsectionHeader>
<bodyText confidence="0.9999731">
The problem of entity search has received a great
deal of attention in recent years (Guo et al., 2009;
Bron et al., 2010; Cheng et al., 2007). Its goal is to
answer information needs that focus on entities.
The problem of structured annotation of queries is
related to entity search because for some queries,
structured annotation items are entities or attributes.
Some existing entity search approaches also
exploit knowledge from the structure of webpages
(Zhao et al., 2005). Annotating query structured
information differs from entity search in the
following aspects. First, structured annotation
based ranking is applicable for all queries, rather
than just entity related queries. Second, the result
of an entity search is usually a list of entities, their
attributes, and associated homepages, whereas our
work uses the structured information from
webpages to annotate query structured information
and further leverage structured annotation of
queries to re-rank top search results.
</bodyText>
<page confidence="0.995306">
470
</page>
<sectionHeader confidence="0.95619" genericHeader="method">
3 Structured Annotation of Queries
</sectionHeader>
<subsectionHeader confidence="0.997501">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.997343255319149">
We start our discussion by defining some basic
concepts. A token is defined as a sequence of
words including space, i.e., one or more words. For
example, the bigram “taylor swift” can be a single
token. As our objective is to find structured
annotation of queries in a specific domain, we
begin with a definition of domain schema.
Definition 1 (Domain Schema): For a given
domain of interest, the domain schema is the set of
attributes. We denote the domain schema as A =
[a1, a2, ... , an), where each ai is the name of an
attribute of the domain. Sample domain schemas
are shown in Table 1. In contrast to previous
methods (Agarwal et al., 2010), our definition of
domain schema does not need attribute values. For
the sake of simplicity, this paper assumes that
attributes in domain schema are available.
However, it is not difficult to pre-specify attributes
in a specific domain.
Definition 2 (Annotated Token): An annotated
token in a specific domain is a pair [v, a], where v
is a token and a is a corresponding attribute for v
in this domain. [hey jude, #song_name] is an
example of an annotated token for the “lyrics”
domain shown in Table 1. The words “hey jude”
comprise a token, and its corresponding attribute
name is #song_name. If a token does not have any
corresponding attributes, we denote it as free token.
Definition 3 (Structured Annotation): A
structured annotation p is a sequence of terms &lt;
s1,s2,...,sk &gt;, where each si could be a free token or
an annotated token, and at least one of the terms is
an annotated token, i.e., 3i E [1, k] for which si is
an annotated token.
Given the schema for the domain “lyrics”,
&lt;[taylor swift, #artist_name] lyrics [falling in love,
#lyrics]&gt; is a possible structured annotation for the
query “taylor swift lyrics falling in love”. In this
annotation, [taylor swift, #artist_name] and
[falling in love, #lyrics] are two annotated tokens.
The word “lyrics” is a free token.
Intuitively, a structured annotation corresponds
to an interpretation of the query as a request for
some structured information from documents. The
set of annotated tokens expresses the information
need of the documents that have been requested.
The free tokens may provide more diverse
</bodyText>
<figure confidence="0.5769632">
Algorithm 1: Query Structured Annotation Generation
Input: a list of weighted annotated tokens T = {t1, ... , tm} ;
a query q = “w1, ... , wn” where wi E W;
a pre-defined threshold score S.
Output: a query structured annotation p = &lt;s1, ... , sk&gt;.
</figure>
<listItem confidence="0.861789333333333">
1: Set p = q = {s1, ..., sn}, where si = wi
2: for u = 1 to T.size do
3: compute Match(p, tu)
</listItem>
<equation confidence="0.9217755">
= Match(p,tu.v)
= tu.W X ma.X05i&lt;j5nSim(pij, tu.v),
</equation>
<bodyText confidence="0.965116">
where pij = si,...,sj, s.t. sl E W for l E [i, j]. //pij is just
in the remaining query words
</bodyText>
<listItem confidence="0.990142818181818">
4: end for
5: find the maximum matching tu with
tmax = argmax15u5mMatch(p, tu)
6: if Match(p, tmax) &gt; S then
7: replace si,...,sj in p with [si,...,sj, tmax.a ]
8: remove tmax from T
9: n ← n – (j - i)
10: go to step 2
11: else
12: return p
13: end if
</listItem>
<bodyText confidence="0.846615">
information. Annotated tokens and free tokens
together cover all query terms, reflecting the
complete user intent of the query.
</bodyText>
<subsectionHeader confidence="0.999891">
3.2 Generating Structured Annotation
</subsectionHeader>
<bodyText confidence="0.76423232">
In this paper, given a domain schema A, we
generate structured annotation for a query q based
on the top search results of q. We propose using
top search results, rather than some global
knowledge base or query logs, because:
(1) Top search results have been proven to be
a successful technique for query explanation
(Bendersky et al., 2010).
(2) We have observed that in most cases, a
reasonable percentage of the top search results are
relevant to the query. By aggregating structured
information from the top search results, we can get
more query-dependent annotated tokens than using
global data sources which may contain more noise
and outdated.
(3) Our goal for generating structured
annotation is to improve the ranking quality of
queries. Using top search results enables
simultaneous and consistent detection of structured
information from documents and queries.
As mentioned in Section 3.1, we generate
structured annotation of queries based on annotated
tokens, which are actually structured data (shown
in Figure 1(b)) embedded in web documents. In
this paper, we assume that the annotated tokens are
</bodyText>
<page confidence="0.99401">
471
</page>
<bodyText confidence="0.9998122">
available and we mainly focus on how to use these
annotated tokens from top search results to
generate structured annotation of queries. The
approach is comprised of two parts, one for
weighting annotated tokens and the other for
generating structured annotation of queries based
on the weighted annotated tokens.
Weighting: As shown in Figure 1, annotated
tokens extracted from top results may be
inconsistent, and hence some of the extracted
annotated tokens are less useful or even useless for
generating structured annotation.
We assume that a better annotated token should
be supported by more top results; while a worse
annotated token may appear in fewer results.
Hence we aggregate all the annotated tokens
extracted from top search results, and evaluate the
importance of each unique one by a ranking-aware
voting model as follows. For an annotated token [v,
a], its weight w is defined as:
</bodyText>
<equation confidence="0.925126857142857">
( )
𝑤 = 1 𝑁∑1≤𝑗≤𝑁𝑤𝑗 1
where wj is a voting from document dj, and
𝑁 − 𝑗+ 1
𝑁 , if [𝑣, 𝑎] ∈ 𝑑𝑗
𝑤𝑗 = {
0, else
</equation>
<bodyText confidence="0.996352448275862">
Here, N is the number of top search results and j
is the ranking position of document dj. We then
generate a weighted annotated token [v, a, w] for
each original unique token [v, a].
Generating: The process by which we map a
query q to Structured Annotation is shown in
Algorithm 1. The algorithm takes as input a list of
weighted annotated tokens and the query q, and
outputs the structured annotation of the query q.
The algorithm first partitions the query q by
comparing each sub-sequence of the query with all
the weighted annotated tokens, and find the
maximum matching annotated token (line 1 to line
5). Then, if the degree of match is greater than the
threshold 𝛿 which is a pre-defined threshold score
for fuzzy string matching, the query substring will
be assigned the attribute label of the maximum
matching annotated token (line 6 to line 8). The
algorithm stops when all the weighted annotated
tokens have been scanned, and outputs the
structured annotation of the query.
Note that in some cases, the query may fail to
exactly match with the annotated tokens, due to
spelling errors, acronyms or abbreviations in users’
queries. For example, in the query “broken and
beatuful lyrics”, “broken and beatuful” is a
misspelling of “broken and beautiful.” We adopt a
fuzzy string matching function for comparing a
sub-sequence string s with a token v:
</bodyText>
<equation confidence="0.998921">
𝑆𝑖𝑚(𝑠, 𝑣) = 1 − 𝐸𝑑𝑖𝑡𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑠,𝑣)
max (|𝑠|,|𝑣|) (2)
</equation>
<bodyText confidence="0.999962">
where EditDistance(s, v) measures the edit
distances of two strings, |s |is the length of string s
and |v |is the length of string v.
</bodyText>
<sectionHeader confidence="0.907773" genericHeader="method">
4 Ranking with Structured Annotation
</sectionHeader>
<bodyText confidence="0.999843384615385">
Given a domain schema 𝐴 = {𝑎1, 𝑎2, ⋯ , 𝑎𝑛}, and a
query q, suppose that 𝑝 = &lt; 𝑠1,𝑠2,⋯,𝑠𝑘 &gt; is the
structured annotation for query q obtained using
the method introduced in the above sections. p can
better reflects the user’s real search intent than the
original q, as it presents the structured semantic
information needed instead of a simple word string.
Therefore, a document di can better satisfy a user’s
information need if it contains corresponding
structured semantic information in p. Suppose that
Ti is the set of annotated tokens extracted from
document di, we compute a re-ranking score,
denoted by RScore, for document di as follows:
</bodyText>
<equation confidence="0.888886666666667">
RScore(q, di) = 𝑀𝑎𝑡𝑐ℎ(𝑞, 𝑑𝑖)
= 𝑀𝑎𝑡𝑐ℎ(𝑝, 𝑇𝑖)
= ∑1≤𝑗≤𝑘 ∑𝑡∈𝑇𝑖 𝑀𝑎𝑡𝑐ℎ(𝑠𝑗, 𝑡)
</equation>
<bodyText confidence="0.938316">
where
</bodyText>
<equation confidence="0.465506">
(3)
</equation>
<bodyText confidence="0.999985">
where 𝑠𝑗 is an annotated token in p and t is an
annotated token in di. We use Equation (2) to
compute the similarity between values in query
annotated tokens and values in document annotated
tokens. We propose two re-ranking models,
namely the conservative re-ranking model, to re-
rank top results based on RScore and relevance
feedback based re-ranking model.
</bodyText>
<subsectionHeader confidence="0.985431">
4.1 Conservative Re-ranking Model
</subsectionHeader>
<bodyText confidence="0.999361545454546">
A nature way to re-rank top search results is
according to their RScore. However, we fail to
obtain annotated tokens from some retrieved
documents, and hence the RScore of these
documents are not available. In the conservative
re-ranking model, we only re-rank search results
that have an RScore. For example, suppose there
are five retrieved documents {d1, d2, d3, d4, d5} for
query q, we can extract structured information
from document d3 and d4 and RScore(q, d4) &gt;
RScore(q, d3). Note that we cannot obtain
</bodyText>
<equation confidence="0.348088">
𝑀𝑎𝑡𝑐ℎ(𝑠𝑗, 𝑡)= {𝑆𝑖𝑚(𝑠𝑗. 𝑣𝑗, 𝑡. 𝑣), if 𝑠𝑗. 𝑎𝑗 = 𝑡. 𝑎
0, else
</equation>
<page confidence="0.985734">
472
</page>
<bodyText confidence="0.999898466666667">
structured information from d1, d2, and d5. In the
conservative re-ranking method, d1, d2, and d5
retain their original positions; while d3 and da will
be re-ranked according to their RScore. Therefore,
the final ranking generated by our conservative re-
ranking model should be {d1, d2, da, d3, d5}, in
which the documents are re-ranked among the
affected positions.
There is also useful information in the
documents without structured data, such as
community question answering websites. However,
in the conservative re-ranking model they will not
be re-ranked. This may hurt the performance of our
re-ranking model. One reasonable solution is
relevance feedback model.
</bodyText>
<sectionHeader confidence="0.6848945" genericHeader="method">
4.2 Relevance Feedback based Re-ranking
Model
</sectionHeader>
<bodyText confidence="0.999641476190476">
The disadvantage of the conservative re-ranking
model is that it only can re-rank those top search
results with structured data. To make up its
limitation, we propose a relevance feedback based
re-ranking model. The key idea of this model is
based on the observation that the search results
with the corrected annotated tokens could give
implicit feedback information. Hence, we use these
structured documents whose RScore are greater
than a threshold γ (empirically set it as 0.6) as
feedback documents, to effectively re-rank other
search results to bring more relevant and novel
information to the user.
Formally, given a query Q and a document
collection C, a retrieval system returns a ranked list
of documents D. Let di denote the i-th ranked
document in the ranked list. Our goal is to study
how to use these feedback documents, J ⊆ {d1,...,
dk}, to effectively re-rank the other r search results:
U ⊆ {dk+1,..., dk+r}. A general formula of relevance
feedback model (Salton et al, 1990) R is as follows:
</bodyText>
<equation confidence="0.995989">
𝑅 (𝑄′) = (1 − α)𝐿𝑞 (Q) + α𝐿𝑑 (J) (4)
</equation>
<bodyText confidence="0.999988142857143">
where α ∈ [0, 1] is the feedback coefficient, and 𝐿𝑞
and 𝐿𝑑 are two models that map a query and a set
of relevant documents, respectively, into some
comparable representations. For example, they can
be represented as vectors of weighted terms or
language models.
In this paper, we explore the problem in the
language model framework, particularly the KL-
divergence retrieval model and mixture-model
feedback method (Zhai and Lafferty, 2001), mainly
because language models deliver state-of-the-art
retrieval performance and the mixture-model based
feedback is one of the most effective feedback
techniques which outperforms Rocchio feedback.
</bodyText>
<subsectionHeader confidence="0.457642">
4.2.1 The KL-Divergence Retrieval Model
</subsectionHeader>
<bodyText confidence="0.999997916666667">
The KL-divergence retrieval model was introduced
in Lafferty and Zhai, (2001) as a special case of the
risk minimization retrieval framework and can
support feedback more naturally. In this model,
queries and documents are represented by unigram
language models. Assuming that these language
models can be appropriately estimated, KL-
divergence retrieval model measures the relevance
value of a document D with respect to a query Q
by computing the negative Kullback-Leibler
divergence between the query language model 𝜃𝑄
and the document language model 𝜃𝐷 as follows:
</bodyText>
<equation confidence="0.997963">
𝑆(𝑄, 𝐷) = −𝐷(𝜃𝑄 ||𝜃𝐷) = − ∑ 𝑝(𝑤|𝜃𝑄)𝑙𝑜𝑔 𝑝(𝑤|𝜃𝑄) (5)
𝑤∈𝑉 𝑝(𝑤|𝜃𝐷)
</equation>
<bodyText confidence="0.998650833333333">
where V is the set of words in our vocabulary.
Intuitively, the retrieval performance of the KL-
divergence relies on the estimation of the
document model 𝜃𝐷 and the query model 𝜃𝑄.
For the set of k relevant documents, the
document model 𝜃𝐷 is estimated as 𝑝(w|𝜃𝐷) =
</bodyText>
<equation confidence="0.991721">
𝑘 ∑ 𝑐(𝑤,𝑟𝑖)
1 𝑘 , where 𝑐(𝑤, 𝑟𝑖) is the count of word
𝑖=1 |𝑟𝑖|
</equation>
<bodyText confidence="0.999917125">
w in the i-th relevant document, and |𝑟𝑖  |is the total
number of words in that document. The document
model 𝜃𝐷 needs to be smoothed and an effective
method is Dirichlet smoothing (Zhai et al., 2001).
The query model intuitively captures what the
user is interested in, and thus would affect retrieval
performance. With feedback documents, 𝜃𝑄 is
estimated by the mixture-model feedback method.
</bodyText>
<subsubsectionHeader confidence="0.532239">
4.2.2 The Mixture Model Feedback Method
</subsubsectionHeader>
<bodyText confidence="0.999696">
As the problem definition in Equation (4), the
query model can be estimated by the original query
</bodyText>
<equation confidence="0.8176295">
model 𝑝(𝑤|𝜃𝑄) = 𝑐(𝑤,𝑄)
|𝑄 |(where c(w,Q) is the count
</equation>
<bodyText confidence="0.999978111111111">
of word w in the query Q, and |Q |is the total
number of words in the query) and the feedback
document model. Zhai and Lafferty, (2001)
proposed a mixture model feedback method to
estimate the feedback document model. More
specifically, the model assumes that the feedback
documents can be generated by a background
language model 𝑝 (𝑤 |𝐶) estimated using the whole
collection and an unknown topic language model
</bodyText>
<page confidence="0.999784">
473
</page>
<tableCaption confidence="0.999335">
Table 2. Domain queries used in our experiment
</tableCaption>
<table confidence="0.999917">
Domain Containing Queries Structured
Keyword Annotation%
lyrics “lyrics” 196 95%
job “job” 124 92%
recipe “recipe” 76 93%
</table>
<tableCaption confidence="0.97079">
Table 3. Quality of Structured Annotation. All the
improvements are significant (p &lt; 0.05)
</tableCaption>
<table confidence="0.999414777777778">
Domain Method Precision Recall F-Measure
lyrics Baseline 90.06% 84.92% 87.41%
Our 95.45% 89.83% 92.55%
job Baseline 89.62% 80.14% 84.62%
Our 95.31% 84.93% 89.82%
recipe Baseline 83.96% 84.23% 84.09%
Our 89.68% 88.44% 89.06%
All Baseline 87.88% 83.10% 85.42%
Our 93.61% 88.45% 90.96%
</table>
<bodyText confidence="0.81962675">
OF to be estimated. Formally, let F ⊂ C be a set of
feedback documents. In this paper, F is comprised
of documents that RScore are greater thanγ. The
log-likelihood function of the mixture model is:
</bodyText>
<equation confidence="0.9983625">
𝑙𝑜𝑔(𝐹|OF) =
G𝐷∈FG𝑤∈V 𝑐(w, 𝐷) log [(1 − A)p(w|OF) + Ap(w|𝐶)] (6)
</equation>
<bodyText confidence="0.999520285714286">
where A ∈ [0,1) is a mixture noise parameter
which controls the weight of the background
model. Given a fixed A, a standard EM algorithm
can then be used to estimate p (w |OF), which is
then interpolated with the original query model
p (w |Q) to obtain an improved estimation of the
query model:
</bodyText>
<equation confidence="0.477470909090909">
p(w|O𝑄) = (1 − a)p(w|𝑄) + ap(w|OF) (7)
where a is the feedback coefficient.
61 * Seg-Ranker Ori-Raker Con-Ranker FB-R
5 Data
0.5
We used a dataset composed of 12,396 queries
0.57
randomly sampled from query logs of a search
055
engine. For each query, we retrieved its top 100
0.54 CG ND
</equation>
<bodyText confidence="0.758057">
results from a commercial search engine. The
</bodyText>
<subsectionHeader confidence="0.939283">
Measurement
</subsectionHeader>
<bodyText confidence="0.999972727272727">
documents were judged by human editors. A five-
grade (from 0 to 4 meaning from bad to perfect)
relevance rating was assigned for each document.
We used a proprietary query domain classifier to
identify queries in three domains, namely “lyrics,”
“recipe,” and “job,” from the dataset. The statistics
about these domains are shown in Table 2. To
investigate how many queries may potentially have
structured annotations, we manually created
structured annotations for these queries. The last
column of Table 2 shows the percentage of queries
</bodyText>
<figureCaption confidence="0.981989">
Figure 2. Ranking Quality (* indicates significant
improvement)
</figureCaption>
<bodyText confidence="0.994014285714286">
that have structured annotations created by
annotators. We found that for each domain, there
was on average more than 90% of queries
identified by us that had a certain structured
annotation. This indicates that a large percentage
of these queries contain structured information, as
we expected.
</bodyText>
<sectionHeader confidence="0.993025" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999972692307692">
In this section, we present the structured annotation
of queries and further re-rank the top search results
for the three domains introduced in Section 5. We
used the ranking returned by a commercial search
engine as our one of the Baselines. Note that as the
baseline already uses a large number of ranking
signals, it is very difficult to improve it any further.
We evaluate the ranking quality using the widely
used Normalized Discounted Cumulative Gain
measure (NDCG) (Javelin and Kekalainen., 2000).
We use the same configuration for NDCG as
(Burges et al. 2005). More specifically, for a given
query q, the NDCG@K is computed as:
</bodyText>
<equation confidence="0.993947666666667">
𝑁 = 1 G𝐾
𝑗= 1(2𝑟(𝑗)−1) (4)
𝑞 𝑀𝑞 log (1 + j)
</equation>
<bodyText confidence="0.99876075">
Mq is a normalization constant (the ideal NDCG)
so that a perfect ordering would obtain an NDCG
of 1; and r(j) is the rating score of the j-th
document in the ranking list.
</bodyText>
<subsectionHeader confidence="0.996476">
6.1 Overall Results
6.1.1 Quality of Structured Annotation of
Queries
</subsectionHeader>
<bodyText confidence="0.999860714285714">
We generated the structured annotation of queries
based on the top 10 search results and used S =
0.04 for Algorithm 1. We used several existing
metrics, P (Precision), R (Recall), and F-Measure
to evaluate the quality of the structured annotation.
As a query structured annotation may contain more
than one annotated token, we concluded that the
</bodyText>
<page confidence="0.997953">
474
</page>
<figure confidence="0.996781357142857">
0.61
0.6
0.59
0.58
0.57
0.56
0.55
0.54
Seg-Ranker Ori-Ranker FB-Ranker
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9
Query structured annotation generation threshold δ
Precision
Recall
F-Measure
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9 perfect
Query structured annotation generation threshold δ
(a) Quality of re-ranking (b) Quality of query structured annotation
</figure>
<figureCaption confidence="0.999472">
Figure 3. Quality of re-ranking and quality of query structured annotation with different number of search results
</figureCaption>
<tableCaption confidence="0.9664005">
Table 4. Detailed ranking results on three domains.
All the improvements are significant (p &lt; 0.05)
</tableCaption>
<table confidence="0.9980077">
Domain Ranking Method NDCG@1 NDCG@3 NDCG@5
lyrics Seg-Ranker 0.572 0.574 0.575
Ori-Ranker 0.621 0.628 0.636
FB-Ranker 0.637 0.639 0.647
recipe Seg-Ranker 0.629 0.631 0.634
Ori-Ranker 0.678 0.687 0.696
FB-Ranker 0.707 0.704 0.709
job Seg-Ranker 0.438 0.413 0.408
Ori-Ranker 0.470 0.453 0.442
FB-Ranker 0.504 0.474 0.459
</table>
<bodyText confidence="0.998456176470588">
annotation was correct only if the entire annotation
was completely the same as the annotation labeled
by annotators. Otherwise we treated the structured
annotation as incorrect. Experimental results for
the three domains are shown in Table 3. We
compare our approach with Xiao Li, (2010)
(denoted as baseline), on the dataset described in
Section 5. They labeled the semantic structure of
noun phrase queries based on semi-Markov CRFs.
Our approach achieves better performance than the
baseline (about 5.5% significant improvement on
F-Measure). This indicates that the approach of
generating structured annotation based on the top
search results is more effective. With the high-
quality structured annotation of queries in hand, it
may be possible to obtain better ranking results
using our proposed re-ranking models.
</bodyText>
<subsectionHeader confidence="0.915971">
6.1.2 Re-ranking Result
</subsectionHeader>
<bodyText confidence="0.999776413043479">
We used the models introduced in Section 4 to re-
rank the top 10 search results, based on structured
annotation of queries and annotated tokens.
Recall that our goal is to quantify the
effectiveness of structured annotation of queries
for real web search. One dimension is to compare
with the original search results of a commercial
search engine (denoted as Ori-Ranker). The other
is to compare with the query segmentation based
re-ranking model (denoted as Seg-Ranker; Li et
al., 2011) which tries to improve web search
ranking by incorporating query segmentation. Li et
al., (2011) incorporated query segmentation in the
BM25, unigram language model and bigram
language model retrieval framework, and bigram
language model achieved the best performance. In
this paper, Seg-Ranker integrates bigram language
model with query segmentation.
The ranking results of these models are shown
in Figure 2. This figure shows that all our two
rankers significantly outperform the Ori-Ranker–
the original search results of a commercial search
engine. This means that using high-quality
structured annotation does help better
understanding of user intent. By comparing these
structured annotations and the annotated tokens in
documents, we can re-rank the more relevant
results higher and yield better ranking quality.
Figure 2 also suggests that structured annotation
based re-ranking models outperform query
segmentation based re-ranking model. This is
mainly because structured annotation can not only
separate the query words into disjoint segments but
can also assign each segment a semantic label.
Taking full advantage of the semantic label can
lead to better ranking performance.
Furthermore, Figure 2 shows that FB-Ranker
outperforms Con-Ranker. The main reason is that
in Con-Ranker, we can only reasonably re-rank the
search results with structured data. However, in
FB-Ranker we can not only re-rank the structured
search results but also can re-rank other documents
by incorporating implicit information from those
structured documents.
On average, FB-Ranker achieves the best
ranking performance. Table 4 shows more detailed
</bodyText>
<page confidence="0.995411">
475
</page>
<figure confidence="0.997665785714286">
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
Number of search results
Precision
Recall
F-Measure
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
Number of search results
0.61
0.59
0.58
0.57
0.56
0.55
0.54
0.6
Seg-Ranker Ori-Ranker FB-Ranker
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
(a) Quality of re-ranking (b) Quality of query structured annotation
</figure>
<figureCaption confidence="0.999972">
Figure 4. Quality of re-ranking and quality of query structured annotation with different number of search results
</figureCaption>
<bodyText confidence="0.999438125">
results for the three selected domains. This table
shows that FB-Ranker consistently outperforms the
two baseline rankers on these domains. In the
remaining part of this paper, we will only report
the results for this ranker, due to space limitations.
Table 4 also indicates that we can get robust
ranking improvement in different domains, and we
will consider applying it to more domains.
</bodyText>
<subsectionHeader confidence="0.996815">
6.2 Experiment with Different Thresholds of
Query Structured Annotation Algorithm
</subsectionHeader>
<bodyText confidence="0.998654977777778">
As introduced in Algorithm 1, we pre-defined a
threshold 6 for fuzzy string matching. We
evaluated the quality of re-ranking and query
structured annotation with different settings for 6.
The results are shown in Figure 3. We found that:
(1) When we use S = 0, which means that the
structured annotations can be generated no matter
how small the similarity between the query string
and a weighted annotated token is, we can get a
significant NDCG@3 gain of 2.15%. Figure 3(b)
shows that the precision of the structured
annotation is lowest when S = 0. However, the
precision is still as high as 0.7375, and the highest
recall is obtained in this case. This means that the
quality of the generated structured annotations is
still reasonable, and hence we can get a ranking
improvement when S = 0, as shown in Figure 3(a).
(2) Figure 3(a) suggests that the quality of re-
ranking increases when the threshold 6 increases
from 0 to 0.05. It then decreases when 6 increases
from 0.06 to 0.5. Comparing these two figures
shows that the trend of re-ranking performance
adheres to the quality of the structured annotation.
The settings for 6 dramatically affect the recall and
precision of the structured annotation; and hence
the ranking quality is impacted. The larger 6 is, the
lower the recall of the structured annotation is.
(3) Since the re-ranking performance
dramatically changes along with the quality of the
structured annotation, we conducted a re-ranking
experiment with perfect structured annotations (F-
Measure equal to 1.0). Perfect structured
annotations mean the annotations created by
annotators as introduced in Section 5. The results
are shown in the last bar of Figure 3(a). We did not
find a large space for ranking improvement. The
NDCG@3 when using perfect structured
annotations was 0.606, which is just slightly better
than our best result (yield when 6=0.05). It
indicates that our structured annotation generation
algorithm is already quite effective.
(4) Figure 3(a) shows that our approach
outperforms the two baseline approaches with most
settings for 6. This indicates that our approach is
relatively stable with different settings for 6.
</bodyText>
<subsectionHeader confidence="0.8266785">
6.3 Experiment with Number of Top Search
Results
</subsectionHeader>
<bodyText confidence="0.9999741875">
The above experiments are conducted based on the
top 10 search results. In this section, by adjusting
the number of top search results, ranging from 2 to
100, we investigate whether the quality of
structured annotation of queries and the
performance of re-ranking are affected by the
quantity of search results. The results shown in
Figure 4 indicate that the number of search results
does affect the quality of structured annotation of
queries and the performance of re-ranking.
Structured annotations of queries become better
when more search results are used from 2 to 20.
This is because more search results cover more
websites in our domain list, and hence can generate
more annotated tokens. More results also provide
more evidence for voting the importance of
</bodyText>
<page confidence="0.997322">
476
</page>
<bodyText confidence="0.954478285714286">
annotated tokens, and hence can improve the
quality of structured annotation of queries.
In addition, we also found that structured
annotation of queries become worse when too
many lower ranked results are used (e.g, using
results ranked lower than 20). This is because the
lower ranked results are less relevant than the
higher ranked results. They may contain more
irrelevant or noisy annotated tokens than higher
ranked documents; and hence using them may
harm the precision of the structured annotations.
Figure 4 also indicates that the quality of ranking
and the accuracy of structured annotations are
correlated.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999927866666667">
In this paper, we studied the problem of improving
web search ranking by incorporating structured
annotation of queries. We proposed a systematic
solution, first to generate structured annotation of
queries based on top search results, and then
launching two structured annotation based re-
ranking models. We performed a large-scale
evaluation over 12,396 queries from a major search
engine. The experiment results show that the F-
Measure of query structured annotation generated
by our approach is as high as 91%. In the same
dataset, our structured annotation based re-ranking
model significantly outperforms the original ranker
– the ranking of a major search engine, with
improvements 5.2%.
</bodyText>
<sectionHeader confidence="0.998359" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.89820125">
This work was supported by National Natural Science
Foundation of China (NSFC) via grant 61273321,
61133012 and the Nation-al 863 Leading Technology
Research Project via grant 2012AA011102.
</bodyText>
<sectionHeader confidence="0.996878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859446428571">
G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards
rich query interpretation: walking back and forth for
mining query templates. In Proc. of WWW &apos;10.
M. Bendersky, W. Bruce Croft and D. A. Smith. Joint
Annotation of Search Queries, In Proc. of ACL-HLT
2011.
M. Bendersky, W. Bruce Croft and D. A. Smith.
Structural Annotation of Search Queries Using
Pseudo-Relevance Feedback, In Proc. Of CIKM 2010.
S. Bergsma and Q. I. Wang. Learning noun phrase
query segmentation. In Proceedings of EMNLP-
CoNLL&apos;07.
M. Bron, K. Balog, and M. de Rijke. Ranking related
entities: components and analyses. In Proc. of
CIKM ’10.
C. Buckley. Automatic query expansion using SMART.
InProc. of TREC-3, pages 69–80, 1995.
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank
usinggradient descent. In Proceedings of ICML &apos;05.
T. Cheng, X. Yan, and K. C.-C. Chang. Supporting
entity search: a large-scale prototype search engine.
In Proc. of SIGMOD ’07.
O. Etzioni, M. Banko, S. Soderland, and D.S. Weld,
(2008). Open Information Extraction from the Web,
Communications of the ACM, 51(12): 68-74.
J. Guo, G. Xu, X. Cheng, and H. Li. Named entity
recognition in query. In Proc. Of SIGIR’ 2009.
K. Jarvelin and J. Kekalainen. Ir evaluation methods for
retrieving highly relevant documents. In SIGIR &apos;00.
J. Lafferty and C. Zhai, Document language models,
query models, and risk minimization for information
retrieval, In Proceedings of SIGIR&apos;01, pages 111-119,
2001.
V. Lavrenko and W. B. Croft. Relevance based
language models. In Proc. of SIGIR, pages 120–127,
2001.
Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised
Query Segmentation Using Clickthrough for
Information Retrieval. In Proc. of SIGIR&apos;11.
X. Li, Y.-Y. Wang, and A. Acero. Extracting structured
information from user queries with semi-supervised
conditional random fields. In Proc. of SIGIR&apos;09.
Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised
Transactional Query Classification Based on
Webpage Form Understanding. In Proc. of CIKM &apos;11.
Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query
type identification based on click-through
information. In LNCS, 2006.
M. Pasca. Asking What No One Has Asked Before:
Using Phrase Similarities To Generate Synthetic
Web Search Queries. In Proc. of CIKM &apos;11.
G. Salton and C. Buckley. Improving retrieval
performance by relevance feedback. Journal of the
American Society for Information Science,
41(4):288-297, 1990.
</reference>
<page confidence="0.980796">
477
</page>
<reference confidence="0.99988021875">
N. Sarkas, S. Paparizos, and P. Tsaparas. Structured
annotations of web queries. In Proc. of SIGMOD&apos;10.
I. Szpektor, A. Gionis, and Y. Maarek. Improving
recommendation for long-tail queries via templates.
In Proc. of WWW &apos;11
B. Tan and F. Peng. Unsupervised query segmentation
using generative language models and wikipedia. In
WWW’08.
T.-L. Wong, W. Lam, and B. Chen. Mining
employment market via text block detection and
adaptive cross-domain information extraction. In
Proc. SIGIR, pages 283–290, 2009.
X. Yu and H. Shi. Query segmentation using
conditional random fields. In Proceedings of KEYS
&apos;09.
C. Zhai and J. Lafferty, Model-based feedback in the
language modeling approach to information
retrieval, In Proceedings of CIKM&apos;01, pages 403-410,
2001.
C. Zhai and J. Lafferty, A study of smoothing methods
for language models applied to ad hoc information
retrieval, In Proceedings of SIGIR&apos;01, pages 334-342,
2001.
Y. Zhai and B. Liu. Structured data extraction from the
Web based on partial tree alignment. IEEE Trans.
Knowl. Data Eng., 18(12):1614−1628, Dec. 2006.
H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu.
Fully automatic wrapper generation for search
engines. In Proceedings of WWW ’05.
S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint
optimization of wrapper generation and template
detection. In Proc. of SIGKDD&apos;07.
</reference>
<page confidence="0.998022">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.247518">
<title confidence="0.9895495">Improving Web Search Ranking by Incorporating Annotation of Queries*</title>
<author confidence="0.98278">Zhicheng Bing Ting Ji-Rong</author>
<affiliation confidence="0.81753575">Center for Social Computing and Information Harbin Institute of Technology, China Research Asia, Beijing 100190, University of China, Beijing, China</affiliation>
<email confidence="0.650839">qinb,tliu}@ir.hit.edu.cn;</email>
<abstract confidence="0.997602565217391">Driving the web search evolution are the user needs. Users usually have a template in mind when formulating queries to search for information. Agarwal et al., (2010) surveyed a search log of 15 million queries from a commercial search engine. They found that 90% of queries follow certain templates. For example, by issuing the query swift lyrics falling in the users are seeking for the lyrics of the “Mary&apos;s Song (oh my my my)” by artist Taylor Swift. The “falling in love” actually part of the lyrics they are searching for. However, some top search results are irrelevant to the query, although they contain all the query terms. For example, the first top search result shown in Figure 1(a) does not contain the required lyrics. It just contains the lyrics of another song of Taylor Swift, rather than the song that users are seeking. A possible way to solve the above ranking problem is to understand the underlying query structure. For example, after recognizing that swift” an name “falling in love” part of the we can improve the ranking by comparing the structured query with the corresponding structured data in documents (shown in Figure 1(b)). Some previous studies investigated how to extract structured information from user queries, such as query segmentation (Bergsma and Wang, 2007). The task of query segmentation is to separate the query words into Abstract Web users are increasingly looking for structured data, such as lyrics, job, or recipes, using unstructured queries on the web. However, retrieving relevant results from such data is a challenging problem due to the unstructured language of the web queries. In this paper, we propose a method to improve search ranking by detecting queries based on top search results. In a structured annotation, the original query is split into different units that are associated with semantic attributes in the corresponding domain. We evaluate our techniques using real world queries and achieve significant improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>G Agarwal</author>
<author>G Kabra</author>
<author>K C-C Chang</author>
</authors>
<title>Towards rich query interpretation: walking back and forth for mining query templates.</title>
<booktitle>In Proc. of WWW &apos;10.</booktitle>
<marker>Agarwal, Kabra, Chang, </marker>
<rawString>G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards rich query interpretation: walking back and forth for mining query templates. In Proc. of WWW &apos;10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bendersky</author>
<author>W Bruce Croft</author>
<author>D A Smith</author>
</authors>
<title>Joint Annotation of Search Queries,</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT</booktitle>
<contexts>
<context position="8325" citStr="Bendersky et al., (2011)" startWordPosition="1301" endWordPosition="1304">ries.” Comparing with previous work, the advantages of our approach are on the following aspects. First, we generate structured annotation of queries based on top search results, not some global knowledge base or query logs. Second, they mainly focus on the method of generating structured annotation of queries, rather than leverage the generated query structures to improve web search rankings. In this paper, we not only offer a novel solution for generating structured annotation of queries, but also propose a re-ranking approach to improve Web search based on structured annotation of queries. Bendersky et al., (2011) also used top search results to generate structured annotation of queries. However, the annotations in their definition are capitalization, POS tags, and segmentation indicators, which are different from ours. 2.2 Query Template Generation The concept of query template has been discussed in a few recent papers (Agarwal et al., 2010; Pasca 2011; Liu et al., 2011; Szpektor et al., 2011). A query template is a sequence of terms, where each term could be a word or an attribute. For example, &lt;#artist_name lyrics #lyrics&gt; is a query template, “#artist_name” and “#lyrics” are attributes, and “lyrics</context>
</contexts>
<marker>Bendersky, Croft, Smith, 2011</marker>
<rawString>M. Bendersky, W. Bruce Croft and D. A. Smith. Joint Annotation of Search Queries, In Proc. of ACL-HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bendersky</author>
<author>W Bruce Croft</author>
<author>D A Smith</author>
</authors>
<title>Structural Annotation of Search Queries Using Pseudo-Relevance Feedback,</title>
<date>2010</date>
<booktitle>In Proc. Of CIKM</booktitle>
<contexts>
<context position="7231" citStr="Bendersky et al., 2010" startWordPosition="1128" endWordPosition="1131">hod for generating structured annotation of queries. We then propose two novel re-ranking models based on structured annotation in Section 4. Section 5 introduces the data used in this paper. We report experimental results in Section 6. Finally we conclude the work in Section 7. 469 2 Related Work There is a great deal of prior research that identifies query structured information. We summarize this research according to their different approaches. 2.1 Structured Annotation of Queries Recently, a lot of work has been done on understanding query structure (Sarkas et al., 2010; Li et al., 2009; Bendersky et al., 2010). One important method is structured annotation of queries which aims to detect the structure of the query and assign a specific label to it. Li et al., (2009) proposed web query tagging and its goal is to assign to each query term a specified category, roughly corresponding to a list of attributes. A semi-supervised Conditional Random Field (CRF) is used to capture dependencies between query words and to identify the most likely joint assignment of words to “categories.” Comparing with previous work, the advantages of our approach are on the following aspects. First, we generate structured an</context>
<context position="14806" citStr="Bendersky et al., 2010" startWordPosition="2374" endWordPosition="2377">with [si,...,sj, tmax.a ] 8: remove tmax from T 9: n ← n – (j - i) 10: go to step 2 11: else 12: return p 13: end if information. Annotated tokens and free tokens together cover all query terms, reflecting the complete user intent of the query. 3.2 Generating Structured Annotation In this paper, given a domain schema A, we generate structured annotation for a query q based on the top search results of q. We propose using top search results, rather than some global knowledge base or query logs, because: (1) Top search results have been proven to be a successful technique for query explanation (Bendersky et al., 2010). (2) We have observed that in most cases, a reasonable percentage of the top search results are relevant to the query. By aggregating structured information from the top search results, we can get more query-dependent annotated tokens than using global data sources which may contain more noise and outdated. (3) Our goal for generating structured annotation is to improve the ranking quality of queries. Using top search results enables simultaneous and consistent detection of structured information from documents and queries. As mentioned in Section 3.1, we generate structured annotation of que</context>
</contexts>
<marker>Bendersky, Croft, Smith, 2010</marker>
<rawString>M. Bendersky, W. Bruce Croft and D. A. Smith. Structural Annotation of Search Queries Using Pseudo-Relevance Feedback, In Proc. Of CIKM 2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Bergsma</author>
<author>Q I Wang</author>
</authors>
<title>Learning noun phrase query segmentation.</title>
<booktitle>In Proceedings of EMNLPCoNLL&apos;07.</booktitle>
<marker>Bergsma, Wang, </marker>
<rawString>S. Bergsma and Q. I. Wang. Learning noun phrase query segmentation. In Proceedings of EMNLPCoNLL&apos;07.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Bron</author>
<author>K Balog</author>
<author>M de Rijke</author>
</authors>
<title>Ranking related entities: components and analyses.</title>
<booktitle>In Proc. of CIKM ’10.</booktitle>
<marker>Bron, Balog, de Rijke, </marker>
<rawString>M. Bron, K. Balog, and M. de Rijke. Ranking related entities: components and analyses. In Proc. of CIKM ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
</authors>
<title>Automatic query expansion using SMART.</title>
<date>1995</date>
<booktitle>InProc. of TREC-3,</booktitle>
<pages>69--80</pages>
<marker>Buckley, 1995</marker>
<rawString>C. Buckley. Automatic query expansion using SMART. InProc. of TREC-3, pages 69–80, 1995.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Burges</author>
<author>T Shaked</author>
<author>E Renshaw</author>
<author>A Lazier</author>
<author>M Deeds</author>
<author>N Hamilton</author>
<author>G Hullender</author>
</authors>
<title>Learning to rank usinggradient descent.</title>
<booktitle>In Proceedings of ICML &apos;05.</booktitle>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, </marker>
<rawString>C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank usinggradient descent. In Proceedings of ICML &apos;05.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Cheng</author>
<author>X Yan</author>
<author>K C-C Chang</author>
</authors>
<title>Supporting entity search: a large-scale prototype search engine.</title>
<booktitle>In Proc. of SIGMOD ’07.</booktitle>
<marker>Cheng, Yan, Chang, </marker>
<rawString>T. Cheng, X. Yan, and K. C.-C. Chang. Supporting entity search: a large-scale prototype search engine. In Proc. of SIGMOD ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Banko</author>
<author>S Soderland</author>
<author>D S Weld</author>
</authors>
<title>Open Information Extraction from the Web,</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<pages>68--74</pages>
<contexts>
<context position="2815" citStr="Etzioni et al., 2008" startWordPosition="428" endWordPosition="431">detecting Structured Annotation of queries based on top search results. In a structured annotation, the original query is split into different units that are associated with semantic attributes in the corresponding domain. We evaluate our techniques using real world queries and achieve significant improvement. 1 Introduction Search engines are getting more sophisticated by utilizing information from multiple diverse sources. One such valuable source of information is structured and semi-structured data, which is not very difficult to access, owing to information extraction (Wong et al., 2009; Etzioni et al., 2008; Zhai and Liu 2006) and semantic web efforts. *Work was done when the first author was visiting Microsoft Research Asia 468 Query: taylor swift lyrics falling in love Search Results (a) Annotated Tokens (b) d1 [Taylor Swift, #artist_name] [Crazier, #song_name] [Feel like I’m falling and ..., #lyrics] d2 [Taylor Swift, #artist_name] [Mary’s Song (oh my my my), #song_name] [Growing up and falling in love..., #lyrics] d3 [Taylor Swift, #artist_name] [Jump Then Fall, #song_name] [I realize your love is the best ..., #lyrics] d4 [Taylor Swift, #artist_name] [Mary’s Song (oh my my my), #song_name] </context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>O. Etzioni, M. Banko, S. Soderland, and D.S. Weld, (2008). Open Information Extraction from the Web, Communications of the ACM, 51(12): 68-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Guo</author>
<author>G Xu</author>
<author>X Cheng</author>
<author>H Li</author>
</authors>
<title>Named entity recognition in query.</title>
<date>2009</date>
<booktitle>In Proc. Of SIGIR’</booktitle>
<contexts>
<context position="10388" citStr="Guo et al., 2009" startWordPosition="1616" endWordPosition="1619">uery words into disjointed segments so that each segment maps to a semantic unit (Li et al., 2011). Query segmentation techniques have been well studied in recent literature (Tan and Peng, 2008; Yu and Shi, 2009). However, structured annotation of queries cannot only separate the query words into disjoint segments but can also assign each segment a semantic label which can help the search engine to judge whether each part of query segmentation matches the right field of the documents or not. 2.4 Entity Search The problem of entity search has received a great deal of attention in recent years (Guo et al., 2009; Bron et al., 2010; Cheng et al., 2007). Its goal is to answer information needs that focus on entities. The problem of structured annotation of queries is related to entity search because for some queries, structured annotation items are entities or attributes. Some existing entity search approaches also exploit knowledge from the structure of webpages (Zhao et al., 2005). Annotating query structured information differs from entity search in the following aspects. First, structured annotation based ranking is applicable for all queries, rather than just entity related queries. Second, the re</context>
</contexts>
<marker>Guo, Xu, Cheng, Li, 2009</marker>
<rawString>J. Guo, G. Xu, X. Cheng, and H. Li. Named entity recognition in query. In Proc. Of SIGIR’ 2009. K. Jarvelin and J. Kekalainen. Ir evaluation methods for retrieving highly relevant documents. In SIGIR &apos;00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval,</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR&apos;01,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="22392" citStr="Lafferty and Zhai, (2001)" startWordPosition="3633" endWordPosition="3636">vely, into some comparable representations. For example, they can be represented as vectors of weighted terms or language models. In this paper, we explore the problem in the language model framework, particularly the KLdivergence retrieval model and mixture-model feedback method (Zhai and Lafferty, 2001), mainly because language models deliver state-of-the-art retrieval performance and the mixture-model based feedback is one of the most effective feedback techniques which outperforms Rocchio feedback. 4.2.1 The KL-Divergence Retrieval Model The KL-divergence retrieval model was introduced in Lafferty and Zhai, (2001) as a special case of the risk minimization retrieval framework and can support feedback more naturally. In this model, queries and documents are represented by unigram language models. Assuming that these language models can be appropriately estimated, KLdivergence retrieval model measures the relevance value of a document D with respect to a query Q by computing the negative Kullback-Leibler divergence between the query language model 𝜃𝑄 and the document language model 𝜃𝐷 as follows: 𝑆(𝑄, 𝐷) = −𝐷(𝜃𝑄 ||𝜃𝐷) = − ∑ 𝑝(𝑤|𝜃𝑄)𝑙𝑜𝑔 𝑝(𝑤|𝜃𝑄) (5) 𝑤∈𝑉 𝑝(𝑤|𝜃𝐷) where V is the set of words in our vocabulary.</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>J. Lafferty and C. Zhai, Document language models, query models, and risk minimization for information retrieval, In Proceedings of SIGIR&apos;01, pages 111-119, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>120--127</pages>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>V. Lavrenko and W. B. Croft. Relevance based language models. In Proc. of SIGIR, pages 120–127, 2001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>CX Zhai Hsu</author>
<author>K Wang</author>
</authors>
<title>Unsupervised Query Segmentation Using Clickthrough for Information Retrieval.</title>
<booktitle>In Proc. of SIGIR&apos;11.</booktitle>
<marker>Hsu, Wang, </marker>
<rawString>Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised Query Segmentation Using Clickthrough for Information Retrieval. In Proc. of SIGIR&apos;11.</rawString>
</citation>
<citation valid="false">
<authors>
<author>X Li</author>
<author>Y-Y Wang</author>
<author>A Acero</author>
</authors>
<title>Extracting structured information from user queries with semi-supervised conditional random fields.</title>
<booktitle>In Proc. of SIGIR&apos;09.</booktitle>
<marker>Li, Wang, Acero, </marker>
<rawString>X. Li, Y.-Y. Wang, and A. Acero. Extracting structured information from user queries with semi-supervised conditional random fields. In Proc. of SIGIR&apos;09.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Liu</author>
<author>X Ni</author>
<author>J-T Sun</author>
<author>Z Chen</author>
</authors>
<title>Unsupervised Transactional Query Classification Based on Webpage Form Understanding.</title>
<booktitle>In Proc. of CIKM &apos;11.</booktitle>
<marker>Liu, Ni, Sun, Chen, </marker>
<rawString>Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised Transactional Query Classification Based on Webpage Form Understanding. In Proc. of CIKM &apos;11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>M Zhang</author>
<author>L Ru</author>
<author>S Ma</author>
</authors>
<title>Automatic query type identification based on click-through information.</title>
<date>2006</date>
<booktitle>In LNCS,</booktitle>
<marker>Liu, Zhang, Ru, Ma, 2006</marker>
<rawString>Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query type identification based on click-through information. In LNCS, 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Pasca</author>
</authors>
<title>Asking What No One Has Asked Before: Using Phrase Similarities To Generate Synthetic Web Search Queries.</title>
<booktitle>In Proc. of CIKM &apos;11.</booktitle>
<marker>Pasca, </marker>
<rawString>M. Pasca. Asking What No One Has Asked Before: Using Phrase Similarities To Generate Synthetic Web Search Queries. In Proc. of CIKM &apos;11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Improving retrieval performance by relevance feedback.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--4</pages>
<marker>Salton, Buckley, 1990</marker>
<rawString>G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41(4):288-297, 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Sarkas</author>
<author>S Paparizos</author>
<author>P Tsaparas</author>
</authors>
<title>Structured annotations of web queries.</title>
<booktitle>In Proc. of SIGMOD&apos;10.</booktitle>
<marker>Sarkas, Paparizos, Tsaparas, </marker>
<rawString>N. Sarkas, S. Paparizos, and P. Tsaparas. Structured annotations of web queries. In Proc. of SIGMOD&apos;10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Szpektor</author>
<author>A Gionis</author>
<author>Y Maarek</author>
</authors>
<title>Improving recommendation for long-tail queries via templates. In</title>
<booktitle>Proc. of WWW &apos;11</booktitle>
<marker>Szpektor, Gionis, Maarek, </marker>
<rawString>I. Szpektor, A. Gionis, and Y. Maarek. Improving recommendation for long-tail queries via templates. In Proc. of WWW &apos;11</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Tan</author>
<author>F Peng</author>
</authors>
<title>Unsupervised query segmentation using generative language models and wikipedia.</title>
<booktitle>In WWW’08.</booktitle>
<marker>Tan, Peng, </marker>
<rawString>B. Tan and F. Peng. Unsupervised query segmentation using generative language models and wikipedia. In WWW’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-L Wong</author>
<author>W Lam</author>
<author>B Chen</author>
</authors>
<title>Mining employment market via text block detection and adaptive cross-domain information extraction.</title>
<date>2009</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="2793" citStr="Wong et al., 2009" startWordPosition="424" endWordPosition="427"> search ranking by detecting Structured Annotation of queries based on top search results. In a structured annotation, the original query is split into different units that are associated with semantic attributes in the corresponding domain. We evaluate our techniques using real world queries and achieve significant improvement. 1 Introduction Search engines are getting more sophisticated by utilizing information from multiple diverse sources. One such valuable source of information is structured and semi-structured data, which is not very difficult to access, owing to information extraction (Wong et al., 2009; Etzioni et al., 2008; Zhai and Liu 2006) and semantic web efforts. *Work was done when the first author was visiting Microsoft Research Asia 468 Query: taylor swift lyrics falling in love Search Results (a) Annotated Tokens (b) d1 [Taylor Swift, #artist_name] [Crazier, #song_name] [Feel like I’m falling and ..., #lyrics] d2 [Taylor Swift, #artist_name] [Mary’s Song (oh my my my), #song_name] [Growing up and falling in love..., #lyrics] d3 [Taylor Swift, #artist_name] [Jump Then Fall, #song_name] [I realize your love is the best ..., #lyrics] d4 [Taylor Swift, #artist_name] [Mary’s Song (oh m</context>
</contexts>
<marker>Wong, Lam, Chen, 2009</marker>
<rawString>T.-L. Wong, W. Lam, and B. Chen. Mining employment market via text block detection and adaptive cross-domain information extraction. In Proc. SIGIR, pages 283–290, 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>X Yu</author>
<author>H Shi</author>
</authors>
<title>Query segmentation using conditional random fields.</title>
<booktitle>In Proceedings of KEYS &apos;09.</booktitle>
<marker>Yu, Shi, </marker>
<rawString>X. Yu and H. Shi. Query segmentation using conditional random fields. In Proceedings of KEYS &apos;09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>Model-based feedback in the language modeling approach to information retrieval,</title>
<date>2001</date>
<booktitle>In Proceedings of CIKM&apos;01,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="22073" citStr="Zhai and Lafferty, 2001" startWordPosition="3592" endWordPosition="3595">ively re-rank the other r search results: U ⊆ {dk+1,..., dk+r}. A general formula of relevance feedback model (Salton et al, 1990) R is as follows: 𝑅 (𝑄′) = (1 − α)𝐿𝑞 (Q) + α𝐿𝑑 (J) (4) where α ∈ [0, 1] is the feedback coefficient, and 𝐿𝑞 and 𝐿𝑑 are two models that map a query and a set of relevant documents, respectively, into some comparable representations. For example, they can be represented as vectors of weighted terms or language models. In this paper, we explore the problem in the language model framework, particularly the KLdivergence retrieval model and mixture-model feedback method (Zhai and Lafferty, 2001), mainly because language models deliver state-of-the-art retrieval performance and the mixture-model based feedback is one of the most effective feedback techniques which outperforms Rocchio feedback. 4.2.1 The KL-Divergence Retrieval Model The KL-divergence retrieval model was introduced in Lafferty and Zhai, (2001) as a special case of the risk minimization retrieval framework and can support feedback more naturally. In this model, queries and documents are represented by unigram language models. Assuming that these language models can be appropriately estimated, KLdivergence retrieval mode</context>
<context position="23994" citStr="Zhai and Lafferty, (2001)" startWordPosition="3909" endWordPosition="3912">cument model 𝜃𝐷 needs to be smoothed and an effective method is Dirichlet smoothing (Zhai et al., 2001). The query model intuitively captures what the user is interested in, and thus would affect retrieval performance. With feedback documents, 𝜃𝑄 is estimated by the mixture-model feedback method. 4.2.2 The Mixture Model Feedback Method As the problem definition in Equation (4), the query model can be estimated by the original query model 𝑝(𝑤|𝜃𝑄) = 𝑐(𝑤,𝑄) |𝑄 |(where c(w,Q) is the count of word w in the query Q, and |Q |is the total number of words in the query) and the feedback document model. Zhai and Lafferty, (2001) proposed a mixture model feedback method to estimate the feedback document model. More specifically, the model assumes that the feedback documents can be generated by a background language model 𝑝 (𝑤 |𝐶) estimated using the whole collection and an unknown topic language model 473 Table 2. Domain queries used in our experiment Domain Containing Queries Structured Keyword Annotation% lyrics “lyrics” 196 95% job “job” 124 92% recipe “recipe” 76 93% Table 3. Quality of Structured Annotation. All the improvements are significant (p &lt; 0.05) Domain Method Precision Recall F-Measure lyrics Baseline 9</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty, Model-based feedback in the language modeling approach to information retrieval, In Proceedings of CIKM&apos;01, pages 403-410, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval,</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR&apos;01,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="22073" citStr="Zhai and Lafferty, 2001" startWordPosition="3592" endWordPosition="3595">ively re-rank the other r search results: U ⊆ {dk+1,..., dk+r}. A general formula of relevance feedback model (Salton et al, 1990) R is as follows: 𝑅 (𝑄′) = (1 − α)𝐿𝑞 (Q) + α𝐿𝑑 (J) (4) where α ∈ [0, 1] is the feedback coefficient, and 𝐿𝑞 and 𝐿𝑑 are two models that map a query and a set of relevant documents, respectively, into some comparable representations. For example, they can be represented as vectors of weighted terms or language models. In this paper, we explore the problem in the language model framework, particularly the KLdivergence retrieval model and mixture-model feedback method (Zhai and Lafferty, 2001), mainly because language models deliver state-of-the-art retrieval performance and the mixture-model based feedback is one of the most effective feedback techniques which outperforms Rocchio feedback. 4.2.1 The KL-Divergence Retrieval Model The KL-divergence retrieval model was introduced in Lafferty and Zhai, (2001) as a special case of the risk minimization retrieval framework and can support feedback more naturally. In this model, queries and documents are represented by unigram language models. Assuming that these language models can be appropriately estimated, KLdivergence retrieval mode</context>
<context position="23994" citStr="Zhai and Lafferty, (2001)" startWordPosition="3909" endWordPosition="3912">cument model 𝜃𝐷 needs to be smoothed and an effective method is Dirichlet smoothing (Zhai et al., 2001). The query model intuitively captures what the user is interested in, and thus would affect retrieval performance. With feedback documents, 𝜃𝑄 is estimated by the mixture-model feedback method. 4.2.2 The Mixture Model Feedback Method As the problem definition in Equation (4), the query model can be estimated by the original query model 𝑝(𝑤|𝜃𝑄) = 𝑐(𝑤,𝑄) |𝑄 |(where c(w,Q) is the count of word w in the query Q, and |Q |is the total number of words in the query) and the feedback document model. Zhai and Lafferty, (2001) proposed a mixture model feedback method to estimate the feedback document model. More specifically, the model assumes that the feedback documents can be generated by a background language model 𝑝 (𝑤 |𝐶) estimated using the whole collection and an unknown topic language model 473 Table 2. Domain queries used in our experiment Domain Containing Queries Structured Keyword Annotation% lyrics “lyrics” 196 95% job “job” 124 92% recipe “recipe” 76 93% Table 3. Quality of Structured Annotation. All the improvements are significant (p &lt; 0.05) Domain Method Precision Recall F-Measure lyrics Baseline 9</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty, A study of smoothing methods for language models applied to ad hoc information retrieval, In Proceedings of SIGIR&apos;01, pages 334-342, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhai</author>
<author>B Liu</author>
</authors>
<title>Structured data extraction from the Web based on partial tree alignment.</title>
<date>2006</date>
<journal>IEEE Trans. Knowl. Data Eng.,</journal>
<volume>18</volume>
<issue>12</issue>
<contexts>
<context position="2835" citStr="Zhai and Liu 2006" startWordPosition="432" endWordPosition="435">nnotation of queries based on top search results. In a structured annotation, the original query is split into different units that are associated with semantic attributes in the corresponding domain. We evaluate our techniques using real world queries and achieve significant improvement. 1 Introduction Search engines are getting more sophisticated by utilizing information from multiple diverse sources. One such valuable source of information is structured and semi-structured data, which is not very difficult to access, owing to information extraction (Wong et al., 2009; Etzioni et al., 2008; Zhai and Liu 2006) and semantic web efforts. *Work was done when the first author was visiting Microsoft Research Asia 468 Query: taylor swift lyrics falling in love Search Results (a) Annotated Tokens (b) d1 [Taylor Swift, #artist_name] [Crazier, #song_name] [Feel like I’m falling and ..., #lyrics] d2 [Taylor Swift, #artist_name] [Mary’s Song (oh my my my), #song_name] [Growing up and falling in love..., #lyrics] d3 [Taylor Swift, #artist_name] [Jump Then Fall, #song_name] [I realize your love is the best ..., #lyrics] d4 [Taylor Swift, #artist_name] [Mary’s Song (oh my my my), #song_name] [Growing up and fall</context>
</contexts>
<marker>Zhai, Liu, 2006</marker>
<rawString>Y. Zhai and B. Liu. Structured data extraction from the Web based on partial tree alignment. IEEE Trans. Knowl. Data Eng., 18(12):1614−1628, Dec. 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Zhao</author>
<author>W Meng</author>
<author>Z Wu</author>
<author>V Raghavan</author>
<author>C Yu</author>
</authors>
<title>Fully automatic wrapper generation for search engines.</title>
<booktitle>In Proceedings of WWW ’05.</booktitle>
<marker>Zhao, Meng, Wu, Raghavan, Yu, </marker>
<rawString>H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. Fully automatic wrapper generation for search engines. In Proceedings of WWW ’05.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Zheng</author>
<author>R Song</author>
<author>J-R Wen</author>
<author>D Wu</author>
</authors>
<title>Joint optimization of wrapper generation and template detection.</title>
<booktitle>In Proc. of SIGKDD&apos;07.</booktitle>
<marker>Zheng, Song, Wen, Wu, </marker>
<rawString>S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint optimization of wrapper generation and template detection. In Proc. of SIGKDD&apos;07.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
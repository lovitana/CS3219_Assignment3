<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003480">
<title confidence="0.9982845">
Decoding with Large-Scale Neural Language Models
Improves Translation
</title>
<author confidence="0.991204">
Ashish Vaswani Yinggong Zhao
</author>
<affiliation confidence="0.999803">
University of Southern California Nanjing University, State Key Laboratory
Department of Computer Science for Novel Software Technology
</affiliation>
<email confidence="0.998186">
avaswani@isi.edu zhaoyg@nlp.nju.edu.cn
</email>
<author confidence="0.998102">
Victoria Fossum and David Chiang
</author>
<affiliation confidence="0.9843155">
University of Southern California
Information Sciences Institute
</affiliation>
<email confidence="0.999287">
{vfossum,chiang}@isi.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989475">
We explore the application of neural language
models to machine translation. We develop a
new model that combines the neural proba-
bilistic language model of Bengio et al., rec-
tified linear units, and noise-contrastive esti-
mation, and we incorporate it into a machine
translation system both by reranking k-best
lists and by direct integration into the decoder.
Our large-scale, large-vocabulary experiments
across four language pairs show that our neu-
ral language model improves translation qual-
ity by up to 1.1 B .
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99886898">
Machine translation (MT) systems rely upon lan-
guage models (LMs) during decoding to ensure flu-
ent output in the target language. Typically, these
LMs are n-gram models over discrete representa-
tions of words. Such models are susceptible to data
sparsity–that is, the probability of an n-gram ob-
served only few times is difficult to estimate reli-
ably, because these models do not use any informa-
tion about similarities between words.
To address this issue, Bengio et al. (2003) pro-
pose distributed word representations, in which each
word is represented as a real-valued vector in a
high-dimensional feature space. Bengio et al. (2003)
introduce a feed-forward neural probabilistic LM
(NPLM) that operates over these distributed repre-
sentations. During training, the NPLM learns both a
distributed representation for each word in the vo-
cabulary and an n-gram probability distribution over
words in terms of these distributed representations.
Although neural LMs have begun to rival or even
surpass traditional n-gram LMs (Mnih and Hin-
ton, 2009; Mikolov et al., 2011), they have not yet
been widely adopted in large-vocabulary applica-
tions such as MT, because standard maximum like-
lihood estimation (MLE) requires repeated summa-
tions over all words in the vocabulary. A variety of
strategies have been proposed to combat this issue,
many of which require severe restrictions on the size
of the network or the size of the data.
In this work, we extend the NPLM of Bengio et
al. (2003) in two ways. First, we use rectified lin-
ear units (Nair and Hinton, 2010), whose activa-
tions are cheaper to compute than sigmoid or tanh
units. There is also evidence that deep neural net-
works with rectified linear units can be trained suc-
cessfully without pre-training (Zeiler et al., 2013).
Second, we train using noise-contrastive estimation
or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and
Teh, 2012), which does not require repeated summa-
tions over the whole vocabulary. This enables us to
efficiently build NPLMs on a larger scale than would
be possible otherwise.
We then apply this LM to MT in two ways. First,
we use it to rerank the k-best output of a hierarchi-
cal phrase-based decoder (Chiang, 2007). Second,
we integrate it directly into the decoder, allowing the
neural LM to more strongly influence the model. We
achieve gains of up to 0.6 B translating French,
German, and Spanish to English, and up to 1.1 B
on Chinese-English translation.
</bodyText>
<page confidence="0.953654">
1387
</page>
<note confidence="0.461012">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.996556">
Figure 1: Neural probabilistic language model (Bengio et
al., 2003).
</figureCaption>
<sectionHeader confidence="0.973884" genericHeader="method">
2 Neural Language Models
</sectionHeader>
<bodyText confidence="0.999987285714286">
Let V be the vocabulary, and n be the order of
the language model; let u range over contexts, i.e.,
strings of length (n−1), and w range over words. For
simplicity, we assume that the training data is a sin-
gle very long string, w1 · · · wN, where wN is a special
stop symbol, &lt;/s&gt;. We write ui for wi−n+1 · · · wi−1,
where, for i ≤ 0, wi is a special start symbol, &lt;s&gt;.
</bodyText>
<subsectionHeader confidence="0.99243">
2.1 Model
</subsectionHeader>
<bodyText confidence="0.999565384615385">
We use a feedforward neural network as shown in
Figure 1, following Bengio et al. (2003). The input
to the network is a sequence of one-hot represen-
tations of the words in context u, which we write
uj (1 ≤ j ≤ n − 1). The output is the probability
P(w  |u) for each word w, which the network com-
putes as follows.
The hidden layers consist of rec-
tified linear units (Nair and Hinton,
2010), which use the activation func-
tion O(x) = max(0, x) (see graph at
right).
The output of the first hidden layer h1 is
</bodyText>
<equation confidence="0.959018">
��������
CjDuj (1)
�
</equation>
<bodyText confidence="0.999563">
where D is a matrix of input word embeddings
which is shared across all positions, the Cj are the
context matrices for each word in u, and 0 is applied
elementwise. The output of the second layer h2 is
</bodyText>
<equation confidence="0.991556">
h2 = 0 (Mh1) ,
</equation>
<bodyText confidence="0.984173666666667">
where M is the matrix of connection weights be-
tween h1 and h2. Finally, the output layer is a soft-
max layer,
</bodyText>
<equation confidence="0.998371">
P(w  |u) ∝ exp (D0h2 + b) (2)
</equation>
<bodyText confidence="0.998465">
where D0 is the output word embedding matrix and b
is a vector of biases for every word in the vocabulary.
</bodyText>
<subsectionHeader confidence="0.998709">
2.2 training
</subsectionHeader>
<bodyText confidence="0.999949857142857">
The typical way to train neural LMs is to maximize
the likelihood of the training data by gradient ascent.
But the softmax layer requires, at each iteration, a
summation over all the units in the output layer, that
is, all words in the whole vocabulary. If the vocabu-
lary is large, this can be prohibitively expensive.
Noise-contrastive estimation or NCE (Gutmann
and Hyv¨arinen, 2010) is an alternative estimation
principle that allows one to avoid these repeated
summations. It has been applied previously to log-
bilinear LMs (Mnih and Teh, 2012), and we apply it
here to the NPLM described above.
We can write the probability of a word w given a
context u under the NPLM as
</bodyText>
<equation confidence="0.9967396">
1
P(w  |u) = Z(u) p(w  |u)
p(w  |u) = exp (D0h2 + b)
Z(u) = Z p(w0  |u) (3)
w0
</equation>
<bodyText confidence="0.99993">
where p(w  |u) is the unnormalized output of the unit
corresponding to w, and Z(u) is the normalization
factor. Let 0 stand for the parameters of the model.
One possibility would be to treat Z(u), instead of
being defined by (3), as an additional set of model
parameters which are learned along with 0. But it is
easy to see that we can make the likelihood arbitrar-
ily large by making the Z(u) arbitrarily small.
In NCE, we create a noise distribution q(w).
For each example uiwi, we add k noise samples
¯wi1, ... ,¯wik into the data, and extend the model to
account for noise samples by introducing a random
</bodyText>
<figure confidence="0.8180447">
output
P(w  |u)
hidden
h2
hidden
h1
input
embeddings
D0
M
C1 C2
D
input
words
u1 u2
l
h1 = 0
n− 1
Z
j=1
</figure>
<page confidence="0.745143">
1388
</page>
<bodyText confidence="0.6448765">
variable C which is 1 for training examples and 0 for
noise samples:
</bodyText>
<equation confidence="0.99500725">
1 1
P(C = 1,w  |u) = 1 + k · Z(u) p(w  |u)
k
P(C = 0, w  |u) = 1 + k · q(w).
</equation>
<bodyText confidence="0.999509">
We then train the model to classify examples as
training data or noise, that is, to maximize the con-
ditional likelihood,
</bodyText>
<equation confidence="0.948062">
log P(C = 1  |uiwi) +
!log P(C = 0  |ui ¯wij)
</equation>
<bodyText confidence="0.993046333333333">
with respect to both 0 and Z(u).
We do this by stochastic gradient ascent. The gra-
dient with respect to 0 turns out to be
</bodyText>
<equation confidence="0.990691">
P(C = 0  |uiwi) aa0 log p(wi  |ui) −
!P(C = 1 |ui ¯wij) a a0logp( ¯wij  |ui)
</equation>
<bodyText confidence="0.999976166666667">
and similarly for the gradient with respect to Z(u).
These can be computed by backpropagation. Unlike
before, the Z(u) will converge to a value that normal-
izes the model, satisfying (3), and, under appropriate
conditions, the parameters will converge to a value
that maximizes the likelihood of the data.
</bodyText>
<sectionHeader confidence="0.996147" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999488">
Both training and scoring of neural LMs are compu-
tationally expensive at the scale needed for machine
translation. In this section, we describe some of the
techniques used to make them practical for transla-
tion.
</bodyText>
<subsectionHeader confidence="0.995838">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999990944444445">
During training, we compute gradients on an en-
tire minibatch at a time, allowing the use of matrix-
matrix multiplications instead of matrix-vector mul-
tiplications (Bengio, 2012). We represent the inputs
as a sparse matrix, allowing the computation of the
input layer (1) to use sparse matrix-matrix multi-
plications. The output activations (2) are computed
only for the word types that occur as the positive ex-
ample or one of the noise samples, yielding a sparse
matrix of outputs. Similarly, during backpropaga-
tion, sparse matrix multiplications are used at both
the output and input layer.
In most of these operations, the examples in a
minibatch can be processed in parallel. However, in
the sparse-dense products used when updating the
parameters D and D0, we found it was best to di-
vide the vocabulary into blocks (16 per thread) and
to process the blocks in parallel.
</bodyText>
<subsectionHeader confidence="0.997426">
3.2 Translation
</subsectionHeader>
<bodyText confidence="0.999866322580645">
To incorporate this neural LM into a MT system, we
can use the LM to rerank k-best lists, as has been
done in previous work. But since the NPLM scores
n-grams, it can also be integrated into a phrase-based
or hierarchical phrase-based decoder just as a con-
ventional n-gram model can, unlike a RNN.
The most time-consuming step in computing n-
gram probabilities is the computation of the nor-
malization constants Z(u). Following Mnih and Teh
(2012), we set all the normalization constants to one
during training, so that the model learns to produce
approximately normalized probabilities. Then, when
applying the LM, we can simply ignore normaliza-
tion. A similar strategy was taken by Niehues and
Waibel (2012). We find that a single n-gram lookup
takes about 40 µs.
The technique, described above, of grouping ex-
amples into minibatches works for scoring of k-best
lists, but not while decoding. But caching n-gram
probabilities helps to reduce the cost of the many
lookups required during decoding.
A final issue when decoding with a neural LM
is that, in order to estimate future costs, we need
to be able to estimate probabilities of n0-grams for
n0 &lt; n. In conventional LMs, this information is
readily available,1 but not in NPLMs. Therefore, we
defined a special word &lt;null&gt; whose embedding is
the weighted average of the (input) embeddings of
all the other words in the vocabulary. Then, to esti-
mate the probability of an n0-gram u0w, we used the
probability of P(w  |&lt;null&gt;n−n0u0).
</bodyText>
<footnote confidence="0.549669">
1However, in Kneser-Ney smoothed LMs, this information
is also incorrect (Heafield et al., 2012).
</footnote>
<equation confidence="0.997413538461538">
XN
i=1
L =
k
X
j=1
aL
XN
i=1
a0 =
k
X
j=1
</equation>
<page confidence="0.981142">
1389
</page>
<table confidence="0.9989465">
setting dev 2004 2005 2006
baseline 38.2 38.4 37.7 34.3
reranking 38.5 38.6 37.8 34.7
decoding 39.1 39.5 38.8 34.9
</table>
<tableCaption confidence="0.9873662">
Table 1: Results for Chinese-English experiments, with-
out neural LM (baseline) and with neural LM for rerank-
ing and integrated decoding. Reranking with the neural
LM improves translation quality, while integrating it into
the decoder improves even more.
</tableCaption>
<sectionHeader confidence="0.997627" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997575">
We ran experiments on four language pairs – Chi-
nese to English and French, German, and Spanish
to English – using a hierarchical phrase-based MT
system (Chiang, 2007) and GIZA++ (Och and Ney,
2003) for word alignments.
For all experiments, we used four LMs. The base-
lines used conventional 5-gram LMs, estimated with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the English side of the bitext and the
329M-word Xinhua portion of English Gigaword
(LDC2011T07). Against these baselines, we tested
systems that included the two conventional LMs as
well as two 5-gram NPLMs trained on the same
datasets. The Europarl bitext NPLMs had a vocab-
ulary size of 50k, while the other NPLMs had a vo-
cabulary size of 100k. We used 150 dimensions for
word embeddings, 750 units in hidden layer h1, and
150 units in hidden layer h2. We initialized the net-
work parameters uniformly from (−0.01, 0.01) and
the output biases to − log |V|, and optimized them by
10 epochs of stochastic gradient ascent, using mini-
batches of size 1000 and a learning rate of 1. We
drew 100 noise samples per training example from
the unigram distribution, using the alias method for
efficiency (Kronmal and Peterson, 1979).
We trained the discriminative models with MERT
(Och, 2003) and the discriminative rerankers on
1000-best lists with MERT. Except where noted, we
ran MERT three times and report the average score.
We evaluated using case-insensitive NIST B .
</bodyText>
<subsectionHeader confidence="0.916624">
4.1 NIST Chinese-English
</subsectionHeader>
<bodyText confidence="0.985211333333333">
For the Chinese-English task (Table 1), the training
data came from the NIST 2012 constrained track,
excluding sentences longer than 60 words. Rules
</bodyText>
<table confidence="0.998668">
Fr-En De-En Es-En
setting dev test dev test dev test
baseline 33.5 25.5 28.8 21.5 33.5 32.0
reranking 33.9 26.0 29.1 21.5 34.1 32.2
decoding 34.12 26.12 29.3 21.9 34.22 32.12
</table>
<tableCaption confidence="0.716549333333333">
Table 2: Results for Europarl MT experiments, without
neural LM (baseline) and with neural LM for reranking
and integrated decoding. The neural LM gives improve-
ments across three different language pairs. Superscript 2
indicates a score averaged between two runs; all other
scores were averaged over three runs.
</tableCaption>
<bodyText confidence="0.998613">
without nonterminals were extracted from all train-
ing data, while rules with nonterminals were ex-
tracted from the FBIS corpus (LDC2003E14). We
ran MERT on the development data, which was the
NIST 2003 test data, and tested on the NIST 2004–
2006 test data.
Reranking using the neural LM yielded improve-
ments of 0.2–0.4 B , while integrating the neural
LM yielded larger improvements, between 0.6 and
</bodyText>
<subsectionHeader confidence="0.959922">
1.1 B .
4.2 Europarl
</subsectionHeader>
<bodyText confidence="0.932758722222222">
For French, German, and Spanish translation, we
used a parallel text of about 50M words from Eu-
roparl v7. Rules without nonterminals were ex-
tracted from all the data, while rules with nonter-
minals were extracted from the first 200k words. We
ran MERT on the development data, which was the
WMT 2005 test data, and tested on the WMT 2006
news commentary test data (nc-test2006).
The improvements, shown in Table 2, were more
modest than on Chinese-English. Reranking with
the neural LM yielded improvements of up to 0.5
B , and integrating the neural LM into the decoder
yielded improvements of up to 0.6 B . In one
case (Spanish-English), integrated decoding scored
higher than reranking on the development data but
lower on the test data – perhaps due to the differ-
ence in domain between the two. On the other tasks,
integrated decoding outperformed reranking.
</bodyText>
<subsectionHeader confidence="0.998851">
4.3 Speed comparison
</subsectionHeader>
<bodyText confidence="0.927643666666667">
We measured the speed of training a NPLM by NCE,
compared with MLE as implemented by the CSLM
toolkit (Schwenk, 2013). We used the first 200k
</bodyText>
<page confidence="0.964219">
1390
</page>
<figure confidence="0.985162">
10 20 30 40 50 60 70
Vocabulary size (x1000)
</figure>
<figureCaption confidence="0.997685">
Figure 2: Noise contrastive estimation (NCE) is much
</figureCaption>
<bodyText confidence="0.949407416666667">
faster, and much less dependent on vocabulary size, than
MLE as implemented by the CSLM toolkit (Schwenk,
2013).
lines (5.2M words) of the Xinhua portion of Giga-
word and timed one epoch of training, for various
values of k and |V|, on a dual hex-core 2.67 GHz
Xeon X5650 machine. For these experiments, we
used minibatches of 128 examples. The timings are
plotted in Figure 2. We see that NCE is considerably
faster than MLE; moreover, as expected, the MLE
training time is roughly linear in |V|, whereas the
NCE training time is basically constant.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99998676">
The problem of training with large vocabularies in
NPLMs has received much attention. One strategy
has been to restructure the network to be more hi-
erarchical (Morin and Bengio, 2005; Mnih and Hin-
ton, 2009) or to group words into classes (Le et al.,
2011). Other strategies include restricting the vocab-
ulary of the NPLM to a shortlist and reverting to a
traditional n-gram LM for other words (Schwenk,
2004), and limiting the number of training examples
using resampling (Schwenk and Gauvain, 2005) or
selecting a subset of the training data (Schwenk et
al., 2012). Our approach can be efficiently applied
to large-scale tasks without limiting either the model
or the data.
NPLMs have previously been applied to MT, most
notably feed-forward NPLMs (Schwenk, 2007;
Schwenk, 2010) and RNN-LMs (Mikolov, 2012).
However, their use in MT has largely been limited
to reranking k-best lists for MT tasks with restricted
vocabularies. Niehues and Waibel (2012) integrate a
RBM-based language model directly into a decoder,
but they only train the RBM LM on a small amount
of data. To our knowledge, our approach is the first
to integrate a large-vocabulary NPLM directly into a
decoder for a large-scale MT task.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999776083333333">
We introduced a new variant of NPLMs that com-
bines the network architecture of Bengio et al.
(2003), rectified linear units (Nair and Hinton,
2010), and noise-contrastive estimation (Gutmann
and Hyv¨arinen, 2010). This model is dramatically
faster to train than previous neural LMs, and can be
trained on a large corpus with a large vocabulary and
directly integrated into the decoder of a MT system.
Our experiments across four language pairs demon-
strated improvements of up to 1.1 B . Code for
training and using our NPLMs is available for down-
load.2
</bodyText>
<sectionHeader confidence="0.994951" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999629111111111">
We would like to thank the anonymous reviewers for
their very helpful comments. This research was sup-
ported in part by DOI IBC grant D12AP00225. This
work was done while the second author was visit-
ing USC/ISI supported by China Scholarship Coun-
cil. He was also supported by the Research Fund for
the Doctoral Program of Higher Education of China
(No. 20110091110003) and the National Fundamen-
tal Research Program of China (2010CB327903).
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.970636875">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research.
Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. CoRR,
abs/1206.5533.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
</reference>
<footnote confidence="0.658635">
2http://nlg.isi.edu/software/nplm
</footnote>
<figure confidence="0.989867333333333">
CSLM
NCE k = 1000
NCE k = 100
NCE k = 10
0 1,000 2,000 3,000 4,000
Training time (s)
</figure>
<page confidence="0.953398">
1391
</page>
<reference confidence="0.999343256756757">
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of
AISTATS.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of EMNLP-CoNLL, pages 1169–1178.
Richard Kronmal and Arthur Peterson. 1979. On the
alias method for generating random variables from
a discrete distribution. The American Statistician,
33(4):214–218.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc¸ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP, pages 5524–5527.
Tom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink, Luk´aˇs
Burget, and Jan “Honza” ˇCernock´y. 2011. Em-
pirical evaluation and combination of advanced lan-
guage modeling techniques. In Proceedings of IN-
TERSPEECH, pages 605–608.
Tom´aˇs Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In Advances
in Neural Information Processing Systems.
Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Pro-
ceedings of AISTATS, pages 246–252.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified lin-
ear units improve restricted Boltzmann machines. In
Proceedings of ICML, pages 807–814.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using Restricted Boltzmann
Machines. In Proceedings of IWSLT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proceedings of EMNLP.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine trans-
lation. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages
11–19.
Holger Schwenk. 2004. Efficient training of large neural
networks for language modeling. In Proceedings of
IJCNN, pages 3059–3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492–
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague Bul-
letin of Mathematical Linguistics, 93:137–146.
Holger Schwenk. 2013. CSLM - a modular open-source
continuous space language modeling toolkit. In Pro-
ceedings of Interspeech.
M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,
Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean,
and G.E. Hinton. 2013. On rectified linear units for
speech processing. In Proceedings of ICASSP.
</reference>
<page confidence="0.994453">
1392
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.175137">
<title confidence="0.999168">Decoding with Large-Scale Neural Language Improves Translation</title>
<author confidence="0.998901">Ashish Vaswani Yinggong Zhao</author>
<affiliation confidence="0.9999815">University of Southern California Nanjing University, State Key Laboratory Department of Computer Science for Novel Software Technology</affiliation>
<email confidence="0.391454">avaswani@isi.eduzhaoyg@nlp.nju.edu.cn</email>
<author confidence="0.534211">Fossum</author>
<affiliation confidence="0.984115">University of Southern Information Sciences</affiliation>
<abstract confidence="0.968177923076923">We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine system both by reranking lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 B .</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="1412" citStr="Bengio et al. (2003)" startWordPosition="203" endWordPosition="206">cabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 B . 1 Introduction Machine translation (MT) systems rely upon language models (LMs) during decoding to ensure fluent output in the target language. Typically, these LMs are n-gram models over discrete representations of words. Such models are susceptible to data sparsity–that is, the probability of an n-gram observed only few times is difficult to estimate reliably, because these models do not use any information about similarities between words. To address this issue, Bengio et al. (2003) propose distributed word representations, in which each word is represented as a real-valued vector in a high-dimensional feature space. Bengio et al. (2003) introduce a feed-forward neural probabilistic LM (NPLM) that operates over these distributed representations. During training, the NPLM learns both a distributed representation for each word in the vocabulary and an n-gram probability distribution over words in terms of these distributed representations. Although neural LMs have begun to rival or even surpass traditional n-gram LMs (Mnih and Hinton, 2009; Mikolov et al., 2011), they have</context>
<context position="3644" citStr="Bengio et al., 2003" startWordPosition="561" endWordPosition="564">ys. First, we use it to rerank the k-best output of a hierarchical phrase-based decoder (Chiang, 2007). Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model. We achieve gains of up to 0.6 B translating French, German, and Spanish to English, and up to 1.1 B on Chinese-English translation. 1387 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Neural probabilistic language model (Bengio et al., 2003). 2 Neural Language Models Let V be the vocabulary, and n be the order of the language model; let u range over contexts, i.e., strings of length (n−1), and w range over words. For simplicity, we assume that the training data is a single very long string, w1 · · · wN, where wN is a special stop symbol, &lt;/s&gt;. We write ui for wi−n+1 · · · wi−1, where, for i ≤ 0, wi is a special start symbol, &lt;s&gt;. 2.1 Model We use a feedforward neural network as shown in Figure 1, following Bengio et al. (2003). The input to the network is a sequence of one-hot representations of the words in context u, which we w</context>
<context position="16005" citStr="Bengio et al. (2003)" startWordPosition="2741" endWordPosition="2744">een applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010). This model is dramatically faster to train than previous neural LMs, and can be trained on a large corpus with a large vocabulary and directly integrated into the decoder of a MT system. Our experiments across four language pairs demonstrated improvements of up to 1.1 B . Code for training and using our NPLMs is available for download.2 Acknowledgements We would like to thank the anonymous reviewers for their very helpful comments. This research was supported in part by DOI IBC gr</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Practical recommendations for gradient-based training of deep architectures.</title>
<date>2012</date>
<location>CoRR, abs/1206.5533.</location>
<contexts>
<context position="7746" citStr="Bengio, 2012" startWordPosition="1353" endWordPosition="1354">the Z(u) will converge to a value that normalizes the model, satisfying (3), and, under appropriate conditions, the parameters will converge to a value that maximizes the likelihood of the data. 3 Implementation Both training and scoring of neural LMs are computationally expensive at the scale needed for machine translation. In this section, we describe some of the techniques used to make them practical for translation. 3.1 Training During training, we compute gradients on an entire minibatch at a time, allowing the use of matrixmatrix multiplications instead of matrix-vector multiplications (Bengio, 2012). We represent the inputs as a sparse matrix, allowing the computation of the input layer (1) to use sparse matrix-matrix multiplications. The output activations (2) are computed only for the word types that occur as the positive example or one of the noise samples, yielding a sparse matrix of outputs. Similarly, during backpropagation, sparse matrix multiplications are used at both the output and input layer. In most of these operations, the examples in a minibatch can be processed in parallel. However, in the sparse-dense products used when updating the parameters D and D0, we found it was b</context>
</contexts>
<marker>Bengio, 2012</marker>
<rawString>Yoshua Bengio. 2012. Practical recommendations for gradient-based training of deep architectures. CoRR, abs/1206.5533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="10843" citStr="Chen and Goodman, 1998" startWordPosition="1871" endWordPosition="1875"> for Chinese-English experiments, without neural LM (baseline) and with neural LM for reranking and integrated decoding. Reranking with the neural LM improves translation quality, while integrating it into the decoder improves even more. 4 Experiments We ran experiments on four language pairs – Chinese to English and French, German, and Spanish to English – using a hierarchical phrase-based MT system (Chiang, 2007) and GIZA++ (Och and Ney, 2003) for word alignments. For all experiments, we used four LMs. The baselines used conventional 5-gram LMs, estimated with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07). Against these baselines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same datasets. The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1, and 150 units in hidden layer h2. We initialized the network parameters uniformly from (−0.01, 0.01) and the output biases to − log |V|, and optimized them by 10 epochs of sto</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="3126" citStr="Chiang, 2007" startWordPosition="486" endWordPosition="487">e cheaper to compute than sigmoid or tanh units. There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (Zeiler et al., 2013). Second, we train using noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which does not require repeated summations over the whole vocabulary. This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise. We then apply this LM to MT in two ways. First, we use it to rerank the k-best output of a hierarchical phrase-based decoder (Chiang, 2007). Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model. We achieve gains of up to 0.6 B translating French, German, and Spanish to English, and up to 1.1 B on Chinese-English translation. 1387 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Neural probabilistic language model (Bengio et al., 2003). 2 Neural Language Models Let V be the vocabulary, and n be the order of the lang</context>
<context position="10638" citStr="Chiang, 2007" startWordPosition="1841" endWordPosition="1842">d et al., 2012). XN i=1 L = k X j=1 aL XN i=1 a0 = k X j=1 1389 setting dev 2004 2005 2006 baseline 38.2 38.4 37.7 34.3 reranking 38.5 38.6 37.8 34.7 decoding 39.1 39.5 38.8 34.9 Table 1: Results for Chinese-English experiments, without neural LM (baseline) and with neural LM for reranking and integrated decoding. Reranking with the neural LM improves translation quality, while integrating it into the decoder improves even more. 4 Experiments We ran experiments on four language pairs – Chinese to English and French, German, and Spanish to English – using a hierarchical phrase-based MT system (Chiang, 2007) and GIZA++ (Och and Ney, 2003) for word alignments. For all experiments, we used four LMs. The baselines used conventional 5-gram LMs, estimated with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07). Against these baselines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same datasets. The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>In Proceedings of AISTATS.</booktitle>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Language model rest costs and space-efficient storage.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1169--1178</pages>
<contexts>
<context position="10040" citStr="Heafield et al., 2012" startWordPosition="1735" endWordPosition="1738">ny lookups required during decoding. A final issue when decoding with a neural LM is that, in order to estimate future costs, we need to be able to estimate probabilities of n0-grams for n0 &lt; n. In conventional LMs, this information is readily available,1 but not in NPLMs. Therefore, we defined a special word &lt;null&gt; whose embedding is the weighted average of the (input) embeddings of all the other words in the vocabulary. Then, to estimate the probability of an n0-gram u0w, we used the probability of P(w |&lt;null&gt;n−n0u0). 1However, in Kneser-Ney smoothed LMs, this information is also incorrect (Heafield et al., 2012). XN i=1 L = k X j=1 aL XN i=1 a0 = k X j=1 1389 setting dev 2004 2005 2006 baseline 38.2 38.4 37.7 34.3 reranking 38.5 38.6 37.8 34.7 decoding 39.1 39.5 38.8 34.9 Table 1: Results for Chinese-English experiments, without neural LM (baseline) and with neural LM for reranking and integrated decoding. Reranking with the neural LM improves translation quality, while integrating it into the decoder improves even more. 4 Experiments We ran experiments on four language pairs – Chinese to English and French, German, and Spanish to English – using a hierarchical phrase-based MT system (Chiang, 2007) a</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2012</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012. Language model rest costs and space-efficient storage. In Proceedings of EMNLP-CoNLL, pages 1169–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kronmal</author>
<author>Arthur Peterson</author>
</authors>
<title>On the alias method for generating random variables from a discrete distribution.</title>
<date>1979</date>
<journal>The American Statistician,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="11669" citStr="Kronmal and Peterson, 1979" startWordPosition="2015" endWordPosition="2018">5-gram NPLMs trained on the same datasets. The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1, and 150 units in hidden layer h2. We initialized the network parameters uniformly from (−0.01, 0.01) and the output biases to − log |V|, and optimized them by 10 epochs of stochastic gradient ascent, using minibatches of size 1000 and a learning rate of 1. We drew 100 noise samples per training example from the unigram distribution, using the alias method for efficiency (Kronmal and Peterson, 1979). We trained the discriminative models with MERT (Och, 2003) and the discriminative rerankers on 1000-best lists with MERT. Except where noted, we ran MERT three times and report the average score. We evaluated using case-insensitive NIST B . 4.1 NIST Chinese-English For the Chinese-English task (Table 1), the training data came from the NIST 2012 constrained track, excluding sentences longer than 60 words. Rules Fr-En De-En Es-En setting dev test dev test dev test baseline 33.5 25.5 28.8 21.5 33.5 32.0 reranking 33.9 26.0 29.1 21.5 34.1 32.2 decoding 34.12 26.12 29.3 21.9 34.22 32.12 Table 2:</context>
</contexts>
<marker>Kronmal, Peterson, 1979</marker>
<rawString>Richard Kronmal and Arthur Peterson. 1979. On the alias method for generating random variables from a discrete distribution. The American Statistician, 33(4):214–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>Jean-Luc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5524--5527</pages>
<contexts>
<context position="14942" citStr="Le et al., 2011" startWordPosition="2568" endWordPosition="2571">arious values of k and |V|, on a dual hex-core 2.67 GHz Xeon X5650 machine. For these experiments, we used minibatches of 128 examples. The timings are plotted in Figure 2. We see that NCE is considerably faster than MLE; moreover, as expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limi</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP, pages 5524–5527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
<author>Anoop Deoras</author>
<author>Stefan Kombrink</author>
<author>Luk´aˇs Burget</author>
<author>Jan “Honza” ˇCernock´y</author>
</authors>
<title>Empirical evaluation and combination of advanced language modeling techniques.</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>605--608</pages>
<marker>Mikolov, Deoras, Kombrink, Burget, ˇCernock´y, 2011</marker>
<rawString>Tom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink, Luk´aˇs Burget, and Jan “Honza” ˇCernock´y. 2011. Empirical evaluation and combination of advanced language modeling techniques. In Proceedings of INTERSPEECH, pages 605–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical Language Models Based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="15494" citStr="Mikolov, 2012" startWordPosition="2656" endWordPosition="2657">Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutma</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1978" citStr="Mnih and Hinton, 2009" startWordPosition="287" endWordPosition="291">ween words. To address this issue, Bengio et al. (2003) propose distributed word representations, in which each word is represented as a real-valued vector in a high-dimensional feature space. Bengio et al. (2003) introduce a feed-forward neural probabilistic LM (NPLM) that operates over these distributed representations. During training, the NPLM learns both a distributed representation for each word in the vocabulary and an n-gram probability distribution over words in terms of these distributed representations. Although neural LMs have begun to rival or even surpass traditional n-gram LMs (Mnih and Hinton, 2009; Mikolov et al., 2011), they have not yet been widely adopted in large-vocabulary applications such as MT, because standard maximum likelihood estimation (MLE) requires repeated summations over all words in the vocabulary. A variety of strategies have been proposed to combat this issue, many of which require severe restrictions on the size of the network or the size of the data. In this work, we extend the NPLM of Bengio et al. (2003) in two ways. First, we use rectified linear units (Nair and Hinton, 2010), whose activations are cheaper to compute than sigmoid or tanh units. There is also ev</context>
<context position="14893" citStr="Mnih and Hinton, 2009" startWordPosition="2557" endWordPosition="2561">tion of Gigaword and timed one epoch of training, for various values of k and |V|, on a dual hex-core 2.67 GHz Xeon X5650 machine. For these experiments, we used minibatches of 128 examples. The timings are plotted in Figure 2. We see that NCE is considerably faster than MLE; moreover, as expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2009. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2820" citStr="Mnih and Teh, 2012" startWordPosition="430" endWordPosition="433"> A variety of strategies have been proposed to combat this issue, many of which require severe restrictions on the size of the network or the size of the data. In this work, we extend the NPLM of Bengio et al. (2003) in two ways. First, we use rectified linear units (Nair and Hinton, 2010), whose activations are cheaper to compute than sigmoid or tanh units. There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (Zeiler et al., 2013). Second, we train using noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which does not require repeated summations over the whole vocabulary. This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise. We then apply this LM to MT in two ways. First, we use it to rerank the k-best output of a hierarchical phrase-based decoder (Chiang, 2007). Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model. We achieve gains of up to 0.6 B translating French, German, and Spanish to English, and up to 1.1 B on Chinese-English translation. 1387 Proceedings of the 2013 Conference on Empir</context>
<context position="5599" citStr="Mnih and Teh, 2012" startWordPosition="935" endWordPosition="938"> b is a vector of biases for every word in the vocabulary. 2.2 training The typical way to train neural LMs is to maximize the likelihood of the training data by gradient ascent. But the softmax layer requires, at each iteration, a summation over all the units in the output layer, that is, all words in the whole vocabulary. If the vocabulary is large, this can be prohibitively expensive. Noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) is an alternative estimation principle that allows one to avoid these repeated summations. It has been applied previously to logbilinear LMs (Mnih and Teh, 2012), and we apply it here to the NPLM described above. We can write the probability of a word w given a context u under the NPLM as 1 P(w |u) = Z(u) p(w |u) p(w |u) = exp (D0h2 + b) Z(u) = Z p(w0 |u) (3) w0 where p(w |u) is the unnormalized output of the unit corresponding to w, and Z(u) is the normalization factor. Let 0 stand for the parameters of the model. One possibility would be to treat Z(u), instead of being defined by (3), as an additional set of model parameters which are learned along with 0. But it is easy to see that we can make the likelihood arbitrarily large by making the Z(u) arb</context>
<context position="8903" citStr="Mnih and Teh (2012)" startWordPosition="1548" endWordPosition="1551"> used when updating the parameters D and D0, we found it was best to divide the vocabulary into blocks (16 per thread) and to process the blocks in parallel. 3.2 Translation To incorporate this neural LM into a MT system, we can use the LM to rerank k-best lists, as has been done in previous work. But since the NPLM scores n-grams, it can also be integrated into a phrase-based or hierarchical phrase-based decoder just as a conventional n-gram model can, unlike a RNN. The most time-consuming step in computing ngram probabilities is the computation of the normalization constants Z(u). Following Mnih and Teh (2012), we set all the normalization constants to one during training, so that the model learns to produce approximately normalized probabilities. Then, when applying the LM, we can simply ignore normalization. A similar strategy was taken by Niehues and Waibel (2012). We find that a single n-gram lookup takes about 40 µs. The technique, described above, of grouping examples into minibatches works for scoring of k-best lists, but not while decoding. But caching n-gram probabilities helps to reduce the cost of the many lookups required during decoding. A final issue when decoding with a neural LM is </context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of AISTATS,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="14869" citStr="Morin and Bengio, 2005" startWordPosition="2553" endWordPosition="2556">words) of the Xinhua portion of Gigaword and timed one epoch of training, for various values of k and |V|, on a dual hex-core 2.67 GHz Xeon X5650 machine. For these experiments, we used minibatches of 128 examples. The timings are plotted in Figure 2. We see that NCE is considerably faster than MLE; moreover, as expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) an</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of AISTATS, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted Boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="2491" citStr="Nair and Hinton, 2010" startWordPosition="378" endWordPosition="381">ations. Although neural LMs have begun to rival or even surpass traditional n-gram LMs (Mnih and Hinton, 2009; Mikolov et al., 2011), they have not yet been widely adopted in large-vocabulary applications such as MT, because standard maximum likelihood estimation (MLE) requires repeated summations over all words in the vocabulary. A variety of strategies have been proposed to combat this issue, many of which require severe restrictions on the size of the network or the size of the data. In this work, we extend the NPLM of Bengio et al. (2003) in two ways. First, we use rectified linear units (Nair and Hinton, 2010), whose activations are cheaper to compute than sigmoid or tanh units. There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (Zeiler et al., 2013). Second, we train using noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which does not require repeated summations over the whole vocabulary. This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise. We then apply this LM to MT in two ways. First, we use it to rerank the k-best output of a hierarchical </context>
<context position="4438" citStr="Nair and Hinton, 2010" startWordPosition="721" endWordPosition="724">ds. For simplicity, we assume that the training data is a single very long string, w1 · · · wN, where wN is a special stop symbol, &lt;/s&gt;. We write ui for wi−n+1 · · · wi−1, where, for i ≤ 0, wi is a special start symbol, &lt;s&gt;. 2.1 Model We use a feedforward neural network as shown in Figure 1, following Bengio et al. (2003). The input to the network is a sequence of one-hot representations of the words in context u, which we write uj (1 ≤ j ≤ n − 1). The output is the probability P(w |u) for each word w, which the network computes as follows. The hidden layers consist of rectified linear units (Nair and Hinton, 2010), which use the activation function O(x) = max(0, x) (see graph at right). The output of the first hidden layer h1 is �������� CjDuj (1) � where D is a matrix of input word embeddings which is shared across all positions, the Cj are the context matrices for each word in u, and 0 is applied elementwise. The output of the second layer h2 is h2 = 0 (Mh1) , where M is the matrix of connection weights between h1 and h2. Finally, the output layer is a softmax layer, P(w |u) ∝ exp (D0h2 + b) (2) where D0 is the output word embedding matrix and b is a vector of biases for every word in the vocabulary.</context>
<context position="16053" citStr="Nair and Hinton, 2010" startWordPosition="2748" endWordPosition="2751">PLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010). This model is dramatically faster to train than previous neural LMs, and can be trained on a large corpus with a large vocabulary and directly integrated into the decoder of a MT system. Our experiments across four language pairs demonstrated improvements of up to 1.1 B . Code for training and using our NPLMs is available for download.2 Acknowledgements We would like to thank the anonymous reviewers for their very helpful comments. This research was supported in part by DOI IBC grant D12AP00225. This work was done while the sec</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of ICML, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Continuous space language models using Restricted Boltzmann Machines.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="9165" citStr="Niehues and Waibel (2012)" startWordPosition="1589" endWordPosition="1592"> lists, as has been done in previous work. But since the NPLM scores n-grams, it can also be integrated into a phrase-based or hierarchical phrase-based decoder just as a conventional n-gram model can, unlike a RNN. The most time-consuming step in computing ngram probabilities is the computation of the normalization constants Z(u). Following Mnih and Teh (2012), we set all the normalization constants to one during training, so that the model learns to produce approximately normalized probabilities. Then, when applying the LM, we can simply ignore normalization. A similar strategy was taken by Niehues and Waibel (2012). We find that a single n-gram lookup takes about 40 µs. The technique, described above, of grouping examples into minibatches works for scoring of k-best lists, but not while decoding. But caching n-gram probabilities helps to reduce the cost of the many lookups required during decoding. A final issue when decoding with a neural LM is that, in order to estimate future costs, we need to be able to estimate probabilities of n0-grams for n0 &lt; n. In conventional LMs, this information is readily available,1 but not in NPLMs. Therefore, we defined a special word &lt;null&gt; whose embedding is the weight</context>
<context position="15640" citStr="Niehues and Waibel (2012)" startWordPosition="2677" endWordPosition="2680">ortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010). This model is dramatically faster to train than previous neural LMs, and can be trained on a large corpus with a large v</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>Jan Niehues and Alex Waibel. 2012. Continuous space language models using Restricted Boltzmann Machines. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10669" citStr="Och and Ney, 2003" startWordPosition="1845" endWordPosition="1848">= k X j=1 aL XN i=1 a0 = k X j=1 1389 setting dev 2004 2005 2006 baseline 38.2 38.4 37.7 34.3 reranking 38.5 38.6 37.8 34.7 decoding 39.1 39.5 38.8 34.9 Table 1: Results for Chinese-English experiments, without neural LM (baseline) and with neural LM for reranking and integrated decoding. Reranking with the neural LM improves translation quality, while integrating it into the decoder improves even more. 4 Experiments We ran experiments on four language pairs – Chinese to English and French, German, and Spanish to English – using a hierarchical phrase-based MT system (Chiang, 2007) and GIZA++ (Och and Ney, 2003) for word alignments. For all experiments, we used four LMs. The baselines used conventional 5-gram LMs, estimated with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07). Against these baselines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same datasets. The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1, a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11729" citStr="Och, 2003" startWordPosition="2026" endWordPosition="2027">cabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1, and 150 units in hidden layer h2. We initialized the network parameters uniformly from (−0.01, 0.01) and the output biases to − log |V|, and optimized them by 10 epochs of stochastic gradient ascent, using minibatches of size 1000 and a learning rate of 1. We drew 100 noise samples per training example from the unigram distribution, using the alias method for efficiency (Kronmal and Peterson, 1979). We trained the discriminative models with MERT (Och, 2003) and the discriminative rerankers on 1000-best lists with MERT. Except where noted, we ran MERT three times and report the average score. We evaluated using case-insensitive NIST B . 4.1 NIST Chinese-English For the Chinese-English task (Table 1), the training data came from the NIST 2012 constrained track, excluding sentences longer than 60 words. Rules Fr-En De-En Es-En setting dev test dev test dev test baseline 33.5 25.5 28.8 21.5 33.5 32.0 reranking 33.9 26.0 29.1 21.5 34.1 32.2 decoding 34.12 26.12 29.3 21.9 34.22 32.12 Table 2: Results for Europarl MT experiments, without neural LM (bas</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Training neural network language models on very large corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15186" citStr="Schwenk and Gauvain, 2005" startWordPosition="2606" endWordPosition="2609"> expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our app</context>
</contexts>
<marker>Schwenk, Gauvain, 2005</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2005. Training neural network language models on very large corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, pruned or continuous space language models on a GPU for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>11--19</pages>
<contexts>
<context position="15252" citStr="Schwenk et al., 2012" startWordPosition="2618" endWordPosition="2621">NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly i</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, pruned or continuous space language models on a GPU for statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Efficient training of large neural networks for language modeling.</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNN,</booktitle>
<pages>3059--3062</pages>
<contexts>
<context position="15095" citStr="Schwenk, 2004" startWordPosition="2595" endWordPosition="2596">tted in Figure 2. We see that NCE is considerably faster than MLE; moreover, as expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a d</context>
</contexts>
<marker>Schwenk, 2004</marker>
<rawString>Holger Schwenk. 2004. Efficient training of large neural networks for language modeling. In Proceedings of IJCNN, pages 3059–3062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>518</pages>
<contexts>
<context position="15450" citStr="Schwenk, 2007" startWordPosition="2650" endWordPosition="2651">archical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 20</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous-space language models for statistical machine translation.</title>
<date>2010</date>
<booktitle>Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--137</pages>
<contexts>
<context position="15466" citStr="Schwenk, 2010" startWordPosition="2652" endWordPosition="2653"> and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. 6 Conclusion We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-c</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. Prague Bulletin of Mathematical Linguistics, 93:137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>CSLM - a modular open-source continuous space language modeling toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="13994" citStr="Schwenk, 2013" startWordPosition="2403" endWordPosition="2404">ovements, shown in Table 2, were more modest than on Chinese-English. Reranking with the neural LM yielded improvements of up to 0.5 B , and integrating the neural LM into the decoder yielded improvements of up to 0.6 B . In one case (Spanish-English), integrated decoding scored higher than reranking on the development data but lower on the test data – perhaps due to the difference in domain between the two. On the other tasks, integrated decoding outperformed reranking. 4.3 Speed comparison We measured the speed of training a NPLM by NCE, compared with MLE as implemented by the CSLM toolkit (Schwenk, 2013). We used the first 200k 1390 10 20 30 40 50 60 70 Vocabulary size (x1000) Figure 2: Noise contrastive estimation (NCE) is much faster, and much less dependent on vocabulary size, than MLE as implemented by the CSLM toolkit (Schwenk, 2013). lines (5.2M words) of the Xinhua portion of Gigaword and timed one epoch of training, for various values of k and |V|, on a dual hex-core 2.67 GHz Xeon X5650 machine. For these experiments, we used minibatches of 128 examples. The timings are plotted in Figure 2. We see that NCE is considerably faster than MLE; moreover, as expected, the MLE training time i</context>
</contexts>
<marker>Schwenk, 2013</marker>
<rawString>Holger Schwenk. 2013. CSLM - a modular open-source continuous space language modeling toolkit. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Zeiler</author>
<author>M Ranzato</author>
<author>R Monga</author>
<author>M Mao</author>
<author>K Yang</author>
<author>Q V Le</author>
<author>P Nguyen</author>
<author>A Senior</author>
<author>V Vanhoucke</author>
<author>J Dean</author>
<author>G E Hinton</author>
</authors>
<title>On rectified linear units for speech processing.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="2709" citStr="Zeiler et al., 2013" startWordPosition="414" endWordPosition="417">ause standard maximum likelihood estimation (MLE) requires repeated summations over all words in the vocabulary. A variety of strategies have been proposed to combat this issue, many of which require severe restrictions on the size of the network or the size of the data. In this work, we extend the NPLM of Bengio et al. (2003) in two ways. First, we use rectified linear units (Nair and Hinton, 2010), whose activations are cheaper to compute than sigmoid or tanh units. There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (Zeiler et al., 2013). Second, we train using noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which does not require repeated summations over the whole vocabulary. This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise. We then apply this LM to MT in two ways. First, we use it to rerank the k-best output of a hierarchical phrase-based decoder (Chiang, 2007). Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model. We achieve gains of up to 0.6 B translating French, German, and Spani</context>
</contexts>
<marker>Zeiler, Ranzato, Monga, Mao, Yang, Le, Nguyen, Senior, Vanhoucke, Dean, Hinton, 2013</marker>
<rawString>M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and G.E. Hinton. 2013. On rectified linear units for speech processing. In Proceedings of ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
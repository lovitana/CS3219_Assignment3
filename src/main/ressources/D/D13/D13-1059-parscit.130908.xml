<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.962197">
Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion
</title>
<author confidence="0.964036">
Rapha¨el Bailly† Xavier Carreras† Franco M. Luque$ Ariadna Quattoni†
</author>
<affiliation confidence="0.934024">
† Universitat Polit`ecnica de Catalunya $ Universidad Nacional de C´ordoba and CONICET
</affiliation>
<address confidence="0.993472">
Barcelona, 08034 X500HUA C´ordoba, Argentina
</address>
<email confidence="0.999187">
rbailly,carreras,aquattoni@lsi.upc.edu francolq@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901222222222">
We derive a spectral method for unsupervised
learning of Weighted Context Free Grammars.
We frame WCFG induction as finding a Han-
kel matrix that has low rank and is linearly
constrained to represent a function computed
by inside-outside recursions. The proposed al-
gorithm picks the grammar that agrees with a
sample and is the simplest with respect to the
nuclear norm of the Hankel matrix.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999788109090909">
Weighted Context Free Grammars (WCFG) define
an important class of languages. Their expressivity
makes them good candidates for modeling a wide
range of natural language phenomena. This expres-
sivity comes at a cost: unsupervised learning of
WCFG seems to be a particularly hard task. And
while it is a well-studied problem, it is still to a great
extent unsolved.
Several methods for unsupervised learning of
WCFG have been proposed. Some rely on heuristics
that are used to build incrementally an approxima-
tion of the unknown grammar (Adriaans et al., 2000;
Van Zaanen, 2000; Tu and Honavar, 2008). Other
methods are based on maximum likelihood estima-
tion, searching for the grammar that has the largest
posterior given the training corpus (Baker, 1979;
Lari and Young, 1990; Pereira and Schabes, 1992;
Klein and Manning, 2002). Several Bayesian in-
ference approaches have also been proposed (Chen,
1995; Kurihara and Sato, 2006; Liang et al., 2007;
Cohen et al., 2010). These approaches perform pa-
rameter estimation by exploiting Markov sampling
techniques.
Recently, for the related problem of unsupervised
dependency parsing, Gormley and Eisner (2013)
proposed a new way of framing the max-likelihood
estimation. In their formulation the problem is ex-
pressed as an integer quadratic program subject to
non-linear constraints. They exploit techniques from
mathematical programming to solve the resulting
optimization.
In spirit, the work by Clark (2001; 2007) is prob-
ably the most similar to our approach since both ap-
proaches share an algebraic view of the problem. In
his case the key idea is to work with an algebraic
representation of a WCFG. The problem of recover-
ing the constituents of the grammar is reduced to the
problem of identifying its syntactic congruence.
In the last years, multiple spectral learning algo-
rithms have been proposed for a wide range of mod-
els (Hsu et al., 2009; Bailly et al., 2009; Bailly et al.,
2010; Balle et al., 2011; Luque et al., 2012; Cohen
et al., 2012). Since the spectral approach provides a
good thinking tool to reason about distributions over
E*, the question of whether they can be used for un-
supervised learning of WCFG seems natural. Still,
while spectral algorithms for unsupervised learning
of languages can learn regular languages, tree lan-
guages and simple dependency grammars, the fron-
tier to WCFG seems hard to reach.
In fact, the most recent theoretical results on spec-
tral learning of WCFG do not seem to be very en-
couraging. Recently, Hsu et al. (2012) showed that
the problem of recovering the joint distribution over
PCFG derivations and their yields is not identifiable.
</bodyText>
<page confidence="0.980866">
624
</page>
<note confidence="0.7336525">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624–635,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999650707692308">
Although, for some simple grammar subclasses (e.g.
independent left and right children), identification in
the weaker sense (over the yields of the grammar)
implies strong identification (e.g. over joint distri-
bution of yields and derivations). In their paper, they
propose a spectral algorithm based on a generaliza-
tion of the method of moments for these restricted
subclasses.
Thus one open direction for spectral research con-
sists on defining subclasses of context free lan-
guages that can be learned (in the strong sense) from
observations of yields. Yet, an alternative research
direction is to consider learnability in the weaker
sense. In this paper we take the second road, and
focus on the problem of approximating the distribu-
tion over yields generated by a WCFG.
Our main contribution is to present a spectral al-
gorithm for unsupervised learning of WCFG. Fol-
lowing ideas from Balle et al. (2012), the algo-
rithm is framed as a convex optimization where we
search for a low-rank matrix satisfying two types
of constraints: (1) Constraints derived from observ-
able statistics over yields; and (2) Constraints de-
rived from certain recurrence relations satisfied by a
WCFG. Our derivations of the learning algorithm il-
lustrate the main ingredients behind the spectral ap-
proach to learning functions over E* which are: (1)
to exploit the recurrence relations satisfied by the
target family of functions and (2) provide algebraic
formulations of these relations.
We alert the reader that although we are able to
frame the problem as a convex optimization, the
number of variables involved is quite large and pro-
hibits a practical implementation of the method on
a realistic scenario. The experiments we present
should be regarded as examples designed to illus-
trate the behavior of the method. More research
is needed to make the optimization more efficient,
and we are optimistic that such improvements can
be achieved by exploiting problem-specific proper-
ties of the optimization. Regardless of this, ours is
a novel way of framing the grammatical inference
problem.
The rest of the paper is organized as follows. Sec-
tion 2 gives preliminaries on WCFG and the type of
functions we will learn. Section 3 establishes that
spectral methods can learn a WCFG from a Han-
kel matrix containing statistics about context-free
cuts. Section 4 presents the unsupervised algorithm,
where we formulate grammar induction as a low-
rank optimization. Section 5 presents experiments,
and finally we conclude the paper.
Notation Let E be an alphabet. We use Q to de-
note an arbitrary symbol in E. The set of all fi-
nite strings over E is denoted by E*, where we
write A for the empty string. We also use the set
E+ = E* \ {A}.
We use bold letters to represent column vectors
v and matrices M. We use In to denote the n-
dimensional identity matrix. We use M+ to de-
note the Moore-Penrose pseudoinverse of some ma-
trix M. M ⊗M&apos; is the Kronecker product between
matrices M ∈ R` and M&apos; ∈ Rp,q resulting in a
matrix in R&amp;quot;1zp,nq. The rest of notation will be given
as needed.
</bodyText>
<sectionHeader confidence="0.991579" genericHeader="method">
2 Weighted Context Free Grammars
</sectionHeader>
<bodyText confidence="0.9999398">
In this section we define Weighted Context Free
Grammars (WCFG). We start with a classic defini-
tion and then describe an algebraic form of WCFG
that will be used throughout the paper. We also de-
scribe the fundamental recursions in WCFG.
</bodyText>
<subsectionHeader confidence="0.946801">
2.1 WCFG in Classic Form
</subsectionHeader>
<bodyText confidence="0.978245">
A WCFG over E is a tuple G =
hV, R, T, w*, wT, wRi where
</bodyText>
<listItem confidence="0.975805461538461">
• V is the set of non-terminal symbols. We as-
sume that V = {1, ... , n} for some natural
number n, and that V ∩ E = ∅.
• R is a set of binary rules of the form i → j k
where i, j, k ∈ V .
• T is a set of unary rules of the form i → Q
where i ∈ V and Q ∈ E.
• w* : V → R, with w*(i) being the weight of
starting a derivation with non-terminal i.
• wT : V × E → R, with wT(i → Q) being the
weight of rule rewriting i into Q.
• wR : V × V × V → R, with wR(i → j k)
being the weight of rewriting i into j k.
</listItem>
<bodyText confidence="0.8408575">
A WCFG G computes a function gG : E+ → R
defined as
</bodyText>
<equation confidence="0.986029">
g o(x) = � w*(i) Qo(i *=⇒ x) , (1)
iEV
</equation>
<page confidence="0.98427">
625
</page>
<bodyText confidence="0.998972">
where we define the inside function β�G�: V ×E+ →
</bodyText>
<equation confidence="0.99212225">
R recursively:
β�0(i ?=⇒ σ) = wT (i → σ) (2)
wR(i → j k) (3)
β�o(j ?=⇒ x1)β�G(k ?=⇒ x2) ,
</equation>
<bodyText confidence="0.992032833333333">
where in the second case we assume |x |&gt; 1. The
inside function β�G(i ?=⇒ x) exploits the fundamen-
tal inside recursion in WCFG (Baker, 1979; Lari and
Young, 1990). We will find useful to define the out-
side function α�G� : E? × V × E? → R defined recur-
sively as:
</bodyText>
<equation confidence="0.999869">
α�o(λ; i; λ) = w?(i) (4)
</equation>
<bodyText confidence="0.9997313">
Finally, we note that Probabilistic Context Free
Grammars (PCFG) are a special case of WCFG
where: w?(i) is the probability to start a derivation
with non-terminal i; wR(i → j k) is the condi-
tional probability of rewriting nonterminal i into j
and k; wT(i → σ) is the probability of rewriting i
into symbol σ; Pi w?(i) = 1; and for each i ∈ V ,
Pj,k wR(i → j k) + Pσ wT (i → σ) = 1. Un-
der these conditions the function gG� is a probability
distibution over E+.
</bodyText>
<subsectionHeader confidence="0.996171">
2.2 WCFG in Algebraic Form
</subsectionHeader>
<bodyText confidence="0.999653333333333">
We now define a WCFG in algebraic form. A
Weighted Context Free Grammar (WCFG) over E
with n states is a tuple G = hα?, {,3σ}, Ai with:
</bodyText>
<equation confidence="0.997984777777778">
β�G(i?=⇒x)= X
j,kEV
x1,x2EE+
s.t. x=x1x2
X wR(j → k i)· (5) • An initial vector α? ∈ Rn.
α�G(x; i; y) = α�o(x1; j; y) · β�G(k ? =⇒ x2) • Terminal vectors 0σ ∈ Rn for σ ∈ E.
j,kEV • A bilinear operator A ∈ Rnxn2.
x1EE*,x2EE+
s.t. x=x1x2
</equation>
<bodyText confidence="0.96846">
where in the second case we assume that either
</bodyText>
<equation confidence="0.916009">
x =6λ or y =6λ.
For x, z ∈ E? and y ∈ E+ we have that
X α�G(x; i; z) · β��G(i ?=⇒ y) (6)
iEV
</equation>
<bodyText confidence="0.997842285714286">
is the weight that the grammar G assigns to a string
xyz that has a cut or bracketing around y. Techni-
cally, it corresponds to the sum of the weights of all
derivations that have a constituent spanning y. In
particular we have that
A WCFG G computes a function gG : E? → R
defined as
</bodyText>
<equation confidence="0.998817">
gG(x) = αT? βG(x) (7)
</equation>
<bodyText confidence="0.988749">
where the inside function βG : E+ → Rn is
</bodyText>
<equation confidence="0.970047789473684">
βG(σ) = )3σ (8)
XβG(x) = A(βG(x1) ⊗ βG(x2)) (9)
x1,x2EE+
x=x1x2
We will define the outside function αG : E? ×
E? → Rn as:
αG(λ; λ) = α? (10)
X wR(j → i k)· β�O(k?=⇒y1),
+ α�G(x; j; y2) ·
j,kEV
y1EE+,y2EE*
s.t. y=y1y2
g G(x) = X α� �G(λ;i;λ)·β� �G(i ? =⇒ x). XαG(x; z)T = αG(x1; z)TA(βG(x2) ⊗ In)
i x1EE*,x2EE+
x=x1x2
If x is a string of length m, and x[t:t,] is the substring
of x from positions t to t&apos;, it also happens that
g G(x) = X α�G(x[1:t−1]; i; x[t+1:m])· β�G(i ? =⇒ x[t]))
i
</equation>
<bodyText confidence="0.9693156">
for any t between 1 and m.
In this paper we will make frequent use of inside
and outside quantities. Notationally, for outsides the
semi-colon between two strings, i.e. x; z, will sim-
bolize a cut where we can insert an inside string y.
</bodyText>
<equation confidence="0.9996686">
+ X αG(x; z2)TA(In ⊗ βG(z1)) (11)
z1EE+,z2EE*
z=z1z2
For x, z ∈ E? and y ∈ E+ we have that
αG(x; z)TβG(y) (12)
</equation>
<bodyText confidence="0.845192">
is the weight that the grammar assigns to the string
xyz with a cut around y. In particular, gG(x) =
αG(λ; λ)TβG(x).
</bodyText>
<page confidence="0.997558">
626
</page>
<bodyText confidence="0.87508325">
Let us make clear that a WCFG is the same
device in classic or algebraic forms. If G =
hV, R, T, w?, wT, wRi and G = hα?, {,3σ}, Ai, the
mapping is:
</bodyText>
<equation confidence="0.9993866">
w?(i) = α?(i) (13)
wT(i → σ) = )3σ[i]
wR(i → j k) = A[i,j,k]
β�¯G(i ?=⇒ x) = βG(x)[i]
α�¯G(x; i; z) = αG(x; z)[i]
</equation>
<bodyText confidence="0.885638">
See Section A.1 for a proof of Eq. 16 and 17.
</bodyText>
<sectionHeader confidence="0.991987" genericHeader="method">
3 WCFG and Hankel Matrices
</sectionHeader>
<bodyText confidence="0.999171151515152">
In this section we describe Hankel matrices for
WCFG. These matrices explicitly capture inside-
outside recursions employed by WCFG functions,
and are key to a derivation of a spectral learning al-
gorithm that learns a grammar G using statistics of
a training sample.
Let us define some sets. We say that I1 = E+
is the set of inside strings. The set of composed in-
side strings I2 is the set of elements (x, x0), where
x, x0 ∈ E+. Intuitively (x, x0) represents two adja-
cent spans with an operation, i.e., it keeps the trace
of the operation that composes x with x0 and yields
xx0. We will use the set I = I1 ∪ I2.
The set of outside contexts O is the set containing
elements hx; zi, where x, z ∈ E?. Intuitively, hx; zi
represents a context where we can insert an inside
element y in between x and z, yielding xyz.
Consider a function f : O × I → R. The Hankel
matrix of f is the bi-infinite matrix Hf ∈ RO×I
such that Hf(o, i) = f(o, i).
In practice we will work with finite sub-blocks of
Hf. To this end we will employ the notion of basis
B = (P, S), where {hλ, λi} ⊆ P ⊆ O is a set
of outside contexts and E ⊆ S ⊆ I1 is a set of
inside strings. We will use p = |P |and s = |S|.
Furthermore, we define the inside completion of S
as the set S† = {(x, x0)  |x, x0 ∈ S}. Note that
S† ⊆ I2. We say that B† = (P, S†) is the inside
completion of B.
The sub-block of Hf defined by B is the p × s
matrix HB ∈ RP×S with HB(o, i) = Hf(o, i) =
f(o, i). In addition to HB, we are interested in these
additional finite vectors and matrices:
</bodyText>
<listItem confidence="0.991723166666667">
• h? ∈ RS is the s-dimensional vector with co-
ordinates h?(x) = f(hλ, λi, x).
• hσ ∈ RP is the p-dimensional vector with co-
ordinates hσ(o) = f(o, σ).
• HA ∈ RP×S† with HA(o, (x1, x2)) =
f(o, (x1, x2)).
</listItem>
<subsectionHeader confidence="0.999761">
3.1 Hankel Factorizations
</subsectionHeader>
<bodyText confidence="0.999768928571429">
If f is computed by a WCFG G, then Hf has rank
n factorization. To see this, consider the follow-
ing matrices. First a matrix S ∈ Rn×I� of inside
vectors for all strings, with column x taking value
Sx = βG(x). Then a matrix P ∈ RO×n of out-
side vectors for all contexts, with row hx; zi tak-
ing value Phx;zi = αG(x; z). It is easy to see that
Hf = PS, since Hf(hx; zi, y) = Phx;ziSy =
αG(x; z)&gt;βG(y). Therefore Hf has rank n.
The same happens for sub-blocks. If HB is the
sub-block associated with basis B = (P, S), then
the sub-blocks PB ∈ RP×n and SB ∈ Rn×S of P
and S also accomplish that HB = PBSB. It also
happens that
</bodyText>
<equation confidence="0.999824">
h&gt;? = α&gt;? SB (18)
hσ = PB,3σ (19)
HA = PBA(SB ⊗ SB) . (20)
</equation>
<bodyText confidence="0.979813">
We say that a basis B is complete for f if
rank(HB) = rank(Hf). The following is a key
result for spectral methods.
Lemma 1. Let B = (P, S) be a complete basis of
dimension n for a function f and let HB ∈ RP×S
be the Hankel sub-block of f for B. Let h?, hσ and
HA be the additional matrices for B. If HB = PS
is a rank n factorization, then the WCFG G =
hα?, {)3σ}, Ai with
</bodyText>
<equation confidence="0.904493">
α&gt;? = h&gt;? S+ (21)
,3σ = P+hσ (22)
A = P+HA(S ⊗ S)+ (23)
computes f.
See proof in Section A.2.
</equation>
<page confidence="0.985326">
627
</page>
<subsectionHeader confidence="0.999455">
3.2 Supervised Spectral Learning of WCFG
</subsectionHeader>
<bodyText confidence="0.993568">
The spectral learning method directly exploits
Lemma 1. In a nutshell, the spectral method is:
</bodyText>
<listItem confidence="0.9755869">
1. Choose a complete basis B = (P, S) and a di-
mension n.
2. Use training data to compute estimates of the
necessary Hankel matrices: HB, h*, hQ, HA.
3. Compute the SVD of HB, HB = UAV &gt;.
4. Create a truncated rank n factorization of HB
as PnSn, having Pn = UnAn and Sn = Vn &gt; ,
where we only consider the top n singular val-
ues/vectors of A, U, V .
5. Use Lemma 1 to compute G, using Pn and Sn.
</listItem>
<bodyText confidence="0.999707869565218">
Because of Lemma 1, if B is complete and we
have access to the true HB, h*, hQ, HA of a WCFG
target function g∗, then the algorithm will compute
a G that exactly computes g∗. In practice, we only
have access to empirical estimates of the Hankel ma-
trices. In this case, there exist PAC-style sample
complexity bounds that state that gG will be a close
approximation to g∗ (Hsu et al., 2009; Bailly et al.,
2009; Bailly et al., 2010).
The parameters of the algorithm are the basis and
the dimension of the grammar n. One typically em-
ploys some validation strategy using held-out data.
Empirically, the performance of these methods has
been shown to be good, and similar to that of EM
(Luque et al., 2012; Cohen et al., 2013). It is also
important to mention that in the case that the target
g∗ is a probability distribution, the function gG will
be close to g∗, but it will only define a distribution in
the limit: in practice it will not sum to one, and for
some inputs it might return negative values. This is a
practical difficulty of spectral methods, for example
to apply evaluation metrics like perplexity which are
only defined for distributions.
</bodyText>
<sectionHeader confidence="0.984989" genericHeader="method">
4 Unsupervised Learning of WCFG
</sectionHeader>
<bodyText confidence="0.999959666666667">
In the previous section we have exposed that if we
have access to estimates of a Hankel matrix of a
WCFG G, we can recover G. However, the statis-
tics in the Hankel require access to strings that have
information about context-free cuts. We will assume
that we only have access to statistics about plain
strings of a distribution, i.e. p(x), which we call
observations. In this scenario, one natural idea is
to search for a Hankel matrix that agrees with the
observations. The method we present in this sec-
tion frames this problem as a low-rank matrix op-
timization problem. We first characterize the space
of solutions to our problem, i.e. Hankel matrices
associated with WCFG that agree with observable
statistics. Then we present the method.
</bodyText>
<subsectionHeader confidence="0.999528">
4.1 Characterization of a WCFG Hankel
</subsectionHeader>
<bodyText confidence="0.999026470588235">
In this section we describe valid WCFG Hankel ma-
trices using linear constraints.
We first describe an inside-outside basis that is
an extension of the one in the previous section. In-
side elements are the same, namely Z = Z1 U Z2,
where Z1 are strings (x) and Z2 are composed
strings (x, x0). The set of outside contexts O1 is
the set containing elements (x; z), defined as be-
fore. The set of composed outside contexts has el-
ements (x, x0; z), and (x; z0, z), where x, z E Σ*
and x0, z0 E Σ+. These outside contexts keep an
operation open in one of the sides. For example, if
we consider (x; z0, z) and insert a string y, we obtain
x(y, z0)z, where we use (y, z0) to explicitly denote a
composed inside string. We will use O = O1 U O2.
In this section, we will assume that Z and O are
finite and closed. By closed, we mean that:
</bodyText>
<listItem confidence="0.999821666666667">
• (x) E Z (x1, x2) E Z for x = x1x2
• (x1, x2) E Z x1 E Z, x2 E Z
• (x; z) E O (x1, x2; z) E O for x = x1x2
• (x; z) E O (x; z1, z2) E O for z = z1z2
• (x1, x2; z) E O (x2) E Z
• (x; z1, z2) E O (z1) E Z
</listItem>
<bodyText confidence="0.9983158">
We will consider a Hankel matrix H E RO×I.
Some entries of this matrix will correspond to ob-
servable quantities. Specifically, for any string x E
Z1 for which we know p(x) we can define the fol-
lowing observable constraint:
</bodyText>
<equation confidence="0.997684">
p(x) = H((A; A), (x)) (24)
</equation>
<bodyText confidence="0.999329833333333">
The rest of entries of H correspond to a string
with an inside-outside cut, and these are not ob-
servable. Our method will infer the values of these
entries. The following constraints will ensure that
the matrix H is a well defined Hankel matrix for
WCFG:
</bodyText>
<page confidence="0.986968">
628
</page>
<listItem confidence="0.98977">
• Hankel constraints: ∀ hx; zi ∈ O, (y1, y2) ∈ I
</listItem>
<equation confidence="0.9361636">
H(hx; zi, (y1, y2)) = H(hx, y1; zi, (y2))
= H(hx; y2, zi, (y1)) (25)
• Inside constraints: ∀ o ∈ O, (x) ∈ I
H(o, (x)) = � H(o, (x1, x2)) (26)
2=2122
</equation>
<listItem confidence="0.984471">
• Outside constraints: ∀ hx; zi ∈ O, i ∈ I
</listItem>
<bodyText confidence="0.999967833333333">
Intuitively, we look for H that agrees with the ob-
servable statistics and satisfies the inside-outside
constraints. µ is a parameter of the method that con-
trols the degree of error in fitting the observables z.
The kHk2 ≤ 1 is satisfied by any Hankel matrix
derived from a true distribution, and is used to avoid
incoherent solutions.
The above optimization problem, however, is
computationally hard because of the rank objective.
We employ a common relaxation of the rank objec-
tive, based on the nuclear norm as in (Balle et al.,
2012). The optimization is:
</bodyText>
<equation confidence="0.9088828">
� H(hx1, x2; zi, i) minimize kHk*
H(hx; zi, i) = H
2=2122
+ � H(hx; z1, z2i, i) (27)
z=z1z2
</equation>
<bodyText confidence="0.928979416666667">
Constraint (25) states that composition operations
that result in the same structure should have the same
value. Constraints (26) and (27) ensure that the val-
ues in the Hankel follow the inside-outside recur-
sions that define the computations of a WCFG func-
tion. The following lemma formalizes this concept.
Let HE be the sub-block of H restricted to O1 ×I1,
i.e. without compositions.
Lemma 2. If H satisfies constraints (25),(26) and
(27), and if rank(H) = rank(HE) then there exists
a WCFG that generates HE.
See proof in Section A.3.
</bodyText>
<subsectionHeader confidence="0.983575">
4.2 Convex Optimization
</subsectionHeader>
<bodyText confidence="0.9990203">
We now present the core optimization program be-
hind our method. Let vec(H) be a vector in 8|O|·|z|
corresponding to all coefficients of H in column
vector form. Let O be a matrix such that O ·
vec(H) = z represents the observation constraints.
For example, if i-th row of O corresponds to the
Hankel coefficient H(hA; Ai, (x)) then z(i) = p(x).
Let K be a matrix such that K · vec(H) = 0 rep-
resents the constraints (25), (26) and (27).
The optimization problem is:
</bodyText>
<equation confidence="0.999401666666667">
subject to kO · vec(H) − zk2 ≤ µ (29)
K · vec(H) = 0
kHk2 ≤ 1.
</equation>
<bodyText confidence="0.9998565">
To optimize (29) we employ a projected gradient
strategy, similar to the FISTA scheme proposed by
Beck and Teboulle (2009). The method alternates
between separate projections for the observable con-
straints, the E2 norm, the inside-outside constraints,
and the nuclear norm. Of these, the latter two are the
most expensive.
Elsewhere, we develop theoretical properties of
the optimization (28) applied to finite-state transduc-
tions (Bailly et al., 2013). One can prove that there is
theoretical identifiability of the rank and the param-
eters of an FST distribution, using a rank minimiza-
tion formulation. However, this problem is NP-hard,
and it remains open whether there exists a polyno-
mial method with identifiability results. These re-
sults should generalize to WCFG.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999624">
In this section we describe some experiments with
the learning algorithms for WCFG. Our goal is
to verify that the algorithms can learn some basic
context-free languages, and to study the possibility
of using them on real data.
</bodyText>
<equation confidence="0.9694932">
minimize rank(H)
H
subject to kO · vec(H) − zk2 ≤ µ
K · vec(H) = 0
kHk2 ≤ 1.
</equation>
<subsectionHeader confidence="0.972271">
5.1 Synthetic Experiments
</subsectionHeader>
<bodyText confidence="0.9932742">
We performed experiments on synthetic data, ob-
tained by choosing a PCFG with random parameters
(28) (∈ [0,1]), with a normalization step in order to get
a probability distribution. We built the Hankel ma-
trix from the inside basis {(x)}2EE and outside basis
</bodyText>
<page confidence="0.993551">
629
</page>
<figure confidence="0.99512475">
KL divergence
KL divergence
100 1000 10000 100000 1e+06
Sample size
</figure>
<figureCaption confidence="0.93481425">
Figure 1: KL divergence for spectral and EM methods,
unsupervised and supervised, for different sizes of learn-
ing sample, on log-log scales. Results are averages over
50 random target PCFG with 2 states and 2 symbols.
</figureCaption>
<figure confidence="0.9873465">
1000 10000 100000 1e+06 1e+07 1e+08 1e+09
Sample size
</figure>
<figureCaption confidence="0.910768">
Figure 2: KL divergence for unsupervised and supervised
spectral methods, for different sizes of learning sample,
on log-log scales. Results are averages over 50 random
target PCFG with 3 states and 6 symbols.
</figureCaption>
<figure confidence="0.997860545454546">
0.0001
0.001
1e-05
1e-06
0.01
0.1
1
Unsupervised Spectral
Supervised Spectral
Unsupervised EM
Supervised EM
0.0001
0.001
1e-05
1e-06
1e-07
1e-08
0.01
0.1
Unsupervised Spectral
Supervised Spectral
{(A; A)} U {(x; A), (A; x)}xEE. The composed in-
</figure>
<bodyText confidence="0.994062666666667">
sides for the operator matrix are thus {(x, y)}x,yEE.
The matrix in the optimizer has the following struc-
ture
</bodyText>
<equation confidence="0.984198666666667">
(y) ··· (y, z)
⎛
(A; A) (A; y; A) ··· (A;y, z; A)
(x; A)(x; y; A) · · · (x; y, z; A)
⎜ ⎜
(A; x)(A; y; x) · · · (A; y, z; x)
⎜ ⎝... · · · · · · · · ·
The constraints we use are:
K ={H((x; y; A)) = H((A; x; y))}x,yEEU
{H((A; x; y)) = H((A; x, y; A))}x,yEEU
{H((x; y; A)) = H((A; x, y; A))}x,yEE
and
O ={H((A; x; A)) = pS(x)}xEE U
{H((A; x; y)) = pS(xy)}x,yEE U
{H((x; y, z; A)) + H((A; x, y; z)) = pS(xyz)}x,y,zEE
</equation>
<bodyText confidence="0.999761588235294">
We use pS to denote the empirical distribution.
Those are simplified versions of the Hankel, inside,
outside and observation constraints. The set O is
built from the following remarks: (1) (xy) = (x, y)
and (2) (xyz) = (xy, z)+(x, yz). The method uses
statistics for sequences up to length 3.
The algorithm we use for the unsupervised spec-
tral method is a simplified version: we use alter-
natively a hard projection on the constraints (by
projecting iteratively on each constraint), and a
thresholding-shrinkage operation for the target di-
mension. We use the same trick as FISTA for the
update. We finally use the regular spectral method
on this matrix to get our model.
We compare this method with an unsupervised
EM, and also with supervised versions of spectral
method and EM. We compare the accuracy of the
different models in terms of KL-divergence for se-
quences up to length 10. We run 50 optimization
steps for the unsupervised spectral method, and 200
iterations for the EM methods. Figure 1 shows the
results, corresponding the the geometric mean over
50 experiments on random targets of 2 symbols and
2 states.
For sample size greater than 105, the unsupervised
spectral method seems to provide better solutions
than both EM and supervised EM. The solution, in
terms of KL-divergence, is comparable to the one
obtained with the supervised spectral method. The
computation time of unsupervised spectral method
is almost constant w.r.t. the sample size, around
1.67s, while computation time of unsupervised EM
(resp. supervised EM) is 6.103s (resp. 2.104s) for
sample size 106.
</bodyText>
<figureCaption confidence="0.876392">
Figure 2 presents learnings curve for random tar-
gets with 3 states and 6 symbols. One can see that,
for big sample sizes (109), the unsupervised spectral
method is losing accuracy compared to the super-
vised method. This is due to a lack of information,
</figureCaption>
<figure confidence="0.945465">
H=
⎞
⎠⎟⎟⎟
630
0 0.01 0.02 0.03 0.04 0.05
Basis factor
</figure>
<figureCaption confidence="0.9995675">
Figure 3: Learning errors for different models in terms of
the size of the basis.
</figureCaption>
<bodyText confidence="0.9647515">
and could be overcome by considering a greater ba-
sis (e.g. inside sequences up to length 2 or 3).
</bodyText>
<subsectionHeader confidence="0.994783">
5.2 Dyck Languages
</subsectionHeader>
<bodyText confidence="0.986135625">
We now present experiments using the following
PCFG:
5-* 5 5 (0.2)  |a 5 b (0.4)  |a b (0.4)
This PCFG generates a probabilistic version of the
well-known Dyck language or balanced parenthesis
language, an archetypical context-free language.
We do experiments with the following models and
algorithms:
</bodyText>
<listItem confidence="0.9996144">
• WFA: a Weighted Finite Automata learned us-
ing spectral methods as described in (Luque et
al., 2012). Parameters: number of states and
size of basis.
• Supervised Spectral: a WCFG learned from
</listItem>
<bodyText confidence="0.938772571428571">
structured strings using the algorithm of sec-
tion 3.2. We choose as basis the most frequent
insides and outsides observed in the training
data. The size of the basis is determined by a
parameter f called the basis factor, that deter-
mines the proportion of total insides and out-
sides that will be in the basis.
</bodyText>
<listItem confidence="0.98463325">
• Unsupervised Spectral: a WCFG learned from
strings using the algorithm of Section 4. The
basis is like in the supervised case, but since
context-free cuts in the strings are not observed,
</listItem>
<table confidence="0.992734416666667">
basis size of H obs. i/o ctr.
1 x 11 39 x 159 34 162
6 x 14 1,163 x 764 146 6,360
12 x 18 4,462 x 2,239 322 25,374
18 x 22 9,124 x 4,149 479 52,524
24 x 26 15,755 x 6,858 657 89,718
30 x 29 19,801 x 8,545 769 112,374
36 x 34 27,989 x 11,682 916 156,690
42 x 37 3,638 x 15,026 1,035 200,346
48 x 41 45,192 x 18,235 1,157 244,398
54 x 45 53,741 x 21,196 1,281 284,466
60 x 48 60,844 x 23,890 1,382 318,354
</table>
<tableCaption confidence="0.995206">
Table 1: Problem sizes for the WSJ10 training corpus.
</tableCaption>
<table confidence="0.997819714285714">
basis / n 5 10 15 20
1 x 11 1.265 10−3
6 x 14 7.06 10−4 6.92 10 −4
12 x 18 7.30 10−4 6.28 10 −4 6.01 10 −4
18 x 22 7.31 10−4 6.29 10 −4 5.84 10 −4 5.59 10 −4
24 x 26 7.35 10−4 6.39 10 −4 5.88 10 −4 5.31 10 −4
30 x 29 7.34 10−4 6.41 10 −4 5.86 10 −4 5.30 10 −4
</table>
<tableCaption confidence="0.988325">
Table 2: Experiments with the unsupervised spectral
method on the WSJ10 corpus. Results are in terms of
expected Ll on the training set, for different basis and
numbers of states.
</tableCaption>
<bodyText confidence="0.9979372">
all possible inside and outsides of the sample
(i.e. all possible substrings and contexts) are
considered.
We generate a training set by sampling 4,000
strings from the target PCFG and counting the rel-
ative frequency of each. For the supervised model,
we generate strings paired with their context-free
derivation. To measure the quality of the learned
models, we use the L1 distance to the target distri-
bution over a fixed set of strings Ern, for n = 7.1
Figure 3 shows the results for the different mod-
els and for different basis sizes (in terms of the basis
factor f). Here we can clearly see that the WCFG
models, even the unsupervised one, outperform the
WFA in reproducing the target distribution.
</bodyText>
<subsectionHeader confidence="0.99878">
5.3 Natural Language Experiments
</subsectionHeader>
<bodyText confidence="0.9999955">
Now we present some preliminar tests using natural
language data. For these tests, we used the WSJ10
subset of the Penn Treebank, as Klein and Manning
(2002). This dataset consists of the sentences of
length :5 10 after filtering punctuation and currency.
We removed lexical items and mapped the POS tags
</bodyText>
<footnote confidence="0.6405065">
1Given two functions f1 and f2 over strings, the L1 distance
is the sum of the absolute difference over all strings in a set:
</footnote>
<figure confidence="0.950842">
E. |f1(x) − f2(x)|.
0.8
0.6
0.4
0.2
1.6
1.4
1.2
0
1
Spectral WFA
Unsupervised Spectral
Supervised Spectral
L1 distance
</figure>
<page confidence="0.995609">
631
</page>
<bodyText confidence="0.999964333333334">
to the Universal Part-of-Speech Tagset (Petrov et al.,
2012), reducing the alphabet to a set of 11 symbols.
Table 1 shows the size of the problem for differ-
ent basis sizes. As described in the previous sub-
section for the unsupervised case, we obtain the ba-
sis by taking the most frequent observed substrings
and contexts. We then compute all yields that can
be generated with this basis, and close the basis to
include all possible insides and outsides with oper-
ations completions, such that we create a Hankel as
described in Section 4.1. Table 1 shows, for each
base, the size of H we induce, the number of ob-
servable constraints (i.e. sentences we train from),
and the number of inside-outside constraints.
With the current implementation of the optimizer
we were only able to run the unsupervised learning
for small basis sizes. Table 2 shows the expected L1
on training data. For a fixed basis, as we increase
the number of states we see that the error decreases,
showing that the method is inducing a Hankel matrix
that explains the observable statistics.
</bodyText>
<sectionHeader confidence="0.995659" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999624">
We have presented a novel approach for unsuper-
vised learning of WCFG. Our method combines in-
gredients of spectral learning with low-rank convex
optimization methods.
Our method optimizes over a matrix that, even if it
grows polynomially with respect to the size of train-
ing, results in a large problem. To scale the method
to learn languages of the complexity of natural lan-
guages we would need to identify optimization algo-
rithms specially suited for this problem.
</bodyText>
<sectionHeader confidence="0.506125" genericHeader="method">
A Proofs
</sectionHeader>
<subsectionHeader confidence="0.97368">
A.1 Proof of Inside-Outside Eq. 16 and 17
</subsectionHeader>
<bodyText confidence="0.997685">
For the inside function, the base case is trivial. By
induction:
</bodyText>
<equation confidence="0.993293777777778">
βG(x)[i] = 1: A(βG(x1) ® βG(x2))[i]
x=x1x2
=1: A[i,j,k] - βG(x1)[j] - βG(x2)[k]
j,k∈V
x=x1x2
=1: wR(i —* j k) - β¯o(j?=�x1)- β¯ �G(k ? =� x2)
j,k∈V
x=x1x2
β¯G(i?=�x)
</equation>
<bodyText confidence="0.95218475">
For the outside function, let ei be an n-
dimensional vector with coordinate i to 1 and the
rest to 0. We reformulate the mapping as:
αG(x; z)&gt;ei = α¯o(x; i; z) (30)
The base case is trivial by definitions. We use the
property of Kronecker products that (v ® In)v0 =
(v ® v0) and (In ® v)v0 = (v0 ® v) for v, v0 E Rn.
We first look at one of the terms of αG(x; z)&gt;ei:
</bodyText>
<equation confidence="0.9958915">
αG(x1; z)&gt;A(βG(x2) ® In)ei
= αG(x1; z)&gt;A(βG(x2) ® ei)
1: = (αG(x1; z)&gt;ej) - A[j, k, i] - βG(x2)[k]
j,k∈V
1: = α¯G(x1; j; z) - wR(j —* k i) - β¯0(k ?=� x2)
j,k∈V
</equation>
<bodyText confidence="0.995947">
Applying the distributive property in αG(x; z)&gt;ei it
is easy to see that all terms are mapped to the corre-
sponding term in α¯o(x; i; z).
</bodyText>
<subsectionHeader confidence="0.798822">
A.2 Proof of Lemma 1
</subsectionHeader>
<bodyText confidence="0.9936501">
Let G0 = (α0?, 1,30 1, A0) be a WCFG for f that in-
σ
duces a rank factorization H = P0S0. We first show
that there exists an invertible matrix M that changes
the basis of the operators of G into those of G0.
Define M = S0S+ and note that P+P0S0S+ =
P+HS+ = I implies that M is invertible with
M−1 = P+P0. We now check that the operators
of G correspond to the operators of G0 under this
change of basis. First we see that
</bodyText>
<equation confidence="0.96371925">
A = P+HA(S ® S)+
= P+P0A0(S0 ® S0)(S ® S)+
= M−1A0(S0S+ ® S0S+)
= M−1A0(M ® M) .
</equation>
<bodyText confidence="0.884349">
Now, since h? = α0&gt;? S0 and hσ = P0β0σ , it follows
that α&gt;? = α0 &gt;M and �σ = M−1�0 σ.
?
Finally we check that G and G0 compute the
same function, namely f(o, i) = αG(o)&gt;βG(i) =
αG,(o)&gt;βG,(i). We first see that βG(x) =
</bodyText>
<page confidence="0.970018">
632
</page>
<equation confidence="0.960826230769231">
M−1βG0(x):
βG(σ) = )3σ = M−1�0 (31)
σ
Lemma 3. One has that βG(x) = h(x), and
βG(x1, x2) = h(x1,x2).
XβG(x) = A(βG(x1) ® βG(x2)) (32)
x=x1x2
X= M−1A0(M ® M)(βG(x1) ® βG(x2))
x=x1x2
= M−1 X A0(MβG(x1) ® MβG(x2))
x=x1x2
= M−1 X A0(βG0(x1) ® βG0(x2))
x=x1x2
</equation>
<bodyText confidence="0.8132435">
It can also be shown that αG(x; z)&gt; =
αG0(x; z)&gt;M. One must see that in any term:
</bodyText>
<equation confidence="0.9998665">
αG(x1;z)&gt;A(βG(x2) ® In) (33)
= αG(x1; z)&gt;M−1A0(M ® M)(βG(x2) ® In)
= αG0(x1; z)&gt;A0(MβG(x2) ® MIn)
= αG0(x1; z)&gt;A0(βG0(x2) ® In)M
</equation>
<bodyText confidence="0.863226">
and the relation follows. Finally:
</bodyText>
<equation confidence="0.993004">
αG(x;z)&gt;βG(y) (34)
= αG0(x; z)&gt;MM−1βG0(y)
= αG0(x; z)&gt;βG0(y)
</equation>
<subsectionHeader confidence="0.481296">
A.3 Proof of Lemma 2
</subsectionHeader>
<bodyText confidence="0.998801">
We will use the following sub-blocks of H:
</bodyText>
<listItem confidence="0.996715153846154">
• Hε is the sub-block restricted to O1 x I1, i.e.
without compositions.
• HA is the sub-block restricted to O1 x I2, i.e.
inside compositions.
• H0A is the sub-block restricted to O2 x I1, i.e.
outside compositions.
• h&gt;? E RI1 is the row of Hε for (λ; λ).
• h(x) E RO1 is the column of Hε for (x).
• h(x1,x2) E RO1 is the column of HA for
(x1, x2).
• h0hx;zi E RI1 is the row of hε for (x; z).
• h0hx1,x2;zi and h0hx;z1,z2i be the rows in RI1 of
h0A for (x1, x2; z) and (x; z1, z2)).
</listItem>
<bodyText confidence="0.985853416666667">
One supposes that rank(Hε) = rank(H). We de-
fine G as
α&gt;? = h&gt;? H* , βa = h(a), A = HA(HE ® H+ )
Proof. By induction. For sequences of size 1, one
has βG(x) = βx = h(x). For the recursive case,
let e(x) be a vector in RI1 with 1 in the coordinate
of (x) in Hε. Let e(x,y) be a vector in RI2 with 1
in the coordinate of (x, y) in HA. For βG(x, y),
one has HE βG(x) = e(x), and HE βG(y) =
e(y), thus HE βG(x) ® H* βG(y) = e(x,y) and
HA(HE βG(x) ® H* βG(y)) = h(x,y). Finally,
one has that βG(x) = Px=x1x2 βG(x1, x2) =
</bodyText>
<equation confidence="0.6980425">
P
x=x1x2 h(x1,x2) = h(x1x2x3) by the equation
</equation>
<bodyText confidence="0.8451478">
(26).
One has a symmetric result for outside vectors. We
define G0 as
α&gt;? = h&gt;? , βa = H* h(a), A = H* HA
Lemma 4. One has that αG0((x; z))&gt; =
</bodyText>
<equation confidence="0.991347">
h0hx;zi, αG0((x1, x2; z))&gt; = h0hx1,x2;zi and
αG0((x; z1, z2))&gt; = h0hx;z1,z2i.
</equation>
<bodyText confidence="0.835449842105263">
Proof. (Sketch) Equation (31) is used in the same
way than (27) before. Equation (25) is used to en-
sure a link between H0A and HA.
Let g be the mapping computed by G and
G0. One has that g(o, i) = αG0(o)&gt;βG0(i) =
αG(o)&gt;βG(i) = αG0(o)&gt;H�ε βG(i) = Hε(o, i).
Acknowledgements We are grateful to Borja Balle
and the anonymous reviewers for providing us with help-
ful comments. This work was supported by a Google
Research Award, and by projects XLike (FP7-288342),
ERA-Net CHISTERA VISEN, TACARDI (TIN2012-
38523-C02-02), BASMATI (TIN2011-27479-C04-03),
SGR-GPLN (2009-SGR-1082) and SGR-LARCA (2009-
SGR-1428). Xavier Carreras was supported by the
Ram´on y Cajal program of the Spanish Government
(RYC-2008-02223). Franco M. Luque was supported by
the National University of C´ordoba and by a Postdoc-
toral fellowship of CONICET, Argentinian Ministry of
Science, Technology and Productive Innovation.
</bodyText>
<page confidence="0.999296">
633
</page>
<sectionHeader confidence="0.983138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847371428572">
Pieter Adriaans, Marten Trautwein, and Marco Vervoort.
2000. Towards high speed grammar induction on large
text corpora. In SOFSEM 2000: Theory and Practice
of Informatics, pages 173–186. Springer.
Rapha¨el Bailly, Franois Denis, and Liva Ralaivola. 2009.
Grammatical inference as a principal component anal-
ysis problem. In L´eon Bottou and Michael Littman,
editors, Proceedings of the 26th International Confer-
ence on Machine Learning, pages 33–40, Montreal,
June. Omnipress.
Rapha¨el Bailly, Amaury Habrard, and Franc¸ois Denis.
2010. A spectral approach for probabilistic grammat-
ical inference on trees. In Proceedings of the 21st
International Conference Algorithmic Learning The-
ory, Lecture Notes in Computer Science, pages 74–88.
Springer.
Rapha¨el Bailly, Xavier Carreras, and Ariadna Quattoni.
2013. Unsupervised spectral learning of finite-state
transducers. In Advances in Neural Information Pro-
cessing Systems 26.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547–550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2011. A spectral learning algorithm for finite state
transducers. In Proceedings of ECML PKDD, pages
156–171.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models: A
new insight into spectral learning. In John Langford
and Joelle Pineau, editors, Proceedings of the 29th In-
ternational Conference on Machine Learning (ICML-
2012), ICML ’12, pages 1879–1886, New York, NY,
USA, July. Omnipress.
Amir Beck and Marc Teboulle. 2009. A fast iter-
ative shrinkage-thresholding algorithm for linear in-
verse problems. SIAM J. Img. Sci., 2(1):183–202,
March.
Stanley F Chen. 1995. Bayesian grammar induction for
language modeling. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 228–235. Association for Computational Lin-
guistics.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In Proceedings of the 2001 workshop on
Computational Natural Language Learning-Volume 7,
page 13. Association for Computational Linguistics.
Alexander Clark. 2007. Learning deterministic context
free grammars: The omphalos competition. Machine
Learning, 66(1):93–110.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564–
572, Los Angeles, California, June. Association for
Computational Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 223–231,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with spec-
tral learning of latent-variable pcfgs. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 148–157, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Matthew Gormley and Jason Eisner. 2013. Nonconvex
global optimization for latent-variable models. In Pro-
ceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL), Sofia, Bulgaria,
August. 11 pages.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden markov models.
In Proceedings of the Annual Conference on Compu-
tational Learning Theory (COLT).
Daniel Hsu, Sham Kakade, and Percy Liang. 2012.
Identifiability and unmixing of latent parse trees. In
P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou,
and K.Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 25, pages 1520–1528.
Dan Klein and Christopher D Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
128–135. Association for Computational Linguistics.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
Grammatical Inference: Algorithms and Applications,
pages 84–96. Springer.
Karim Lari and Steve J Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer speech &amp; language,
4(1):35–56.
Percy Liang, Slav Petrov, Michael I Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In EMNLP-CoNLL, pages 688–
697.
</reference>
<page confidence="0.98586">
634
</page>
<reference confidence="0.999685583333333">
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for non-
deterministic dependency parsing. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
409–419, Avignon, France, April. Association for
Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128–
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Kewei Tu and Vasant Honavar. 2008. Unsupervised
learning of probabilistic context-free grammar using
iterative biclustering. In Grammatical Inference: Al-
gorithms and Applications, pages 224–237. Springer.
Menno Van Zaanen. 2000. Abl: Alignment-based learn-
ing. In Proceedings of the 18th conference on Compu-
tational linguistics-Volume 2, pages 961–967. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.998787">
635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450385">
<title confidence="0.981632">Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion</title>
<author confidence="0.489113">M</author>
<affiliation confidence="0.875479">Polit`ecnica de Catalunya Nacional de C´ordoba and CONICET</affiliation>
<address confidence="0.98549">Barcelona, 08034 X500HUA C´ordoba, Argentina</address>
<email confidence="0.989829">rbailly,carreras,aquattoni@lsi.upc.edufrancolq@famaf.unc.edu.ar</email>
<abstract confidence="0.9938221">We derive a spectral method for unsupervised learning of Weighted Context Free Grammars. We frame WCFG induction as finding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter Adriaans</author>
<author>Marten Trautwein</author>
<author>Marco Vervoort</author>
</authors>
<title>Towards high speed grammar induction on large text corpora.</title>
<date>2000</date>
<booktitle>In SOFSEM 2000: Theory and Practice of Informatics,</booktitle>
<pages>173--186</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1307" citStr="Adriaans et al., 2000" startWordPosition="191" endWordPosition="194">espect to the nuclear norm of the Hankel matrix. 1 Introduction Weighted Context Free Grammars (WCFG) define an important class of languages. Their expressivity makes them good candidates for modeling a wide range of natural language phenomena. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) pr</context>
</contexts>
<marker>Adriaans, Trautwein, Vervoort, 2000</marker>
<rawString>Pieter Adriaans, Marten Trautwein, and Marco Vervoort. 2000. Towards high speed grammar induction on large text corpora. In SOFSEM 2000: Theory and Practice of Informatics, pages 173–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franois Denis Rapha¨el Bailly</author>
<author>Liva Ralaivola</author>
</authors>
<title>Grammatical inference as a principal component analysis problem.</title>
<date>2009</date>
<booktitle>In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th International Conference on Machine Learning,</booktitle>
<pages>33--40</pages>
<publisher>Omnipress.</publisher>
<location>Montreal,</location>
<marker>Rapha¨el Bailly, Ralaivola, 2009</marker>
<rawString>Rapha¨el Bailly, Franois Denis, and Liva Ralaivola. 2009. Grammatical inference as a principal component analysis problem. In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th International Conference on Machine Learning, pages 33–40, Montreal, June. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amaury Habrard Rapha¨el Bailly</author>
<author>Franc¸ois Denis</author>
</authors>
<title>A spectral approach for probabilistic grammatical inference on trees.</title>
<date>2010</date>
<booktitle>In Proceedings of the 21st International Conference Algorithmic Learning Theory, Lecture Notes in Computer Science,</booktitle>
<pages>74--88</pages>
<publisher>Springer.</publisher>
<marker>Rapha¨el Bailly, Denis, 2010</marker>
<rawString>Rapha¨el Bailly, Amaury Habrard, and Franc¸ois Denis. 2010. A spectral approach for probabilistic grammatical inference on trees. In Proceedings of the 21st International Conference Algorithmic Learning Theory, Lecture Notes in Computer Science, pages 74–88. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras Rapha¨el Bailly</author>
<author>Ariadna Quattoni</author>
</authors>
<title>Unsupervised spectral learning of finite-state transducers.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26.</booktitle>
<marker>Rapha¨el Bailly, Quattoni, 2013</marker>
<rawString>Rapha¨el Bailly, Xavier Carreras, and Ariadna Quattoni. 2013. Unsupervised spectral learning of finite-state transducers. In Advances in Neural Information Processing Systems 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<editor>D. H. Klatt and J. J. Wolf, editors,</editor>
<contexts>
<context position="1503" citStr="Baker, 1979" startWordPosition="224" endWordPosition="225">e range of natural language phenomena. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques </context>
<context position="7865" citStr="Baker, 1979" startWordPosition="1359" endWordPosition="1360"> R, with w*(i) being the weight of starting a derivation with non-terminal i. • wT : V × E → R, with wT(i → Q) being the weight of rule rewriting i into Q. • wR : V × V × V → R, with wR(i → j k) being the weight of rewriting i into j k. A WCFG G computes a function gG : E+ → R defined as g o(x) = � w*(i) Qo(i *=⇒ x) , (1) iEV 625 where we define the inside function β�G�: V ×E+ → R recursively: β�0(i ?=⇒ σ) = wT (i → σ) (2) wR(i → j k) (3) β�o(j ?=⇒ x1)β�G(k ?=⇒ x2) , where in the second case we assume |x |&gt; 1. The inside function β�G(i ?=⇒ x) exploits the fundamental inside recursion in WCFG (Baker, 1979; Lari and Young, 1990). We will find useful to define the outside function α�G� : E? × V × E? → R defined recursively as: α�o(λ; i; λ) = w?(i) (4) Finally, we note that Probabilistic Context Free Grammars (PCFG) are a special case of WCFG where: w?(i) is the probability to start a derivation with non-terminal i; wR(i → j k) is the conditional probability of rewriting nonterminal i into j and k; wT(i → σ) is the probability of rewriting i into symbol σ; Pi w?(i) = 1; and for each i ∈ V , Pj,k wR(i → j k) + Pσ wT (i → σ) = 1. Under these conditions the function gG� is a probability distibution </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Borja Balle</author>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
</authors>
<title>A spectral learning algorithm for finite state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of ECML PKDD,</booktitle>
<pages>156--171</pages>
<contexts>
<context position="2706" citStr="Balle et al., 2011" startWordPosition="417" endWordPosition="420">oit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over E*, the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the </context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2011</marker>
<rawString>Borja Balle, Ariadna Quattoni, and Xavier Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of ECML PKDD, pages 156–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Borja Balle</author>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
</authors>
<title>Local loss optimization in operator models: A new insight into spectral learning.</title>
<date>2012</date>
<booktitle>In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML2012), ICML ’12,</booktitle>
<pages>1879--1886</pages>
<publisher>Omnipress.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="4492" citStr="Balle et al. (2012)" startWordPosition="701" endWordPosition="704">lgorithm based on a generalization of the method of moments for these restricted subclasses. Thus one open direction for spectral research consists on defining subclasses of context free languages that can be learned (in the strong sense) from observations of yields. Yet, an alternative research direction is to consider learnability in the weaker sense. In this paper we take the second road, and focus on the problem of approximating the distribution over yields generated by a WCFG. Our main contribution is to present a spectral algorithm for unsupervised learning of WCFG. Following ideas from Balle et al. (2012), the algorithm is framed as a convex optimization where we search for a low-rank matrix satisfying two types of constraints: (1) Constraints derived from observable statistics over yields; and (2) Constraints derived from certain recurrence relations satisfied by a WCFG. Our derivations of the learning algorithm illustrate the main ingredients behind the spectral approach to learning functions over E* which are: (1) to exploit the recurrence relations satisfied by the target family of functions and (2) provide algebraic formulations of these relations. We alert the reader that although we are</context>
<context position="18369" citStr="Balle et al., 2012" startWordPosition="3493" endWordPosition="3496">(o, (x)) = � H(o, (x1, x2)) (26) 2=2122 • Outside constraints: ∀ hx; zi ∈ O, i ∈ I Intuitively, we look for H that agrees with the observable statistics and satisfies the inside-outside constraints. µ is a parameter of the method that controls the degree of error in fitting the observables z. The kHk2 ≤ 1 is satisfied by any Hankel matrix derived from a true distribution, and is used to avoid incoherent solutions. The above optimization problem, however, is computationally hard because of the rank objective. We employ a common relaxation of the rank objective, based on the nuclear norm as in (Balle et al., 2012). The optimization is: � H(hx1, x2; zi, i) minimize kHk* H(hx; zi, i) = H 2=2122 + � H(hx; z1, z2i, i) (27) z=z1z2 Constraint (25) states that composition operations that result in the same structure should have the same value. Constraints (26) and (27) ensure that the values in the Hankel follow the inside-outside recursions that define the computations of a WCFG function. The following lemma formalizes this concept. Let HE be the sub-block of H restricted to O1 ×I1, i.e. without compositions. Lemma 2. If H satisfies constraints (25),(26) and (27), and if rank(H) = rank(HE) then there exists </context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2012</marker>
<rawString>Borja Balle, Ariadna Quattoni, and Xavier Carreras. 2012. Local loss optimization in operator models: A new insight into spectral learning. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML2012), ICML ’12, pages 1879–1886, New York, NY, USA, July. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Beck</author>
<author>Marc Teboulle</author>
</authors>
<title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems.</title>
<date>2009</date>
<journal>SIAM J. Img. Sci.,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="19695" citStr="Beck and Teboulle (2009)" startWordPosition="3735" endWordPosition="3738">timization program behind our method. Let vec(H) be a vector in 8|O|·|z| corresponding to all coefficients of H in column vector form. Let O be a matrix such that O · vec(H) = z represents the observation constraints. For example, if i-th row of O corresponds to the Hankel coefficient H(hA; Ai, (x)) then z(i) = p(x). Let K be a matrix such that K · vec(H) = 0 represents the constraints (25), (26) and (27). The optimization problem is: subject to kO · vec(H) − zk2 ≤ µ (29) K · vec(H) = 0 kHk2 ≤ 1. To optimize (29) we employ a projected gradient strategy, similar to the FISTA scheme proposed by Beck and Teboulle (2009). The method alternates between separate projections for the observable constraints, the E2 norm, the inside-outside constraints, and the nuclear norm. Of these, the latter two are the most expensive. Elsewhere, we develop theoretical properties of the optimization (28) applied to finite-state transductions (Bailly et al., 2013). One can prove that there is theoretical identifiability of the rank and the parameters of an FST distribution, using a rank minimization formulation. However, this problem is NP-hard, and it remains open whether there exists a polynomial method with identifiability re</context>
</contexts>
<marker>Beck, Teboulle, 2009</marker>
<rawString>Amir Beck and Marc Teboulle. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Img. Sci., 2(1):183–202, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Bayesian grammar induction for language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>228--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1653" citStr="Chen, 1995" startWordPosition="247" endWordPosition="248"> it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our appro</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Stanley F Chen. 1995. Bayesian grammar induction for language modeling. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 228–235. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 workshop on Computational Natural Language Learning-Volume 7,</booktitle>
<pages>page</pages>
<contexts>
<context position="2204" citStr="Clark (2001" startWordPosition="327" endWordPosition="328">ian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking to</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proceedings of the 2001 workshop on Computational Natural Language Learning-Volume 7, page 13. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning deterministic context free grammars: The omphalos competition.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>66</volume>
<issue>1</issue>
<marker>Clark, 2007</marker>
<rawString>Alexander Clark. 2007. Learning deterministic context free grammars: The omphalos competition. Machine Learning, 66(1):93–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>David M Blei</author>
<author>Noah A Smith</author>
</authors>
<title>Variational inference for adaptor grammars. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>564--572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1719" citStr="Cohen et al., 2010" startWordPosition="257" endWordPosition="260">ent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. </context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 564– 572, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable pcfgs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>223--231</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2747" citStr="Cohen et al., 2012" startWordPosition="425" endWordPosition="428">ming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over E*, the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution over PCFG derivations </context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable pcfgs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 223–231, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable pcfgs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>148--157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="14808" citStr="Cohen et al., 2013" startWordPosition="2804" endWordPosition="2807">ction g∗, then the algorithm will compute a G that exactly computes g∗. In practice, we only have access to empirical estimates of the Hankel matrices. In this case, there exist PAC-style sample complexity bounds that state that gG will be a close approximation to g∗ (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010). The parameters of the algorithm are the basis and the dimension of the grammar n. One typically employs some validation strategy using held-out data. Empirically, the performance of these methods has been shown to be good, and similar to that of EM (Luque et al., 2012; Cohen et al., 2013). It is also important to mention that in the case that the target g∗ is a probability distribution, the function gG will be close to g∗, but it will only define a distribution in the limit: in practice it will not sum to one, and for some inputs it might return negative values. This is a practical difficulty of spectral methods, for example to apply evaluation metrics like perplexity which are only defined for distributions. 4 Unsupervised Learning of WCFG In the previous section we have exposed that if we have access to estimates of a Hankel matrix of a WCFG G, we can recover G. However, the</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle Ungar. 2013. Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 148–157, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gormley</author>
<author>Jason Eisner</author>
</authors>
<title>Nonconvex global optimization for latent-variable models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<volume>11</volume>
<pages>pages.</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1904" citStr="Gormley and Eisner (2013)" startWordPosition="281" endWordPosition="284">ammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its sy</context>
</contexts>
<marker>Gormley, Eisner, 2013</marker>
<rawString>Matthew Gormley and Jason Eisner. 2013. Nonconvex global optimization for latent-variable models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), Sofia, Bulgaria, August. 11 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Conference on Computational Learning Theory (COLT).</booktitle>
<contexts>
<context position="2644" citStr="Hsu et al., 2009" startWordPosition="405" endWordPosition="408">adratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over E*, the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently</context>
<context position="14474" citStr="Hsu et al., 2009" startWordPosition="2745" endWordPosition="2748">D of HB, HB = UAV &gt;. 4. Create a truncated rank n factorization of HB as PnSn, having Pn = UnAn and Sn = Vn &gt; , where we only consider the top n singular values/vectors of A, U, V . 5. Use Lemma 1 to compute G, using Pn and Sn. Because of Lemma 1, if B is complete and we have access to the true HB, h*, hQ, HA of a WCFG target function g∗, then the algorithm will compute a G that exactly computes g∗. In practice, we only have access to empirical estimates of the Hankel matrices. In this case, there exist PAC-style sample complexity bounds that state that gG will be a close approximation to g∗ (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010). The parameters of the algorithm are the basis and the dimension of the grammar n. One typically employs some validation strategy using held-out data. Empirically, the performance of these methods has been shown to be good, and similar to that of EM (Luque et al., 2012; Cohen et al., 2013). It is also important to mention that in the case that the target g∗ is a probability distribution, the function gG will be close to g∗, but it will only define a distribution in the limit: in practice it will not sum to one, and for some inputs it might return neg</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A spectral algorithm for learning hidden markov models. In Proceedings of the Annual Conference on Computational Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham Kakade</author>
<author>Percy Liang</author>
</authors>
<title>Identifiability and unmixing of latent parse trees.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1520--1528</pages>
<editor>In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="3263" citStr="Hsu et al. (2012)" startWordPosition="511" endWordPosition="514">Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over E*, the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution over PCFG derivations and their yields is not identifiable. 624 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624–635, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Although, for some simple grammar subclasses (e.g. independent left and right children), identification in the weaker sense (over the yields of the grammar) implies strong identification (e.g. over joint distribution of yields and derivations). In their paper, they propose a </context>
</contexts>
<marker>Hsu, Kakade, Liang, 2012</marker>
<rawString>Daniel Hsu, Sham Kakade, and Percy Liang. 2012. Identifiability and unmixing of latent parse trees. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1520–1528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1578" citStr="Klein and Manning, 2002" startWordPosition="234" endWordPosition="237"> at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spiri</context>
<context position="27141" citStr="Klein and Manning (2002)" startWordPosition="5071" endWordPosition="5074">s paired with their context-free derivation. To measure the quality of the learned models, we use the L1 distance to the target distribution over a fixed set of strings Ern, for n = 7.1 Figure 3 shows the results for the different models and for different basis sizes (in terms of the basis factor f). Here we can clearly see that the WCFG models, even the unsupervised one, outperform the WFA in reproducing the target distribution. 5.3 Natural Language Experiments Now we present some preliminar tests using natural language data. For these tests, we used the WSJ10 subset of the Penn Treebank, as Klein and Manning (2002). This dataset consists of the sentences of length :5 10 after filtering punctuation and currency. We removed lexical items and mapped the POS tags 1Given two functions f1 and f2 over strings, the L1 distance is the sum of the absolute difference over all strings in a set: E. |f1(x) − f2(x)|. 0.8 0.6 0.4 0.2 1.6 1.4 1.2 0 1 Spectral WFA Unsupervised Spectral Supervised Spectral L1 distance 631 to the Universal Part-of-Speech Tagset (Petrov et al., 2012), reducing the alphabet to a set of 11 symbols. Table 1 shows the size of the problem for different basis sizes. As described in the previous s</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128–135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational bayesian grammar induction for natural language. In Grammatical Inference: Algorithms and Applications,</title>
<date>2006</date>
<pages>84--96</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1678" citStr="Kurihara and Sato, 2006" startWordPosition="249" endWordPosition="252">l-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2006. Variational bayesian grammar induction for natural language. In Grammatical Inference: Algorithms and Applications, pages 84–96. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>1990</date>
<journal>Computer speech &amp; language,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1525" citStr="Lari and Young, 1990" startWordPosition="226" endWordPosition="229">tural language phenomena. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical prog</context>
<context position="7888" citStr="Lari and Young, 1990" startWordPosition="1361" endWordPosition="1364">) being the weight of starting a derivation with non-terminal i. • wT : V × E → R, with wT(i → Q) being the weight of rule rewriting i into Q. • wR : V × V × V → R, with wR(i → j k) being the weight of rewriting i into j k. A WCFG G computes a function gG : E+ → R defined as g o(x) = � w*(i) Qo(i *=⇒ x) , (1) iEV 625 where we define the inside function β�G�: V ×E+ → R recursively: β�0(i ?=⇒ σ) = wT (i → σ) (2) wR(i → j k) (3) β�o(j ?=⇒ x1)β�G(k ?=⇒ x2) , where in the second case we assume |x |&gt; 1. The inside function β�G(i ?=⇒ x) exploits the fundamental inside recursion in WCFG (Baker, 1979; Lari and Young, 1990). We will find useful to define the outside function α�G� : E? × V × E? → R defined recursively as: α�o(λ; i; λ) = w?(i) (4) Finally, we note that Probabilistic Context Free Grammars (PCFG) are a special case of WCFG where: w?(i) is the probability to start a derivation with non-terminal i; wR(i → j k) is the conditional probability of rewriting nonterminal i into j and k; wT(i → σ) is the probability of rewriting i into symbol σ; Pi w?(i) = 1; and for each i ∈ V , Pj,k wR(i → j k) + Pσ wT (i → σ) = 1. Under these conditions the function gG� is a probability distibution over E+. 2.2 WCFG in Al</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Karim Lari and Steve J Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer speech &amp; language, 4(1):35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical dirichlet processes.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="1698" citStr="Liang et al., 2007" startWordPosition="253" endWordPosition="256">still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical dirichlet processes. In EMNLP-CoNLL, pages 688– 697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franco M Luque</author>
<author>Ariadna Quattoni</author>
<author>Borja Balle</author>
<author>Xavier Carreras</author>
</authors>
<title>Spectral learning for nondeterministic dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>409--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="2726" citStr="Luque et al., 2012" startWordPosition="421" endWordPosition="424">mathematical programming to solve the resulting optimization. In spirit, the work by Clark (2001; 2007) is probably the most similar to our approach since both approaches share an algebraic view of the problem. In his case the key idea is to work with an algebraic representation of a WCFG. The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence. In the last years, multiple spectral learning algorithms have been proposed for a wide range of models (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010; Balle et al., 2011; Luque et al., 2012; Cohen et al., 2012). Since the spectral approach provides a good thinking tool to reason about distributions over E*, the question of whether they can be used for unsupervised learning of WCFG seems natural. Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach. In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging. Recently, Hsu et al. (2012) showed that the problem of recovering the joint distribution o</context>
<context position="14787" citStr="Luque et al., 2012" startWordPosition="2800" endWordPosition="2803">of a WCFG target function g∗, then the algorithm will compute a G that exactly computes g∗. In practice, we only have access to empirical estimates of the Hankel matrices. In this case, there exist PAC-style sample complexity bounds that state that gG will be a close approximation to g∗ (Hsu et al., 2009; Bailly et al., 2009; Bailly et al., 2010). The parameters of the algorithm are the basis and the dimension of the grammar n. One typically employs some validation strategy using held-out data. Empirically, the performance of these methods has been shown to be good, and similar to that of EM (Luque et al., 2012; Cohen et al., 2013). It is also important to mention that in the case that the target g∗ is a probability distribution, the function gG will be close to g∗, but it will only define a distribution in the limit: in practice it will not sum to one, and for some inputs it might return negative values. This is a practical difficulty of spectral methods, for example to apply evaluation metrics like perplexity which are only defined for distributions. 4 Unsupervised Learning of WCFG In the previous section we have exposed that if we have access to estimates of a Hankel matrix of a WCFG G, we can re</context>
<context position="24756" citStr="Luque et al., 2012" startWordPosition="4609" endWordPosition="4612"> 0.05 Basis factor Figure 3: Learning errors for different models in terms of the size of the basis. and could be overcome by considering a greater basis (e.g. inside sequences up to length 2 or 3). 5.2 Dyck Languages We now present experiments using the following PCFG: 5-* 5 5 (0.2) |a 5 b (0.4) |a b (0.4) This PCFG generates a probabilistic version of the well-known Dyck language or balanced parenthesis language, an archetypical context-free language. We do experiments with the following models and algorithms: • WFA: a Weighted Finite Automata learned using spectral methods as described in (Luque et al., 2012). Parameters: number of states and size of basis. • Supervised Spectral: a WCFG learned from structured strings using the algorithm of section 3.2. We choose as basis the most frequent insides and outsides observed in the training data. The size of the basis is determined by a parameter f called the basis factor, that determines the proportion of total insides and outsides that will be in the basis. • Unsupervised Spectral: a WCFG learned from strings using the algorithm of Section 4. The basis is like in the supervised case, but since context-free cuts in the strings are not observed, basis s</context>
</contexts>
<marker>Luque, Quattoni, Balle, Carreras, 2012</marker>
<rawString>Franco M. Luque, Ariadna Quattoni, Borja Balle, and Xavier Carreras. 2012. Spectral learning for nondeterministic dependency parsing. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Newark, Delaware, USA,</location>
<contexts>
<context position="1552" citStr="Pereira and Schabes, 1992" startWordPosition="230" endWordPosition="233">na. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likelihood estimation. In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints. They exploit techniques from mathematical programming to solve the result</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128– 135, Newark, Delaware, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="27598" citStr="Petrov et al., 2012" startWordPosition="5150" endWordPosition="5153">ments Now we present some preliminar tests using natural language data. For these tests, we used the WSJ10 subset of the Penn Treebank, as Klein and Manning (2002). This dataset consists of the sentences of length :5 10 after filtering punctuation and currency. We removed lexical items and mapped the POS tags 1Given two functions f1 and f2 over strings, the L1 distance is the sum of the absolute difference over all strings in a set: E. |f1(x) − f2(x)|. 0.8 0.6 0.4 0.2 1.6 1.4 1.2 0 1 Spectral WFA Unsupervised Spectral Supervised Spectral L1 distance 631 to the Universal Part-of-Speech Tagset (Petrov et al., 2012), reducing the alphabet to a set of 11 symbols. Table 1 shows the size of the problem for different basis sizes. As described in the previous subsection for the unsupervised case, we obtain the basis by taking the most frequent observed substrings and contexts. We then compute all yields that can be generated with this basis, and close the basis to include all possible insides and outsides with operations completions, such that we create a Hankel as described in Section 4.1. Table 1 shows, for each base, the size of H we induce, the number of observable constraints (i.e. sentences we train fro</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kewei Tu</author>
<author>Vasant Honavar</author>
</authors>
<title>Unsupervised learning of probabilistic context-free grammar using iterative biclustering.</title>
<date>2008</date>
<booktitle>In Grammatical Inference: Algorithms and Applications,</booktitle>
<pages>224--237</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1348" citStr="Tu and Honavar, 2008" startWordPosition="198" endWordPosition="201">matrix. 1 Introduction Weighted Context Free Grammars (WCFG) define an important class of languages. Their expressivity makes them good candidates for modeling a wide range of natural language phenomena. This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task. And while it is a well-studied problem, it is still to a great extent unsolved. Several methods for unsupervised learning of WCFG have been proposed. Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (Adriaans et al., 2000; Van Zaanen, 2000; Tu and Honavar, 2008). Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus (Baker, 1979; Lari and Young, 1990; Pereira and Schabes, 1992; Klein and Manning, 2002). Several Bayesian inference approaches have also been proposed (Chen, 1995; Kurihara and Sato, 2006; Liang et al., 2007; Cohen et al., 2010). These approaches perform parameter estimation by exploiting Markov sampling techniques. Recently, for the related problem of unsupervised dependency parsing, Gormley and Eisner (2013) proposed a new way of framing the max-likel</context>
</contexts>
<marker>Tu, Honavar, 2008</marker>
<rawString>Kewei Tu and Vasant Honavar. 2008. Unsupervised learning of probabilistic context-free grammar using iterative biclustering. In Grammatical Inference: Algorithms and Applications, pages 224–237. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno Van Zaanen</author>
</authors>
<title>Abl: Alignment-based learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 2,</booktitle>
<pages>961--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Van Zaanen, 2000</marker>
<rawString>Menno Van Zaanen. 2000. Abl: Alignment-based learning. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 961–967. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
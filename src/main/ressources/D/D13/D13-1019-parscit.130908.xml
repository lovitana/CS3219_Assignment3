<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.975666">
Joint Learning of Phonetic Units and Word Pronunciations for ASR
</title>
<author confidence="0.997858">
Chia-ying Lee, Yu Zhang, James Glass
</author>
<affiliation confidence="0.9988215">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.776501">
Cambridge, MA 02139, USA
</address>
<email confidence="0.999211">
{chiaying,yzhang87,jrg}@csail.mit.edu
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998923090909091">
The creation of a pronunciation lexicon re-
mains the most inefficient process in develop-
ing an Automatic Speech Recognizer (ASR).
In this paper, we propose an unsupervised
alternative – requiring no language-specific
knowledge – to the conventional manual ap-
proach for creating pronunciation dictionar-
ies. We present a hierarchical Bayesian model,
which jointly discovers the phonetic inven-
tory and the Letter-to-Sound (L2S) mapping
rules in a language using only transcribed
data. When tested on a corpus of spontaneous
queries, the results demonstrate the superior-
ity of the proposed joint learning scheme over
its sequential counterpart, in which the la-
tent phonetic inventory and L2S mappings are
learned separately. Furthermore, the recogniz-
ers built with the automatically induced lexi-
con consistently outperform grapheme-based
recognizers and even approach the perfor-
mance of recognition systems trained using
conventional supervised procedures.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977604166667">
Modern automatic speech recognizers require a few
essential ingredients such as a signal representation
of the speech signal, a search component, and typ-
ically a set of stochastic models that capture 1) the
acoustic realizations of the basic sounds of a lan-
guage, for example, phonemes, 2) the realization of
words in terms of these sounds, and 3) how words
are combined in spoken language. When creating
a speech recognizer for a new language the usual
requirements are: first, a large speech corpus with
word-level annotations; second, a pronunciation dic-
tionary that essentially defines a phonetic inventory
for the language as well as word-level pronuncia-
tions, and third, optional additional text data that
can be used to train the language model. Given
these data and some decision about the signal rep-
resentation, e.g., centi-second Mel-Frequency Cep-
stral Coefficients (MFCCs) (Davis and Mermelstein,
1980) with various derivatives, as well as the nature
of the acoustic and language model such as 3-state
HMMs and n-grams, iterative training methods can
be used to effectively learn the model parameters for
the acoustic and language models. Although the de-
tails of the components have changed through the
years, this basic ASR formulation was well estab-
lished by the late 1980’s, and has not really changed
much since then.
One of the interesting aspects of this formulation
is the inherent dependence on the dictionary, which
defines both the phonetic inventory of a language,
and the pronunciations of all the words in the vo-
cabulary. The dictionary is arguably the cornerstone
of a speech recognizer as it provides the essential
transduction from sounds to words. Unfortunately,
the dependency on this resource is a significant im-
pediment to the creation of speech recognizers for
new languages, since they are typically created by
experts, whereas annotated corpora can be relatively
more easily created by native speakers of a language.
The existence of an expert-derived dictionary in
the midst of stochastic speech recognition models is
somewhat ironic, and it is natural to ask why it con-
tinues to receive special status after all these years.
Why can we not learn the inventory of sounds of a
language and associated word pronunciations auto-
matically, much as we learn our acoustic model pa-
rameters? If successful, we would move one step
forward towards breaking the language barrier that
</bodyText>
<page confidence="0.96451">
182
</page>
<note confidence="0.735085">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182–192,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99871344">
limits us from having speech recognizers for all lan-
guages of the world, instead of the less than 2% that
currently exist.
In this paper, we investigate the problem of infer-
ring a pronunciation lexicon from an annotated cor-
pus without exploiting any language-specific knowl-
edge. We formulate our approach as a hierarchi-
cal Bayesian model, which jointly discovers the
acoustic inventory and the latent encoding scheme
between the letters and the sounds of a language.
We evaluate the quality of the induced lexicon and
acoustic model through a series of speech recogni-
tion experiments on a conversational weather query
corpus (Zue et al., 2000). The results demonstrate
that our model consistently generates close perfor-
mance to recognizers that are trained with expert-
defined phonetic inventory and lexicon. Compared
to grapheme-based recognizers, our model is capa-
ble of improving the Word Error Rates (WERs) by
at least 15.3%. Finally, the joint learning framework
proposed in this paper is proven to be much more
effective than modeling the acoustic units and the
letter-to-sound mappings separately, as shown in a
45% WER deduction our model achieves compared
to a sequential approach.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998915">
Various algorithms for learning sub-word based pro-
nunciations were proposed in (Lee et al., 1988;
Fukada et al., 1996; Bacchiani and Ostendorf, 1999;
Paliwal, 1990). In these previous approaches, spo-
ken samples of a word are gathered, and usually
only one single pronunciation for the word is de-
rived based on the acoustic evidence observed in the
spoken samples. The major difference between our
work and these previous works is that our model
learns word pronunciations in the context of letter
sequences. More specifically, our model learns letter
pronunciations first and then concatenates the pro-
nunciation of each letter in a word to form the word
pronunciation. The advantage of our approach is
that pronunciation knowledge learned for a particu-
lar letter in some arbitrary word can subsequently be
used to help learn the letter’s pronunciation in other
words. This property allows our model to potentially
learn better pronunciations for less frequent words.
The more recent work by Garcia and Gish (2006)
and Siu et al. (2013) has made extensive use
of self-organizing units for keyword spotting and
other tasks for languages with limited linguistic
resources. Others who have more recently ex-
plored the unsupervised space include (Varadarajan
et al., 2008; Jansen and Church, 2011; Lee and
Glass, 2012). The latter work introduced a non-
parametric Bayesian inference procedure for auto-
matically learning acoustic units that is most similar
to our current work except that our model also infers
word pronunciations simultaneously.
The concept of creating a speech recognizer for
a language with only orthographically annotated
speech data has also been explored previously by
means of graphemes. This approach has been shown
to be effective for alphabetic languages with rela-
tively straightforward grapheme to phoneme trans-
formations and does not require any unsupervised
learning of units or pronunciations (Killer et al.,
2003; St¨uker and Schultz, 2004). As we explain in
later sections, grapheme-based systems can actually
be regarded as a special case of our model; therefore,
we expect our model to have greater flexibilities for
capturing pronunciation rules of graphemes.
</bodyText>
<sectionHeader confidence="0.993837" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999941428571429">
The goal of our model is to induce a word pronunci-
ation lexicon from spoken utterances and their cor-
responding word transcriptions. No other language-
specific knowledge is assumed to be available, in-
cluding the phonetic inventory of the language. To
achieve the goal, our model needs to solve the fol-
lowing two tasks:
</bodyText>
<listItem confidence="0.959353">
• Discover the phonetic inventory.
• Reveal the latent mapping between the letters
and the discovered phonetic units.
</listItem>
<bodyText confidence="0.996489222222222">
We propose a hierarchical Bayesian model for
jointly discovering the two latent structures from
an annotated speech corpus. Before presenting our
model, we first describe the key latent and observed
variables of the problem.
Letter (M) We use M � to denote the ith let-
ter observed in the word transcription of the
mth training sample. To be sure, a train-
ing sample involves a speech utterance and its
</bodyText>
<page confidence="0.998602">
183
</page>
<bodyText confidence="0.998973861111111">
corresponding text transcription. The letter se-
quence composed of lm i and its context, namely
lmi−κ, · · · ,lmi−1, lmi , lmi�1, · · · ,lmi�κ, is denoted as ~lm i,κ.
Although lmi is referred to as a letter in this paper,
it can represent any character observed in the text
data, including space and symbols indicating sen-
tence boundaries. The set of unique characters ob-
served in the data set is denoted as G. For notation
simplicity, we use Lκ to denote the set of letter se-
quences of length 2κ + 1 that appear in the dataset
and use ~lκ to denote the elements in Lκ. Finally,
P(~lκ) is used to represent the parent of ~lκ, which is
a substring of ~lκ with the first and the last characters
truncated.
Number of Mapped Acoustic Units (nmi ) Each
letter lm i in the transcriptions is assumed to be
mapped to a certain number of phonetic units. For
example, the letter x in the word fox is mapped to
2 phonetic units /k/ and /s/, while the letter e in the
word lake is mapped to 0 phonetic units. We denote
this number as nm i and limit its value to be 0, 1 or 2
in our model. The value of nm i is always unobserved
and needs to be inferred by the our model.
Identity of the Acoustic Unit (cmi,p) For each pho-
netic unit that lm i maps to, we use cmi,p, for 1 G p G
nmi , to denote the identity of the phonetic unit. Note
that the phonetic inventory that describes the data
set is unknown to our model, and the identities of
the phonetic units are associated with the acoustic
units discovered automatically by our model.
Speech Feature xm t The observed speech data in
our problem are converted to a series of 25 ms 13-
dimensional MFCCs (Davis and Mermelstein, 1980)
and their first- and second-order time derivatives at
a 10 ms analysis rate. We use xmt E R39 to denote
the tth feature frame of the mth utterance.
</bodyText>
<subsectionHeader confidence="0.905436">
3.1 Generative Process
</subsectionHeader>
<bodyText confidence="0.9864705">
We present the generative process for a single train-
ing sample (i.e., a speech utterance and its corre-
sponding text transcription); to keep notation sim-
ple, we discard the index variable m in this section.
For each li in the transcription, the model gener-
ates ni, given ~li,κ, from the 3-dimensional categori-
cal distribution φ~li,κ(ni). Note that for every unique
~li,κ letter sequence, there is an associated φ~li,κ(ni)
</bodyText>
<figureCaption confidence="0.953553">
Figure 1: The graphical representation of the pro-
</figureCaption>
<bodyText confidence="0.979843583333333">
posed hierarchical Bayesian model. The shaded cir-
cle denotes the observed text and speech data, and
the squares denote the hyperparameters of the priors
in our model. See Sec. 3 for a detailed explanation
of the generative process of our model.
distribution, which captures the fact that the number
of phonetic units a letter maps to may depend on its
context. In our model, we impose a Dirichlet distri-
bution prior Dir(η) on φ~li,κ(ni).
If ni = 0, li is not mapped to any acoustic units
and the generative process stops for li; otherwise,
for 1 G p G ni, the model generates ci,p from:
</bodyText>
<equation confidence="0.991466">
ci,p — π~li,κ,ni,p (1)
</equation>
<bodyText confidence="0.9997358">
where π~li,κ,ni,p is a K-dimensional categorical dis-
tribution, whose outcomes correspond to the pho-
netic units discovered by the model from the given
speech data. Eq. 1 shows that for each combination
of ~li,κ, ni and p, there is an unique categorical distri-
bution. An important property of these categorical
distributions is that they are coupled together such
that their outcomes point to a consistent set of pho-
netic units. In order to enforce the coupling, we con-
struct π~li,κ,ni,p through a hierarchical process.
</bodyText>
<equation confidence="0.930236666666667">
β — Dir(γ) (2)
π~li,κ,ni,p — Dir(ακβ) for κ = 0 (3)
π~li,κ,ni,p — Dir(ακπ~li,κ��,ni,p) for κ &gt; 1 (4)
17
lj
i-2 5 j 5 i+2
</equation>
<figure confidence="0.963266545454545">
1&lt; p &lt; ni
ci, p
xt
ni
1 &lt; i &lt; Lm
di,p
1 &lt; m &lt; M
G x{(n,p) |
0 5 n 5 2, 1 5 p 5 n}
Rl2,n,p
Rl1,n,p
G xG
0
Rl,n,p
0c
G xG
K
α0
Y
α1
α2
00
</figure>
<page confidence="0.991865">
184
</page>
<bodyText confidence="0.999998238095238">
To interpret Eq. 2 to Eq. 4, we envision that
the observed speech data are generated by a K-
component mixture model, of which the components
correspond to the phonetic units in the language. As
a result, Q in Eq. 2 can be viewed as the mixture
weight over the components, which indicates how
likely we are to observe each acoustic unit in the
data overall. By adopting this point of view, we
can also regard the mapping between li and the pho-
netic units as a mixture model, and 7rli,ni,p1 repre-
sents how probable li is mapped to each phonetic
unit given ni and p. We apply a Dirichlet distribu-
tion prior parametrized by α0Q to 7rli,ni,p as shown
in Eq. 3. With this parameterization, the mean of
7rli,ni,p is the global mixture weight Q, and α0 con-
trols how similar 7rli,ni,p is to the mean. More specif-
ically, for large α0 » K, the Dirichlet distribution
is highly peaked around the mean; on the contrary,
for α0 « K, the mean lies in a valley. The parame-
ters of a Dirichlet distribution can also be viewed as
pseudo-counts for each category. Eq. 4 shows that
the prior for 7r~li ni p is seeded by pseudo-counts
that are proportional to the mapping weights over
the phonetic units of li in a shorter context. In other
words, the mapping distribution of li in a shorter
context can be thought of as a back-off distribution
of li’s mapping weights in a longer context.
Each component of the K-dimensional mixture
model is linked to a 3-state Hidden Markov Model
(HMM). These K HMMs are used to model the
phonetic units in the language (Jelinek, 1976). The
emission probability of each HMM state is modeled
by a diagonal Gaussian Mixture Model (GMM). We
use 0c to represent the set of parameters that define
the cth HMM, which includes the state transition
probability and the GMM parameters of each state
emission distribution. The conjugate prior of 0c is
denoted as H(00)2.
Finally, to finish the generative process, for each
ci,p we use the corresponding HMM 0ci,p to gen-
erate the observed speech data xt, and the genera-
tive process of the HMM determines the duration,
</bodyText>
<subsectionHeader confidence="0.925801">
1An abbreviation of ir~li�0,ni,p
</subsectionHeader>
<bodyText confidence="0.999919285714286">
2H(Bo) includes a Dirichlet prior for the transition probabil-
ity of each state, and a Dirichlet prior for each mixture weight
of the three GMMs, and a normal-Gamma distribution for the
mean and precision of each Gaussian mixture in the 3-state
HMM.
di,p, of the speech segment. The complete genera-
tive model, with r. set to 2, is depicted in Fig. 1; M
is the total number of transcribed utterances in the
corpus, and Lm is the number of letters in utterance
m. The shaded circles denote the observed data, and
the squares denote the hyperparameters of the priors
used in our model. Lastly, the unshaded circles de-
note the latent variables of our model, for which we
derive inference algorithms in the next section.
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.972458545454545">
We employ Gibbs sampling (Gelman et al., 2004) to
approximate the posterior distribution of the latent
variables in our model. In the following sections, we
first present a message-passing algorithm for block-
sampling ni and ci,p, and then describe how we
leverage acoustic cues to accelerate the computa-
tion of the message-passing algorithm. Note that the
block-sampling algorithm for ni and ci,p can be par-
allelized across utterances. Finally, we briefly dis-
cuss the inference procedures for O~l, , 7r~ , Q, 0c
� lκ,n,p
</bodyText>
<subsectionHeader confidence="0.999302">
4.1 Block-sampling ni and ci,p
</subsectionHeader>
<bodyText confidence="0.999962857142857">
To understand the message-passing algorithm in this
study, it is helpful to think of our model as a sim-
plified Hidden Semi-Markov Model (HSMM), in
which the letters represent the states and the speech
features are the observations. However, unlike in
a regular HSMM, where the state sequence is hid-
den, in our case, the state sequence is fixed to be the
given letter sequence. With this point of view, we
can modify the message-passing algorithms of Mur-
phy (2002) and Johnson and Willsky (2013) to com-
pute the posterior information required for block-
sampling ni and ci,p.
Let L(xt) be a function that returns the index
of the letter from which xt is generated; also, let
Ft = 1 be a tag indicating that a new phone segment
starts at t + 1. Given the constraint that 0 &lt; ni &lt; 2,
for 0 &lt; i &lt; Lm and 0 &lt; t &lt; Tm, the backwards
messages Bt(i) and Bt (i) for the mth training sam-
ple can be defined and computed as in Eq. 5 and
Eq. 7. Note that for clarity we discard the index vari-
able m in the derivation of the algorithm.
</bodyText>
<page confidence="0.91384">
185
</page>
<equation confidence="0.998345444444444">
Bt(i) p(xt+1:T |L(xt) = i, Ft = 1)
min{L,i+1+U1
X
j=i+1
O~li,κ(0) (5)
Bt (i) p(xt+1:T |L(xt+1) = i, Ft = 1)
= TX−t p(xt+1:t+d |(,κ)Bt+d(i) (6)
d=1
O~li,κ(1)7r~li,κ,1,1(ci,1)p(xt+1:t+d|eci,1)
</equation>
<construct confidence="0.7278795">
Algorithm 1 Block-sample ni and ci,p from Bt(i)
and Bt (i)
</construct>
<listItem confidence="0.956764333333333">
1: i i 0
2: t i 0
3: while i &lt; L ∧ t &lt; T do
4: nexti i 5ampleFromBt(i)
5: if nexti &gt; i + 1 then
6: for k = i + 1 to k = nexti − 1 do
7: nk i 0
8: end for
9: end if
</listItem>
<figure confidence="0.850854538461539">
10: d, ni, (ci,p), v i 5ampleFromBt (nexti)
11: tit+d
12: i i nexti
13: end while
Bt (j) j−1 Y p(nk = 0|(,κ)
k=i+1
min{L,i+1+U1 Bt (j) j−1 Y
X k=i+1
j=i+1
K
1X
ci,1=1
TX−t
d=1
O~li,κ(2)7r~li,κ,2,1(ci,1)7r~li,κ,2,2(ci,2)
X p(xt+1:t+v|Bci,1)p(xt+v+1:t+d|Bci,2)}Bt+d(i)
(7) B0(0) = min{L,U+11 Bo(j) j−1 Y O~li,κ(0) (8)
X k=1
j=1
d− 1
+X
v=1
XK
ci,1
XK
ci,2
</figure>
<bodyText confidence="0.99764472">
We use xt1:t2 to denote the segment consisting of
xt1, · · · , xt2. Our inference algorithm only allows
up to U letters to emit 0 acoustic units in a row. The
value of U is set to 2 for our experiments. Bt(i)
represents the total probability of all possible align-
ments between xt+1:T and li+1:L. Bt (i) contains
the probability of all the alignments between xt+1:T
and li+1:L that map xt+1 to li particularly. This
alignment constraint between xt+1 and li is explic-
itly shown in the first term of Eq. 6, which represents
how likely the speech segment xt+1:t+d is generated
by li given li’s context. This likelihood is simply
the marginal probability of p(xt+1:t+d, ni, ci,p |(i,κ)
with ni and ci,p integrated out, which can be ex-
panded and computed as shown in the last three rows
of Eq. 7. The index v specifies where the phone
boundary is between the two acoustic units that li
is aligned with when ni = 2. Eq. 8 to Eq. 10 are
the boundary conditions of the message passing al-
gorithm. B0(0) carries the total probably of all pos-
sible alignments between l1:L and x1:T. Eq. 9 spec-
ifies that at most U letters at the end of an sentence
can be left unaligned with any speech features, while
Eq. 10 indicates that all of the speech features in an
utterance must be assigned to a letter.
</bodyText>
<equation confidence="0.996341818181818">
1 ifi=L
L
Qj=i+1 0 if L − U &lt; i &lt; L
O r c )
(
0 ifi&lt;L − U
(9)
(
0 1 if t = T
Bt (L) (10)
0 otherwise
</equation>
<bodyText confidence="0.9571315">
Given Bt(i) and Bt (i), ni and ci,p for each letter
in the utterance can be sampled using Alg. 1. The
5ampleFromBt(i) function in line 4 returns a ran-
dom sample from the relative probability distribu-
tion composed by entries of the summation in Eq. 5.
Line 5 to line 9 check whether li (and maybe li+1)
is mapped to zero phonetic units. nexti points to
the letter that needs to be aligned with 1 or 2 phone
segments starting from xt. The number of phonetic
units that lnexti maps to and the identities of the
units are sampled in 5ampleFromBt (i). This sub-
routine generates a tuple of d, ni, (ci,p) as well as
v (if ni = 2) from all the entries of the summation
shown in Eq. 73.
3We use (ci,p) to denote that (ci,p) may consist of two num-
bers, ci,1 and ci,2, when ni = 2.
</bodyText>
<figure confidence="0.737474">
⎧
⎨⎪
⎪⎩
BT (i)
</figure>
<page confidence="0.968602">
186
</page>
<subsectionHeader confidence="0.950278">
4.2 Heuristic Phone Boundary Elimination
</subsectionHeader>
<bodyText confidence="0.99996">
The variables d and v in Eq. 7 enumerate through
every frame index in a sentence, treating each fea-
ture frame as a potential boundary between acous-
tic units. However, it is possible to exploit acoustic
cues to avoid checking feature frames that are un-
likely to be phonetic boundaries. We follow the pre-
segmentation method described in Glass (2003) to
skip roughly 80% of the feature frames and greatly
speed up the computation of Bt (i).
Another heuristic applied to our algorithm to re-
duce the search space for d and v is based on the
observation that the average duration of phonetic
units is usually no longer than 300 ms. Therefore,
when computing Bt (i), we only consider speech
segments that are shorter than 300 ms to avoid align-
ing letters to speech segments that are too long to be
phonetic units.
</bodyText>
<subsectionHeader confidence="0.977553">
4.3 Sampling φ~l�, π~l�,n�,p, βand θc
</subsectionHeader>
<bodyText confidence="0.95013">
Sampling φ~l� To compute the posterior distribu-
tion of φ~l�, we count how many times ~lκ is mapped
to 0, 1 and 2 phonetic units from nmi . More specifi-
cally, we define N�(j) for 0 &lt; j &lt; 2 as follows:
δ(nm i , j)δ(~lm i,κ,~lκ)
where we use δ(�) to denote the discrete Kronecker
delta. With N�, we can simply sample a new value
for φ~l� from the following distribution:
</bodyText>
<equation confidence="0.964785">
φ~l� —Dir(η + JV~l�)
</equation>
<bodyText confidence="0.97855325">
Sampling π~l�,n,p and β The posterior distribu-
tions of π~l�,n,p and β are constructed recursively due
to the hierarchical structure imposed on π~l�,n,p and
β. We start with gathering counts for updating the
π variables at the lowest level, i.e., π~l2,n,p given that
κ is set to 2 in our model implementation, and then
sample pseudo-counts for the π variables at higher
hierarchies as well as β. With the pseudo-counts, a
new β can be generated, which allows π~ to be
ln,n,p
re-sampled sequentially.
More specifically, we define C~l2,n,p(k) to be the
number of times that ~l2 is mapped to n units and
the unit in position p is the kth phonetic unit. This
value can be counted from the current values of cmi,p
as follows.
</bodyText>
<equation confidence="0.71383">
δ(~li,2,~l2)δ(nmi , n)δ(cmi,p, k)
</equation>
<bodyText confidence="0.998350833333333">
To derive the posterior distribution of π~l1,n,p an-
alytically, we need to sample pseudo-counts C~l1,n,p,
which is defined as follows.
~l2 whose parent is~l1 and νi to represent random vari-
ables sampled from a uniform distribution between
0 and 1. Eq. 11 can be applied recursively to com-
pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts that
are applied to the conjugate priors of π~l0,n,p and
β.
With the pseudo-count variables computed, new val-
ues for β and π~l�,n,p can be sampled sequentially as
shown in Eq. 12 to Eq. 14.
</bodyText>
<equation confidence="0.996931">
β — Dir(γ + C ,n,p) (12)
π~l,,n,p — Dir(ακβ + C~l,,n,p) for κ = 0 (13)
π~l�,n,p — Dir(ακπ~l�−1,n,p + C~l,,n,p) for κ &gt; 1
(14)
</equation>
<sectionHeader confidence="0.999181" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999926285714286">
To test the effectiveness of our model for joint learn-
ing phonetic units and word pronunciations from an
annotated speech corpus, we construct speech rec-
ognizers out of the training results of our model.
The performance of the recognizers is evaluated and
compared against three baselines: first, a grapheme-
based speech recognizer; second, a recognizer built
by using an expert-crafted lexicon, which is referred
to as an expert lexicon in the rest of the paper for
simplicity; and third, a recognizer built by discover-
ing the phonetic units and L2S pronunciation rules
sequentially without using a lexicon. In this section,
we provide a detailed description of the experimen-
tal setup.
</bodyText>
<equation confidence="0.982381857142857">
�Lm
i=1
Nr�(j) =
M
m=1
C~l2,n,p(k) =
Lm
i=1
M
m=1
�C~l1,n,p(k) = Cr2,n,p(k) ff[νi &lt; α2π~l1,n,p(k)
~l2EUr1 � i + α2π~l1,n,p(k)�
i=1 (11)
We use U~l1 = { ~l2|P(~l2) = ~l1} to denote the set of
</equation>
<page confidence="0.992818">
187
</page>
<table confidence="0.982425">
77 &apos;y a0 a1 a2 90 K K
(0.1)3 (10)100 1 0.1 0.2 * 2 100
</table>
<tableCaption confidence="0.998405">
Table 1: The values of the hyperparameters of our
</tableCaption>
<bodyText confidence="0.98995">
model. We use (a)D to denote a D-dimensional vec-
tor with all entries being a. *We follow the proce-
dure reported in (Lee and Glass, 2012) to set up the
HMM prior 00.
</bodyText>
<subsectionHeader confidence="0.928769">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999989">
All the speech recognition experiments reported in
this paper are performed on a weather query dataset,
which consists of narrow-band, conversational tele-
phone speech (Zue et al., 2000). We follow the ex-
perimental setup of McGraw et al. (2013) and split
the corpus into a training set of 87,351 utterances, a
dev set of 1,179 utterances and a test set of 3,497 ut-
terances. A subset of 10,000 utterances is randomly
selected from the training set. We use this subset of
data for training our model to demonstrate that our
model is able to discover the phonetic composition
and the pronunciation rules of a language even from
just a few hours of data.
</bodyText>
<subsectionHeader confidence="0.999801">
5.2 Building a Recognizer from Our Model
</subsectionHeader>
<bodyText confidence="0.999990530612245">
The values of the hyperparameters of our model are
listed in Table 1. We run the inference procedure de-
scribed in Sec. 4 for 10,000 times on the randomly
selected 10,000 utterances. The samples of O~l and
�~l�n,p from the last iteration are used to decode nmi
and cmi,p for each sentence in the entire training set by
following the block-sampling algorithm described
in Sec. 4.1. Since cmi,p is the phonetic mapping of
lmi , by concatenating the phonetic mapping of ev-
ery letter in a word, we can obtain a pronunciation
of the word represented in the labels of discovered
phonetic units. For example, assume that word w
appears in sentence m and consists of l3l4l5 (the
sentence index m is ignored for simplicity). Also,
assume that after decoding, n3 = 1, n4 = 2 and
n5 = 1. A pronunciation of w is then encoded by the
sequence of phonetic labels c3,1c4,1c4,2c5,1. By re-
peating this process for each word in every sentence
for the training set, a list of word pronunciations can
be compiled and used as a stochastic lexicon to build
a speech recognizer.
In theory, the HMMs inferred by our model can be
directly used as the acoustic model of a monophone
speech recognizer. However, if we regard the ci,p
labels of each utterance as the phone transcription
of the sentence, then a new acoustic model can be
easily re-trained on the entire data set. More conve-
niently, the phone boundaries corresponding to the
ci,p labels are the by-products of the block-sampling
algorithm, which are indicated by the values of d and
v in line 10 of Alg. 1 and can be easily saved during
the sampling procedure. Since these data are readily
available, we re-build a context-independent model
on the entire data set. In this new acoustic model,
a 3-state HMM is used to model each phonetic unit,
and the emission probability of each state is modeled
by a 32-mixture GMM.
Finally, a trigram language model is built by using
the word transcriptions in the full training set. This
language model is utilized in all speech recogni-
tion experiments reported in this paper. Finite State
Transducers (FSTs) are used to build all the recog-
nizers used in this study. With the language model,
the lexicon and the context-independent acoustic
model constructed by the methods described in this
section, we can build a speech recognizer from
the learning output of the proposed model without
the need of a pre-defined phone inventory and any
expert-crafted lexicons.
</bodyText>
<subsectionHeader confidence="0.495202">
5.2.1 Pronunciation Mixture Model Retraining
</subsectionHeader>
<bodyText confidence="0.999918210526316">
McGraw et al. (2013) presented the Pronuncia-
tion Mixture Model (PMM) for composing stochas-
tic lexicons that outperform pronunciation dictionar-
ies created by experts. Although the PMM frame-
work was designed to incorporate and augment ex-
pert lexicons, we found that it can be adapted to pol-
ish the pronunciation list generated by our model.
In particular, the training procedure for PMMs in-
cludes three steps. First, train a L2S model from
a manually specified expert-pronunciation lexicon;
second, generate a list of pronunciations for each
word in the dataset using the L2S model; and finally,
use an acoustic model to re-weight the pronuncia-
tions based on the acoustic scores of the spoken ex-
amples of each word.
To adapt this procedure for our purposes, we sim-
ply plug in the word pronunciations and the acous-
tic model generated by our model. Once we ob-
tain the re-weighted lexicon, we re-generate forced
</bodyText>
<page confidence="0.996887">
188
</page>
<bodyText confidence="0.9997838">
phone alignments and retrain the acoustic model,
which can be utilized to repeat the PMM lexicon re-
weighting procedure. For our experiments, we it-
erate through this model refining process until the
recognition performance converges.
</bodyText>
<subsubsectionHeader confidence="0.698657">
5.2.2 Triphone Model
</subsubsectionHeader>
<bodyText confidence="0.999955666666667">
Conventionally, to train a context-dependent
acoustic model, a list of questions based on the
linguistic properties of phonetic units is required
for growing decision tree classifiers (Young et al.,
1994). However, such language-specific knowledge
is not available for our training framework; there-
fore, our strategy is to compile a question list that
treats each phonetic unit as a unique linguistic class.
In other words, our approach to training a context-
dependent acoustic model for the automatically dis-
covered units is to let the decision trees grow fully
based on acoustic evidence.
</bodyText>
<subsectionHeader confidence="0.999238">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999716185185185">
We compare the recognizers trained by following
the procedures described in Sec. 5.2 against three
baselines. The first baseline is a grapheme-based
speech recognizer. We follow the procedure de-
scribed in Killer et al. (2003) and train a 3-state
HMM for each grapheme, which we refer to as the
monophone grapheme model. Furthermore, we cre-
ate a singleton question set (Killer et al., 2003), in
which each grapheme is listed as a question, to train
a triphone grapheme model. Note that to enforce
better initial alignments between the graphemes and
the speech data, we use a pre-trained acoustic model
to identify the non-speech segments at the beginning
and the end of each utterance before starting training
the monophone grapheme model.
Our model jointly discovers the phonetic inven-
tory and the L2S mapping rules from a set of tran-
scribed data. An alternative of our approach is to
learn the two latent structures sequentially. We fol-
low the training procedure of Lee and Glass (2012)
to learn a set of acoustic models from the speech
data and use these acoustic models to generate a
phone transcription for each utterance. The phone
transcriptions along with the corresponding word
transcriptions are fed as inputs to the L2S model
proposed in Bisani and Ney (2008). A stochastic
lexicon can be learned by applying the L2S model
</bodyText>
<table confidence="0.998423">
unit(%) Monophone
Our model 17.0
Oracle 13.8
Grapheme 32.7
Sequential model 31.4
</table>
<tableCaption confidence="0.988254">
Table 2: Word error rates generated by the four
</tableCaption>
<bodyText confidence="0.962966208333333">
monophone recognizers described in Sec. 5.2 and
Sec. 5.3 on the weather query corpus.
and the discovered acoustic models to PMM. This
two-stage approach for training a speech recognizer
without an expert lexicon is referred to as the se-
quential model in this paper.
Finally, we compare our system against a rec-
ognizer trained from an oracle recognition system.
We build the oracle recognizer on the same weather
query corpus by following the procedure presented
in McGraw et al. (2013). This oracle recognizer is
then applied to generate forced-aligned phone tran-
scriptions for the training utterances, from which
we can build both monophone and triphone acous-
tic models. The expert-crafted lexicon used in the
oracle recognizer is also used in this baseline. Note
that for training the triphone model, we compose a
singleton question list (Killer et al., 2003) that has
every expert-defined phonetic unit as a question. We
use this singleton question list instead of a more so-
phisticated one to ensure that this baseline and our
system differ only in the acoustic model and the lex-
icon used to generate the initial phone transcriptions.
We call this baseline the oracle baseline.
</bodyText>
<sectionHeader confidence="0.998832" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999501">
6.1 Monophone Systems
</subsectionHeader>
<bodyText confidence="0.99995">
Table 2 shows the WERs produced by the four
monophone recognizers described in Sec. 5.2 and
Sec. 5.3. It can be seen that our model outper-
forms the grapheme and the sequential model base-
lines significantly while approaching the perfor-
mance of the supervised oracle baseline. The im-
provement over the sequential baseline demonstrates
the strength of the proposed joint learning frame-
work. More specifically, unlike the sequential base-
line, in which the acoustic units are discovered in-
dependently from the text data, our model is able to
exploit the L2S mapping constraints provided by the
word transcriptions to cluster speech segments.
</bodyText>
<page confidence="0.997801">
189
</page>
<bodyText confidence="0.99996725">
By comparing our model to the grapheme base-
line, we can see the advantage of modeling the
pronunciations of a letter using a mixture model,
especially for a language like English which has
many pronunciation irregularities. However, even
for languages with straightforward pronunciation
rules, the concept of modeling letter pronunciations
using mixture models still applies. The main dif-
ference is that the mixture weights for letters of
languages with simple pronunciation rules will be
sparser and spikier. In other words, in theory, our
model should always perform comparable to, if not
better than, grapheme recognizers.
Last but not least, the recognizer trained with the
automatically induced lexicon performs similarly to
the recognizer initialized by an oracle recognition
system, which demonstrates the effectiveness of the
proposed model for discovering the phonetic inven-
tory and a pronunciation lexicon from an annotated
corpus. In the next section, we provide some in-
sights into the quality of the learned lexicon and
into what could have caused the performance gap
between our model and the conventionally trained
recognizer.
</bodyText>
<subsectionHeader confidence="0.999899">
6.2 Pronunciation Entropy
</subsectionHeader>
<bodyText confidence="0.999964444444444">
The major difference between the recognizer that is
trained by using our model and the recognizer that
is seeded by an oracle recognition system is that
the former uses an automatically discovered lexicon,
while the latter exploits an expert-defined pronun-
ciation dictionary. In order to quantify, as well as
to gain insights into, the difference between these
two lexicons, we define the average pronunciation
entropy, H, of a lexicon as follows.
</bodyText>
<equation confidence="0.958823">
p(b) logp(b) (15)
</equation>
<bodyText confidence="0.999905625">
where V denotes the vocabulary of a lexicon, B(w)
represents the set of pronunciations of a word w
and p(b) stands for the weight of a certain pronun-
ciation b. Intuitively, we can regard H as an in-
dicator of how much pronunciation variation that
each word in a lexicon has on average. Table 3
shows that the H values of the lexicon induced by
our model and the expert-defined lexicon as well as
</bodyText>
<table confidence="0.9994855">
Our model PMM iterations
(Discovered lexicon)
0 1 2
H (bit) 4.58 3.47 3.03
WER (%) 17.0 16.6 15.9
Oracle PMM iterations
(Expert lexicon)
0 1 2
H (bit) 0.69 0.90 0.92
WER (%) 13.8 12.8 12.4
</table>
<tableCaption confidence="0.998557">
Table 3: The upper-half of the table shows the aver-
</tableCaption>
<bodyText confidence="0.995464375">
age pronunciation entropies, H, of the lexicons in-
duced by our model and refined by PMM as well
as the WERs of the monophone recognizers built
with the corresponding lexicons for the weather
query corpus. The definition of H can be found in
Sec. 6.2. The first row of the lower-half of the ta-
ble lists the average pronunciation entropies, H, of
the expert-defined lexicon and the lexicons gener-
ated and weighted by the L2P-PMM framework de-
scribed in McGraw et al. (2013). The second row of
the lower-half of the table shows the WERs of the
recognizers that are trained with the expert-lexicon
and its PMM-refined versions.
their respective PMM-refined versions4. In Table 3,
we can see that the automatically-discovered lexi-
con and its PMM-reweighted versions have much
higher H values than their expert-defined counter-
parts. These higher H values imply that the lexicon
induced by our model contains more pronunciation
variation than the expert-defined lexicon. Therefore,
the lattices constructed during the decoding process
for our recognizer tend to be larger than those con-
structed for the oracle baseline, which explains the
performance gap between the two systems in Table 2
and Table 3.
As shown in Table 3, even though the lexicon
induced by our model is noisier than the expert-
defined dictionary, the PMM retraining framework
consistently refines the induced lexicon and im-
proves the performance of the recognizers5. To the
best of our knowledge, we are the first to apply
PMM to lexicons that are created by a fully unsu-
</bodyText>
<footnote confidence="0.9982688">
4We build the PMM-refined version of the expert-defined
lexicon by following the L2P-PMM framework described
in McGraw et al. (2013).
5The recognition results all converge in 2 — 3 PMM retrain-
ing iterations.
</footnote>
<table confidence="0.936428384615385">
H- 1 �
|V  |wEV bEB(w)
190
pronunciations pronunciation probabilities
Our model 1 PMM 2 PMM
93 56 87 39 19 0.125 - -
93 56 61 87 73 99 0.125 - -
11 56 61 87 73 99 0.125 0.400 0.419
93 20 75 87 17 27 52 0.125 0.125 0.124
55 93 56 61 87 73 84 19 0.125 0.220 0.210
93 26 61 87 49 0.125 0.128 0.140
63 83 86 87 73 53 19 0.125 - -
93 26 61 87 61 0.125 0.127 0.107
</table>
<tableCaption confidence="0.865339333333333">
Table 4: Pronunciation lists of the word Burma pro-
duced by our model and refined by PMM after 1 and
2 iterations.
</tableCaption>
<bodyText confidence="0.992994074074074">
pervised method. Therefore, in this paper, we pro-
vide further analysis on how PMM helps enhance
the performance of our model.
We compare the pronunciation lists for the word
Burma generated by our model and refined itera-
tively by PMM in Table 4. The first column of Ta-
ble 4 shows all the pronunciations of Burma dis-
covered by our model, to which our model assigns
equal probabilities to create a stochastic list6. As
demonstrated in the third and the fourth columns of
Table 4, the PMM framework is able to iteratively
re-distribute the pronunciation weights and filter out
less-likely pronunciations, which effectively reduces
both the size and the entropy of the stochastic lexi-
con generated by our model. The benefits of using
the PMM to refine the induced lexicon are twofold.
First, the search space constructed during the recog-
nition decoding process with the refined lexicon is
more constrained, which is the main reason why the
PMM is capable of improving the performance of
the monophone recognizer that is trained with the
output of our model. Secondly, and more impor-
tantly, the refined lexicon can greatly reduce the size
of the FST built for the triphone recognizer of our
model. These two observations illustrate why the
PMM framework can be an useful tool for enhancing
the lexicon discovered automatically by our model.
</bodyText>
<subsectionHeader confidence="0.992805">
6.3 Triphone Systems
</subsectionHeader>
<bodyText confidence="0.9990145">
The best monophone systems of the grapheme base-
line, the oracle baseline and our model are used to
</bodyText>
<footnote confidence="0.990121">
6It is also possible to assign probabilities proportional to the
decoding scores of the word tokens.
</footnote>
<table confidence="0.996556">
Unit(%) Triphone
Our model 13.4
Oracle 10.0
Grapheme 15.7
</table>
<tableCaption confidence="0.950273">
Table 5: Word error rates of the triphone recogniz-
</tableCaption>
<bodyText confidence="0.998252536585366">
ers. The triphone recognizers are all built by us-
ing the phone transcriptions generated by their best
monohpone system. For the oracle initialized base-
line and for our model, the PMM-refined lexicons
are used to build the triphone recognizers.
generate forced-aligned phone transcriptions, which
are used to train the triphone models described in
Sec. 5.2.2 and Sec. 5.3. Table 5 shows the WERs
of the triphone recognition systems. Note that if a
more conventional question list, for example, a list
that contains rules to classify phones into different
broad classes, is used to build the oracle triphone
system, the WER can be reduced to 6.5%. However,
as mentioned earlier, in order to gain insights into
the quality of the induced lexicon and the discovered
phonetic set, we compare our model against an ora-
cle triphone system that is built by using a singleton
question set.
By comparing Table 2 and Table 5, we can see
that the grapheme triphone improves by a large mar-
gin compared to its monophone counterpart, which
is consistent with the results reported in (Killer et
al., 2003). However, even though the grapheme
baseline achieves a great performance gain with
context-dependent acoustic models, the recognizer
trained using the lexicon learned by our model and
subsequently refined by PMM still outperforms the
grapheme baseline. The consistently better perfor-
mance our model achieves over the grapheme base-
line demonstrates the strength of modeling the pro-
nunciation of each letter with a mixture model that
is presented in this paper.
Last but not least, by comparing Table 2 and
Table 5, it can be seen that the relative perfor-
mance gain achieved by our model is similar to
that obtained by the oracle baseline. Both Table 2
and Table 5 show that even without exploiting any
language-specific knowledge during training, our
recognizer is able to perform comparably with the
recognizer trained using an expert lexicon. The abil-
ity of our model to obtain such similar performance
</bodyText>
<page confidence="0.994905">
191
</page>
<bodyText confidence="0.99993125">
further supports the effectiveness of the joint learn-
ing framework proposed in this paper for discover-
ing the phonetic inventory and the word pronuncia-
tions from simply an annotated speech corpus.
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999974727272727">
We present a hierarchical Bayesian model for si-
multaneously discovering acoustic units and learn-
ing word pronunciations from transcribed spoken ut-
terances. Both monophone and triphone recogniz-
ers can be built on the discovered acoustic units and
the inferred lexicon. The recognizers trained with
the proposed unsupervised method consistently out-
performs grapheme-based recognizers and approach
the performance of recognizers trained with expert-
defined lexicons. In the future, we plan to apply this
technology to develop ASRs for more languages.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999967857142857">
The authors would like to thank Ian McGraw and
Ekapol Chuangsuwanich for their advice on the
PMM and recognition experiments presented in this
paper. Thanks to the anonymous reviewers for help-
ful comments. Finally, the authors would like to
thank Stephen Shum for proofreading and editing
the early drafts of this paper.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999798666666667">
Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexi-
con, acoustic unit inventory and model design. Speech
Communication, 29:99 – 114.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434–451, May.
Steven B. Davis and Paul Mermelstein. 1980. Com-
parison of parametric representations for monosyllabic
word recognition in continuously spoken sentences.
IEEE Trans. on Acoustics, Speech, and Signal Pro-
cessing, 28(4):357–366.
Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and
Yoshinori Sagisaka. 1996. Speech recognition based
on acoustically derived segment units. In Proceedings
of ICSLP, pages 1077 – 1080.
Alvin Garcia and Herbert Gish. 2006. Keyword spotting
of arbitrary words using minimal speech resources. In
Proceedings of ICASSP, pages 949–952.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-
ald B. Rubin. 2004. Bayesian Data Analysis. Texts
in Statistical Science. Chapman &amp; Hall/CRC, second
edition.
James Glass. 2003. A probabilistic framework for
segment-based speech recognition. Computer Speech
and Language, 17:137 – 152.
Aren Jansen and Kenneth Church. 2011. Towards un-
supervised training of speaker independent acoustic
models. In Proceedings of INTERSPEECH, pages
1693 – 1696.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64:532 – 556.
Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian
nonparametric hidden semi-markov models. Journal
of Machine Learning Research, 14:673–701, February.
Mirjam Killer, Sebastian St¨uker, and Tanja Schultz.
2003. Grapheme based speech recognition. In Pro-
ceeding of the Eurospeech, pages 3141–3144.
Chia-ying Lee and James Glass. 2012. A nonparamet-
ric Bayesian approach to acoustic model discovery. In
Proceedings of ACL, pages 40–49.
Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.
1988. A segment model based approach to speech
recognition. In Proceedings of ICASSP, pages 501–
504.
Ian McGraw, Ibrahim Badr, and James Glass. 2013.
Learning lexicons from speech using a pronunciation
mixture model. IEEE Trans. on Speech and Audio
Processing, 21(2):357–366.
Kevin P. Murphy. 2002. Hidden semi-Markov mod-
els (hsmms). Technical report, University of British
Columbia.
Kuldip Paliwal. 1990. Lexicon-building methods for an
acoustic sub-word based speech recognizer. In Pro-
ceedings of ICASSP, pages 729–732.
Man-hung Siu, Herbert Gish, Arthur Chan, William
Belfield, and Steve Lowe. 2013. Unsupervised train-
ing of an HMM-based self-organizing unit recgonizer
with applications to topic classification and keyword
discovery. Computer, Speech, and Language.
Sebastian St¨uker and Tanja Schultz. 2004. A grapheme
based speech recognition system for Russian. In Pro-
ceedings of the 9th Conference Speech and Computer.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of ACL-08:
HLT, Short Papers, pages 165–168.
Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic mod-
elling. In Proceedings of HLT, pages 307–312.
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-
herington. 2000. Jupiter: A telephone-based con-
versational interface for weather information. IEEE
Trans. on Speech and Audio Processing, 8:85–96.
</reference>
<page confidence="0.998186">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975177">
<title confidence="0.998619">Joint Learning of Phonetic Units and Word Pronunciations for ASR</title>
<author confidence="0.997021">Chia-ying Lee</author>
<author confidence="0.997021">Yu Zhang</author>
<author confidence="0.997021">James</author>
<affiliation confidence="0.993025">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<address confidence="0.999751">Cambridge, MA 02139,</address>
<abstract confidence="0.99970752173913">The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative – requiring no language-specific knowledge – to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using conventional supervised procedures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Mari Ostendorf</author>
</authors>
<title>Joint lexicon, acoustic unit inventory and model design.</title>
<date>1999</date>
<journal>Speech Communication, 29:99 –</journal>
<pages>114</pages>
<contexts>
<context position="5197" citStr="Bacchiani and Ostendorf, 1999" startWordPosition="794" endWordPosition="797"> to recognizers that are trained with expertdefined phonetic inventory and lexicon. Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%. Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. 2 Related Work Various algorithms for learning sub-word based pronunciations were proposed in (Lee et al., 1988; Fukada et al., 1996; Bacchiani and Ostendorf, 1999; Paliwal, 1990). In these previous approaches, spoken samples of a word are gathered, and usually only one single pronunciation for the word is derived based on the acoustic evidence observed in the spoken samples. The major difference between our work and these previous works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approach is that pronunciation knowledge learned for a par</context>
</contexts>
<marker>Bacchiani, Ostendorf, 1999</marker>
<rawString>Michiel Bacchiani and Mari Ostendorf. 1999. Joint lexicon, acoustic unit inventory and model design. Speech Communication, 29:99 – 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>5</issue>
<contexts>
<context position="29206" citStr="Bisani and Ney (2008)" startWordPosition="5055" endWordPosition="5058">nning and the end of each utterance before starting training the monophone grapheme model. Our model jointly discovers the phonetic inventory and the L2S mapping rules from a set of transcribed data. An alternative of our approach is to learn the two latent structures sequentially. We follow the training procedure of Lee and Glass (2012) to learn a set of acoustic models from the speech data and use these acoustic models to generate a phone transcription for each utterance. The phone transcriptions along with the corresponding word transcriptions are fed as inputs to the L2S model proposed in Bisani and Ney (2008). A stochastic lexicon can be learned by applying the L2S model unit(%) Monophone Our model 17.0 Oracle 13.8 Grapheme 32.7 Sequential model 31.4 Table 2: Word error rates generated by the four monophone recognizers described in Sec. 5.2 and Sec. 5.3 on the weather query corpus. and the discovered acoustic models to PMM. This two-stage approach for training a speech recognizer without an expert lexicon is referred to as the sequential model in this paper. Finally, we compare our system against a recognizer trained from an oracle recognition system. We build the oracle recognizer on the same wea</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5):434–451, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven B Davis</author>
<author>Paul Mermelstein</author>
</authors>
<title>Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.</title>
<date>1980</date>
<journal>IEEE Trans. on Acoustics, Speech, and Signal Processing,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2148" citStr="Davis and Mermelstein, 1980" startWordPosition="310" endWordPosition="313">, 2) the realization of words in terms of these sounds, and 3) how words are combined in spoken language. When creating a speech recognizer for a new language the usual requirements are: first, a large speech corpus with word-level annotations; second, a pronunciation dictionary that essentially defines a phonetic inventory for the language as well as word-level pronunciations, and third, optional additional text data that can be used to train the language model. Given these data and some decision about the signal representation, e.g., centi-second Mel-Frequency Cepstral Coefficients (MFCCs) (Davis and Mermelstein, 1980) with various derivatives, as well as the nature of the acoustic and language model such as 3-state HMMs and n-grams, iterative training methods can be used to effectively learn the model parameters for the acoustic and language models. Although the details of the components have changed through the years, this basic ASR formulation was well established by the late 1980’s, and has not really changed much since then. One of the interesting aspects of this formulation is the inherent dependence on the dictionary, which defines both the phonetic inventory of a language, and the pronunciations of </context>
<context position="9751" citStr="Davis and Mermelstein, 1980" startWordPosition="1576" endWordPosition="1579">s value to be 0, 1 or 2 in our model. The value of nm i is always unobserved and needs to be inferred by the our model. Identity of the Acoustic Unit (cmi,p) For each phonetic unit that lm i maps to, we use cmi,p, for 1 G p G nmi , to denote the identity of the phonetic unit. Note that the phonetic inventory that describes the data set is unknown to our model, and the identities of the phonetic units are associated with the acoustic units discovered automatically by our model. Speech Feature xm t The observed speech data in our problem are converted to a series of 25 ms 13- dimensional MFCCs (Davis and Mermelstein, 1980) and their first- and second-order time derivatives at a 10 ms analysis rate. We use xmt E R39 to denote the tth feature frame of the mth utterance. 3.1 Generative Process We present the generative process for a single training sample (i.e., a speech utterance and its corresponding text transcription); to keep notation simple, we discard the index variable m in this section. For each li in the transcription, the model generates ni, given ~li,κ, from the 3-dimensional categorical distribution φ~li,κ(ni). Note that for every unique ~li,κ letter sequence, there is an associated φ~li,κ(ni) Figure </context>
</contexts>
<marker>Davis, Mermelstein, 1980</marker>
<rawString>Steven B. Davis and Paul Mermelstein. 1980. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE Trans. on Acoustics, Speech, and Signal Processing, 28(4):357–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Fukada</author>
<author>Michiel Bacchiani</author>
<author>Kuldip Paliwal</author>
<author>Yoshinori Sagisaka</author>
</authors>
<title>Speech recognition based on acoustically derived segment units.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>1077--1080</pages>
<contexts>
<context position="5166" citStr="Fukada et al., 1996" startWordPosition="790" endWordPosition="793">tes close performance to recognizers that are trained with expertdefined phonetic inventory and lexicon. Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%. Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. 2 Related Work Various algorithms for learning sub-word based pronunciations were proposed in (Lee et al., 1988; Fukada et al., 1996; Bacchiani and Ostendorf, 1999; Paliwal, 1990). In these previous approaches, spoken samples of a word are gathered, and usually only one single pronunciation for the word is derived based on the acoustic evidence observed in the spoken samples. The major difference between our work and these previous works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approach is that pronunciat</context>
</contexts>
<marker>Fukada, Bacchiani, Paliwal, Sagisaka, 1996</marker>
<rawString>Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, and Yoshinori Sagisaka. 1996. Speech recognition based on acoustically derived segment units. In Proceedings of ICSLP, pages 1077 – 1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvin Garcia</author>
<author>Herbert Gish</author>
</authors>
<title>Keyword spotting of arbitrary words using minimal speech resources.</title>
<date>2006</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>949--952</pages>
<contexts>
<context position="6062" citStr="Garcia and Gish (2006)" startWordPosition="933" endWordPosition="936">n our work and these previous works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. This property allows our model to potentially learn better pronunciations for less frequent words. The more recent work by Garcia and Gish (2006) and Siu et al. (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. Others who have more recently explored the unsupervised space include (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012). The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. The concept of creating a speech recognizer for a language with only ortho</context>
</contexts>
<marker>Garcia, Gish, 2006</marker>
<rawString>Alvin Garcia and Herbert Gish. 2006. Keyword spotting of arbitrary words using minimal speech resources. In Proceedings of ICASSP, pages 949–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gelman</author>
<author>John B Carlin</author>
<author>Hal S Stern</author>
<author>Donald B Rubin</author>
</authors>
<title>Bayesian Data Analysis. Texts in Statistical Science. Chapman &amp; Hall/CRC,</title>
<date>2004</date>
<note>second edition.</note>
<contexts>
<context position="14658" citStr="Gelman et al., 2004" startWordPosition="2461" endWordPosition="2464">rmal-Gamma distribution for the mean and precision of each Gaussian mixture in the 3-state HMM. di,p, of the speech segment. The complete generative model, with r. set to 2, is depicted in Fig. 1; M is the total number of transcribed utterances in the corpus, and Lm is the number of letters in utterance m. The shaded circles denote the observed data, and the squares denote the hyperparameters of the priors used in our model. Lastly, the unshaded circles denote the latent variables of our model, for which we derive inference algorithms in the next section. 4 Inference We employ Gibbs sampling (Gelman et al., 2004) to approximate the posterior distribution of the latent variables in our model. In the following sections, we first present a message-passing algorithm for blocksampling ni and ci,p, and then describe how we leverage acoustic cues to accelerate the computation of the message-passing algorithm. Note that the block-sampling algorithm for ni and ci,p can be parallelized across utterances. Finally, we briefly discuss the inference procedures for O~l, , 7r~ , Q, 0c � lκ,n,p 4.1 Block-sampling ni and ci,p To understand the message-passing algorithm in this study, it is helpful to think of our model</context>
</contexts>
<marker>Gelman, Carlin, Stern, Rubin, 2004</marker>
<rawString>Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2004. Bayesian Data Analysis. Texts in Statistical Science. Chapman &amp; Hall/CRC, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
</authors>
<title>A probabilistic framework for segment-based speech recognition.</title>
<date>2003</date>
<journal>Computer Speech and Language,</journal>
<pages>17--137</pages>
<contexts>
<context position="19517" citStr="Glass (2003)" startWordPosition="3393" endWordPosition="3394">erates a tuple of d, ni, (ci,p) as well as v (if ni = 2) from all the entries of the summation shown in Eq. 73. 3We use (ci,p) to denote that (ci,p) may consist of two numbers, ci,1 and ci,2, when ni = 2. ⎧ ⎨⎪ ⎪⎩ BT (i) 186 4.2 Heuristic Phone Boundary Elimination The variables d and v in Eq. 7 enumerate through every frame index in a sentence, treating each feature frame as a potential boundary between acoustic units. However, it is possible to exploit acoustic cues to avoid checking feature frames that are unlikely to be phonetic boundaries. We follow the presegmentation method described in Glass (2003) to skip roughly 80% of the feature frames and greatly speed up the computation of Bt (i). Another heuristic applied to our algorithm to reduce the search space for d and v is based on the observation that the average duration of phonetic units is usually no longer than 300 ms. Therefore, when computing Bt (i), we only consider speech segments that are shorter than 300 ms to avoid aligning letters to speech segments that are too long to be phonetic units. 4.3 Sampling φ~l�, π~l�,n�,p, βand θc Sampling φ~l� To compute the posterior distribution of φ~l�, we count how many times ~lκ is mapped to </context>
</contexts>
<marker>Glass, 2003</marker>
<rawString>James Glass. 2003. A probabilistic framework for segment-based speech recognition. Computer Speech and Language, 17:137 – 152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aren Jansen</author>
<author>Kenneth Church</author>
</authors>
<title>Towards unsupervised training of speaker independent acoustic models.</title>
<date>2011</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1693--1696</pages>
<contexts>
<context position="6339" citStr="Jansen and Church, 2011" startWordPosition="976" endWordPosition="979">. The advantage of our approach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. This property allows our model to potentially learn better pronunciations for less frequent words. The more recent work by Garcia and Gish (2006) and Siu et al. (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. Others who have more recently explored the unsupervised space include (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012). The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. The concept of creating a speech recognizer for a language with only orthographically annotated speech data has also been explored previously by means of graphemes. This approach has been shown to be effective for alphabetic languages with relatively straightforward grapheme to phoneme transformations and does not require any unsupervised learning o</context>
</contexts>
<marker>Jansen, Church, 2011</marker>
<rawString>Aren Jansen and Kenneth Church. 2011. Towards unsupervised training of speaker independent acoustic models. In Proceedings of INTERSPEECH, pages 1693 – 1696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Continuous speech recognition by statistical methods.</title>
<date>1976</date>
<booktitle>Proceedings of the IEEE, 64:532 –</booktitle>
<pages>556</pages>
<contexts>
<context position="13330" citStr="Jelinek, 1976" startWordPosition="2236" endWordPosition="2237"> a valley. The parameters of a Dirichlet distribution can also be viewed as pseudo-counts for each category. Eq. 4 shows that the prior for 7r~li ni p is seeded by pseudo-counts that are proportional to the mapping weights over the phonetic units of li in a shorter context. In other words, the mapping distribution of li in a shorter context can be thought of as a back-off distribution of li’s mapping weights in a longer context. Each component of the K-dimensional mixture model is linked to a 3-state Hidden Markov Model (HMM). These K HMMs are used to model the phonetic units in the language (Jelinek, 1976). The emission probability of each HMM state is modeled by a diagonal Gaussian Mixture Model (GMM). We use 0c to represent the set of parameters that define the cth HMM, which includes the state transition probability and the GMM parameters of each state emission distribution. The conjugate prior of 0c is denoted as H(00)2. Finally, to finish the generative process, for each ci,p we use the corresponding HMM 0ci,p to generate the observed speech data xt, and the generative process of the HMM determines the duration, 1An abbreviation of ir~li�0,ni,p 2H(Bo) includes a Dirichlet prior for the tra</context>
</contexts>
<marker>Jelinek, 1976</marker>
<rawString>Frederick Jelinek. 1976. Continuous speech recognition by statistical methods. Proceedings of the IEEE, 64:532 – 556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Johnson</author>
<author>Alan S Willsky</author>
</authors>
<title>Bayesian nonparametric hidden semi-markov models.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--673</pages>
<contexts>
<context position="15658" citStr="Johnson and Willsky (2013)" startWordPosition="2628" endWordPosition="2631">oss utterances. Finally, we briefly discuss the inference procedures for O~l, , 7r~ , Q, 0c � lκ,n,p 4.1 Block-sampling ni and ci,p To understand the message-passing algorithm in this study, it is helpful to think of our model as a simplified Hidden Semi-Markov Model (HSMM), in which the letters represent the states and the speech features are the observations. However, unlike in a regular HSMM, where the state sequence is hidden, in our case, the state sequence is fixed to be the given letter sequence. With this point of view, we can modify the message-passing algorithms of Murphy (2002) and Johnson and Willsky (2013) to compute the posterior information required for blocksampling ni and ci,p. Let L(xt) be a function that returns the index of the letter from which xt is generated; also, let Ft = 1 be a tag indicating that a new phone segment starts at t + 1. Given the constraint that 0 &lt; ni &lt; 2, for 0 &lt; i &lt; Lm and 0 &lt; t &lt; Tm, the backwards messages Bt(i) and Bt (i) for the mth training sample can be defined and computed as in Eq. 5 and Eq. 7. Note that for clarity we discard the index variable m in the derivation of the algorithm. 185 Bt(i) p(xt+1:T |L(xt) = i, Ft = 1) min{L,i+1+U1 X j=i+1 O~li,κ(0) (5) Bt</context>
</contexts>
<marker>Johnson, Willsky, 2013</marker>
<rawString>Matthew J. Johnson and Alan S. Willsky. 2013. Bayesian nonparametric hidden semi-markov models. Journal of Machine Learning Research, 14:673–701, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirjam Killer</author>
<author>Sebastian St¨uker</author>
<author>Tanja Schultz</author>
</authors>
<title>Grapheme based speech recognition.</title>
<date>2003</date>
<booktitle>In Proceeding of the Eurospeech,</booktitle>
<pages>3141--3144</pages>
<marker>Killer, St¨uker, Schultz, 2003</marker>
<rawString>Mirjam Killer, Sebastian St¨uker, and Tanja Schultz. 2003. Grapheme based speech recognition. In Proceeding of the Eurospeech, pages 3141–3144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chia-ying Lee</author>
<author>James Glass</author>
</authors>
<title>A nonparametric Bayesian approach to acoustic model discovery.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>40--49</pages>
<contexts>
<context position="6361" citStr="Lee and Glass, 2012" startWordPosition="980" endWordPosition="983">proach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. This property allows our model to potentially learn better pronunciations for less frequent words. The more recent work by Garcia and Gish (2006) and Siu et al. (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. Others who have more recently explored the unsupervised space include (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012). The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. The concept of creating a speech recognizer for a language with only orthographically annotated speech data has also been explored previously by means of graphemes. This approach has been shown to be effective for alphabetic languages with relatively straightforward grapheme to phoneme transformations and does not require any unsupervised learning of units or pronunciati</context>
<context position="22954" citStr="Lee and Glass, 2012" startWordPosition="4007" endWordPosition="4010">built by discovering the phonetic units and L2S pronunciation rules sequentially without using a lexicon. In this section, we provide a detailed description of the experimental setup. �Lm i=1 Nr�(j) = M m=1 C~l2,n,p(k) = Lm i=1 M m=1 �C~l1,n,p(k) = Cr2,n,p(k) ff[νi &lt; α2π~l1,n,p(k) ~l2EUr1 � i + α2π~l1,n,p(k)� i=1 (11) We use U~l1 = { ~l2|P(~l2) = ~l1} to denote the set of 187 77 &apos;y a0 a1 a2 90 K K (0.1)3 (10)100 1 0.1 0.2 * 2 100 Table 1: The values of the hyperparameters of our model. We use (a)D to denote a D-dimensional vector with all entries being a. *We follow the procedure reported in (Lee and Glass, 2012) to set up the HMM prior 00. 5.1 Dataset All the speech recognition experiments reported in this paper are performed on a weather query dataset, which consists of narrow-band, conversational telephone speech (Zue et al., 2000). We follow the experimental setup of McGraw et al. (2013) and split the corpus into a training set of 87,351 utterances, a dev set of 1,179 utterances and a test set of 3,497 utterances. A subset of 10,000 utterances is randomly selected from the training set. We use this subset of data for training our model to demonstrate that our model is able to discover the phonetic</context>
<context position="28924" citStr="Lee and Glass (2012)" startWordPosition="5008" endWordPosition="5011"> (Killer et al., 2003), in which each grapheme is listed as a question, to train a triphone grapheme model. Note that to enforce better initial alignments between the graphemes and the speech data, we use a pre-trained acoustic model to identify the non-speech segments at the beginning and the end of each utterance before starting training the monophone grapheme model. Our model jointly discovers the phonetic inventory and the L2S mapping rules from a set of transcribed data. An alternative of our approach is to learn the two latent structures sequentially. We follow the training procedure of Lee and Glass (2012) to learn a set of acoustic models from the speech data and use these acoustic models to generate a phone transcription for each utterance. The phone transcriptions along with the corresponding word transcriptions are fed as inputs to the L2S model proposed in Bisani and Ney (2008). A stochastic lexicon can be learned by applying the L2S model unit(%) Monophone Our model 17.0 Oracle 13.8 Grapheme 32.7 Sequential model 31.4 Table 2: Word error rates generated by the four monophone recognizers described in Sec. 5.2 and Sec. 5.3 on the weather query corpus. and the discovered acoustic models to P</context>
</contexts>
<marker>Lee, Glass, 2012</marker>
<rawString>Chia-ying Lee and James Glass. 2012. A nonparametric Bayesian approach to acoustic model discovery. In Proceedings of ACL, pages 40–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Hui Lee</author>
<author>Frank Soong</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>A segment model based approach to speech recognition.</title>
<date>1988</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>501--504</pages>
<contexts>
<context position="5145" citStr="Lee et al., 1988" startWordPosition="786" endWordPosition="789">onsistently generates close performance to recognizers that are trained with expertdefined phonetic inventory and lexicon. Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%. Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. 2 Related Work Various algorithms for learning sub-word based pronunciations were proposed in (Lee et al., 1988; Fukada et al., 1996; Bacchiani and Ostendorf, 1999; Paliwal, 1990). In these previous approaches, spoken samples of a word are gathered, and usually only one single pronunciation for the word is derived based on the acoustic evidence observed in the spoken samples. The major difference between our work and these previous works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approa</context>
</contexts>
<marker>Lee, Soong, Juang, 1988</marker>
<rawString>Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang. 1988. A segment model based approach to speech recognition. In Proceedings of ICASSP, pages 501– 504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian McGraw</author>
<author>Ibrahim Badr</author>
<author>James Glass</author>
</authors>
<title>Learning lexicons from speech using a pronunciation mixture model.</title>
<date>2013</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="23238" citStr="McGraw et al. (2013)" startWordPosition="4055" endWordPosition="4058">2EUr1 � i + α2π~l1,n,p(k)� i=1 (11) We use U~l1 = { ~l2|P(~l2) = ~l1} to denote the set of 187 77 &apos;y a0 a1 a2 90 K K (0.1)3 (10)100 1 0.1 0.2 * 2 100 Table 1: The values of the hyperparameters of our model. We use (a)D to denote a D-dimensional vector with all entries being a. *We follow the procedure reported in (Lee and Glass, 2012) to set up the HMM prior 00. 5.1 Dataset All the speech recognition experiments reported in this paper are performed on a weather query dataset, which consists of narrow-band, conversational telephone speech (Zue et al., 2000). We follow the experimental setup of McGraw et al. (2013) and split the corpus into a training set of 87,351 utterances, a dev set of 1,179 utterances and a test set of 3,497 utterances. A subset of 10,000 utterances is randomly selected from the training set. We use this subset of data for training our model to demonstrate that our model is able to discover the phonetic composition and the pronunciation rules of a language even from just a few hours of data. 5.2 Building a Recognizer from Our Model The values of the hyperparameters of our model are listed in Table 1. We run the inference procedure described in Sec. 4 for 10,000 times on the randoml</context>
<context position="26188" citStr="McGraw et al. (2013)" startWordPosition="4565" endWordPosition="4568">model is built by using the word transcriptions in the full training set. This language model is utilized in all speech recognition experiments reported in this paper. Finite State Transducers (FSTs) are used to build all the recognizers used in this study. With the language model, the lexicon and the context-independent acoustic model constructed by the methods described in this section, we can build a speech recognizer from the learning output of the proposed model without the need of a pre-defined phone inventory and any expert-crafted lexicons. 5.2.1 Pronunciation Mixture Model Retraining McGraw et al. (2013) presented the Pronunciation Mixture Model (PMM) for composing stochastic lexicons that outperform pronunciation dictionaries created by experts. Although the PMM framework was designed to incorporate and augment expert lexicons, we found that it can be adapted to polish the pronunciation list generated by our model. In particular, the training procedure for PMMs includes three steps. First, train a L2S model from a manually specified expert-pronunciation lexicon; second, generate a list of pronunciations for each word in the dataset using the L2S model; and finally, use an acoustic model to r</context>
<context position="29884" citStr="McGraw et al. (2013)" startWordPosition="5167" endWordPosition="5170">model unit(%) Monophone Our model 17.0 Oracle 13.8 Grapheme 32.7 Sequential model 31.4 Table 2: Word error rates generated by the four monophone recognizers described in Sec. 5.2 and Sec. 5.3 on the weather query corpus. and the discovered acoustic models to PMM. This two-stage approach for training a speech recognizer without an expert lexicon is referred to as the sequential model in this paper. Finally, we compare our system against a recognizer trained from an oracle recognition system. We build the oracle recognizer on the same weather query corpus by following the procedure presented in McGraw et al. (2013). This oracle recognizer is then applied to generate forced-aligned phone transcriptions for the training utterances, from which we can build both monophone and triphone acoustic models. The expert-crafted lexicon used in the oracle recognizer is also used in this baseline. Note that for training the triphone model, we compose a singleton question list (Killer et al., 2003) that has every expert-defined phonetic unit as a question. We use this singleton question list instead of a more sophisticated one to ensure that this baseline and our system differ only in the acoustic model and the lexico</context>
<context position="34004" citStr="McGraw et al. (2013)" startWordPosition="5844" endWordPosition="5847">.6 15.9 Oracle PMM iterations (Expert lexicon) 0 1 2 H (bit) 0.69 0.90 0.92 WER (%) 13.8 12.8 12.4 Table 3: The upper-half of the table shows the average pronunciation entropies, H, of the lexicons induced by our model and refined by PMM as well as the WERs of the monophone recognizers built with the corresponding lexicons for the weather query corpus. The definition of H can be found in Sec. 6.2. The first row of the lower-half of the table lists the average pronunciation entropies, H, of the expert-defined lexicon and the lexicons generated and weighted by the L2P-PMM framework described in McGraw et al. (2013). The second row of the lower-half of the table shows the WERs of the recognizers that are trained with the expert-lexicon and its PMM-refined versions. their respective PMM-refined versions4. In Table 3, we can see that the automatically-discovered lexicon and its PMM-reweighted versions have much higher H values than their expert-defined counterparts. These higher H values imply that the lexicon induced by our model contains more pronunciation variation than the expert-defined lexicon. Therefore, the lattices constructed during the decoding process for our recognizer tend to be larger than t</context>
</contexts>
<marker>McGraw, Badr, Glass, 2013</marker>
<rawString>Ian McGraw, Ibrahim Badr, and James Glass. 2013. Learning lexicons from speech using a pronunciation mixture model. IEEE Trans. on Speech and Audio Processing, 21(2):357–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<title>Hidden semi-Markov models (hsmms).</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>University of British Columbia.</institution>
<contexts>
<context position="15627" citStr="Murphy (2002)" startWordPosition="2624" endWordPosition="2626">e parallelized across utterances. Finally, we briefly discuss the inference procedures for O~l, , 7r~ , Q, 0c � lκ,n,p 4.1 Block-sampling ni and ci,p To understand the message-passing algorithm in this study, it is helpful to think of our model as a simplified Hidden Semi-Markov Model (HSMM), in which the letters represent the states and the speech features are the observations. However, unlike in a regular HSMM, where the state sequence is hidden, in our case, the state sequence is fixed to be the given letter sequence. With this point of view, we can modify the message-passing algorithms of Murphy (2002) and Johnson and Willsky (2013) to compute the posterior information required for blocksampling ni and ci,p. Let L(xt) be a function that returns the index of the letter from which xt is generated; also, let Ft = 1 be a tag indicating that a new phone segment starts at t + 1. Given the constraint that 0 &lt; ni &lt; 2, for 0 &lt; i &lt; Lm and 0 &lt; t &lt; Tm, the backwards messages Bt(i) and Bt (i) for the mth training sample can be defined and computed as in Eq. 5 and Eq. 7. Note that for clarity we discard the index variable m in the derivation of the algorithm. 185 Bt(i) p(xt+1:T |L(xt) = i, Ft = 1) min{L,</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Kevin P. Murphy. 2002. Hidden semi-Markov models (hsmms). Technical report, University of British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuldip Paliwal</author>
</authors>
<title>Lexicon-building methods for an acoustic sub-word based speech recognizer.</title>
<date>1990</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>729--732</pages>
<contexts>
<context position="5213" citStr="Paliwal, 1990" startWordPosition="798" endWordPosition="799">d with expertdefined phonetic inventory and lexicon. Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%. Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. 2 Related Work Various algorithms for learning sub-word based pronunciations were proposed in (Lee et al., 1988; Fukada et al., 1996; Bacchiani and Ostendorf, 1999; Paliwal, 1990). In these previous approaches, spoken samples of a word are gathered, and usually only one single pronunciation for the word is derived based on the acoustic evidence observed in the spoken samples. The major difference between our work and these previous works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approach is that pronunciation knowledge learned for a particular letter i</context>
</contexts>
<marker>Paliwal, 1990</marker>
<rawString>Kuldip Paliwal. 1990. Lexicon-building methods for an acoustic sub-word based speech recognizer. In Proceedings of ICASSP, pages 729–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Man-hung Siu</author>
<author>Herbert Gish</author>
<author>Arthur Chan</author>
<author>William Belfield</author>
<author>Steve Lowe</author>
</authors>
<title>Unsupervised training of an HMM-based self-organizing unit recgonizer with applications to topic classification and keyword discovery.</title>
<date>2013</date>
<journal>Computer, Speech, and Language.</journal>
<contexts>
<context position="6084" citStr="Siu et al. (2013)" startWordPosition="938" endWordPosition="941">us works is that our model learns word pronunciations in the context of letter sequences. More specifically, our model learns letter pronunciations first and then concatenates the pronunciation of each letter in a word to form the word pronunciation. The advantage of our approach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. This property allows our model to potentially learn better pronunciations for less frequent words. The more recent work by Garcia and Gish (2006) and Siu et al. (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. Others who have more recently explored the unsupervised space include (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012). The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. The concept of creating a speech recognizer for a language with only orthographically annotated </context>
</contexts>
<marker>Siu, Gish, Chan, Belfield, Lowe, 2013</marker>
<rawString>Man-hung Siu, Herbert Gish, Arthur Chan, William Belfield, and Steve Lowe. 2013. Unsupervised training of an HMM-based self-organizing unit recgonizer with applications to topic classification and keyword discovery. Computer, Speech, and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian St¨uker</author>
<author>Tanja Schultz</author>
</authors>
<title>A grapheme based speech recognition system for Russian.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th Conference Speech and Computer.</booktitle>
<marker>St¨uker, Schultz, 2004</marker>
<rawString>Sebastian St¨uker and Tanja Schultz. 2004. A grapheme based speech recognition system for Russian. In Proceedings of the 9th Conference Speech and Computer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balakrishnan Varadarajan</author>
<author>Sanjeev Khudanpur</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Unsupervised learning of acoustic sub-word units.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>165--168</pages>
<contexts>
<context position="6314" citStr="Varadarajan et al., 2008" startWordPosition="972" endWordPosition="975">orm the word pronunciation. The advantage of our approach is that pronunciation knowledge learned for a particular letter in some arbitrary word can subsequently be used to help learn the letter’s pronunciation in other words. This property allows our model to potentially learn better pronunciations for less frequent words. The more recent work by Garcia and Gish (2006) and Siu et al. (2013) has made extensive use of self-organizing units for keyword spotting and other tasks for languages with limited linguistic resources. Others who have more recently explored the unsupervised space include (Varadarajan et al., 2008; Jansen and Church, 2011; Lee and Glass, 2012). The latter work introduced a nonparametric Bayesian inference procedure for automatically learning acoustic units that is most similar to our current work except that our model also infers word pronunciations simultaneously. The concept of creating a speech recognizer for a language with only orthographically annotated speech data has also been explored previously by means of graphemes. This approach has been shown to be effective for alphabetic languages with relatively straightforward grapheme to phoneme transformations and does not require an</context>
</contexts>
<marker>Varadarajan, Khudanpur, Dupoux, 2008</marker>
<rawString>Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. 2008. Unsupervised learning of acoustic sub-word units. In Proceedings of ACL-08: HLT, Short Papers, pages 165–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve J Young</author>
<author>J J Odell</author>
<author>Philip C Woodland</author>
</authors>
<title>Tree-based state tying for high accuracy acoustic modelling.</title>
<date>1994</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>307--312</pages>
<contexts>
<context position="27537" citStr="Young et al., 1994" startWordPosition="4781" endWordPosition="4784">oses, we simply plug in the word pronunciations and the acoustic model generated by our model. Once we obtain the re-weighted lexicon, we re-generate forced 188 phone alignments and retrain the acoustic model, which can be utilized to repeat the PMM lexicon reweighting procedure. For our experiments, we iterate through this model refining process until the recognition performance converges. 5.2.2 Triphone Model Conventionally, to train a context-dependent acoustic model, a list of questions based on the linguistic properties of phonetic units is required for growing decision tree classifiers (Young et al., 1994). However, such language-specific knowledge is not available for our training framework; therefore, our strategy is to compile a question list that treats each phonetic unit as a unique linguistic class. In other words, our approach to training a contextdependent acoustic model for the automatically discovered units is to let the decision trees grow fully based on acoustic evidence. 5.3 Baselines We compare the recognizers trained by following the procedures described in Sec. 5.2 against three baselines. The first baseline is a grapheme-based speech recognizer. We follow the procedure describe</context>
</contexts>
<marker>Young, Odell, Woodland, 1994</marker>
<rawString>Steve J. Young, J.J. Odell, and Philip C. Woodland. 1994. Tree-based state tying for high accuracy acoustic modelling. In Proceedings of HLT, pages 307–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>James Glass</author>
<author>Joseph Polifroni</author>
<author>Christine Pao</author>
<author>Timothy J Hazen</author>
<author>Lee Hetherington</author>
</authors>
<title>Jupiter: A telephone-based conversational interface for weather information.</title>
<date>2000</date>
<booktitle>IEEE Trans. on Speech and Audio Processing,</booktitle>
<pages>8--85</pages>
<contexts>
<context position="4487" citStr="Zue et al., 2000" startWordPosition="684" endWordPosition="687">ch recognizers for all languages of the world, instead of the less than 2% that currently exist. In this paper, we investigate the problem of inferring a pronunciation lexicon from an annotated corpus without exploiting any language-specific knowledge. We formulate our approach as a hierarchical Bayesian model, which jointly discovers the acoustic inventory and the latent encoding scheme between the letters and the sounds of a language. We evaluate the quality of the induced lexicon and acoustic model through a series of speech recognition experiments on a conversational weather query corpus (Zue et al., 2000). The results demonstrate that our model consistently generates close performance to recognizers that are trained with expertdefined phonetic inventory and lexicon. Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%. Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach. 2 Related Work Various algorithms for learning sub-wo</context>
<context position="23180" citStr="Zue et al., 2000" startWordPosition="4044" endWordPosition="4047"> m=1 �C~l1,n,p(k) = Cr2,n,p(k) ff[νi &lt; α2π~l1,n,p(k) ~l2EUr1 � i + α2π~l1,n,p(k)� i=1 (11) We use U~l1 = { ~l2|P(~l2) = ~l1} to denote the set of 187 77 &apos;y a0 a1 a2 90 K K (0.1)3 (10)100 1 0.1 0.2 * 2 100 Table 1: The values of the hyperparameters of our model. We use (a)D to denote a D-dimensional vector with all entries being a. *We follow the procedure reported in (Lee and Glass, 2012) to set up the HMM prior 00. 5.1 Dataset All the speech recognition experiments reported in this paper are performed on a weather query dataset, which consists of narrow-band, conversational telephone speech (Zue et al., 2000). We follow the experimental setup of McGraw et al. (2013) and split the corpus into a training set of 87,351 utterances, a dev set of 1,179 utterances and a test set of 3,497 utterances. A subset of 10,000 utterances is randomly selected from the training set. We use this subset of data for training our model to demonstrate that our model is able to discover the phonetic composition and the pronunciation rules of a language even from just a few hours of data. 5.2 Building a Recognizer from Our Model The values of the hyperparameters of our model are listed in Table 1. We run the inference pro</context>
</contexts>
<marker>Zue, Seneff, Glass, Polifroni, Pao, Hazen, Hetherington, 2000</marker>
<rawString>Victor Zue, Stephanie Seneff, James Glass, Joseph Polifroni, Christine Pao, Timothy J. Hazen, and Lee Hetherington. 2000. Jupiter: A telephone-based conversational interface for weather information. IEEE Trans. on Speech and Audio Processing, 8:85–96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9899355">
Joint Chinese Word Segmentation and POS Tagging on
Heterogeneous Annotated Corpora with Multiple Task Learning
</title>
<author confidence="0.902595">
Xipeng Qiu, Jiayi Zhao, Xuanjing Huang
</author>
<affiliation confidence="0.687029">
Fudan University, 825 Zhangheng Road, Shanghai, China
</affiliation>
<email confidence="0.992071">
xpqiu@fudan.edu.cn, zjy.fudan@gmail.com, xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.979919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998670739130435">
Chinese word segmentation and part-of-
speech tagging (S&amp;T) are fundamental
steps for more advanced Chinese language
processing tasks. Recently, it has at-
tracted more and more research interests
to exploit heterogeneous annotation cor-
pora for Chinese S&amp;T. In this paper, we
propose a unified model for Chinese S&amp;T
with heterogeneous annotation corpora.
We first automatically construct a loose
and uncertain mapping between two rep-
resentative heterogeneous corpora, Penn
Chinese Treebank (CTB) and PKU’s Peo-
ple’s Daily (PPD). Then we regard the
Chinese S&amp;T with heterogeneous corpora
as two “related” tasks and train our model
on two heterogeneous corpora simultane-
ously. Experiments show that our method
can boost the performances of both of the
heterogeneous corpora by using the shared
information, and achieves significant im-
provements over the state-of-the-art meth-
ods.
</bodyText>
<sectionHeader confidence="0.997334" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999504680851064">
Currently, most of statistical natural language
processing (NLP) systems rely heavily on manu-
ally annotated resources to train their statistical
models. The more of the data scale, the better
the performance will be. However, the costs are
extremely expensive to build the large scale re-
sources for some NLP tasks. Even worse, the ex-
isting resources are often incompatible even for a
same task and the annotation guidelines are usu-
ally different for different projects, since there
are many underlying linguistic theories which
explain the same language with different per-
spectives. As a result, there often exist multi-
ple heterogeneous annotated corpora for a same
task with vastly different and incompatible an-
notation philosophies. These heterogeneous re-
sources are waste on some level if we cannot fully
exploit them.
However, though most of statistical NLP
methods are not bound to specific annota-
tion standards, almost all of them cannot deal
simultaneously with the training data with
different and incompatible annotation. The
co-existence of heterogeneous annotation data
therefore presents a new challenge to utilize
these resources.
The problem of incompatible annotation stan-
dards is very serious for many tasks in NLP,
especially for Chinese word segmentation and
part-of-speech (POS) tagging (Chinese S&amp;T). In
Chinese S&amp;T, the annotation standards are of-
ten incompatible for two main reasons. One is
that there is no widely accepted segmentation
standard due to the lack of a clear definition
of Chinese words. Another is that there are no
morphology for Chinese word so that there are
many ambiguities to tag the parts-of-speech for
Chinese word. For example, the two commonly-
used corpora, PKU’s People’s Daily (PPD) (Yu
et al., 2001) and Penn Chinese Treebank (CTB)
(Xia, 2000), use very different segmentation and
POS tagging standards.
For example, in Table 1, it is very different
to annotate the sentence “刘翔进入中国C总
决赛 (Liu Xiang reaches the national final in
China)” with guidelines of CTB and PDD. PDD
breaks some phrases, which are single words in
</bodyText>
<page confidence="0.967281">
658
</page>
<note confidence="0.8978062">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658–668,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Liu Xiang reachs China final
CTB 刘翔/NR 进入/VV 中国区/NN 总决赛/NN
PDD 刘/nrf 翔/nrg 进入/v 中国/ns 区/n 总/b 决赛/vn
</note>
<tableCaption confidence="0.999189">
Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD
</tableCaption>
<bodyText confidence="0.999764186046512">
CTB, into two words. The POS tagsets are also
significantly different. For example, PDD gives
diverse tags “n” and “vn” for the noun, while
CTB just gives “NN”. For proper names, they
may be tagged as “nr”, “ns”, etc in PDD, while
they are just tagged as “NR” in CTB.
Recently, it has attracted more and more re-
search interests to exploit heterogeneous anno-
tation data for Chinese word segmentation and
POS tagging. (Jiang et al., 2009) presented a
preliminary study for the annotation adapta-
tion topic. (Sun and Wan, 2012) proposed a
structure-based stacking model to fully utilize
heterogeneous word structures. They also re-
ported that there is no one-to-one mapping be-
tween the heterogeneous word classification and
the mapping between heterogeneous tags is very
uncertain.
These methods usually have a two-step pro-
cess. The first step is to train the preliminary
taggers on heterogeneous annotations. The sec-
ond step is to train the final taggers by using
the outputs of the preliminary taggers as fea-
tures. We call these methods as “pipeline-
based” methods.
In this paper, we propose a method for joint
Chinese word segmentation and POS tagging
with heterogeneous annotation corpora. We re-
gard the Chinese S&amp;T with heterogeneous cor-
pora as two “related” tasks which can improve
the performance of each other. Since it is impos-
sible to establish an exact mapping between two
annotations, we first automatically construct a
loose and uncertain mapping the heterogeneous
tagsets of CTB and PPD. Thus we can tag a sen-
tence in one style with the help of the “related”
information in another heterogeneous style. The
proposed method can improve the performances
of joint Chinese S&amp;T on both corpora by using
the shared information of each other, which is
proven effective by experiments.
There are three main contributions of our
model:
</bodyText>
<listItem confidence="0.991833214285714">
• First, we regard these two joint S&amp;T tasks
on different corpora as two related tasks
which have interdependent and peer rela-
tionship.
• Second, different to the pipeline-based
methods, our model can be trained simul-
taneously on the heterogeneous corpora.
Thus, it can also produce two different
styles of POS tags.
• Third, our model do not depend on the
exactly correct mappings between the two
heterogeneous tagsets. The correct map-
ping relations can be automatically built in
training phase.
</listItem>
<bodyText confidence="0.999736">
The rest of the paper is organized as follows:
We first introduce the related works in section 2
and describe the background of character-based
method for joint Chinese S&amp;T in section 3. Sec-
tion 4 presents an automatic method to build
the loose mapping function. Then we propose
our method on heterogeneous corpora in 5 and
</bodyText>
<listItem confidence="0.973031">
6. The experimental results are given in section
7. Finally, we conclude our work in section 8.
</listItem>
<sectionHeader confidence="0.986666" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999844916666667">
There are some works to exploit heteroge-
neous annotation data for Chinese S&amp;T.
(Gao et al., 2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
However, this converter need human designed
transformation templates, and is hard to be gen-
eralized to POS tagging.
(Jiang et al., 2009) proposed an automatic
adaptation method of heterogeneous annotation
standards, which depicts a general pipeline to in-
tegrate the knowledge of corpora with different
</bodyText>
<page confidence="0.999102">
659
</page>
<figureCaption confidence="0.968686">
Figure 1: Traditional Pipeline-based Strategy for
Heterogeneous POS Tagging
</figureCaption>
<bodyText confidence="0.999495307692308">
underling annotation guidelines. They further
proposed two optimization strategies, iterative
training and predict-self re-estimation, to fur-
ther improve the accuracy of annotation guide-
line transformation (Jiang et al., 2012).
(Sun and Wan, 2012) proposed a structure-
based stacking model to fully utilize heteroge-
neous word structures.
These methods regard one annotation as the
main target and another annotation as the com-
plementary/auxiliary purposes. For example, in
their solution, an auxiliary tagger TaggerPPD
is trained on a complementary corpus PPD, to
assist the target CTB-style TaggerCTB. To re-
fine the character-based tagger, PPD-style char-
acter labels are directly incorporated as new
features. The brief sketch of these methods is
shown in Figure 1.
The related work in machine learning liter-
ature is multiple task learning (Ben-David and
Schuller, 2003), which learns a problem together
with other related problems at the same time,
using a shared representation. This often leads
to a better model for the main task, because
it allows the learner to use the commonality
among the tasks. Multiple task learning has
been proven quite successful in practice and has
been also applied to NLP (Ando and Zhang,
2005). We also preliminarily verified that mul-
tiple task learning can improve the performance
on this problem in our previous work (Zhao et
al., 2013), which is a simplified case of the work
in this paper and has a relative low complexity.
Different with the multiple task learning,
whose tasks are actually different labels in the
same classification task, our model utilizes the
shared information between the real different
tasks and can produce the corresponding differ-
ent styles of outputs.
</bodyText>
<sectionHeader confidence="0.9080095" genericHeader="method">
3 Joint Chinese Word Segmentation
and POS Tagging
</sectionHeader>
<bodyText confidence="0.999671166666667">
Currently, the mainstream method of Chi-
nese POS tagging is joint segmentation &amp; tag-
ging with character-based sequence labeling
models(Lafferty et al., 2001), which can avoid
the problem of segmentation error propagation
and achieve higher performance on both sub-
tasks(Ng and Low, 2004; Jiang et al., 2008; Sun,
2011; Qiu et al., 2012).
The label of each character is the cross-
product of a segmentation label and a tagging
label. If we employ the commonly used label set
{B, I, E, S} for the segmentation part of cross-
labels ({B, I, E} represent Begin, Inside, End of
a multi-node segmentation respectively, and S
represents a Single node segmentation), the la-
bel of character can be in the form of {B-T}(T
represents POS tag). For example, B-NN indi-
cates that the character is the begin of a noun.
</bodyText>
<sectionHeader confidence="0.826338" genericHeader="method">
4 Automatically Establishing the
</sectionHeader>
<subsectionHeader confidence="0.8597575">
Loose Mapping Function for the
Labels of Characters
</subsectionHeader>
<bodyText confidence="0.991595538461538">
To combine two human-annotated corpora,
the relationship of their guidelines should be
found. A mapping function should be estab-
lished to represent the relationship between two
different annotation guidelines. However, the
exact mapping relations are hard to establish.
As reported in (Sun and Wan, 2012), there is
no one-to-one mapping between their heteroge-
neous word classification, and the mapping be-
tween heterogeneous tags is very uncertain.
Fortunately, there is a loose mapping
can be found in CTB annotation guide-
line&apos; (Xia, 2000). Table 2 shows some
</bodyText>
<footnote confidence="0.337998">
&apos;Available at http://www.cis.upenn.edu/ ˜chi-
</footnote>
<figure confidence="0.996296571428572">
Input: x
z=f(x)
Output: f(x)
y=h(x,f(x))
Output: CTB-style Tags
TaggerPPo
TaggerCTB
</figure>
<page confidence="0.957977">
660
</page>
<table confidence="0.896980833333333">
CTB’s Tag PDD’ Tagl
Total tags 33 26
verbal noun NN v[+nom]
proper noun NR n
是 (shi4) VC v
有 (you3) VE, VV v
conjunctions CC, CS c
other verb VV, VA v, a, z
number CD, OD m
1 The tag set of PDD just includes the 26 broad
categories in the mapping table. The whole tag set
of PDD has 103 sub categories.
</table>
<tableCaption confidence="0.9903625">
Table 2: Examples of mapping between CTB and
PDD’s tagset
</tableCaption>
<bodyText confidence="0.993715923076923">
mapping relations in CTB annotation guide-
line. These loose mapping relations are
many-to-many mapping. For example, the
mapping may be “NN/CTB↔{n,nt,nz}/PDD”,
“NR/CTB↔{nr,ns}/PDD”, “v/PDD↔{VV,
VA}/CTB” and so on.
We define T1 and T2 as the tag sets for two
different annotations, and t1 ∈ T1 and t2 ∈ T2
are the corresponding tags in two tag sets re-
spectively.
We first establish a loose mapping function
m : T1 × T2 → {0, 1} between the tags of CTB
and PDD.
</bodyText>
<equation confidence="0.906134">
{ 1 if t1 and t2 have mapping relation
m(t1,t2) = 0 else
(1)
</equation>
<bodyText confidence="0.995413659090909">
The mapping relations are automatically
build from the CTB guideline (Xia, 2000). Due
to the fact that the tag set of PPD used in
the CTB guideline is just broad categories, we
expand the mapping relations to include the
sub categories. If a PPD’s tag is involved
in the mapping, all its sub categories should
be involved. For example, for the mapping
“NR/CTB↔nr/PDD”, the relation of NR and
nrf/nrg should be added in the mapping rela-
tions too (nrf/nrg belong to nr).
Since we use the character-based joint S&amp;T
model, we also need to find the mapping func-
tion between the labels of characters.
nese/posguide.3rd.ch.pdf
In this paper, we employ the commonly used
label set {B, I, E, S} for the segmentation part
of cross-labels and the label of character can be
in the form of {B-T}(T represents POS tag).
Thus, each mapping relation t1 ↔ t2 can be
automatically transformed to four forms: B-
t1 ↔B-t2, I-t1 ↔I-t2, E-t1 ↔E-t2 and S-t1 ↔S-
t2. (“B-NR/CTB↔{B-nr,B-ns}/PPD” for ex-
ample).
Beside the above transformation, we also
give a slight modification to adapt the dif-
ferent segmentation guidelines. For in-
stance, the person name “莫 a (Mo Yan)”
is tagged as “B-NR, E-NR” in CTB but
“S-nrf, S-nrg” in PPD. So, some spe-
cial mappings may need to be added like
“B-NR/CTB↔S-nrf/PPD”, “E-NR/CTB↔{S-
nrg, E-nrg}/PPD”, “M-NR/CTB↔{B-nrg, M-
nrg}/PPD” and so on. Although these spe-
cial mappings are also established automatically
with an exhaustive solution. In fact, we give seg-
mentation alignment only to proper names due
to the limitation of computing ability.
Thus, we can easily build the loose bidirec-
tional mapping function m� for the labels of
characters. An illustration of our construction
flowchart is shown in Figure 2.
Finally, total 524 mappings relationships are
established.
</bodyText>
<sectionHeader confidence="0.941815" genericHeader="method">
5 Joint Chinese S&amp;T with
</sectionHeader>
<subsectionHeader confidence="0.94027">
Heterogeneous Data with Multiple
Task Learning
</subsectionHeader>
<bodyText confidence="0.999947166666667">
Inspired by the multiple task learning (Ben-
David and Schuller, 2003), we can regard the
joint Chinese S&amp;T with heterogeneous data as
two “related” tasks, which can improve the
performance of each other simultaneously with
shared information.
</bodyText>
<subsectionHeader confidence="0.987227">
5.1 Sequence Labeling Model
</subsectionHeader>
<bodyText confidence="0.99461">
We first introduce the commonly used se-
quence labeling model in character-based joint
Chinese S&amp;T.
</bodyText>
<equation confidence="0.641066666666667">
Sequence labeling is the task of assigning la-
bels y = y1, ... , yn(yi ∈ Y) to an input sequence
x = x1, ... , xn. Y is the set of labels.
</equation>
<page confidence="0.98178">
661
</page>
<figure confidence="0.961291">
mapping function m()
between tags
mapping function m()
~
between labels
</figure>
<figureCaption confidence="0.637462">
Figure 2: An Illustration of Automatically Establishing the Loose Mapping Function
</figureCaption>
<figure confidence="0.878897285714286">
NR
B-NR ... S-NR
CTB-style NR
B-nrf B-nrg ... S-nrg S-nrg
nr
nrf nrg
PPD-style
</figure>
<bodyText confidence="0.806453666666667">
Given a sample x, we define the feature
4)(x, y). Thus, we can label x with a score func-
tion,
</bodyText>
<equation confidence="0.8801925">
y� = arg max S(w, 4)(x, y)), (2)
y
</equation>
<bodyText confidence="0.999968307692308">
where w is the parameter of score function S(·).
The feature vector 4)(x, y) consists of lots of
overlapping features, which is the chief benefit of
discriminative model. Different algorithms vary
in the definition of S(·) and the corresponding
objective function. S(·) is usually defined as lin-
ear or exponential family function.
For first-order sequence labeling, the feature
can be denoted as Ok(x, yi−1:i), where i stands
for the position in the sequence and k stands for
the number of feature templates. For the linear
classifier, the score function can be rewritten in
detail as
</bodyText>
<equation confidence="0.99537">
y� = arg max EL ((u, f(x, yi)) + (v, g(x, yi−1:i))) ,
y i=1
</equation>
<bodyText confidence="0.9121885">
(3)
where yi:j denotes label subsequence
yiyi+1 · · · yj; f and g denote the state and
transition feature vectors respectively, u and v
are their corresponding weight vectors; L is the
length of x.
</bodyText>
<subsectionHeader confidence="0.996158">
5.2 The Proposed Model
</subsectionHeader>
<bodyText confidence="0.9859845">
Different to the single task learning, the het-
erogeneous data have two sets of labels Y and
Z.
The heterogeneous datasets SDs and SDs con-
sist of {xi, yi}(i = 0, · · · , m) and {xi, zi}(i =
0, · · · , n) respectively.
For a sequence x = x1, ... , xL with length
L. , there may have two output sequence labels
y = y1, . . . , yL and z = z1, ... , zL, where yi E Y
and zi E Z.
We rewrite the loose mapping function m� be-
tween two label sets into the following forms,
</bodyText>
<equation confidence="0.9999875">
W(y) = {zJ m(y, z) = 1}, (4)
W(z) = {yJ m(y, z) = 1}, (5)
</equation>
<bodyText confidence="0.999374857142857">
where W(z) C Y and W(y) C Z are the subsets
of Y and Z. Give a label y(or z) in an annota-
tion, the loose mapping function W returns the
corresponding mapping label set in another het-
erogeneous annotation.
Our model for heterogeneous sequence label-
ing can be write as
</bodyText>
<equation confidence="0.9733608125">
((u, f(x, yi))
E+ (s, h(x, z))
zEW(yi)
+ (v1, g1(x, yi−1:i))
E+ (v2, )g2(x,zi−1:i)) , (6)
zi−1Eφ(yi−1)
ziEφ(yi)
and
z� = arg max EL ( E f(x, y))+
z,ziEZ i=1 (u,
yEW(zi)
(s, h(x, zi))
E+ (v1, g1(x, yi−1:i))
yi−1Eφ(zi−1)
yiEφ(zi)
)+ (v2, g2(x, zi−1:i)) , (7)
</equation>
<bodyText confidence="0.9998986">
where f and h represent the state feature vectors
on two label sets Y and Z respectively.
In Eq.(6) and (7), the score of the label of
every character is decided by the weights of the
corresponding mapping labels and itself.
</bodyText>
<equation confidence="0.91758575">
y� = arg max
y,yiEY
EL
i=1
</equation>
<page confidence="0.912297">
662
</page>
<figure confidence="0.749599714285714">
Input sequence: x
Output: PPD-style Tags
Output: CTB-style Tags
Shared
TaggerPPD TaggerCTB
Information
6 Training
</figure>
<bodyText confidence="0.997553">
We use online Passive-Aggressive (PA) algo-
rithm (Crammer and Singer, 2003; Crammer et
al., 2006) to train the model parameters. Fol-
lowing (Collins, 2002), the average strategy is
used to avoid the overfitting problem.
For the sake of simplicity, we merge the Eq.(6)
and (7) into a unified formula.
Given a sequence x and the expect type of
tags T, the merged model is
</bodyText>
<figureCaption confidence="0.965563666666667">
Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8)
y zEψ(y)
t(y)=T
</figureCaption>
<bodyText confidence="0.9999634375">
The main challenge of our model is the effi-
ciency of decoding algorithm, which is similar to
structured learning with latent variables(Liang
et al., 2006) (Yu and Joachims, 2009). Most
methods for structured learning with latent vari-
ables have not expand all possible mappings.
In this paper, we also only expand the map-
ping that with highest according to the current
model.
Our model is shown in Figure 3 and the
flowchart is shown in Algorithm 1. If given the
output type of label T, we only consider the la-
bels in T to initialize the Viterbi matrix, and
the score of each node is determined by all the
involved heterogeneous labels according to the
loose mapping function.
</bodyText>
<equation confidence="0.877797428571429">
input : character sequence x1:L
loose mapping function φ
output type: T(T ∈ {Ty,Tz})
output: label sequence ls
if T == Ty then
calculate ls using Eq. (6);
else if T == Tz then
</equation>
<bodyText confidence="0.503888555555556">
calculate ls using Eq. (7) ;
else
return null;
end
return ls
Algorithm 1: Flowchart of the Tagging pro-
cess of the proposed model
where t(y) is a function to judge the type of
output tags; ψ(y) represents the set {φ(y1) ⊗
</bodyText>
<equation confidence="0.990494">
φ(y2) ⊗ · · · ⊗ φ(yL)} ∪ {y}, where ⊗ means
Cartesian product; w = (uT, sT, vT1 , vT2 )T and
Φ = (fT,hT,gT1 ,gT2 )T.
</equation>
<bodyText confidence="0.858204">
We redefine the score function as
</bodyText>
<equation confidence="0.9962815">
∑S(w, x, y) = ⟨w, Φ(x, z)⟩. (9)
zEψ(y)
</equation>
<bodyText confidence="0.6311935">
Thus, we rewrite the model into a unified for-
mula
</bodyText>
<equation confidence="0.99631">
yˆ = arg max S(w, x, y). (10)
y
t(y)=T
</equation>
<bodyText confidence="0.9996915">
Given an example (x, y), yˆ is denoted as the
incorrect label sequence with the highest score
</bodyText>
<equation confidence="0.9918032">
yˆ = arg max S(w, x, ¯y). (11)
3̸=y
t(Y)=t(y)
The margin γ(w; (x, y)) is defined as
γ(w; (x, y)) = S(w, x, y) − S(w, x, ˆy). (12)
Thus, we calculate the hinge loss
ℓ(w; (x, y)), (abbreviated as ℓw) by
ℓw ={0, γ(w; (x, y)) &gt; 1
1 − γ(w; (x, y)), otherwise
(13)
</equation>
<bodyText confidence="0.683233">
In round k, the new weight vector wk+1 is
calculated by
</bodyText>
<equation confidence="0.9983975">
wk+1 = arg min
w 2||w − wk||2 + C · ξ,
1
s.t. ℓ(w; (xk, yk)) &lt;= ξ and ξ &gt;= 0 (14)
</equation>
<page confidence="0.98517">
663
</page>
<bodyText confidence="0.999586">
where ξ is a non-negative slack variable, and C
is a positive parameter which controls the influ-
ence of the slack term on the objective function.
Following the derivation in PA (Crammer et
al., 2006), we can get the update rule,
</bodyText>
<equation confidence="0.997335">
wk+1 = wk + Tkek, (15)
</equation>
<bodyText confidence="0.537353">
where
</bodyText>
<equation confidence="0.9989205">
∑ek = ∑�(xk, z) − 4)(xk, z),
zEψ(yk� zEψ( Yk�
ℓwk
Tk = min(C, ∥ek∥2 ).
</equation>
<bodyText confidence="0.999980789473684">
As we can see from the Eq. (15), when we up-
date the weight vector, the update information
includes not only the features extracted from
current input, but also that extracted from the
loose mapping sequence of input. For each fea-
ture, the weights of its corresponding related
features derived from the loose mapping func-
tion will be updated with the same magnitude
as well as itself.
Our method regards two annotations to be in-
terdependence and peer relationship. Therefore,
the two heterogeneous annotated corpora can be
simultaneously used as the input of our training
algorithm. Because of the tagging and training
algorithm, the weights and tags of two corpora
can be used separately with the only dependent
part built by the loose mapping function.
Our training algorithm based on PA is shown
in Algorithm 2.
</bodyText>
<subsectionHeader confidence="0.993435">
6.1 Analysis
</subsectionHeader>
<bodyText confidence="0.999983615384615">
Although our mapping function between two
heterogeneous annotations is loose and uncer-
tain, our online training method can automat-
ically increase the relative weights of features
from the beneficial mapping relations and de-
crease the relative weights of features from the
unprofitable mapping relations.
Consider an illustrative loose mapping re-
lation “NN/CTB↔n,nt,nz/PDD”. For an in-
put sequence x and PDD-style output is ex-
pected. If the algorithm tagging a charac-
ter as “n/PDD”(with help of the weight of
“NN/CTB”) and the right tag isn’t one of
</bodyText>
<equation confidence="0.933184">
input : mixed heterogeneous datasets:
(xi,yi),i = 1,··· ,N;
parameters: C, K;
loose mapping function: W ;
output: wK
Initialize: wTemp ← 0, w ← 0;
for k = 0···K − 1 do
for i = 1 ··· N do
</equation>
<bodyText confidence="0.90227825">
receive an example (xi, yi);
predict: yi with Eq.(11);
if hinge loss ℓw &gt; 0 then
update w with Eq. (15);
</bodyText>
<equation confidence="0.926524833333333">
end
end
wTemp = wTemp + w ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
</equation>
<bodyText confidence="0.999912666666667">
“n,nt,nz/PDD”, the weight of “NN/CTB” will
also be decreased, which is reasonable since
it is beneficial to distinguish the right tag.
And if the right tag is one of “n,nt,nz/PDD”
but not “n/PDD” (for example, “nt/PDD”),
which means it is a “NN/CTB”, the weight of
“NN/CTB” will remain unchanged according to
the algorithm (updating “n/PDD” changes the
“NN/CTB”, but updating “nt/PDD” changes it
back).
Therefore, after multiple iterations, useful fea-
tures derived from the mapping function are
typically receive more updates, which take rela-
tively more responsibility for correct prediction.
The final model has good parameter estimates
for the shared information.
We implement our method based on Fu-
danNLP(Qiu et al., 2013).
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.878678">
7.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99948925">
We use the two representative corpora men-
tioned above, Penn Chinese Treebank (CTB)
and PKU’s People’s Daily (PPD) in our ex-
periments.
</bodyText>
<page confidence="0.995798">
664
</page>
<table confidence="0.9999376">
Dataset Partition Sections Words
CTB-5 Training 1−270 0.47M
400−931
1001−1151
Develop 301−325 6.66K
Test 271−300 7.82K
CTB-S Training 0.64M
Test - 59.96K
PPD Training - 1.11M
Test - 0.16M
</table>
<tableCaption confidence="0.99955">
Table 3: Data partitioning for CTB and PD
</tableCaption>
<subsubsectionHeader confidence="0.709341">
7.1.1 CTB Dataset
</subsubsectionHeader>
<bodyText confidence="0.999236">
To better comparison with the previous
works, we use two commonly used criterions to
partition CTB dataset into the train and test
sets.
</bodyText>
<listItem confidence="0.99420975">
• One is the partition criterion used in (Jin
and Chen, 2008; Jiang et al., 2009; Sun and
Wan, 2012) for CTB 5.0.
• Another is the CTB dataset from the
POS tagging task of the Fourth Interna-
tional Chinese Language Processing Bake-
off (SIGHAN Bakeoff 2008)(Jin and Chen,
2008).
</listItem>
<subsubsectionHeader confidence="0.83751">
7.1.2 PPD Dataset
</subsubsectionHeader>
<bodyText confidence="0.9963655">
For the PPD dataset, we use the PKU dataset
from SIGHAN Bakeoff 2008.
The details of all datasets are shown in Table
3. Our experiment on these datasets may lead to
a fair comparison of our system and the related
works.
</bodyText>
<subsectionHeader confidence="0.997764">
7.2 Setting
</subsectionHeader>
<bodyText confidence="0.9998251">
We conduct two experiments on CTB-5 +
PPD and CTB-S + PPD respectively.
The form of feature templates we used is
shown in Table 7.2, where C represents a Chi-
nese character, and T represents the character-
based tag. The subscript i indicates its position
related to the current character.
Our method can be easily combined with
some other complicated models, but we only use
the simple one for the purpose of observing the
</bodyText>
<equation confidence="0.974357666666667">
Ci, T0(i = −2,−1,0,1,2)
Ci, Ci+1, T0(i = −1, 0)
T_1, T0
</equation>
<tableCaption confidence="0.934153">
Table 4: Feature Templates
</tableCaption>
<bodyText confidence="0.999407230769231">
sole influence of our unified model. The parame-
ter C is tested on develop dataset, and we found
that it just impact the speed of convergence and
have no effect on the accuracy. Moreover, since
we use the averaged strategy, we wish more iter-
ations to avoid overfitting and set a small value
0.01 to it. The maximum number of iterations
K is 50.
The F1 score is used for evaluation, which is
the harmonic mean of precision P (percentage of
predict phrases that exactly match the reference
phrases) and recall R (percentage of reference
phrases that returned by system).
</bodyText>
<subsectionHeader confidence="0.875824">
7.3 Evaluation on CTB-5 + PPD
</subsectionHeader>
<bodyText confidence="0.998930148148148">
The experiment results on the heterogeneous
corpora CTB-5 + PPD are shown in Table
5. Our method obtains an error reductions of
24.08% and 90.8% over the baseline on CTB-5
and PDD respectively.
Our method also gives better performance
than the pipeline-based methods on heteroge-
neous corpora, such as (Jiang et al., 2009) and
(Sun and Wan, 2012).
The reason is that our model can utilize the
information of both corpora effectively, which
can boost the performance of each other.
Although the loose mapping function are bidi-
rectional between two annotation tagsets, we
may also use unidirectional mapping. Therefore,
we also evaluate the performance when we use
unidirectional mapping. We just use the map-
ping function OPDD-CTB, which means we ob-
tain the PDD-style output without the informa-
tion from CTB in tagging stage. Thus, in train-
ing stage, there are no updates for the weights of
CTB-features for the instances from PDD cor-
pus, while instances from CTB corpus can result
to updates for PDD-features.
Surprisedly, we find that the one-way map-
ping can also improve the performances of both
corpora. The results are shown in Table 7. The
</bodyText>
<page confidence="0.995418">
665
</page>
<table confidence="0.989010428571429">
Method Training Dataset Test Dataset P R F1
(Jiang et al., 2009) CTB-5, PDD CTB-5 - - 94.02
(Sun and Wan, 2012) CTB-5, PDD CTB-5 94.42 94.93 94.68
Our Model CTB-5 CTB-5 93.28 93.35 93.31
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-5, PDD CTB-5 94.74 95.11 94.92
Our Model CTB-5, PDD PDD 90.25 89.73 89.99
</table>
<tableCaption confidence="0.990469">
Table 5: Performances of different systems on CTB-5 and PPD.
</tableCaption>
<table confidence="0.999588">
Method Training Dataset Test Dataset P R F1
Our Model CTB-S CTB-S 89.11 89.16 89.13
Our Model PDD PDD 89.41 88.58 88.99
Our Model CTB-S, PDD CTB-S 89.86 90.02 89.94
Our Model CTB-S, PDD PDD 90.5 89.82 90.16
</table>
<tableCaption confidence="0.997623">
Table 6: Performances of different systems on CTB-S and PPD.
</tableCaption>
<bodyText confidence="0.769232666666667">
modelPPD,CTB obtains an error reductions of
14.63% and 6.12% over the baseline on CTB-5
and PDD respectively.
</bodyText>
<table confidence="0.995985333333333">
Method P R F1
ModelS on CTB-5 93.86 94.73 94.29
ModelS on PDD 90.05 89.28 89.66
</table>
<tableCaption confidence="0.9698226">
“ModelS” is the model which is trained on both CTB-
5 and PDD training datasets with just just using the
unidirectional mapping function PPDD→CTB.
Table 7: Performances of unidirectional PPD→CTB
mapping on CTB-5 and PPD.
</tableCaption>
<subsectionHeader confidence="0.883098">
7.4 Evaluation on CTB-S + PPD
</subsectionHeader>
<bodyText confidence="0.9994968">
Table 6 shows the experiment results on the
heterogeneous corpora CTB-S + PPD. Our
method obtains an error reductions of 7.41% and
10.59% over the baseline on CTB-S and PDD re-
spectively.
</bodyText>
<subsectionHeader confidence="0.948703">
7.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999973904761905">
As we can see from the above experiments,
our proposed unified model can improve the
performances of the two heterogeneous corpora
with unidirectional or bidirectional loose map-
ping functions. Different to the pipeline-based
methods, our model can use the shared infor-
mation between two heterogeneous POS tag-
gers. Although the mapping function is loose
and uncertain, it is still can boost the perfor-
mances. The features derived from the wrong
mapping function take relatively less responsi-
bility for prediction after multiple updates of
their weights in training stage. The final model
has good parameter estimates for the shared in-
formation.
Another phenomenon is that the performance
of one corpus can gains when the data size of an-
other corpus increases. In our two experiments,
the training set’s size of CTB-S is larger than
CTB-5, so the performance of PDD is higher in
latter experiment.
</bodyText>
<sectionHeader confidence="0.997851" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999626">
We proposed a method for joint Chinese word
segmentation and POS tagging with heteroge-
neous annotation data. Different to the previous
pipeline-based works, our model is learned on
heterogeneous annotation data simultaneously.
Our method also does not require the exact
corresponding relation between the standards
of heterogeneous annotations. The experimen-
tal results show our method leads to a signif-
icant improvement with heterogeneous annota-
tions over the best performance for this task.
Although our work is for a specific task on joint
Chinese word segmentation and POS, the key
idea to leverage heterogeneous annotations is
very general and applicable to other NLP tasks.
</bodyText>
<page confidence="0.996131">
666
</page>
<bodyText confidence="0.99995425">
In the future, we will continue to refine the
proposed model in two ways: (1) We wish to use
the unsupervised method to extract the loose
mapping relation between the different annota-
tion standards, which is useful to the corpora
without loose mapping guideline. (2) We will
analyze the shared information (weights of the
features derived from the tags which have the
mapping relation) in detail and propose a more
effective model. Besides, we would also like to
investigate for other NLP tasks which have dif-
ferent annotation-style corpora.
</bodyText>
<sectionHeader confidence="0.998592" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998588444444444">
We would like to thank the anonymous re-
viewers for their valuable comments. This
work was funded by NSFC (No.61003091), Key
Projects in the National Science &amp; Technol-
ogy Pillar Program (2012BAH18B01), Shang-
hai Municipal Science and Technology Com-
mission (12511504500), Shanghai Leading Aca-
demic Discipline Project (B114) and 973 Pro-
gram (No.2010CB327900).
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999194423076923">
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from mul-
tiple tasks and unlabeled data. J. Mach. Learn.
Res., 6:1817–1853, December.
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. Learning
Theory and Kernel Machines, pages 567–580.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951–991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
J. Gao, A. Wu, M. Li, C.N. Huang, H. Li, X. Xia,
and H. Qin. 2004. Adaptive chinese word segmen-
tation. In Proceedings of ACL-2004.
W. Jiang, L. Huang, Q. Liu, and Y. Lu. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics. Citeseer.
W. Jiang, L. Huang, and Q. Liu. 2009. Automatic
adaptation of annotation standards: Chinese word
segmentation and POS tagging: a case study. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language
Processing, pages 522–530.
Wenbin Jiang, Fandong Meng, Qun Liu, and Ya-
juan Lü. 2012. Iterative annotation transfor-
mation with predict-self reestimation for Chinese
word segmentation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 412–420, Jeju Is-
land, Korea, July. Association for Computational
Linguistics.
C. Jin and X. Chen. 2008. The fourth interna-
tional Chinese language processing bakeoff: Chi-
nese word segmentation, named entity recognition
and Chinese pos tagging. In Sixth SIGHAN Work-
shop on Chinese Language Processing, page 69.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-Côté, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimi-
native approach to machine translation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 761–768. Association for Computa-
tional Linguistics.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based
or character-based. In Proceedings of EMNLP,
volume 4.
Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing
Huang. 2012. Joint segmentation and tagging
with coupled sequences labeling. In Proceedings
of COLING 2012, pages 951–964, Mumbai, India,
December. The COLING 2012 Organizing Com-
mittee.
Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
FudanNLP: A toolkit for Chinese natural language
processing. In Proceedings of ACL.
Weiwei Sun and Xiaojun Wan. 2012. Reducing
approximation and estimation errors for Chinese
lexical processing with heterogeneous annotations.
In Proceedings of the 50th Annual Meeting of the
</reference>
<page confidence="0.970519">
667
</page>
<reference confidence="0.999532708333333">
Association for Computational Linguistics, pages
232–241.
W. Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
1385–1394.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn Chinese treebank (3.0).
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169–1176.
ACM.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern Chinese corpus. Technical
report, Technical report.
Jiayi Zhao, Xipeng Qiu, and Xuanjing Huang. 2013.
A unified model for joint chinese word segmen-
tation and pos tagging with heterogeneous anno-
tation corpora. In International Conference on
Asian Language Processing, IALP.
</reference>
<page confidence="0.996521">
668
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.427031">
<title confidence="0.86516">Joint Chinese Word Segmentation and POS Tagging Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu, Jiayi Zhao, Xuanjing</title>
<address confidence="0.844587">Fudan University, 825 Zhangheng Road, Shanghai, China</address>
<email confidence="0.982999">xpqiu@fudan.edu.cn,zjy.fudan@gmail.com,xjhuang@fudan.edu.cn</email>
<abstract confidence="0.992034125">Chinese word segmentation and part-ofspeech tagging (S&amp;T) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&amp;T. In this paper, we propose a unified model for Chinese S&amp;T with heterogeneous annotation corpora. We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). Then we regard the Chinese S&amp;T with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled</title>
<date>2005</date>
<pages>6--1817</pages>
<contexts>
<context position="8220" citStr="Ando and Zhang, 2005" startWordPosition="1282" endWordPosition="1285">fine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple task learning (Ben-David and Schuller, 2003), which learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily verified that multiple task learning can improve the performance on this problem in our previous work (Zhao et al., 2013), which is a simplified case of the work in this paper and has a relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817–1853, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ben-David</author>
<author>R Schuller</author>
</authors>
<title>Exploiting task relatedness for multiple task learning. Learning Theory and Kernel Machines,</title>
<date>2003</date>
<pages>567--580</pages>
<contexts>
<context position="7862" citStr="Ben-David and Schuller, 2003" startWordPosition="1221" endWordPosition="1224"> 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger TaggerPPD is trained on a complementary corpus PPD, to assist the target CTB-style TaggerCTB. To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple task learning (Ben-David and Schuller, 2003), which learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily verified that multiple task learning can improve the performance on this problem in our previous work (Zhao et al., 2013), which is a simplified case of the work in this paper and has a relative low complexity. Differe</context>
</contexts>
<marker>Ben-David, Schuller, 2003</marker>
<rawString>S. Ben-David and R. Schuller. 2003. Exploiting task relatedness for multiple task learning. Learning Theory and Kernel Machines, pages 567–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16372" citStr="Collins, 2002" startWordPosition="2713" endWordPosition="2714">i) (s, h(x, zi)) E+ (v1, g1(x, yi−1:i)) yi−1Eφ(zi−1) yiEφ(zi) )+ (v2, g2(x, zi−1:i)) , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.(6) and (7), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. y� = arg max y,yiEY EL i=1 662 Input sequence: x Output: PPD-style Tags Output: CTB-style Tags Shared TaggerPPD TaggerCTB Information 6 Training We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a unified formula. Given a sequence x and the expect type of tags T, the merged model is Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8) y zEψ(y) t(y)=T The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all possible mappings. In this p</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="16292" citStr="Crammer and Singer, 2003" startWordPosition="2698" endWordPosition="2701">i−1:i)) , (6) zi−1Eφ(yi−1) ziEφ(yi) and z� = arg max EL ( E f(x, y))+ z,ziEZ i=1 (u, yEW(zi) (s, h(x, zi)) E+ (v1, g1(x, yi−1:i)) yi−1Eφ(zi−1) yiEφ(zi) )+ (v2, g2(x, zi−1:i)) , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.(6) and (7), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. y� = arg max y,yiEY EL i=1 662 Input sequence: x Output: PPD-style Tags Output: CTB-style Tags Shared TaggerPPD TaggerCTB Information 6 Training We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a unified formula. Given a sequence x and the expect type of tags T, the merged model is Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8) y zEψ(y) t(y)=T The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="16315" citStr="Crammer et al., 2006" startWordPosition="2702" endWordPosition="2705"> ziEφ(yi) and z� = arg max EL ( E f(x, y))+ z,ziEZ i=1 (u, yEW(zi) (s, h(x, zi)) E+ (v1, g1(x, yi−1:i)) yi−1Eφ(zi−1) yiEφ(zi) )+ (v2, g2(x, zi−1:i)) , (7) where f and h represent the state feature vectors on two label sets Y and Z respectively. In Eq.(6) and (7), the score of the label of every character is decided by the weights of the corresponding mapping labels and itself. y� = arg max y,yiEY EL i=1 662 Input sequence: x Output: PPD-style Tags Output: CTB-style Tags Shared TaggerPPD TaggerCTB Information 6 Training We use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a unified formula. Given a sequence x and the expect type of tags T, the merged model is Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8) y zEψ(y) t(y)=T The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent v</context>
<context position="18731" citStr="Crammer et al., 2006" startWordPosition="3172" endWordPosition="3175">th the highest score yˆ = arg max S(w, x, ¯y). (11) 3̸=y t(Y)=t(y) The margin γ(w; (x, y)) is defined as γ(w; (x, y)) = S(w, x, y) − S(w, x, ˆy). (12) Thus, we calculate the hinge loss ℓ(w; (x, y)), (abbreviated as ℓw) by ℓw ={0, γ(w; (x, y)) &gt; 1 1 − γ(w; (x, y)), otherwise (13) In round k, the new weight vector wk+1 is calculated by wk+1 = arg min w 2||w − wk||2 + C · ξ, 1 s.t. ℓ(w; (xk, yk)) &lt;= ξ and ξ &gt;= 0 (14) 663 where ξ is a non-negative slack variable, and C is a positive parameter which controls the influence of the slack term on the objective function. Following the derivation in PA (Crammer et al., 2006), we can get the update rule, wk+1 = wk + Tkek, (15) where ∑ek = ∑�(xk, z) − 4)(xk, z), zEψ(yk� zEψ( Yk� ℓwk Tk = min(C, ∥ek∥2 ). As we can see from the Eq. (15), when we update the weight vector, the update information includes not only the features extracted from current input, but also that extracted from the loose mapping sequence of input. For each feature, the weights of its corresponding related features derived from the loose mapping function will be updated with the same magnitude as well as itself. Our method regards two annotations to be interdependence and peer relationship. Theref</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>A Wu</author>
<author>M Li</author>
<author>C N Huang</author>
<author>H Li</author>
<author>X Xia</author>
<author>H Qin</author>
</authors>
<title>Adaptive chinese word segmentation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004.</booktitle>
<contexts>
<context position="6486" citStr="Gao et al., 2004" startWordPosition="1021" endWordPosition="1024">agsets. The correct mapping relations can be automatically built in training phase. The rest of the paper is organized as follows: We first introduce the related works in section 2 and describe the background of character-based method for joint Chinese S&amp;T in section 3. Section 4 presents an automatic method to build the loose mapping function. Then we propose our method on heterogeneous corpora in 5 and 6. The experimental results are given in section 7. Finally, we conclude our work in section 8. 2 Related Works There are some works to exploit heterogeneous annotation data for Chinese S&amp;T. (Gao et al., 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different 659 Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative </context>
</contexts>
<marker>Gao, Wu, Li, Huang, Li, Xia, Qin, 2004</marker>
<rawString>J. Gao, A. Wu, M. Li, C.N. Huang, H. Li, X. Xia, and H. Qin. 2004. Adaptive chinese word segmentation. In Proceedings of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jiang</author>
<author>L Huang</author>
<author>Q Liu</author>
<author>Y Lu</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Citeseer.</booktitle>
<contexts>
<context position="9065" citStr="Jiang et al., 2008" startWordPosition="1416" endWordPosition="1419">lexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-</context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>W. Jiang, L. Huang, Q. Liu, and Y. Lu. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jiang</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing,</booktitle>
<pages>522--530</pages>
<contexts>
<context position="4057" citStr="Jiang et al., 2009" startWordPosition="621" endWordPosition="624"> Xiang reachs China final CTB 刘翔/NR 进入/VV 中国区/NN 总决赛/NN PDD 刘/nrf 翔/nrg 进入/v 中国/ns 区/n 总/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also significantly different. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The first step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the final taggers by using the outputs of the preliminary taggers as features. We call these methods as</context>
<context position="6746" citStr="Jiang et al., 2009" startWordPosition="1060" endWordPosition="1063">n section 3. Section 4 presents an automatic method to build the loose mapping function. Then we propose our method on heterogeneous corpora in 5 and 6. The experimental results are given in section 7. Finally, we conclude our work in section 8. 2 Related Works There are some works to exploit heterogeneous annotation data for Chinese S&amp;T. (Gao et al., 2004) described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different 659 Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012). (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods reg</context>
<context position="21964" citStr="Jiang et al., 2009" startWordPosition="3721" endWordPosition="3724">s 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. 664 Dataset Partition Sections Words CTB-5 Training 1−270 0.47M 400−931 1001−1151 Develop 301−325 6.66K Test 271−300 7.82K CTB-S Training 0.64M Test - 59.96K PPD Training - 1.11M Test - 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. • Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008). 7.1.2 PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoff 2008. The details of all datasets are shown in Table 3. Our experiment on these datasets may lead to a fair comparison of our system and the related works. 7.2 Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7.2, where C represents a </context>
<context position="23833" citStr="Jiang et al., 2009" startWordPosition="4050" endWordPosition="4053">. The maximum number of iterations K is 50. The F1 score is used for evaluation, which is the harmonic mean of precision P (percentage of predict phrases that exactly match the reference phrases) and recall R (percentage of reference phrases that returned by system). 7.3 Evaluation on CTB-5 + PPD The experiment results on the heterogeneous corpora CTB-5 + PPD are shown in Table 5. Our method obtains an error reductions of 24.08% and 90.8% over the baseline on CTB-5 and PDD respectively. Our method also gives better performance than the pipeline-based methods on heterogeneous corpora, such as (Jiang et al., 2009) and (Sun and Wan, 2012). The reason is that our model can utilize the information of both corpora effectively, which can boost the performance of each other. Although the loose mapping function are bidirectional between two annotation tagsets, we may also use unidirectional mapping. Therefore, we also evaluate the performance when we use unidirectional mapping. We just use the mapping function OPDD-CTB, which means we obtain the PDD-style output without the information from CTB in tagging stage. Thus, in training stage, there are no updates for the weights of CTB-features for the instances fr</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>W. Jiang, L. Huang, and Q. Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing, pages 522–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Fandong Meng</author>
<author>Qun Liu</author>
<author>Yajuan Lü</author>
</authors>
<title>Iterative annotation transformation with predict-self reestimation for Chinese word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>412--420</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="7218" citStr="Jiang et al., 2012" startWordPosition="1122" endWordPosition="1125">her style. However, this converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different 659 Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012). (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger TaggerPPD is trained on a complementary corpus PPD, to assist the target CTB-style TaggerCTB. To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple </context>
</contexts>
<marker>Jiang, Meng, Liu, Lü, 2012</marker>
<rawString>Wenbin Jiang, Fandong Meng, Qun Liu, and Yajuan Lü. 2012. Iterative annotation transformation with predict-self reestimation for Chinese word segmentation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 412–420, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jin</author>
<author>X Chen</author>
</authors>
<title>The fourth international Chinese language processing bakeoff: Chinese word segmentation, named entity recognition and Chinese pos tagging.</title>
<date>2008</date>
<booktitle>In Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>69</pages>
<contexts>
<context position="21944" citStr="Jin and Chen, 2008" startWordPosition="3717" endWordPosition="3720"> 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. 664 Dataset Partition Sections Words CTB-5 Training 1−270 0.47M 400−931 1001−1151 Develop 301−325 6.66K Test 271−300 7.82K CTB-S Training 0.64M Test - 59.96K PPD Training - 1.11M Test - 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. • Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008). 7.1.2 PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoff 2008. The details of all datasets are shown in Table 3. Our experiment on these datasets may lead to a fair comparison of our system and the related works. 7.2 Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7.2, w</context>
</contexts>
<marker>Jin, Chen, 2008</marker>
<rawString>C. Jin and X. Chen. 2008. The fourth international Chinese language processing bakeoff: Chinese word segmentation, named entity recognition and Chinese pos tagging. In Sixth SIGHAN Workshop on Chinese Language Processing, page 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8917" citStr="Lafferty et al., 2001" startWordPosition="1392" endWordPosition="1395">e performance on this problem in our previous work (Zhao et al., 2013), which is a simplified case of the work in this paper and has a relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Côté</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--768</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16839" citStr="Liang et al., 2006" startWordPosition="2795" endWordPosition="2798">e use online Passive-Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a unified formula. Given a sequence x and the expect type of tags T, the merged model is Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8) y zEψ(y) t(y)=T The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the flowchart is shown in Algorithm 1. If given the output type of label T, we only consider the labels in T to initialize the Viterbi matrix, and the score of each node is determined by all the involved heterogeneous labels according to the loose mapping function. input : character sequence x1:L loose mapping function φ output type: T(T ∈ {T</context>
</contexts>
<marker>Liang, Bouchard-Côté, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Côté, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 761–768. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J K Low</author>
</authors>
<title>Chinese part-of-speech tagging: one-at-a-time or all-at-once? word-based or character-based.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<contexts>
<context position="9045" citStr="Ng and Low, 2004" startWordPosition="1411" endWordPosition="1415"> relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters T</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>H.T. Ng and J.K. Low. 2004. Chinese part-of-speech tagging: one-at-a-time or all-at-once? word-based or character-based. In Proceedings of EMNLP, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Feng Ji</author>
<author>Jiayi Zhao</author>
<author>Xuanjing Huang</author>
</authors>
<title>Joint segmentation and tagging with coupled sequences labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>951--964</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="9095" citStr="Qiu et al., 2012" startWordPosition="1422" endWordPosition="1425">iple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relatio</context>
</contexts>
<marker>Qiu, Ji, Zhao, Huang, 2012</marker>
<rawString>Xipeng Qiu, Feng Ji, Jiayi Zhao, and Xuanjing Huang. 2012. Joint segmentation and tagging with coupled sequences labeling. In Proceedings of COLING 2012, pages 951–964, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>FudanNLP: A toolkit for Chinese natural language processing.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21332" citStr="Qiu et al., 2013" startWordPosition="3614" endWordPosition="3618">distinguish the right tag. And if the right tag is one of “n,nt,nz/PDD” but not “n/PDD” (for example, “nt/PDD”), which means it is a “NN/CTB”, the weight of “NN/CTB” will remain unchanged according to the algorithm (updating “n/PDD” changes the “NN/CTB”, but updating “nt/PDD” changes it back). Therefore, after multiple iterations, useful features derived from the mapping function are typically receive more updates, which take relatively more responsibility for correct prediction. The final model has good parameter estimates for the shared information. We implement our method based on FudanNLP(Qiu et al., 2013). 7 Experiments 7.1 Datasets We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. 664 Dataset Partition Sections Words CTB-5 Training 1−270 0.47M 400−931 1001−1151 Develop 301−325 6.66K Test 271−300 7.82K CTB-S Training 0.64M Test - 59.96K PPD Training - 1.11M Test - 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin an</context>
</contexts>
<marker>Qiu, Zhang, Huang, 2013</marker>
<rawString>Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. FudanNLP: A toolkit for Chinese natural language processing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Xiaojun Wan</author>
</authors>
<title>Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="4144" citStr="Sun and Wan, 2012" startWordPosition="635" endWordPosition="638">/b 决赛/vn Table 1: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words. The POS tagsets are also significantly different. For example, PDD gives diverse tags “n” and “vn” for the noun, while CTB just gives “NN”. For proper names, they may be tagged as “nr”, “ns”, etc in PDD, while they are just tagged as “NR” in CTB. Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging. (Jiang et al., 2009) presented a preliminary study for the annotation adaptation topic. (Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heterogeneous word structures. They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain. These methods usually have a two-step process. The first step is to train the preliminary taggers on heterogeneous annotations. The second step is to train the final taggers by using the outputs of the preliminary taggers as features. We call these methods as “pipelinebased” methods. In this paper, we propose a method for joint Chinese word seg</context>
<context position="7239" citStr="Sun and Wan, 2012" startWordPosition="1126" endWordPosition="1129">is converter need human designed transformation templates, and is hard to be generalized to POS tagging. (Jiang et al., 2009) proposed an automatic adaptation method of heterogeneous annotation standards, which depicts a general pipeline to integrate the knowledge of corpora with different 659 Figure 1: Traditional Pipeline-based Strategy for Heterogeneous POS Tagging underling annotation guidelines. They further proposed two optimization strategies, iterative training and predict-self re-estimation, to further improve the accuracy of annotation guideline transformation (Jiang et al., 2012). (Sun and Wan, 2012) proposed a structurebased stacking model to fully utilize heterogeneous word structures. These methods regard one annotation as the main target and another annotation as the complementary/auxiliary purposes. For example, in their solution, an auxiliary tagger TaggerPPD is trained on a complementary corpus PPD, to assist the target CTB-style TaggerCTB. To refine the character-based tagger, PPD-style character labels are directly incorporated as new features. The brief sketch of these methods is shown in Figure 1. The related work in machine learning literature is multiple task learning (Ben-Da</context>
<context position="9948" citStr="Sun and Wan, 2012" startWordPosition="1563" endWordPosition="1566">multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relationship of their guidelines should be found. A mapping function should be established to represent the relationship between two different annotation guidelines. However, the exact mapping relations are hard to establish. As reported in (Sun and Wan, 2012), there is no one-to-one mapping between their heterogeneous word classification, and the mapping between heterogeneous tags is very uncertain. Fortunately, there is a loose mapping can be found in CTB annotation guideline&apos; (Xia, 2000). Table 2 shows some &apos;Available at http://www.cis.upenn.edu/ ˜chiInput: x z=f(x) Output: f(x) y=h(x,f(x)) Output: CTB-style Tags TaggerPPo TaggerCTB 660 CTB’s Tag PDD’ Tagl Total tags 33 26 verbal noun NN v[+nom] proper noun NR n 是 (shi4) VC v 有 (you3) VE, VV v conjunctions CC, CS c other verb VV, VA v, a, z number CD, OD m 1 The tag set of PDD just includes the </context>
<context position="21984" citStr="Sun and Wan, 2012" startWordPosition="3725" endWordPosition="3728">e the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD) in our experiments. 664 Dataset Partition Sections Words CTB-5 Training 1−270 0.47M 400−931 1001−1151 Develop 301−325 6.66K Test 271−300 7.82K CTB-S Training 0.64M Test - 59.96K PPD Training - 1.11M Test - 0.16M Table 3: Data partitioning for CTB and PD 7.1.1 CTB Dataset To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets. • One is the partition criterion used in (Jin and Chen, 2008; Jiang et al., 2009; Sun and Wan, 2012) for CTB 5.0. • Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008). 7.1.2 PPD Dataset For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoff 2008. The details of all datasets are shown in Table 3. Our experiment on these datasets may lead to a fair comparison of our system and the related works. 7.2 Setting We conduct two experiments on CTB-5 + PPD and CTB-S + PPD respectively. The form of feature templates we used is shown in Table 7.2, where C represents a Chinese character, a</context>
<context position="23857" citStr="Sun and Wan, 2012" startWordPosition="4055" endWordPosition="4058">terations K is 50. The F1 score is used for evaluation, which is the harmonic mean of precision P (percentage of predict phrases that exactly match the reference phrases) and recall R (percentage of reference phrases that returned by system). 7.3 Evaluation on CTB-5 + PPD The experiment results on the heterogeneous corpora CTB-5 + PPD are shown in Table 5. Our method obtains an error reductions of 24.08% and 90.8% over the baseline on CTB-5 and PDD respectively. Our method also gives better performance than the pipeline-based methods on heterogeneous corpora, such as (Jiang et al., 2009) and (Sun and Wan, 2012). The reason is that our model can utilize the information of both corpora effectively, which can boost the performance of each other. Although the loose mapping function are bidirectional between two annotation tagsets, we may also use unidirectional mapping. Therefore, we also evaluate the performance when we use unidirectional mapping. We just use the mapping function OPDD-CTB, which means we obtain the PDD-style output without the information from CTB in tagging stage. Thus, in training stage, there are no updates for the weights of CTB-features for the instances from PDD corpus, while ins</context>
</contexts>
<marker>Sun, Wan, 2012</marker>
<rawString>Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sun</author>
</authors>
<title>A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1385--1394</pages>
<contexts>
<context position="9076" citStr="Sun, 2011" startWordPosition="1420" endWordPosition="1421">th the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation error propagation and achieve higher performance on both subtasks(Ng and Low, 2004; Jiang et al., 2008; Sun, 2011; Qiu et al., 2012). The label of each character is the crossproduct of a segmentation label and a tagging label. If we employ the commonly used label set {B, I, E, S} for the segmentation part of crosslabels ({B, I, E} represent Begin, Inside, End of a multi-node segmentation respectively, and S represents a Single node segmentation), the label of character can be in the form of {B-T}(T represents POS tag). For example, B-NN indicates that the character is the begin of a noun. 4 Automatically Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated c</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>W. Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1385–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the penn Chinese treebank (3.0).</title>
<date>2000</date>
<contexts>
<context position="2956" citStr="Xia, 2000" startWordPosition="443" endWordPosition="444">standards is very serious for many tasks in NLP, especially for Chinese word segmentation and part-of-speech (POS) tagging (Chinese S&amp;T). In Chinese S&amp;T, the annotation standards are often incompatible for two main reasons. One is that there is no widely accepted segmentation standard due to the lack of a clear definition of Chinese words. Another is that there are no morphology for Chinese word so that there are many ambiguities to tag the parts-of-speech for Chinese word. For example, the two commonlyused corpora, PKU’s People’s Daily (PPD) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Xia, 2000), use very different segmentation and POS tagging standards. For example, in Table 1, it is very different to annotate the sentence “刘翔进入中国C总 决赛 (Liu Xiang reaches the national final in China)” with guidelines of CTB and PDD. PDD breaks some phrases, which are single words in 658 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658–668, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Liu Xiang reachs China final CTB 刘翔/NR 进入/VV 中国区/NN 总决赛/NN PDD 刘/nrf 翔/nrg 进入/v 中国/ns 区/n 总/b 决赛/vn Table 1: Incompatible</context>
<context position="10183" citStr="Xia, 2000" startWordPosition="1602" endWordPosition="1603">y Establishing the Loose Mapping Function for the Labels of Characters To combine two human-annotated corpora, the relationship of their guidelines should be found. A mapping function should be established to represent the relationship between two different annotation guidelines. However, the exact mapping relations are hard to establish. As reported in (Sun and Wan, 2012), there is no one-to-one mapping between their heterogeneous word classification, and the mapping between heterogeneous tags is very uncertain. Fortunately, there is a loose mapping can be found in CTB annotation guideline&apos; (Xia, 2000). Table 2 shows some &apos;Available at http://www.cis.upenn.edu/ ˜chiInput: x z=f(x) Output: f(x) y=h(x,f(x)) Output: CTB-style Tags TaggerPPo TaggerCTB 660 CTB’s Tag PDD’ Tagl Total tags 33 26 verbal noun NN v[+nom] proper noun NR n 是 (shi4) VC v 有 (you3) VE, VV v conjunctions CC, CS c other verb VV, VA v, a, z number CD, OD m 1 The tag set of PDD just includes the 26 broad categories in the mapping table. The whole tag set of PDD has 103 sub categories. Table 2: Examples of mapping between CTB and PDD’s tagset mapping relations in CTB annotation guideline. These loose mapping relations are many-</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>F. Xia, 2000. The part-of-speech tagging guidelines for the penn Chinese treebank (3.0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural svms with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1169--1176</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16863" citStr="Yu and Joachims, 2009" startWordPosition="2799" endWordPosition="2802">Aggressive (PA) algorithm (Crammer and Singer, 2003; Crammer et al., 2006) to train the model parameters. Following (Collins, 2002), the average strategy is used to avoid the overfitting problem. For the sake of simplicity, we merge the Eq.(6) and (7) into a unified formula. Given a sequence x and the expect type of tags T, the merged model is Figure 3: Our model for Heterogeneous POS Tagging yˆ = arg max ∑⟨w, Φ(x, z)⟩, (8) y zEψ(y) t(y)=T The main challenge of our model is the efficiency of decoding algorithm, which is similar to structured learning with latent variables(Liang et al., 2006) (Yu and Joachims, 2009). Most methods for structured learning with latent variables have not expand all possible mappings. In this paper, we also only expand the mapping that with highest according to the current model. Our model is shown in Figure 3 and the flowchart is shown in Algorithm 1. If given the output type of label T, we only consider the labels in T to initialize the Viterbi matrix, and the score of each node is determined by all the involved heterogeneous labels according to the loose mapping function. input : character sequence x1:L loose mapping function φ output type: T(T ∈ {Ty,Tz}) output: label seq</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural svms with latent variables. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1169–1176. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yu</author>
<author>J Lu</author>
<author>X Zhu</author>
<author>H Duan</author>
<author>S Kang</author>
<author>H Sun</author>
<author>H Wang</author>
<author>Q Zhao</author>
<author>W Zhan</author>
</authors>
<title>Processing norms of modern Chinese corpus.</title>
<date>2001</date>
<tech>Technical report, Technical report.</tech>
<contexts>
<context position="2912" citStr="Yu et al., 2001" startWordPosition="434" endWordPosition="437">resources. The problem of incompatible annotation standards is very serious for many tasks in NLP, especially for Chinese word segmentation and part-of-speech (POS) tagging (Chinese S&amp;T). In Chinese S&amp;T, the annotation standards are often incompatible for two main reasons. One is that there is no widely accepted segmentation standard due to the lack of a clear definition of Chinese words. Another is that there are no morphology for Chinese word so that there are many ambiguities to tag the parts-of-speech for Chinese word. For example, the two commonlyused corpora, PKU’s People’s Daily (PPD) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Xia, 2000), use very different segmentation and POS tagging standards. For example, in Table 1, it is very different to annotate the sentence “刘翔进入中国C总 决赛 (Liu Xiang reaches the national final in China)” with guidelines of CTB and PDD. PDD breaks some phrases, which are single words in 658 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658–668, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Liu Xiang reachs China final CTB 刘翔/NR 进入/VV 中国区/NN 总决赛/NN PDD 刘/nrf 翔/nrg 进入</context>
</contexts>
<marker>Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, Zhan, 2001</marker>
<rawString>S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun, H. Wang, Q. Zhao, and W. Zhan. 2001. Processing norms of modern Chinese corpus. Technical report, Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiayi Zhao</author>
<author>Xipeng Qiu</author>
<author>Xuanjing Huang</author>
</authors>
<title>A unified model for joint chinese word segmentation and pos tagging with heterogeneous annotation corpora.</title>
<date>2013</date>
<booktitle>In International Conference on Asian Language Processing, IALP.</booktitle>
<contexts>
<context position="8365" citStr="Zhao et al., 2013" startWordPosition="1306" endWordPosition="1309"> Figure 1. The related work in machine learning literature is multiple task learning (Ben-David and Schuller, 2003), which learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Multiple task learning has been proven quite successful in practice and has been also applied to NLP (Ando and Zhang, 2005). We also preliminarily verified that multiple task learning can improve the performance on this problem in our previous work (Zhao et al., 2013), which is a simplified case of the work in this paper and has a relative low complexity. Different with the multiple task learning, whose tasks are actually different labels in the same classification task, our model utilizes the shared information between the real different tasks and can produce the corresponding different styles of outputs. 3 Joint Chinese Word Segmentation and POS Tagging Currently, the mainstream method of Chinese POS tagging is joint segmentation &amp; tagging with character-based sequence labeling models(Lafferty et al., 2001), which can avoid the problem of segmentation er</context>
</contexts>
<marker>Zhao, Qiu, Huang, 2013</marker>
<rawString>Jiayi Zhao, Xipeng Qiu, and Xuanjing Huang. 2013. A unified model for joint chinese word segmentation and pos tagging with heterogeneous annotation corpora. In International Conference on Asian Language Processing, IALP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9947">
Structured Penalties for Log-linear Language Models
</title>
<author confidence="0.926456">
Anil Nelakanti,*t C´edric Archambeau,* Julien Mairal,t Francis Bach,t Guillaume Bouchard*
</author>
<affiliation confidence="0.898978">
*Xerox Research Centre Europe, Grenoble, France
</affiliation>
<address confidence="0.565645">
tINRIA-LEAR Project-Team, Grenoble, France
tINRIA-SIERRA Project-Team, Paris, France
</address>
<email confidence="0.996697">
firstname.lastname@xrce.xerox.com firstname.lastname@inria.fr
</email>
<sectionHeader confidence="0.996617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837142857143">
Language models can be formalized as log-
linear regression models where the input fea-
tures represent previously observed contexts
up to a certain length m. The complexity
of existing algorithms to learn the parameters
by maximum likelihood scale linearly in nd,
where n is the length of the training corpus
and d is the number of observed features. We
present a model that grows logarithmically
in d, making it possible to efficiently leverage
longer contexts. We account for the sequen-
tial structure of natural language using tree-
structured penalized objectives to avoid over-
fitting and achieve better generalization.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973980392157">
Language models are crucial parts of advanced nat-
ural language processing pipelines, such as speech
recognition (Burget et al., 2007), machine trans-
lation (Chang and Collins, 2011), or information
retrieval (Vargas et al., 2012). When a sequence
of symbols is observed, a language model pre-
dicts the probability of occurrence of the next sym-
bol in the sequence. Models based on so-called
back-off smoothing have shown good predictive
power (Goodman, 2001). In particular, Kneser-Ney
(KN) and its variants (Kneser and Ney, 1995) are
still achieving state-of-the-art results for more than a
decade after they were originally proposed. Smooth-
ing methods are in fact clever heuristics that require
tuning parameters in an ad-hoc fashion. Hence,
more principled ways of learning language mod-
els have been proposed based on maximum en-
tropy (Chen and Rosenfeld, 2000) or conditional
random fields (Roark et al., 2004), or by adopting
a Bayesian approach (Wood et al., 2009).
In this paper, we focus on penalized maxi-
mum likelihood estimation in log-linear models.
In contrast to language models based on unstruc-
tured norms such as 4 (quadratic penalties) or
, (absolute discounting), we use tree-structured
norms (Zhao et al., 2009; Jenatton et al., 2011).
Structured penalties have been successfully applied
to various NLP tasks, including chunking and named
entity recognition (Martins et al., 2011), but not lan-
guage modelling. Such penalties are particularly
well-suited to this problem as they mimic the nested
nature of word contexts. However, existing optimiz-
ing techniques are not scalable for large contexts m.
In this work, we show that structured tree norms
provide an efficient framework for language mod-
elling. For a special case of these tree norms, we
obtain an memory-efficient learning algorithm for
log-linear language models. Furthermore, we aslo
give the first efficient learning algorithm for struc-
tured t,,, tree norms with a complexity nearly lin-
ear in the number of training samples. This leads to
a memory-efficient and time-efficient learning algo-
rithm for generalized linear language models.
The paper is organized as follows. The model
and other preliminary material is introduced in Sec-
tion 2. In Section 3, we review unstructured penal-
ties that were proposed earlier. Next, we propose
structured penalties and compare their memory and
time requirements. We summarize the characteris-
tics of the proposed algorithms in Section 5 and ex-
perimentally validate our findings in Section 6.
</bodyText>
<page confidence="0.980131">
233
</page>
<note confidence="0.78355">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 233–243,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.997352888888889">
5 7
6
6
4
3
7
7
4 5
6 [2]
4
3
7 [2]
2.3 3
4.8
4.3
2.8
3.5
4.9
5.6
3.2 4.2
5.2 [2]
4
3
5.4 [2]
(a) Trie-structured vector. (b) Tree-structured vector. (c) `T2 -proximal Π`T (w, 0.8) = (d) `T∞-proximal Π`T�(w, 0.8) =
w = [ 3 4 6 6 4 5 7 7 ]&gt;. w = [ 3 4 6 6 4 5 7 7 ]&gt;. 2 [ 3 4 5.2 5.2 3.2 4.2 5.4 5.4 ]&gt;.
[ 2.8 3.5 4.8 4.3 2.3 3 5.6 4.9 ]&gt;.
</figure>
<figureCaption confidence="0.999909">
Figure 1: Example of uncollapsed (trie) and corresponding collapsed (tree) structured vectors and proximal
</figureCaption>
<bodyText confidence="0.469403">
operators applied to them. Weight values are written inside the node. Subfigure (a) shows the complete
trie S and Subfigure (b) shows the corresponding collapsed tree T. The number in the brackets shows the
number of nodes collapsed. Subfigure (c) shows vector after proximal projection for tT2 -norm (which cannot
be collapsed), and Subfigure (d) that of tT -norm proximal projection which can be collapsed.
cc
</bodyText>
<sectionHeader confidence="0.937414" genericHeader="method">
2 Log-linear language models
</sectionHeader>
<bodyText confidence="0.9988513">
Multinomial logistic regression and Poisson regres-
sion are examples of log-linear models (McCullagh
and Nelder, 1989), where the likelihood belongs
to an exponential family and the predictor is lin-
ear. The application of log-linear models to lan-
guage modelling was proposed more than a decade
ago (Della Pietra et al., 1997) and it was shown to
be competitive with state-of-the-art language mod-
elling such as Knesser-Ney smoothing (Chen and
Rosenfeld, 2000).
</bodyText>
<subsectionHeader confidence="0.994219">
2.1 Model definition
</subsectionHeader>
<bodyText confidence="0.9951806">
Let V be a set of words or more generally a set of
symbols, which we call vocabulary. Further, let xy
be a sequence of n+1 symbols of V , where x E Vn
and y E V . We model the probability that symbol y
succeeds x as
</bodyText>
<equation confidence="0.988436333333333">
ew&gt;v φm(x)
P(y = v|x) = m (x) , (1)
EuEV ew&gt;u φ
</equation>
<bodyText confidence="0.999953875">
where W = {wv}vEV is the set of parameters, and
0m(x) is the vector of features extracted from x, the
sequence preceding y. We will describe the features
shortly.
Let x1:i denote the subsequence of x starting at
the first position up to the ith position and yi the next
symbol in the sequence. Parameters are estimated by
minimizing the penalized log-loss:
</bodyText>
<equation confidence="0.96787">
W* E argmin f(W) + AQ(W), (2)
WEJC
</equation>
<bodyText confidence="0.9996582">
where f(W) := −Eni=1 lnp(yi|x1:i;W) and K is
a convex set representing the constraints applied on
the parameters. Overfitting is avoided by adjust-
ing the regularization parameter A, e.g., by cross-
validation.
</bodyText>
<subsectionHeader confidence="0.999786">
2.2 Suffix tree encoding
</subsectionHeader>
<bodyText confidence="0.999923833333333">
Suffix trees provide an efficient way to store and
manipulate discrete sequences and can be con-
structed in linear time when the vocabulary is
fixed (Giegerich and Kurtz, 1997). Recent examples
include language models based on a variable-length
Markovian assumption (Kennington et al., 2012)
and the sequence memoizer (Wood et al., 2011). The
suffix tree data structure encodes all the unique suf-
fixes observed in a sequence up to a maximum given
length. It exploits the fact that the set of observed
contexts is a small subset of all possible contexts.
When a series of suffixes of increasing lengths are
</bodyText>
<page confidence="0.996492">
234
</page>
<table confidence="0.398319555555556">
Algorithm 1 W* argmin If(X, Y ; W)+
AQ(W)} Stochastic optimization algorithm (Hu et
al., 2009)
1 Input: λ regularization parameter , L Lipschitz constant of
Vf , µ coefficient of strong-convexity of f + λQ, X design
matrix, Y label set
2 Initialize: W = Z = 0, τ = δ = 1, ρ = L + µ
3 repeat until maximum iterations
4 #estimate point for gradient update
</table>
<equation confidence="0.988538846153846">
W = (1 − τ)W + τZ
5 #use mini-batch {Xϑ, YϑI for update
W = ParamUpdate(Xϑ, Yϑ, W, λ, ρ)
6 #weighted combination of estimates
Z = 1((1 − µ)Z + (µ − ρ)W + ρW)
ρτ+µ
7 #update constants
ρ = L + µ/δ, τ = V/aδ2δ2−δ, δ = (1 − τ)δ
Procedure: W := ParamUpdate(Xϑ, Yϑ, W, λ, ρ)
1 W0 = W − 1 Vf(Xϑ, Yϑ, W) #gradient step
ρ
2 W = [W]+ #projection to non-negative orthant
3 W = Iln(w, κ) #proximal step
</equation>
<bodyText confidence="0.997354975">
always observed in the same context, the successive
suffixes are collapsed into a single node. The un-
collapsed version of the suffix tree T is called a suf-
fix trie, which we denote S. A suffix trie also has
a tree structure, but it potentially has much larger
number of nodes. An example of a suffix trie S and
the associated suffix tree T are shown in Figures 1(a)
and 1(b) respectively. We use |S |to denote the num-
ber of nodes in the trie S and |T |for the number of
nodes in the tree T.
Suffix tree encoding is particularly helpful in ap-
plications where the resulting hierarchical structures
are thin and tall with numerous non-branching paths.
In the case of text, it has been observed that the num-
ber of nodes in the tree grows slower than that of
the trie with the length of the sequence (Wood et
al., 2011; Kennington et al., 2012). This is a signif-
icant gain in the memory requirements and, as we
will show in Section 4, can also lead to important
computational gains when this structure is exploited.
The feature vector 0,,,(x) encodes suffixes (or
contexts) of increasing length up to a maximum
length m. Hence, the model defined in (1) is simi-
lar to m-gram language models. Naively, the feature
vector 0,,,,(x) corresponds to one path of length m
starting at the root of the suffix trie S. The entries
in W correspond to weights for each suffix. We thus
have a trie structure S on W (see Figure 1(a)) con-
straining the number of free parameters. In other
words, there is one weight parameter per node in the
trie S and the matrix of parameters W is of size |S|.
In this work, however, we consider models where
the number of parameters is equal to the size of the
suffix tree T, which has much fewer nodes than S.
This is achieved by ensuring that all parameters cor-
responding to suffixes at a node share the same pa-
rameter value (see Figure 1(b)). These parameters
correspond to paths in the suffix trie that do not
branch i.e. sequence of words that always appear to-
gether in the same order.
</bodyText>
<subsectionHeader confidence="0.999694">
2.3 Proximal gradient algorithm
</subsectionHeader>
<bodyText confidence="0.999931375">
The objective function (2) involves a smooth convex
loss f and a possibly non-smooth penalty Q. Sub-
gradient descent methods for non-smooth Q could
be used, but they are unfortunately very slow to con-
verge. Instead, we choose proximal methods (Nes-
terov, 2007), which have fast convergence rates
and can deal with a large number of penalties Q,
see (Bach et al., 2012).
Proximal methods iteratively update the current
estimate by making a generalized gradient update at
each iteration. Formally, they are based on a lin-
earization of the smooth function f around a param-
eter estimate W, adding a quadratic penalty term
to keep the updated estimate in the neighborhood
of W. At iteration t, the update of the parameter W
is given by
</bodyText>
<equation confidence="0.9917735">
{
f(W) + (W − W)TVf(W)
�
L
+Q(W) + 2 IIW − WII� , (3)
2
</equation>
<bodyText confidence="0.9995583">
where L &gt; 0 is an upper-bound on the Lipschitz
constant of the gradient Vf. The matrix W could
either be the current estimate Wt or its weighted
combination with the previous estimate for accel-
erated convergence depending on the specific algo-
rithm used (Beck and Teboulle, 2009). Equation (3)
can be rewritten to be solved in two independent
steps: a gradient update from the smooth part fol-
lowed by a projection depending only on the non-
smooth penalty:
</bodyText>
<equation confidence="0.826435333333333">
1
W0 = W − LVf(W), (4)
Wt+1 = argmin
WE1C
235
� W − W&apos;��22 + λQ(W) L .(5)
</equation>
<bodyText confidence="0.990286">
Update (5) is called the proximal operator of W&apos;
with parameter L that we denote lln (W&apos;, λL) . Ef-
ficiently computing the proximal step is crucial to
maintain the fast convergence rate of these methods.
</bodyText>
<subsectionHeader confidence="0.999375">
2.4 Stochastic proximal gradient algorithm
</subsectionHeader>
<bodyText confidence="0.9999979375">
In language modelling applications, the number of
training samples n is typically in the range of 105
or larger. Stochastic version of the proximal meth-
ods (Hu et al., 2009) have been known to be well
adapted when n is large. At every update, the
stochastic algorithm estimates the gradient on a
mini-batch, that is, a subset of the samples. The size
of the mini-batches controls the trade-off between
the variance in the estimate of gradient and the time
required for compute it. In our experiments we use
mini-batches of size 400. The training algorithm is
summarized in Algorithm 1. The acceleration is ob-
tained by making the gradient update at a specific
weighted combination of the current and the previ-
ous estimates of the parameters. The weighting is
shown in step 6 of the Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.998536">
2.5 Positivity constraints
</subsectionHeader>
<bodyText confidence="0.9999861">
Without constraining the parameters, the memory
required by a model scales linearly with the vocabu-
lary size |V |. Any symbol in V observed in a given
context is a positive example, while any symbols
in V that does not appear in this context is a neg-
ative example. When adopting a log-linear language
model, the negative examples are associated with a
small negative gradient step in (4), so that the solu-
tion is not sparse accross multiple categories in gen-
eral. By constraining the parameters to be positive
(i.e., the set of feasible solutions K is the positive
orthant), the projection step 2 in Algorithm 1 can be
done with the same complexity, while maintaining
sparse parameters accross multiple categories. More
precisely, the weights for the category k associated
to a given context x, is always zeros if the category k
never occured after context x. A significant gain in
memory (nearly |V |-fold for large context lengths)
was obtained without loss of accuracy in our exper-
iments.
</bodyText>
<sectionHeader confidence="0.939443" genericHeader="method">
3 Unstructured penalties
</sectionHeader>
<bodyText confidence="0.9987996">
Standard choices for the penalty function Q(W) in-
clude the `1-norm and the squared `2-norm. The
former typically leads to a solution that is sparse
and easily interpretable, while the latter leads to a
non-sparse, generally more stable one. In partic-
ular, the squared `2 and `1 penalties were used in
the context of log-linear language models (Chen and
Rosenfeld, 2000; Goodman, 2004), reporting perfor-
mances competitive with bi-gram and tri-gram inter-
polated Kneser-Ney smoothing.
</bodyText>
<subsectionHeader confidence="0.999736">
3.1 Proximal step on the suffix trie
</subsectionHeader>
<bodyText confidence="0.8229485">
For squared `2 penalties, the proximal step
H`2 2(wt, κ2) is the element-wise rescaling operation:
</bodyText>
<equation confidence="0.9910082">
w(t+1) ← w(t)(1 + κ )−1 (6)
For `1 penalties, the proximal step II`,(wt, κ)] is the
soft-thresholding operator:
wi ← max(0, w(t)
(t+1) i − κ). (7)
</equation>
<bodyText confidence="0.988328">
These projections have linear complexity in the
number of features.
</bodyText>
<subsectionHeader confidence="0.999625">
3.2 Proximal step on the suffix tree
</subsectionHeader>
<bodyText confidence="0.999837416666667">
When feature values are identical, the corresponding
proximal (and gradient) steps are identical. This can
be seen from the proximal steps (7) and (6), which
apply to single weight entries. This property can be
used to group together parameters for which the fea-
ture values are equal. Hence, we can collapse suc-
cessive nodes that always have the same values in a
suffix trie (as in Figure 1(b)), that is to say we can
directly work on the suffix tree. This leads to a prox-
imal step with complexity that scales linearly with
the number of symbols seen in the corpus (Ukkonen,
1995) and logarithmically with context length.
</bodyText>
<sectionHeader confidence="0.972668" genericHeader="method">
4 Structured penalties
</sectionHeader>
<bodyText confidence="0.999983857142857">
The `1 and squared `2 penalties do not account for
the sequential dependencies in the data, treating suf-
fixes of different lengths equally. This is inappro-
priate considering that longer suffixes are typically
observed less frequently than shorter ones. More-
over, the fact that suffixes might be nested is disre-
garded. Hence, we propose to use the tree-structured
</bodyText>
<equation confidence="0.9275705">
Wt+1 = argmin
WEJC
1
2
</equation>
<page confidence="0.990335">
236
</page>
<bodyText confidence="0.983724">
Algorithm 2 w := HfT 2(w, κ) Proximal projection
step for `T2 on grouping G.
</bodyText>
<equation confidence="0.966385733333333">
1 Input: T suffix tree, w trie-structured vector, κ threshold
2 Initialize: {γi} = 0, {ηi} = 1
3 η = UpwardPass(η, γ, κ, w)
4 w = DownwardPass(η, w)
Procedure: η := UpwardPass(η, γ, κ, w)
1 for x ∈ DepthFirstSuffixTraversal(T, PostOrder)
2 γx = w2x
+ Eh∈children(x) γh
3 ηx = [1 − κ/√γx]+
4 γx = η2xγx
Procedure: w := DownwardPass(η, w)
1 for x ∈ DepthFirstSuffixTraversal(T, PreOrder)
2 wx = ηxwx
3 for h ∈ children(x)
4 ηh = ηxηh
</equation>
<tableCaption confidence="0.614492833333333">
a DepthFirstSuffixTraversal(T,Order) returns observed suf-
fixes from the suffix tree T by depth-first traversal in the order
prescribed by Order.
b wx is the weights corresponding to the suffix x from the
weight vector w and children(x) returns all the immediate
children to suffix x in the tree.
</tableCaption>
<bodyText confidence="0.998926333333333">
norms (Zhao et al., 2009; Jenatton et al., 2011),
which are based on the suffix trie or tree, where sub-
trees correspond to contexts of increasing lengths.
As will be shown in the experiments, this prevents
the model to overfit unlike the `1- or squared `2-
norm.
</bodyText>
<subsectionHeader confidence="0.970925">
4.1 Definition of tree-structured `Tp norms
</subsectionHeader>
<bodyText confidence="0.489201">
Definition 1. Let x be a training sequence. Group
g(w, j) is the subvector of w associated with the
subtree rooted at the node j of the suffix trie S(x).
Definition 2. Let G denote the ordered set of nodes
of the tree T(x) such that for r &lt; s, g(w, r) n
g(w, s) = 0 or g(w, r) C g(w, s). The tree-
structured `p-norm is defined as follows:
</bodyText>
<equation confidence="0.990515">
`Tp (w) = � 11g(w,j)11p . (8)
jE9
</equation>
<bodyText confidence="0.9989084">
We specifically consider the cases p = 2, oo for
which efficient optimization algorithms are avail-
able. The `Tp -norms can be viewed as a group
sparsity-inducing norms, where the groups are or-
ganized in a tree. This means that when the weight
associated with a parent in the tree is driven to zero,
the weights associated to all its descendants should
also be driven to zero.
Algorithm 3 w := HfT ∞(w, κ) Proximal projection
step for `T on grouping G.
</bodyText>
<figure confidence="0.925232181818182">
cc
Input: T suffix tree, w=[v c] tree-structured vector v with
corresponding number of suffixes collapsed at each node in
c, κ threshold
1 for x ∈ DepthFirstNodeTraversal(T, PostOrder)
2 g(v, x) := π`g�( g(v, x), cxκ )
Procedure: q := π`_(q, κ)
Input: q = [v c], qi = [vi ci], i = 1, ··· , |q|
Initialize: U = {}, L = {}, I = {1, · · · ,|q|}
1 while I =6 0
2 pick random ρ ∈ I #choose pivot
3 U = {j|vj ≥ vρ} #larger than vρ
4 L = {j  |vj &lt; vρ } #smalller than vρ
L
5 δS = i,U vi · ci, δ L
= i∈U ci
6 if (S + δS) − (C + δC)ρ &lt; κ
7 S := (S + δS), C := (C + δC), I := L
8 else I := U\{ρ}
9 r = S−κ
C , vi := vi − max(0, vi − r) #take residuals
a DepthFirstNodeTraversal(T,Order) returns nodes x from the
</figure>
<figureCaption confidence="0.517912">
suffix tree T by depth-first traversal in the order prescribed
by Order.
</figureCaption>
<bodyText confidence="0.987310230769231">
For structured `Tp -norm, the proximal step
amounts to residuals of recursive projections on the
`q-ball in the order defined by G (Jenatton et al.,
2011), where `q-norm is the dual norm of `p-norm1.
In the case `T2 -norm this comes to a series of pro-
jections on the `2-ball. For `T -norm it is instead
cc
projections on the `1-ball. The order of projections
defined by G is generated by an upward pass of the
suffix trie. At each node through the upward pass,
the subtree below is projected on the dual norm ball
of size κ, the parameter of proximal step. We detail
the projections on the norm ball below.
</bodyText>
<subsectionHeader confidence="0.993007">
4.2 Projections on `q-ball for q = 1, 2
</subsectionHeader>
<bodyText confidence="0.9999906">
Each of the above projections on the dual norm ball
takes one of the following forms depending on the
choice of the norm. Projection of vector w on the
`2-ball is equivalent to thresholding the magnitude
of w by κ units while retaining its direction:
</bodyText>
<equation confidence="0.981399666666667">
w
w &lt;-- [||w||2 − κ]+ . (9)
||w||2
</equation>
<bodyText confidence="0.99568">
This can be performed in time linear in size of w,
O(|w|). Projection of a non-negative vector w on the
`1-ball is more involved and requires thresholding
</bodyText>
<footnote confidence="0.945895">
1`p-norm and `q-norm are dual to each other if 1p + 1q = 1.
`2-norm is self-dual while the dual of `∞-norm is the `1-norm.
</footnote>
<page confidence="0.992189">
237
</page>
<bodyText confidence="0.9559795">
by a value such that the entries in the resulting vector
add up to κ, otherwise w remains the same:
</bodyText>
<equation confidence="0.920848">
w &lt;-- [w − τ]+ s.t. ||w||1 = κ or τ = 0. (10)
</equation>
<bodyText confidence="0.97664475">
τ = 0 is the case where w lies inside the `1-ball
of size κ with ||w||1 &lt; κ, leaving w intact. In the
other case, the threshold τ is to be computed such
that after thresholding, the resulting vector has an
`1-norm of κ. The simplest way to achieve this is
to sort by descending order the entries w = sort(w)
and pick the k largest values such that the (k + 1)th
largest entry is smaller than τ:
</bodyText>
<equation confidence="0.996109">
k
wi − τ = κ and τ &gt; wk+1. (11)
i=1
</equation>
<bodyText confidence="0.99314">
We refer to wk as the pivot and are only interested in
entries larger than the pivot. Given a sorted vector,
it requires looking up to exactly k entries, however,
sorting itself take O(|w |log |w|).
</bodyText>
<subsectionHeader confidence="0.999231">
4.3 Proximal step
</subsectionHeader>
<bodyText confidence="0.999855787878788">
Naively employing the projection on the `2-ball de-
scribed above leads to an O(d2) algorithm for `T2
proximal step. This could be improved to a linear al-
gorithm by aggregating all necessary scaling factors
while making an upward pass of the trie S and ap-
plying them in a single downward pass as described
in (Jenatton et al., 2011). In Algorithm 2, we detail
this procedure for trie-structured vectors.
The complexity of `T -norm proximal step de-
∞
pends directly on that of the pivot finding algorithm
used within its `1-projection method. Naively sort-
ing vectors to find the pivot leads to an O(d2 log d)
algorithm. Pivot finding can be improved by ran-
domly choosing candidates for the pivot and the
best known algorithm due to (Bruckner, 1984) has
amortized linear time complexity in the size of the
vector. This leaves us with O(d2) complexity for
`T∞-norm proximal step. (Duchi et al., 2008) pro-
poses a method that scales linearly with the num-
ber of non-zero entries in the gradient update (s)
but logarithmically in d. But recursive calls to
`1-projection over subtrees will fail the sparsity
assumption (with s � d) making proximal step
quadratic. Procedure for I-IPT � on trie-structured vec-
tors using randomized pivoting method is described
in Algorithm 3.
We next explain how the number of `1-projections
can be reduced by switching to the tree T instead of
trie S which is possible due to the good properties of
`T∞-norm. Then we present a pivot finding method
that is logarithmic in the feature size for our appli-
cation.
</bodyText>
<subsectionHeader confidence="0.783527">
4.4 `T -norm with suffix trees
</subsectionHeader>
<bodyText confidence="0.975398846153846">
∞
We consider the case where all parameters are ini-
tialized with the same value for the optimization pro-
cedure, typically with zeros. The condition that the
parameters at any given node continue to share the
same value requires that both the gradient update (4)
and proximal step (5) have this property. We mod-
ify the tree structure to ensure that after gradient up-
dates parameters at a given node continue to share a
single value. Nodes that do not share a value after
gradient update are split into multiple nodes where
each node has a single value. We formally define
this property as follows:
</bodyText>
<construct confidence="0.5768435">
Definition 3. A constant value non-branching path
is a set of nodes P E P(T, w) of a tree structure T
w.r.t. vector w if P has |P |nodes with |P |− 1 edges
between them and each node has at most one child
and all nodes i, j E P have the same value in vector
waswi=wj.
</construct>
<bodyText confidence="0.861522263157894">
The nodes of Figure 1(b) correspond to constant
value non-branching paths when the values for all
parameters at each of the nodes are the same. Next
we show that this tree structure is retained after
proximal steps of `T -norm.
∞
Proposition 1. Constant value non-branching paths
P(T, w) of T structured vector w are preserved un-
der the proximal projection step I-IPT (w, κ).
Figure 1(d) illustrates this idea showing `T pro-
∞
jection applied on the collapsed tree. This makes it
memory efficient but the time required for the prox-
imal step remains the same since we must project
each subtree of S on the `1-ball. The sequence of
projections at nodes of S in a non-branching path
can be rewritten into a single projection step using
the following technique bringing the number of pro-
jections from |S |to |T|.
</bodyText>
<figureCaption confidence="0.9019425">
Proposition 2. Successive projection steps for sub-
trees with root in a constant value non-branching
</figureCaption>
<bodyText confidence="0.780441">
path P = {g1, , g|P|} E P(T, w) for I-IPT (w, κ)
</bodyText>
<page confidence="0.978429">
238
</page>
<bodyText confidence="0.853342444444445">
is 7rg|P |o· · ·o7rg1(w, K) applied in bottom-up order
defined by G. The composition of projections can be
rewritten into a single projection step with K scaled
by the number ofprojections |P |as,
7rg|P|(w, K|P|) _ 7rg|P |o ··· o 7rg1(w, K)-
The above propositions show that J.-norm can be
used with the suffix tree with fewer projection steps.
We now propose a method to further improve each
of these projection steps.
</bodyText>
<subsectionHeader confidence="0.998028">
4.5 Fast proximal step for tT -norm
</subsectionHeader>
<bodyText confidence="0.992453956521739">
.
Let k be the cardinality of the set of values larger
than the pivot in a vector to compute the thresh-
old for `1-projection as referred in (11). This value
varies from one application to another, but for lan-
guage applications, our experiments on 100K en-
glish words (APNews dataset) showed that k is gen-
erally small: its value is on average 2.5, and its
maximum is around 10 and 20, depending on the
regularization level. We propose using a max-heap
data structure (Cormen et al., 1990) to fetch the k-
largest values necessary to compute the threshold.
Given the heap of the entries the cost of finding the
pivot is O(k log(d)) if the pivot is the kth largest en-
try and there are d features. This operation is per-
formed d times for _ -norm as we traverse the tree
.
bottom-up. The heap itself is built on the fly dur-
ing this upward pass. At each subtree, the heap is
built by merging those of their children in constant
time by using Fibonacci heaps. This leaves us with a
O(dk log(d)) complexity for the proximal step. This
procedure is detailed in Algorithm 4.
</bodyText>
<sectionHeader confidence="0.813291" genericHeader="method">
5 Summary of the algorithms
</sectionHeader>
<bodyText confidence="0.983963933333333">
Table 1 summarizes the characteristics of the algo-
rithms associated to the different penalties:
1. The unstructured norms tp do not take into
account the varying sparsity level with con-
text length. For p=1, this leads to a sparse
solution and for p=2, we obtain the classical
quadratic penalty. The suffix tree representa-
tion leads to an efficient memory usage. Fur-
thermore, to make the training algorithm time
efficient, the parameters corresponding to con-
texts which always occur in the same larger
Algorithm 4 w := H`T ∞(w, K) Proximal projection
step for J. on grouping G using heap data structure.
Input: T suffix tree, w=[v c] tree-structured vector v with
corresponding number of suffixes collapsed at each node in
</bodyText>
<equation confidence="0.982247545454546">
c, κ threshold
Initialize H = {} # empty set of heaps
1 for x ∈ DepthFirstNodeTraversal(T, PostOrder)
g(v, x) := π`g�(w, x, cxκ, H )
Procedure: q := π`_(w, x, κ, H )
1 Hx = NewHeap(vx, cx, vx)
2 for j ∈ children(x) # merge with child heaps
τx = τx + τj # update `1-norm
Hx = Merge(Hx, Hj), H = H\Hj
3 H = H ∪ Hx, S = 0, C = 0, J = {}
4 if Hx(τ) &lt; κ, set Hx = 0 return
5 for j ∈ OrderedIterator(Hx) # get max values
S+(vj·cj)−κ
if vj &gt; C+cj
S = S + (vj·cj), C = C + cj, J = J ∪ {j}
else break
6 r = S−κ
C , δ = 0 # compute threshold
7 for j ∈ J # apply threshold
ν = min(vj, r), δ = δ + (vj − ν)
Hj(v) = ν
8 Hx(τ) = Hj(τ) − δ #update `1-norm
</equation>
<bodyText confidence="0.868094">
a. Heap structure on vector w holds three values (v, c, τ) at
each node. v, c being value and its count, τ is the `1-norm of
the sub-vector below. Tuples are ordered by decreasing value
of v and Hj refers to heap with values in sub-tree rooted at
j. Merge operation merges the heaps passed. OrderedIterator
returns values from the heap in decreasing order of v.
context are grouped. We will illustrate in the
experiments that these penalties do not lead to
good predictive performances.
2. The tT2 -norm nicely groups features by subtrees
which concurs with the sequential structure of
sequences. This leads to a powerful algorithm
in terms of generalization. But it can only be
applied on the uncollapsed tree since there is
no closure property of the constant value non-
branching path for its proximal step making it
less amenable for larger tree depths.
</bodyText>
<listItem confidence="0.9197565">
3. The tT -norm groups features like the J-norm
.
</listItem>
<bodyText confidence="0.999406">
while additionally encouraging numerous fea-
ture groups to share a single value, leading to
a substantial reduction in memory usage. The
generalization properties of this algorithm is as
good as the generalization obtained with the J
penalty, if not better. However, it has the con-
stant value non-branching path property, which
</bodyText>
<page confidence="0.989806">
239
</page>
<table confidence="0.861950375">
Penalty good generalization memory efficient time efficient
unstructured , and t2 no yes O(|T|) yes O(|T|)
2
struct. �� yes no O(|S|) no O(|S|)
2
� rand. pivot yes yes O(|T|) no O(|T|2)
�� cc
heap yes yes O(|T|) yes O(|T |log |T|)
</table>
<tableCaption confidence="0.959859">
Table 1: Properties of the algorithms proposed in this paper. Generalization properties are as compared by
their performance with increasing context length. Memory efficiency is measured by the number of free
parameters of W in the optimization. Note that the suffix tree is much smaller than the trie (uncollapsed
tree): |T |&lt;&lt; |S|. Time complexities reported are that of one proximal projection step.
</tableCaption>
<figure confidence="0.992598">
(a) Unweighted penalties. (b) Weighted penalties. (c) Model complexity for structured
penalties.
</figure>
<figureCaption confidence="0.99734975">
Figure 2: (a) compares average perplexity (lower is better) of different methods from 2-gram through 12-
gram on four different 100K-20K train-test splits. (b) plot compares the same with appropriate feature
weighting. (c) compares model complexity for weighted structured penalties wJ and w���measure by
then number of parameters.
</figureCaption>
<figure confidence="0.987873540540541">
perplexity
260
250
KN
i2
2
it
iT
iT∞
perplexity
260
250
240
230
220
KN
w`22
w`1
w`T2
w`T∞
# of parameters
4
2
8x 105
6
KN
WET2
WET∞
2102 4 6 8 10 12
order of language model
2102 4 6 8 10 12
order of language model
0 24 6 8 10 12
order of language model
240
230
220
</figure>
<bodyText confidence="0.965197">
means that the proximal step can be applied di-
rectly to the suffix tree. There is thus also a
significant gain of performances.
</bodyText>
<sectionHeader confidence="0.998639" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999962047619048">
In this section, we demonstrate empirically the prop-
erties of the algorithms summarized in Table 1. We
consider four distinct subsets of the Associated Press
News (AP-news) text corpus with train-test sizes of
100K-20K for our experiments. The corpus was
preprocessed as described in (Bengio et al., 2003)
by replacing proper nouns, numbers and rare words
with special symbols “(proper noun)”, “#n” and
“(unknown)” respectively. Punctuation marks are
retained which are treated like other normal words.
Vocabulary size for each of the training subsets was
around 8,500 words. The model was reset at the start
of each sentence, meaning that a word in any given
sentence does not depend on any word in the previ-
ous sentence. The regularization parameter A is cho-
sen for each model by cross-validation on a smaller
subset of data. Models are fitted to training sequence
of 30K words for different values of A and validated
against a sequence of 10K words to choose A.
We quantitatively evaluate the proposed model
using perplexity, which is computed as follows:
</bodyText>
<equation confidence="0.9917065">
nv F-z—lII(yiEV)IOgP(�Jilx1:iiW)
P({xi,yi},W) = 10{ } ,
</equation>
<bodyText confidence="0.999928545454545">
where nV = Ei lf(yi E V ). Performance is mea-
sured for varying depth of the suffix trie with dif-
ferent penalties. Interpolated Kneser-Ney results
were computed using the openly available SRILM
toolkit (Stolcke, 2002).
Figure 2(a) shows perplexity values averaged over
four data subsets as a function of the language model
order. It can be observed that performance of un-
structured t, and squared 4 penalties improve until
a relatively low order and then degrade, while �� 2
penalty does not show such degradation, indicating
</bodyText>
<page confidence="0.973301">
240
</page>
<figure confidence="0.9977472">
2 4 6 8 10 12
tree depth
(a) Iteration time of random-pivoting on
the collapsed and uncollapsed trees.
1 2 3 4 5
train size X 106
(b) Iteration time of random-pivoting and
k-best heap on the collapsed tree.
time(sec)
40
60
20
rand-pivot
rand-pivot-col
time(sec)
40
60
20
k-best heap
rand-pivot-col
</figure>
<figureCaption confidence="0.996867">
Figure 3: Comparison of different methods for performing tT∞ proximal projection. The rand-pivot
is the random pivoting method of (Bruckner, 1984) and rand-pivot-col is the same applied with the
nodes collapsed. The k-best heap is the method described in Algorithm 4.
</figureCaption>
<bodyText confidence="0.997083015151515">
that taking the tree-structure into account is benefi-
cial. Moreover, the log-linear language model with
tT2 penalty performs similar to interpolated Kneser-
Ney. The tT∞-norm outperforms all other models
at order 5, but taking the structure into account
does not prevent a degradation of the performance
at higher orders, unlike tT2 . This means that a single
regularization for all model orders is still inappro-
priate.
To investigate this further, we adjust the penal-
ties by choosing an exponential decrease of weights
varying as αm for a feature at depth m in the suffix
tree. Parameter α was tuned on a smaller validation
set. The best performing values for these weighted
models wt22, wt1, wtT2 and wtT∞ are 0.5, 0.7, 1.1
and 0.85 respectively. The weighting scheme fur-
ther appropriates the regularization at various levels
to suit the problem’s structure. Perplexity plots for
weighted models are shown in Figure 2(b). While
wt1 improves at larger depths, it fails to compare
to others showing that the problem does not admit
sparse solutions. Weighted t2 2 improves consider-
ably and performs comparably to the unweighted
tree-structured norms. However, the introduction of
weighted features prevents us from using the suf-
fix tree representation, making these models inef-
ficient in terms of memory. Weighted tT is cor-
∞
rected for overfitting at larger depths and wtT2 gains
more than others. Optimal values for α are frac-
tional for all norms except wtT2 -norm showing that
the unweighted model tT2 -norm was over-penalizing
features at larger depths, while that of others were
under-penalizing them. Interestingly, perplexity im-
proves up to about 9-grams with wtT2 penalty for
the data set we considered, indicating that there is
more to gain from longer dependencies in natural
language sentences than what is currently believed.
Figure 2(c) compares model complexity mea-
sured by the number of parameters for weighted
models using structured penalties. The tT2 penalty
is applied on trie-structured vectors, which grows
roughly at a linear rate with increasing model order.
This is similar to Kneser-Ney. However, the number
of parameters for the wtT∞ penalty grows logarith-
mically with the model order. This is due to the fact
that it operates on the suffix tree-structured vectors
instead of the suffix trie-structured vectors. These
results are valid for, both, weighted and unweighted
penalties.
Next, we compare the average time taken per iter-
ation for different implementations of the tT prox-
∞
imal step. Figure 3(a) shows this time against in-
creasing depth of the language model order for ran-
dom pivoting method with and without the collaps-
ing of parameters at different constant value non-
branching paths. The trend in this plot resembles
that of the number of parameters in Figure 2(c). This
shows that the complexity of the full proximal step
is sublinear when accounting for the suffix tree data
structure. Figure 3(b) plots time per iteration ran-
dom pivoting and k-best heap against the varying
size of training sequence. The two algorithms are
operating directly on the suffix tree. It can be ob-
served that the heap-based method are superior with
</bodyText>
<page confidence="0.992033">
241
</page>
<bodyText confidence="0.900591">
increasing size of training data.
</bodyText>
<sectionHeader confidence="0.990484" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.984415846153846">
In this paper, we proposed several log-linear lan-
guage models. We showed that with an efficient
data structure and structurally appropriate convex
regularization schemes, they were able to outper-
form standard Kneser-Ney smoothing. We also de-
veloped a proximal projection algorithm for the tree-
structured tT -norm suitable for large trees.
cc
Further, we showed that these models can be
trained online, that they accurately learn the m-gram
weights and that they are able to better take advan-
tage of long contexts. The time required to run the
optimization is still a concern. It takes 7583 min-
utes on a standard desktop computer for one pass of
the of the complete AP-news dataset with 13 mil-
lion words which is little more than time reported
for (Mnih and Hinton, 2007). The most time con-
suming part is computing the normalization factor
for the log-loss. A hierarchical model in the flavour
of (Mnih and Hinton, 2008) should lead to signifi-
cant improvements to this end. Currently, the com-
putational bottleneck is due to the normalization fac-
tor in (1) as it appears in every gradient step com-
putation. Significant savings would be obtained by
computing it as described in (Wu and Khundanpur,
2000).
</bodyText>
<sectionHeader confidence="0.996739" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999759">
The authors would like to thank anonymous review-
ers for their comments. This work was partially
supported by the CIFRE grant 1178/2010 from the
French ANRT.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709430769231">
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2012.
Optimization with sparsity-inducing penalties. Foun-
dations and Trends in Machine Learning, pages 1–
106.
A. Beck and M. Teboulle. 2009. A fast itera-
tive shrinkage-thresholding algorithm for linear in-
verse problems. SIAM Journal of Imaging Sciences,
2(1):183–202.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137–1155.
P. Bruckner. 1984. An o(n) algorithm for quadratic
knapsack problems. Operations Research Letters,
3:163–166.
L. Burget, P. Matejka, P. Schwarz, O. Glembek, and J.H.
Cernocky. 2007. Analysis of feature extraction and
channel compensation in a GMM speaker recognition
system. IEEE Transactions on Audio, Speech and
Language Processing, 15(7):1979–1986, September.
Y-W. Chang and M. Collins. 2011. Exact decoding
of phrase-based translation models through lagrangian
relaxation. In Proc. Conf. Empirical Methods for Nat-
ural Language Processing, pages 26–37.
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models.
IEEE Transactions on Speech and Audio Processing,
8(1):37–50.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990.
An Introduction to Algorithms. MIT Press.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380–393.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the `1-ball for learn-
ing in high dimensions. Proc. 25th Int. Conf. Machine
Learning.
R. Giegerich and S. Kurtz. 1997. From ukkonen to Mc-
Creight and weiner: A unifying view of linear-time
suffix tree construction. Algorithmica.
J. Goodman. 2001. A bit of progress in language mod-
elling. Computer Speech and Language, pages 403–
434, October.
J. Goodman. 2004. Exponential priors for maximum en-
tropy models. In Proc. North American Chapter of the
Association of Computational Linguistics.
C. Hu, J.T. Kwok, and W. Pan. 2009. Accelerated gra-
dient methods for stochastic optimization and online
learning. Advances in Neural Information Processing
Systems.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
2011. Proximal methods for hierarchical sparse cod-
ing. Journal of Machine Learning Research, 12:2297–
2334.
C. R. Kennington, M. Kay, and A. Friedrich. 2012. Sufx
trees as language models. Language Resources and
Evaluation Conference.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. IEEE Int. Conf.
Acoustics, Speech and Signal Processing, volume 1.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Structured sparsity in
structured prediction. In Proc. Conf. Empirical Meth-
ods for Natural Language Processing, pages 1500–
1511.
</reference>
<page confidence="0.962906">
242
</page>
<reference confidence="0.996767358974359">
P. McCullagh and J. Nelder. 1989. Generalized linear
models. Chapman and Hall. 2nd edition.
A. Mnih and G. Hinton. 2007. Three new graphical mod-
els for statistical language modelling. Proc. 24th Int.
Conference on Machine Learning.
A. Mnih and G. Hinton. 2008. A scalable hierarchical
distributed language model. Advances in Neural In-
formation Processing Systems.
Y. Nesterov. 2007. Gradient methods for minimizing
composite objective function. CORE Discussion Pa-
per.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with con-
ditional random fields and the perceptron algorithm.
Proc. Association for Computation Linguistics.
A. Stolcke. 2002. Srilm- an extensible language mod-
eling toolkit. Proc. Int. Conf. Spoken Language Pro-
cessing, 2:901–904.
E. Ukkonen. 1995. Online construction of suffix trees.
Algorithmica.
S. Vargas, P. Castells, and D. Vallet. 2012. Explicit rel-
evance models in intent-oriented information retrieval
diversification. In Proc. 35th Int. ACM SIGIR Conf.
Research and development in information retrieval,
SIGIR ’12, pages 75–84. ACM.
F. Wood, C. Archambeau, J. Gasthaus, J. Lancelot, and
Y.-W. Teh. 2009. A stochastic memoizer for sequence
data. In Proc. 26th Intl. Conf. on Machine Learning.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. In Com-
munications of the ACM, volume 54, pages 91–98.
J. Wu and S. Khundanpur. 2000. Efficient training meth-
ods for maximum entropy language modeling. Proc.
6th Inter. Conf. Spoken Language Technologies, pages
114–117.
P. Zhao, G. Rocha, and B. Yu. 2009. The compos-
ite absolute penalties family for grouped and hierar-
chical variable selection. The Annals of Statistics,
37(6A):3468–3497.
</reference>
<page confidence="0.999027">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.371392">
<title confidence="0.957989">Structured Penalties for Log-linear Language Models</title>
<affiliation confidence="0.519849">Research Centre Europe, Grenoble,</affiliation>
<address confidence="0.9146795">Project-Team, Grenoble, Project-Team, Paris, France</address>
<email confidence="0.998379">firstname.lastname@xrce.xerox.comfirstname.lastname@inria.fr</email>
<abstract confidence="0.995498533333333">Language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Bach</author>
<author>R Jenatton</author>
<author>J Mairal</author>
<author>G Obozinski</author>
</authors>
<title>Optimization with sparsity-inducing penalties. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--106</pages>
<contexts>
<context position="9698" citStr="Bach et al., 2012" startWordPosition="1665" endWordPosition="1668">ng to suffixes at a node share the same parameter value (see Figure 1(b)). These parameters correspond to paths in the suffix trie that do not branch i.e. sequence of words that always appear together in the same order. 2.3 Proximal gradient algorithm The objective function (2) involves a smooth convex loss f and a possibly non-smooth penalty Q. Subgradient descent methods for non-smooth Q could be used, but they are unfortunately very slow to converge. Instead, we choose proximal methods (Nesterov, 2007), which have fast convergence rates and can deal with a large number of penalties Q, see (Bach et al., 2012). Proximal methods iteratively update the current estimate by making a generalized gradient update at each iteration. Formally, they are based on a linearization of the smooth function f around a parameter estimate W, adding a quadratic penalty term to keep the updated estimate in the neighborhood of W. At iteration t, the update of the parameter W is given by { f(W) + (W − W)TVf(W) � L +Q(W) + 2 IIW − WII� , (3) 2 where L &gt; 0 is an upper-bound on the Lipschitz constant of the gradient Vf. The matrix W could either be the current estimate Wt or its weighted combination with the previous estima</context>
</contexts>
<marker>Bach, Jenatton, Mairal, Obozinski, 2012</marker>
<rawString>F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2012. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, pages 1– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Beck</author>
<author>M Teboulle</author>
</authors>
<title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems.</title>
<date>2009</date>
<journal>SIAM Journal of Imaging Sciences,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="10395" citStr="Beck and Teboulle, 2009" startWordPosition="1792" endWordPosition="1795">neralized gradient update at each iteration. Formally, they are based on a linearization of the smooth function f around a parameter estimate W, adding a quadratic penalty term to keep the updated estimate in the neighborhood of W. At iteration t, the update of the parameter W is given by { f(W) + (W − W)TVf(W) � L +Q(W) + 2 IIW − WII� , (3) 2 where L &gt; 0 is an upper-bound on the Lipschitz constant of the gradient Vf. The matrix W could either be the current estimate Wt or its weighted combination with the previous estimate for accelerated convergence depending on the specific algorithm used (Beck and Teboulle, 2009). Equation (3) can be rewritten to be solved in two independent steps: a gradient update from the smooth part followed by a projection depending only on the nonsmooth penalty: 1 W0 = W − LVf(W), (4) Wt+1 = argmin WE1C 235 � W − W&apos;��22 + λQ(W) L .(5) Update (5) is called the proximal operator of W&apos; with parameter L that we denote lln (W&apos;, λL) . Efficiently computing the proximal step is crucial to maintain the fast convergence rate of these methods. 2.4 Stochastic proximal gradient algorithm In language modelling applications, the number of training samples n is typically in the range of 105 or</context>
</contexts>
<marker>Beck, Teboulle, 2009</marker>
<rawString>A. Beck and M. Teboulle. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal of Imaging Sciences, 2(1):183–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="28662" citStr="Bengio et al., 2003" startWordPosition="5132" endWordPosition="5135">of parameters 4 2 8x 105 6 KN WET2 WET∞ 2102 4 6 8 10 12 order of language model 2102 4 6 8 10 12 order of language model 0 24 6 8 10 12 order of language model 240 230 220 means that the proximal step can be applied directly to the suffix tree. There is thus also a significant gain of performances. 6 Experiments In this section, we demonstrate empirically the properties of the algorithms summarized in Table 1. We consider four distinct subsets of the Associated Press News (AP-news) text corpus with train-test sizes of 100K-20K for our experiments. The corpus was preprocessed as described in (Bengio et al., 2003) by replacing proper nouns, numbers and rare words with special symbols “(proper noun)”, “#n” and “(unknown)” respectively. Punctuation marks are retained which are treated like other normal words. Vocabulary size for each of the training subsets was around 8,500 words. The model was reset at the start of each sentence, meaning that a word in any given sentence does not depend on any word in the previous sentence. The regularization parameter A is chosen for each model by cross-validation on a smaller subset of data. Models are fitted to training sequence of 30K words for different values of A</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bruckner</author>
</authors>
<title>An o(n) algorithm for quadratic knapsack problems.</title>
<date>1984</date>
<journal>Operations Research Letters,</journal>
<pages>3--163</pages>
<contexts>
<context position="20097" citStr="Bruckner, 1984" startWordPosition="3582" endWordPosition="3583">improved to a linear algorithm by aggregating all necessary scaling factors while making an upward pass of the trie S and applying them in a single downward pass as described in (Jenatton et al., 2011). In Algorithm 2, we detail this procedure for trie-structured vectors. The complexity of `T -norm proximal step de∞ pends directly on that of the pivot finding algorithm used within its `1-projection method. Naively sorting vectors to find the pivot leads to an O(d2 log d) algorithm. Pivot finding can be improved by randomly choosing candidates for the pivot and the best known algorithm due to (Bruckner, 1984) has amortized linear time complexity in the size of the vector. This leaves us with O(d2) complexity for `T∞-norm proximal step. (Duchi et al., 2008) proposes a method that scales linearly with the number of non-zero entries in the gradient update (s) but logarithmically in d. But recursive calls to `1-projection over subtrees will fail the sparsity assumption (with s � d) making proximal step quadratic. Procedure for I-IPT � on trie-structured vectors using randomized pivoting method is described in Algorithm 3. We next explain how the number of `1-projections can be reduced by switching to </context>
<context position="30445" citStr="Bruckner, 1984" startWordPosition="5428" endWordPosition="5429">rmance of unstructured t, and squared 4 penalties improve until a relatively low order and then degrade, while �� 2 penalty does not show such degradation, indicating 240 2 4 6 8 10 12 tree depth (a) Iteration time of random-pivoting on the collapsed and uncollapsed trees. 1 2 3 4 5 train size X 106 (b) Iteration time of random-pivoting and k-best heap on the collapsed tree. time(sec) 40 60 20 rand-pivot rand-pivot-col time(sec) 40 60 20 k-best heap rand-pivot-col Figure 3: Comparison of different methods for performing tT∞ proximal projection. The rand-pivot is the random pivoting method of (Bruckner, 1984) and rand-pivot-col is the same applied with the nodes collapsed. The k-best heap is the method described in Algorithm 4. that taking the tree-structure into account is beneficial. Moreover, the log-linear language model with tT2 penalty performs similar to interpolated KneserNey. The tT∞-norm outperforms all other models at order 5, but taking the structure into account does not prevent a degradation of the performance at higher orders, unlike tT2 . This means that a single regularization for all model orders is still inappropriate. To investigate this further, we adjust the penalties by choo</context>
</contexts>
<marker>Bruckner, 1984</marker>
<rawString>P. Bruckner. 1984. An o(n) algorithm for quadratic knapsack problems. Operations Research Letters, 3:163–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burget</author>
<author>P Matejka</author>
<author>P Schwarz</author>
<author>O Glembek</author>
<author>J H Cernocky</author>
</authors>
<title>Analysis of feature extraction and channel compensation in a GMM speaker recognition system.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="1112" citStr="Burget et al., 2007" startWordPosition="149" endWordPosition="152">ength m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization. 1 Introduction Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition (Burget et al., 2007), machine translation (Chang and Collins, 2011), or information retrieval (Vargas et al., 2012). When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion.</context>
</contexts>
<marker>Burget, Matejka, Schwarz, Glembek, Cernocky, 2007</marker>
<rawString>L. Burget, P. Matejka, P. Schwarz, O. Glembek, and J.H. Cernocky. 2007. Analysis of feature extraction and channel compensation in a GMM speaker recognition system. IEEE Transactions on Audio, Speech and Language Processing, 15(7):1979–1986, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Chang</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proc. Conf. Empirical Methods for Natural Language Processing,</booktitle>
<pages>26--37</pages>
<contexts>
<context position="1159" citStr="Chang and Collins, 2011" startWordPosition="156" endWordPosition="159">hms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization. 1 Introduction Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition (Burget et al., 2007), machine translation (Chang and Collins, 2011), or information retrieval (Vargas et al., 2012). When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning langua</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Y-W. Chang and M. Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proc. Conf. Empirical Methods for Natural Language Processing, pages 26–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for maximum entropy models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1839" citStr="Chen and Rosenfeld, 2000" startWordPosition="263" endWordPosition="266">a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well</context>
<context position="5030" citStr="Chen and Rosenfeld, 2000" startWordPosition="796" endWordPosition="799">al projection for tT2 -norm (which cannot be collapsed), and Subfigure (d) that of tT -norm proximal projection which can be collapsed. cc 2 Log-linear language models Multinomial logistic regression and Poisson regression are examples of log-linear models (McCullagh and Nelder, 1989), where the likelihood belongs to an exponential family and the predictor is linear. The application of log-linear models to language modelling was proposed more than a decade ago (Della Pietra et al., 1997) and it was shown to be competitive with state-of-the-art language modelling such as Knesser-Ney smoothing (Chen and Rosenfeld, 2000). 2.1 Model definition Let V be a set of words or more generally a set of symbols, which we call vocabulary. Further, let xy be a sequence of n+1 symbols of V , where x E Vn and y E V . We model the probability that symbol y succeeds x as ew&gt;v φm(x) P(y = v|x) = m (x) , (1) EuEV ew&gt;u φ where W = {wv}vEV is the set of parameters, and 0m(x) is the vector of features extracted from x, the sequence preceding y. We will describe the features shortly. Let x1:i denote the subsequence of x starting at the first position up to the ith position and yi the next symbol in the sequence. Parameters are esti</context>
<context position="13097" citStr="Chen and Rosenfeld, 2000" startWordPosition="2257" endWordPosition="2260"> associated to a given context x, is always zeros if the category k never occured after context x. A significant gain in memory (nearly |V |-fold for large context lengths) was obtained without loss of accuracy in our experiments. 3 Unstructured penalties Standard choices for the penalty function Q(W) include the `1-norm and the squared `2-norm. The former typically leads to a solution that is sparse and easily interpretable, while the latter leads to a non-sparse, generally more stable one. In particular, the squared `2 and `1 penalties were used in the context of log-linear language models (Chen and Rosenfeld, 2000; Goodman, 2004), reporting performances competitive with bi-gram and tri-gram interpolated Kneser-Ney smoothing. 3.1 Proximal step on the suffix trie For squared `2 penalties, the proximal step H`2 2(wt, κ2) is the element-wise rescaling operation: w(t+1) ← w(t)(1 + κ )−1 (6) For `1 penalties, the proximal step II`,(wt, κ)] is the soft-thresholding operator: wi ← max(0, w(t) (t+1) i − κ). (7) These projections have linear complexity in the number of features. 3.2 Proximal step on the suffix tree When feature values are identical, the corresponding proximal (and gradient) steps are identical. </context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing techniques for maximum entropy models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>An Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23683" citStr="Cormen et al., 1990" startWordPosition="4222" endWordPosition="4225">ewer projection steps. We now propose a method to further improve each of these projection steps. 4.5 Fast proximal step for tT -norm . Let k be the cardinality of the set of values larger than the pivot in a vector to compute the threshold for `1-projection as referred in (11). This value varies from one application to another, but for language applications, our experiments on 100K english words (APNews dataset) showed that k is generally small: its value is on average 2.5, and its maximum is around 10 and 20, depending on the regularization level. We propose using a max-heap data structure (Cormen et al., 1990) to fetch the klargest values necessary to compute the threshold. Given the heap of the entries the cost of finding the pivot is O(k log(d)) if the pivot is the kth largest entry and there are d features. This operation is performed d times for _ -norm as we traverse the tree . bottom-up. The heap itself is built on the fly during this upward pass. At each subtree, the heap is built by merging those of their children in constant time by using Fibonacci heaps. This leaves us with a O(dk log(d)) complexity for the proximal step. This procedure is detailed in Algorithm 4. 5 Summary of the algorit</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990. An Introduction to Algorithms. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="4897" citStr="Pietra et al., 1997" startWordPosition="776" endWordPosition="779">onding collapsed tree T. The number in the brackets shows the number of nodes collapsed. Subfigure (c) shows vector after proximal projection for tT2 -norm (which cannot be collapsed), and Subfigure (d) that of tT -norm proximal projection which can be collapsed. cc 2 Log-linear language models Multinomial logistic regression and Poisson regression are examples of log-linear models (McCullagh and Nelder, 1989), where the likelihood belongs to an exponential family and the predictor is linear. The application of log-linear models to language modelling was proposed more than a decade ago (Della Pietra et al., 1997) and it was shown to be competitive with state-of-the-art language modelling such as Knesser-Ney smoothing (Chen and Rosenfeld, 2000). 2.1 Model definition Let V be a set of words or more generally a set of symbols, which we call vocabulary. Further, let xy be a sequence of n+1 symbols of V , where x E Vn and y E V . We model the probability that symbol y succeeds x as ew&gt;v φm(x) P(y = v|x) = m (x) , (1) EuEV ew&gt;u φ where W = {wv}vEV is the set of parameters, and 0m(x) is the vector of features extracted from x, the sequence preceding y. We will describe the features shortly. Let x1:i denote t</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>T Chandra</author>
</authors>
<title>Efficient projections onto the `1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>Proc. 25th Int. Conf. Machine Learning.</booktitle>
<contexts>
<context position="20247" citStr="Duchi et al., 2008" startWordPosition="3605" endWordPosition="3608">e downward pass as described in (Jenatton et al., 2011). In Algorithm 2, we detail this procedure for trie-structured vectors. The complexity of `T -norm proximal step de∞ pends directly on that of the pivot finding algorithm used within its `1-projection method. Naively sorting vectors to find the pivot leads to an O(d2 log d) algorithm. Pivot finding can be improved by randomly choosing candidates for the pivot and the best known algorithm due to (Bruckner, 1984) has amortized linear time complexity in the size of the vector. This leaves us with O(d2) complexity for `T∞-norm proximal step. (Duchi et al., 2008) proposes a method that scales linearly with the number of non-zero entries in the gradient update (s) but logarithmically in d. But recursive calls to `1-projection over subtrees will fail the sparsity assumption (with s � d) making proximal step quadratic. Procedure for I-IPT � on trie-structured vectors using randomized pivoting method is described in Algorithm 3. We next explain how the number of `1-projections can be reduced by switching to the tree T instead of trie S which is possible due to the good properties of `T∞-norm. Then we present a pivot finding method that is logarithmic in t</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. 2008. Efficient projections onto the `1-ball for learning in high dimensions. Proc. 25th Int. Conf. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Giegerich</author>
<author>S Kurtz</author>
</authors>
<title>From ukkonen to McCreight and weiner: A unifying view of linear-time suffix tree construction.</title>
<date>1997</date>
<publisher>Algorithmica.</publisher>
<contexts>
<context position="6117" citStr="Giegerich and Kurtz, 1997" startWordPosition="996" endWordPosition="999">note the subsequence of x starting at the first position up to the ith position and yi the next symbol in the sequence. Parameters are estimated by minimizing the penalized log-loss: W* E argmin f(W) + AQ(W), (2) WEJC where f(W) := −Eni=1 lnp(yi|x1:i;W) and K is a convex set representing the constraints applied on the parameters. Overfitting is avoided by adjusting the regularization parameter A, e.g., by crossvalidation. 2.2 Suffix tree encoding Suffix trees provide an efficient way to store and manipulate discrete sequences and can be constructed in linear time when the vocabulary is fixed (Giegerich and Kurtz, 1997). Recent examples include language models based on a variable-length Markovian assumption (Kennington et al., 2012) and the sequence memoizer (Wood et al., 2011). The suffix tree data structure encodes all the unique suffixes observed in a sequence up to a maximum given length. It exploits the fact that the set of observed contexts is a small subset of all possible contexts. When a series of suffixes of increasing lengths are 234 Algorithm 1 W* argmin If(X, Y ; W)+ AQ(W)} Stochastic optimization algorithm (Hu et al., 2009) 1 Input: λ regularization parameter , L Lipschitz constant of Vf , µ co</context>
</contexts>
<marker>Giegerich, Kurtz, 1997</marker>
<rawString>R. Giegerich and S. Kurtz. 1997. From ukkonen to McCreight and weiner: A unifying view of linear-time suffix tree construction. Algorithmica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A bit of progress in language modelling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>403--434</pages>
<contexts>
<context position="1434" citStr="Goodman, 2001" startWordPosition="202" endWordPosition="203"> the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization. 1 Introduction Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition (Burget et al., 2007), machine translation (Chang and Collins, 2011), or information retrieval (Vargas et al., 2012). When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. A bit of progress in language modelling. Computer Speech and Language, pages 403– 434, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. North American Chapter of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="13113" citStr="Goodman, 2004" startWordPosition="2261" endWordPosition="2262">text x, is always zeros if the category k never occured after context x. A significant gain in memory (nearly |V |-fold for large context lengths) was obtained without loss of accuracy in our experiments. 3 Unstructured penalties Standard choices for the penalty function Q(W) include the `1-norm and the squared `2-norm. The former typically leads to a solution that is sparse and easily interpretable, while the latter leads to a non-sparse, generally more stable one. In particular, the squared `2 and `1 penalties were used in the context of log-linear language models (Chen and Rosenfeld, 2000; Goodman, 2004), reporting performances competitive with bi-gram and tri-gram interpolated Kneser-Ney smoothing. 3.1 Proximal step on the suffix trie For squared `2 penalties, the proximal step H`2 2(wt, κ2) is the element-wise rescaling operation: w(t+1) ← w(t)(1 + κ )−1 (6) For `1 penalties, the proximal step II`,(wt, κ)] is the soft-thresholding operator: wi ← max(0, w(t) (t+1) i − κ). (7) These projections have linear complexity in the number of features. 3.2 Proximal step on the suffix tree When feature values are identical, the corresponding proximal (and gradient) steps are identical. This can be seen</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>J. Goodman. 2004. Exponential priors for maximum entropy models. In Proc. North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hu</author>
<author>J T Kwok</author>
<author>W Pan</author>
</authors>
<title>Accelerated gradient methods for stochastic optimization and online learning.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6645" citStr="Hu et al., 2009" startWordPosition="1084" endWordPosition="1087">n be constructed in linear time when the vocabulary is fixed (Giegerich and Kurtz, 1997). Recent examples include language models based on a variable-length Markovian assumption (Kennington et al., 2012) and the sequence memoizer (Wood et al., 2011). The suffix tree data structure encodes all the unique suffixes observed in a sequence up to a maximum given length. It exploits the fact that the set of observed contexts is a small subset of all possible contexts. When a series of suffixes of increasing lengths are 234 Algorithm 1 W* argmin If(X, Y ; W)+ AQ(W)} Stochastic optimization algorithm (Hu et al., 2009) 1 Input: λ regularization parameter , L Lipschitz constant of Vf , µ coefficient of strong-convexity of f + λQ, X design matrix, Y label set 2 Initialize: W = Z = 0, τ = δ = 1, ρ = L + µ 3 repeat until maximum iterations 4 #estimate point for gradient update W = (1 − τ)W + τZ 5 #use mini-batch {Xϑ, YϑI for update W = ParamUpdate(Xϑ, Yϑ, W, λ, ρ) 6 #weighted combination of estimates Z = 1((1 − µ)Z + (µ − ρ)W + ρW) ρτ+µ 7 #update constants ρ = L + µ/δ, τ = V/aδ2δ2−δ, δ = (1 − τ)δ Procedure: W := ParamUpdate(Xϑ, Yϑ, W, λ, ρ) 1 W0 = W − 1 Vf(Xϑ, Yϑ, W) #gradient step ρ 2 W = [W]+ #projection to n</context>
<context position="11064" citStr="Hu et al., 2009" startWordPosition="1914" endWordPosition="1917">ndependent steps: a gradient update from the smooth part followed by a projection depending only on the nonsmooth penalty: 1 W0 = W − LVf(W), (4) Wt+1 = argmin WE1C 235 � W − W&apos;��22 + λQ(W) L .(5) Update (5) is called the proximal operator of W&apos; with parameter L that we denote lln (W&apos;, λL) . Efficiently computing the proximal step is crucial to maintain the fast convergence rate of these methods. 2.4 Stochastic proximal gradient algorithm In language modelling applications, the number of training samples n is typically in the range of 105 or larger. Stochastic version of the proximal methods (Hu et al., 2009) have been known to be well adapted when n is large. At every update, the stochastic algorithm estimates the gradient on a mini-batch, that is, a subset of the samples. The size of the mini-batches controls the trade-off between the variance in the estimate of gradient and the time required for compute it. In our experiments we use mini-batches of size 400. The training algorithm is summarized in Algorithm 1. The acceleration is obtained by making the gradient update at a specific weighted combination of the current and the previous estimates of the parameters. The weighting is shown in step 6</context>
</contexts>
<marker>Hu, Kwok, Pan, 2009</marker>
<rawString>C. Hu, J.T. Kwok, and W. Pan. 2009. Accelerated gradient methods for stochastic optimization and online learning. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jenatton</author>
<author>J Mairal</author>
<author>G Obozinski</author>
<author>F Bach</author>
</authors>
<title>Proximal methods for hierarchical sparse coding.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2334</pages>
<contexts>
<context position="2227" citStr="Jenatton et al., 2011" startWordPosition="326" endWordPosition="329">posed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well-suited to this problem as they mimic the nested nature of word contexts. However, existing optimizing techniques are not scalable for large contexts m. In this work, we show that structured tree norms provide an efficient framework for language modelling. For a special case of these tree norms, we obtain an memory-efficient learning algorithm for log-linear language models. Furthermor</context>
<context position="15487" citStr="Jenatton et al., 2011" startWordPosition="2670" endWordPosition="2673">dure: η := UpwardPass(η, γ, κ, w) 1 for x ∈ DepthFirstSuffixTraversal(T, PostOrder) 2 γx = w2x + Eh∈children(x) γh 3 ηx = [1 − κ/√γx]+ 4 γx = η2xγx Procedure: w := DownwardPass(η, w) 1 for x ∈ DepthFirstSuffixTraversal(T, PreOrder) 2 wx = ηxwx 3 for h ∈ children(x) 4 ηh = ηxηh a DepthFirstSuffixTraversal(T,Order) returns observed suffixes from the suffix tree T by depth-first traversal in the order prescribed by Order. b wx is the weights corresponding to the suffix x from the weight vector w and children(x) returns all the immediate children to suffix x in the tree. norms (Zhao et al., 2009; Jenatton et al., 2011), which are based on the suffix trie or tree, where subtrees correspond to contexts of increasing lengths. As will be shown in the experiments, this prevents the model to overfit unlike the `1- or squared `2- norm. 4.1 Definition of tree-structured `Tp norms Definition 1. Let x be a training sequence. Group g(w, j) is the subvector of w associated with the subtree rooted at the node j of the suffix trie S(x). Definition 2. Let G denote the ordered set of nodes of the tree T(x) such that for r &lt; s, g(w, r) n g(w, s) = 0 or g(w, r) C g(w, s). The treestructured `p-norm is defined as follows: `Tp</context>
<context position="17498" citStr="Jenatton et al., 2011" startWordPosition="3084" endWordPosition="3087">U = {}, L = {}, I = {1, · · · ,|q|} 1 while I =6 0 2 pick random ρ ∈ I #choose pivot 3 U = {j|vj ≥ vρ} #larger than vρ 4 L = {j |vj &lt; vρ } #smalller than vρ L 5 δS = i,U vi · ci, δ L = i∈U ci 6 if (S + δS) − (C + δC)ρ &lt; κ 7 S := (S + δS), C := (C + δC), I := L 8 else I := U\{ρ} 9 r = S−κ C , vi := vi − max(0, vi − r) #take residuals a DepthFirstNodeTraversal(T,Order) returns nodes x from the suffix tree T by depth-first traversal in the order prescribed by Order. For structured `Tp -norm, the proximal step amounts to residuals of recursive projections on the `q-ball in the order defined by G (Jenatton et al., 2011), where `q-norm is the dual norm of `p-norm1. In the case `T2 -norm this comes to a series of projections on the `2-ball. For `T -norm it is instead cc projections on the `1-ball. The order of projections defined by G is generated by an upward pass of the suffix trie. At each node through the upward pass, the subtree below is projected on the dual norm ball of size κ, the parameter of proximal step. We detail the projections on the norm ball below. 4.2 Projections on `q-ball for q = 1, 2 Each of the above projections on the dual norm ball takes one of the following forms depending on the choic</context>
<context position="19683" citStr="Jenatton et al., 2011" startWordPosition="3510" endWordPosition="3513"> 1)th largest entry is smaller than τ: k wi − τ = κ and τ &gt; wk+1. (11) i=1 We refer to wk as the pivot and are only interested in entries larger than the pivot. Given a sorted vector, it requires looking up to exactly k entries, however, sorting itself take O(|w |log |w|). 4.3 Proximal step Naively employing the projection on the `2-ball described above leads to an O(d2) algorithm for `T2 proximal step. This could be improved to a linear algorithm by aggregating all necessary scaling factors while making an upward pass of the trie S and applying them in a single downward pass as described in (Jenatton et al., 2011). In Algorithm 2, we detail this procedure for trie-structured vectors. The complexity of `T -norm proximal step de∞ pends directly on that of the pivot finding algorithm used within its `1-projection method. Naively sorting vectors to find the pivot leads to an O(d2 log d) algorithm. Pivot finding can be improved by randomly choosing candidates for the pivot and the best known algorithm due to (Bruckner, 1984) has amortized linear time complexity in the size of the vector. This leaves us with O(d2) complexity for `T∞-norm proximal step. (Duchi et al., 2008) proposes a method that scales linea</context>
</contexts>
<marker>Jenatton, Mairal, Obozinski, Bach, 2011</marker>
<rawString>R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. 2011. Proximal methods for hierarchical sparse coding. Journal of Machine Learning Research, 12:2297– 2334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Kennington</author>
<author>M Kay</author>
<author>A Friedrich</author>
</authors>
<title>Sufx trees as language models. Language Resources and Evaluation Conference.</title>
<date>2012</date>
<contexts>
<context position="6232" citStr="Kennington et al., 2012" startWordPosition="1011" endWordPosition="1014">e. Parameters are estimated by minimizing the penalized log-loss: W* E argmin f(W) + AQ(W), (2) WEJC where f(W) := −Eni=1 lnp(yi|x1:i;W) and K is a convex set representing the constraints applied on the parameters. Overfitting is avoided by adjusting the regularization parameter A, e.g., by crossvalidation. 2.2 Suffix tree encoding Suffix trees provide an efficient way to store and manipulate discrete sequences and can be constructed in linear time when the vocabulary is fixed (Giegerich and Kurtz, 1997). Recent examples include language models based on a variable-length Markovian assumption (Kennington et al., 2012) and the sequence memoizer (Wood et al., 2011). The suffix tree data structure encodes all the unique suffixes observed in a sequence up to a maximum given length. It exploits the fact that the set of observed contexts is a small subset of all possible contexts. When a series of suffixes of increasing lengths are 234 Algorithm 1 W* argmin If(X, Y ; W)+ AQ(W)} Stochastic optimization algorithm (Hu et al., 2009) 1 Input: λ regularization parameter , L Lipschitz constant of Vf , µ coefficient of strong-convexity of f + λQ, X design matrix, Y label set 2 Initialize: W = Z = 0, τ = δ = 1, ρ = L + µ</context>
<context position="8135" citStr="Kennington et al., 2012" startWordPosition="1385" endWordPosition="1388">tructure, but it potentially has much larger number of nodes. An example of a suffix trie S and the associated suffix tree T are shown in Figures 1(a) and 1(b) respectively. We use |S |to denote the number of nodes in the trie S and |T |for the number of nodes in the tree T. Suffix tree encoding is particularly helpful in applications where the resulting hierarchical structures are thin and tall with numerous non-branching paths. In the case of text, it has been observed that the number of nodes in the tree grows slower than that of the trie with the length of the sequence (Wood et al., 2011; Kennington et al., 2012). This is a significant gain in the memory requirements and, as we will show in Section 4, can also lead to important computational gains when this structure is exploited. The feature vector 0,,,(x) encodes suffixes (or contexts) of increasing length up to a maximum length m. Hence, the model defined in (1) is similar to m-gram language models. Naively, the feature vector 0,,,,(x) corresponds to one path of length m starting at the root of the suffix trie S. The entries in W correspond to weights for each suffix. We thus have a trie structure S on W (see Figure 1(a)) constraining the number of</context>
</contexts>
<marker>Kennington, Kay, Friedrich, 2012</marker>
<rawString>C. R. Kennington, M. Kay, and A. Friedrich. 2012. Sufx trees as language models. Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<contexts>
<context position="1506" citStr="Kneser and Ney, 1995" startWordPosition="211" endWordPosition="214">d penalized objectives to avoid overfitting and achieve better generalization. 1 Introduction Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition (Burget et al., 2007), machine translation (Chang and Collins, 2011), or information retrieval (Vargas et al., 2012). When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proc. Conf. Empirical Methods for Natural Language Processing,</booktitle>
<pages>1500--1511</pages>
<contexts>
<context position="2373" citStr="Martins et al., 2011" startWordPosition="346" endWordPosition="349"> language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well-suited to this problem as they mimic the nested nature of word contexts. However, existing optimizing techniques are not scalable for large contexts m. In this work, we show that structured tree norms provide an efficient framework for language modelling. For a special case of these tree norms, we obtain an memory-efficient learning algorithm for log-linear language models. Furthermore, we aslo give the first efficient learning algorithm for structured t,,, tree norms with a complexity nearly linear in the number of training sa</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proc. Conf. Empirical Methods for Natural Language Processing, pages 1500– 1511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McCullagh</author>
<author>J Nelder</author>
</authors>
<title>Generalized linear models. Chapman and Hall. 2nd edition.</title>
<date>1989</date>
<contexts>
<context position="4690" citStr="McCullagh and Nelder, 1989" startWordPosition="741" endWordPosition="744">) and corresponding collapsed (tree) structured vectors and proximal operators applied to them. Weight values are written inside the node. Subfigure (a) shows the complete trie S and Subfigure (b) shows the corresponding collapsed tree T. The number in the brackets shows the number of nodes collapsed. Subfigure (c) shows vector after proximal projection for tT2 -norm (which cannot be collapsed), and Subfigure (d) that of tT -norm proximal projection which can be collapsed. cc 2 Log-linear language models Multinomial logistic regression and Poisson regression are examples of log-linear models (McCullagh and Nelder, 1989), where the likelihood belongs to an exponential family and the predictor is linear. The application of log-linear models to language modelling was proposed more than a decade ago (Della Pietra et al., 1997) and it was shown to be competitive with state-of-the-art language modelling such as Knesser-Ney smoothing (Chen and Rosenfeld, 2000). 2.1 Model definition Let V be a set of words or more generally a set of symbols, which we call vocabulary. Further, let xy be a sequence of n+1 symbols of V , where x E Vn and y E V . We model the probability that symbol y succeeds x as ew&gt;v φm(x) P(y = v|x)</context>
</contexts>
<marker>McCullagh, Nelder, 1989</marker>
<rawString>P. McCullagh and J. Nelder. 1989. Generalized linear models. Chapman and Hall. 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>Proc. 24th Int. Conference on Machine Learning.</booktitle>
<contexts>
<context position="34551" citStr="Mnih and Hinton, 2007" startWordPosition="6097" endWordPosition="6100">egularization schemes, they were able to outperform standard Kneser-Ney smoothing. We also developed a proximal projection algorithm for the treestructured tT -norm suitable for large trees. cc Further, we showed that these models can be trained online, that they accurately learn the m-gram weights and that they are able to better take advantage of long contexts. The time required to run the optimization is still a concern. It takes 7583 minutes on a standard desktop computer for one pass of the of the complete AP-news dataset with 13 million words which is little more than time reported for (Mnih and Hinton, 2007). The most time consuming part is computing the normalization factor for the log-loss. A hierarchical model in the flavour of (Mnih and Hinton, 2008) should lead to significant improvements to this end. Currently, the computational bottleneck is due to the normalization factor in (1) as it appears in every gradient step computation. Significant savings would be obtained by computing it as described in (Wu and Khundanpur, 2000). Acknowledgements The authors would like to thank anonymous reviewers for their comments. This work was partially supported by the CIFRE grant 1178/2010 from the French </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. 2007. Three new graphical models for statistical language modelling. Proc. 24th Int. Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<marker>Mnih, Hinton, 2008</marker>
<rawString>A. Mnih and G. Hinton. 2008. A scalable hierarchical distributed language model. Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nesterov</author>
</authors>
<title>Gradient methods for minimizing composite objective function.</title>
<date>2007</date>
<journal>CORE Discussion Paper.</journal>
<contexts>
<context position="9590" citStr="Nesterov, 2007" startWordPosition="1646" endWordPosition="1648">x tree T, which has much fewer nodes than S. This is achieved by ensuring that all parameters corresponding to suffixes at a node share the same parameter value (see Figure 1(b)). These parameters correspond to paths in the suffix trie that do not branch i.e. sequence of words that always appear together in the same order. 2.3 Proximal gradient algorithm The objective function (2) involves a smooth convex loss f and a possibly non-smooth penalty Q. Subgradient descent methods for non-smooth Q could be used, but they are unfortunately very slow to converge. Instead, we choose proximal methods (Nesterov, 2007), which have fast convergence rates and can deal with a large number of penalties Q, see (Bach et al., 2012). Proximal methods iteratively update the current estimate by making a generalized gradient update at each iteration. Formally, they are based on a linearization of the smooth function f around a parameter estimate W, adding a quadratic penalty term to keep the updated estimate in the neighborhood of W. At iteration t, the update of the parameter W is given by { f(W) + (W − W)TVf(W) � L +Q(W) + 2 IIW − WII� , (3) 2 where L &gt; 0 is an upper-bound on the Lipschitz constant of the gradient V</context>
</contexts>
<marker>Nesterov, 2007</marker>
<rawString>Y. Nesterov. 2007. Gradient methods for minimizing composite objective function. CORE Discussion Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Saraclar</author>
<author>M Collins</author>
<author>M Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>Proc. Association for Computation Linguistics.</booktitle>
<contexts>
<context position="1889" citStr="Roark et al., 2004" startWordPosition="271" endWordPosition="274">icts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well-suited to this problem as they mimic the nested n</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>B. Roark, M. Saraclar, M. Collins, and M. Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. Proc. Association for Computation Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm- an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>2--901</pages>
<contexts>
<context position="29688" citStr="Stolcke, 2002" startWordPosition="5302" endWordPosition="5303">ence. The regularization parameter A is chosen for each model by cross-validation on a smaller subset of data. Models are fitted to training sequence of 30K words for different values of A and validated against a sequence of 10K words to choose A. We quantitatively evaluate the proposed model using perplexity, which is computed as follows: nv F-z—lII(yiEV)IOgP(�Jilx1:iiW) P({xi,yi},W) = 10{ } , where nV = Ei lf(yi E V ). Performance is measured for varying depth of the suffix trie with different penalties. Interpolated Kneser-Ney results were computed using the openly available SRILM toolkit (Stolcke, 2002). Figure 2(a) shows perplexity values averaged over four data subsets as a function of the language model order. It can be observed that performance of unstructured t, and squared 4 penalties improve until a relatively low order and then degrade, while �� 2 penalty does not show such degradation, indicating 240 2 4 6 8 10 12 tree depth (a) Iteration time of random-pivoting on the collapsed and uncollapsed trees. 1 2 3 4 5 train size X 106 (b) Iteration time of random-pivoting and k-best heap on the collapsed tree. time(sec) 40 60 20 rand-pivot rand-pivot-col time(sec) 40 60 20 k-best heap rand</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm- an extensible language modeling toolkit. Proc. Int. Conf. Spoken Language Processing, 2:901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ukkonen</author>
</authors>
<title>Online construction of suffix trees.</title>
<date>1995</date>
<publisher>Algorithmica.</publisher>
<contexts>
<context position="14179" citStr="Ukkonen, 1995" startWordPosition="2443" endWordPosition="2444"> Proximal step on the suffix tree When feature values are identical, the corresponding proximal (and gradient) steps are identical. This can be seen from the proximal steps (7) and (6), which apply to single weight entries. This property can be used to group together parameters for which the feature values are equal. Hence, we can collapse successive nodes that always have the same values in a suffix trie (as in Figure 1(b)), that is to say we can directly work on the suffix tree. This leads to a proximal step with complexity that scales linearly with the number of symbols seen in the corpus (Ukkonen, 1995) and logarithmically with context length. 4 Structured penalties The `1 and squared `2 penalties do not account for the sequential dependencies in the data, treating suffixes of different lengths equally. This is inappropriate considering that longer suffixes are typically observed less frequently than shorter ones. Moreover, the fact that suffixes might be nested is disregarded. Hence, we propose to use the tree-structured Wt+1 = argmin WEJC 1 2 236 Algorithm 2 w := HfT 2(w, κ) Proximal projection step for `T2 on grouping G. 1 Input: T suffix tree, w trie-structured vector, κ threshold 2 Init</context>
</contexts>
<marker>Ukkonen, 1995</marker>
<rawString>E. Ukkonen. 1995. Online construction of suffix trees. Algorithmica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vargas</author>
<author>P Castells</author>
<author>D Vallet</author>
</authors>
<title>Explicit relevance models in intent-oriented information retrieval diversification.</title>
<date>2012</date>
<booktitle>In Proc. 35th Int. ACM SIGIR Conf. Research and development in information retrieval, SIGIR ’12,</booktitle>
<pages>75--84</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1207" citStr="Vargas et al., 2012" startWordPosition="163" endWordPosition="166">ale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efficiently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overfitting and achieve better generalization. 1 Introduction Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition (Burget et al., 2007), machine translation (Chang and Collins, 2011), or information retrieval (Vargas et al., 2012). When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum en</context>
</contexts>
<marker>Vargas, Castells, Vallet, 2012</marker>
<rawString>S. Vargas, P. Castells, and D. Vallet. 2012. Explicit relevance models in intent-oriented information retrieval diversification. In Proc. 35th Int. ACM SIGIR Conf. Research and development in information retrieval, SIGIR ’12, pages 75–84. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>C Archambeau</author>
<author>J Gasthaus</author>
<author>J Lancelot</author>
<author>Y-W Teh</author>
</authors>
<title>A stochastic memoizer for sequence data. In</title>
<date>2009</date>
<booktitle>Proc. 26th Intl. Conf. on Machine Learning.</booktitle>
<contexts>
<context position="1945" citStr="Wood et al., 2009" startWordPosition="281" endWordPosition="284">the sequence. Models based on so-called back-off smoothing have shown good predictive power (Goodman, 2001). In particular, Kneser-Ney (KN) and its variants (Kneser and Ney, 1995) are still achieving state-of-the-art results for more than a decade after they were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well-suited to this problem as they mimic the nested nature of word contexts. However, existing optimizing tec</context>
</contexts>
<marker>Wood, Archambeau, Gasthaus, Lancelot, Teh, 2009</marker>
<rawString>F. Wood, C. Archambeau, J. Gasthaus, J. Lancelot, and Y.-W. Teh. 2009. A stochastic memoizer for sequence data. In Proc. 26th Intl. Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>J Gasthaus</author>
<author>C Archambeau</author>
<author>L James</author>
<author>Y W Teh</author>
</authors>
<title>The sequence memoizer.</title>
<date>2011</date>
<journal>In Communications of the ACM,</journal>
<volume>54</volume>
<pages>91--98</pages>
<contexts>
<context position="6278" citStr="Wood et al., 2011" startWordPosition="1019" endWordPosition="1022">zed log-loss: W* E argmin f(W) + AQ(W), (2) WEJC where f(W) := −Eni=1 lnp(yi|x1:i;W) and K is a convex set representing the constraints applied on the parameters. Overfitting is avoided by adjusting the regularization parameter A, e.g., by crossvalidation. 2.2 Suffix tree encoding Suffix trees provide an efficient way to store and manipulate discrete sequences and can be constructed in linear time when the vocabulary is fixed (Giegerich and Kurtz, 1997). Recent examples include language models based on a variable-length Markovian assumption (Kennington et al., 2012) and the sequence memoizer (Wood et al., 2011). The suffix tree data structure encodes all the unique suffixes observed in a sequence up to a maximum given length. It exploits the fact that the set of observed contexts is a small subset of all possible contexts. When a series of suffixes of increasing lengths are 234 Algorithm 1 W* argmin If(X, Y ; W)+ AQ(W)} Stochastic optimization algorithm (Hu et al., 2009) 1 Input: λ regularization parameter , L Lipschitz constant of Vf , µ coefficient of strong-convexity of f + λQ, X design matrix, Y label set 2 Initialize: W = Z = 0, τ = δ = 1, ρ = L + µ 3 repeat until maximum iterations 4 #estimate</context>
<context position="8109" citStr="Wood et al., 2011" startWordPosition="1381" endWordPosition="1384">e also has a tree structure, but it potentially has much larger number of nodes. An example of a suffix trie S and the associated suffix tree T are shown in Figures 1(a) and 1(b) respectively. We use |S |to denote the number of nodes in the trie S and |T |for the number of nodes in the tree T. Suffix tree encoding is particularly helpful in applications where the resulting hierarchical structures are thin and tall with numerous non-branching paths. In the case of text, it has been observed that the number of nodes in the tree grows slower than that of the trie with the length of the sequence (Wood et al., 2011; Kennington et al., 2012). This is a significant gain in the memory requirements and, as we will show in Section 4, can also lead to important computational gains when this structure is exploited. The feature vector 0,,,(x) encodes suffixes (or contexts) of increasing length up to a maximum length m. Hence, the model defined in (1) is similar to m-gram language models. Naively, the feature vector 0,,,,(x) corresponds to one path of length m starting at the root of the suffix trie S. The entries in W correspond to weights for each suffix. We thus have a trie structure S on W (see Figure 1(a)) </context>
</contexts>
<marker>Wood, Gasthaus, Archambeau, James, Teh, 2011</marker>
<rawString>F. Wood, J. Gasthaus, C. Archambeau, L. James, and Y. W. Teh. 2011. The sequence memoizer. In Communications of the ACM, volume 54, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wu</author>
<author>S Khundanpur</author>
</authors>
<title>Efficient training methods for maximum entropy language modeling.</title>
<date>2000</date>
<booktitle>Proc. 6th Inter. Conf. Spoken Language Technologies,</booktitle>
<pages>114--117</pages>
<marker>Wu, Khundanpur, 2000</marker>
<rawString>J. Wu and S. Khundanpur. 2000. Efficient training methods for maximum entropy language modeling. Proc. 6th Inter. Conf. Spoken Language Technologies, pages 114–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zhao</author>
<author>G Rocha</author>
<author>B Yu</author>
</authors>
<title>The composite absolute penalties family for grouped and hierarchical variable selection. The Annals of Statistics,</title>
<date>2009</date>
<pages>37--6</pages>
<contexts>
<context position="2203" citStr="Zhao et al., 2009" startWordPosition="322" endWordPosition="325">were originally proposed. Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion. Hence, more principled ways of learning language models have been proposed based on maximum entropy (Chen and Rosenfeld, 2000) or conditional random fields (Roark et al., 2004), or by adopting a Bayesian approach (Wood et al., 2009). In this paper, we focus on penalized maximum likelihood estimation in log-linear models. In contrast to language models based on unstructured norms such as 4 (quadratic penalties) or , (absolute discounting), we use tree-structured norms (Zhao et al., 2009; Jenatton et al., 2011). Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition (Martins et al., 2011), but not language modelling. Such penalties are particularly well-suited to this problem as they mimic the nested nature of word contexts. However, existing optimizing techniques are not scalable for large contexts m. In this work, we show that structured tree norms provide an efficient framework for language modelling. For a special case of these tree norms, we obtain an memory-efficient learning algorithm for log-linear lan</context>
<context position="15463" citStr="Zhao et al., 2009" startWordPosition="2666" endWordPosition="2669">ardPass(η, w) Procedure: η := UpwardPass(η, γ, κ, w) 1 for x ∈ DepthFirstSuffixTraversal(T, PostOrder) 2 γx = w2x + Eh∈children(x) γh 3 ηx = [1 − κ/√γx]+ 4 γx = η2xγx Procedure: w := DownwardPass(η, w) 1 for x ∈ DepthFirstSuffixTraversal(T, PreOrder) 2 wx = ηxwx 3 for h ∈ children(x) 4 ηh = ηxηh a DepthFirstSuffixTraversal(T,Order) returns observed suffixes from the suffix tree T by depth-first traversal in the order prescribed by Order. b wx is the weights corresponding to the suffix x from the weight vector w and children(x) returns all the immediate children to suffix x in the tree. norms (Zhao et al., 2009; Jenatton et al., 2011), which are based on the suffix trie or tree, where subtrees correspond to contexts of increasing lengths. As will be shown in the experiments, this prevents the model to overfit unlike the `1- or squared `2- norm. 4.1 Definition of tree-structured `Tp norms Definition 1. Let x be a training sequence. Group g(w, j) is the subvector of w associated with the subtree rooted at the node j of the suffix trie S(x). Definition 2. Let G denote the ordered set of nodes of the tree T(x) such that for r &lt; s, g(w, r) n g(w, s) = 0 or g(w, r) C g(w, s). The treestructured `p-norm is</context>
</contexts>
<marker>Zhao, Rocha, Yu, 2009</marker>
<rawString>P. Zhao, G. Rocha, and B. Yu. 2009. The composite absolute penalties family for grouped and hierarchical variable selection. The Annals of Statistics, 37(6A):3468–3497.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
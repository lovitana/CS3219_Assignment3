<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004706">
<title confidence="0.9981535">
Detecting Compositionality of Multi-Word Expressions using Nearest
Neighbours in Vector Space Models
</title>
<author confidence="0.993967">
Douwe Kiela Stephen Clark
</author>
<affiliation confidence="0.99859">
University of Cambridge University of Cambridge
Computer Laboratory Computer Laboratory
</affiliation>
<email confidence="0.997699">
douwe.kiela@cl.cam.ac.uk stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999248461538462">
We present a novel unsupervised approach to
detecting the compositionality of multi-word
expressions. We compute the compositional-
ity of a phrase through substituting the con-
stituent words with their “neighbours” in a se-
mantic vector space and averaging over the
distance between the original phrase and the
substituted neighbour phrases. Several meth-
ods of obtaining neighbours are presented.
The results are compared to existing super-
vised results and achieve state-of-the-art per-
formance on a verb-object dataset of human
compositionality ratings.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999683">
Multi-word expressions (MWEs) are defined as “id-
iosyncratic interpretations that cross word bound-
aries” (Sag et al., 2002). They tend to have a
standard syntactic structure but are often semanti-
cally non-compositional; i.e. their meaning is not
fully determined by their syntactic structure and the
meanings of their constituents. A classic example
is kick the bucket, which means to die rather than to
hit a bucket with the foot. These types of expres-
sions account for a large proportion of day-to-day
language interactions (Schuler and Joshi, 2011) and
present a significant problem for natural language
processing systems (Sag et al., 2002).
This paper presents a novel unsupervised ap-
proach to detecting the compositionality of MWEs,
specifically of verb-noun collocations. The idea is
that we can recognize compositional phrases by sub-
stituting related words for constituent words in the
phrase: if the result of a substitution yields a mean-
ingful phrase, its individual constituents are likely to
contribute toward the overall meaning of the phrase.
Conversely, if a substitution yields a non-sensical
phrase, its constituents are likely to contribute less
or not at all to the overall meaning of the phrase.
For the phrase eat her hat, for example, we might
consider the following substituted phrases:
</bodyText>
<listItem confidence="0.984827">
1. consume her hat
2. eat her trousers
</listItem>
<bodyText confidence="0.999747947368421">
Both phrases are semantically anomalous, implying
that eat hat is a highly non-compositional verb-noun
collocation. Following a similar procedure for eat
apple, however, would not lead to an anomaly: con-
sume apple and eat pear are perfectly meaningful,
leading us to believe that eat apple is compositional.
In the context of distributional models, this idea
can be formalised in terms of vector spaces:
the average distance between a phrase
vector and its substituted phrase vectors is
related to its compositionality.
Since we are relying on the relative distances of
phrases in semantic space, we require a method
for computing vectors for phrases. We experi-
mented with a number of composition operators
from Mitchell and Lapata (2010), in order to com-
pose constituent word vectors into phrase vectors.
The relation between phrase vectors and substituted
phrase vectors is most pronounced in the case of
</bodyText>
<page confidence="0.941697">
1427
</page>
<note confidence="0.731691">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1427–1432,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999913888888889">
pointwise multiplication, which has the effect of
placing semantically anomalous phrases relatively
close together in space (since the vectors for the con-
stituent words have little in common), whereas the
semantically meaningful phrases are further apart.
This implies that compositional phrases are less sim-
ilar to their neighbours, which is to say that the
greater the average distance between a phrase vec-
tor and its substituted phrase vectors, the greater its
compositionality.
The contribution of this short focused research pa-
per is a novel approach to detecting the composition-
ality of multi-word expressions that makes full use
of the ability of semantic vector space models to cal-
culate distances between words and phrases. Using
this unsupervised approach, we achieve state-of-the-
art performance in a direct comparison with existing
supervised methods.
</bodyText>
<sectionHeader confidence="0.956539" genericHeader="introduction">
2 Dataset and Vectors
</sectionHeader>
<bodyText confidence="0.999979757575757">
The verb-noun collocation dataset from Venkatapa-
thy and Joshi (2005), which consists of 765 verb-
object pairs with human compositionality ratings,
was used for evaluation. Venkatapathy &amp; Joshi used
a support vector machine (SVM) to obtain a Spear-
man ps correlation of 0.448. They employed a va-
riety of features ranging from frequency to LSA-
derived similarity measures and used 10% of the
dataset as training data with tenfold cross-validation.
McCarthy et al. (2007) used the same dataset and ex-
panded on the original approach by adding WordNet
and distributional prototypes to the SVM, achieving
a ps correlation of 0.454.
The distributional vectors for our experiments
were constructed from the ukWaC corpus (Baroni
et al., 2009). Vectors were obtained using a stan-
dard window method (with a window size of 5) and
the 50,000 most frequent context words as features,
with stopwords removed. We also experimented
with syntax-based co-occurrence features extracted
from a dependency-parsed version of ukWaC, but
in agreement with results obtained by Schulte im
Walde et al. (2013) for predicting compositional-
ity in German, the window-based co-occurrence
method produced better results.
We tried several weighting schemes from the liter-
ature, such as t-test (Curran, 2004), positive mutual
information (Bullinaria and Levy, 2012) and the ra-
tio of the probability of the context word given the
target word1 to the context word’s overall probabil-
ity (Mitchell and Lapata, 2010). We found that a
tf-idf variant called LTU yielded the best results, de-
fined as follows (Reed et al., 2006):
</bodyText>
<equation confidence="0.822450666666667">
(log(fij) + 1.0) log(&apos;)
0.8 + 0.2 x |context word|
Iavg context wordl
</equation>
<bodyText confidence="0.999549833333333">
where fij is the number of times that the target word
and context word co-occur in the same window, nj
is the context word frequency, N is the total fre-
quency and |context word |is the total number of oc-
currences of a context word. Distance is calculated
using the standard cosine measure:
</bodyText>
<equation confidence="0.9985835">
v1 v2
dist(v1, v2) = 1
</equation>
<bodyText confidence="0.9996825">
where v1 and v2 are vectors in the semantic vector
space model.
</bodyText>
<sectionHeader confidence="0.96177" genericHeader="method">
3 Finding Neighbours and Computing
Compositionality
</sectionHeader>
<bodyText confidence="0.99996455">
We experimented with two different ways of obtain-
ing neighbours for the constituent words in a phrase.
Since vector space models lend themselves naturally
to similarity computations, one way to get neigh-
bours is to take the k-most similar vectors from a
similarity matrix. This approach is straightforward,
but has some potential drawbacks: it assumes that
we have a large number of vectors to select neigh-
bours from, and becomes computationally expensive
when the number of neighbours is increased.
An alternative source for obtaining neighbours is
the lexical database WordNet (Fellbaum, 1998). We
define neighbours as siblings in the hypernym hier-
archy, so that the neighbours of a word can be found
by taking the hyponyms of its hypernyms. Word-
Net also allows us to extract only neighbours of the
same grammatical type (yielding noun neighbours
for nouns and verb neighbours for verbs, for exam-
ple). Since not every word has the same number
of neighbours in WordNet, we use only the first k
</bodyText>
<footnote confidence="0.8523655">
1We use target word to refer to the word for which a vector
is being constructed.
</footnote>
<equation confidence="0.9728815">
wij =
|v1||v2|
</equation>
<page confidence="0.929265">
1428
</page>
<bodyText confidence="0.999273105263158">
neighbours, which means that the neighbours have
to be ranked. An obvious ranking method is to use
the frequency with which each neighbour co-occurs
with the other constituent(s) of the same phrase. For
example, for all the WordNet neighbours of eat (for
all senses of eat), we count the co-occurrences with
hat in a given window size and rank them accord-
ingly. This ranking method also has the desirable
side-effect of performing some word sense disam-
biguation, at least in some cases. For example, the
highly ranked neighbours of apple for eat apple are
likely to be items of food, and not (inedible) trees
(apple is also a tree in WordNet).
In order to obtain frequency-ranked neighbours,
we used the ukWaC corpus with a window size of
5. One reason for having multiple neighbours is that
it allows us to correct for word sense disambigua-
tion errors (as mentioned above), since averaging
over results for several neighbours reduces the im-
pact of including incorrect senses. For example, the
first 20 neighbours of eat, ranked by co-occurrence
frequency with all the objects of eat in the dataset,
are:
eat use consume drink sample smoke
swallow spend break hit save afford burn
partake dine breakfast worry damage de-
plete drug
One problem with the evaluation dataset is that
it does not solely consist of verb-noun pairs: 84
phrases contain pronouns, while there are also sev-
eral examples containing words that WordNet con-
siders to be adjectives rather than nouns. This prob-
lem was mitigated by part-of-speech tagging the
dataset. As neighbours for pronouns (which are not
included in WordNet), we used the other pronouns
present in the dataset. For the remaining words,
we included the part-of-speech when looking up the
word in WordNet.
</bodyText>
<subsectionHeader confidence="0.999286">
3.1 Average distance compositionality score
</subsectionHeader>
<bodyText confidence="0.973808352941177">
We considered several different ways of construct-
ing phrasal vectors. We chose not to use the com-
positional models of Baroni and Zamparelli (2010)
and Socher et al. (2011) because we believe that it is
important that our methods are completely unsuper-
vised and do not require any initial learning phase.
Hence, we experimented with different ways of con-
structing phrasal vectors according to Mitchell and
Lapata (2010) and found that pointwise multiplica-
tion O worked best in our experiments. Thus, we
−−−−→
define the composed vector eat hat as:
eat O hat
We can now compute a compositionality score sc by
averaging the distance between the original phrase
vector and its substituted neighbour phrase vectors
via the following formula:
</bodyText>
<equation confidence="0.998635857142857">
1 k
sc(−−−−→
eat hat) = (�
2k i=1
eat → (D neighbouri) +
−−−−−−−→ −→
neighbourj � hat))
</equation>
<bodyText confidence="0.999966">
We also experimented with substituting only for
the noun or the verb, and in fact found that only tak-
ing neighbours for the verb yields better results:
</bodyText>
<equation confidence="0.864482333333333">
−−−−→ 1�
sc(eat hat) = k
neighbourj (D hat)
</equation>
<bodyText confidence="0.9918221875">
To illustrate the method, consider the collocations
take breath and lend money. The annotators as-
signed these phrases a compositionality score of 1
out of 6 and 6 out of 6, respectively, meaning that the
former is non-compositional and the latter is com-
positional. The distances between the first ten verb-
substituted phrases and the original phrase, together
with the average distance, are shown in Table 1 and
Table 2.
Substituting the verb in the non-compositional
phrase yields semantically anomalous vectors,
which leads to very small changes in the distance
between it and the original phrase vector. This is a
result of using pointwise multiplication, where over-
lapping components are stressed: since the vectors
for take and breath have little overlap outside of
</bodyText>
<figure confidence="0.968781655172414">
→
dist(eat O hat,
→
dist(eat (D hat,
�k
j=1
→
dist(eat O hat,
j=1
1429
Dist
Neighbour
get breath
0.049
find breath
0.051
use breath
0.050
work breath
0.060
hold breath
0.094
run breath
0.079
carry breath
0.076
look breath
play breath
0.065
</figure>
<table confidence="0.946069666666667">
System ps
Venkatapathy and Joshi (2005) 0.447
McCarthy et al. (2007) 0.454
AvgDist VSM neighbours-both 0.131
AvgDist VSM neighbours-verb 0.420
AvgDist VSM neighbours-noun 0.245
AvgDist WN-ranked neighbours-both 0.165
AvgDist WN-ranked neighbours-verb 0.461
AvgDist WN-ranked neighbours-noun 0.169
0.071
buy breath
0.100
Table 3: Spearman ps results
AvgDist
0.069
</table>
<tableCaption confidence="0.999493">
Table 1: Example take breath
</tableCaption>
<bodyText confidence="0.959584272727273">
Neighbour
pay money
put money
bring money
provide money
owe money
sell money
cost money
look money
distribute money
offer money
</bodyText>
<sectionHeader confidence="0.561828" genericHeader="method">
AvgDist
</sectionHeader>
<tableCaption confidence="0.992253">
Table 2: Example lend money
</tableCaption>
<bodyText confidence="0.99966775">
the idiomatic sense in take breath, its neighbour-
substituted phrases also have little overlap, result-
ing in a smaller change in distance upon substitu-
tion. Conversely, substituting the verb in the com-
positional phrase yields meaningful vectors, putting
them in locations in semantic vector space which are
sufficiently far apart to distinguish them from the
non-compositional cases.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999955414634147">
Results are given for the two methods of obtaining
neighbours: via frequency-ranked WordNet neigh-
bours and via vector space neighbours. The com-
positionality score was computed by using only the
verb, only the noun, or both constituent neighbours
in the substituted phrase vectors.
The results are compared with the scores reported
in Venkatapathy and Joshi (2005) and McCarthy et
al. (2007), which were achieved using SVMs with a
wide variety of features. Values of 1 G k G 20 were
tried. If a phrase has fewer than k neighbours be-
cause not enough neighbours have been found to co-
occur with the other constituent, we use all of them.
The results for k = 20 are reported here because
that gave the best overall score. The dataset has an
inter-annotator agreement of Kendall’s r of 0.61 and
a Spearman ps of 0.71 and all reported differences
in values are highly significant. Table 3 gives the
results.
Note that, even though the current approach is un-
supervised (in terms of not having access to compo-
sitionality ratings during training, although it does
rely on WordNet), it outperforms SVMs that require
an ensemble of complex feature sets (some of which
are also based on WordNet).
It is interesting to observe that the state-of-the-art
performance is reached when only using the verb’s
neighbours to compute substituted phrase vectors.
One might initially expect this not to be the case,
since e.g. eat trousers, where the noun has been
substituted, does not make a lot of sense either —
which we would expect to be informative for de-
termining compositionality. There are two possi-
ble explanations for this, which might be at play
simultaneously: since our dataset consists of verb-
object pairs, the verb constituent is always the head
word of the phrase, and the dataset contains several
so-called “light verbs”, which have little semantic
content of their own. Head words have been found
to have a higher impact on compositionality scores
for compound nouns: Reddy et al. (2011) weighted
</bodyText>
<figure confidence="0.997155083333333">
Dist
0.446
0.432
0.405
0.442
0.559
0.404
0.482
0.425
0.544
0.428
0.457
</figure>
<page confidence="0.977962">
1430
</page>
<bodyText confidence="0.99993625">
the contribution of individual constituents in such a
way that the modifier’s contribution is included but
is weighted less highly than the head’s contribution,
which led to an improvement in performance. Our
results might be improved by weighting the contri-
bution of constituent words in a similar fashion, and
by more closely examining the impact of light verbs
for the compositionality of a phrase.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999984551724138">
The past decade has seen extensive work on compu-
tational and statistical methods in detecting the com-
positionality of MWEs (Lin, 1999; Schone and Ju-
rafsky, 2001; Katz and Giesbrecht, 2006; Sporleder
and Li, 2009; Biemann and Giesbrecht, 2011).
Many of these methods rely on distributional mod-
els and vector space models (Sch¨utze, 1993; Tur-
ney and Pantel, 2010; Erk, 2012). Work has been
done on different types of phrases, including work
on particle verbs (McCarthy et al., 2003; Bannard
et al., 2003), verb-noun collocations (Venkatapathy
and Joshi, 2005; McCarthy et al., 2007), adjective-
noun combinations (Vecchi et al., 2011) and noun-
noun compounds (Reddy et al., 2011), as well as on
languages other than English (Schulte im Walde et
al., 2013). Recent developments in distributional
compositional models (Widdows, 2008; Mitchell
and Lapata, 2010; Baroni and Zamparelli, 2010; Co-
ecke et al., 2010; Socher et al., 2011) have opened
up a number of possibilities for constructing vectors
for phrases, which have also been applied to com-
positionality tests (Giesbrecht, 2009; Kochmar and
Briscoe, 2013).
This paper takes that work a step further: by con-
structing phrase vectors and evaluating these vectors
on a dataset of human compositionality ratings, we
show that existing compositional models allow us to
detect compositionality of multi-word expressions
in a straightforward and intuitive manner.
</bodyText>
<sectionHeader confidence="0.99671" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999872">
We have presented a novel unsupervised approach
that can be used to detect the compositionality of
multi-word expressions. Our results show that the
underlying intuition appears to be sound: substitut-
ing neighbours may lead to meaningful or meaning-
less phrases depending on whether or not the phrase
is compositional. This can be formalized in vec-
tor space models to obtain compositionality scores
by computing the average distance to the original
phrase’s substituted neighbour phrases. In this short
focused research paper, we show that, depending on
how we obtain neighbours, we are able to achieve
a higher performance than that achieved by super-
vised methods which rely on a complex feature set
and support vector machines.
</bodyText>
<sectionHeader confidence="0.983953" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999850666666667">
This work has been supported by EPSRC grant
EP/I037512/1. The authors would like to thank Di-
ana McCarthy for providing the dataset; and Ed
Grefenstette, Eva Maria Vecchi, Laura Rimell and
Tamara Polajnar and the anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994082">
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 Workshop
on Multiword expressions: analysis, acquisition and
treatment, MWE 03.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’10, pages 1183–1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Chris Biemann and Eugenie Giesbrecht. 2011. Disco-
11: Proceedings of the workshop on distributional se-
mantics and compositionality.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting
Semantic Representations from Word Co-occurrence
Statistics: Stop-lists, Stemming and SVD. Behavior
Research Methods, 44:890–907.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguistic
Analysis (Lambek Festschrift), volume 36, pages 345–
384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
</reference>
<page confidence="0.924345">
1431
</page>
<reference confidence="0.9997284">
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635–653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Sebastian Rudolph,
Frithjof Dau, and SergeiO. Kuznetsov, editors, Con-
ceptual Structures: Leveraging Semantic Technolo-
gies, volume 5662 of Lecture Notes in Computer Sci-
ence, pages 173–184. Springer Berlin Heidelberg.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
’06, pages 12–19.
Ekaterina Kochmar and Ted Briscoe. 2013. Capturing
Anomalies in the Choice of Content Words in Com-
positional Distributional Semantic Space. In Recent
Advances in Natural Language Processing.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ’99,
pages 317–324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment - Volume 18, MWE ’03, pages 73–80.
Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369–379.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Thailand.
J.W. Reed, Y. Jiao, T.E. Potok, B.A. Klump, M.T. El-
more, and A.R. Hurson. 2006. TF-ICF: A new term
weighting scheme for clustering dynamic data streams.
In Machine Learning and Applications, 2006. ICMLA
’06. 5th International Conference on, pages 258–263.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
CICLing ’02, pages 1–15.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
Empirical Methods in Natural Language Processing,
EMNLP ’01.
William Schuler and Aravind K. Joshi. 2011. Tree-
rewriting models of multi-word expressions. In Pro-
ceedings of the Workshop on Multiword Expressions:
from Parsing and Generation to the Real World, MWE
’11, pages 25–30.
Sabine Schulte im Walde, Stefan M¨uller, and Stephen
Roller. 2013. Exploring Vector Space Models to
Predict the Compositionality of German Noun-Noun
Compounds. In Proceedings of the 2nd Joint Confer-
ence on Lexical and Computational Semantics, pages
255–265, Atlanta, GA.
Hinrich Sch¨utze. 1993. Word space. In Advances in
Neural Information Processing Systems 5, pages 895–
902. Morgan Kaufmann.
Richard Socher, Cliff Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011. Parsing Natural Scenes
and Natural Language with Recursive Neural Net-
works. In The 28th International Conference on Ma-
chine Learning, ICML 2011.
Caroline Sporleder and Linlin Li. 2009. 2009. unsuper-
vised recognition of literal and non-literal use of id-
iomatic expressions. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL, EACL
’09.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141–188, January.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (linear) maps of the impossible: Capturing
semantic anomalies in distributional space. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality, pages 1–9, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-n)
collocations by integrating features. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
HLT ’05, pages 899–906.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sympo-
sium on Quantum Interaction, Oxford.
</reference>
<page confidence="0.994563">
1432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863827">
<title confidence="0.980888">Detecting Compositionality of Multi-Word Expressions using Neighbours in Vector Space Models</title>
<author confidence="0.99538">Douwe Kiela Stephen Clark</author>
<affiliation confidence="0.9997395">University of Cambridge University of Cambridge Computer Laboratory Computer Laboratory</affiliation>
<email confidence="0.914746">douwe.kiela@cl.cam.ac.ukstephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.998872857142857">We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Timothy Baldwin</author>
<author>Alex Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multiword expressions: analysis, acquisition and treatment, MWE 03.</booktitle>
<contexts>
<context position="15119" citStr="Bannard et al., 2003" startWordPosition="2421" endWordPosition="2424"> fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 201</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proceedings of the ACL 2003 Workshop on Multiword expressions: analysis, acquisition and treatment, MWE 03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="9298" citStr="Baroni and Zamparelli (2010)" startWordPosition="1467" endWordPosition="1470"> of verb-noun pairs: 84 phrases contain pronouns, while there are also several examples containing words that WordNet considers to be adjectives rather than nouns. This problem was mitigated by part-of-speech tagging the dataset. As neighbours for pronouns (which are not included in WordNet), we used the other pronouns present in the dataset. For the remaining words, we included the part-of-speech when looking up the word in WordNet. 3.1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. (2011) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication O worked best in our experiments. Thus, we −−−−→ define the composed vector eat hat as: eat O hat We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula: 1 k sc(−−</context>
<context position="15498" citStr="Baroni and Zamparelli, 2010" startWordPosition="2476" endWordPosition="2479">ese methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsupervised approach that can be use</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="4917" citStr="Baroni et al., 2009" startWordPosition="739" endWordPosition="742">ith human compositionality ratings, was used for evaluation. Venkatapathy &amp; Joshi used a support vector machine (SVM) to obtain a Spearman ps correlation of 0.448. They employed a variety of features ranging from frequency to LSAderived similarity measures and used 10% of the dataset as training data with tenfold cross-validation. McCarthy et al. (2007) used the same dataset and expanded on the original approach by adding WordNet and distributional prototypes to the SVM, achieving a ps correlation of 0.454. The distributional vectors for our experiments were constructed from the ukWaC corpus (Baroni et al., 2009). Vectors were obtained using a standard window method (with a window size of 5) and the 50,000 most frequent context words as features, with stopwords removed. We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual information (Bullinaria and Levy, 2012) an</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Eugenie Giesbrecht</author>
</authors>
<date>2011</date>
<booktitle>Disco11: Proceedings of the workshop on distributional semantics and compositionality.</booktitle>
<contexts>
<context position="14859" citStr="Biemann and Giesbrecht, 2011" startWordPosition="2377" endWordPosition="2380"> constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and La</context>
</contexts>
<marker>Biemann, Giesbrecht, 2011</marker>
<rawString>Chris Biemann and Eugenie Giesbrecht. 2011. Disco11: Proceedings of the workshop on distributional semantics and compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming and SVD. Behavior Research Methods,</title>
<date>2012</date>
<pages>44--890</pages>
<contexts>
<context position="5514" citStr="Bullinaria and Levy, 2012" startWordPosition="827" endWordPosition="830"> corpus (Baroni et al., 2009). Vectors were obtained using a standard window method (with a window size of 5) and the 50,000 most frequent context words as features, with stopwords removed. We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual information (Bullinaria and Levy, 2012) and the ratio of the probability of the context word given the target word1 to the context word’s overall probability (Mitchell and Lapata, 2010). We found that a tf-idf variant called LTU yielded the best results, defined as follows (Reed et al., 2006): (log(fij) + 1.0) log(&apos;) 0.8 + 0.2 x |context word| Iavg context wordl where fij is the number of times that the target word and context word co-occur in the same window, nj is the context word frequency, N is the total frequency and |context word |is the total number of occurrences of a context word. Distance is calculated using the standard </context>
</contexts>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2012. Extracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming and SVD. Behavior Research Methods, 44:890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<booktitle>Linguistic Analysis (Lambek Festschrift),</booktitle>
<volume>36</volume>
<pages>345--384</pages>
<editor>In J. van Bentham, M. Moortgat, and W. Buszkowski, editors,</editor>
<contexts>
<context position="15519" citStr="Coecke et al., 2010" startWordPosition="2480" endWordPosition="2484">ional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsupervised approach that can be used to detect the compo</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis (Lambek Festschrift), volume 36, pages 345– 384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5457" citStr="Curran, 2004" startWordPosition="822" endWordPosition="823"> experiments were constructed from the ukWaC corpus (Baroni et al., 2009). Vectors were obtained using a standard window method (with a window size of 5) and the 50,000 most frequent context words as features, with stopwords removed. We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual information (Bullinaria and Levy, 2012) and the ratio of the probability of the context word given the target word1 to the context word’s overall probability (Mitchell and Lapata, 2010). We found that a tf-idf variant called LTU yielded the best results, defined as follows (Reed et al., 2006): (log(fij) + 1.0) log(&apos;) 0.8 + 0.2 x |context word| Iavg context wordl where fij is the number of times that the target word and context word co-occur in the same window, nj is the context word frequency, N is the total frequency and |context word |is the total number of occurrences of a</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="14989" citStr="Erk, 2012" startWordPosition="2401" endWordPosition="2402">provement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for co</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="6864" citStr="Fellbaum, 1998" startWordPosition="1057" endWordPosition="1058">ng Compositionality We experimented with two different ways of obtaining neighbours for the constituent words in a phrase. Since vector space models lend themselves naturally to similarity computations, one way to get neighbours is to take the k-most similar vectors from a similarity matrix. This approach is straightforward, but has some potential drawbacks: it assumes that we have a large number of vectors to select neighbours from, and becomes computationally expensive when the number of neighbours is increased. An alternative source for obtaining neighbours is the lexical database WordNet (Fellbaum, 1998). We define neighbours as siblings in the hypernym hierarchy, so that the neighbours of a word can be found by taking the hyponyms of its hypernyms. WordNet also allows us to extract only neighbours of the same grammatical type (yielding noun neighbours for nouns and verb neighbours for verbs, for example). Since not every word has the same number of neighbours in WordNet, we use only the first k 1We use target word to refer to the word for which a vector is being constructed. wij = |v1||v2| 1428 neighbours, which means that the neighbours have to be ranked. An obvious ranking method is to use</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>In search of semantic compositionality in vector spaces.</title>
<date>2009</date>
<booktitle>Conceptual Structures: Leveraging Semantic Technologies,</booktitle>
<volume>5662</volume>
<pages>173--184</pages>
<editor>In Sebastian Rudolph, Frithjof Dau, and SergeiO. Kuznetsov, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="15693" citStr="Giesbrecht, 2009" startWordPosition="2510" endWordPosition="2511">Carthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsupervised approach that can be used to detect the compositionality of multi-word expressions. Our results show that the underlying intuition appears to be sound: substituting neighbours may lead to meaningful or meaningless phras</context>
</contexts>
<marker>Giesbrecht, 2009</marker>
<rawString>Eugenie Giesbrecht. 2009. In search of semantic compositionality in vector spaces. In Sebastian Rudolph, Frithjof Dau, and SergeiO. Kuznetsov, editors, Conceptual Structures: Leveraging Semantic Technologies, volume 5662 of Lecture Notes in Computer Science, pages 173–184. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06,</booktitle>
<pages>12--19</pages>
<contexts>
<context position="14804" citStr="Katz and Giesbrecht, 2006" startWordPosition="2369" endWordPosition="2372">544 0.428 0.457 1430 the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distribution</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06, pages 12–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Kochmar</author>
<author>Ted Briscoe</author>
</authors>
<date>2013</date>
<booktitle>Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space. In Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="15721" citStr="Kochmar and Briscoe, 2013" startWordPosition="2512" endWordPosition="2515">3; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsupervised approach that can be used to detect the compositionality of multi-word expressions. Our results show that the underlying intuition appears to be sound: substituting neighbours may lead to meaningful or meaningless phrases depending on whether or n</context>
</contexts>
<marker>Kochmar, Briscoe, 2013</marker>
<rawString>Ekaterina Kochmar and Ted Briscoe. 2013. Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space. In Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="14750" citStr="Lin, 1999" startWordPosition="2362" endWordPosition="2363">0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 1430 the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Wa</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume 18, MWE ’03,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="15096" citStr="McCarthy et al., 2003" startWordPosition="2417" endWordPosition="2420">uent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; K</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume 18, MWE ’03, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Sriram Venkatapathy</author>
<author>Aravind Joshi</author>
</authors>
<title>Detecting compositionality of verbobject combinations using selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>369--379</pages>
<contexts>
<context position="4652" citStr="McCarthy et al. (2007)" startWordPosition="697" endWordPosition="700">ses. Using this unsupervised approach, we achieve state-of-theart performance in a direct comparison with existing supervised methods. 2 Dataset and Vectors The verb-noun collocation dataset from Venkatapathy and Joshi (2005), which consists of 765 verbobject pairs with human compositionality ratings, was used for evaluation. Venkatapathy &amp; Joshi used a support vector machine (SVM) to obtain a Spearman ps correlation of 0.448. They employed a variety of features ranging from frequency to LSAderived similarity measures and used 10% of the dataset as training data with tenfold cross-validation. McCarthy et al. (2007) used the same dataset and expanded on the original approach by adding WordNet and distributional prototypes to the SVM, achieving a ps correlation of 0.454. The distributional vectors for our experiments were constructed from the ukWaC corpus (Baroni et al., 2009). Vectors were obtained using a standard window method (with a window size of 5) and the 50,000 most frequent context words as features, with stopwords removed. We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde </context>
<context position="11251" citStr="McCarthy et al. (2007)" startWordPosition="1796" endWordPosition="1799">positional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of → dist(eat O hat, → dist(eat (D hat, �k j=1 → dist(eat O hat, j=1 1429 Dist Neighbour get breath 0.049 find breath 0.051 use breath 0.050 work breath 0.060 hold breath 0.094 run breath 0.079 carry breath 0.076 look breath play breath 0.065 System ps Venkatapathy and Joshi (2005) 0.447 McCarthy et al. (2007) 0.454 AvgDist VSM neighbours-both 0.131 AvgDist VSM neighbours-verb 0.420 AvgDist VSM neighbours-noun 0.245 AvgDist WN-ranked neighbours-both 0.165 AvgDist WN-ranked neighbours-verb 0.461 AvgDist WN-ranked neighbours-noun 0.169 0.071 buy breath 0.100 Table 3: Spearman ps results AvgDist 0.069 Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have little overlap, resulting in a sm</context>
<context position="12522" citStr="McCarthy et al. (2007)" startWordPosition="1984" endWordPosition="1987">rsely, substituting the verb in the compositional phrase yields meaningful vectors, putting them in locations in semantic vector space which are sufficiently far apart to distinguish them from the non-compositional cases. 4 Results Results are given for the two methods of obtaining neighbours: via frequency-ranked WordNet neighbours and via vector space neighbours. The compositionality score was computed by using only the verb, only the noun, or both constituent neighbours in the substituted phrase vectors. The results are compared with the scores reported in Venkatapathy and Joshi (2005) and McCarthy et al. (2007), which were achieved using SVMs with a wide variety of features. Values of 1 G k G 20 were tried. If a phrase has fewer than k neighbours because not enough neighbours have been found to cooccur with the other constituent, we use all of them. The results for k = 20 are reported here because that gave the best overall score. The dataset has an inter-annotator agreement of Kendall’s r of 0.61 and a Spearman ps of 0.71 and all reported differences in values are highly significant. Table 3 gives the results. Note that, even though the current approach is unsupervised (in terms of not having acces</context>
<context position="15197" citStr="McCarthy et al., 2007" startWordPosition="2431" endWordPosition="2434">positionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors </context>
</contexts>
<marker>McCarthy, Venkatapathy, Joshi, 2007</marker>
<rawString>Diana McCarthy, Sriram Venkatapathy, and Aravind Joshi. 2007. Detecting compositionality of verbobject combinations using selectional preferences. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 369–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2928" citStr="Mitchell and Lapata (2010)" startWordPosition="436" endWordPosition="439">on. Following a similar procedure for eat apple, however, would not lead to an anomaly: consume apple and eat pear are perfectly meaningful, leading us to believe that eat apple is compositional. In the context of distributional models, this idea can be formalised in terms of vector spaces: the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. Since we are relying on the relative distances of phrases in semantic space, we require a method for computing vectors for phrases. We experimented with a number of composition operators from Mitchell and Lapata (2010), in order to compose constituent word vectors into phrase vectors. The relation between phrase vectors and substituted phrase vectors is most pronounced in the case of 1427 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1427–1432, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics pointwise multiplication, which has the effect of placing semantically anomalous phrases relatively close together in space (since the vectors for the constituent words have little in common), whereas the semantically meaningf</context>
<context position="5660" citStr="Mitchell and Lapata, 2010" startWordPosition="853" endWordPosition="856"> words as features, with stopwords removed. We also experimented with syntax-based co-occurrence features extracted from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual information (Bullinaria and Levy, 2012) and the ratio of the probability of the context word given the target word1 to the context word’s overall probability (Mitchell and Lapata, 2010). We found that a tf-idf variant called LTU yielded the best results, defined as follows (Reed et al., 2006): (log(fij) + 1.0) log(&apos;) 0.8 + 0.2 x |context word| Iavg context wordl where fij is the number of times that the target word and context word co-occur in the same window, nj is the context word frequency, N is the total frequency and |context word |is the total number of occurrences of a context word. Distance is calculated using the standard cosine measure: v1 v2 dist(v1, v2) = 1 where v1 and v2 are vectors in the semantic vector space model. 3 Finding Neighbours and Computing Composit</context>
<context position="9570" citStr="Mitchell and Lapata (2010)" startWordPosition="1511" endWordPosition="1514">cluded in WordNet), we used the other pronouns present in the dataset. For the remaining words, we included the part-of-speech when looking up the word in WordNet. 3.1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. (2011) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication O worked best in our experiments. Thus, we −−−−→ define the composed vector eat hat as: eat O hat We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula: 1 k sc(−−−−→ eat hat) = (� 2k i=1 eat → (D neighbouri) + −−−−−−−→ −→ neighbourj � hat)) We also experimented with substituting only for the noun or the verb, and in fact found that only taking neighbours for the verb yields better results: −−−−→ 1� sc(eat hat) = k neighbourj (D ha</context>
<context position="15469" citStr="Mitchell and Lapata, 2010" startWordPosition="2472" endWordPosition="2475">esbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsuperv</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An empirical study on compositionality in compound nouns.</title>
<date>2011</date>
<booktitle>In Proceedings of The 5th International Joint Conference on Natural Language Processing</booktitle>
<contexts>
<context position="14114" citStr="Reddy et al. (2011)" startWordPosition="2257" endWordPosition="2260">ct this not to be the case, since e.g. eat trousers, where the noun has been substituted, does not make a lot of sense either — which we would expect to be informative for determining compositionality. There are two possible explanations for this, which might be at play simultaneously: since our dataset consists of verbobject pairs, the verb constituent is always the head word of the phrase, and the dataset contains several so-called “light verbs”, which have little semantic content of their own. Head words have been found to have a higher impact on compositionality scores for compound nouns: Reddy et al. (2011) weighted Dist 0.446 0.432 0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 1430 the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in compound nouns. In Proceedings of The 5th International Joint Conference on Natural Language Processing 2011 (IJCNLP 2011), Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Reed</author>
<author>Y Jiao</author>
<author>T E Potok</author>
<author>B A Klump</author>
<author>M T Elmore</author>
<author>A R Hurson</author>
</authors>
<title>TF-ICF: A new term weighting scheme for clustering dynamic data streams.</title>
<date>2006</date>
<booktitle>In Machine Learning and Applications,</booktitle>
<pages>258--263</pages>
<contexts>
<context position="5768" citStr="Reed et al., 2006" startWordPosition="873" endWordPosition="876">from a dependency-parsed version of ukWaC, but in agreement with results obtained by Schulte im Walde et al. (2013) for predicting compositionality in German, the window-based co-occurrence method produced better results. We tried several weighting schemes from the literature, such as t-test (Curran, 2004), positive mutual information (Bullinaria and Levy, 2012) and the ratio of the probability of the context word given the target word1 to the context word’s overall probability (Mitchell and Lapata, 2010). We found that a tf-idf variant called LTU yielded the best results, defined as follows (Reed et al., 2006): (log(fij) + 1.0) log(&apos;) 0.8 + 0.2 x |context word| Iavg context wordl where fij is the number of times that the target word and context word co-occur in the same window, nj is the context word frequency, N is the total frequency and |context word |is the total number of occurrences of a context word. Distance is calculated using the standard cosine measure: v1 v2 dist(v1, v2) = 1 where v1 and v2 are vectors in the semantic vector space model. 3 Finding Neighbours and Computing Compositionality We experimented with two different ways of obtaining neighbours for the constituent words in a phra</context>
</contexts>
<marker>Reed, Jiao, Potok, Klump, Elmore, Hurson, 2006</marker>
<rawString>J.W. Reed, Y. Jiao, T.E. Potok, B.A. Klump, M.T. Elmore, and A.R. Hurson. 2006. TF-ICF: A new term weighting scheme for clustering dynamic data streams. In Machine Learning and Applications, 2006. ICMLA ’06. 5th International Conference on, pages 258–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A Pain in the Neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’02,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="964" citStr="Sag et al., 2002" startWordPosition="125" endWordPosition="128">ity of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings. 1 Introduction Multi-word expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). They tend to have a standard syntactic structure but are often semantically non-compositional; i.e. their meaning is not fully determined by their syntactic structure and the meanings of their constituents. A classic example is kick the bucket, which means to die rather than to hit a bucket with the foot. These types of expressions account for a large proportion of day-to-day language interactions (Schuler and Joshi, 2011) and present a significant problem for natural language processing systems (Sag et al., 2002). This paper presents a novel unsupervised approach to detecting the compositio</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A Pain in the Neck for NLP. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’02, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing, EMNLP ’01.</booktitle>
<contexts>
<context position="14777" citStr="Schone and Jurafsky, 2001" startWordPosition="2364" endWordPosition="2368"> 0.559 0.404 0.482 0.425 0.544 0.428 0.457 1430 the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent d</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of Empirical Methods in Natural Language Processing, EMNLP ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Aravind K Joshi</author>
</authors>
<title>Treerewriting models of multi-word expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World, MWE ’11,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="1392" citStr="Schuler and Joshi, 2011" startWordPosition="194" endWordPosition="197">n a verb-object dataset of human compositionality ratings. 1 Introduction Multi-word expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). They tend to have a standard syntactic structure but are often semantically non-compositional; i.e. their meaning is not fully determined by their syntactic structure and the meanings of their constituents. A classic example is kick the bucket, which means to die rather than to hit a bucket with the foot. These types of expressions account for a large proportion of day-to-day language interactions (Schuler and Joshi, 2011) and present a significant problem for natural language processing systems (Sag et al., 2002). This paper presents a novel unsupervised approach to detecting the compositionality of MWEs, specifically of verb-noun collocations. The idea is that we can recognize compositional phrases by substituting related words for constituent words in the phrase: if the result of a substitution yields a meaningful phrase, its individual constituents are likely to contribute toward the overall meaning of the phrase. Conversely, if a substitution yields a non-sensical phrase, its constituents are likely to con</context>
</contexts>
<marker>Schuler, Joshi, 2011</marker>
<rawString>William Schuler and Aravind K. Joshi. 2011. Treerewriting models of multi-word expressions. In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World, MWE ’11, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Stefan M¨uller</author>
<author>Stephen Roller</author>
</authors>
<title>Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>255--265</pages>
<location>Atlanta, GA.</location>
<marker>Walde, M¨uller, Roller, 2013</marker>
<rawString>Sabine Schulte im Walde, Stefan M¨uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 255–265, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>In Advances in Neural Information Processing Systems 5,</booktitle>
<pages>895--902</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>Sch¨utze, 1993</marker>
<rawString>Hinrich Sch¨utze. 1993. Word space. In Advances in Neural Information Processing Systems 5, pages 895– 902. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In The 28th International Conference on Machine Learning, ICML</booktitle>
<contexts>
<context position="9323" citStr="Socher et al. (2011)" startWordPosition="1472" endWordPosition="1475">ontain pronouns, while there are also several examples containing words that WordNet considers to be adjectives rather than nouns. This problem was mitigated by part-of-speech tagging the dataset. As neighbours for pronouns (which are not included in WordNet), we used the other pronouns present in the dataset. For the remaining words, we included the part-of-speech when looking up the word in WordNet. 3.1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. (2011) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication O worked best in our experiments. Thus, we −−−−→ define the composed vector eat hat as: eat O hat We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula: 1 k sc(−−−−→ eat hat) = (� 2k i=1 </context>
<context position="15541" citStr="Socher et al., 2011" startWordPosition="2485" endWordPosition="2488">or space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have presented a novel unsupervised approach that can be used to detect the compositionality of multi-w</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In The 28th International Conference on Machine Learning, ICML 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL, EACL ’09.</booktitle>
<contexts>
<context position="14828" citStr="Sporleder and Li, 2009" startWordPosition="2373" endWordPosition="2376">ntribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models </context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. 2009. unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of the 12th Conference of the European Chapter of the ACL, EACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="14977" citStr="Turney and Pantel, 2010" startWordPosition="2396" endWordPosition="2400">ution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibil</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: vector space models of semantics. J. Artif. Int. Res., 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>(linear) maps of the impossible: Capturing semantic anomalies in distributional space.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="15247" citStr="Vecchi et al., 2011" startWordPosition="2438" endWordPosition="2441">ecade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human</context>
</contexts>
<marker>Vecchi, Baroni, Zamparelli, 2011</marker>
<rawString>Eva Maria Vecchi, Marco Baroni, and Roberto Zamparelli. 2011. (linear) maps of the impossible: Capturing semantic anomalies in distributional space. In Proceedings of the Workshop on Distributional Semantics and Compositionality, pages 1–9, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>899--906</pages>
<contexts>
<context position="4255" citStr="Venkatapathy and Joshi (2005)" startWordPosition="632" endWordPosition="636">ighbours, which is to say that the greater the average distance between a phrase vector and its substituted phrase vectors, the greater its compositionality. The contribution of this short focused research paper is a novel approach to detecting the compositionality of multi-word expressions that makes full use of the ability of semantic vector space models to calculate distances between words and phrases. Using this unsupervised approach, we achieve state-of-theart performance in a direct comparison with existing supervised methods. 2 Dataset and Vectors The verb-noun collocation dataset from Venkatapathy and Joshi (2005), which consists of 765 verbobject pairs with human compositionality ratings, was used for evaluation. Venkatapathy &amp; Joshi used a support vector machine (SVM) to obtain a Spearman ps correlation of 0.448. They employed a variety of features ranging from frequency to LSAderived similarity measures and used 10% of the dataset as training data with tenfold cross-validation. McCarthy et al. (2007) used the same dataset and expanded on the original approach by adding WordNet and distributional prototypes to the SVM, achieving a ps correlation of 0.454. The distributional vectors for our experiment</context>
<context position="11222" citStr="Venkatapathy and Joshi (2005)" startWordPosition="1791" endWordPosition="1794">Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of → dist(eat O hat, → dist(eat (D hat, �k j=1 → dist(eat O hat, j=1 1429 Dist Neighbour get breath 0.049 find breath 0.051 use breath 0.050 work breath 0.060 hold breath 0.094 run breath 0.079 carry breath 0.076 look breath play breath 0.065 System ps Venkatapathy and Joshi (2005) 0.447 McCarthy et al. (2007) 0.454 AvgDist VSM neighbours-both 0.131 AvgDist VSM neighbours-verb 0.420 AvgDist VSM neighbours-noun 0.245 AvgDist WN-ranked neighbours-both 0.165 AvgDist WN-ranked neighbours-verb 0.461 AvgDist WN-ranked neighbours-noun 0.169 0.071 buy breath 0.100 Table 3: Spearman ps results AvgDist 0.069 Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have litt</context>
<context position="12495" citStr="Venkatapathy and Joshi (2005)" startWordPosition="1979" endWordPosition="1982"> distance upon substitution. Conversely, substituting the verb in the compositional phrase yields meaningful vectors, putting them in locations in semantic vector space which are sufficiently far apart to distinguish them from the non-compositional cases. 4 Results Results are given for the two methods of obtaining neighbours: via frequency-ranked WordNet neighbours and via vector space neighbours. The compositionality score was computed by using only the verb, only the noun, or both constituent neighbours in the substituted phrase vectors. The results are compared with the scores reported in Venkatapathy and Joshi (2005) and McCarthy et al. (2007), which were achieved using SVMs with a wide variety of features. Values of 1 G k G 20 were tried. If a phrase has fewer than k neighbours because not enough neighbours have been found to cooccur with the other constituent, we use all of them. The results for k = 20 are reported here because that gave the best overall score. The dataset has an inter-annotator agreement of Kendall’s r of 0.61 and a Spearman ps of 0.71 and all reported differences in values are highly significant. Table 3 gives the results. Note that, even though the current approach is unsupervised (i</context>
<context position="15173" citStr="Venkatapathy and Joshi, 2005" startWordPosition="2427" endWordPosition="2430">act of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by cons</context>
</contexts>
<marker>Venkatapathy, Joshi, 2005</marker>
<rawString>Sriram Venkatapathy and Aravind K. Joshi. 2005. Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 899–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Second AAAI Symposium on Quantum Interaction,</booktitle>
<location>Oxford.</location>
<contexts>
<context position="15442" citStr="Widdows, 2008" startWordPosition="2470" endWordPosition="2471"> Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human compositionality ratings, we show that existing compositional models allow us to detect compositionality of multi-word expressions in a straightforward and intuitive manner. 6 Conclusion We have</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Second AAAI Symposium on Quantum Interaction, Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
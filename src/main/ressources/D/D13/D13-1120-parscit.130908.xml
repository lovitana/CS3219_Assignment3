<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.948512">
Ubertagging: Joint segmentation and supertagging for English
</title>
<author confidence="0.974409">
Rebecca Dridan
</author>
<affiliation confidence="0.7835515">
Institutt for Informatikk
Universitetet i Oslo
</affiliation>
<email confidence="0.984011">
rdridan@ifi.uio.no
</email>
<sectionHeader confidence="0.993511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999553923076923">
A precise syntacto-semantic analysis of En-
glish requires a large detailed lexicon with the
possibility of treating multiple tokens as a sin-
gle meaning-bearing unit, a word-with-spaces.
However parsing with such a lexicon, as in-
cluded in the English Resource Grammar, can
be very slow. We show that we can apply
supertagging techniques over an ambiguous
token lattice without resorting to previously
used heuristics, a process we call ubertagging.
Our model achieves an ubertagging accuracy
that can lead to a four to eight fold speed up
while improving parser accuracy.
</bodyText>
<sectionHeader confidence="0.985938" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999938833333334">
Over the last decade or so, supertagging has become
a standard method for increasing parser efficiency
for heavily lexicalised grammar formalisms such as
LTAG (Bangalore and Joshi, 1999), CCG (Clark and
Curran, 2007) and HPSG (Matsuzaki et al., 2007).
In each of these systems, fine-grained lexical cate-
gories, known as supertags, are used to prune the
parser search space prior to full syntactic parsing,
leading to faster parsing at the risk of removing nec-
essary lexical items. Various methods are used to
configure the degree of pruning in order to balance
this trade-off.
The English Resource Grammar (ERG;
Flickinger (2000)) is a large hand-written HPSG-
based grammar of English that produces fine-
grained syntacto-semantic analyses. Given the high
level of lexical ambiguity in its lexicon, parsing
with the ERG should therefore also benefit from
supertagging, but while various attempts have
shown possibilities (Blunsom, 2007; Dridan et
al., 2008; Dridan, 2009), supertagging is still not
a standard element in the ERG parsing pipeline.
There are two main reasons for this. The first is
that the ERG lexicon does not assign simple atomic
categories to words, but instead builds complex
structured signs from information about lemmas and
lexical rules, and hence the shape and integration
of the supertags is not straightforward. Bangalore
and Joshi (2010) define a supertag as a primitive
structure that contains all the information about
a lexical item, including argument structure, and
where the arguments should be found. Within
the ERG, that information is not all contained in
the lexicon, but comes from different places. The
choice, therefore, of what information may be
predicted prior to parsing and how it should be
integrated into parsing is an open question.
The second reason that supertagging is not stan-
dard with ERG processing is one that is rarely con-
sidered when processing English, namely ambigu-
ous segmentation. In most mainstream English pars-
ing, the segmentation of parser input into tokens that
will become the leaves of the parse tree is consid-
ered a fixed, unambiguous process. While recent
work (Dridan and Oepen, 2012) has shown that pro-
ducing even these tokens is not a solved problem,
the issue we focus on here is the ambiguous map-
ping from these tokens to meaning-bearing units that
we might call words. Within the ERG lexicon are
many multi-token lexical entries that are sometimes
referred to as words-with-spaces. These multi-token
entries are added to the lexicon where the grammar-
ian finds that the semantics of a fixed expression is
non-compositional and has the distributional prop-
erties of other single word entries. Some examples
include an adverb-like all of a sudden, a preposition-
like for example and an adjective-like over and done
with. Each of these entries create an segmentation
ambiguity between treating the whole expression as
a single unit, or allowing analyses comprising en-
</bodyText>
<page confidence="0.908273">
1201
</page>
<note confidence="0.7290785">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1201–1212,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999855">
tries triggered by the individual tokens. Previous su-
pertagging research using the ERG has either used
the gold standard tokenisation, hence making the
task artificially easier, or else tagged the individual
tokens, using various heuristics to apply multi-token
tags to single tokens. Neither approach has been
wholly satisfactory.
In this work we avoid the heuristic approaches
and learn a sequential classification model that can
simultaneously determine the most likely segmen-
tation and supertag sequences, a process we dub
ubertagging. We also experiment with more fine-
grained tag sets than have been previously used, and
find that it is possible to achieve a level of ubertag-
ging accuracy that can improve both parser speed
and accuracy for a precise semantic parser.
</bodyText>
<sectionHeader confidence="0.997215" genericHeader="related work">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999975310344827">
As stated above, supertagging has become a stan-
dard tool for particular parsing paradigms, but the
definitions of a supertag, the methods used to learn
them, and the way they are used in parsing varies
across formalisms. The original supertags were 300
LTAG elementary trees, predicted using a fairly sim-
ple trigram tagger that provided a configurable num-
ber of tags per token, since the tagger was not ac-
curate enough to make assigning a single tree vi-
able parser input (Bangalore and Joshi, 1999). The
C&amp;C CCG parser uses a more complex Maximum
Entropy tagger to assign tags from a set of 425 CCG
lexical categories (Clark and Curran, 2007). They
also found it necessary to supply more than one tag
per token, and hence assign all tags that have a prob-
ability within a percentage Q of the most likely tag
for each token. Their standard parser configura-
tion uses a very restrictive Q value initially, relax-
ing it when no parse can be found. Matsuzaki et al.
(2007) use a supertagger similar to the C&amp;C tagger
alongside a CFG filter to improve the speed of their
HPSG parser, feeding sequences of single tags to the
parser until a parse is possible. As in the ERG, cate-
gory and inflectional information are separate in the
automatically-extracted ENJU grammar: their su-
pertag set consists of 1361 tags constructed by com-
bining lexical categories and lexical rules. Figure 1
shows examples of supertags from these three tag
sets, all describing the simple transitive use of lends.
</bodyText>
<figure confidence="0.9753374">
S
(S[dcl]\NP)/NP
(b) CCG
[NP.nom&lt;V.bse&gt;NP.acc]-singular3rd verb rule
(c) ENJU HPSG
</figure>
<figureCaption confidence="0.916637">
Figure 1: Examples of supertags from LTAG, CCG
and ENJU HPSG, for the word lends.
</figureCaption>
<bodyText confidence="0.999638666666666">
The ALPINO system for parsing Dutch is the
closest in spirit to our ERG parsing setup, since
it also uses a hand-written HPSG-based grammar,
including multi-token entries in its lexicon. Prins
and van Noord (2003) use a trigram HMM tagger
to calculate the likelihood of up to 2392 supertags,
and discard those that are not within T of the most
likely tag. For their multi-token entries, they as-
sign a constructed category to each token, so that
instead of assigning preposition to the expres-
sion met betrekking tot (“with respect to”), they
use (1,preposition), (2,preposition),
(3,preposition). Without these constructed
categories, they would only have 1365 supertags.
Most previous supertagging attempts with the
ERG have used the grammar’s lexical types, which
describe the coarse-grained part of speech, and the
subcategorisation of a word, but not the inflection.
Hence both lends and lent have a possible lexical
type v np* pp* to le, which indicates a verb,
with optional noun phrase and prepositional phrase
arguments, where the preposition has the form to.
The number of lexical types changes as the gram-
mar grows, and is currently just over 1000. Dridan
(2009) and Fares (2013) experimented with other tag
types, but both found lexical types to be the opti-
mal balance between predictability and efficiency.
Both used a multi-tagging approach dubbed selec-
tive tagging to integrate the supertags into the parser.
This involved only applying the supertag filter when
the tag probability is above a configurable threshold,
and not pruning otherwise.
For multi-token entries, both Blunsom (2007) and
</bodyText>
<figure confidence="0.675266578947368">
NP0J, VP
V
lends
(a) LTAG
NP11
1202
aj - i le
Foreign
v nger-tr dlr
v prp olr
v np*-pp* to le
lending
v pst olr
v - unacc le
increased
adverb adverb adverb
adverb ditto ditto
1,adverb 2,adverb 3,adverb
all in all
</figure>
<figureCaption confidence="0.9912065">
Figure 2: Options for tagging parts of the multi-
token adverb all in all separately.
</figureCaption>
<bodyText confidence="0.988948627906977">
Dridan (2009) assigned separate tags to each token,
with Blunsom (2007) assigning a special ditto tag
all but the initial token of a multi-token entry, while
Dridan (2009) just assigned the same tag to each to-
ken (leading to example in the expression for exam-
ple receiving p np i le, a preposition-type cate-
gory). Both of these solutions (demonstrated in Fig-
ure 2), as well as that of Prins and van Noord (2003),
in some ways defeat one of the purposes of treating
these expressions as fixed units. The grammarian,
by assigning the same category to, for example, all
of a sudden and suddenly, is declaring that these two
expressions have the same distributional properties,
the properties that a sequential classifier is trying to
exploit. Separating the tokens loses that informa-
tion, and introduces extra noise into the sequence
model.
Ytrestøl (2012) and Fares (2013) treat the multi-
entry tokens as single expressions for tagging, but
with no ambiguity. Ytrestøl (2012) manages this
by using gold standard tokenisation, which is, as he
states, the standard practice for statistical parsing,
but is an artificially simplified setup. Fares (2013) is
the only work we know about that has tried to predict
the final segmentation that the ERG produces. We
compare segmentation accuracy between our joint
model and his stand-alone tokeniser in Section 6.
Looking at other instances of joint segmentation
and tagging leads to work in non-whitespace sepa-
rated languages such as Chinese (Zhang and Clark,
2010) and Japanese (Kudo et al., 2004). While at a
high level, this work is solving the same problem,
the shape of the problems are quite different from
a data point of view. Regular joint morphological
analysis and segmentation has much greater ambi-
guity in terms of possible segmentations but, in most
cases, less ambiguity in terms of labelling than our
situation. This also holds for other lemmatisation
and morphological research, such as Toutanova and
Cherry (2009). While we drew inspiration from this
w period plr p vp i le w period plr
av - s-vp-po le as av - dg-v le
as well. well.
</bodyText>
<figureCaption confidence="0.971501">
Figure 3: A selection from the 70 lexitems instanti-
</figureCaption>
<subsubsectionHeader confidence="0.434786">
ated for Foreign lending increased as well.
</subsubsectionHeader>
<bodyText confidence="0.9980594">
related area, as well as from the speech recognition
field, differences in the relative frequency of obser-
vations and labels, as well as in segmentation ambi-
guity mean that conclusions found in these areas did
not always hold true in our problem space.
</bodyText>
<sectionHeader confidence="0.987227" genericHeader="method">
3 The Parser
</sectionHeader>
<bodyText confidence="0.999853444444444">
The parsing environment we work with is the PET
parser (Callmeier, 2000), a unification-based chart
parser that has been engineered for efficiency with
precision grammars, and incorporates subsumption-
based ambiguity packing (Oepen and Carroll, 2000)
and statistical model driven selective unpacking
(Zhang et al., 2007). Parsing in PET is divided in
two stages. The first stage, lexical parsing, covers
everything from tokenising the raw input string to
populating the base of the parse chart with the ap-
propriate lexical items, ready for the second — syn-
tactic parsing — stage. In this work, we embed our
ubertagging model between the two stages. By this
point, the input has been segmented into what we
call internal tokens, which broadly means
splitting at whitespace and hyphens, and making
’s a separate token. These tokens are subject to a
morphological analysis component which proposes
possible inflectional and derivational rules based on
word form, and then are used in retrieving possible
lexical entries from the lexicon. The results of ap-
plying the appropriate lexical rules, plus affixation
rules triggered by punctuation, to the lexical entries
form a lexical item object, that for this work we dub
a lexitem.
Figure 3 shows some examples of lexitems
instantiated after the lexical parsing stage when
</bodyText>
<page confidence="0.903681">
1203
</page>
<bodyText confidence="0.999628428571428">
analysing Foreign lending increased as well. The
pre-terminal labels on these subtrees are the lexical
types that have previously been used as supertags
for the ERG. For uninflected words, with no punctu-
ation affixed, the lexical type is the only element in
the lexitem, other than the word form (e.g. Foreign,
as). In this example, we also see lexitems with in-
flectional rules (v prp olr, v pst olr), deriva-
tional rules (v nger-tr dlr) and punctuation af-
fixation rules (w period plr).
These lexitems are put in to a chart, forming a
lexical lattice, and it is over this lattice that we apply
our ubertagging model, removing unlikely lexitems
before they are seen by the syntactic parsing stage.
</bodyText>
<sectionHeader confidence="0.988646" genericHeader="method">
4 The Data
</sectionHeader>
<bodyText confidence="0.9999816">
The primary data sets we use in these experiments
are from the 1.0 version of DeepBank (Flickinger et
al., 2012), an HPSG annotation of the Wall Street
Journal text used for the Penn Treebank (PTB; Mar-
cus et al. (1993)). The current version has gold stan-
dard annotations for approximately 85% of the first
22 sections. We follow the recommendations of the
DeepBank developers in using Sections 00–19 for
training, Section 20 (WSJ20) for development and
Section 21 (WSJ21) as test data.
In addition, we use two further sources of training
data: the training portions of the LinGO Redwoods
Treebank (Oepen et al., 2004), a steadily growing
collection of gold standard HPSG annotations in a
variety of domains; and the Wall Street Journal sec-
tion of the North American News Corpus (NANC),
which has been parsed, but not manually annotated.
This builds on observations by Prins and van Noord
(2003), Dridan (2009) and Ytrestøl (2012) that even
uncorrected parser output makes very good train-
ing data for a supertagger, since the constraints in
the parser lead to viable, if not entirely correct se-
quences. This allows us to use much larger training
sets than would be possible if we required manually
annotated data.
In final testing, we also include two further data
sets to observe how domain affects the contribution
of the ubertagging. These are both from the test
portion of the Redwoods Treebank: CatB, an es-
say about open-source software;1 and WeScience13,
</bodyText>
<footnote confidence="0.904906">
1http://catb.org/esr/writings/
</footnote>
<bodyText confidence="0.999834846153846">
text from Wikipedia articles about Natural Language
Processing from the WeScience project (Ytrestøl et
al., 2009). Table 1 summarises the vital statistics of
the data we use.
With the focus on multi-token lexitems, it is in-
structive to see just how frequent they are. In terms
of type frequency, almost 10% of the approximately
38500 lexical entries in the current ERG lexicon
have more than one token in their canonical form.2
However, while this is a significant percentage of the
lexicon, they do not account for the same percentage
of tokens during parsing. An analysis of WSJ00:19
shows that approximately one third of the sentences
had at least one multi-token lexitem in the unpruned
lexical lattice, and in just under half of those, the
gold standard analysis included a multi-word entry.
That gives the multi-token lexitems the awkward
property of being rare enough to be difficult for a
statistical classifier to accurately detect (just under
1% of the leaves of gold parse trees contain multi-
ple tokens), but too frequent to ignore. In addition,
since these multi-token expressions have often been
distinguished because they are non-compositional,
failing to detect the multi-word usage can lead to
a disproportionately adverse effect on the semantic
analysis of the text.
</bodyText>
<sectionHeader confidence="0.994814" genericHeader="method">
5 Ubertagging Model
</sectionHeader>
<bodyText confidence="0.99037305">
Our ubertagging model is very similar to a standard
trigram Hidden Markov Model (HMM), except that
the states are not all of the same length. Our states
are based on the lexitems in the lexical lattice pro-
duced by the lexical parsing stage of PET, and as
such, can be partially overlapping. We formalise this
be defining each state by its start position, end po-
sition, and tag. This turns out to make our model
equivalent to a type of Hidden semi-Markov Model
called a segmental HMM in Murphy (2002). In a
segmental HMM, the states are segments with a tag
(t) and a length in frames (l). In our setup, the
frames are the ERG internal tokens and the segments
are the lexitems, which are the potential candidates
cathedral-bazaar/ by Eric S. Raymond
2While the parser has mechanisms for handling words un-
known to the lexicon, with the current grammar these mecha-
nisms will never propose a multi-token lexitem, and so only the
multi-token entries explicitly in the lexicon will be recognised
as such.
</bodyText>
<page confidence="0.97728">
1204
</page>
<table confidence="0.998862555555556">
Lexitems
Data Set Source Use Gold? Trees All M-T
WSJ00:19 DeepBank 1.0 §00–19 train yes 33783 661451 6309
Redwoods Redwoods Treebank train yes 39478 432873 6568
NANC LDC2008T15 train no 2185323 42376523 399936
WSJ20 DeepBank 1.0 §20 dev yes 1721 34063 312
WSJ21 DeepBank 1.0 §21 test yes 1414 27515 253
WeScience13 Redwoods Treebank test yes 802 11844 153
CatB Redwoods Treebank test yes 608 11653 115
</table>
<tableCaption confidence="0.999684">
Table 1: Test, development and training data used in these experiments. The final two columns show the
</tableCaption>
<bodyText confidence="0.947994615384615">
total number of lexitems used for training (All), as well as how many of those were multi-token lexitems
(M-T).
to become leaves of the parse tree. As indicated
above, the majority of segments (over 99%) will be
one frame long, but segments of up to four frames
are regularly seen in the training data.
A standard trigram HMM has a transition proba-
bility matrix A, where the elements Aijk represent
the probability P(k|ij), and an emission probability
matrix B whose elements Bjo record the probabili-
ties P(o|j). Given these matrices and a vector of ob-
served frames, O, the posterior probabilities of each
state at frame v are calculated as:3
</bodyText>
<equation confidence="0.991547">
αv(qy)βv(qy)
P(qv = qy|O) = (1)
P (O)
</equation>
<bodyText confidence="0.999921">
where αv(qy) is the forward probability at frame v,
given a current state qy (i.e. the probability of the
observation up to v, given the state):
</bodyText>
<equation confidence="0.969698">
αv(qy) = P(O0:v|qv = qy) (2)
X= αv(qxqy) (3)
qx
Xαv(qxqy) = Bqyov αv−1(qwqx)Aqwqxqy (4)
qw
</equation>
<bodyText confidence="0.94663">
βv(qy) is the backwards probability at frame v, given
a current state qy (the probability of the observation
</bodyText>
<footnote confidence="0.661692333333333">
3Since we will require per-state probabilities for integration
to the parser, we focus on the calculation of posterior probabil-
ities, rather than determing the single best path.
</footnote>
<equation confidence="0.929296166666667">
from v, given the state):
βv(qy) = P(Ov+1:V |qv = qy) (5)
X= βv(qxqy) (6)
qx
βv(qxqy) = X βv+1(qyqz)AqxqyqzBqzov+1 (7)
qz
</equation>
<bodyText confidence="0.99947425">
and the probability of the full observation sequence
is equal to the forward probability at the end of the
sequence, or the backwards probability at the start
of the sequence:
</bodyText>
<equation confidence="0.999691">
P(O) = αV ((£)) = β0((S)) (8)
</equation>
<bodyText confidence="0.993570142857143">
In implementation, our model varies only in what
we consider the previous or next states. While v still
indexes frames, qv now indicates a state that ends
with frame v, and we look forwards and backwards
to adjacent states, not frames, formally designated in
terms of l, the length of the state. Hence, we modify
equation (4):
</bodyText>
<equation confidence="0.882539333333333">
Xαv(qxqy) = BqyOv−l+1:v αv−l(qwqx)Aqwqxqy
qw
(9)
</equation>
<bodyText confidence="0.999447">
where v−l indexes the frame before the current state
starts, and hence we are summing over all states
that lead directly to our current state. An equivalent
modification to equation (7) gives:
</bodyText>
<equation confidence="0.969379">
X X βv+l(qyqz)AqxqyqzBqzOv+1:v+l
βv(qxqy) = l(qz) (10)
</equation>
<page confidence="0.740106333333333">
qz
EQ�
1205
</page>
<figure confidence="0.71717525">
w period plr
v pas odlr
v np-pp* to le
recommended.
</figure>
<figureCaption confidence="0.96441">
Figure 4: Possible tag types and their tag set size, with examples derived from the lexitem on the right.
</figureCaption>
<figure confidence="0.560053">
LTYPE v np-pp* to le 1028
INFL v np-pp* to le:v pas odlr 3626
FULL v np-pp* to le:v pas odlr:w period plr 21866
Type Example #Tags
</figure>
<bodyText confidence="0.9998878">
where Q,,, is the set of states that start at v + 1 (i.e.,
the states immediately following the current state),
and l(qz) is the length of state qz.
We construct the transition and emission prob-
ability matrices using relative frequencies directly
observed from the training data, where we make
the simplifying assumption that P(qk|qiqj) �
P(t(qk)|t(qi)t(qk)). Which is to say, while lex-
items with the same tag, but different length will
trigger distinct states with distinct emission proba-
bilities, they will have the same transition probabili-
ties, given the same proceeding tag.4 Even with our
large training set, some tag trigrams are rare or un-
seen. To smooth these probabilities, we use deleted
interpolation to calculate a weighted sum of the tri-
gram, bigram and unigram probabilities, since it has
been successfully used in effective PoS taggers like
the TnT tagger (Brants, 2000). Future work will
look more closely at the effects of different smooth-
ing methods.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="method">
6 Intrinsic Ubertag Evaluation
</sectionHeader>
<bodyText confidence="0.999934307692308">
In order to develop and tune the ubertagging model,
we first looked at segmentation and tagging per-
formance in isolation over the development set.
We looked at three tag granularities: lexical types
(LTYPE) which have previously been shown to be the
optimal granularity for supertagging with the ERG,
inflected types (INFL) which encompass inflectional
and derivational rules applied to the lexical type, and
the full lexical item (FULL), which also includes af-
fixation rules used for punctuation handling. Exam-
ples of each tag type are shown in Figure 4, along
with the number of tags of each type seen in the
training data.
</bodyText>
<footnote confidence="0.89238525">
4Since the multi-token lexical entries are defined because
they have the same properties as the single token variants, there
is no reason to think the length of a state should influence the
tag sequence probability.
</footnote>
<table confidence="0.959943">
Segmentation Tagging
Tag Type F1 Sent. F1 Sent.
FULL 99.55 94.48 93.92 42.13
INFL 99.45 93.55 93.74 41.49
LTYPE 99.40 93.03 93.27 38.12
</table>
<tableCaption confidence="0.994379">
Table 2: Segmentation and tagging performance of
</tableCaption>
<bodyText confidence="0.988202481481481">
the best path found for each model, measured per
segment in terms of F1, and also as complete sen-
tence accuracy.
Single sequence results Table 2 shows the results
when considering the best path through the lattice.
In terms of segmentation, our sentence accuracy is
comparable to that of the stand-alone segmentation
performance reported by Fares et al. (2013) over
similar data.5 In that work, the authors used a bi-
nary CRF classifier to label points between objects
they called micro-tokens as either SPLIT or NOS-
PLIT. The CRF classifier used a less informed in-
put (since it was external to the parser), but a much
more complex model, to produce a best single path
sentence accuracy of 94.06%. Encouragingly, this
level of segmentation performance was shown in
later work to produce a viable parser input (Fares,
2013).
Switching to the tagging results, we see that the
F1 numbers are quite good for tag sets of this size.6
The best tag accuracy seen for ERG LTYPE-style
tags was 95.55 in Ytrestøl (2012), using gold stan-
dard segmentation on a different data set. Dridan
(2009) experimented with a tag granularity similar
to our INFL (letype+morph) and saw a tag ac-
curacy of 91.51, but with much less training data.
From other formalisms, Kummerfeld et al. (2010)
</bodyText>
<footnote confidence="0.9191672">
5Fares et al. (2013) used a different section of an earlier ver-
sion of DeepBank, but with the same style of annotation.
6We need to measure F, rather than tag accuracy here, since
the number of tokens tagged will vary according to the segmen-
tation.
</footnote>
<page confidence="0.992606">
1206
</page>
<bodyText confidence="0.999835733333333">
report a single tag accuracy of 95.91, with the
smaller CCG supertag set. Despite the promising
tag F1 numbers however, the sentence level accu-
racy still indicates a performance level unacceptable
for parser input. Comparing between tag types, we
see that, possibly surprisingly, the more fine-grained
tags are more accurately assigned, although the dif-
ferences are small. While instinctively a larger tag
set should present a more difficult problem, we find
that this is mitigated both by the sparse lexical lattice
provided by the parser, and by the extra constraints
provided by the more informative tags.
Multi-tagging results The multi-tagging methods
from previous supertagging work becomes more
complicated when dealing with ambiguous tokeni-
sation. Where, in other setups, one can compare
tag probabilities for all tags for a particular token,
that no longer holds directly when tokens can par-
tially overlap. Since ultimately, the parser uses lex-
items which encompass segmentation and tagging
information, we decided to use a simple integration
method, where we remove any lexitem which our
model assigns a probability below a certain thresh-
old (p). The effect of the different tag granular-
ities is now mediated by the relationship between
the states in the ubertagging lattice and the lexitems
in the parser’s lattice: for the FULL model, this is
a one-to-one relationship, but states from the mod-
els that use coarser-grained tags may affect multiple
lexitems. To illustrate this point, Figure 5 shows
some lexitems for the token forecast,, where there
are multiple possible analyses for the comma. A
FULL tag of v cp le:v pst olr:w comma plr
will select only lexitem (b), whereas an INFL tag
v cp le:v pst olr will select (b) and (c) and
the LTYPE tag v cp le picks out (a), (b) and (c).
On the other hand, where there is no ambiguity in
inflection or affixation, an LTYPE tag of n - mc le
may relate to only a single lexitem ((f) in this case).
Since we are using an absolute, rather than rel-
ative, threshold, the number needs to be tuned for
each model7 and comparisons between models can
only be made based on the effects (accuracy or prun-
ing power) of the threshold. Table 3 shows how
a selection of threshold values affect the accuracy
</bodyText>
<footnote confidence="0.8914795">
7A tag set size of 1028 will lead to higher probabilities in
general than a tag set size of 21866.
</footnote>
<figureCaption confidence="0.977964">
Figure 5: Some of the lexitems triggered by fore-
cast, in Despite the gloomy forecast, profits were up.
</figureCaption>
<table confidence="0.999880857142857">
Tag p Acc. Lexitems
Type Kept Ave.
FULL 0.00001 99.71 41.6 3.34
FULL 0.0001 99.44 33.1 2.66
FULL 0.001 98.92 25.5 2.05
FULL 0.01 97.75 19.4 1.56
INFL 0.0001 99.67 37.9 3.04
INFL 0.001 99.25 29.0 2.33
INFL 0.01 98.21 21.6 1.73
INFL 0.02 97.68 19.7 1.58
LTYPE 0.0002 99.75 66.3 5.33
LTYPE 0.002 99.43 55.0 4.42
LTYPE 0.02 98.41 43.5 3.50
LTYPE 0.05 97.54 39.4 3.17
</table>
<tableCaption confidence="0.998161">
Table 3: Accuracy and ambiguity after pruning lex-
</tableCaption>
<bodyText confidence="0.971108333333333">
items in WSJ20, at a selection of thresholds p for
each model. Accuracy is measured as the percent-
age of gold lexitems remaining after pruning, while
ambiguity is presented both as a percentage of lex-
items kept, and the average number of lexitems per
initial token still remaining.
</bodyText>
<figure confidence="0.9914155">
w comma-nf plr
vcple
forecast,
(a)
w comma-nf plr
v pst olr
vcple
forecast,
(b)
w comma plr
v pst olr
vcple
forecast,
(c)
w comma plr
v pst olr
v np le
forecast,
(d)
w comma plr
v pas olr
vnple
forecast,
(e)
w comma plr
n - mc le
forecast,
(f)
1207
Tag accuracy versus ambiguity
1 2 3 4 5 6 7 8 9
Average lexitems per initial token
</figure>
<figureCaption confidence="0.999768">
Figure 6: Accuracy over gold lexitems versus aver-
</figureCaption>
<bodyText confidence="0.990586740740741">
age lexitems per initial token over the development
set, for each of the different ubertagging models.
and pruning impact of our different disambiguation
models, where the accuracy is measured in terms
of percentage of gold lexitems retained. The prun-
ing effect is given both as percentage of lexitems
retained after pruning, and average number of lex-
items per initial token.8 Comparison between the
different models can be more easily made by exam-
ining Figure 6. Here we see clearly that the LTYPE
model provides much less pruning for any given
level of lexitem accuracy, while the performance of
the other models is almost indistinguishable.
Analysis The current state-of-the-art POS tagging
accuracy (using the 45 tags in the PTB) is approx-
imately 97.5%. The most restrictive p value we
report for each model was selected to demonstrate
that level of accuracy, which we can see would lead
to pruning over 80% of lexitems when using FULL
tags, an average of 1.56 tags per token. While
this level of accuracy has been sufficient for statisti-
cal treebank parsing, previous work (Dridan, 2009)
has shown that tag accuracy cannot directly predict
parser performance, since errors of different types
can have very different effects. This is hard to
quantify without parsing, but we made a qualitative
analysis at the lexitems that were incorrectly being
</bodyText>
<footnote confidence="0.9432775">
8The average number of lexitems per token for the unre-
stricted parser is 8.03, although the actual assignment is far from
uniform, with up to 70 lexitems per token seen for the very am-
biguous tokens.
</footnote>
<bodyText confidence="0.999817277777778">
pruned. For all models, the most difficult lexitems
to get correct were proper nouns, particular those
that are also used as common nouns (e.g. Bank, Air-
line, Report). While capitalisation provides a clue
here, it is not always deterministic, particularly since
the treebank incorporates detailed decisions regard-
ing the distinction between a name and a capitalised
common noun that require real world knowledge,
and are not necessarily always consistent. Almost
two thirds of the errors made by the FULL and INFL
models are related to these decisions, but only about
40% for the LTYPE model. The other errors are pre-
dominately over noun and verb type lexitems, as the
open classes, with the only difference between mod-
els being that the FULL model seems marginally bet-
ter at classifying verbs. The next section describes
the end-to-end setup and results when parsing the
development set.
</bodyText>
<sectionHeader confidence="0.981035" genericHeader="method">
7 Parsing
</sectionHeader>
<bodyText confidence="0.999983518518518">
With encouraging ubertagging results, we now take
the next step and evaluate the effect on end-to-end
parsing. Apart from the issue of different error types
having unpredictable effects, there are two other
factors that make the isolated ubertagging results
only an approximate indication of parsing perfor-
mance. The first confounding factor is the statisti-
cal parsing disambiguation model. To show the ef-
fect of ubertagging in a realistic configuration, we
only evaluate the first analysis that the parser returns.
That means that when the unrestricted parser does
not rank the gold analysis first, errors made by our
model may not be visible, because we would never
see the gold analysis in any case. On the other hand,
it is possible to improve parser accuracy by pruning
incorrect lexitems that were in a top ranked, non-
gold analysis.
The second new factor that parser integration
brings to the picture is the effect of resource limi-
tations. For reasons of tractability, PET is run with
per sentence time and memory limits. For treebank
creation, these limits are quite high (up to four min-
utes), but for these experiments, we set the time-
out to a more practical 60 seconds and the memory
limit to 2048Mb. Without lexical pruning, this leads
to approximately 3% of sentences not receiving an
analysis. Since the main aim of ubertagging is to in-
</bodyText>
<figure confidence="0.9963967">
Accuracy
0.995
0.985
0.975
0.99
0.98
1
FULL
INFL
LTYPE
</figure>
<page confidence="0.950831">
1208
</page>
<table confidence="0.999955666666667">
Tag P F1 Time
Type Lexitem Bracket
No Pruning 94.06 88.58 6.58
FULL 0.00001 95.62 89.84 3.99
FULL 0.0001 95.95 90.09 2.69
FULL 0.001 95.81 89.88 1.34
FULL 0.01 94.19 88.29 0.64
INFL 0.0001 96.10 90.37 3.45
INFL 0.001 96.14 90.33 1.78
INFL 0.01 95.07 89.27 0.84
INFL 0.02 94.32 88.49 0.64
LTYPE 0.0002 95.37 89.63 4.73
LTYPE 0.002 96.03 90.20 2.89
LTYPE 0.02 95.04 89.04 1.23
LTYPE 0.05 93.36 87.26 0.88
</table>
<tableCaption confidence="0.949576">
Table 4: Lexitem and bracket F1over WSJ20, with
average per sentence parsing time in seconds.
</tableCaption>
<bodyText confidence="0.999948482758621">
crease efficiency, we would expect to regain at least
some of these unanalysed sentences, even when a
lexitem needed for the gold analysis has been re-
moved.
Table 4 shows the parsing results at the same
threshold values used in Table 3. Accuracy is cal-
culated in terms of F1 both over lexitems, and PAR-
SEVAL-style labelled brackets (Black et al., 1991),
while efficiency is represented by average parsing
time per sentence. We can see here that an ubertag-
ging F1 of below 98 (cf. Table 3) leads to a drop
in parser accuracy, but that an ubertagging perfor-
mance of between 98 and 99 can improve parser F1
while also achieving speed increases up to 8-fold.
From the table we confirm that, contrary to ear-
lier pipeline supertagging configurations, tags of a
finer granularity than LTYPE can deliver better per-
formance, both in terms of accuracy and efficiency.
Again, comparing graphically in Figure 7 gives a
clearer picture. Here we have graphed labelled
bracket F1 against parsing time for the full range of
threshold values explored, with the unpruned pars-
ing results indicated by a cross.
From this figure, we see that the INFL model, de-
spite being marginally less accurate when measured
in isolation, leads to slightly more accurate parse re-
sults than the FULL model at all levels of efficiency.
Looking at the same graph for different samples
of the development set (not shown) shows some
</bodyText>
<figure confidence="0.975862333333333">
Parser accuracy versus efficiency
0 1 2 3 4 5 6 7
Time per sentence
</figure>
<figureCaption confidence="0.998707">
Figure 7: Labelled bracket F1 versus parsing time
</figureCaption>
<bodyText confidence="0.980051071428572">
per sentence over the development set, for each of
the different ubertagging models. The cross indi-
cates unpruned performance, while the circle pin-
points the configuration we chose for the final test
runs.
variance in which threshold value gives the best F1,
but the relative differences and basic curve shape re-
mains the same. From these different views, using
the guideline of maximum efficiency without harm-
ing accuracy we selected our final configuration: the
INFL model with a threshold value of 0.001 (marked
with a circle in Figure 7). On the development set,
this configuration leads to a 1.75 point improvement
in F1 in 27% of the parsing time.
</bodyText>
<sectionHeader confidence="0.995162" genericHeader="method">
8 Final Results
</sectionHeader>
<bodyText confidence="0.999946733333333">
Table 5 shows the results obtained when parsing us-
ing the configuration selected on the development
set, over our three test sets. The first, WSJ21 is from
the same domain as the development set. Here we
see that the effect over the WSJ21 set fairly closely
mirrored that of the development set, with an F1 in-
crease of 1.81 in 29% of the parsing time.
The Wikipedia domain of our WeScience13 test
set, while very different to the newswire domain of
the development set could still be considered in do-
main for the parsing and ubertagging models, since
there is Wikipedia data in the training sets. With
an average sentence length of 15.18 (compared to
18.86 in WSJ21), the baseline parsing time is faster
than for WSJ21, and the speedup is not quite as large
</bodyText>
<figure confidence="0.989414090909091">
F1
90
89
88
87
86
FULL
INFL
LTYPE
Unrestricted
Selected configuration
</figure>
<page confidence="0.952017">
1209
</page>
<table confidence="0.9817714">
Baseline Pruned
Data Set F1 Time F1 Time
WSJ21 88.12 6.06 89.93 1.77
WeScience13 86.25 4.09 87.14 1.48
CatB 86.31 5.00 87.11 1.78
</table>
<tableCaption confidence="0.979539">
Table 5: Parsing accuracy in terms of labelled
</tableCaption>
<bodyText confidence="0.998822846153846">
bracket F1 and average time per sentence when pars-
ing the test sets, without pruning, and then with lex-
ical pruning using the INFL model with a threshold
of 0.001.
but still welcome, at 36% of the baseline time. The
increase is accuracy is likewise smaller (due to less
issues with resource exhaustion in the baseline), but
as our primary goal is to not harm accuracy, the re-
sults are pleasing.
The CatB test set is the standard out-of-domain
test for the parser, and is also out of domain for
the ubertagging model. The average sentence length
is not much below that of WSJ21, at 18.61, but
the baseline parsing speed is still noticeably faster,
which appears to be a reflection of greater structural
ambiguity in the newswire text. We still achieve a re-
duction in parsing time to 35% of the baseline, again
with a small improvement in accuracy.
The across-the-board performance improvement
on all our test sets suggests that, while tuning the
pruning threshold could help, it is a robust parame-
ter that can provide good performance across a va-
riety of domains. This means that we finally have a
robust supertagging setup for use with the ERG that
doesn’t require heuristic shortcuts and can be reli-
ably applied in general parsing.
</bodyText>
<sectionHeader confidence="0.985524" genericHeader="conclusions">
9 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999989838709678">
In this work we have demonstrated a lexical disam-
biguation process dubbed ubertagging that can as-
sign fine-grained supertags over an ambiguous to-
ken lattice, a setup previously ignored for English. It
is the first completely integrated supertagging setup
for use with the English Resource Grammar, which
avoids the previously necessary heuristics for deal-
ing with ambiguous tokenisation, and can be ro-
bustly configured for improved performance without
loss of accuracy. Indeed, by learning a joint segmen-
tation and supertagging model, we have been able
to achieve usefully high tagging accuracies for very
fine-grained tags, which leads to potential parser
speedups of between 4 and 8 fold.
Analysis of the tagging errors still being made
have suggested some possibly avoidable inconsis-
tencies in the grammar and treebank, which have
been fed back to the developers, hopefully leading
to even better results in the future.
In future work, we will investigate more advanced
smoothing methods to try and boost the ubertagging
accuracy. We also intend to more fully explore the
domain adaptation potentials of the lexical model
that have been seen in other parsing setups (see
Rimell and Clark (2008) for example), as well as ex-
amine the limits on the effects of more training data.
Finally, we would like to explore just how much the
statistic properties of our data dictate the success of
the model by looking at related problems like mor-
phological analysis of unsegmented languages such
as Japanese.
</bodyText>
<sectionHeader confidence="0.994726" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999862714285714">
I am grateful to my colleagues from the Oslo Lan-
guage Technology Group and the DELPH-IN con-
sortium for many discussions on the issues involved
in this work, and particularly to Stephan Oepen who
inspired the initial lattice tagging idea. Thanks also
to three anonymous reviewers for their very con-
structive feedback which improved the final ver-
sion. Large-scale experimentation and engineering
is made possible though access to the TITAN high-
performance computing facilities at the University
of Oslo, and I am grateful to the Scientific Com-
putating staff at UiO, as well as to the Norwegian
Metacenter for Computational Science and the Nor-
wegian tax payer.
</bodyText>
<page confidence="0.983969">
1210
</page>
<sectionHeader confidence="0.995817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827490566038">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237 –265.
Srinavas Bangalore and Aravind Joshi, editors. 2010.
Supertagging: Using Complex Lexical Descriptions in
Natural Language Processing. The MIT Press, Cam-
bridge, US.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark
Liberman, Mitch Marcus, S. Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of the Workshop on
Speech and Natural Language, page 306 – 311, Pacific
Grove, USA.
Philip Blunsom. 2007. Structured Classification for
Multilingual Natural Language Processing. Ph.D.
thesis, Department of Computer Science and Software
Engineering, University of Melbourne.
Thorsten Brants. 2000. TnT — a statistical part-of-
speech tagger. In Proceedings of the Sixth Conference
on Applied Natural Language Processing ANLP-2000,
page 224–231, Seattle, USA.
Ulrich Callmeier. 2000. PET. A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6(1):99–108, March.
Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Meeting of the Asso-
ciation for Computational Linguistics, page 248 – 255,
Prague, Czech Republic.
Rebecca Dridan and Stephan Oepen. 2012. Tokeniza-
tion. Returning to a long solved problem. A survey,
contrastive experiment, recommendations, and toolkit.
In Proceedings of the 50th Meeting of the Association
for Computational Linguistics, page 378–382, Jeju,
Republic of Korea, July.
Rebecca Dridan, Valia Kordoni, and Jeremy Nicholson.
2008. Enhancing performance of lexicalised gram-
mars. page 613 – 621.
Rebecca Dridan. 2009. Using lexical statistics to im-
prove HPSG parsing. Ph.D. thesis, Department of
Computational Linguistics, Saarland University.
Murhaf Fares, Stephan Oepen, and Yi Zhang. 2013. Ma-
chine learning for high-quality tokenization. Replicat-
ing variable tokenization schemes. In Computational
Linguistics and Intelligent Text Processing, page 231–
244. Springer.
Murhaf Fares. 2013. ERG tokenization and lexical cat-
egorization: a sequence labeling approach. Master’s
thesis, Department of Informatics, University of Oslo.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank. A dynamically annotated treebank of the
Wall Street Journal. In Proceedings of the 11th Inter-
national Workshop on Treebanks and Linguistic Theo-
ries, page 85 – 96, Lisbon, Portugal. Edic¸˜oes Colibri.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1):15–28.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, page 230 – 237.
Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-
born, James Haggerty, James R. Curran, and Stephen
Clark. 2010. Faster parsing by supertagger adapta-
tion. In Proceedings of the 48th Meeting of the Asso-
ciation for Computational Linguistics, page 345 – 355,
Uppsala, Sweden.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pora of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and CFG-filtering. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2007), page 1671–1676, Hyderabad, India.
Kevin P. Murphy. 2002. Hidden semi-Markov models
(HSMMs).
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing. Practical results. In
Proceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Linguis-
tics, page 162 –169, Seattle, WA, USA.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG. Re-
search on Language and Computation, 2(4):575 – 596.
Robbert Prins and Gertjan van Noord. 2003. Reinforcing
parser preferences through tagging. Traitement Au-
tomatique des Langues, 44(3):121–139.
Laura Rimell and Stephen Clark. 2008. Adapting
a lexicalized-grammar parser to contrasting domains.
page 475 – 484.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the 47th Meeting of the
Association for Computational Linguistics, page 486 –
494, Singapore.
Gisle Ytrestøl. 2012. Transition-based Parsing for
Large-scale Head-Driven Phrase Structure Gram-
mars. Ph.D. thesis, Department of Informatics, Uni-
versity of Oslo.
</reference>
<page confidence="0.777285">
1211
</page>
<reference confidence="0.9993470625">
Gisle Ytrestøl, Stephan Oepen, and Dan Flickinger.
2009. Extracting and annotating Wikipedia sub-
domains. In Proceedings of the 7th International
Workshop on Treebanks and Linguistic Theories, page
185 –197, Groningen, The Netherlands.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, page 843 – 852, Cambridge, MA,
USA.
Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, page 48 – 59, Prague, Czech Repub-
lic, July.
</reference>
<page confidence="0.993339">
1212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429577">
<title confidence="0.995735">Ubertagging: Joint segmentation and supertagging for English</title>
<author confidence="0.59358">Rebecca</author>
<affiliation confidence="0.6513205">Institutt for Universitetet i</affiliation>
<email confidence="0.808532">rdridan@ifi.uio.no</email>
<abstract confidence="0.998911857142857">A precise syntacto-semantic analysis of English requires a large detailed lexicon with the possibility of treating multiple tokens as a sinmeaning-bearing unit, a However parsing with such a lexicon, as included in the English Resource Grammar, can be very slow. We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<pages>265</pages>
<contexts>
<context position="937" citStr="Bangalore and Joshi, 1999" startWordPosition="135" endWordPosition="138">nit, a word-with-spaces. However parsing with such a lexicon, as included in the English Resource Grammar, can be very slow. We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy. 1 Introduction and Motivation Over the last decade or so, supertagging has become a standard method for increasing parser efficiency for heavily lexicalised grammar formalisms such as LTAG (Bangalore and Joshi, 1999), CCG (Clark and Curran, 2007) and HPSG (Matsuzaki et al., 2007). In each of these systems, fine-grained lexical categories, known as supertags, are used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its</context>
<context position="5178" citStr="Bangalore and Joshi, 1999" startWordPosition="810" endWordPosition="813"> a level of ubertagging accuracy that can improve both parser speed and accuracy for a precise semantic parser. 2 Previous Work As stated above, supertagging has become a standard tool for particular parsing paradigms, but the definitions of a supertag, the methods used to learn them, and the way they are used in parsing varies across formalisms. The original supertags were 300 LTAG elementary trees, predicted using a fairly simple trigram tagger that provided a configurable number of tags per token, since the tagger was not accurate enough to make assigning a single tree viable parser input (Bangalore and Joshi, 1999). The C&amp;C CCG parser uses a more complex Maximum Entropy tagger to assign tags from a set of 425 CCG lexical categories (Clark and Curran, 2007). They also found it necessary to supply more than one tag per token, and hence assign all tags that have a probability within a percentage Q of the most likely tag for each token. Their standard parser configuration uses a very restrictive Q value initially, relaxing it when no parse can be found. Matsuzaki et al. (2007) use a supertagger similar to the C&amp;C tagger alongside a CFG filter to improve the speed of their HPSG parser, feeding sequences of s</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics, 25(2):237 –265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinavas Bangalore</author>
<author>Aravind Joshi</author>
<author>editors</author>
</authors>
<date>2010</date>
<booktitle>Supertagging: Using Complex Lexical Descriptions in Natural Language Processing.</booktitle>
<publisher>The MIT Press,</publisher>
<location>Cambridge, US.</location>
<marker>Bangalore, Joshi, editors, 2010</marker>
<rawString>Srinavas Bangalore and Aravind Joshi, editors. 2010. Supertagging: Using Complex Lexical Descriptions in Natural Language Processing. The MIT Press, Cambridge, US.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra Black</author>
<author>Steve Abney</author>
<author>Dan Flickinger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>Phil Harrison</author>
<author>Don Hindle</author>
<author>Robert Ingria</author>
<author>Fred Jelinek</author>
<author>Judith Klavans</author>
<author>Mark Liberman</author>
<author>Mitch Marcus</author>
<author>S Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the Workshop on Speech and Natural Language, page 306 – 311,</booktitle>
<location>Pacific Grove, USA.</location>
<contexts>
<context position="31250" citStr="Black et al., 1991" startWordPosition="5201" endWordPosition="5204">.78 INFL 0.01 95.07 89.27 0.84 INFL 0.02 94.32 88.49 0.64 LTYPE 0.0002 95.37 89.63 4.73 LTYPE 0.002 96.03 90.20 2.89 LTYPE 0.02 95.04 89.04 1.23 LTYPE 0.05 93.36 87.26 0.88 Table 4: Lexitem and bracket F1over WSJ20, with average per sentence parsing time in seconds. crease efficiency, we would expect to regain at least some of these unanalysed sentences, even when a lexitem needed for the gold analysis has been removed. Table 4 shows the parsing results at the same threshold values used in Table 3. Accuracy is calculated in terms of F1 both over lexitems, and PARSEVAL-style labelled brackets (Black et al., 1991), while efficiency is represented by average parsing time per sentence. We can see here that an ubertagging F1 of below 98 (cf. Table 3) leads to a drop in parser accuracy, but that an ubertagging performance of between 98 and 99 can improve parser F1 while also achieving speed increases up to 8-fold. From the table we confirm that, contrary to earlier pipeline supertagging configurations, tags of a finer granularity than LTYPE can deliver better performance, both in terms of accuracy and efficiency. Again, comparing graphically in Figure 7 gives a clearer picture. Here we have graphed labelle</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra Black, Steve Abney, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Phil Harrison, Don Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitch Marcus, S. Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of the Workshop on Speech and Natural Language, page 306 – 311, Pacific Grove, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Blunsom</author>
</authors>
<title>Structured Classification for Multilingual Natural Language Processing.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science and Software Engineering, University of Melbourne.</institution>
<contexts>
<context position="1683" citStr="Blunsom, 2007" startWordPosition="253" endWordPosition="254"> supertags, are used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG should therefore also benefit from supertagging, but while various attempts have shown possibilities (Blunsom, 2007; Dridan et al., 2008; Dridan, 2009), supertagging is still not a standard element in the ERG parsing pipeline. There are two main reasons for this. The first is that the ERG lexicon does not assign simple atomic categories to words, but instead builds complex structured signs from information about lemmas and lexical rules, and hence the shape and integration of the supertags is not straightforward. Bangalore and Joshi (2010) define a supertag as a primitive structure that contains all the information about a lexical item, including argument structure, and where the arguments should be found.</context>
<context position="7926" citStr="Blunsom (2007)" startWordPosition="1262" endWordPosition="1263">rase and prepositional phrase arguments, where the preposition has the form to. The number of lexical types changes as the grammar grows, and is currently just over 1000. Dridan (2009) and Fares (2013) experimented with other tag types, but both found lexical types to be the optimal balance between predictability and efficiency. Both used a multi-tagging approach dubbed selective tagging to integrate the supertags into the parser. This involved only applying the supertag filter when the tag probability is above a configurable threshold, and not pruning otherwise. For multi-token entries, both Blunsom (2007) and NP0J, VP V lends (a) LTAG NP11 1202 aj - i le Foreign v nger-tr dlr v prp olr v np*-pp* to le lending v pst olr v - unacc le increased adverb adverb adverb adverb ditto ditto 1,adverb 2,adverb 3,adverb all in all Figure 2: Options for tagging parts of the multitoken adverb all in all separately. Dridan (2009) assigned separate tags to each token, with Blunsom (2007) assigning a special ditto tag all but the initial token of a multi-token entry, while Dridan (2009) just assigned the same tag to each token (leading to example in the expression for example receiving p np i le, a preposition-</context>
</contexts>
<marker>Blunsom, 2007</marker>
<rawString>Philip Blunsom. 2007. Structured Classification for Multilingual Natural Language Processing. Ph.D. thesis, Department of Computer Science and Software Engineering, University of Melbourne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT — a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000,</booktitle>
<pages>224--231</pages>
<location>Seattle, USA.</location>
<contexts>
<context position="20354" citStr="Brants, 2000" startWordPosition="3348" endWordPosition="3349"> data, where we make the simplifying assumption that P(qk|qiqj) � P(t(qk)|t(qi)t(qk)). Which is to say, while lexitems with the same tag, but different length will trigger distinct states with distinct emission probabilities, they will have the same transition probabilities, given the same proceeding tag.4 Even with our large training set, some tag trigrams are rare or unseen. To smooth these probabilities, we use deleted interpolation to calculate a weighted sum of the trigram, bigram and unigram probabilities, since it has been successfully used in effective PoS taggers like the TnT tagger (Brants, 2000). Future work will look more closely at the effects of different smoothing methods. 6 Intrinsic Ubertag Evaluation In order to develop and tune the ubertagging model, we first looked at segmentation and tagging performance in isolation over the development set. We looked at three tag granularities: lexical types (LTYPE) which have previously been shown to be the optimal granularity for supertagging with the ERG, inflected types (INFL) which encompass inflectional and derivational rules applied to the lexical type, and the full lexical item (FULL), which also includes affixation rules used for </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT — a statistical part-ofspeech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing ANLP-2000, page 224–231, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET. A platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="10752" citStr="Callmeier, 2000" startWordPosition="1746" endWordPosition="1747">nd morphological research, such as Toutanova and Cherry (2009). While we drew inspiration from this w period plr p vp i le w period plr av - s-vp-po le as av - dg-v le as well. well. Figure 3: A selection from the 70 lexitems instantiated for Foreign lending increased as well. related area, as well as from the speech recognition field, differences in the relative frequency of observations and labels, as well as in segmentation ambiguity mean that conclusions found in these areas did not always hold true in our problem space. 3 The Parser The parsing environment we work with is the PET parser (Callmeier, 2000), a unification-based chart parser that has been engineered for efficiency with precision grammars, and incorporates subsumptionbased ambiguity packing (Oepen and Carroll, 2000) and statistical model driven selective unpacking (Zhang et al., 2007). Parsing in PET is divided in two stages. The first stage, lexical parsing, covers everything from tokenising the raw input string to populating the base of the parse chart with the appropriate lexical items, ready for the second — syntactic parsing — stage. In this work, we embed our ubertagging model between the two stages. By this point, the input</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Ulrich Callmeier. 2000. PET. A platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6(1):99–108, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics, page 248 – 255,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="967" citStr="Clark and Curran, 2007" startWordPosition="140" endWordPosition="143">parsing with such a lexicon, as included in the English Resource Grammar, can be very slow. We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy. 1 Introduction and Motivation Over the last decade or so, supertagging has become a standard method for increasing parser efficiency for heavily lexicalised grammar formalisms such as LTAG (Bangalore and Joshi, 1999), CCG (Clark and Curran, 2007) and HPSG (Matsuzaki et al., 2007). In each of these systems, fine-grained lexical categories, known as supertags, are used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG</context>
<context position="5322" citStr="Clark and Curran, 2007" startWordPosition="836" endWordPosition="839">pertagging has become a standard tool for particular parsing paradigms, but the definitions of a supertag, the methods used to learn them, and the way they are used in parsing varies across formalisms. The original supertags were 300 LTAG elementary trees, predicted using a fairly simple trigram tagger that provided a configurable number of tags per token, since the tagger was not accurate enough to make assigning a single tree viable parser input (Bangalore and Joshi, 1999). The C&amp;C CCG parser uses a more complex Maximum Entropy tagger to assign tags from a set of 425 CCG lexical categories (Clark and Curran, 2007). They also found it necessary to supply more than one tag per token, and hence assign all tags that have a probability within a percentage Q of the most likely tag for each token. Their standard parser configuration uses a very restrictive Q value initially, relaxing it when no parse can be found. Matsuzaki et al. (2007) use a supertagger similar to the C&amp;C tagger alongside a CFG filter to improve the speed of their HPSG parser, feeding sequences of single tags to the parser until a parse is possible. As in the ERG, category and inflectional information are separate in the automatically-extra</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Meeting of the Association for Computational Linguistics, page 248 – 255, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
<author>Stephan Oepen</author>
</authors>
<title>Tokenization. Returning to a long solved problem. A survey, contrastive experiment, recommendations, and toolkit.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>378--382</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="2904" citStr="Dridan and Oepen, 2012" startWordPosition="449" endWordPosition="452">d. Within the ERG, that information is not all contained in the lexicon, but comes from different places. The choice, therefore, of what information may be predicted prior to parsing and how it should be integrated into parsing is an open question. The second reason that supertagging is not standard with ERG processing is one that is rarely considered when processing English, namely ambiguous segmentation. In most mainstream English parsing, the segmentation of parser input into tokens that will become the leaves of the parse tree is considered a fixed, unambiguous process. While recent work (Dridan and Oepen, 2012) has shown that producing even these tokens is not a solved problem, the issue we focus on here is the ambiguous mapping from these tokens to meaning-bearing units that we might call words. Within the ERG lexicon are many multi-token lexical entries that are sometimes referred to as words-with-spaces. These multi-token entries are added to the lexicon where the grammarian finds that the semantics of a fixed expression is non-compositional and has the distributional properties of other single word entries. Some examples include an adverb-like all of a sudden, a prepositionlike for example and a</context>
</contexts>
<marker>Dridan, Oepen, 2012</marker>
<rawString>Rebecca Dridan and Stephan Oepen. 2012. Tokenization. Returning to a long solved problem. A survey, contrastive experiment, recommendations, and toolkit. In Proceedings of the 50th Meeting of the Association for Computational Linguistics, page 378–382, Jeju, Republic of Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
<author>Valia Kordoni</author>
<author>Jeremy Nicholson</author>
</authors>
<title>Enhancing performance of lexicalised grammars.</title>
<date>2008</date>
<pages>613--621</pages>
<contexts>
<context position="1704" citStr="Dridan et al., 2008" startWordPosition="255" endWordPosition="258"> used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG should therefore also benefit from supertagging, but while various attempts have shown possibilities (Blunsom, 2007; Dridan et al., 2008; Dridan, 2009), supertagging is still not a standard element in the ERG parsing pipeline. There are two main reasons for this. The first is that the ERG lexicon does not assign simple atomic categories to words, but instead builds complex structured signs from information about lemmas and lexical rules, and hence the shape and integration of the supertags is not straightforward. Bangalore and Joshi (2010) define a supertag as a primitive structure that contains all the information about a lexical item, including argument structure, and where the arguments should be found. Within the ERG, that</context>
</contexts>
<marker>Dridan, Kordoni, Nicholson, 2008</marker>
<rawString>Rebecca Dridan, Valia Kordoni, and Jeremy Nicholson. 2008. Enhancing performance of lexicalised grammars. page 613 – 621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
</authors>
<title>Using lexical statistics to improve HPSG parsing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computational Linguistics, Saarland University.</institution>
<contexts>
<context position="1719" citStr="Dridan, 2009" startWordPosition="259" endWordPosition="260">rser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG should therefore also benefit from supertagging, but while various attempts have shown possibilities (Blunsom, 2007; Dridan et al., 2008; Dridan, 2009), supertagging is still not a standard element in the ERG parsing pipeline. There are two main reasons for this. The first is that the ERG lexicon does not assign simple atomic categories to words, but instead builds complex structured signs from information about lemmas and lexical rules, and hence the shape and integration of the supertags is not straightforward. Bangalore and Joshi (2010) define a supertag as a primitive structure that contains all the information about a lexical item, including argument structure, and where the arguments should be found. Within the ERG, that information is</context>
<context position="7496" citStr="Dridan (2009)" startWordPosition="1197" endWordPosition="1198">n), (2,preposition), (3,preposition). Without these constructed categories, they would only have 1365 supertags. Most previous supertagging attempts with the ERG have used the grammar’s lexical types, which describe the coarse-grained part of speech, and the subcategorisation of a word, but not the inflection. Hence both lends and lent have a possible lexical type v np* pp* to le, which indicates a verb, with optional noun phrase and prepositional phrase arguments, where the preposition has the form to. The number of lexical types changes as the grammar grows, and is currently just over 1000. Dridan (2009) and Fares (2013) experimented with other tag types, but both found lexical types to be the optimal balance between predictability and efficiency. Both used a multi-tagging approach dubbed selective tagging to integrate the supertags into the parser. This involved only applying the supertag filter when the tag probability is above a configurable threshold, and not pruning otherwise. For multi-token entries, both Blunsom (2007) and NP0J, VP V lends (a) LTAG NP11 1202 aj - i le Foreign v nger-tr dlr v prp olr v np*-pp* to le lending v pst olr v - unacc le increased adverb adverb adverb adverb di</context>
<context position="13617" citStr="Dridan (2009)" startWordPosition="2217" endWordPosition="2218">the first 22 sections. We follow the recommendations of the DeepBank developers in using Sections 00–19 for training, Section 20 (WSJ20) for development and Section 21 (WSJ21) as test data. In addition, we use two further sources of training data: the training portions of the LinGO Redwoods Treebank (Oepen et al., 2004), a steadily growing collection of gold standard HPSG annotations in a variety of domains; and the Wall Street Journal section of the North American News Corpus (NANC), which has been parsed, but not manually annotated. This builds on observations by Prins and van Noord (2003), Dridan (2009) and Ytrestøl (2012) that even uncorrected parser output makes very good training data for a supertagger, since the constraints in the parser lead to viable, if not entirely correct sequences. This allows us to use much larger training sets than would be possible if we required manually annotated data. In final testing, we also include two further data sets to observe how domain affects the contribution of the ubertagging. These are both from the test portion of the Redwoods Treebank: CatB, an essay about open-source software;1 and WeScience13, 1http://catb.org/esr/writings/ text from Wikipedi</context>
<context position="22575" citStr="Dridan (2009)" startWordPosition="3718" endWordPosition="3719">s they called micro-tokens as either SPLIT or NOSPLIT. The CRF classifier used a less informed input (since it was external to the parser), but a much more complex model, to produce a best single path sentence accuracy of 94.06%. Encouragingly, this level of segmentation performance was shown in later work to produce a viable parser input (Fares, 2013). Switching to the tagging results, we see that the F1 numbers are quite good for tag sets of this size.6 The best tag accuracy seen for ERG LTYPE-style tags was 95.55 in Ytrestøl (2012), using gold standard segmentation on a different data set. Dridan (2009) experimented with a tag granularity similar to our INFL (letype+morph) and saw a tag accuracy of 91.51, but with much less training data. From other formalisms, Kummerfeld et al. (2010) 5Fares et al. (2013) used a different section of an earlier version of DeepBank, but with the same style of annotation. 6We need to measure F, rather than tag accuracy here, since the number of tokens tagged will vary according to the segmentation. 1206 report a single tag accuracy of 95.91, with the smaller CCG supertag set. Despite the promising tag F1 numbers however, the sentence level accuracy still indic</context>
<context position="27635" citStr="Dridan, 2009" startWordPosition="4595" endWordPosition="4596">arly that the LTYPE model provides much less pruning for any given level of lexitem accuracy, while the performance of the other models is almost indistinguishable. Analysis The current state-of-the-art POS tagging accuracy (using the 45 tags in the PTB) is approximately 97.5%. The most restrictive p value we report for each model was selected to demonstrate that level of accuracy, which we can see would lead to pruning over 80% of lexitems when using FULL tags, an average of 1.56 tags per token. While this level of accuracy has been sufficient for statistical treebank parsing, previous work (Dridan, 2009) has shown that tag accuracy cannot directly predict parser performance, since errors of different types can have very different effects. This is hard to quantify without parsing, but we made a qualitative analysis at the lexitems that were incorrectly being 8The average number of lexitems per token for the unrestricted parser is 8.03, although the actual assignment is far from uniform, with up to 70 lexitems per token seen for the very ambiguous tokens. pruned. For all models, the most difficult lexitems to get correct were proper nouns, particular those that are also used as common nouns (e.</context>
</contexts>
<marker>Dridan, 2009</marker>
<rawString>Rebecca Dridan. 2009. Using lexical statistics to improve HPSG parsing. Ph.D. thesis, Department of Computational Linguistics, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murhaf Fares</author>
<author>Stephan Oepen</author>
<author>Yi Zhang</author>
</authors>
<title>Machine learning for high-quality tokenization. Replicating variable tokenization schemes.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>231--244</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="21856" citStr="Fares et al. (2013)" startWordPosition="3591" endWordPosition="3594">hink the length of a state should influence the tag sequence probability. Segmentation Tagging Tag Type F1 Sent. F1 Sent. FULL 99.55 94.48 93.92 42.13 INFL 99.45 93.55 93.74 41.49 LTYPE 99.40 93.03 93.27 38.12 Table 2: Segmentation and tagging performance of the best path found for each model, measured per segment in terms of F1, and also as complete sentence accuracy. Single sequence results Table 2 shows the results when considering the best path through the lattice. In terms of segmentation, our sentence accuracy is comparable to that of the stand-alone segmentation performance reported by Fares et al. (2013) over similar data.5 In that work, the authors used a binary CRF classifier to label points between objects they called micro-tokens as either SPLIT or NOSPLIT. The CRF classifier used a less informed input (since it was external to the parser), but a much more complex model, to produce a best single path sentence accuracy of 94.06%. Encouragingly, this level of segmentation performance was shown in later work to produce a viable parser input (Fares, 2013). Switching to the tagging results, we see that the F1 numbers are quite good for tag sets of this size.6 The best tag accuracy seen for ERG</context>
</contexts>
<marker>Fares, Oepen, Zhang, 2013</marker>
<rawString>Murhaf Fares, Stephan Oepen, and Yi Zhang. 2013. Machine learning for high-quality tokenization. Replicating variable tokenization schemes. In Computational Linguistics and Intelligent Text Processing, page 231– 244. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murhaf Fares</author>
</authors>
<title>ERG tokenization and lexical categorization: a sequence labeling approach.</title>
<date>2013</date>
<tech>Master’s thesis,</tech>
<institution>Department of Informatics, University of Oslo.</institution>
<contexts>
<context position="7513" citStr="Fares (2013)" startWordPosition="1200" endWordPosition="1201">), (3,preposition). Without these constructed categories, they would only have 1365 supertags. Most previous supertagging attempts with the ERG have used the grammar’s lexical types, which describe the coarse-grained part of speech, and the subcategorisation of a word, but not the inflection. Hence both lends and lent have a possible lexical type v np* pp* to le, which indicates a verb, with optional noun phrase and prepositional phrase arguments, where the preposition has the form to. The number of lexical types changes as the grammar grows, and is currently just over 1000. Dridan (2009) and Fares (2013) experimented with other tag types, but both found lexical types to be the optimal balance between predictability and efficiency. Both used a multi-tagging approach dubbed selective tagging to integrate the supertags into the parser. This involved only applying the supertag filter when the tag probability is above a configurable threshold, and not pruning otherwise. For multi-token entries, both Blunsom (2007) and NP0J, VP V lends (a) LTAG NP11 1202 aj - i le Foreign v nger-tr dlr v prp olr v np*-pp* to le lending v pst olr v - unacc le increased adverb adverb adverb adverb ditto ditto 1,adver</context>
<context position="9098" citStr="Fares (2013)" startWordPosition="1468" endWordPosition="1469">mple receiving p np i le, a preposition-type category). Both of these solutions (demonstrated in Figure 2), as well as that of Prins and van Noord (2003), in some ways defeat one of the purposes of treating these expressions as fixed units. The grammarian, by assigning the same category to, for example, all of a sudden and suddenly, is declaring that these two expressions have the same distributional properties, the properties that a sequential classifier is trying to exploit. Separating the tokens loses that information, and introduces extra noise into the sequence model. Ytrestøl (2012) and Fares (2013) treat the multientry tokens as single expressions for tagging, but with no ambiguity. Ytrestøl (2012) manages this by using gold standard tokenisation, which is, as he states, the standard practice for statistical parsing, but is an artificially simplified setup. Fares (2013) is the only work we know about that has tried to predict the final segmentation that the ERG produces. We compare segmentation accuracy between our joint model and his stand-alone tokeniser in Section 6. Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separated languages such </context>
<context position="22316" citStr="Fares, 2013" startWordPosition="3672" endWordPosition="3673">ttice. In terms of segmentation, our sentence accuracy is comparable to that of the stand-alone segmentation performance reported by Fares et al. (2013) over similar data.5 In that work, the authors used a binary CRF classifier to label points between objects they called micro-tokens as either SPLIT or NOSPLIT. The CRF classifier used a less informed input (since it was external to the parser), but a much more complex model, to produce a best single path sentence accuracy of 94.06%. Encouragingly, this level of segmentation performance was shown in later work to produce a viable parser input (Fares, 2013). Switching to the tagging results, we see that the F1 numbers are quite good for tag sets of this size.6 The best tag accuracy seen for ERG LTYPE-style tags was 95.55 in Ytrestøl (2012), using gold standard segmentation on a different data set. Dridan (2009) experimented with a tag granularity similar to our INFL (letype+morph) and saw a tag accuracy of 91.51, but with much less training data. From other formalisms, Kummerfeld et al. (2010) 5Fares et al. (2013) used a different section of an earlier version of DeepBank, but with the same style of annotation. 6We need to measure F, rather than</context>
</contexts>
<marker>Fares, 2013</marker>
<rawString>Murhaf Fares. 2013. ERG tokenization and lexical categorization: a sequence labeling approach. Master’s thesis, Department of Informatics, University of Oslo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
</authors>
<title>DeepBank. A dynamically annotated treebank of the Wall Street Journal.</title>
<date>2012</date>
<journal>Edic¸˜oes Colibri.</journal>
<booktitle>In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories, page 85 – 96,</booktitle>
<location>Lisbon,</location>
<contexts>
<context position="12820" citStr="Flickinger et al., 2012" startWordPosition="2083" endWordPosition="2086">o punctuation affixed, the lexical type is the only element in the lexitem, other than the word form (e.g. Foreign, as). In this example, we also see lexitems with inflectional rules (v prp olr, v pst olr), derivational rules (v nger-tr dlr) and punctuation affixation rules (w period plr). These lexitems are put in to a chart, forming a lexical lattice, and it is over this lattice that we apply our ubertagging model, removing unlikely lexitems before they are seen by the syntactic parsing stage. 4 The Data The primary data sets we use in these experiments are from the 1.0 version of DeepBank (Flickinger et al., 2012), an HPSG annotation of the Wall Street Journal text used for the Penn Treebank (PTB; Marcus et al. (1993)). The current version has gold standard annotations for approximately 85% of the first 22 sections. We follow the recommendations of the DeepBank developers in using Sections 00–19 for training, Section 20 (WSJ20) for development and Section 21 (WSJ21) as test data. In addition, we use two further sources of training data: the training portions of the LinGO Redwoods Treebank (Oepen et al., 2004), a steadily growing collection of gold standard HPSG annotations in a variety of domains; and </context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. DeepBank. A dynamically annotated treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories, page 85 – 96, Lisbon, Portugal. Edic¸˜oes Colibri.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<pages>1--15</pages>
<contexts>
<context position="1380" citStr="Flickinger (2000)" startWordPosition="209" endWordPosition="210">t decade or so, supertagging has become a standard method for increasing parser efficiency for heavily lexicalised grammar formalisms such as LTAG (Bangalore and Joshi, 1999), CCG (Clark and Curran, 2007) and HPSG (Matsuzaki et al., 2007). In each of these systems, fine-grained lexical categories, known as supertags, are used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG should therefore also benefit from supertagging, but while various attempts have shown possibilities (Blunsom, 2007; Dridan et al., 2008; Dridan, 2009), supertagging is still not a standard element in the ERG parsing pipeline. There are two main reasons for this. The first is that the ERG lexicon does not assign simple atomic categories to words, but instead builds complex structured signs from information ab</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="9765" citStr="Kudo et al., 2004" startWordPosition="1572" endWordPosition="1575">s for tagging, but with no ambiguity. Ytrestøl (2012) manages this by using gold standard tokenisation, which is, as he states, the standard practice for statistical parsing, but is an artificially simplified setup. Fares (2013) is the only work we know about that has tried to predict the final segmentation that the ERG produces. We compare segmentation accuracy between our joint model and his stand-alone tokeniser in Section 6. Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separated languages such as Chinese (Zhang and Clark, 2010) and Japanese (Kudo et al., 2004). While at a high level, this work is solving the same problem, the shape of the problems are quite different from a data point of view. Regular joint morphological analysis and segmentation has much greater ambiguity in terms of possible segmentations but, in most cases, less ambiguity in terms of labelling than our situation. This also holds for other lemmatisation and morphological research, such as Toutanova and Cherry (2009). While we drew inspiration from this w period plr p vp i le w period plr av - s-vp-po le as av - dg-v le as well. well. Figure 3: A selection from the 70 lexitems ins</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese morphological analysis. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, page 230 – 237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Jessika Roesner</author>
<author>Tim Dawborn</author>
<author>James Haggerty</author>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Faster parsing by supertagger adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Meeting of the Association for Computational Linguistics, page 345 – 355,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="22761" citStr="Kummerfeld et al. (2010)" startWordPosition="3747" endWordPosition="3750">roduce a best single path sentence accuracy of 94.06%. Encouragingly, this level of segmentation performance was shown in later work to produce a viable parser input (Fares, 2013). Switching to the tagging results, we see that the F1 numbers are quite good for tag sets of this size.6 The best tag accuracy seen for ERG LTYPE-style tags was 95.55 in Ytrestøl (2012), using gold standard segmentation on a different data set. Dridan (2009) experimented with a tag granularity similar to our INFL (letype+morph) and saw a tag accuracy of 91.51, but with much less training data. From other formalisms, Kummerfeld et al. (2010) 5Fares et al. (2013) used a different section of an earlier version of DeepBank, but with the same style of annotation. 6We need to measure F, rather than tag accuracy here, since the number of tokens tagged will vary according to the segmentation. 1206 report a single tag accuracy of 95.91, with the smaller CCG supertag set. Despite the promising tag F1 numbers however, the sentence level accuracy still indicates a performance level unacceptable for parser input. Comparing between tag types, we see that, possibly surprisingly, the more fine-grained tags are more accurately assigned, although</context>
</contexts>
<marker>Kummerfeld, Roesner, Dawborn, Haggerty, Curran, Clark, 2010</marker>
<rawString>Jonathan K. Kummerfeld, Jessika Roesner, Tim Dawborn, James Haggerty, James R. Curran, and Stephen Clark. 2010. Faster parsing by supertagger adaptation. In Proceedings of the 48th Meeting of the Association for Computational Linguistics, page 345 – 355, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpora of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="12926" citStr="Marcus et al. (1993)" startWordPosition="2102" endWordPosition="2106">eign, as). In this example, we also see lexitems with inflectional rules (v prp olr, v pst olr), derivational rules (v nger-tr dlr) and punctuation affixation rules (w period plr). These lexitems are put in to a chart, forming a lexical lattice, and it is over this lattice that we apply our ubertagging model, removing unlikely lexitems before they are seen by the syntactic parsing stage. 4 The Data The primary data sets we use in these experiments are from the 1.0 version of DeepBank (Flickinger et al., 2012), an HPSG annotation of the Wall Street Journal text used for the Penn Treebank (PTB; Marcus et al. (1993)). The current version has gold standard annotations for approximately 85% of the first 22 sections. We follow the recommendations of the DeepBank developers in using Sections 00–19 for training, Section 20 (WSJ20) for development and Section 21 (WSJ21) as test data. In addition, we use two further sources of training data: the training portions of the LinGO Redwoods Treebank (Oepen et al., 2004), a steadily growing collection of gold standard HPSG annotations in a variety of domains; and the Wall Street Journal section of the North American News Corpus (NANC), which has been parsed, but not m</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpora of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFG-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2007),</booktitle>
<pages>1671--1676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1001" citStr="Matsuzaki et al., 2007" startWordPosition="146" endWordPosition="149">cluded in the English Resource Grammar, can be very slow. We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy. 1 Introduction and Motivation Over the last decade or so, supertagging has become a standard method for increasing parser efficiency for heavily lexicalised grammar formalisms such as LTAG (Bangalore and Joshi, 1999), CCG (Clark and Curran, 2007) and HPSG (Matsuzaki et al., 2007). In each of these systems, fine-grained lexical categories, known as supertags, are used to prune the parser search space prior to full syntactic parsing, leading to faster parsing at the risk of removing necessary lexical items. Various methods are used to configure the degree of pruning in order to balance this trade-off. The English Resource Grammar (ERG; Flickinger (2000)) is a large hand-written HPSGbased grammar of English that produces finegrained syntacto-semantic analyses. Given the high level of lexical ambiguity in its lexicon, parsing with the ERG should therefore also benefit fro</context>
<context position="5645" citStr="Matsuzaki et al. (2007)" startWordPosition="896" endWordPosition="899">figurable number of tags per token, since the tagger was not accurate enough to make assigning a single tree viable parser input (Bangalore and Joshi, 1999). The C&amp;C CCG parser uses a more complex Maximum Entropy tagger to assign tags from a set of 425 CCG lexical categories (Clark and Curran, 2007). They also found it necessary to supply more than one tag per token, and hence assign all tags that have a probability within a percentage Q of the most likely tag for each token. Their standard parser configuration uses a very restrictive Q value initially, relaxing it when no parse can be found. Matsuzaki et al. (2007) use a supertagger similar to the C&amp;C tagger alongside a CFG filter to improve the speed of their HPSG parser, feeding sequences of single tags to the parser until a parse is possible. As in the ERG, category and inflectional information are separate in the automatically-extracted ENJU grammar: their supertag set consists of 1361 tags constructed by combining lexical categories and lexical rules. Figure 1 shows examples of supertags from these three tag sets, all describing the simple transitive use of lends. S (S[dcl]\NP)/NP (b) CCG [NP.nom&lt;V.bse&gt;NP.acc]-singular3rd verb rule (c) ENJU HPSG Fi</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient HPSG parsing with supertagging and CFG-filtering. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2007), page 1671–1676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<date>2002</date>
<note>Hidden semi-Markov models (HSMMs).</note>
<contexts>
<context position="16002" citStr="Murphy (2002)" startWordPosition="2608" endWordPosition="2609">d usage can lead to a disproportionately adverse effect on the semantic analysis of the text. 5 Ubertagging Model Our ubertagging model is very similar to a standard trigram Hidden Markov Model (HMM), except that the states are not all of the same length. Our states are based on the lexitems in the lexical lattice produced by the lexical parsing stage of PET, and as such, can be partially overlapping. We formalise this be defining each state by its start position, end position, and tag. This turns out to make our model equivalent to a type of Hidden semi-Markov Model called a segmental HMM in Murphy (2002). In a segmental HMM, the states are segments with a tag (t) and a length in frames (l). In our setup, the frames are the ERG internal tokens and the segments are the lexitems, which are the potential candidates cathedral-bazaar/ by Eric S. Raymond 2While the parser has mechanisms for handling words unknown to the lexicon, with the current grammar these mechanisms will never propose a multi-token lexitem, and so only the multi-token entries explicitly in the lexicon will be recognised as such. 1204 Lexitems Data Set Source Use Gold? Trees All M-T WSJ00:19 DeepBank 1.0 §00–19 train yes 33783 66</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Kevin P. Murphy. 2002. Hidden semi-Markov models (HSMMs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing. Practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>162--169</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="10929" citStr="Oepen and Carroll, 2000" startWordPosition="1767" endWordPosition="1770">s well. well. Figure 3: A selection from the 70 lexitems instantiated for Foreign lending increased as well. related area, as well as from the speech recognition field, differences in the relative frequency of observations and labels, as well as in segmentation ambiguity mean that conclusions found in these areas did not always hold true in our problem space. 3 The Parser The parsing environment we work with is the PET parser (Callmeier, 2000), a unification-based chart parser that has been engineered for efficiency with precision grammars, and incorporates subsumptionbased ambiguity packing (Oepen and Carroll, 2000) and statistical model driven selective unpacking (Zhang et al., 2007). Parsing in PET is divided in two stages. The first stage, lexical parsing, covers everything from tokenising the raw input string to populating the base of the parse chart with the appropriate lexical items, ready for the second — syntactic parsing — stage. In this work, we embed our ubertagging model between the two stages. By this point, the input has been segmented into what we call internal tokens, which broadly means splitting at whitespace and hyphens, and making ’s a separate token. These tokens are subject to a mor</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Stephan Oepen and John Carroll. 2000. Ambiguity packing in constraint-based parsing. Practical results. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, page 162 –169, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Daniel Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>LinGO Redwoods. A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<pages>596</pages>
<contexts>
<context position="13325" citStr="Oepen et al., 2004" startWordPosition="2167" endWordPosition="2170">a The primary data sets we use in these experiments are from the 1.0 version of DeepBank (Flickinger et al., 2012), an HPSG annotation of the Wall Street Journal text used for the Penn Treebank (PTB; Marcus et al. (1993)). The current version has gold standard annotations for approximately 85% of the first 22 sections. We follow the recommendations of the DeepBank developers in using Sections 00–19 for training, Section 20 (WSJ20) for development and Section 21 (WSJ21) as test data. In addition, we use two further sources of training data: the training portions of the LinGO Redwoods Treebank (Oepen et al., 2004), a steadily growing collection of gold standard HPSG annotations in a variety of domains; and the Wall Street Journal section of the North American News Corpus (NANC), which has been parsed, but not manually annotated. This builds on observations by Prins and van Noord (2003), Dridan (2009) and Ytrestøl (2012) that even uncorrected parser output makes very good training data for a supertagger, since the constraints in the parser lead to viable, if not entirely correct sequences. This allows us to use much larger training sets than would be possible if we required manually annotated data. In f</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>Stephan Oepen, Daniel Flickinger, Kristina Toutanova, and Christopher D. Manning. 2004. LinGO Redwoods. A rich and dynamic treebank for HPSG. Research on Language and Computation, 2(4):575 – 596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reinforcing parser preferences through tagging.</title>
<date>2003</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>44</volume>
<issue>3</issue>
<marker>Prins, van Noord, 2003</marker>
<rawString>Robbert Prins and Gertjan van Noord. 2003. Reinforcing parser preferences through tagging. Traitement Automatique des Langues, 44(3):121–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<pages>475--484</pages>
<contexts>
<context position="36545" citStr="Rimell and Clark (2008)" startWordPosition="6099" endWordPosition="6102">igh tagging accuracies for very fine-grained tags, which leads to potential parser speedups of between 4 and 8 fold. Analysis of the tagging errors still being made have suggested some possibly avoidable inconsistencies in the grammar and treebank, which have been fed back to the developers, hopefully leading to even better results in the future. In future work, we will investigate more advanced smoothing methods to try and boost the ubertagging accuracy. We also intend to more fully explore the domain adaptation potentials of the lexical model that have been seen in other parsing setups (see Rimell and Clark (2008) for example), as well as examine the limits on the effects of more training data. Finally, we would like to explore just how much the statistic properties of our data dictate the success of the model by looking at related problems like morphological analysis of unsegmented languages such as Japanese. Acknowledgements I am grateful to my colleagues from the Oslo Language Technology Group and the DELPH-IN consortium for many discussions on the issues involved in this work, and particularly to Stephan Oepen who inspired the initial lattice tagging idea. Thanks also to three anonymous reviewers f</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. page 475 – 484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Colin Cherry</author>
</authors>
<title>A global model for joint lemmatization and part-of-speech prediction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>486--494</pages>
<contexts>
<context position="10198" citStr="Toutanova and Cherry (2009)" startWordPosition="1642" endWordPosition="1645">ction 6. Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separated languages such as Chinese (Zhang and Clark, 2010) and Japanese (Kudo et al., 2004). While at a high level, this work is solving the same problem, the shape of the problems are quite different from a data point of view. Regular joint morphological analysis and segmentation has much greater ambiguity in terms of possible segmentations but, in most cases, less ambiguity in terms of labelling than our situation. This also holds for other lemmatisation and morphological research, such as Toutanova and Cherry (2009). While we drew inspiration from this w period plr p vp i le w period plr av - s-vp-po le as av - dg-v le as well. well. Figure 3: A selection from the 70 lexitems instantiated for Foreign lending increased as well. related area, as well as from the speech recognition field, differences in the relative frequency of observations and labels, as well as in segmentation ambiguity mean that conclusions found in these areas did not always hold true in our problem space. 3 The Parser The parsing environment we work with is the PET parser (Callmeier, 2000), a unification-based chart parser that has be</context>
</contexts>
<marker>Toutanova, Cherry, 2009</marker>
<rawString>Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmatization and part-of-speech prediction. In Proceedings of the 47th Meeting of the Association for Computational Linguistics, page 486 – 494, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
</authors>
<title>Transition-based Parsing for Large-scale Head-Driven Phrase Structure Grammars.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Informatics, University of Oslo.</institution>
<contexts>
<context position="9081" citStr="Ytrestøl (2012)" startWordPosition="1465" endWordPosition="1466">e expression for example receiving p np i le, a preposition-type category). Both of these solutions (demonstrated in Figure 2), as well as that of Prins and van Noord (2003), in some ways defeat one of the purposes of treating these expressions as fixed units. The grammarian, by assigning the same category to, for example, all of a sudden and suddenly, is declaring that these two expressions have the same distributional properties, the properties that a sequential classifier is trying to exploit. Separating the tokens loses that information, and introduces extra noise into the sequence model. Ytrestøl (2012) and Fares (2013) treat the multientry tokens as single expressions for tagging, but with no ambiguity. Ytrestøl (2012) manages this by using gold standard tokenisation, which is, as he states, the standard practice for statistical parsing, but is an artificially simplified setup. Fares (2013) is the only work we know about that has tried to predict the final segmentation that the ERG produces. We compare segmentation accuracy between our joint model and his stand-alone tokeniser in Section 6. Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separate</context>
<context position="13637" citStr="Ytrestøl (2012)" startWordPosition="2220" endWordPosition="2221">ons. We follow the recommendations of the DeepBank developers in using Sections 00–19 for training, Section 20 (WSJ20) for development and Section 21 (WSJ21) as test data. In addition, we use two further sources of training data: the training portions of the LinGO Redwoods Treebank (Oepen et al., 2004), a steadily growing collection of gold standard HPSG annotations in a variety of domains; and the Wall Street Journal section of the North American News Corpus (NANC), which has been parsed, but not manually annotated. This builds on observations by Prins and van Noord (2003), Dridan (2009) and Ytrestøl (2012) that even uncorrected parser output makes very good training data for a supertagger, since the constraints in the parser lead to viable, if not entirely correct sequences. This allows us to use much larger training sets than would be possible if we required manually annotated data. In final testing, we also include two further data sets to observe how domain affects the contribution of the ubertagging. These are both from the test portion of the Redwoods Treebank: CatB, an essay about open-source software;1 and WeScience13, 1http://catb.org/esr/writings/ text from Wikipedia articles about Nat</context>
<context position="22502" citStr="Ytrestøl (2012)" startWordPosition="3706" endWordPosition="3707">rk, the authors used a binary CRF classifier to label points between objects they called micro-tokens as either SPLIT or NOSPLIT. The CRF classifier used a less informed input (since it was external to the parser), but a much more complex model, to produce a best single path sentence accuracy of 94.06%. Encouragingly, this level of segmentation performance was shown in later work to produce a viable parser input (Fares, 2013). Switching to the tagging results, we see that the F1 numbers are quite good for tag sets of this size.6 The best tag accuracy seen for ERG LTYPE-style tags was 95.55 in Ytrestøl (2012), using gold standard segmentation on a different data set. Dridan (2009) experimented with a tag granularity similar to our INFL (letype+morph) and saw a tag accuracy of 91.51, but with much less training data. From other formalisms, Kummerfeld et al. (2010) 5Fares et al. (2013) used a different section of an earlier version of DeepBank, but with the same style of annotation. 6We need to measure F, rather than tag accuracy here, since the number of tokens tagged will vary according to the segmentation. 1206 report a single tag accuracy of 95.91, with the smaller CCG supertag set. Despite the </context>
</contexts>
<marker>Ytrestøl, 2012</marker>
<rawString>Gisle Ytrestøl. 2012. Transition-based Parsing for Large-scale Head-Driven Phrase Structure Grammars. Ph.D. thesis, Department of Informatics, University of Oslo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
</authors>
<title>Extracting and annotating Wikipedia subdomains.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>185--197</pages>
<location>Groningen, The Netherlands.</location>
<contexts>
<context position="14312" citStr="Ytrestøl et al., 2009" startWordPosition="2323" endWordPosition="2326">training data for a supertagger, since the constraints in the parser lead to viable, if not entirely correct sequences. This allows us to use much larger training sets than would be possible if we required manually annotated data. In final testing, we also include two further data sets to observe how domain affects the contribution of the ubertagging. These are both from the test portion of the Redwoods Treebank: CatB, an essay about open-source software;1 and WeScience13, 1http://catb.org/esr/writings/ text from Wikipedia articles about Natural Language Processing from the WeScience project (Ytrestøl et al., 2009). Table 1 summarises the vital statistics of the data we use. With the focus on multi-token lexitems, it is instructive to see just how frequent they are. In terms of type frequency, almost 10% of the approximately 38500 lexical entries in the current ERG lexicon have more than one token in their canonical form.2 However, while this is a significant percentage of the lexicon, they do not account for the same percentage of tokens during parsing. An analysis of WSJ00:19 shows that approximately one third of the sentences had at least one multi-token lexitem in the unpruned lexical lattice, and i</context>
</contexts>
<marker>Ytrestøl, Oepen, Flickinger, 2009</marker>
<rawString>Gisle Ytrestøl, Stephan Oepen, and Dan Flickinger. 2009. Extracting and annotating Wikipedia subdomains. In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories, page 185 –197, Groningen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, page 843 – 852,</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9732" citStr="Zhang and Clark, 2010" startWordPosition="1566" endWordPosition="1569">ultientry tokens as single expressions for tagging, but with no ambiguity. Ytrestøl (2012) manages this by using gold standard tokenisation, which is, as he states, the standard practice for statistical parsing, but is an artificially simplified setup. Fares (2013) is the only work we know about that has tried to predict the final segmentation that the ERG produces. We compare segmentation accuracy between our joint model and his stand-alone tokeniser in Section 6. Looking at other instances of joint segmentation and tagging leads to work in non-whitespace separated languages such as Chinese (Zhang and Clark, 2010) and Japanese (Kudo et al., 2004). While at a high level, this work is solving the same problem, the shape of the problems are quite different from a data point of view. Regular joint morphological analysis and segmentation has much greater ambiguity in terms of possible segmentations but, in most cases, less ambiguity in terms of labelling than our situation. This also holds for other lemmatisation and morphological research, such as Toutanova and Cherry (2009). While we drew inspiration from this w period plr p vp i le w period plr av - s-vp-po le as av - dg-v le as well. well. Figure 3: A s</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, page 843 – 852, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Efficiency in unification-based n-best parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies, page 48 – 59,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10999" citStr="Zhang et al., 2007" startWordPosition="1777" endWordPosition="1780">oreign lending increased as well. related area, as well as from the speech recognition field, differences in the relative frequency of observations and labels, as well as in segmentation ambiguity mean that conclusions found in these areas did not always hold true in our problem space. 3 The Parser The parsing environment we work with is the PET parser (Callmeier, 2000), a unification-based chart parser that has been engineered for efficiency with precision grammars, and incorporates subsumptionbased ambiguity packing (Oepen and Carroll, 2000) and statistical model driven selective unpacking (Zhang et al., 2007). Parsing in PET is divided in two stages. The first stage, lexical parsing, covers everything from tokenising the raw input string to populating the base of the parse chart with the appropriate lexical items, ready for the second — syntactic parsing — stage. In this work, we embed our ubertagging model between the two stages. By this point, the input has been segmented into what we call internal tokens, which broadly means splitting at whitespace and hyphens, and making ’s a separate token. These tokens are subject to a morphological analysis component which proposes possible inflectional and</context>
</contexts>
<marker>Zhang, Oepen, Carroll, 2007</marker>
<rawString>Yi Zhang, Stephan Oepen, and John Carroll. 2007. Efficiency in unification-based n-best parsing. In Proceedings of the 10th International Conference on Parsing Technologies, page 48 – 59, Prague, Czech Republic, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
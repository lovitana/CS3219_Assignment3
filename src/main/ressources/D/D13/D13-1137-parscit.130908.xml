<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006136">
<title confidence="0.9891555">
Simple Customization of Recursive Neural Networks
for Semantic Relation Classification
</title>
<author confidence="0.743184">
Kazuma Hashimoto†, Makoto Miwa††, Yoshimasa Tsuruoka†, and Takashi Chikayama††The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
</author>
<email confidence="0.558938">
{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp
</email>
<affiliation confidence="0.991842">
††The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UK
</affiliation>
<email confidence="0.994814">
makoto.miwa@manchester.ac.uk
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998436">
In this paper, we present a recursive neural
network (RNN) model that works on a syn-
tactic tree. Our model differs from previous
RNN models in that the model allows for an
explicit weighting of important phrases for the
target task. We also propose to average param-
eters in training. Our experimental results on
semantic relation classification show that both
phrase categories and task-specific weighting
significantly improve the prediction accuracy
of the model. We also show that averaging the
model parameters is effective in stabilizing the
learning and improves generalization capacity.
The proposed model marks scores competitive
with state-of-the-art RNN-based models.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99970866">
Recursive Neural Network (RNN) models are
promising deep learning models which have been
applied to a variety of natural language processing
(NLP) tasks, such as sentiment classification, com-
pound similarity, relation classification and syntactic
parsing (Hermann and Blunsom, 2013; Socher et al.,
2012; Socher et al., 2013). RNN models can repre-
sent phrases of arbitrary length in a vector space of
a fixed dimension. Most of them use minimal syn-
tactic information (Socher et al., 2012).
Recently, Hermann and Blunsom (2013) pro-
posed a method for leveraging syntactic informa-
tion, namely CCG combinatory operators, to guide
composition of phrases in RNN models. While their
models were successfully applied to binary senti-
ment classification and compound similarity tasks,
there are questions yet to be answered, e.g., whether
such enhancement is beneficial in other NLP tasks
as well, and whether a similar improvement can
be achieved by using syntactic information of more
commonly available types such as phrase categories
and syntactic heads.
In this paper, we present a supervised RNN model
for a semantic relation classification task. Our model
is different from existing RNN models in that impor-
tant phrases can be explicitly weighted for the task.
Syntactic information used in our model includes
part-of-speech (POS) tags, phrase categories and
syntactic heads. POS tags are used to assign vec-
tor representations to word-POS pairs. Phrase cate-
gories are used to determine which weight matrices
are chosen to combine phrases. Syntactic heads are
used to determine which phrase is weighted during
combining phrases. To incorporate task-specific in-
formation, phrases on the path between entity pairs
are further weighted.
The second contribution of our work is the intro-
duction of parameter averaging into RNN models.
In our preliminary experiments, we observed that
the prediction performance of the model often fluc-
tuates significantly between training iterations. This
fluctuation not only leads to unstable performance
of the resulting models, but also makes it difficult to
fine-tune the hyperparameters of the model. Inspired
by Swersky et al. (2010), we propose to average the
model parameters in the course of training. A re-
cent technique for deep learning models of similar
vein is dropout (Hinton et al., 2012), but averaging
is simpler to implement.
Our experimental results show that our model per-
</bodyText>
<page confidence="0.920292">
1372
</page>
<note confidence="0.456456">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9889176">
Figure 1: A recursive representations of a phrase “a
word vector” with POS tags of the words (DT, NN and
NN respectively). For example, the two word-POS pairs
“word NN” and “vector NN” with a syntactic category
N are combined to represent the phrase “word vector”.
</figureCaption>
<bodyText confidence="0.9987192">
forms better than standard RNN models. By av-
eraging the model parameters, our model achieves
performance competitive with the MV-RNN model
in Socher et al. (2012), without using computation-
ally expensive word-dependent matrices.
</bodyText>
<sectionHeader confidence="0.842922" genericHeader="method">
2 An Averaged RII Model with Syntax
</sectionHeader>
<bodyText confidence="0.999959181818182">
Our model is a supervised RNN that works on a bi-
nary syntactic tree. As our first step to leverage in-
formation available in the tree, we distinguish words
with the same spelling but POS tags in the vector
space. Our model also uses different weight ma-
trices dependent on the phrase categories of child
nodes (phrases or words) in combining phrases. Our
model further weights those nodes that appear to be
important.
Compositional functions of our model follow
those of the SU-RNN model in Socher et al. (2013).
</bodyText>
<subsectionHeader confidence="0.983999">
2.1 Word-POS Vector Representations
</subsectionHeader>
<bodyText confidence="0.999947">
Our model simply assigns vector representations to
word-POS pairs. For example, a word “caused”
can be represented in two ways: “caused VBD” and
“caused VBN”. The vectors are represented as col-
umn vectors in a matrix We E Rd×|�|, where d is
the dimension of a vector and V is a set of all word-
POS pairs we consider.
</bodyText>
<subsectionHeader confidence="0.996177">
2.2 Compositional Functions with Syntax
</subsectionHeader>
<bodyText confidence="0.999326875">
In construction of parse trees, we associate each of
the tree node with its d-dimensional vector represen-
tation computed from vector representations of its
subtrees. For leaf nodes, we look up word-POS vec-
tor representations in V. Figure 1 shows an example
of such recursive representations. A parent vector
p E Rd×1 is computed from its direct child vectors
cl and crE Rd×1:
</bodyText>
<equation confidence="0.994183">
p = tanh(αlW Tcl,Tcr cl+αrW Tcl,Tcr cr+bTcl,Tcr ),
l r
</equation>
<bodyText confidence="0.942411">
where W Tcl,Tcr and WTcl,Tcr E Rd×d are weight
</bodyText>
<equation confidence="0.729892">
l r
</equation>
<bodyText confidence="0.999949076923077">
matrices that depend on the phrase categories of cl
and cr. Here, cl and cr have phrase categories Tcl
and Tcr respectively (such as N, V, etc.). bTcl,Tcr E
Rd×1 is a bias vector. To incorporate the impor-
tance of phrases into the model, two subtrees of a
node may have different weights αl E [0, 1] and
αr(= 1 − αl), taking phrase importance into ac-
count. The value of αl is manually specified and
automatically applied to all nodes based on prior
knowledge about the task. In this way, we can com-
pute vector representations for phrases of arbitrary
length. We denote a set of such matrices as Wlr and
bias vectors as b.
</bodyText>
<subsectionHeader confidence="0.999646">
2.3 Objective Function and Learning
</subsectionHeader>
<bodyText confidence="0.9989295">
As with other RNN models, we add on the top of a
node x a softmax classifier. The classifier is used to
predict a K-class distribution d(x) E RK×1 over a
specific task to train our model:
</bodyText>
<equation confidence="0.998319">
d(x) = softmax(Wlabelx + blabel), (1)
</equation>
<bodyText confidence="0.996798428571429">
where W label E RK×d is a weight matrix and
blabel E RK×1 is a bias vector. We denote t(x) E
RK×1 as the target distribution vector at node x.
t(x) has a 0-1 encoding: the entry at the correct la-
bel of t(x) is 1, and the remaining entries are 0. We
then compute the cross entropy error between d(x)
and t(x):
</bodyText>
<equation confidence="0.9993685">
E(x) = − ∑K tk(x)logdk(x),
k=1
</equation>
<bodyText confidence="0.9897555">
and define an objective function as the sum of E(x)
over all training data:
</bodyText>
<equation confidence="0.832725666666667">
∑ A
J(0) = E(x) + 2 110112,
x
</equation>
<bodyText confidence="0.998984666666667">
where 0 = (We, Wlr, b, W label, blabel) is the set of
our model parameters that should be learned. A is a
vector of regularization parameters.
</bodyText>
<page confidence="0.953767">
1373
</page>
<bodyText confidence="0.9999525">
To compute d(x), we can directly leverage any
other nodes’ feature vectors in the same tree. We
denote such additional feature vectors as x′i E Rd×1,
and extend Eq. (1):
</bodyText>
<equation confidence="0.931512">
∑d(x) = softmax(W labelx + Wadd
i i x′ i + blabel),
</equation>
<bodyText confidence="0.998652875">
where Wadd iE RK×d are weight matrices for addi-
tional features. We denote these matrices W add
i as
Wadd. We also add Wadd to θ:
is efficiently computed via backpropagation through
structure (Goller and K¨uchler, 1996). To minimize
J(θ), we use batch L-BFGS1 (Hermann and Blun-
som, 2013; Socher et al., 2012).
</bodyText>
<subsectionHeader confidence="0.991644">
2.4 Averaging
</subsectionHeader>
<bodyText confidence="0.998187">
We use averaged model parameters
</bodyText>
<equation confidence="0.9958495">
1
θ = T + 1
</equation>
<bodyText confidence="0.9998025">
at test time, where θt is the vector of model parame-
ters after t iterations of the L-BFGS optimization.
Our preliminary experimental results suggest that
averaging θ except We works well.
</bodyText>
<sectionHeader confidence="0.998393" genericHeader="method">
3 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999616666666667">
We used the Enju parser (Miyao and Tsujii, 2008)
for syntactic parsing. We used 13 phrase categories
given by Enju.
</bodyText>
<subsectionHeader confidence="0.996221">
3.1 Task: Semantic Relation Classification
</subsectionHeader>
<bodyText confidence="0.999976">
We evaluated our model on a semantic relation clas-
sification task: SemEval 2010 Task 8 (Hendrickx et
al., 2010). Following Socher et al. (2012), we re-
garded the task as a 19-class classification problem.
There are 8,000 samples for training, and 2,717 for
</bodyText>
<footnote confidence="0.9842505">
1We used libLBFGS provided at http://www.
chokkan.org/software/liblbfgs/.
</footnote>
<figureCaption confidence="0.999201">
Figure 2: Classifying the relation between two entities.
</figureCaption>
<bodyText confidence="0.999474454545455">
test. For the validation set, we randomly sampled
2,182 samples from the training data.
To predict a class label, we first find the minimal
phrase that covers the target entities and then use the
vector representation of the phrase (Figure 2).
As explained in Section 2.3, we can directly con-
nect features on any other nodes to the softmax clas-
sifier. In this work, we used three such internal fea-
tures: two vector representations of target entities
and one averaged vector representation of words be-
tween the entities2.
</bodyText>
<subsectionHeader confidence="0.999823">
3.2 Weights on Phrases
</subsectionHeader>
<bodyText confidence="0.997447142857143">
We tuned the weight αl (or αr) introduced in Sec-
tion 2.2 for this particular task. There are two fac-
tors: syntactic heads and syntactic path between tar-
get entities. Our model puts a weight β E [0.5, 1]
on head phrases, and 1 − β on the others. For re-
lation classification tasks, syntactic paths between
target entities are important (Zhang et al., 2006), so
our model also puts another weight γ E [0.5, 1] on
phrases on the path, and 1 − γ on the others. When
both child nodes are on the path or neither of them
on the path, we set γ = 0.5. The two weight fac-
tors are summed up and divided by 2 to be the final
weights αl and αr to combine the phrases. For ex-
ample, we set αl = (1−β)+γ 2and αr = β+(1−γ)
</bodyText>
<page confidence="0.394236">
2
</page>
<bodyText confidence="0.9996175">
when the right child node is the head and the left
child node is on the path.
</bodyText>
<subsectionHeader confidence="0.9972065">
3.3 Initialization of Model Parameters and
Tuning of Hyperparameters
</subsectionHeader>
<bodyText confidence="0.9923535">
We initialized We with 50-dimensional word vec-
tors3 trained with the model of Collobert et
</bodyText>
<footnote confidence="0.9966302">
2Socher et al. (2012) used richer features including words
around entity pairs in their implementation.
3The word vectors are provided at http://ronan.
collobert.com/senna/. We used the vectors without any
modifications such as normalization.
</footnote>
<equation confidence="0.9856345">
θ = (We,Wlr,b,W label,W add, blabel).
The gradient of J(θ)
∂J(θ) ∑
∂θ =
x
∂E(x)
∂θ + λθ
θt
∑T
t=0
</equation>
<page confidence="0.983076">
1374
</page>
<table confidence="0.99808225">
Method F1 (%)
Our model 79.4
RNN 74.8
MV-RNN 79.1
RNN w/ POS, WordNet, NER 77.6
MV-RNN w/ POS, WordNet, NER 82.4
SVM w/ bag-of-words 73.1
SVM w/ lexical and semantic features 82.2
</table>
<tableCaption confidence="0.9595065">
Table 1: Comparison of our model with other methods on
SemEval 2010 task 8.
</tableCaption>
<table confidence="0.986818428571429">
Method F1 (%)
Our model 79.4
Our model w/o phrase categories (PC) 77.7
Our model w/o head weights (HW) 78.8
Our model w/o path weights (PW) 78.7
Our model w/o averaging (AVE) 76.9
Our model w/o PC, HW, PW, AVE 74.1
</table>
<tableCaption confidence="0.992091">
Table 2: Contributions of syntactic and task-specific in-
formation and averaging.
</tableCaption>
<bodyText confidence="0.996857142857143">
al. (2011), and Wlr with 2 + E, where I ∈ Rd×d
is an identity matrix. Here, E is zero-mean gaussian
random variable with a variance of 0.01. The ini-
tialization of Wlr is the same as that of Socher et
al. (2013). The remaining model parameters were
initialized with 0.
We tuned hyperparameters in our model using the
validation set for each experimental setting. The hy-
perparameters include the regularization parameters
for We, Wlr, Wlabel and Wadd, and the weights Q
and &apos;y. For example, the best performance for our
model with all the proposed methods was obtained
with the values: 10−6, 10−4, 10−3, 10−3, 0.7 and
0.9 respectively.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999866111111111">
Table 1 shows the performance of our model and that
of previously reported systems on the test set. The
performance of an SVM system with bag-of-words
features was reported in Rink and Harabagiu (2010),
and the performance of the RNN and MV-RNN
models was reported in Socher et al. (2012). Our
model achieves an F1 score of 79.4% and outper-
forms the RNN model (74.8% F1) as well as the
simple SVM-based system (73.1% F1). More no-
</bodyText>
<figureCaption confidence="0.998562">
Figure 3: F1 vs Training iterations.
</figureCaption>
<bodyText confidence="0.999956047619048">
tably, the score of our model is competitive with that
of the MV-RNN model (79.1% F1), which is com-
putationally much more expensive. Readers are re-
ferred to Hermann and Blunsom (2013) for the dis-
cussion about the computational complexity of the
MV-RNN model. We improved the performance of
RNN models on the task without much increasing
the complexity. This is a significant practical advan-
tage of our model, although its expressive power is
not the same as that of the MV-RNN model.
Our model outperforms the RNN model with one
lexical and two semantic external features: POS
tags, WordNet hypernyms and named entity tags
(NER) of target word pairs (external features). The
MV-RNN model with external features shows bet-
ter performance than our model. An SVM with rich
lexical and semantic features (Rink and Harabagiu,
2010) also outperforms ours. Note, however, that
this is not a fair comparison because those mod-
els use rich external resources such as WordNet and
named entity tags.
</bodyText>
<subsectionHeader confidence="0.999671">
4.1 Contributions of Proposed Methods
</subsectionHeader>
<bodyText confidence="0.99999425">
We conducted additional experiments to quantify the
contributions of phrase categories, heads, paths and
averaging to our classification score. As shown in
Table 2, our model without phrase categories, heads
or paths still outperforms the RNN model with ex-
ternal features. On the other hand, our model with-
out averaging yields a lower score than the RNN
model with external features, though it is still bet-
</bodyText>
<page confidence="0.967501">
1375
</page>
<bodyText confidence="0.999976428571428">
ter than the RNN model alone. Without utiliz-
ing these four properties, our model obtained only
74.1% F1. These results indicate that syntactic and
task-specific information and averaging contribute
to the performance improvement. The improvement
is achieved by a simple modification of composi-
tional functions in RNN models.
</bodyText>
<subsectionHeader confidence="0.99808">
4.2 Effects of Averaging in Training
</subsectionHeader>
<bodyText confidence="0.99984575">
Figure 3 shows the training curves in terms of F1
scores. These curves clearly demonstrate that pa-
rameter averaging helps to stabilize the learning and
improve generalization capacity.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978181818182">
We have presented an averaged RNN model for se-
mantic relation classification. Our experimental re-
sults show that syntactic information such as phrase
categories and heads improves the performance, and
the task-specific weighting is also beneficial. The
results also demonstrate that averaging the model
parameters not only stabilizes the learning but also
improves the generalization capacity of the model.
As future work, we plan to combine deep learning
models with richer information such as predicate-
argument structures.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99988">
We thank the anonymous reviewers for their insight-
ful comments.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998560725">
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
In JMLR, 12:2493–2537.
Christoph Goller and Andreas K¨uchler. 1996. Learning
Task-Dependent Distributed Representations by Back-
propagation Through Structure. In ICNN.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano and Stan Szpakowicz.
2010. SemEval-2010 Task 8: Multi-Way Classication
of Semantic Relations Between Pairs of Nominals. In
SemEval 2010.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
ofSyntax in Vector Space Models of Compositional Se-
mantics. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever and Ruslan R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation offeature detectors. In arXiv:1207.0580.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, 34(1):35–80, MIT Press.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical and
Semantic Resources. In SemEval 2010.
Richard Socher, Brody Huval, Christopher D. Manning
and Andrew Y. Ng. 2012. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning and
Andrew Y. Ng. 2013. Parsing with Compositional
Vector Grammars. In ACL.
Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-
itas. 2010. A tutorial on stochastic approximation al-
gorithms for training Restricted Boltzmann Machines
and Deep BeliefNets. In ITA workshop.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006.
A Composite Kernel to Extract Relations between En-
tities with Both Flat and Structured Features. In COL-
ING/ACL.
</reference>
<page confidence="0.992402">
1376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432235">
<title confidence="0.998334">Simple Customization of Recursive Neural Networks for Semantic Relation Classification</title>
<author confidence="0.75107">Makoto Yoshimasa</author>
<author confidence="0.75107">Takashi University of Tokyo</author>
<affiliation confidence="0.712796">tsuruoka, University of Manchester, 131 Princess Street, Manchester, M1 7DN,</affiliation>
<email confidence="0.997149">makoto.miwa@manchester.ac.uk</email>
<abstract confidence="0.998804">In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classification show that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (almost) from Scratch. In</title>
<date>2011</date>
<booktitle>JMLR,</booktitle>
<pages>12--2493</pages>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (almost) from Scratch. In JMLR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning Task-Dependent Distributed Representations by Backpropagation Through Structure.</title>
<date>1996</date>
<booktitle>In ICNN.</booktitle>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning Task-Dependent Distributed Representations by Backpropagation Through Structure. In ICNN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 8: Multi-Way Classication of Semantic Relations Between Pairs of Nominals. In SemEval</booktitle>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classication of Semantic Relations Between Pairs of Nominals. In SemEval 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The Role ofSyntax in Vector Space Models of Compositional Semantics.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1366" citStr="Hermann and Blunsom, 2013" startWordPosition="181" endWordPosition="184"> that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as </context>
<context position="7605" citStr="Hermann and Blunsom, 2013" startWordPosition="1243" endWordPosition="1247">, blabel) is the set of our model parameters that should be learned. A is a vector of regularization parameters. 1373 To compute d(x), we can directly leverage any other nodes’ feature vectors in the same tree. We denote such additional feature vectors as x′i E Rd×1, and extend Eq. (1): ∑d(x) = softmax(W labelx + Wadd i i x′ i + blabel), where Wadd iE RK×d are weight matrices for additional features. We denote these matrices W add i as Wadd. We also add Wadd to θ: is efficiently computed via backpropagation through structure (Goller and K¨uchler, 1996). To minimize J(θ), we use batch L-BFGS1 (Hermann and Blunsom, 2013; Socher et al., 2012). 2.4 Averaging We use averaged model parameters 1 θ = T + 1 at test time, where θt is the vector of model parameters after t iterations of the L-BFGS optimization. Our preliminary experimental results suggest that averaging θ except We works well. 3 Experimental Settings We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. We used 13 phrase categories given by Enju. 3.1 Task: Semantic Relation Classification We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010). Following Socher et al. (2012), w</context>
<context position="12141" citStr="Hermann and Blunsom (2013)" startWordPosition="2040" endWordPosition="2043">rmance of our model and that of previously reported systems on the test set. The performance of an SVM system with bag-of-words features was reported in Rink and Harabagiu (2010), and the performance of the RNN and MV-RNN models was reported in Socher et al. (2012). Our model achieves an F1 score of 79.4% and outperforms the RNN model (74.8% F1) as well as the simple SVM-based system (73.1% F1). More noFigure 3: F1 vs Training iterations. tably, the score of our model is competitive with that of the MV-RNN model (79.1% F1), which is computationally much more expensive. Readers are referred to Hermann and Blunsom (2013) for the discussion about the computational complexity of the MV-RNN model. We improved the performance of RNN models on the task without much increasing the complexity. This is a significant practical advantage of our model, although its expressive power is not the same as that of the MV-RNN model. Our model outperforms the RNN model with one lexical and two semantic external features: POS tags, WordNet hypernyms and named entity tags (NER) of target word pairs (external features). The MV-RNN model with external features shows better performance than our model. An SVM with rich lexical and se</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The Role ofSyntax in Vector Space Models of Compositional Semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever and Ruslan</title>
<date>2012</date>
<booktitle>In arXiv:1207.0580.</booktitle>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing coadaptation offeature detectors. In arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature Forest Models for Probabilistic HPSG Parsing.</title>
<date>2008</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>34</volume>
<issue>1</issue>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7948" citStr="Miyao and Tsujii, 2008" startWordPosition="1304" endWordPosition="1307"> Wadd iE RK×d are weight matrices for additional features. We denote these matrices W add i as Wadd. We also add Wadd to θ: is efficiently computed via backpropagation through structure (Goller and K¨uchler, 1996). To minimize J(θ), we use batch L-BFGS1 (Hermann and Blunsom, 2013; Socher et al., 2012). 2.4 Averaging We use averaged model parameters 1 θ = T + 1 at test time, where θt is the vector of model parameters after t iterations of the L-BFGS optimization. Our preliminary experimental results suggest that averaging θ except We works well. 3 Experimental Settings We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. We used 13 phrase categories given by Enju. 3.1 Task: Semantic Relation Classification We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010). Following Socher et al. (2012), we regarded the task as a 19-class classification problem. There are 8,000 samples for training, and 2,717 for 1We used libLBFGS provided at http://www. chokkan.org/software/liblbfgs/. Figure 2: Classifying the relation between two entities. test. For the validation set, we randomly sampled 2,182 samples from the training data. To predict a c</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. In Computational Linguistics, 34(1):35–80, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources. In SemEval</title>
<date>2010</date>
<contexts>
<context position="11693" citStr="Rink and Harabagiu (2010)" startWordPosition="1959" endWordPosition="1962">ameters were initialized with 0. We tuned hyperparameters in our model using the validation set for each experimental setting. The hyperparameters include the regularization parameters for We, Wlr, Wlabel and Wadd, and the weights Q and &apos;y. For example, the best performance for our model with all the proposed methods was obtained with the values: 10−6, 10−4, 10−3, 10−3, 0.7 and 0.9 respectively. 4 Results and Discussion Table 1 shows the performance of our model and that of previously reported systems on the test set. The performance of an SVM system with bag-of-words features was reported in Rink and Harabagiu (2010), and the performance of the RNN and MV-RNN models was reported in Socher et al. (2012). Our model achieves an F1 score of 79.4% and outperforms the RNN model (74.8% F1) as well as the simple SVM-based system (73.1% F1). More noFigure 3: F1 vs Training iterations. tably, the score of our model is competitive with that of the MV-RNN model (79.1% F1), which is computationally much more expensive. Readers are referred to Hermann and Blunsom (2013) for the discussion about the computational complexity of the MV-RNN model. We improved the performance of RNN models on the task without much increasin</context>
</contexts>
<marker>Rink, Harabagiu, 2010</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2010. UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources. In SemEval 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1387" citStr="Socher et al., 2012" startWordPosition="185" endWordPosition="188">s and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as well, and whether a s</context>
<context position="4133" citStr="Socher et al. (2012)" startWordPosition="614" endWordPosition="617">ings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: A recursive representations of a phrase “a word vector” with POS tags of the words (DT, NN and NN respectively). For example, the two word-POS pairs “word NN” and “vector NN” with a syntactic category N are combined to represent the phrase “word vector”. forms better than standard RNN models. By averaging the model parameters, our model achieves performance competitive with the MV-RNN model in Socher et al. (2012), without using computationally expensive word-dependent matrices. 2 An Averaged RII Model with Syntax Our model is a supervised RNN that works on a binary syntactic tree. As our first step to leverage information available in the tree, we distinguish words with the same spelling but POS tags in the vector space. Our model also uses different weight matrices dependent on the phrase categories of child nodes (phrases or words) in combining phrases. Our model further weights those nodes that appear to be important. Compositional functions of our model follow those of the SU-RNN model in Socher e</context>
<context position="7627" citStr="Socher et al., 2012" startWordPosition="1248" endWordPosition="1251"> model parameters that should be learned. A is a vector of regularization parameters. 1373 To compute d(x), we can directly leverage any other nodes’ feature vectors in the same tree. We denote such additional feature vectors as x′i E Rd×1, and extend Eq. (1): ∑d(x) = softmax(W labelx + Wadd i i x′ i + blabel), where Wadd iE RK×d are weight matrices for additional features. We denote these matrices W add i as Wadd. We also add Wadd to θ: is efficiently computed via backpropagation through structure (Goller and K¨uchler, 1996). To minimize J(θ), we use batch L-BFGS1 (Hermann and Blunsom, 2013; Socher et al., 2012). 2.4 Averaging We use averaged model parameters 1 θ = T + 1 at test time, where θt is the vector of model parameters after t iterations of the L-BFGS optimization. Our preliminary experimental results suggest that averaging θ except We works well. 3 Experimental Settings We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. We used 13 phrase categories given by Enju. 3.1 Task: Semantic Relation Classification We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010). Following Socher et al. (2012), we regarded the task as</context>
<context position="9956" citStr="Socher et al. (2012)" startWordPosition="1660" endWordPosition="1663">., 2006), so our model also puts another weight γ E [0.5, 1] on phrases on the path, and 1 − γ on the others. When both child nodes are on the path or neither of them on the path, we set γ = 0.5. The two weight factors are summed up and divided by 2 to be the final weights αl and αr to combine the phrases. For example, we set αl = (1−β)+γ 2and αr = β+(1−γ) 2 when the right child node is the head and the left child node is on the path. 3.3 Initialization of Model Parameters and Tuning of Hyperparameters We initialized We with 50-dimensional word vectors3 trained with the model of Collobert et 2Socher et al. (2012) used richer features including words around entity pairs in their implementation. 3The word vectors are provided at http://ronan. collobert.com/senna/. We used the vectors without any modifications such as normalization. θ = (We,Wlr,b,W label,W add, blabel). The gradient of J(θ) ∂J(θ) ∑ ∂θ = x ∂E(x) ∂θ + λθ θt ∑T t=0 1374 Method F1 (%) Our model 79.4 RNN 74.8 MV-RNN 79.1 RNN w/ POS, WordNet, NER 77.6 MV-RNN w/ POS, WordNet, NER 82.4 SVM w/ bag-of-words 73.1 SVM w/ lexical and semantic features 82.2 Table 1: Comparison of our model with other methods on SemEval 2010 task 8. Method F1 (%) Our m</context>
<context position="11780" citStr="Socher et al. (2012)" startWordPosition="1975" endWordPosition="1978">set for each experimental setting. The hyperparameters include the regularization parameters for We, Wlr, Wlabel and Wadd, and the weights Q and &apos;y. For example, the best performance for our model with all the proposed methods was obtained with the values: 10−6, 10−4, 10−3, 10−3, 0.7 and 0.9 respectively. 4 Results and Discussion Table 1 shows the performance of our model and that of previously reported systems on the test set. The performance of an SVM system with bag-of-words features was reported in Rink and Harabagiu (2010), and the performance of the RNN and MV-RNN models was reported in Socher et al. (2012). Our model achieves an F1 score of 79.4% and outperforms the RNN model (74.8% F1) as well as the simple SVM-based system (73.1% F1). More noFigure 3: F1 vs Training iterations. tably, the score of our model is competitive with that of the MV-RNN model (79.1% F1), which is computationally much more expensive. Readers are referred to Hermann and Blunsom (2013) for the discussion about the computational complexity of the MV-RNN model. We improved the performance of RNN models on the task without much increasing the complexity. This is a significant practical advantage of our model, although its </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1409" citStr="Socher et al., 2013" startWordPosition="189" endWordPosition="192">eighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as well, and whether a similar improvement can</context>
<context position="4745" citStr="Socher et al. (2013)" startWordPosition="717" endWordPosition="720">. (2012), without using computationally expensive word-dependent matrices. 2 An Averaged RII Model with Syntax Our model is a supervised RNN that works on a binary syntactic tree. As our first step to leverage information available in the tree, we distinguish words with the same spelling but POS tags in the vector space. Our model also uses different weight matrices dependent on the phrase categories of child nodes (phrases or words) in combining phrases. Our model further weights those nodes that appear to be important. Compositional functions of our model follow those of the SU-RNN model in Socher et al. (2013). 2.1 Word-POS Vector Representations Our model simply assigns vector representations to word-POS pairs. For example, a word “caused” can be represented in two ways: “caused VBD” and “caused VBN”. The vectors are represented as column vectors in a matrix We E Rd×|�|, where d is the dimension of a vector and V is a set of all wordPOS pairs we consider. 2.2 Compositional Functions with Syntax In construction of parse trees, we associate each of the tree node with its d-dimensional vector representation computed from vector representations of its subtrees. For leaf nodes, we look up word-POS vect</context>
<context position="11043" citStr="Socher et al. (2013)" startWordPosition="1854" endWordPosition="1857">w/ lexical and semantic features 82.2 Table 1: Comparison of our model with other methods on SemEval 2010 task 8. Method F1 (%) Our model 79.4 Our model w/o phrase categories (PC) 77.7 Our model w/o head weights (HW) 78.8 Our model w/o path weights (PW) 78.7 Our model w/o averaging (AVE) 76.9 Our model w/o PC, HW, PW, AVE 74.1 Table 2: Contributions of syntactic and task-specific information and averaging. al. (2011), and Wlr with 2 + E, where I ∈ Rd×d is an identity matrix. Here, E is zero-mean gaussian random variable with a variance of 0.01. The initialization of Wlr is the same as that of Socher et al. (2013). The remaining model parameters were initialized with 0. We tuned hyperparameters in our model using the validation set for each experimental setting. The hyperparameters include the regularization parameters for We, Wlr, Wlabel and Wadd, and the weights Q and &apos;y. For example, the best performance for our model with all the proposed methods was obtained with the values: 10−6, 10−4, 10−3, 10−3, 0.7 and 0.9 respectively. 4 Results and Discussion Table 1 shows the performance of our model and that of previously reported systems on the test set. The performance of an SVM system with bag-of-words </context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning and Andrew Y. Ng. 2013. Parsing with Compositional Vector Grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Swersky</author>
<author>Bo Chen</author>
<author>Ben Marlin</author>
<author>Nando de Freitas</author>
</authors>
<title>A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep BeliefNets. In</title>
<date>2010</date>
<booktitle>ITA workshop.</booktitle>
<marker>Swersky, Chen, Marlin, de Freitas, 2010</marker>
<rawString>Kevin Swersky, Bo Chen, Ben Marlin and Nando de Freitas. 2010. A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep BeliefNets. In ITA workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
</authors>
<title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
<date>2006</date>
<booktitle>In COLING/ACL.</booktitle>
<contexts>
<context position="9344" citStr="Zhang et al., 2006" startWordPosition="1535" endWordPosition="1538"> directly connect features on any other nodes to the softmax classifier. In this work, we used three such internal features: two vector representations of target entities and one averaged vector representation of words between the entities2. 3.2 Weights on Phrases We tuned the weight αl (or αr) introduced in Section 2.2 for this particular task. There are two factors: syntactic heads and syntactic path between target entities. Our model puts a weight β E [0.5, 1] on head phrases, and 1 − β on the others. For relation classification tasks, syntactic paths between target entities are important (Zhang et al., 2006), so our model also puts another weight γ E [0.5, 1] on phrases on the path, and 1 − γ on the others. When both child nodes are on the path or neither of them on the path, we set γ = 0.5. The two weight factors are summed up and divided by 2 to be the final weights αl and αr to combine the phrases. For example, we set αl = (1−β)+γ 2and αr = β+(1−γ) 2 when the right child node is the head and the left child node is on the path. 3.3 Initialization of Model Parameters and Tuning of Hyperparameters We initialized We with 50-dimensional word vectors3 trained with the model of Collobert et 2Socher e</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features. In COLING/ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
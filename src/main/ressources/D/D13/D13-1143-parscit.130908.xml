<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014086">
<title confidence="0.99201">
Dependency language models for sentence completion
</title>
<author confidence="0.996792">
Joseph Gubbins Andreas Vlachos
</author>
<affiliation confidence="0.999466">
Computer Laboratory Computer Laboratory
University of Cambridge University of Cambridge
</affiliation>
<email confidence="0.993487">
jsg52@cam.ac.uk av308@cam.ac.uk
</email>
<sectionHeader confidence="0.998547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808105263158">
Sentence completion is a challenging seman-
tic modeling task in which models must
choose the most appropriate word from a
given set to complete a sentence. Although
a variety of language models have been ap-
plied to this task in previous work, none of the
existing approaches incorporate syntactic in-
formation. In this paper we propose to tackle
this task using a pair of simple language mod-
els in which the probability of a sentence is
estimated as the probability of the lexicalisa-
tion of a given syntactic dependency tree. We
apply our approach to the Microsoft Research
Sentence Completion Challenge and show that
it improves on n-gram language models by 8.7
percentage points, achieving the highest accu-
racy reported to date apart from neural lan-
guage models that are more complex and ex-
pensive to train.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999074681818182">
The verbal reasoning sections of standardised tests
such as the Scholastic Aptitude Test (SAT) fea-
ture problems where a partially complete sentence
is given and the candidate must choose the word
or phrase from a list of options which completes
the sentence in a logically consistent way. Sen-
tence completion is a challenging semantic mod-
elling problem. Systematic approaches for solving
such problems require models that can judge the
global coherence of sentences. Such measures of
global coherence may prove to be useful in various
applications, including machine translation and nat-
ural language generation (Zweig and Burges, 2012).
Most approaches to sentence completion employ
language models which use a window of immedi-
ate context around the missing word and choose the
word that results in the completed sentence with the
highest probability (Zweig and Burges, 2012; Mnih
and Teh, 2012). However, such language models
may fail to identify sentences that are locally co-
herent but are improbable due to long-range syntac-
tic/semantic dependencies. Consider, for example,
completing the sentence
I saw a tiger which was really very ...
with either fierce or talkative. A language model
relying on up to five words of immediate context
would ignore the crucial dependency between the
missing word and the noun tiger.
In this paper we tackle sentence completion us-
ing language models based on dependency gram-
mar. These models are similar to standard n-gram
language models, but instead of using the linear or-
dering of the words in the sentence, they generate
words along paths in the dependency tree of the sen-
tence. Unlike other approaches incorporating syntax
into language models (e.g., Chelba et al., 1997), our
models are relatively easy to train and estimate, and
can exploit standard smoothing methods. We apply
them to the Microsoft Research Sentence Comple-
tion Challenge (Zweig and Burges, 2012) and show
an improvement of 8.7 points in accuracy over n-
gram models, giving the best results to date for any
method apart from the more computationally de-
manding neural language models.
</bodyText>
<page confidence="0.900404">
1405
</page>
<note confidence="0.453117">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405–1410,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998843">
Figure 1: Dependency tree example
</figureCaption>
<sectionHeader confidence="0.970036" genericHeader="introduction">
2 Unlabelled Dependency Language
Models
</sectionHeader>
<bodyText confidence="0.998441666666667">
In dependency grammar, each word in a sentence is
associated with a node in a dependency tree (Figure
1). We define a dependency tree as a rooted, con-
nected, acyclic directed graph together with a map-
ping from the nodes of the tree to a set of gram-
matical relation labels R. We define a lexicalised
dependency tree as a dependency tree along with a
mapping from the vertices of the tree to a vocabulary
V.
We seek to model the probability distribution of
the lexicalisation of a given dependency tree. We
will use this as a language model; we neglect the
fact that a given lexicalised dependency tree can
correspond to more than one sentence due to vari-
ations in word order. Let ST be a lexicalised de-
pendency tree, where T is the unlexicalised tree and
let w1w2 ... wm be an ordering of the words corre-
sponding to a breadth-first enumeration of the tree.
In order for this representation to be unique, when
we parse a sentence, we will use the unique breadth-
first ordering where the children of any node appear
in the same order as they did in the sentence. We
define w0 to be a special symbol denoting the root
of the tree. We denote the grammatical relation be-
tween wk and its parent by 9k E R.
We apply the chain rule to the words in the tree in
the order of this breadth-first enumeration:
</bodyText>
<equation confidence="0.9959965">
P[wi|(wk)i−1
k=0,T] (1)
</equation>
<bodyText confidence="0.904234571428572">
Given a word wi, we define the ancestor sequence
A(w) to be the subsequence of (wk)i−1 k=0describ-
ing the path from the root node to the parent of
w, where each element of the sequence is the par-
ent of the next element. For example in Figure 1,
A(w8) = (w0, w1, w3). We make the following two
assumptions:
</bodyText>
<listItem confidence="0.908686">
• that each word wi is conditionally independent
</listItem>
<bodyText confidence="0.8696125">
of the words outside of its ancestor sequence
(wk)i−1
k=0nA(wi)c, given the ancestor sequence
A(wi);
</bodyText>
<listItem confidence="0.9565115">
• that the words are independent of the labels
(9k)mk=1.
</listItem>
<bodyText confidence="0.853287">
Using these assumptions, we can write the probabil-
ity as:
</bodyText>
<equation confidence="0.99963">
P[ST |T] = �m P[wi|A(wi)] (2)
i=1
</equation>
<bodyText confidence="0.999739714285714">
Given a training data corpus consisting of sen-
tences parsed into dependency trees, the maximum
likelihood estimator for the probability P[wi|A(wi)]
is given by the proportion of cases where the ances-
tor sequence A(wi) was followed by wi. Let C(·) be
the count of the number of observations of a pattern
in the corpus. We have
</bodyText>
<equation confidence="0.991224">
�P[wi|A(wi)] = C((A(wi), wi)) (3)
EwEV C((A(wi), w))
</equation>
<bodyText confidence="0.9999814">
As is the case for n-gram language models, we can’t
hope to observe all possible sequences of words no
matter how big the corpus. To deal with this data
sparsity issue, we take inspiration from n-gram mod-
els and assume a Markov property of order (N −1):
</bodyText>
<equation confidence="0.992106">
P[w|A(w)] = P[w|A(N−1)(w)] (4)
</equation>
<bodyText confidence="0.9817375">
where A(N−1)(w) denotes the sequence of up to
(N − 1) closest ancestors of w.
The maximum likelihood estimator for this prob-
ability is:
</bodyText>
<equation confidence="0.99244">
ffwi|A(N−1)(wi)] = C((A(N−1)(wi),wi))
</equation>
<bodyText confidence="0.848896">
l EwEV C((A(N−1)(wi), w))
We have arrived at a model which is quite similar
to n-gram language models. The main difference
</bodyText>
<equation confidence="0.9563845">
P[ST |T] = �m
i=1
</equation>
<page confidence="0.847995">
1406
</page>
<bodyText confidence="0.999746421052632">
is that each word in the tree can have several chil-
dren, while in the n-gram models it can only be fol-
lowed by one word. Thus the sum in the denomina-
tor above does not simplify to the count of the ances-
tor sequence in the way that it does for n-gram lan-
guage models. However, we can calculate and store
the denominators easily during training, so that we
do not need to sum over the vocabulary each time we
evaluate the estimator. We refer to this model as the
order N unlabelled dependency language model.
As is the case for n-gram language models, even
for low values of N, we will often encounter se-
quences (A(N−1)(w), w) which were not observed
in training. In order to avoid assigning zero prob-
ability to the entire sentence, we need to use a
smoothing method. We can use any of the smooth-
ing methods used for n-gram language models. For
simplicity, we use stupid backoff smoothing (Brants
et al., 2007).
</bodyText>
<sectionHeader confidence="0.968696" genericHeader="method">
3 Labelled Dependency Language Models
</sectionHeader>
<bodyText confidence="0.999174153846154">
We assumed above that the words are generated in-
dependently from the grammatical relations. How-
ever, we are likely to ignore valuable information in
doing so. To illustrate this point, consider the fol-
lowing pair of sentences:
You ate an apple
The dependency trees of the two sentences are
very similar, with only the grammatical relations be-
tween ate and its arguments differing. The unla-
belled dependency language model will assign the
same probability to both of the sentences as it ig-
nores the labels of grammatical relations. In order
to be able to distinguish between them, the nature
of the grammatical relations between the words in
the dependency tree needs to be incorporated in the
language model. We relax the assumption that the
words are independent of the labels of the parse tree,
assuming instead the each word is conditionally in-
dependent of the words and labels outside its ances-
tor path given the words and labels in its ancestor
path. We define G(wi) to be the sequence of gram-
matical relations between the successive elements of
(A(wi), wi). G(wi) is the sequence of grammatical
relations found on the path from the root node to
wi. For example, in Figure 1, G(w8) _ (91, 93, 98).
With our modified assumption we have:
</bodyText>
<equation confidence="0.994847">
m
P[ST |T] _ P[wi|A(wi), G(wi)] (5)
i=1
</equation>
<bodyText confidence="0.9976128">
Once again we apply a Markov assumption.
Let G(N−1)(w) be the sequence of grammat-
ical relations between successive elements of
(A(N−1)(w), w). With an (N − 1)1h order Markov
assumption, we have:
</bodyText>
<equation confidence="0.995852">
m
P[ST |T] _ P[wi|A(N−1)(wi), G(N−1)(wi)]
i=1
</equation>
<bodyText confidence="0.99947925">
The maximum likelihood estimator for the probabil-
ity is once again given by the ratio of the counts of
labelled paths. We refer to this model as the order
N labelled dependency language model.
</bodyText>
<sectionHeader confidence="0.992973" genericHeader="method">
4 Dataset and Implementation Details
</sectionHeader>
<bodyText confidence="0.999627956521739">
We carried out experiments using the Microsoft
Research Sentence (MSR) Completion Challenge
(Zweig and Burges, 2012). This consists of a set
of 1,040 sentence completion problems taken from
five of the Sherlock Holmes novels by Arthur Co-
nan Doyle. Each problem consists of a sentence
in which one word has been removed and replaced
with a blank and a set of 5 candidate words to com-
plete the sentence. The task is to choose the can-
didate word which, when inserted into the blank,
gives the most probable complete sentence. The set
of candidates consists of the original word and 4
imposter words with similar distributional statistics.
Human judges were tasked with choosing imposter
words which would lead to grammatically correct
sentences and such that, with some thought, the cor-
rect answer should be unambiguous. The training
data set consists of 522 19th century novels from
Project Gutenberg. We parsed the training data us-
ing the Nivre arc-eager deterministic dependency
parsing algorithm (Nivre and Scholz, 2004) as im-
plemented in MaltParser (Nivre et al., 2006). We
trained order N labelled and unabelled dependency
</bodyText>
<figure confidence="0.951499461538461">
nsubj
dobj
det
det nsubj dobj
An apple ate you
1407
ROOT
I saw a tiger which was really very PARSE
a. fierce
b. talkative
I saw a tiger which was really very fierce
P[“fierce”] = P[saw|ROOT] × P[I|ROOT, saw] × P[tiger|ROOT, saw] × P[a|saw, tiger] × P[fierce|saw, tiger]
×P[which|tiger, fierce] × P[was|tiger, fierce] × P[really|tiger, fierce] × P[very|tiger, fierce]
</figure>
<figureCaption confidence="0.995637">
Figure 2: Procedure for evaluating sentence completion problems
</figureCaption>
<table confidence="0.989224">
EVALUATE PROBABILITY
N Unlab-SB Lab-SB Ngm-SB Ngm-KN
2 43.2% 43.0% 28.1% 27.8%
3 48.3% 49.8% 38.5% 38.4%
4 48.3% 50.0% 40.8% 41.1%
5 47.4% 49.9% 41.3% 40.8%
</table>
<tableCaption confidence="0.999918">
Table 1: Summary of results for Sentence Completion
</tableCaption>
<bodyText confidence="0.9999494">
language models for 2 G N G 5. Words which
occured fewer than 5 times were excluded from the
vocabulary. In order to have a baseline to compare
against, we also trained n-gram language models
with Kneser-Ney smoothing and stupid backoff us-
ing the Berkeley Language Modeling Toolkit (Pauls
and Klein, 2011).
To test a given language model, we calculated the
scores it assigned to each candidate sentence and
chose the completion with the highest score. For
the dependency language models we parsed the sen-
tence with each of the 5 possible completions and
calculated the probability in each case. Figure 2 il-
lustrates an example of this process for the order 3
unlabelled model.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999835666666666">
Table 1 summarises the results. Unlab-SB is the or-
der N unlabelled dependency language model with
Stupid Backoff, Lab-SB is the order N labelled
dependency language model with Stupid Backoff,
Ngm-SB is the n-gram language model with Stupid
Backoff and Ngm-KN is the interpolated Kneser-
Ney smoothed n-gram language model.
Both of the dependency language models outper-
fomed the n-gram language models by a substantial
</bodyText>
<table confidence="0.998769375">
Method Accuracy
n-grams (Various) 39% - 41%
Skip-grams (Mikolov) 48%
Unlabelled Dependency Model 48.3%
Average LSA (Zweig) 49%
Labelled Dependency Model 50.0%
Log-bilinear Neural LM (Mnih) 54.7%
Recurrent Neural LM (Mikolov) 55.4%
</table>
<tableCaption confidence="0.999929">
Table 2: Comparison against previous results
</tableCaption>
<bodyText confidence="0.999759739130435">
margin for all orders considered. The best result was
achieved by the order 4 labelled dependency model
which is 8.7 points in accuracy better than the best n-
gram model. Furthermore, the labelled dependency
models outperformed their unlabelled counterparts
for every order except 2.
Comparing against previous work (Table 2), the
performance of our n-gram baseline is slightly better
than the accuracy reported by other authors (Mnih
and Teh, 2012; Zweig et al., 2012) for models of this
type. The performance of the labelled dependency
language model is superior to the results reported
for any single model method, apart from those rely-
ing on neural language models (Mnih and Teh, 2012;
Mikolov et al., 2013) . However the superior perfor-
mance of neural networks comes at the cost of long
training times. The best result achieved in Zweig et
al. (2012) using a single method was 49% accuracy
with a method based on LSA. Mikolov et al. (2013)
also reported accuracy of 48% for a method called
skip-grams, which uses a log-linear classifier to pre-
dict which words will appear close to each other in
sentences.
</bodyText>
<page confidence="0.99425">
1408
</page>
<sectionHeader confidence="0.999096" genericHeader="related work">
6 Related Work and Discussion
</sectionHeader>
<bodyText confidence="0.999974395348837">
The best-known language model based on depen-
dency parsing is that of Chelba et al. (1997). This
model writes the probability in the familiar left-to-
right chain rule decomposition in the linear order
of the sentence, conditioning the probability of the
next word on the linear trigram context, as well as
some part of the dependency graph information re-
lating to the words on its left. The language mod-
els we propose are far simpler to train and compute.
A somewhat similar model to our unlabelled depen-
dency language model was proposed in Graham and
van Genabith (2010). However they seem to have
used different probability estimators which ignore
the fact that each node in the dependency tree can
have multiple children. Other research on syntac-
tic language modelling has focused on using phrase
structure grammars (Pauls and Klein, 2012; Char-
niak, 2001; Roark, 2001; Hall and Johnson, 2003).
The linear complexity of deterministic dependency
parsing makes dependency language models such as
ours more scalable than these approaches.
The most similar task to sentence completion is
lexical substitution (McCarthy and Navigli, 2007).
The main difference between them is that in the lat-
ter the word to be substituted provides a very im-
portant clue in choosing the right candidate, while
in sentence completion this is not available. An-
other related task is selectional preference modeling
(S´eaghdha, 2010; Ritter et al., 2010), where the aim
is to assess the plausibility of possible syntactic ar-
guments for a given word.
The dependency language models described in
this paper assign probabilities to full sentences. Lan-
guage models which require full sentences can be
used in automatic speech recognition (ASR) and ma-
chine translation (MT). The approach is to use a con-
ventional ASR or MT decoder to produce an N-best
list of the most likely candidate sentences and then
re-score these with the language model. This was
done by Chelba et al. (1997) for ASR using a de-
pendency language model and by Pauls and Klein
(2011) for MT using a PSG-based syntactic lan-
guage model.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999884">
We have proposed a pair of language models which
are probabilistic models for the lexicalisation of a
given dependency tree. These models are simple
to train and evaluate and are scalable to large data
sets. We applied them to the Microsoft Research
Sentence Completion Challenge. They performed
substantially better than n-gram language models,
achieving the best result reported for any single
method except for the more expensive and complex
to train neural language models.
</bodyText>
<sectionHeader confidence="0.998229" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.934145833333333">
Andreas Vlachos is funded by the European
Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no.
270019 (SPACEBOOK project www.spacebook-
project.eu). The authors would like to thank Dr.
Stephen Clark for his helpful comments.
</bodyText>
<sectionHeader confidence="0.99806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999672222222222">
Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och,
and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 858–867. Association for
Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting on Association for Computational Linguis-
tics, pages 124–131. Association for Computational
Linguistics.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, et al. 1997. Structure and performance of a
dependency language model. In Proceedings of Eu-
rospeech, volume 5, pages 2775–2778.
Yvette Graham and Josef van Genabith. 2010. Deep syn-
tax language models and statistical machine transla-
tion. In Proceedings of the 4th Workshop on Syntax
and Structure in Statistical Translation, pages 118–
126. Coling 2010 Organizing Committee, August.
Keith Hall and Mark Johnson. 2003. Language mod-
eling using efficient best-first bottom-up parsing. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 507–512. IEEE.
</reference>
<page confidence="0.902111">
1409
</page>
<reference confidence="0.999489566037736">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Andriy Mnih and Yee W Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of the 29th International Con-
ference on Machine Learning, pages 1751–1758.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
the 20th International Conference on Computational
Linguistics, page 64. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC, volume 6, pages
2216–2219.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N-Gram Language Models. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 258–267. Association for Computational Lin-
guistics.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, pages
959–968. Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 424–434.
Association for Computational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.
Diarmuid O S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435–444. Association for Computa-
tional Linguistics.
Geoffrey Zweig and Chris JC Burges. 2012. A challenge
set for advancing language modeling. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever Re-
ally Replace the N-gram Model? On the Future of
Language Modeling for HLT, pages 29–36. Associa-
tion for Computational Linguistics.
Geoffrey Zweig, John C Platt, Christopher Meek,
Christopher JC Burges, Ainur Yessenalina, and Qiang
Liu. 2012. Computational approaches to sentence
completion. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 601–610. Association
for Computational Linguistics.
</reference>
<page confidence="0.990001">
1410
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840724">
<title confidence="0.99906">Dependency language models for sentence completion</title>
<author confidence="0.999529">Joseph Gubbins Andreas Vlachos</author>
<affiliation confidence="0.999957">Computer Laboratory Computer Laboratory University of Cambridge University of Cambridge</affiliation>
<email confidence="0.939952">jsg52@cam.ac.ukav308@cam.ac.uk</email>
<abstract confidence="0.9946428">Sentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence. Although a variety of language models have been applied to this task in previous work, none of the existing approaches incorporate syntactic information. In this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisation of a given syntactic dependency tree. We apply our approach to the Microsoft Research Sentence Completion Challenge and show that it improves on n-gram language models by 8.7 percentage points, achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large Language Models in Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7222" citStr="Brants et al., 2007" startWordPosition="1221" endWordPosition="1224">late and store the denominators easily during training, so that we do not need to sum over the vocabulary each time we evaluate the estimator. We refer to this model as the order N unlabelled dependency language model. As is the case for n-gram language models, even for low values of N, we will often encounter sequences (A(N−1)(w), w) which were not observed in training. In order to avoid assigning zero probability to the entire sentence, we need to use a smoothing method. We can use any of the smoothing methods used for n-gram language models. For simplicity, we use stupid backoff smoothing (Brants et al., 2007). 3 Labelled Dependency Language Models We assumed above that the words are generated independently from the grammatical relations. However, we are likely to ignore valuable information in doing so. To illustrate this point, consider the following pair of sentences: You ate an apple The dependency trees of the two sentences are very similar, with only the grammatical relations between ate and its arguments differing. The unlabelled dependency language model will assign the same probability to both of the sentences as it ignores the labels of grammatical relations. In order to be able to distin</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>124--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14159" citStr="Charniak, 2001" startWordPosition="2367" endWordPosition="2369">ity of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 124–131. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>David Engle</author>
<author>Frederick Jelinek</author>
<author>Victor Jimenez</author>
<author>Sanjeev Khudanpur</author>
<author>Lidia Mangu</author>
<author>Harry Printz</author>
</authors>
<title>Eric Ristad,</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<location>Ronald Rosenfeld, Andreas</location>
<contexts>
<context position="2747" citStr="Chelba et al., 1997" startWordPosition="431" endWordPosition="434">entence I saw a tiger which was really very ... with either fierce or talkative. A language model relying on up to five words of immediate context would ignore the crucial dependency between the missing word and the noun tiger. In this paper we tackle sentence completion using language models based on dependency grammar. These models are similar to standard n-gram language models, but instead of using the linear ordering of the words in the sentence, they generate words along paths in the dependency tree of the sentence. Unlike other approaches incorporating syntax into language models (e.g., Chelba et al., 1997), our models are relatively easy to train and estimate, and can exploit standard smoothing methods. We apply them to the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012) and show an improvement of 8.7 points in accuracy over ngram models, giving the best results to date for any method apart from the more computationally demanding neural language models. 1405 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405–1410, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Depe</context>
<context position="13393" citStr="Chelba et al. (1997)" startWordPosition="2239" endWordPosition="2242">d, apart from those relying on neural language models (Mnih and Teh, 2012; Mikolov et al., 2013) . However the superior performance of neural networks comes at the cost of long training times. The best result achieved in Zweig et al. (2012) using a single method was 49% accuracy with a method based on LSA. Mikolov et al. (2013) also reported accuracy of 48% for a method called skip-grams, which uses a log-linear classifier to predict which words will appear close to each other in sentences. 1408 6 Related Work and Discussion The best-known language model based on dependency parsing is that of Chelba et al. (1997). This model writes the probability in the familiar left-toright chain rule decomposition in the linear order of the sentence, conditioning the probability of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency</context>
<context position="15254" citStr="Chelba et al. (1997)" startWordPosition="2543" endWordPosition="2546">e. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess the plausibility of possible syntactic arguments for a given word. The dependency language models described in this paper assign probabilities to full sentences. Language models which require full sentences can be used in automatic speech recognition (ASR) and machine translation (MT). The approach is to use a conventional ASR or MT decoder to produce an N-best list of the most likely candidate sentences and then re-score these with the language model. This was done by Chelba et al. (1997) for ASR using a dependency language model and by Pauls and Klein (2011) for MT using a PSG-based syntactic language model. 7 Conclusion We have proposed a pair of language models which are probabilistic models for the lexicalisation of a given dependency tree. These models are simple to train and evaluate and are scalable to large data sets. We applied them to the Microsoft Research Sentence Completion Challenge. They performed substantially better than n-gram language models, achieving the best result reported for any single method except for the more expensive and complex to train neural la</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, 1997</marker>
<rawString>Ciprian Chelba, David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, et al. 1997. Structure and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 2775–2778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Josef van Genabith</author>
</authors>
<title>Deep syntax language models and statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation, pages 118– 126. Coling 2010 Organizing Committee,</booktitle>
<marker>Graham, van Genabith, 2010</marker>
<rawString>Yvette Graham and Josef van Genabith. 2010. Deep syntax language models and statistical machine translation. In Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation, pages 118– 126. Coling 2010 Organizing Committee, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Language modeling using efficient best-first bottom-up parsing.</title>
<date>2003</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>507--512</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="14197" citStr="Hall and Johnson, 2003" startWordPosition="2372" endWordPosition="2375">inear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess the plausibility of possible syntacti</context>
</contexts>
<marker>Hall, Johnson, 2003</marker>
<rawString>Keith Hall and Mark Johnson. 2003. Language modeling using efficient best-first bottom-up parsing. In IEEE Workshop on Automatic Speech Recognition and Understanding, pages 507–512. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="12869" citStr="Mikolov et al., 2013" startWordPosition="2146" endWordPosition="2149">pendency model which is 8.7 points in accuracy better than the best ngram model. Furthermore, the labelled dependency models outperformed their unlabelled counterparts for every order except 2. Comparing against previous work (Table 2), the performance of our n-gram baseline is slightly better than the accuracy reported by other authors (Mnih and Teh, 2012; Zweig et al., 2012) for models of this type. The performance of the labelled dependency language model is superior to the results reported for any single model method, apart from those relying on neural language models (Mnih and Teh, 2012; Mikolov et al., 2013) . However the superior performance of neural networks comes at the cost of long training times. The best result achieved in Zweig et al. (2012) using a single method was 49% accuracy with a method based on LSA. Mikolov et al. (2013) also reported accuracy of 48% for a method called skip-grams, which uses a log-linear classifier to predict which words will appear close to each other in sentences. 1408 6 Related Work and Discussion The best-known language model based on dependency parsing is that of Chelba et al. (1997). This model writes the probability in the familiar left-toright chain rule </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee W Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="1929" citStr="Mnih and Teh, 2012" startWordPosition="298" endWordPosition="301">sistent way. Sentence completion is a challenging semantic modelling problem. Systematic approaches for solving such problems require models that can judge the global coherence of sentences. Such measures of global coherence may prove to be useful in various applications, including machine translation and natural language generation (Zweig and Burges, 2012). Most approaches to sentence completion employ language models which use a window of immediate context around the missing word and choose the word that results in the completed sentence with the highest probability (Zweig and Burges, 2012; Mnih and Teh, 2012). However, such language models may fail to identify sentences that are locally coherent but are improbable due to long-range syntactic/semantic dependencies. Consider, for example, completing the sentence I saw a tiger which was really very ... with either fierce or talkative. A language model relying on up to five words of immediate context would ignore the crucial dependency between the missing word and the noun tiger. In this paper we tackle sentence completion using language models based on dependency grammar. These models are similar to standard n-gram language models, but instead of usi</context>
<context position="12606" citStr="Mnih and Teh, 2012" startWordPosition="2101" endWordPosition="2104">erage LSA (Zweig) 49% Labelled Dependency Model 50.0% Log-bilinear Neural LM (Mnih) 54.7% Recurrent Neural LM (Mikolov) 55.4% Table 2: Comparison against previous results margin for all orders considered. The best result was achieved by the order 4 labelled dependency model which is 8.7 points in accuracy better than the best ngram model. Furthermore, the labelled dependency models outperformed their unlabelled counterparts for every order except 2. Comparing against previous work (Table 2), the performance of our n-gram baseline is slightly better than the accuracy reported by other authors (Mnih and Teh, 2012; Zweig et al., 2012) for models of this type. The performance of the labelled dependency language model is superior to the results reported for any single model method, apart from those relying on neural language models (Mnih and Teh, 2012; Mikolov et al., 2013) . However the superior performance of neural networks comes at the cost of long training times. The best result achieved in Zweig et al. (2012) using a single method was 49% accuracy with a method based on LSA. Mikolov et al. (2013) also reported accuracy of 48% for a method called skip-grams, which uses a log-linear classifier to pre</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee W Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th International Conference on Machine Learning, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>page</pages>
<contexts>
<context position="10035" citStr="Nivre and Scholz, 2004" startWordPosition="1691" endWordPosition="1694">ce. The task is to choose the candidate word which, when inserted into the blank, gives the most probable complete sentence. The set of candidates consists of the original word and 4 imposter words with similar distributional statistics. Human judges were tasked with choosing imposter words which would lead to grammatically correct sentences and such that, with some thought, the correct answer should be unambiguous. The training data set consists of 522 19th century novels from Project Gutenberg. We parsed the training data using the Nivre arc-eager deterministic dependency parsing algorithm (Nivre and Scholz, 2004) as implemented in MaltParser (Nivre et al., 2006). We trained order N labelled and unabelled dependency nsubj dobj det det nsubj dobj An apple ate you 1407 ROOT I saw a tiger which was really very PARSE a. fierce b. talkative I saw a tiger which was really very fierce P[“fierce”] = P[saw|ROOT] × P[I|ROOT, saw] × P[tiger|ROOT, saw] × P[a|saw, tiger] × P[fierce|saw, tiger] ×P[which|tiger, fierce] × P[was|tiger, fierce] × P[really|tiger, fierce] × P[very|tiger, fierce] Figure 2: Procedure for evaluating sentence completion problems EVALUATE PROBABILITY N Unlab-SB Lab-SB Ngm-SB Ngm-KN 2 43.2% 43.</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of the 20th International Conference on Computational Linguistics, page 64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>2216--2219</pages>
<contexts>
<context position="10085" citStr="Nivre et al., 2006" startWordPosition="1700" endWordPosition="1703">en inserted into the blank, gives the most probable complete sentence. The set of candidates consists of the original word and 4 imposter words with similar distributional statistics. Human judges were tasked with choosing imposter words which would lead to grammatically correct sentences and such that, with some thought, the correct answer should be unambiguous. The training data set consists of 522 19th century novels from Project Gutenberg. We parsed the training data using the Nivre arc-eager deterministic dependency parsing algorithm (Nivre and Scholz, 2004) as implemented in MaltParser (Nivre et al., 2006). We trained order N labelled and unabelled dependency nsubj dobj det det nsubj dobj An apple ate you 1407 ROOT I saw a tiger which was really very PARSE a. fierce b. talkative I saw a tiger which was really very fierce P[“fierce”] = P[saw|ROOT] × P[I|ROOT, saw] × P[tiger|ROOT, saw] × P[a|saw, tiger] × P[fierce|saw, tiger] ×P[which|tiger, fierce] × P[was|tiger, fierce] × P[really|tiger, fierce] × P[very|tiger, fierce] Figure 2: Procedure for evaluating sentence completion problems EVALUATE PROBABILITY N Unlab-SB Lab-SB Ngm-SB Ngm-KN 2 43.2% 43.0% 28.1% 27.8% 3 48.3% 49.8% 38.5% 38.4% 4 48.3% 5</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, volume 6, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and Smaller N-Gram Language Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>258--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11085" citStr="Pauls and Klein, 2011" startWordPosition="1863" endWordPosition="1866">ally|tiger, fierce] × P[very|tiger, fierce] Figure 2: Procedure for evaluating sentence completion problems EVALUATE PROBABILITY N Unlab-SB Lab-SB Ngm-SB Ngm-KN 2 43.2% 43.0% 28.1% 27.8% 3 48.3% 49.8% 38.5% 38.4% 4 48.3% 50.0% 40.8% 41.1% 5 47.4% 49.9% 41.3% 40.8% Table 1: Summary of results for Sentence Completion language models for 2 G N G 5. Words which occured fewer than 5 times were excluded from the vocabulary. In order to have a baseline to compare against, we also trained n-gram language models with Kneser-Ney smoothing and stupid backoff using the Berkeley Language Modeling Toolkit (Pauls and Klein, 2011). To test a given language model, we calculated the scores it assigned to each candidate sentence and chose the completion with the highest score. For the dependency language models we parsed the sentence with each of the 5 possible completions and calculated the probability in each case. Figure 2 illustrates an example of this process for the order 3 unlabelled model. 5 Results Table 1 summarises the results. Unlab-SB is the order N unlabelled dependency language model with Stupid Backoff, Lab-SB is the order N labelled dependency language model with Stupid Backoff, Ngm-SB is the n-gram langu</context>
<context position="15326" citStr="Pauls and Klein (2011)" startWordPosition="2557" endWordPosition="2560"> 2010; Ritter et al., 2010), where the aim is to assess the plausibility of possible syntactic arguments for a given word. The dependency language models described in this paper assign probabilities to full sentences. Language models which require full sentences can be used in automatic speech recognition (ASR) and machine translation (MT). The approach is to use a conventional ASR or MT decoder to produce an N-best list of the most likely candidate sentences and then re-score these with the language model. This was done by Chelba et al. (1997) for ASR using a dependency language model and by Pauls and Klein (2011) for MT using a PSG-based syntactic language model. 7 Conclusion We have proposed a pair of language models which are probabilistic models for the lexicalisation of a given dependency tree. These models are simple to train and evaluate and are scalable to large data sets. We applied them to the Microsoft Research Sentence Completion Challenge. They performed substantially better than n-gram language models, achieving the best result reported for any single method except for the more expensive and complex to train neural language models. Acknowledgments Andreas Vlachos is funded by the European</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and Smaller N-Gram Language Models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>959--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14143" citStr="Pauls and Klein, 2012" startWordPosition="2363" endWordPosition="2366">nditioning the probability of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the </context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>Adam Pauls and Dan Klein. 2012. Large-scale syntactic language modeling with treelets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 959–968. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="14172" citStr="Roark, 2001" startWordPosition="2370" endWordPosition="2371">word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess the plausibi</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435–444. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>Chris JC Burges</author>
</authors>
<title>A challenge set for advancing language modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>29--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1669" citStr="Zweig and Burges, 2012" startWordPosition="256" endWordPosition="259">easoning sections of standardised tests such as the Scholastic Aptitude Test (SAT) feature problems where a partially complete sentence is given and the candidate must choose the word or phrase from a list of options which completes the sentence in a logically consistent way. Sentence completion is a challenging semantic modelling problem. Systematic approaches for solving such problems require models that can judge the global coherence of sentences. Such measures of global coherence may prove to be useful in various applications, including machine translation and natural language generation (Zweig and Burges, 2012). Most approaches to sentence completion employ language models which use a window of immediate context around the missing word and choose the word that results in the completed sentence with the highest probability (Zweig and Burges, 2012; Mnih and Teh, 2012). However, such language models may fail to identify sentences that are locally coherent but are improbable due to long-range syntactic/semantic dependencies. Consider, for example, completing the sentence I saw a tiger which was really very ... with either fierce or talkative. A language model relying on up to five words of immediate con</context>
<context position="2941" citStr="Zweig and Burges, 2012" startWordPosition="461" endWordPosition="464">the missing word and the noun tiger. In this paper we tackle sentence completion using language models based on dependency grammar. These models are similar to standard n-gram language models, but instead of using the linear ordering of the words in the sentence, they generate words along paths in the dependency tree of the sentence. Unlike other approaches incorporating syntax into language models (e.g., Chelba et al., 1997), our models are relatively easy to train and estimate, and can exploit standard smoothing methods. We apply them to the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012) and show an improvement of 8.7 points in accuracy over ngram models, giving the best results to date for any method apart from the more computationally demanding neural language models. 1405 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405–1410, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Dependency tree example 2 Unlabelled Dependency Language Models In dependency grammar, each word in a sentence is associated with a node in a dependency tree (Figure 1). We define a dependency tree </context>
<context position="9130" citStr="Zweig and Burges, 2012" startWordPosition="1541" endWordPosition="1544">ST |T] _ P[wi|A(wi), G(wi)] (5) i=1 Once again we apply a Markov assumption. Let G(N−1)(w) be the sequence of grammatical relations between successive elements of (A(N−1)(w), w). With an (N − 1)1h order Markov assumption, we have: m P[ST |T] _ P[wi|A(N−1)(wi), G(N−1)(wi)] i=1 The maximum likelihood estimator for the probability is once again given by the ratio of the counts of labelled paths. We refer to this model as the order N labelled dependency language model. 4 Dataset and Implementation Details We carried out experiments using the Microsoft Research Sentence (MSR) Completion Challenge (Zweig and Burges, 2012). This consists of a set of 1,040 sentence completion problems taken from five of the Sherlock Holmes novels by Arthur Conan Doyle. Each problem consists of a sentence in which one word has been removed and replaced with a blank and a set of 5 candidate words to complete the sentence. The task is to choose the candidate word which, when inserted into the blank, gives the most probable complete sentence. The set of candidates consists of the original word and 4 imposter words with similar distributional statistics. Human judges were tasked with choosing imposter words which would lead to gramma</context>
</contexts>
<marker>Zweig, Burges, 2012</marker>
<rawString>Geoffrey Zweig and Chris JC Burges. 2012. A challenge set for advancing language modeling. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 29–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
<author>Christopher JC Burges</author>
<author>Ainur Yessenalina</author>
<author>Qiang Liu</author>
</authors>
<title>Computational approaches to sentence completion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>601--610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12627" citStr="Zweig et al., 2012" startWordPosition="2105" endWordPosition="2108">% Labelled Dependency Model 50.0% Log-bilinear Neural LM (Mnih) 54.7% Recurrent Neural LM (Mikolov) 55.4% Table 2: Comparison against previous results margin for all orders considered. The best result was achieved by the order 4 labelled dependency model which is 8.7 points in accuracy better than the best ngram model. Furthermore, the labelled dependency models outperformed their unlabelled counterparts for every order except 2. Comparing against previous work (Table 2), the performance of our n-gram baseline is slightly better than the accuracy reported by other authors (Mnih and Teh, 2012; Zweig et al., 2012) for models of this type. The performance of the labelled dependency language model is superior to the results reported for any single model method, apart from those relying on neural language models (Mnih and Teh, 2012; Mikolov et al., 2013) . However the superior performance of neural networks comes at the cost of long training times. The best result achieved in Zweig et al. (2012) using a single method was 49% accuracy with a method based on LSA. Mikolov et al. (2013) also reported accuracy of 48% for a method called skip-grams, which uses a log-linear classifier to predict which words will</context>
</contexts>
<marker>Zweig, Platt, Meek, Burges, Yessenalina, Liu, 2012</marker>
<rawString>Geoffrey Zweig, John C Platt, Christopher Meek, Christopher JC Burges, Ainur Yessenalina, and Qiang Liu. 2012. Computational approaches to sentence completion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 601–610. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000371">
<title confidence="0.989386">
Unsupervised Induction of Contingent Event Pairs from Film Scenes
</title>
<author confidence="0.998261">
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina, Reid Swanson and Marilyn A. Walker
</author>
<affiliation confidence="0.995874">
Natural Language and Dialogue Systems Lab
Department of Computer Science, University of California, Santa Cruz
</affiliation>
<address confidence="0.954899">
Santa Cruz, CA, 95064
</address>
<email confidence="0.97394">
{zhu, elahe, mlarissa, reid, maw}@soe.ucsc.edu
</email>
<sectionHeader confidence="0.99839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719739130435">
Human engagement in narrative is partially
driven by reasoning about discourse relations
between narrative events, and the expectations
about what is likely to happen next that results
from such reasoning. Researchers in NLP
have tackled modeling such expectations from
a range of perspectives, including treating it as
the inference of the CONTINGENT discourse
relation, or as a type of common-sense causal
reasoning. Our approach is to model likeli-
hood between events by drawing on several of
these lines of previous work. We implement
and evaluate different unsupervised methods
for learning event pairs that are likely to be
CONTINGENT on one another. We refine event
pairs that we learn from a corpus of film scene
descriptions utilizing web search counts, and
evaluate our results by collecting human judg-
ments of contingency. Our results indicate that
the use of web search counts increases the av-
erage accuracy of our best method to 85.64%
over a baseline of 50%, as compared to an av-
erage accuracy of 75.15% without web search.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.801125766666667">
Human engagement in narrative is partially driven
by reasoning about discourse relations between nar-
rative events, and the expectations about what is
likely to happen next that results from such reason-
ing (Gerrig, 1993; Graesser et al., 1994; Lehnert,
1981; Goyal et al., 2010). Thus discourse relations
are one of the primary means to structure narrative
in genres as diverse as weblogs, search queries, sto-
ries, film scripts and news articles (Chambers and
Jurafsky, 2009; Manshadi et al., 2008; Gordon and
Swanson, 2009; Gordon et al., 2011; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al., 2011).
DOUGLAS QUAIL and his wife KRISTEN, are
asleep in bed.
Gradually the room lights brighten. the clock chimes
and begins speaking in a soft, feminine voice.
They don’t budge. Shortly, the clock chimes again.
Quail’s wife stirs. Maddeningly, the clock chimes a
third time.
CLOCK (continuing)Tick, tock –.
Quail reaches out and shuts the clock off. Then he sits
up in bed.
He swings his legs out from under the covers and sits
on the edge of the bed. He puts on his glasses and sits,
lost in thought.
He is a good-looking but conventional man in his early
thirties. He seems rather in awe of his wife, who is
attractive and rather off-hand towards him.
Kirsten pulls on her robe, lights a cigarette, sits fishing
for her slippers.
</bodyText>
<figureCaption confidence="0.99897">
Figure 1: Opening Scene from Total Recall
</figureCaption>
<bodyText confidence="0.9997358">
Recent work in NLP has tackled the inference of
relations between events from a broad range of per-
spectives: (1) as inference of a discourse relations
(e.g. the Penn Discourse Treebank (PDTB) CON-
TINGENT relation and its specializations); (2) as a
type of common sense reasoning; (3) as part of text
understanding to support question-answering; and
(4) as way of learning script-like or plot-like knowl-
edge structures. All these lines of work aim to model
narrative understanding, i.e. to enable systems to in-
fer which events are likely to have happened even
though they have not been mentioned in the text
(Schank et al., 1977), and which events are likely
to happen in the future. Such knowledge has prac-
tical applications in commonsense reasoning, infor-
</bodyText>
<page confidence="0.976289">
369
</page>
<note confidence="0.734259">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999870030303031">
mation retrieval, question answering, narrative un-
derstanding and inferring discourse relations.
We model this likelihood between events by
drawing on the PTDB’s general definition of the
CONTINGENT relation, which encapsulates relations
elsewhere called CAUSE, CONDITION and ENABLE-
MENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et
al., 2009; Louis et al., 2010). Our aim in this paper
is to implement and evaluate a range of different un-
supervised methods for learning event pairs that are
likely to be CONTINGENT on one another.
We first utilize a corpus of scene descriptions
from films because they are guaranteed to have an
explicit narrative structure. Moreover, screenplay
scene descriptions tend to be told in temporal or-
der (Beamer and Girju, 2009; Gordon and Swan-
son, 2009), which makes them a good resource for
learning about contingencies between events. In
addition, scenes in film represent many typical se-
quences from real life, while providing a rich source
of event clusters related to battles, love and mys-
tery. We carry out separate experiments for the ac-
tion movie genre and the romance movie genre. For
example, in the scene from Total Recall, from the
action movie genre (See Fig. 1), we might learn that
the event of sits up is CONTINGENT on the event
of clock chimes. The subset of the corpus we
use comprises 123,869 total unique event pairs.
We produce initial scalar estimates of poten-
tial CONTINGENCY between events using four
previously defined measures of distributional co-
occurrence. We then refine these estimates through
web searches that explicitly model the patterns of
narrative event sequences that were previously ob-
served to be likely within a particular genre. There
are several advantages of this method: (1) events in
the same genre tend to be more similar than events
across genres, so less data is needed to estimate
co-occurrence; (2) film scenes are typically nar-
rated via simple tenses in the correct temporal order,
which allows the ordering of events to contribute to
the estimation of the CONTINGENCY relation; (3)
The web counts focus on validating event pairs al-
ready deemed to be likely to be CONTINGENT in
the smaller, more controlled, film scene corpus. To
test our method, we conduct perceptual experiments
with human subjects on Mechanical Turk by asking
them to select which of two pairs of events are the
most likely. For example, given the scene from To-
tal Recall in Fig. 1, Mechanical Turkers are asked
to select whether the sequential event pair clock
chimes, sits up is more likely than clock
chimes followed by a randomly selected event
from the action film genre. Our experimental data
and annotations are available at http://nlds.
soe.ucsc.edu/data/EventPairs.
Sec. 2 describes our experimental method in de-
tail. Sec. 3 describes how we set up our evaluation
experiments and the results. We show that none of
the methods from previous work perform better on
our data than 75.15% average accuracy as measured
by human perceptions of CONTINGENCY. But after
web search refinement, we achieve an average accu-
racy of 85.64%. We delay a more detailed compari-
son to previous work to Sec. 4 where we summarize
our results and compare previous work to our own.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="method">
2 Experimental Method
</sectionHeader>
<bodyText confidence="0.9999771875">
Our method uses a combination of estimating
the likelihood of a CONTINGENT relation between
events in a corpus of film scenes (Walker et al.,
2012b), with estimates then revised through web
search. Our experiments are based on two sub-
sets of 862 film screen plays collected from the
IMSDb website using its ontology of film genres
(Walker et al., 2012b): a set of action movies of 115
screenplays totalling 748 MB, and a set of romance
movies of 71 screenplays totalling 390 MB. Fig. 1
provided an example scene from the action movie
genre from the IMSDb corpus.
We assume that the relation we are aiming to learn
is the PDTB CONTINGENT relation, which is de-
fined as a relation that exists when one of the sit-
uations described in the text spans that are identi-
fied as the two arguments of the relation, i.e. Arg1
and Arg2, causally influences the other (Prasad et
al., 2008b). As Girju notes, it is notoriously dif-
ficult to define causality without making the defi-
nition circular, but we follow Beamer and Girju’s
work in assuming that if events A, B are causally
related then B should occur less frequently when it
is not preceded by A and that B —4A should be much
less frequent than A —4B. We assume that both the
CAUSE and CONDITION subtypes of the CONTIN-
GENCY relation will result in pairs of events that are
likely to occur together and in a particular order. In
particular we assume that the subtypes of the PDTB
taxonomy of Contingency.Cause.Reason and Con-
tingency.Cause.Result are the most likely to occur
together as noted in previous work. Other related
</bodyText>
<page confidence="0.995151">
370
</page>
<bodyText confidence="0.998633565217391">
work has made use of discourse connectives or dis-
course taggers (implicit discourse relations) to pro-
vide additional evidence of CONTINGENCY (Do et
al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler
et al., 2009; Lin et al., 2010), but we do not because
the results have been mixed. In particular these dis-
course taggers are trained on The Wall Street Journal
(WSJ) and are unlikely to work well on our data.
We define an event as a verb lemma with its sub-
ject and object. Two events are considered equal if
they have the same verb. We do not believe word
ambiguities to be a primary concern, and previous
work also defines events to be the same if they have
the same surface verb, in some cases with a restric-
tion that the dependency relations should also be
the same (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Do et al., 2011; Riaz and Girju,
2010; Manshadi et al., 2008). Word sense ambigu-
ities are also reduced in specific genres (Action and
Romance) of film scenes.
Our method for estimating the likelihood of a
CONTINGENT relations between events consists of
four steps:
</bodyText>
<listItem confidence="0.998724666666667">
1. TEXT PROCESSING: We use Stanford
CoreNLP to annotate the corpus docu-
ment by document and store the annotated text
in XML format (Sec. 2.1);
2. COMPUTE EVENT REPRESENTATIONS: We
form intermediate artifacts such as events, pro-
tagonists and event pairs from the annotated
documents. Each event has its arguments (sub-
ject and object). We calculate the frequency of
the event across the relevant genre (Sec. 2.2);
3. CALCULATE CONTINGENCY MEASURES: We
define 4 different measures of contingency and
calculate each one separately using the results
from Steps 1 and 2 above. We call each re-
sult a PREDICTED CONTINGENT EVENT PAIR
(PCEP). All measures return scalar values that
we use to rank the PCEPs (Sec. 2.3);
4. WEB SEARCH REFINEMENT: We select the top
100 event pairs calculated by each contingency
measure, and construct a RANDOM EVENT
PAIR (REP) for each PCEP that preserves the
first element of the PCEP, and replaces the sec-
ond element with another event selected ran-
domly from within the same genre. We then
</listItem>
<bodyText confidence="0.996371">
define web search patterns for both PCEP and
REPs and compare the counts (Sec. 2.4).
</bodyText>
<subsectionHeader confidence="0.994632">
2.1 Text Processing
</subsectionHeader>
<bodyText confidence="0.999958375">
We first separate our screen plays into two sets of
documents, one for the action genre and one for the
romance genre. Because we are interested in the
event descriptions that are part of the scene descrip-
tions, we excise the dialog from each screen play.
Then using the Stanford CoreNLP pipeline, we an-
notate the film scene files. Annotations include tok-
enization, lemmatization, named entity recognition,
parsing and coreference resolution.
We extract the events by keeping all tokens whose
POS tags begin with VB. We then use the depen-
dency parse to find the subject and object of each
verb (if any), considering only nsubj, agent,
dobj, iobj, nsubjpass. We keep the orig-
inal tokens of the subject and the object for further
processing.
</bodyText>
<subsectionHeader confidence="0.999151">
2.2 Compute Event Representations
</subsectionHeader>
<bodyText confidence="0.98224119047619">
Given the results of Step 1 we start by generalizing
the subject and object stored with each event by sub-
stituting tokens with named entities if there are any
named entities tagged. Otherwise we generalize the
subjects and the objects using their lemmas. For ex-
ample, person UNLOCK door, as illustrated in
Table 1.
We then integrate all the subjects and objects
across all film scene files, keeping a record of the
frequency of each subject and object. For example,
[person (115), organization (14),
door (3)] UNLOCK [door (127),
person (5), bars (2)]. The most frequent
subject and object are selected as representative ar-
guments for the event. We then count the frequency
of each event across all the film scene files.
Within each film scene file, we count adjacent
events as potential CONTINGENT event pairs. Two
event pairs are defined as equal if they have the same
verbs in the same order. We also count the frequency
of each event pair.
</bodyText>
<subsectionHeader confidence="0.999704">
2.3 Calculate Contingency Measures
</subsectionHeader>
<bodyText confidence="0.999676">
We calculate four different measures of CONTIN-
GENCY based on previous work using the results
of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These
measures are pointwise mutual information, causal
potential, bigram probability and protagonist-based
</bodyText>
<page confidence="0.993098">
371
</page>
<bodyText confidence="0.974995818181818">
causal potential as described in detail below. We
calculate each measure separately by genre for the
action and romance genres of the film corpus.
Pointwise Mutual Information. The majority of
related work uses pointwise mutual information
(PMI) in some form or another (Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Riaz
and Girju, 2010; Do et al., 2011). Given a set of
events (a verb and its collected set of subjects and
objects), we calculate the PMI using the standard
definition:
</bodyText>
<equation confidence="0.999652666666667">
P(e1, e2)
pmi(e1, e2) = log (1)
P(e1)P(e2)
</equation>
<bodyText confidence="0.9994605">
in which e1 and e2 are two events. P(e1) is the
probability that event e1 occur in the corpus:
</bodyText>
<equation confidence="0.996447666666667">
count(e1)
P(e1) = E (2)
x count(ex)
</equation>
<bodyText confidence="0.999928">
where count(e1) is the count of how many times
event e1 occurs in the corpus, and Ex count(ex) is
the count of all the events in the corpus. The nu-
merator is the probability that the two events occur
together in the corpus:
</bodyText>
<equation confidence="0.998723666666667">
count(e1, e2)
P(e1, e2) = (3)
Ex Ey count(ex, ey)
</equation>
<bodyText confidence="0.999942923076923">
in which count(e1, e2) is the number of times the
two events e1 and e2 occur together in the corpus
regardless of their order. Only adjacent events in
each document are paired up. PMI is a symmetric
measurement for the relationship between two
events. The order of the events does not matter.
Causal Potential. Beamer and Girju proposed a
measure called Causal Potential (CP) based on pre-
vious work in philosophy and logic, along with an
annotation test for causality. An annotator decid-
ing whether event A causes event B asks herself the
following questions, where answering yes to both
means the two events are causally related:
</bodyText>
<listItem confidence="0.788780833333333">
• Does event A occur before (or simultaneously)
with event B?
• Keeping constant as many other states of affairs
of the world in the given text context as possi-
ble, does modifying event A entail predictably
modifying event B?
</listItem>
<bodyText confidence="0.998908714285714">
As Beamer &amp; Girju note, this annotation test is
objective, and it is simple to execute mentally. It
only assumes that the average person knows a lot
about how things work in the world and can reliably
answer these questions. CP is then defined below,
where the arrow notation means ordered bigrams,
i.e. event e1 occurs before event e2:
</bodyText>
<equation confidence="0.984307666666667">
P(e1 � e2)
O(e1, e2) = pmi(e1, e2) + log (4)
P(e2 —4 e1)
P(e1, e2)
where pmi(e1, e2) = log
P(e1)P(e2)
</equation>
<bodyText confidence="0.99648235">
The causal potential consists of two terms: the
first is pair-wise mutual information (PMI) and
the second is relative ordering of bigrams. PMI
measures how often events occur as a pair; whereas
relative ordering counts how often event order
occurs in the bigram. If there is no ordering of
events, the relative ordering is zero. We smooth
unseen event pairs by setting their frequency equal
to 1 to avoid zero probabilities. For CP as with PMI,
we restrict these calculations to adjacent events.
Column CP of Table 1 below provides sample
values for the CP measure.
Probabilistic Language Models. Our third method
models event sequences using statistical language
models (Manshadi et al., 2008). A language model
estimates the probability of a sequence of words us-
ing a sample corpus. To identify contingent event
sequences, we apply a bigram model which esti-
mates the probability of observing the sequence of
two words w1 and w2 as follows:
</bodyText>
<equation confidence="0.999731">
P(w1, w2) = P(w2Jw1) = count(w1, w2) (5)
count (w1 )
</equation>
<bodyText confidence="0.9226371">
Here, the words are events. Each verb is a single
event and each film scene is treated as a sequence of
verbs. For example, consider the following sentence
from Total Recall:
Quail and Kirsten sit at a small table,
eating breakfast.
This sentence is represented as the sequence of its
two verbs: sit, eat. We estimate the probability
of verb bigrams using Equation 5 and hypothesize
that the verb sequences with higher probability are
</bodyText>
<page confidence="0.983251">
372
</page>
<table confidence="0.9999326875">
Row # Causal Potential Pair CP PCEP Search pat- NumHits Random Pair REP Search pat- NumHits
tern tern
1 person KNOW person - 2.18 he knows * means 415M person KNOW person - he knows * ped- 2
person MEAN what person PEDDLE papers dles
2 person COME - person 2.12 he comes * rests 158M person COME - person he comes * 41
REST head GLANCE window glances
3 person SLAM person - 2.11 he slams * shuts 11 person SLAM person - he slams * chuck- 0
person SHUT door person CHUCKLE les
4 person UNLOCK door - 2.11 he unlocks * en- 80 person UNLOCK door - he unlocks * acts 0
person ENTER room ters person ACT shot
5 person SLOW person - 2.10 he slows * stops 697K person SLOW person - he slows * rivets 0
person STOP person eyes RIVET eyes
6 person LOOK window - 2.06 he looks * won- 342M person LOOK window - he looks * edges 98
person WONDER thing ders person EDGE hardness
7 person TAKE person - 2.01 he takes * looks 163M person TAKE person - he takes * catches 311M
person LOOK window person CATCH person
8 person MANAGE smile - 2.01 he manages * gets 80M person MANAGE smile he manages * ap- 16
person GET person - person APPROACH proaches
person
9 person DIVE escape - 2.00 he dives * swims 1.5M person DIVE escape - he dives *jams 6
person SWIM way gun JAM person
10 person STAGGER person 2.00 he staggers * 33 person STAGGER per- he staggers * 1
- person DROP person drops son - plain WHEEL per- wheels
son
11 person SHOOT person - 1.99 he shoots * falls 55.7M person SHOOT person - he shoots * pre- 6
person FALL feet person PREVENT per- vents
son
12 person SQUEEZE person 1.87 he squeezes * 5 person SQUEEZE per- he squeezes * 1
- person SHUT door shuts son - person MARK per- marks
son
13 person SEE person - per- 1.87 he sees * goes 184M person SEE person - im- he sees * quivers 2
son GO age QUIVER hips
</table>
<tableCaption confidence="0.999915">
Table 1: Sample web search patterns and values used in web search refinement algorithm from action genre
</tableCaption>
<bodyText confidence="0.999836428571429">
more likely to be contingent. We apply a threshold
of 20 for count(w1i w2) to avoid infrequent and
uncommon bigrams.
Protagonist-based Models. We also used a method
of generating event pairs based not only on the con-
secutive events in text but on their protagonist. This
is based on the assumption that the agent, or protag-
onist, will tend to perform actions that further her
own goals, and are thus causally related. We called
this method protagonist-based because all events
were partitioned into multiple sets where each set of
events has one protagonist. This method is roughly
based on previous work using chains of discourse
entities to induce narrative schemas (Chambers and
Jurafsky, 2009).
Events that share one protagonist were extracted
from text according to co-referring mentions pro-
vided by the Stanford CoreNLP toolkit.1 A man-
ual examination of coreference results on a sample
of movie scripts suggests that the accuracy is only
around 60%: most of the time the same entity (in its
</bodyText>
<footnote confidence="0.898289">
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.999485153846154">
nominal and pronominal forms) was not recognized
and was assigned as a new entity.
We preserve the order of events based on their tex-
tual order assuming as above that film scripts tend
to preserve temporal order. An ordered event pair
is generated if both events share a protagonist. We
further filter event pairs by eliminating those whose
frequency is less than 5 to filter insignificant and rare
event pairs. This also tends to catch errors generated
by the Stanford parser.
CP was then calculated accordingly to Equa-
tion 4. To calculate the PMI part of CP, we combine
the frequencies of event pairs in both orders.
</bodyText>
<subsectionHeader confidence="0.983358">
2.4 Web Search Refinement
</subsectionHeader>
<bodyText confidence="0.999962111111111">
The final step of our method is WEB SEARCH RE-
FINEMENT. Our hypothesis is that using the film
corpus within a particular genre to do the initial esti-
mates of contingency takes advantage of genre prop-
erties such as similar events and narration of scenes
in chronological order. However the film corpus is
necessarily small, and we can augment the evidence
for a particular contingent relation by defining spe-
cific narrative sequence patterns and collecting web
</bodyText>
<page confidence="0.996539">
373
</page>
<bodyText confidence="0.998767153846154">
counts.
Recall that PCEP stands for predicted contin-
gent event pair and that REP stands for random
event pair. We first select the top 100 event pairs
calculated by each CONTINGENCY measure, and
construct a RANDOM EVENT PAIR (REP) for each
PCEP that preserves the first element of the PCEP,
and replaces the second element with another event
selected randomly from within the same genre. We
then define web search patterns for both PCEP and
REPs and compare the counts. PCEPs should be fre-
quent in web search and REPs should be infrequent.
Our web refinement procedure is:
</bodyText>
<listItem confidence="0.970455785714286">
• For each event pair, PCEPs and REPs, create a
Google search pattern as illustrated by Table 1,
and described in more detail below.
• Search for the exact match in Google gen-
eral web search using incognito browsing and
record the estimated count of results returned;
• Remove all the PCEP/REP pairs with PCEP
Google search count less than 100: highly con-
tingent events should be frequent in a general
web search;
• Remove all PCEP/REP pairs with REP Google
search count greater than 100: events that are
not contingent on one another should not be
frequent in a general web search.
</listItem>
<bodyText confidence="0.993483449275362">
The motivation for this step is to provide addi-
tional evidence for or against the contingency of a
pair of events. Table 1 shows a selection of the top
100 PCEPs learned using the Causal potential (CP)
Metric, the web search patterns that are automati-
cally derived from the PCEPs (Column 4), the REPs
that were constructed for each PCEP (Column 6),
the web search patterns that were automatically de-
rived from the REPs (Column 7). Column 5 shows
the results of web search hits for the PCEP patterns
and Column 8 shows the results of web search hits
for the REP patterns. These hit counts were then
used in refining our estimates of CONTINGENCY for
the learned patterns as described above.
Note that the web search patterns do not aim to
find every possible match of the targeted CONTIN-
GENT relation that could possibly occur. Instead,
they are generalizations of the instances of PCEPs
that we found in the films corpus that are targeted at
finding hits that are the most likely to occur in nar-
rative sequences. Narrative sequences are most re-
liably signalled by use of the historical present tense,
e.g. as instantiated in the search patterns He knows
in Row 1 and He comes in Row 2 of Table 1 (Swan-
son and Gordon, 2012; Beamer and Girju, 2009;
Labov and Waletzky, 1997). In addition, we use
the “*” operator in Google Search to limit search
to pairs of events reported in the historical present
tense, that are “near” one another, and in a particular
sequence. We don’t care whether the events are in
the same utterance or in sequential utterances, thus
for the second verb (event) we do not include a sub-
ject pronoun he. These search patterns are not in-
tended to match the original instances in the film
corpus and in general they are unlikely to match
those instances.
For example, consider the search patterns and
results shown in Row 1 of Table 1. The
PCEP is (person KNOW person, person
MEAN what). The REP is (person KNOW
person, person PEDDLE papers). Our
prediction is that the REP should be much less likely
in web search counts and the results validate that
predication. A paired t-test over the 100 top PCEP
pairs for the CP measure comparing the hit counts
for the PCEP pairs vs. the REP pairs was highly
significant (p &lt; .00001). However, consider Row
7. Even though in general the PCEP pairs are more
likely (as measured by the paired t-test compar-
ing web search counts for PCEPs vs REPs), there
are cases where the REP is highly likely as shown
by the REP (person take person, person
CATCH person) in Row 7. Alternatively there are
cases where the web search counts provide evidence
against one of the PCEPs. Consider Rows 3, 4, 10
and 12. In all of these cases the web counts NumHits
for the PCEP number in the tens.
After the web search refinement, we retain the
PCEP/REP pairs with initially high PCEP estimates,
for which we found good evidence for contingency
and for randomness, e.g. Row 1 and 2 in Table 1.
We use 100 as a threshold because most of the time
the estimate result count from Google is either a
very large number (millions) or a very small num-
ber (tens), as illustrated by the NumHits columns in
Table 1.
We experimented with different types of patterns
with a development set of PCEPs before we settled
on the search pattern template shown in Table 1. We
</bodyText>
<page confidence="0.997794">
374
</page>
<bodyText confidence="0.999937555555556">
decided to use third person rather than first person
patterns, because first person patterns are only one
type of narrative (Swanson and Gordon, 2012). We
also decided to utilize event patterns without typical
objects, such as head in person REST head in
Row 2 of Table 1. We do not have any evidence that
this is the optimal search pattern template because
we did not systematically try other types of search
patterns.
</bodyText>
<sectionHeader confidence="0.994888" genericHeader="evaluation">
3 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999582">
While other work uses a range of methods for evalu-
ating accuracy, to our knowledge our work is the first
to use human judgments from Mechanical Turk to
evaluate the accuracy of the learned PCEPs. We first
describe the evaluation setup in Sec. 3.1 and then re-
port the results in Sec. 3.2
</bodyText>
<subsectionHeader confidence="0.895509">
3.1 Mechanical Turk Contingent Pair
Evaluations
</subsectionHeader>
<bodyText confidence="0.999919254237288">
We used three different types of HITs (Human Intel-
ligence Tasks) on Mechanical Turk for our evalua-
tion. Two of the HITS are in Fig. 2 and Fig. 3. The
differences in the different types of HITS involve:
(1) whether the arguments of events were given in
the HIT, as in Fig. 2 and (2): whether the Turkers
were told that the order of the events mattered, as
in Fig. 3. We initially thought that providing the
arguments to the events as shown in Fig. 2 would
help Turkers to reason about which event was more
likely. We tested this hypothesis only in the action
genre for the Causal Potential Measure. For CP, Bi-
gram and Protag the order of events always matters.
For the PMI task, the order of the events doesn’t
matter because PMI is a symmetric measure. Fig. 2
illustrates the instructions that were given with the
HIT when the event order doesn’t matter. In all the
other cases, the instructions that were given with the
HIT are those in Fig. 3 where the Turkers are in-
structed to pay attention to the order of the events.
For all types of HITS, for all measures of CON-
TINGENCY, we set up the task as a choice over
two alternatives, where for each predicted contin-
gent pair (PCEP), we generate a random event pair
(REP), with the first event the same and the second
one randomly chosen from all the events in the same
film genre. The REPs are constructed the same way
as we construct REPs for web search refinement,
as illustrated by Table 1. This is illustrated in both
Fig. 2 and Fig. 3. For all types of HITS, we ask 15
Turkers from a pre-qualified group to select which
pair (the PCEP or the REP) is more likely to oc-
cur together. Thus, the framing of these Mechani-
cal Turk tasks only assumes that the average person
knows how the world works; we do not ask them to
explicitly reason about causality as other work does
(Beamer and Girju, 2009; Gordon et al., 2011; Do et
al., 2011).
For each measure of CONTINGENCY, we take 100
event pairs with highest PCEP scores, and put them
in 5 HITs with twenty items per HIT. Previous work
has shown that for many common NLP tasks, 7
Turkers’ average score can match expert annotations
(Snow et al., 2008). However, we use 15 Turkers
because we had no gold-standard data and because
we were not sure how difficult the task is. It is
clearly subjective. To calculate the accuracy of each
method, we computed the average correlation coef-
ficient between each pair of raters and eliminate the
5 lowest scoring workers. We then used the percep-
tions of the 10 remaining workers to calculate accu-
racy as # of correct answers / total # of answers.
In general, deciding when a MTurk worker is un-
reliable when the data is subjective is a difficult
problem. In the future we plan to test other solu-
tions to measuring annotator reliability as proposed
in related work (Callison-Burch, 2009; Snow et al.,
2008; Karger et al., 2011; Dawid and Skene, 1979;
Welinder et al., 2010; Liu et al., 2012).
</bodyText>
<subsectionHeader confidence="0.970808">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.733581">
We report our results in terms of overall accuracy.
Because the Mechanical Turk task is a choose-
one question rather than a binary classification,
Precision = Recall in our experimental results:
True Positive = Number of Correct Answers
True Negative = Number of Correct Answers
False Positive = Number of Incorrect Answers
False Positive = Number of Incorrect Answers
</bodyText>
<table confidence="0.8405954">
Precision =
True Positive
True Positive + False Positive
True Positive
True Positive + False Negative
</table>
<tableCaption confidence="0.7982308">
The accuracies of all the methods are shown in
Table 2. The results of using event arguments
(person KNOW person) in the Mechanical Turk
evaluation task (i.e. Fig. 2) is given in Rows 1 and
2 of Table 2. The accuracies for Rows 1 and 2 are
</tableCaption>
<equation confidence="0.736653">
Recall =
</equation>
<page confidence="0.983899">
375
</page>
<figureCaption confidence="0.997338">
Figure 2: Mechanical Turk HIT with event arguments provided. This HIT also illustrates instructions where Turkers
are told that the order of the events does not matter.
</figureCaption>
<table confidence="0.999384666666667">
Row Contingency Estimation Method Action Romance Average
# Acc% Acc% Acc%
1 CP with event arguments 69.30 NA 69.30
2 CP with event arguments + Web search 77.57 NA 77.57
3 CP no args 75.20 75.10 75.15
4 CP no args +Web Search 87.67 83.61 85.64
5 PMI no args 68.70 79.60 74.15
6 PMI no args +Web Search 72.11 88.52 80.32
7 Bigram no args 67.10 66.50 66.80
8 Bigram no args +Web Search 72.40 70.14 71.27
9 Protag CP no args 65.40 68.20 66.80
10 Protag CP no args +Web Search 76.59 64.10 70.35
</table>
<tableCaption confidence="0.99957">
Table 2: Evaluation results for the top 100 event pairs using all methods.
</tableCaption>
<bodyText confidence="0.998541375">
considerably lower than when the PCEPs are tested
without arguments. Comparing Rows 1 and 2 with
Rows 3 and 4 suggests that even if the arguments
provide extra information that help to ground the
type of event, in some cases these constraints on
events may mislead the Turkers or make the eval-
uation task more difficult. There is an over 10% in-
crease in CP + Web search accuracy for the task that
omits the event arguments (i.e. Fig. 3 as can be seen
by comparing Row 2 with Row 4. Thus omitting the
arguments of events in evaluations actually appears
to allow Turkers to make better judgments.
In addition, Table 2 shows clearly that for ev-
ery single method, accuracy is improved by refin-
ing the initial estimates of contingency using the
narrative-based web search patterns. Web search in-
</bodyText>
<page confidence="0.998225">
376
</page>
<figureCaption confidence="0.9012385">
Figure 3: Mechanical Turk HIT for evaluation with no event arguments provided. This HIT also illustrates instructions
where Turkers are told that the order of the events does matter.
</figureCaption>
<bodyText confidence="0.999893515151515">
creases the accuracy of almost all evaluation tasks,
with increases ranging from 3.45% to 12.5% when
averaged over both film genres (Column 5 Average
Acc%). The best performing method for the Ac-
tion genre is CP+Web Search at 87.67%, while the
best performing method for the Romance genre is
PMI+Web search at 88.52%. However PMI+Web
Search does not beat CP+Web Search on average
over both genres we tested, even though the Me-
chanical Turk HIT for CP specifies that the order of
the events matters: a more stringent criterion. Also
overall the CP+WebSearch method achieves a very
high 85.64% average accuracy.
It is also interesting to note the variation across
the different methods. For example, while it is
well known that PMI typically requires very large
corpora to make good estimates, the PMI method
without web search refinement has an initially high
accuracy of 79.60% for the romance genre, while
only achieving 68.70% for action. Perhaps this dif-
ference arises because the romance genre is more
highly causal, or because situations are more struc-
tured in romance, providing better estimates with a
small corpus. However even in this case of romance
with PMI, adding web search refinement provides
an almost 10% increase in absolute accuracy to the
highest accuracy of any combination, i.e. 88.52%.
There is also an interesting case of Protag CP for
the romance genre where web search refinement ac-
tually decreases accuracy by 4.1%. In future work
we plan to examine more genres from the film cor-
pus and also examine the role of corpus size in more
detail.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="discussions">
4 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999969142857143">
We induced event pairs using several methods from
previous work with similar aims but widely different
problem formulations and evaluation methods. We
used a verb-rich film scene corpus where events are
normally narrated in temporal order. We used Me-
chanical Turk to evaluate the learned pairs of CON-
TINGENT events using human perceptions. In the
first stage drawing on previous measures of distribu-
tional co-occurrence, we achieved an overall average
accuracy of around 70%, over a 50% baseline. We
then implemented a novel method of defining narra-
tive sequence patterns using the Google Search API,
and used web counts to further refine our estimates
of the contingency of the learned event pairs. This
increased the overall average accuracy to around
77%, which is 27% above the baseline. Our results
indicate that the use of web search counts increases
the average accuracy of our Causal Potential-based
method to 85.64% as compared to an average accu-
racy of 75.15% without web search. To our knowl-
edge this is the highest accuracy achieved in tasks of
</bodyText>
<page confidence="0.993973">
377
</page>
<bodyText confidence="0.999490181818182">
this kind to date.
Previous work on recognition of the PDTB CON-
TINGENT relation has used both supervised and un-
supervised learning, and evaluation typically mea-
sures precision and recall against a PDTB annotated
corpus (Do et al., 2011; Pitler et al., 2009; Zhou et
al., 2010; Chiarcos, 2012; Louis et al., 2010). We
use an unsupervised approach and measure accuracy
using human perceptions. Other work by Girju and
her students defined a measure called causal poten-
tial and then used film screen plays to learn a knowl-
edge base of causal pairs of events. They evaluate
the pairs by asking two trained human annotators to
label whether occurrences of those pairs in their cor-
pus are causally related (Beamer and Girju, 2009;
Riaz and Girju, 2010). We also make use of their
causal potential measure. Work on commonsense
causal reasoning aims to learn causal relations be-
ween pairs of events using a range of methods ap-
plied to a large corpus of weblog narratives (Gordon
et al., 2011; Gordon and Swanson, 2009; Manshadi
et al., 2008). One form of evaluation aimed to pre-
dict the last event in a sequence (Manshadi et al.,
2008), while more recent work uses the learned pairs
to improve performance on the COPA SEMEVAL
task (Gordon et al., 2011).
Related work on SCRIPT LEARNING induces
likely sequences of temporally ordered events in
news, rather than CONTINGENCY or CAUSALITY
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). Chambers &amp; Jurafsky also evaluate
against a corpus of existing documents, by leaving
one event out of a document (news story), and then
testing the system’s ability to predict the missing
event. To our knowledge, our method is the first
to augment distributional semantics measures from
a corpus with web search data. We are also the first
to evaluate the learned event pairs with a human per-
ceptual evaluation with native speakers.
We hypothesize that there are several advantages
to our method: (1) events in the same genre tend to
be more similar than events across genres, so less
data is needed to estimate co-occurrence; (2) film
scenes are typically narrated via simple tenses in the
correct temporal order, which allows the ordering
of events to contribute to estimates of the CONTIN-
GENCY relation; (3) The web counts focus on vali-
dating event pairs already deemed to be likely to be
CONTINGENT in the smaller, more controlled, film
scence corpus.
Our work capitalizes on event sequences narrated
in temporal order as a cue to causality. We expect
this approach to generalize to other domains where
these properties hold, such as fables, personal stories
and news articles. We do not expect this technique
to generalize without further refinements to genres
frequently told out of temporal order or when events
are not mentioned consecutively in the text, for ex-
ample in certain types of fiction.
In future work we want to explore in more de-
tail the differences in performance of the different
contingency measures. For example, previous work
would suggest that the the higher the measure is, the
more likely the two events are to be contingent on
one another. To date, while we have only tested the
top 100, we have not found that the bottom set of
20 are less accurate than the top set of 20. This
could be due to corpus size, or the measures them-
selves, or noise from parser accuracy etc. As shown
in Table 2, web search refinement is able to eliminate
most noise in event pairs, but we would still aim to
achieve a better understanding of the circumstances
which lead particular methods to work better.
In future work we also want to explore ways of in-
ducing larger event structures than event pairs, such
as the causal chains, scripts, or narrative schemas of
previous work.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99749375">
We would like to thank Yan Li for setting up au-
tomatic search query. We also thank members of
NLDS for their discussions and suggestions, espe-
cially Stephanie Lukin, Rob Abbort, and Grace Lin.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995626">
B. Beamer and R. Girju. 2009. Using a bigram event
model to predict causal potential. In Computational
Linguistics and Intelligent Text Processing, p. 430–
441. Springer.
C. Callison-Burch. 2009. Fast, cheap, and creative: eval-
uating translation quality using amazon’s mechanical
turk. In Proc. of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1 -
Volume 1, p. 286–295. Association for Computational
Linguistics.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. Proc. of ACL-08:
HLT, p. 789–797.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
</reference>
<page confidence="0.983893">
378
</page>
<reference confidence="0.999810708737864">
Proc. of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 2-Volume 2, p. 602–610.
C. Chiarcos. 2012. Towards the unsupervised acquisi-
tion of discourse relations. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Short Papers-Volume 2, p. 213–217. Association
for Computational Linguistics.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries C (Applied Statistics), 28(1):20–28.
Q. X. Do, Y. S. Chan, and D. Roth. 2011. Minimally
supervised event causality identification. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 294–303. Association for Com-
putational Linguistics.
R.J. Gerrig. 1993. Experiencing narrative worlds: On
the psychological activities of reading. Yale Univ Pr.
A. Gordon and R. Swanson. 2009. Identifying personal
stories in millions of weblog entries. In Third Interna-
tional Conference on Weblogs and Social Media, Data
Challenge Workshop.
A. Gordon, Cosmin Bejan, and Kenji Sagae. 2011. Com-
monsense causal reasoning using millions of personal
stories. In Twenty-Fifth Conference on Artificial Intel-
ligence (AAAI-11).
A. Goyal, E. Riloff, and H. Daum´e III. 2010. Automat-
ically producing plot unit representations for narrative
text. In Proc. of the 2010 Conference on Empirical
Methods in Natural Language Processing, p. 77–86.
Association for Computational Linguistics.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological review, 101(3):371.
D. R. Karger, S. Oh, and D. Shah. 2011. Iterative
learning for reliable crowdsourcing systems. In John
Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett,
Fernando C. N. Pereira, and Kilian Q. Weinberger, ed-
itors, NIPS, p. 1953–1961.
W. Labov and J. Waletzky. 1997. Narrative analysis:
Oral versions of personal experience.
W. G. Lehnert. 1981. Plot units and narrative summa-
rization. Cognitive Science, 5(4):293–331.
Z. Lin, M.-Y. Kan, and H. T Ng. 2010. A pdtb-styled
end-to-end discourse parser. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Q. Liu, J. Peng, and A. Ihler. 2012. Variational inference
for crowdsourcing. In Advances in Neural Information
Processing Systems 25, p. 701–709.
A. Louis, A. Joshi, R. Prasad, and A. Nenkova. 2010.
Using entity features to classify implicit relations. In
Proc. of the 11th Annual SIGdial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
M. Manshadi, R. Swanson, and A. S Gordon. 2008.
Learning a probabilistic model of event sequences
from internet weblog stories. In Proc. of the 21st
FLAIRS Conference.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of the 47th Meeting of the Association for
Computational Linguistics.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The penn discourse
treebank 2.0. In Proc. of the 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008), p. 2961–2968.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008b. The Penn Discourse
TreeBank 2.0. In Proc. of 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008).
M. Riaz and R. Girju. 2010. Another look at causal-
ity: Discovering scenario-specific contingency rela-
tionships with no supervision. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, p. 361–368. IEEE.
R. Schank and R. Abelson. 1977. Scripts Plans Goals.
Lea.
R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast—but is it good?: evaluating non-expert
annotations for natural language tasks. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 254–263. Association for Com-
putational Linguistics.
R. Swanson and A. S. Gordon. 2012. Say anything:
Using textual case-based reasoning to enable open-
domain interactive storytelling. ACM Transactions on
Interactive Intelligent Systems (TiiS), 2(3):16.
M. A. Walker, G. Lin, and J. Sawyer. 2012b. An anno-
tated corpus of film dialogue for learning and charac-
terizing character style. In Language Resources and
Evaluation Conference, LREC2012.
P. Welinder, S. Branson, S. Belongie, and P. Perona.
2010. The multidimensional wisdom of crowds. In
Advances in Neural Information Processing Systems
23, p. 2424–2432.
Z.-M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, , and
C. L. Tan. 2010. Predicting discourse connectives for
implicit discourse relation recognition. In In Coling
2010: Posters, p. 1507–1514.
</reference>
<page confidence="0.999183">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592547">
<title confidence="0.997887">Unsupervised Induction of Contingent Event Pairs from Film Scenes</title>
<author confidence="0.910073">Zhichao Hu</author>
<author confidence="0.910073">Elahe Rahimtoroghi</author>
<author confidence="0.910073">Larissa Munishkina</author>
<author confidence="0.910073">Reid Swanson</author>
<author confidence="0.910073">A Marilyn</author>
<affiliation confidence="0.867688">Natural Language and Dialogue Systems Department of Computer Science, University of California, Santa</affiliation>
<address confidence="0.934559">Santa Cruz, CA,</address>
<email confidence="0.937484">elahe,mlarissa,reid,</email>
<abstract confidence="0.996871708333333">Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as inference of the relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be one another. We refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Beamer</author>
<author>R Girju</author>
</authors>
<title>Using a bigram event model to predict causal potential.</title>
<date>2009</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>430--441</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1961" citStr="Beamer and Girju, 2009" startWordPosition="303" endWordPosition="306"> accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in thought. He is a good-looking but conventional man in his early thirti</context>
<context position="4491" citStr="Beamer and Girju, 2009" startWordPosition="718" endWordPosition="721">drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real life, while providing a rich source of event clusters related to battles, love and mystery. We carry out separate experiments for the action movie genre and the romance movie genre. For example, in the scene from Total Recall, from the action movie genre (See Fig. 1), we might learn that the event of sits up is CONTINGENT on the event of clock chimes. The subset of the corpus we use comprises 123,869 total unique e</context>
<context position="22904" citStr="Beamer and Girju, 2009" startWordPosition="3920" endWordPosition="3923">imates of CONTINGENCY for the learned patterns as described above. Note that the web search patterns do not aim to find every possible match of the targeted CONTINGENT relation that could possibly occur. Instead, they are generalizations of the instances of PCEPs that we found in the films corpus that are targeted at finding hits that are the most likely to occur in narrative sequences. Narrative sequences are most reliably signalled by use of the historical present tense, e.g. as instantiated in the search patterns He knows in Row 1 and He comes in Row 2 of Table 1 (Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997). In addition, we use the “*” operator in Google Search to limit search to pairs of events reported in the historical present tense, that are “near” one another, and in a particular sequence. We don’t care whether the events are in the same utterance or in sequential utterances, thus for the second verb (event) we do not include a subject pronoun he. These search patterns are not intended to match the original instances in the film corpus and in general they are unlikely to match those instances. For example, consider the search patterns and results shown in Row 1 of</context>
<context position="27583" citStr="Beamer and Girju, 2009" startWordPosition="4770" endWordPosition="4773">h the first event the same and the second one randomly chosen from all the events in the same film genre. The REPs are constructed the same way as we construct REPs for web search refinement, as illustrated by Table 1. This is illustrated in both Fig. 2 and Fig. 3. For all types of HITS, we ask 15 Turkers from a pre-qualified group to select which pair (the PCEP or the REP) is more likely to occur together. Thus, the framing of these Mechanical Turk tasks only assumes that the average person knows how the world works; we do not ask them to explicitly reason about causality as other work does (Beamer and Girju, 2009; Gordon et al., 2011; Do et al., 2011). For each measure of CONTINGENCY, we take 100 event pairs with highest PCEP scores, and put them in 5 HITs with twenty items per HIT. Previous work has shown that for many common NLP tasks, 7 Turkers’ average score can match expert annotations (Snow et al., 2008). However, we use 15 Turkers because we had no gold-standard data and because we were not sure how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scor</context>
<context position="34490" citStr="Beamer and Girju, 2009" startWordPosition="5959" endWordPosition="5962">sed and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordere</context>
</contexts>
<marker>Beamer, Girju, 2009</marker>
<rawString>B. Beamer and R. Girju. 2009. Using a bigram event model to predict causal potential. In Computational Linguistics and Intelligent Text Processing, p. 430– 441. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>286--295</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="28558" citStr="Callison-Burch, 2009" startWordPosition="4944" endWordPosition="4945">standard data and because we were not sure how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers Precision = True Positive True Positive + False Positive True Positive True Positive + False Negative The accurac</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>C. Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk. In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 -Volume 1, p. 286–295. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>Proc. of ACL-08: HLT,</booktitle>
<pages>789--797</pages>
<contexts>
<context position="9345" citStr="Chambers and Jurafsky, 2008" startWordPosition="1554" endWordPosition="1557">2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes. Our method for estimating the likelihood of a CONTINGENT relations between events consists of four steps: 1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and store the annotated text in XML format (Sec. 2.1); 2. COMPUTE EVENT REPRESENTATIONS: We form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its ar</context>
<context position="13058" citStr="Chambers and Jurafsky, 2008" startWordPosition="2169" endWordPosition="2173">e also count the frequency of each event pair. 2.3 Calculate Contingency Measures We calculate four different measures of CONTINGENCY based on previous work using the results of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These measures are pointwise mutual information, causal potential, bigram probability and protagonist-based 371 causal potential as described in detail below. We calculate each measure separately by genre for the action and romance genres of the film corpus. Pointwise Mutual Information. The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011). Given a set of events (a verb and its collected set of subjects and objects), we calculate the PMI using the standard definition: P(e1, e2) pmi(e1, e2) = log (1) P(e1)P(e2) in which e1 and e2 are two events. P(e1) is the probability that event e1 occur in the corpus: count(e1) P(e1) = E (2) x count(ex) where count(e1) is the count of how many times event e1 occurs in the corpus, and Ex count(ex) is the count of all the events in the corpus. The numerator is the probability that the two events occur together in the corpus: c</context>
<context position="35173" citStr="Chambers and Jurafsky, 2008" startWordPosition="6074" endWordPosition="6077">l potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers &amp; Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing the system’s ability to predict the missing event. To our knowledge, our method is the first to augment distributional semantics measures from a corpus with web search data. We are also the first to evaluate the learned event pairs with a human perceptual evaluation with native speakers. We hypothesize that there are several advantages to our method: (1) events in the same genre tend to be more similar than events across</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>N. Chambers and D. Jurafsky. 2008. Unsupervised learning of narrative event chains. Proc. of ACL-08: HLT, p. 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>602--610</pages>
<contexts>
<context position="1867" citStr="Chambers and Jurafsky, 2009" startWordPosition="287" endWordPosition="290">the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his gl</context>
<context position="9374" citStr="Chambers and Jurafsky, 2009" startWordPosition="1558" endWordPosition="1561">et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes. Our method for estimating the likelihood of a CONTINGENT relations between events consists of four steps: 1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and store the annotated text in XML format (Sec. 2.1); 2. COMPUTE EVENT REPRESENTATIONS: We form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its arguments (subject and object).</context>
<context position="13087" citStr="Chambers and Jurafsky, 2009" startWordPosition="2174" endWordPosition="2177"> each event pair. 2.3 Calculate Contingency Measures We calculate four different measures of CONTINGENCY based on previous work using the results of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These measures are pointwise mutual information, causal potential, bigram probability and protagonist-based 371 causal potential as described in detail below. We calculate each measure separately by genre for the action and romance genres of the film corpus. Pointwise Mutual Information. The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011). Given a set of events (a verb and its collected set of subjects and objects), we calculate the PMI using the standard definition: P(e1, e2) pmi(e1, e2) = log (1) P(e1)P(e2) in which e1 and e2 are two events. P(e1) is the probability that event e1 occur in the corpus: count(e1) P(e1) = E (2) x count(ex) where count(e1) is the count of how many times event e1 occurs in the corpus, and Ex count(ex) is the count of all the events in the corpus. The numerator is the probability that the two events occur together in the corpus: count(e1, e2) P(e1, e2) = (3) </context>
<context position="19048" citStr="Chambers and Jurafsky, 2009" startWordPosition="3257" endWordPosition="3260">i w2) to avoid infrequent and uncommon bigrams. Protagonist-based Models. We also used a method of generating event pairs based not only on the consecutive events in text but on their protagonist. This is based on the assumption that the agent, or protagonist, will tend to perform actions that further her own goals, and are thus causally related. We called this method protagonist-based because all events were partitioned into multiple sets where each set of events has one protagonist. This method is roughly based on previous work using chains of discourse entities to induce narrative schemas (Chambers and Jurafsky, 2009). Events that share one protagonist were extracted from text according to co-referring mentions provided by the Stanford CoreNLP toolkit.1 A manual examination of coreference results on a sample of movie scripts suggests that the accuracy is only around 60%: most of the time the same entity (in its 1http://nlp.stanford.edu/software/corenlp.shtml nominal and pronominal forms) was not recognized and was assigned as a new entity. We preserve the order of events based on their textual order assuming as above that film scripts tend to preserve temporal order. An ordered event pair is generated if b</context>
<context position="35203" citStr="Chambers and Jurafsky, 2009" startWordPosition="6078" endWordPosition="6082">commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers &amp; Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing the system’s ability to predict the missing event. To our knowledge, our method is the first to augment distributional semantics measures from a corpus with web search data. We are also the first to evaluate the learned event pairs with a human perceptual evaluation with native speakers. We hypothesize that there are several advantages to our method: (1) events in the same genre tend to be more similar than events across genres, so less data is neede</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>N. Chambers and D. Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, p. 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chiarcos</author>
</authors>
<title>Towards the unsupervised acquisition of discourse relations.</title>
<date>2012</date>
<booktitle>In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>213--217</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="8738" citStr="Chiarcos, 2012" startWordPosition="1442" endWordPosition="1443">4A should be much less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related 370 work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsk</context>
<context position="34058" citStr="Chiarcos, 2012" startWordPosition="5887" endWordPosition="5888">uracy to around 77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of 377 this kind to date. Previous work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events </context>
</contexts>
<marker>Chiarcos, 2012</marker>
<rawString>C. Chiarcos. 2012. Towards the unsupervised acquisition of discourse relations. In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, p. 213–217. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm.</title>
<date>1979</date>
<journal>Journal of the Royal Statistical Society. Series C (Applied Statistics),</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="28621" citStr="Dawid and Skene, 1979" startWordPosition="4954" endWordPosition="4957">ask is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers Precision = True Positive True Positive + False Positive True Positive True Positive + False Negative The accuracies of all the methods are shown in Table 2. The results of usi</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q X Do</author>
<author>Y S Chan</author>
<author>D Roth</author>
</authors>
<title>Minimally supervised event causality identification.</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2001" citStr="Do et al., 2011" startWordPosition="311" endWordPosition="314">oduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in thought. He is a good-looking but conventional man in his early thirties. He seems rather in awe of his wife, </context>
<context position="8701" citStr="Do et al., 2011" startWordPosition="1434" endWordPosition="1437">n it is not preceded by A and that B —4A should be much less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related 370 work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should a</context>
<context position="13127" citStr="Do et al., 2011" startWordPosition="2182" endWordPosition="2185">s We calculate four different measures of CONTINGENCY based on previous work using the results of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These measures are pointwise mutual information, causal potential, bigram probability and protagonist-based 371 causal potential as described in detail below. We calculate each measure separately by genre for the action and romance genres of the film corpus. Pointwise Mutual Information. The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011). Given a set of events (a verb and its collected set of subjects and objects), we calculate the PMI using the standard definition: P(e1, e2) pmi(e1, e2) = log (1) P(e1)P(e2) in which e1 and e2 are two events. P(e1) is the probability that event e1 occur in the corpus: count(e1) P(e1) = E (2) x count(ex) where count(e1) is the count of how many times event e1 occurs in the corpus, and Ex count(ex) is the count of all the events in the corpus. The numerator is the probability that the two events occur together in the corpus: count(e1, e2) P(e1, e2) = (3) Ex Ey count(ex, ey) in which count(e1, e</context>
<context position="27622" citStr="Do et al., 2011" startWordPosition="4778" endWordPosition="4781"> randomly chosen from all the events in the same film genre. The REPs are constructed the same way as we construct REPs for web search refinement, as illustrated by Table 1. This is illustrated in both Fig. 2 and Fig. 3. For all types of HITS, we ask 15 Turkers from a pre-qualified group to select which pair (the PCEP or the REP) is more likely to occur together. Thus, the framing of these Mechanical Turk tasks only assumes that the average person knows how the world works; we do not ask them to explicitly reason about causality as other work does (Beamer and Girju, 2009; Gordon et al., 2011; Do et al., 2011). For each measure of CONTINGENCY, we take 100 event pairs with highest PCEP scores, and put them in 5 HITs with twenty items per HIT. Previous work has shown that for many common NLP tasks, 7 Turkers’ average score can match expert annotations (Snow et al., 2008). However, we use 15 Turkers because we had no gold-standard data and because we were not sure how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptio</context>
<context position="34002" citStr="Do et al., 2011" startWordPosition="5875" endWordPosition="5878">arned event pairs. This increased the overall average accuracy to around 77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of 377 this kind to date. Previous work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasonin</context>
</contexts>
<marker>Do, Chan, Roth, 2011</marker>
<rawString>Q. X. Do, Y. S. Chan, and D. Roth. 2011. Minimally supervised event causality identification. In Proc. of the Conference on Empirical Methods in Natural Language Processing, p. 294–303. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Gerrig</author>
</authors>
<title>Experiencing narrative worlds: On the psychological activities of reading. Yale Univ Pr.</title>
<date>1993</date>
<contexts>
<context position="1612" citStr="Gerrig, 1993" startWordPosition="248" endWordPosition="249"> refine event pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Qua</context>
</contexts>
<marker>Gerrig, 1993</marker>
<rawString>R.J. Gerrig. 1993. Experiencing narrative worlds: On the psychological activities of reading. Yale Univ Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>R Swanson</author>
</authors>
<title>Identifying personal stories in millions of weblog entries.</title>
<date>2009</date>
<booktitle>In Third International Conference on Weblogs and Social Media, Data Challenge Workshop.</booktitle>
<contexts>
<context position="1916" citStr="Gordon and Swanson, 2009" startWordPosition="295" endWordPosition="298">er a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in thought. He is a good-loo</context>
<context position="4518" citStr="Gordon and Swanson, 2009" startWordPosition="722" endWordPosition="726">neral definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real life, while providing a rich source of event clusters related to battles, love and mystery. We carry out separate experiments for the action movie genre and the romance movie genre. For example, in the scene from Total Recall, from the action movie genre (See Fig. 1), we might learn that the event of sits up is CONTINGENT on the event of clock chimes. The subset of the corpus we use comprises 123,869 total unique event pairs. We produce init</context>
<context position="34776" citStr="Gordon and Swanson, 2009" startWordPosition="6009" endWordPosition="6012">tions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers &amp; Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing the system’s ability to pre</context>
</contexts>
<marker>Gordon, Swanson, 2009</marker>
<rawString>A. Gordon and R. Swanson. 2009. Identifying personal stories in millions of weblog entries. In Third International Conference on Weblogs and Social Media, Data Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>Cosmin Bejan</author>
<author>Kenji Sagae</author>
</authors>
<title>Commonsense causal reasoning using millions of personal stories.</title>
<date>2011</date>
<booktitle>In Twenty-Fifth Conference on Artificial Intelligence (AAAI-11).</booktitle>
<contexts>
<context position="1937" citStr="Gordon et al., 2011" startWordPosition="299" endWordPosition="302">ompared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in thought. He is a good-looking but conventional</context>
<context position="8722" citStr="Gordon et al., 2011" startWordPosition="1438" endWordPosition="1441">ded by A and that B —4A should be much less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related 370 work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Cham</context>
<context position="27604" citStr="Gordon et al., 2011" startWordPosition="4774" endWordPosition="4777">me and the second one randomly chosen from all the events in the same film genre. The REPs are constructed the same way as we construct REPs for web search refinement, as illustrated by Table 1. This is illustrated in both Fig. 2 and Fig. 3. For all types of HITS, we ask 15 Turkers from a pre-qualified group to select which pair (the PCEP or the REP) is more likely to occur together. Thus, the framing of these Mechanical Turk tasks only assumes that the average person knows how the world works; we do not ask them to explicitly reason about causality as other work does (Beamer and Girju, 2009; Gordon et al., 2011; Do et al., 2011). For each measure of CONTINGENCY, we take 100 event pairs with highest PCEP scores, and put them in 5 HITs with twenty items per HIT. Previous work has shown that for many common NLP tasks, 7 Turkers’ average score can match expert annotations (Snow et al., 2008). However, we use 15 Turkers because we had no gold-standard data and because we were not sure how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then </context>
<context position="34750" citStr="Gordon et al., 2011" startWordPosition="6005" endWordPosition="6008">cy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers &amp; Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing t</context>
</contexts>
<marker>Gordon, Bejan, Sagae, 2011</marker>
<rawString>A. Gordon, Cosmin Bejan, and Kenji Sagae. 2011. Commonsense causal reasoning using millions of personal stories. In Twenty-Fifth Conference on Artificial Intelligence (AAAI-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goyal</author>
<author>E Riloff</author>
<author>H Daum´e</author>
</authors>
<title>Automatically producing plot unit representations for narrative text.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>77--86</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Goyal, Riloff, Daum´e, 2010</marker>
<rawString>A. Goyal, E. Riloff, and H. Daum´e III. 2010. Automatically producing plot unit representations for narrative text. In Proc. of the 2010 Conference on Empirical Methods in Natural Language Processing, p. 77–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>M Singer</author>
<author>T Trabasso</author>
</authors>
<title>Constructing inferences during narrative text comprehension.</title>
<date>1994</date>
<journal>Psychological review,</journal>
<volume>101</volume>
<issue>3</issue>
<contexts>
<context position="1635" citStr="Graesser et al., 1994" startWordPosition="250" endWordPosition="253">pairs that we learn from a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Madden</context>
</contexts>
<marker>Graesser, Singer, Trabasso, 1994</marker>
<rawString>A. C. Graesser, M. Singer, and T. Trabasso. 1994. Constructing inferences during narrative text comprehension. Psychological review, 101(3):371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Karger</author>
<author>S Oh</author>
<author>D Shah</author>
</authors>
<title>Iterative learning for reliable crowdsourcing systems. In</title>
<date>2011</date>
<pages>1953--1961</pages>
<editor>John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS,</editor>
<contexts>
<context position="28598" citStr="Karger et al., 2011" startWordPosition="4950" endWordPosition="4953">e how difficult the task is. It is clearly subjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers Precision = True Positive True Positive + False Positive True Positive True Positive + False Negative The accuracies of all the methods are shown in Tabl</context>
</contexts>
<marker>Karger, Oh, Shah, 2011</marker>
<rawString>D. R. Karger, S. Oh, and D. Shah. 2011. Iterative learning for reliable crowdsourcing systems. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS, p. 1953–1961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
<author>J Waletzky</author>
</authors>
<title>Narrative analysis: Oral versions of personal experience.</title>
<date>1997</date>
<contexts>
<context position="22931" citStr="Labov and Waletzky, 1997" startWordPosition="3924" endWordPosition="3927">r the learned patterns as described above. Note that the web search patterns do not aim to find every possible match of the targeted CONTINGENT relation that could possibly occur. Instead, they are generalizations of the instances of PCEPs that we found in the films corpus that are targeted at finding hits that are the most likely to occur in narrative sequences. Narrative sequences are most reliably signalled by use of the historical present tense, e.g. as instantiated in the search patterns He knows in Row 1 and He comes in Row 2 of Table 1 (Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997). In addition, we use the “*” operator in Google Search to limit search to pairs of events reported in the historical present tense, that are “near” one another, and in a particular sequence. We don’t care whether the events are in the same utterance or in sequential utterances, thus for the second verb (event) we do not include a subject pronoun he. These search patterns are not intended to match the original instances in the film corpus and in general they are unlikely to match those instances. For example, consider the search patterns and results shown in Row 1 of Table 1. The PCEP is (pers</context>
</contexts>
<marker>Labov, Waletzky, 1997</marker>
<rawString>W. Labov and J. Waletzky. 1997. Narrative analysis: Oral versions of personal experience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Lehnert</author>
</authors>
<title>Plot units and narrative summarization.</title>
<date>1981</date>
<journal>Cognitive Science,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="1650" citStr="Lehnert, 1981" startWordPosition="254" endWordPosition="255">m a corpus of film scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the cloc</context>
</contexts>
<marker>Lehnert, 1981</marker>
<rawString>W. G. Lehnert. 1981. Plot units and narrative summarization. Cognitive Science, 5(4):293–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>M-Y Kan</author>
<author>H T Ng</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2010</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4055" citStr="Lin et al., 2010" startWordPosition="644" endWordPosition="647">in the future. Such knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical se</context>
<context position="8778" citStr="Lin et al., 2010" startWordPosition="1448" endWordPosition="1451">A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related 370 work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do</context>
</contexts>
<marker>Lin, Kan, Ng, 2010</marker>
<rawString>Z. Lin, M.-Y. Kan, and H. T Ng. 2010. A pdtb-styled end-to-end discourse parser. In Proc. of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Liu</author>
<author>J Peng</author>
<author>A Ihler</author>
</authors>
<title>Variational inference for crowdsourcing.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>701--709</pages>
<contexts>
<context position="28663" citStr="Liu et al., 2012" startWordPosition="4962" endWordPosition="4965"> the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers Precision = True Positive True Positive + False Positive True Positive True Positive + False Negative The accuracies of all the methods are shown in Table 2. The results of using event arguments (person KNOW person) in</context>
</contexts>
<marker>Liu, Peng, Ihler, 2012</marker>
<rawString>Q. Liu, J. Peng, and A. Ihler. 2012. Variational inference for crowdsourcing. In Advances in Neural Information Processing Systems 25, p. 701–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Joshi</author>
<author>R Prasad</author>
<author>A Nenkova</author>
</authors>
<title>Using entity features to classify implicit relations.</title>
<date>2010</date>
<booktitle>In Proc. of the 11th Annual SIGdial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="4097" citStr="Louis et al., 2010" startWordPosition="652" endWordPosition="655">ical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real life, while providing a </context>
<context position="34079" citStr="Louis et al., 2010" startWordPosition="5889" endWordPosition="5892">77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of 377 this kind to date. Previous work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of meth</context>
</contexts>
<marker>Louis, Joshi, Prasad, Nenkova, 2010</marker>
<rawString>A. Louis, A. Joshi, R. Prasad, and A. Nenkova. 2010. Using entity features to classify implicit relations. In Proc. of the 11th Annual SIGdial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Manshadi</author>
<author>R Swanson</author>
<author>A S Gordon</author>
</authors>
<title>Learning a probabilistic model of event sequences from internet weblog stories.</title>
<date>2008</date>
<booktitle>In Proc. of the 21st FLAIRS Conference.</booktitle>
<contexts>
<context position="1890" citStr="Manshadi et al., 2008" startWordPosition="291" endWordPosition="294">est method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in</context>
<context position="9437" citStr="Manshadi et al., 2008" startWordPosition="1570" endWordPosition="1573">ave been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes. Our method for estimating the likelihood of a CONTINGENT relations between events consists of four steps: 1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and store the annotated text in XML format (Sec. 2.1); 2. COMPUTE EVENT REPRESENTATIONS: We form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its arguments (subject and object). We calculate the frequency of the event across the relevant ge</context>
<context position="15698" citStr="Manshadi et al., 2008" startWordPosition="2633" endWordPosition="2636"> information (PMI) and the second is relative ordering of bigrams. PMI measures how often events occur as a pair; whereas relative ordering counts how often event order occurs in the bigram. If there is no ordering of events, the relative ordering is zero. We smooth unseen event pairs by setting their frequency equal to 1 to avoid zero probabilities. For CP as with PMI, we restrict these calculations to adjacent events. Column CP of Table 1 below provides sample values for the CP measure. Probabilistic Language Models. Our third method models event sequences using statistical language models (Manshadi et al., 2008). A language model estimates the probability of a sequence of words using a sample corpus. To identify contingent event sequences, we apply a bigram model which estimates the probability of observing the sequence of two words w1 and w2 as follows: P(w1, w2) = P(w2Jw1) = count(w1, w2) (5) count (w1 ) Here, the words are events. Each verb is a single event and each film scene is treated as a sequence of verbs. For example, consider the following sentence from Total Recall: Quail and Kirsten sit at a small table, eating breakfast. This sentence is represented as the sequence of its two verbs: sit</context>
<context position="34800" citStr="Manshadi et al., 2008" startWordPosition="6013" endWordPosition="6016"> and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rather than CONTINGENCY or CAUSALITY (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Chambers &amp; Jurafsky also evaluate against a corpus of existing documents, by leaving one event out of a document (news story), and then testing the system’s ability to predict the missing event. </context>
</contexts>
<marker>Manshadi, Swanson, Gordon, 2008</marker>
<rawString>M. Manshadi, R. Swanson, and A. S Gordon. 2008. Learning a probabilistic model of event sequences from internet weblog stories. In Proc. of the 21st FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proc. of the 47th Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4076" citStr="Pitler et al., 2009" startWordPosition="648" endWordPosition="651">h knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film represent many typical sequences from real lif</context>
<context position="8759" citStr="Pitler et al., 2009" startWordPosition="1444" endWordPosition="1447">h less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.Cause.Result are the most likely to occur together as noted in previous work. Other related 370 work has made use of discourse connectives or discourse taggers (implicit discourse relations) to provide additional evidence of CONTINGENCY (Do et al., 2011; Gordon et al., 2011; Chiarcos, 2012; Pitler et al., 2009; Lin et al., 2010), but we do not because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and</context>
<context position="34023" citStr="Pitler et al., 2009" startWordPosition="5879" endWordPosition="5882">. This increased the overall average accuracy to around 77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of 377 this kind to date. Previous work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causa</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proc. of the 47th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proc. of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>2961--2968</pages>
<contexts>
<context position="4036" citStr="Prasad et al., 2008" startWordPosition="640" endWordPosition="643"> are likely to happen in the future. Such knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film repres</context>
<context position="7848" citStr="Prasad et al., 2008" startWordPosition="1288" endWordPosition="1291">een plays collected from the IMSDb website using its ontology of film genres (Walker et al., 2012b): a set of action movies of 115 screenplays totalling 748 MB, and a set of romance movies of 71 screenplays totalling 390 MB. Fig. 1 provided an example scene from the action movie genre from the IMSDb corpus. We assume that the relation we are aiming to learn is the PDTB CONTINGENT relation, which is defined as a relation that exists when one of the situations described in the text spans that are identified as the two arguments of the relation, i.e. Arg1 and Arg2, causally influences the other (Prasad et al., 2008b). As Girju notes, it is notoriously difficult to define causality without making the definition circular, but we follow Beamer and Girju’s work in assuming that if events A, B are causally related then B should occur less frequently when it is not preceded by A and that B —4A should be much less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008a. The penn discourse treebank 2.0. In Proc. of the 6th International Conference on Language Resources and Evaluation (LREC 2008), p. 2961–2968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proc. of 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="4036" citStr="Prasad et al., 2008" startWordPosition="640" endWordPosition="643"> are likely to happen in the future. Such knowledge has practical applications in commonsense reasoning, infor369 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369–379, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics mation retrieval, question answering, narrative understanding and inferring discourse relations. We model this likelihood between events by drawing on the PTDB’s general definition of the CONTINGENT relation, which encapsulates relations elsewhere called CAUSE, CONDITION and ENABLEMENT (Prasad et al., 2008a; Lin et al., 2010; Pitler et al., 2009; Louis et al., 2010). Our aim in this paper is to implement and evaluate a range of different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We first utilize a corpus of scene descriptions from films because they are guaranteed to have an explicit narrative structure. Moreover, screenplay scene descriptions tend to be told in temporal order (Beamer and Girju, 2009; Gordon and Swanson, 2009), which makes them a good resource for learning about contingencies between events. In addition, scenes in film repres</context>
<context position="7848" citStr="Prasad et al., 2008" startWordPosition="1288" endWordPosition="1291">een plays collected from the IMSDb website using its ontology of film genres (Walker et al., 2012b): a set of action movies of 115 screenplays totalling 748 MB, and a set of romance movies of 71 screenplays totalling 390 MB. Fig. 1 provided an example scene from the action movie genre from the IMSDb corpus. We assume that the relation we are aiming to learn is the PDTB CONTINGENT relation, which is defined as a relation that exists when one of the situations described in the text spans that are identified as the two arguments of the relation, i.e. Arg1 and Arg2, causally influences the other (Prasad et al., 2008b). As Girju notes, it is notoriously difficult to define causality without making the definition circular, but we follow Beamer and Girju’s work in assuming that if events A, B are causally related then B should occur less frequently when it is not preceded by A and that B —4A should be much less frequent than A —4B. We assume that both the CAUSE and CONDITION subtypes of the CONTINGENCY relation will result in pairs of events that are likely to occur together and in a particular order. In particular we assume that the subtypes of the PDTB taxonomy of Contingency.Cause.Reason and Contingency.</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008b. The Penn Discourse TreeBank 2.0. In Proc. of 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riaz</author>
<author>R Girju</author>
</authors>
<title>Another look at causality: Discovering scenario-specific contingency relationships with no supervision.</title>
<date>2010</date>
<booktitle>In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on,</booktitle>
<pages>361--368</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1983" citStr="Riaz and Girju, 2010" startWordPosition="307" endWordPosition="310">out web search. 1 Introduction Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus discourse relations are one of the primary means to structure narrative in genres as diverse as weblogs, search queries, stories, film scripts and news articles (Chambers and Jurafsky, 2009; Manshadi et al., 2008; Gordon and Swanson, 2009; Gordon et al., 2011; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011). DOUGLAS QUAIL and his wife KRISTEN, are asleep in bed. Gradually the room lights brighten. the clock chimes and begins speaking in a soft, feminine voice. They don’t budge. Shortly, the clock chimes again. Quail’s wife stirs. Maddeningly, the clock chimes a third time. CLOCK (continuing)Tick, tock –. Quail reaches out and shuts the clock off. Then he sits up in bed. He swings his legs out from under the covers and sits on the edge of the bed. He puts on his glasses and sits, lost in thought. He is a good-looking but conventional man in his early thirties. He seems rather in</context>
<context position="9413" citStr="Riaz and Girju, 2010" startWordPosition="1566" endWordPosition="1569"> because the results have been mixed. In particular these discourse taggers are trained on The Wall Street Journal (WSJ) and are unlikely to work well on our data. We define an event as a verb lemma with its subject and object. Two events are considered equal if they have the same verb. We do not believe word ambiguities to be a primary concern, and previous work also defines events to be the same if they have the same surface verb, in some cases with a restriction that the dependency relations should also be the same (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Riaz and Girju, 2010; Manshadi et al., 2008). Word sense ambiguities are also reduced in specific genres (Action and Romance) of film scenes. Our method for estimating the likelihood of a CONTINGENT relations between events consists of four steps: 1. TEXT PROCESSING: We use Stanford CoreNLP to annotate the corpus document by document and store the annotated text in XML format (Sec. 2.1); 2. COMPUTE EVENT REPRESENTATIONS: We form intermediate artifacts such as events, protagonists and event pairs from the annotated documents. Each event has its arguments (subject and object). We calculate the frequency of the even</context>
<context position="13109" citStr="Riaz and Girju, 2010" startWordPosition="2178" endWordPosition="2181">te Contingency Measures We calculate four different measures of CONTINGENCY based on previous work using the results of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These measures are pointwise mutual information, causal potential, bigram probability and protagonist-based 371 causal potential as described in detail below. We calculate each measure separately by genre for the action and romance genres of the film corpus. Pointwise Mutual Information. The majority of related work uses pointwise mutual information (PMI) in some form or another (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Riaz and Girju, 2010; Do et al., 2011). Given a set of events (a verb and its collected set of subjects and objects), we calculate the PMI using the standard definition: P(e1, e2) pmi(e1, e2) = log (1) P(e1)P(e2) in which e1 and e2 are two events. P(e1) is the probability that event e1 occur in the corpus: count(e1) P(e1) = E (2) x count(ex) where count(e1) is the count of how many times event e1 occurs in the corpus, and Ex count(ex) is the count of all the events in the corpus. The numerator is the probability that the two events occur together in the corpus: count(e1, e2) P(e1, e2) = (3) Ex Ey count(ex, ey) in</context>
<context position="34513" citStr="Riaz and Girju, 2010" startWordPosition="5963" endWordPosition="5966">rning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween pairs of events using a range of methods applied to a large corpus of weblog narratives (Gordon et al., 2011; Gordon and Swanson, 2009; Manshadi et al., 2008). One form of evaluation aimed to predict the last event in a sequence (Manshadi et al., 2008), while more recent work uses the learned pairs to improve performance on the COPA SEMEVAL task (Gordon et al., 2011). Related work on SCRIPT LEARNING induces likely sequences of temporally ordered events in news, rathe</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>M. Riaz and R. Girju. 2010. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, p. 361–368. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts Plans Goals.</title>
<date>1977</date>
<location>Lea.</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>R. Schank and R. Abelson. 1977. Scripts Plans Goals. Lea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A.Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proc. of the Conference on Empirical Methods in Natural Language Processing, p. 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Swanson</author>
<author>A S Gordon</author>
</authors>
<title>Say anything: Using textual case-based reasoning to enable opendomain interactive storytelling.</title>
<date>2012</date>
<journal>ACM Transactions on Interactive Intelligent Systems (TiiS),</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="22880" citStr="Swanson and Gordon, 2012" startWordPosition="3915" endWordPosition="3919">n used in refining our estimates of CONTINGENCY for the learned patterns as described above. Note that the web search patterns do not aim to find every possible match of the targeted CONTINGENT relation that could possibly occur. Instead, they are generalizations of the instances of PCEPs that we found in the films corpus that are targeted at finding hits that are the most likely to occur in narrative sequences. Narrative sequences are most reliably signalled by use of the historical present tense, e.g. as instantiated in the search patterns He knows in Row 1 and He comes in Row 2 of Table 1 (Swanson and Gordon, 2012; Beamer and Girju, 2009; Labov and Waletzky, 1997). In addition, we use the “*” operator in Google Search to limit search to pairs of events reported in the historical present tense, that are “near” one another, and in a particular sequence. We don’t care whether the events are in the same utterance or in sequential utterances, thus for the second verb (event) we do not include a subject pronoun he. These search patterns are not intended to match the original instances in the film corpus and in general they are unlikely to match those instances. For example, consider the search patterns and r</context>
<context position="25107" citStr="Swanson and Gordon, 2012" startWordPosition="4314" endWordPosition="4317">estimates, for which we found good evidence for contingency and for randomness, e.g. Row 1 and 2 in Table 1. We use 100 as a threshold because most of the time the estimate result count from Google is either a very large number (millions) or a very small number (tens), as illustrated by the NumHits columns in Table 1. We experimented with different types of patterns with a development set of PCEPs before we settled on the search pattern template shown in Table 1. We 374 decided to use third person rather than first person patterns, because first person patterns are only one type of narrative (Swanson and Gordon, 2012). We also decided to utilize event patterns without typical objects, such as head in person REST head in Row 2 of Table 1. We do not have any evidence that this is the optimal search pattern template because we did not systematically try other types of search patterns. 3 Evaluation and Results While other work uses a range of methods for evaluating accuracy, to our knowledge our work is the first to use human judgments from Mechanical Turk to evaluate the accuracy of the learned PCEPs. We first describe the evaluation setup in Sec. 3.1 and then report the results in Sec. 3.2 3.1 Mechanical Tur</context>
</contexts>
<marker>Swanson, Gordon, 2012</marker>
<rawString>R. Swanson and A. S. Gordon. 2012. Say anything: Using textual case-based reasoning to enable opendomain interactive storytelling. ACM Transactions on Interactive Intelligent Systems (TiiS), 2(3):16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>G Lin</author>
<author>J Sawyer</author>
</authors>
<title>An annotated corpus of film dialogue for learning and characterizing character style.</title>
<date>2012</date>
<booktitle>In Language Resources and Evaluation Conference, LREC2012.</booktitle>
<contexts>
<context position="7121" citStr="Walker et al., 2012" startWordPosition="1157" endWordPosition="1160"> in detail. Sec. 3 describes how we set up our evaluation experiments and the results. We show that none of the methods from previous work perform better on our data than 75.15% average accuracy as measured by human perceptions of CONTINGENCY. But after web search refinement, we achieve an average accuracy of 85.64%. We delay a more detailed comparison to previous work to Sec. 4 where we summarize our results and compare previous work to our own. 2 Experimental Method Our method uses a combination of estimating the likelihood of a CONTINGENT relation between events in a corpus of film scenes (Walker et al., 2012b), with estimates then revised through web search. Our experiments are based on two subsets of 862 film screen plays collected from the IMSDb website using its ontology of film genres (Walker et al., 2012b): a set of action movies of 115 screenplays totalling 748 MB, and a set of romance movies of 71 screenplays totalling 390 MB. Fig. 1 provided an example scene from the action movie genre from the IMSDb corpus. We assume that the relation we are aiming to learn is the PDTB CONTINGENT relation, which is defined as a relation that exists when one of the situations described in the text spans t</context>
</contexts>
<marker>Walker, Lin, Sawyer, 2012</marker>
<rawString>M. A. Walker, G. Lin, and J. Sawyer. 2012b. An annotated corpus of film dialogue for learning and characterizing character style. In Language Resources and Evaluation Conference, LREC2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Welinder</author>
<author>S Branson</author>
<author>S Belongie</author>
<author>P Perona</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems 23,</booktitle>
<pages>2424--2432</pages>
<contexts>
<context position="28644" citStr="Welinder et al., 2010" startWordPosition="4958" endWordPosition="4961">ubjective. To calculate the accuracy of each method, we computed the average correlation coefficient between each pair of raters and eliminate the 5 lowest scoring workers. We then used the perceptions of the 10 remaining workers to calculate accuracy as # of correct answers / total # of answers. In general, deciding when a MTurk worker is unreliable when the data is subjective is a difficult problem. In the future we plan to test other solutions to measuring annotator reliability as proposed in related work (Callison-Burch, 2009; Snow et al., 2008; Karger et al., 2011; Dawid and Skene, 1979; Welinder et al., 2010; Liu et al., 2012). 3.2 Results We report our results in terms of overall accuracy. Because the Mechanical Turk task is a chooseone question rather than a binary classification, Precision = Recall in our experimental results: True Positive = Number of Correct Answers True Negative = Number of Correct Answers False Positive = Number of Incorrect Answers False Positive = Number of Incorrect Answers Precision = True Positive True Positive + False Positive True Positive True Positive + False Negative The accuracies of all the methods are shown in Table 2. The results of using event arguments (per</context>
</contexts>
<marker>Welinder, Branson, Belongie, Perona, 2010</marker>
<rawString>P. Welinder, S. Branson, S. Belongie, and P. Perona. 2010. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems 23, p. 2424–2432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z-M Zhou</author>
<author>Y Xu</author>
<author>Z Y Niu</author>
<author>M Lan</author>
<author>J Su</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In In Coling 2010: Posters,</booktitle>
<pages>1507--1514</pages>
<contexts>
<context position="34042" citStr="Zhou et al., 2010" startWordPosition="5883" endWordPosition="5886">overall average accuracy to around 77%, which is 27% above the baseline. Our results indicate that the use of web search counts increases the average accuracy of our Causal Potential-based method to 85.64% as compared to an average accuracy of 75.15% without web search. To our knowledge this is the highest accuracy achieved in tasks of 377 this kind to date. Previous work on recognition of the PDTB CONTINGENT relation has used both supervised and unsupervised learning, and evaluation typically measures precision and recall against a PDTB annotated corpus (Do et al., 2011; Pitler et al., 2009; Zhou et al., 2010; Chiarcos, 2012; Louis et al., 2010). We use an unsupervised approach and measure accuracy using human perceptions. Other work by Girju and her students defined a measure called causal potential and then used film screen plays to learn a knowledge base of causal pairs of events. They evaluate the pairs by asking two trained human annotators to label whether occurrences of those pairs in their corpus are causally related (Beamer and Girju, 2009; Riaz and Girju, 2010). We also make use of their causal potential measure. Work on commonsense causal reasoning aims to learn causal relations beween </context>
</contexts>
<marker>Zhou, Xu, Niu, Lan, Su, 2010</marker>
<rawString>Z.-M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, , and C. L. Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. In In Coling 2010: Posters, p. 1507–1514.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
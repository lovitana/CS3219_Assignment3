<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.951233">
Multi-domain Adaptation for SMT Using Multi-task Learning*
</title>
<author confidence="0.998288">
Lei Cui1, Xilun Chen2, Dongdong Zhang&apos;, Shujie Liu&apos;, Mu Li&apos;, and Ming Zhou&apos;
</author>
<affiliation confidence="0.998016">
1Harbin Institute of Technology, Harbin, P.R. China
</affiliation>
<email confidence="0.485608">
leicui@hit.edu.cn
</email>
<note confidence="0.335948">
2Cornell University, Ithaca, NY, U.S.
</note>
<email confidence="0.795628">
xlchen@cs.cornell.edu
</email>
<affiliation confidence="0.937831">
&apos;Microsoft Research Asia, Beijing, P.R. China
</affiliation>
<email confidence="0.998634">
{dozhang,shujliu,muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822714285714">
Domain adaptation for SMT usually adapts
models to an individual specific domain.
However, it often lacks some correlation
among different domains where common
knowledge could be shared to improve the
overall translation quality. In this paper, we
propose a novel multi-domain adaptation ap-
proach for SMT using Multi-Task Learning
(MTL), with in-domain models tailored for
each specific domain and a general-domain
model shared by different domains. The pa-
rameters of these models are tuned jointly via
MTL so that they can learn general knowledge
more accurately and exploit domain knowl-
edge better. Our experiments on a large-
scale English-to-Chinese translation task val-
idate that the MTL-based adaptation approach
significantly and consistently improves the
translation quality compared to a non-adapted
baseline. Furthermore, it also outperforms the
individual adaptation of each specific domain.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998211166666667">
Domain adaptation is an active topic in statisti-
cal machine learning and aims to alleviate the do-
main mismatch between training and testing data.
Like many machine learning tasks, Statistical Ma-
chine Translation (SMT) assumes that the data dis-
tributions of training and testing domains are sim-
ilar. However, this assumption does not hold for
real world SMT systems since training data for
SMT models may come from a variety of domains.
The translation quality is often unsatisfactory when
This work was done while the first and second authors were
visiting Microsoft Research Asia.
translating texts from a specific domain using a gen-
eral model that is trained over a hotchpotch of bilin-
gual corpora. Therefore, domain adaptation is cru-
cial for SMT systems to achieve better performance.
Previous research on domain adaptation for SMT
includes data selection and weighting (Eck et al.,
2004; L¨u et al., 2007; Foster et al., 2010; Moore and
Lewis, 2010; Axelrod et al., 2011), mixture mod-
els (Foster and Kuhn, 2007; Koehn and Schroeder,
2007; Sennrich, 2012; Razmara et al., 2012), and
semi-supervised transductive learning (Ueffing et
al., 2007), etc. Most of these methods adapt SMT
models to a specific domain according to testing data
and have achieved good performance. It is natural
that real world SMT systems should adapt the mod-
els to multiple domains because the input may be
heterogeneous, so that the overall translation qual-
ity can be improved. Although we can easily ap-
ply these methods to multiple domains individually,
it is difficult to use the common knowledge across
different domains. To leverage the common knowl-
edge, we need to devise a multi-domain adaptation
approach that jointly adapts the SMT models.
Multi-domain adaptation has been proved quite
effective in sentiment analysis (Dredze and Cram-
mer, 2008) and web ranking (Chapelle et al., 2011),
where the commonalities and differences across
multiple domains are explicitly addressed by Multi-
task Learning (MTL). MTL is an approach that
learns one target problem with other related prob-
lems at the same time, using a shared feature repre-
sentation. The key advantage of MTL is to enable
implicit data sharing and regularization. Therefore,
it often leads to a better model for each task. Anal-
ogously, we expect that the overall translation qual-
ity can be further improved by using an MTL-based
</bodyText>
<page confidence="0.947303">
1055
</page>
<note confidence="0.8796125">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055–1065,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997972">
Figure 1: An example with N pre-defined domains, where T is the entire training corpus. Ti is the in-domain training
data for the i-th domain selected from T using the bilingual cross-entropy based method (Axelrod et al., 2011). The
in-domain TMi and LMi are trained using the in-domain training data Ti. The general-domain models TM-G and LM-
G are trained using the entire training corpus T. Si is the domain-specific SMT system for the i-th domain, leveraging
the in-domain models and the general-domain models as features.
</figureCaption>
<figure confidence="0.998800911764706">
Entire
Training Data
T
1 T2 T3
Training Data
In-domain T
...
TN
Multiple In-domain
Translation Models
&amp; Language Models
LM1
TM1
LM2
TM2
LM3
TM3
...
...
LMN
TMN
TM-G
LM-G
Domain-specific
SMT Systems
S1
S2
S3
...
SN
MTL-based Tuning
One General-domain
Translation Model
&amp; Language Model
</figure>
<figureCaption confidence="0.302206">
multi-domain adaptation approach.
</figureCaption>
<bodyText confidence="0.999948806451613">
In this paper, we use MTL to jointly adapt SMT
models to multiple domains. Specifically, we de-
velop multiple SMT systems based on mixture mod-
els, where each system is tailored for one specific
domain with an in-domain Translation Model (TM)
and an in-domain Language Model (LM). Mean-
while, all the systems share a same general-domain
TM and LM. These SMT systems are considered as
several related tasks with a shared feature represen-
tation, which fits well into a unified MTL frame-
work. With the MTL-based joint tuning, general
knowledge can be better learned by the general-
domain models, while domain knowledge can be
better exploited by the in-domain models as well.
By using a distributed stochastic learning approach
(Simianer et al., 2012), we can estimate the fea-
ture weights of multiple SMT systems at the same
time. Furthermore, we modify the algorithm to treat
in-domain and general-domain features separately,
which brings regularization to multiple SMT sys-
tems in an efficient way. Experimental results have
shown that our method can significantly improve the
translation quality on multiple domains over a non-
adapted baseline. Moreover, the MTL-based adap-
tation also outperforms the conventional individual
adaptation approach towards each domain.
The rest of the paper is organized as follows: The
proposed approach is explained in Section 2. Exper-
imental results are presented in Section 3. Section 4
introduces some related work. Section 5 concludes
the paper and suggests future research directions.
</bodyText>
<sectionHeader confidence="0.982009" genericHeader="method">
2 The Proposed Approach
</sectionHeader>
<bodyText confidence="0.9975587">
Figure 1 gives an example with N pre-defined do-
mains to illustrate the main idea. There are three
steps in the training phase. First, in-domain train-
ing data is selected according to the pre-defined do-
mains (Section 2.1). Second, in-domain models and
general-domain models are trained to develop the
domain-specific SMT systems (Section 2.2). Third,
multiple domain-specific SMT systems are tuned
jointly by using an MTL-based approach (Section
2.3).
</bodyText>
<subsectionHeader confidence="0.996873">
2.1 In-domain Data Selection
</subsectionHeader>
<bodyText confidence="0.99645775">
In the first step, in-domain bilingual data is selected
from all the bilingual data to train in-domain TMs.
We use the bilingual cross-entropy based approach
(Axelrod et al., 2011) to obtain the in-domain data:
</bodyText>
<equation confidence="0.865882">
[HI−src(s)−HG−src(s)]+[HI−tgt(t)−HG−tgt(t)] (1)
</equation>
<page confidence="0.925839">
1056
</page>
<bodyText confidence="0.999986054054054">
where {s,t} is a bilingual sentence pair in the entire
bilingual corpus. HI−xxx(·) and HG−xxx(·) repre-
sent the cross-entropy of a string according to an in-
domain LM and a general-domain LM, respectively.
”xxx” denotes either the source language (src) or the
target language (tgt). HI−sr,(s) − HG−sr,(s) is the
cross-entropy difference of string s between the in-
domain and general-domain source-side LMs, and
HI−tgt(t) − HG−tgt(t) is the cross-entropy differ-
ence of string t between the in-domain and general-
domain target-side LMs. This criterion biases to-
wards sentence pairs that are like the in-domain cor-
pus but unlike the general-domain corpus. There-
fore, the sentence pairs with lower scores (larger dif-
ferences) are presumed to be better.
Now, the question is how to find sufficient mono-
lingual data to train in-domain LMs. A straight-
forward solution is to collect the data from the in-
ternet. There are a large number of monolingual
webpages with domain information from web por-
tal sites1, which can be collected to train in-domain
LMs. In large-scale real world SMT systems, practi-
cal domain adaptation techniques should target more
domains rather than just one due to heterogeneous
input. Therefore, we use a web crawler to collect
monolingual webpages of N domains from web por-
tal sites, for both the source language and the tar-
get language. The statistics of web-crawled data is
given in Section 3.1. We use the web-crawled mono-
lingual documents to train N in-domain source-side
LMs and N in-domain target-side LMs. Addition-
ally, we also train the source-side and target-side
general-domain LMs with all the web-crawled doc-
uments from different domains. Finally, these in-
domain and general-domain LMs are used to select
in-domain bilingual data for different domains ac-
cording to Formula (1).
</bodyText>
<subsectionHeader confidence="0.997409">
2.2 SMT Systems with Mixture Models
</subsectionHeader>
<bodyText confidence="0.99996725">
In the second step, with the selected in-domain train-
ing data, we develop SMT systems based on mix-
ture models. In particular, we use the mixture model
based approach proposed by Koehn and Schroeder
</bodyText>
<footnote confidence="0.6957202">
1Many web portal sites contain domain information
for webpages, such as ”www.yahoo.com” in English and
”www.sina.com.cn” in Chinese and etc. The webpages are of-
ten categorized by human editors into different domains, such
as politics, sports, business, etc.
</footnote>
<bodyText confidence="0.911719">
(2007). Specifically, we have developed N SMT
systems for N domains respectively, where each
system is a typical log-linear model. For each sys-
tem, the best translation candidate fˆ is given by:
</bodyText>
<equation confidence="0.985371">
fˆ = arg max {P(f|e)} (2)
f
</equation>
<bodyText confidence="0.993283">
where the translation probability P(f|e) is given by:
</bodyText>
<equation confidence="0.997955">
P(f|e) oc X wi · log Oi(f, e)
i
</equation>
<bodyText confidence="0.9995718">
where Oj(f, e) is the in-domain feature function and
wj is the corresponding feature weight. Ok(f, e) is
the general-domain feature function and wk is the
feature weight. The detailed feature description is
as follows:
</bodyText>
<sectionHeader confidence="0.523393" genericHeader="method">
In-domain features
</sectionHeader>
<listItem confidence="0.999699076923077">
• An in-domain TM, including phrase translation
probabilities and lexical weights for both direc-
tions (4 features)
• An in-domain target-side LM (1 feature)
• word count (1 feature)
• phrase count (1 feature)
• NULL penalty (1 feature)
• Number of hierarchical rules used (1 feature)
General-domain features
• A general-domain TM, including phrase trans-
lation probabilities and lexical weights for both
directions (4 features)
• A general-domain target-side LM (1 feature)
</listItem>
<bodyText confidence="0.999967571428572">
The feature description indicates that each SMT
system contains two TMs and two LMs. The in-
domain TMs are trained using the selected bilin-
gual training data according to Formula (1), and the
general-domain TM is trained using the entire bilin-
gual training data. For the LMs, we re-use the target-
side in-domain LMs and general-domain LM trained
</bodyText>
<figure confidence="0.997000666666667">
X= wj · log Oj(f, e) X wk · log Ok(f, e)
jEI +
kEG
 |{z }  |{z }
In-domain General domain
(3)
</figure>
<page confidence="0.986676">
1057
</page>
<bodyText confidence="0.9997448">
for data selection (Section 2.1). Compared with a
normal single-model system, the system with mix-
ture models can balance the contributions from the
general-domain and in-domain knowledge. Hence it
potentially benefits from both.
</bodyText>
<subsectionHeader confidence="0.99504">
2.3 MTL-based Tuning
</subsectionHeader>
<bodyText confidence="0.9999435">
In the third step, the feature weights in multiple
domain-specific SMT systems are estimated. In-
stead of tuning each domain-specific system sepa-
rately, we treat different systems as related tasks and
tune them jointly in an MTL framework. There are
two main reasons for MTL-based tuning:
</bodyText>
<listItem confidence="0.976311125">
1. Domain-specific translation tasks share the
same general-domain LM and TM. MTL often
leads to better performance by leveraging com-
monalities among different tasks.
2. By enforcing that the general-domain LM and
TM perform equally across different domains,
MTL provides a kind of regularization to pre-
vent over-fitting.
</listItem>
<bodyText confidence="0.9998856">
Formally, the objective function of the proposed
MTL-based approach is described as follows:
where N is the number of pre-defined domains.
{Fi,Ei} is the in-domain development dataset for the
i-th domain. Fi denotes the source sentences and Ei
denotes the reference translations. wi is a D-length
feature weight column vector for the i-th domain,
where D is the dimension of the feature space. W is
a N-by-D matrix, representing [w1|w2 |... |wn,]T .
ˆe(Fi, wi) are the best translations obtained for Fi
with parameters wi. Loss(·, ·) denotes the loss be-
tween the system’s output and the reference trans-
lations. The basic idea of the objective function is
to minimize the sum of loss functions for all the do-
mains, rather than one domain at a time. Therefore,
by adjusting the in-domain and general-domain fea-
tureweights, the translation quality is expected to be
good across different domains.
To effectively tune SMT systems jointly, we mod-
ify the asynchronous Stochastic Gradient Descend
(SGD) Algorithm (Simianer et al., 2012) to optimize
objective function (4). We follow the pairwise rank-
ing approach with the perceptron algorithm (Shen
and Joshi, 2005) to update feature weights. Let a
translation candidate be denoted by its feature vector
v ∈ ][8D, the pairwise preference for training is con-
structed by ranking two candidates according to the
smoothed sentence-level BLEU (Liang et al., 2006).
For a preference pair vU]=(v(1), v(2)) where v(1) is
preferred, a hinge loss is used:
</bodyText>
<equation confidence="0.998422">
L(wi) = (−hwi, v(1) − v(2)i)+ (5)
</equation>
<bodyText confidence="0.99975325">
where (x)+ = max(0, x) and h·, ·i denotes the in-
ner product of two vectors. With the perceptron al-
gorithm (Shen and Joshi, 2005), the gradient of the
hinge loss is:
</bodyText>
<equation confidence="0.933782">
�
v(2) − v(1) ifhwi, v(1) − v(2)i ≤ 0
∇L(wi) = (6)
0 otherwise
</equation>
<bodyText confidence="0.999966">
The training instances for the discriminative
learning in pairwise ranking are made by comparing
the N-best list of the translation candidates scored
by the smoothed sentence-level BLEU (Liang et al.,
2006). Following Simianer et al. (2012), the N-best
list is divided into three bins: the top 10% (High),
the middle 80% (Middle), and the last 10% (Low).
These bins are used for pairwise ranking where the
translation preference pairs are built between the
candidates in High-Middle, Middle-Low, and High-
Low, but not the candidates within the same bin,
which is shown in Figure 2. The idea is to guar-
antee that the ranker is more discriminative to prefer
the good translations to the bad ones.
</bodyText>
<figureCaption confidence="0.993713">
Figure 2: Training instances for pairwise ranking.
</figureCaption>
<figure confidence="0.9246118">
I-best list
Middle: 80%
High: 10%
Low: 10%
min� Loss (Ei,6(Fi,wi)) I (4)
W
N
i=1
1058
Algorithm 1 Modified Asynchronous SGD
</figure>
<listItem confidence="0.974335818181818">
1: Distribute N domain-specific decoders to N ma-
chines
2: Initialize w1, w2, ... , wN ← 0
3: for epochs t ← 0... T − 1 do
4: for all domains d ∈ {1... N}: parallel do
5: ud,t,0,0 = wd
6: S = |Fd|
7: for all i ∈ {0 ... S − 1} do
8: Decode i-th sentence with ud,t,i,0
9: P = No. of pairs built from the N-best list
10: for all pairs v[j], j ∈ {0 ... P − 1} do
</listItem>
<figure confidence="0.858042473684211">
11: ud,t,i,j+1 ← ud,t,i,j − q∇L(ud,t,i,j)
12: end for
13: ud,t,i+1,0 ← ud,t,i,P
14: end for
15: end for
16: for all domains d ∈ {1... N} do
17: wd = ud,t,S,0
18: end for
19: WG ← [wG1  |... |wGN]T
20: for all domains d ∈ {1... N} do
21: for k ← 1... |wGd  |do
22: wGd [k] = N �n 1 W G [n] [k]
23: end for
trI
24: wd ← LWI]
�
25: end for
26: end for
27: return w1, w2, ... , wN
</figure>
<bodyText confidence="0.998883">
Our modified algorithm is illustrated in Algorithm
1. Each column vector wi is further split into two
parts w and wG , representing the In-domain and
General-domain feature weights respectively. In Al-
gorithm 1, we first distribute the domain-specific
SMT decoders to different machines and initialize
the feature weights (line 1-2). Typically, the SGD al-
gorithm runs in several iterations (In this study, we
set the number of epochs T to 20) (line 3). Multi-
ple SMT decoders run in parallel and each decoder
updates its feature weights individually using its in-
domain development data (line 4-15). For each do-
main, the domain-specific decoder translates each
in-domain development sentence and determines the
N-best translations (line 4-8). The preference pairs
are built and used to update the parameters by gra-
dient descent with q = 0.0001 (line 9-13). Each
domain-specific decoder translates its in-domain de-
velopment data multiple times. After each itera-
tion, feature weights from all decoders are collected
(line 16-19). In contrast to the original algorithm
(Simianer et al., 2012), we only average the general-
domain feature weights w1 ,... , w�N, but do not av-
erage the in-domain feature weights (line 20-25).
The reason is we hope to leverage the commonalities
among these systems. Meanwhile, general knowl-
edge is enforced to be conveyed equally across dif-
ferent domains. Finally, the algorithm returns all
the domain-specific feature weights w1, w2, ... , wN
that are used for testing (line 27).
After the joint MTL-based tuning, the feature
weights tailored for domain-specific SMT systems
are used to translate the testing data. We collect in-
domain testing data for each domain to evaluate the
domain-specific systems. Although this is not al-
ways the case in real applications where the testing
domain is known, this study mainly focuses on the
effectiveness of the MTL-based tuning approach.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.973627">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999995416666667">
We evaluated our MTL-based domain adaptation
approach on a large-scale English-to-Chinese ma-
chine translation task. The training data consisted
of two parts: monolingual data and bilingual data.
The monolingual data was used to train the source-
side and target-side LMs, both of which were used
for data selection in Section 2.1. In addition, the
target-side LMs were re-used in the SMT systems
as features. As mentioned in Section 2.1, we built a
web crawler to collect a large number of webpages
from web portal sites in English and Chinese respec-
tively. In the experiments, we mainly focused on six
popular domains, namely Business, Entertainment,
Health, Science &amp; Technology, Sports, and Politics.
For both English and Chinese webpages, the HTML
tags were removed and the main content was ex-
tracted. The data statistics are shown in Table 1.
The bilingual data we used was mainly mined
from the web using the method proposed by Jiang
et al. (2009), with a post-processing step using our
bilingual data cleaning method (Cui et al., 2013).
Therefore, the data quality is pretty good. In addi-
tion, we also used the English-Chinese parallel cor-
pus released by LDC2. In total, the bilingual data
</bodyText>
<equation confidence="0.298503">
2LDC2003E07, LDC2003E14, LDC2004E12,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,
</equation>
<page confidence="0.702695">
1059
</page>
<table confidence="0.9999595">
Domain English Chinese
Docs Words Docs Words
Business 21M 10.4B 7.91M 2.73B
Ent. 18.3M 8.29B 4.16M 1.31B
Health 8.7M 4.73B 0.9M 0.42B
Sci&amp;Tech 10.9M 5.33B 5.28M 1.6B
Sports 18.9M 9.58B 2.49M 0.59B
Politics 10.3M 5.56B 1.67M 0.39B
</table>
<tableCaption confidence="0.977141333333333">
Table 1: Statistics of web-crawled monolingual data, in
numbers of documents and words (main content). ”M”
refers to million and ”B” refers to billion.
</tableCaption>
<bodyText confidence="0.9982489">
contained around 30 million sentence pairs, with
404M words in English and 329M words in Chi-
nese. For each domain, we used the cross-entropy
based method in Section 2.1 to rank the entire bilin-
gual data, and the top 10% sentence pairs from the
ranked bilingual data were selected as the in-domain
data to train the in-domain TM. Moreover, we pre-
pared 2,000 in-domain sentences for development
and 1,000 in-domain sentences for testing in each
domain. The details are shown in Table 2.
</bodyText>
<table confidence="0.99970325">
Domain Train Dev Test
En Ch En Ch En Ch
Business 30M 28M 36K 35K 19K 19K
Ent. 25M 22M 21K 18K 13K 12K
Health 23M 20M 33K 33K 21K 22K
Sci&amp;Tech 28M 26M 46K 45K 27K 27K
Sports 19M 16M 18K 14K 10K 9K
Politics 28M 24M 19K 17K 13K 12K
</table>
<tableCaption confidence="0.9071025">
Table 2: Statistics of in-domain training, development
and testing data, in number of words.
</tableCaption>
<subsectionHeader confidence="0.995834">
3.2 Setup
</subsectionHeader>
<bodyText confidence="0.986778363636364">
An in-house hierarchical phrase-based SMT de-
coder was implemented for our experiments. The
CKY decoding algorithm was used and cube prun-
ing was performed with the same default parameter
settings as in Chiang (2007). We used a 100-best
list from the decoder for the pairwise ranking al-
gorithm. Translation models were trained over the
bilingual data that was automatically word-aligned
using GIZA++ (Och and Ney, 2003) in both direc-
tions, and the diag-grow-final heuristic was used to
LDC2006E34, LDC2006E85, LDC2006E92.
refine the symmetric word alignment. The phrase
tables were filtered to retain top-20 translation can-
didates for each source phrase for efficiency. An
in-house language modeling toolkit was used to
train the 4-gram language models with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) over
the web-crawled data. The evaluation metric for
the overall translation quality was case-insensitive
BLEU4 (Papineni et al., 2002). A statistical sig-
nificance test was performed using the bootstrap re-
sampling method (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.987786">
3.3 Baseline
</subsectionHeader>
<bodyText confidence="0.999985">
We have two baselines. The first baseline is a non-
adapted Hiero using our implementation. It con-
tained the general-domain TM and LM, as well as
other standard features. In addition, the fix-discount
method (Foster et al., 2006) for phrase table smooth-
ing was also used. The system was general-domain
oriented and it was tuned by using MERT (Och,
2003) with a combination of six in-domain develop-
ment datasets. The second baseline is Google Online
Translation Service3. We obtained the English-to-
Chinese translations of the testing data from Google
Translation to have a more solid comparison.
Moreover, we also compared our method with the
adapted systems towards each domain individually
(Koehn and Schroeder, 2007). This is to demon-
strate the superiority of our MTL-based tuning ap-
proach across different domains.
</bodyText>
<sectionHeader confidence="0.837541" genericHeader="evaluation">
3.4 Results
</sectionHeader>
<bodyText confidence="0.994234071428571">
The end-to-end translation performance is shown in
Table 3. We found that the baseline has a similar
performance to Google Translation, with certain do-
mains performed even better (Business, Sci&amp;Tech,
Sports, Politics). This demonstrates that the transla-
tion quality of our baseline is state-of-the-art. More-
over, we can answer three questions according to the
experimental results as follow:
First, is domain mismatch a significant prob-
lem for a real world SMT system? We used the
same system only with general-domain TM and LM,
but tuned towards each domain individually using
in-domain dev data. Table 3 shows that the setting
”[A] G-TM + G-LM” performs much better than
</bodyText>
<footnote confidence="0.985464">
3http://translate.google.com
</footnote>
<page confidence="0.886255">
1060
</page>
<table confidence="0.947139272727273">
Business Ent. Health Sci&amp;Tech Sports Politics
[N] Baseline (G-TM + G-LM) 27.19 17.87 25.79 25.34 25.53 23.01
Google Translation 26.01 18.44 27.71 25.07 24.08 22.97
[A] G-TM + G-LM 29.58 19.08 28.80 26.84 30.28 25.64
[A] I-TM + I-LM 28.20 17.25 27.20 25.41 30.12 22.97
[A] (G+I)-TM + G-LM 29.45 19.22 28.93 27.01 31.01 25.40
[A] (G+I)-TM + I-LM 29.60 19.43 28.94 27.05 34.36 25.98
[A] (G+I)-LM + G-TM 29.66 19.50 29.00 27.10 33.60 26.03
[A] (G+I)-LM + I-TM 28.50 17.66 27.58 25.99 30.44 23.30
[A] (G+I)-TM + (G+I)-LM 29.82 19.53 29.03 26.94 33.77 26.09
[A,MTL] (G+I)-TM + (G+I)-LM 30.26 19.94 29.08 27.17 34.11 26.50
</table>
<tableCaption confidence="0.697732166666667">
Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p &lt; 0.05). ”[N]” means the system
is non-adapted and tuned using MERT on general-domain dev data. ”[A]” denotes that the system is adapted towards
each domain individually using MERT on in-domain dev data. ”[A,MTL]” indicates that the system was tuned using
our MTL-based approach on in-domain dev data. ”I-TM” and ”G-TM” denote the in-domain and general-domain
translation model. ”I-LM” and ”G-LM” denote the in-domain and general-domain language model. We also obtained
translations of the testing data using Google Translation for comparison.
</tableCaption>
<bodyText confidence="0.999144375">
the non-adapted baseline across all domains with at
least 1.2 BLEU points. In addition, the setting ”[A]
G-TM + G-LM” also outperforms Google Transla-
tion on all domains. Analogous to previous research,
this confirms that the domain mismatch indeed ex-
ists and the parameter estimation using in-domain
dev data is quite useful.
Second, does the mixture models based adap-
tation work for a variety of domains? We experi-
mented with different settings with multiple TMs or
LMs, or both. It is interesting to note that for large-
scale SMT systems, using in-domain models alone
is inferior to using the general models alone. The
setting ”[A] G-TM + G-LM” is better than the set-
ting ”[A] I-TM + I-LM” across different domains.
The reason is the data for general models has already
included the in-domain data and the data coverage is
much larger, thus the probability estimation is more
reliable and the translation quality is much better.
For the LM, the in-domain LM performs better
than the general-domain LM because our mono-
lingual data (Table 1) for each domain is already
sufficient for training an in-domain LM with good
performance. From Table 3, we observed that the
setting ”[A] (G+I)-TM + I-LM” outperforms ”[A]
(G+I)-TM + G-LM”, with the ”Sports” domain be-
ing the most significant. For the TM, the per-
formance of the in-domain TM is inferior to the
general-domain TM. The results show that the set-
ting ”[A] (G+I)-LM + G-TM” is significantly better
than ”[A] (G+I)-LM + I-TM”. The main reason is
the data coverage for in-domain TM is much smaller
than the general model. When each system uses two
TMs and two LMs, it consistently results in better
performance, indicating that mixture models are cru-
cial for domain adaptation in SMT.
Third, can MTL further improve the transla-
tion quality? We used the MTL-based approach to
jointly tune multiple domain-specific systems, lever-
aging the commonalities among different but related
tasks. From Table 3, the MTL-based approach sig-
nificantly improve the translation quality over the
non-adapted baseline, and also outperforms conven-
tional mixture models based methods. In particular,
the ”Sports” domain benefits the most from the in-
domain knowledge, which confirms that domain dis-
crepancy should be addressed and may bring large
improvements on certain domains.
</bodyText>
<subsectionHeader confidence="0.544953">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999950625">
According to our experiments, only averaging over
the out-of-domain feature weights returned robust
and converged results. We do not have theoreti-
cally grounded guarantee. However, we observed
that the BLEU score of our method on DEV data
was slightly lower than that in the baseline system,
which indicates the out-of-domain features are less
over-fitting on the domain-specific DEV data since
</bodyText>
<page confidence="0.974754">
1061
</page>
<bodyText confidence="0.52519425">
SOURSE A point begins with a player serving the ball. This means one player hits
the ball towards the other player. (The serve must be played from behind the
baseline and must ����
land in the service box . Players get two attempts to make
</bodyText>
<equation confidence="0.922382333333334">
a good serve.)
REF 得 分 由 一 个球员发 球 开 始 , A 是 指 一 个球员向 另 一 个球员击
球。(发球时选手必须站在AA之外,球必须要����
落在对方的 发球区
每次发球允�有一次失�。)
[N] Baseline (G-TM + G-LM) 舞 会 始 于X家服 务 的 一 个 点 。 A 意 味 着X家对 A 他X家的 击 球 。
(A服务必须从背后4T的基A和必须�������
降落在 服务框 。球员两次i4图成
为一个好的服务。)
[A] (G+I)-TM + (G+I)-LM 一 开 始 球 的球员, A 意 味 着 一 名球员 球 4T 向 A 他球员。(必 须 从
A A发球, 必 须 在 发球区 的�����
区域。 球员只 有 两次 尝 i4去做一个好
的发球。)
[A,MTL](G+I)-TM + (G+I)-LM 第 一 球 的球员, A 意 味 着 一 名 球员对 另 一 个球员击 球 。(必 须 在 A
A后面发球, 并且 必须�������
降落在 发球区 。 球 员 两次 i4 图 成 为 一 个好
的发球。)
内,
</equation>
<tableCaption confidence="0.94051">
Table 4: Examples illustrating some different translations, where the Chinese phrases are translated from the English
phrases with the same symbols (e.g., underline, wavy-line, and box). The details are explained in Section 3.5.
</tableCaption>
<bodyText confidence="0.999949446808511">
we enforced them to play the same role across dif-
ferent domains. It seems that averaging the out-of-
domain feature weights can be considered as a kind
of regularization.
An example sentence from the Sports domain
with translations from different methods is shown
in Table 4. In this sentence, the baseline always
translates ”player” to ”X家” (game player), which
should be ”球 员” (ball player). And, the base-
line translates ”serve” to ”服务” (work for), which
should be ”发球” (put the ball into play). The phrase
”service box” here means ”发球区”, which denotes
the zone where the ball is to be served. However, the
baseline incorrectly splits them into two words, then
translates ”service” to ”服务” and ”box” to ”框”.
In contrast, the approaches with adapted models are
able to translate these words very well.
Both our MTL-based approach and the conven-
tional adaptation methods leverage the mixture mod-
els. A natural question is why our MTL-based ap-
proach performs better than the individual adapta-
tion. To answer this question, we looked into the
details of the tuning and decoding procedures in the
MTL-based approach. We observed that the BLEU
score on the development data for each system was
lower than the score when conducting individual
adaptation. Considering that the algorithm enforc-
ing the general features play the same role across
different domains, we suspect that MTL-based ap-
proach introduces a kind of regularization for each
domain-specific system. The regularization prevents
the general features from biasing towards certain do-
mains to the extreme. This property is quite impor-
tant for real world SMT systems. Usually, a sen-
tence is composed of some domain-specific words
and some general words, so it is often improper to
translate every word in the sentence using the in-
domain knowledge. For the example in Table 4,
the individual adaptation method ”[A] (G+I)-TM +
(G+I)-LM” translates ”land” to ”区域” (zone) im-
properly, because ”区域” appears more often in the
Sports text than the general-domain text. This shows
that the individual adaptation methods tend to over-
fit the in-domain development data. In contrast, the
MTL-based approach ”[A,MTL](G+I)-TM + (G+I)-
LM” just translates ”land” to ”降落在” (fall on),
which is more appropriate.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="related work">
4 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999475">
4.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999177">
One direction of domain adaptation explored the
data selection and weighting approach to improve
the performance of SMT on specific domains. Eck
</bodyText>
<page confidence="0.984226">
1062
</page>
<bodyText confidence="0.999988333333333">
et al. (2004) first decoded the testing data with a
general TM, and then used the translation results
to train an adapted LM, which was in turn used to
re-decode the testing data. L¨u et al. (2007) tried
to weight the training data according to the similar-
ity with test data using information retrieval mod-
els, while Foster et al. (2010) trained a discrimina-
tive model to estimate a weight for each sentence
in the training corpus. Other methods conducted
data selection based on cross-entropy (Moore and
Lewis, 2010), and Axelrod et al. (2011) further ex-
tended their cross-entropy based method to the se-
lection of bilingual corpus in the hope that more rel-
evant corpus to the target domain could yield smaller
models with better performance. Other methods
included using semi-supervised transductive learn-
ing techniques to exploit the monolingual in-domain
data (Ueffing et al., 2007).
Adaptation methods also involved the utiliza-
tion of mixture models. Foster and Kuhn (2007)
explored a number of variants of utilizing multi-
ple TMs and LMs by interpolation. Koehn and
Schroeder (2007) used MERT to simultaneously
tune two TMs or LMs. Sennrich (2012) investi-
gated the TM perplexity minimization as a method
to set model weights in mixture modeling. In ad-
dition, inspired by system combination approaches,
Razmara et al. (2012) used the ensemble decoding
method to mix multiple translation models, which
outperformed a variety of strong baselines.
Generally, most previous methods merely con-
ducted domain adaption for a single domain, rather
than multiple domains at the same time. One could
also simply build multiple SMT systems that were
adapted to multiple domains, but they were often
separated and not tuned together. So far, there has
been little research into the multi-domain adaptation
problem over mixture models for SMT systems, as
proposed in this paper.
</bodyText>
<subsectionHeader confidence="0.966415">
4.2 Multi-task Learning
</subsectionHeader>
<bodyText confidence="0.999944027027027">
In machine learning, MTL is an approach to learn
one target problem with other related problems at
the same time. This often leads to a better model for
the main task because it allows the learner to use the
commonality among the tasks. MTL is performed
by learning tasks in parallel while using a shared
representation. Therefore, what is learned for each
task can help other tasks be learned better.
MTL was successfully applied in some Natu-
ral Language Processing (NLP) tasks. For exam-
ple, Blitzer et al. (2006) extended the MTL ap-
proach (Ando and Zhang, 2005) to domain adapta-
tion tasks in part-of-speech tagging. Collobert and
Weston (2008) proposed using deep neural networks
to train a set of tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic roles labeling. They reported that jointly
learning these tasks led to superior performance.
MTL was also applied in sentiment analysis (Dredze
and Crammer, 2008) and web ranking (Chapelle
et al., 2011) to address the multi-domain learning
and adaptation. In SMT, Duh et al. (2010) pro-
posed using MTL for N-best re-ranking on sparse
feature sets, where each N-best list corresponded to
a distinct task. Simianer et al. (2012) proposed dis-
tributed stochastic learning with feature selection in-
spired by MTL. The distributed learning approach
outperformed several other training methods includ-
ing MIRA and SGD.
Inspired by these methods, we used MTL to tune
multiple SMT systems at the same time, where each
system was composed of in-domain and general-
domain models. Through a shared feature represen-
tation, the commonalities among the SMT systems
were better learned by the general models. In ad-
dition, domain-specific translation knowledge was
also better characterized by the in-domain models.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999907133333333">
In this paper, we propose an MTL-based approach to
address multi-domain adaptation for SMT. We first
use the cross-entropy based data selection method
to obtain in-domain bilingual data. After that, in-
domain TMs and LMs are trained for each domain-
specific SMT system. In addition, the general-
domain TM and LM are also trained and shared
across different systems. Finally, MTL is lever-
aged to tune multiple systems jointly. Experimen-
tal results have shown that our approach is quite
promising for the multi-domain adaptation problem,
and it brings significant improvement over both the
non-adapted baselines and the conventional domain
adaptation methods with mixture models.
We assume the domain information for testing
</bodyText>
<page confidence="0.87403">
1063
</page>
<bodyText confidence="0.999958714285714">
data is known beforehand in this study. However,
this is not always the case for real world SMT sys-
tems. Therefore, to apply our approach in real appli-
cations, the domain information needs to be identi-
fied automatically. In the future, we will pre-define
more popular domains and develop automatic do-
main classifiers. For those domains that are iden-
tified with high confidence, we use the domain-
specific system to translate the texts. For other texts,
we use the general system to translate them. Fur-
thermore, since our approach is a general training
method, we may also combine this approach with
other domain adaptation methods to get more per-
formance improvement.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999185">
We are especially grateful to Nan Yang, Yajuan
Duan, Hong Sun and Danran Chen for the helpful
discussions. We also thank the anonymous review-
ers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.986527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969543271604938">
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learning
Research, 6:1817–1853.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 355–362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120–128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Machine
learning, 85(1-2):149–173.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, pages 160–167. ACM.
Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and Ming
Zhou. 2013. Bilingual data cleaning for smt using
graph-based random walk. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 340–345,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 689–697, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best reranking
by multitask learning. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
ricsMATR, pages 375–383, Uppsala, Sweden, July.
Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Proc.
of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128–135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53–61, Sydney, Australia, July. Association for
Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451–459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining bilingual data from the
web with adaptively learnt patterns. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
870–878, Suntec, Singapore, August. Association for
Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. ICASSP-
95., 1995 International Conference on, volume 1,
pages 181–184. IEEE.
</reference>
<page confidence="0.588188">
1064
</page>
<reference confidence="0.999793986842105">
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224–227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761–768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343–350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 940–949, Jeju Island, Korea, July. Association
for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 539–549, Avignon, France,
April. Association for Computational Linguistics.
Libin Shen and Aravind K Joshi. 2005. Ranking and
reranking with perceptron. Machine Learning, 60(1-
3):73–96.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11–21, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25–32, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.974145">
1065
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377932">
<title confidence="0.988776">Adaptation for SMT Using Multi-task</title>
<affiliation confidence="0.784701">Institute of Technology, Harbin, P.R. University, Ithaca, NY,</affiliation>
<address confidence="0.819838">Research Asia, Beijing, P.R.</address>
<abstract confidence="0.999056454545455">Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>6--1817</pages>
<contexts>
<context position="31784" citStr="Ando and Zhang, 2005" startWordPosition="5201" endWordPosition="5204">stems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list cor</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2257" citStr="Axelrod et al., 2011" startWordPosition="331" endWordPosition="334">MT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowle</context>
<context position="4086" citStr="Axelrod et al., 2011" startWordPosition="620" endWordPosition="623">ing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055–1065, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: An example with N pre-defined domains, where T is the entire training corpus. Ti is the in-domain training data for the i-th domain selected from T using the bilingual cross-entropy based method (Axelrod et al., 2011). The in-domain TMi and LMi are trained using the in-domain training data Ti. The general-domain models TM-G and LMG are trained using the entire training corpus T. Si is the domain-specific SMT system for the i-th domain, leveraging the in-domain models and the general-domain models as features. Entire Training Data T 1 T2 T3 Training Data In-domain T ... TN Multiple In-domain Translation Models &amp; Language Models LM1 TM1 LM2 TM2 LM3 TM3 ... ... LMN TMN TM-G LM-G Domain-specific SMT Systems S1 S2 S3 ... SN MTL-based Tuning One General-domain Translation Model &amp; Language Model multi-domain adap</context>
<context position="6903" citStr="Axelrod et al., 2011" startWordPosition="1065" endWordPosition="1068">fined domains to illustrate the main idea. There are three steps in the training phase. First, in-domain training data is selected according to the pre-defined domains (Section 2.1). Second, in-domain models and general-domain models are trained to develop the domain-specific SMT systems (Section 2.2). Third, multiple domain-specific SMT systems are tuned jointly by using an MTL-based approach (Section 2.3). 2.1 In-domain Data Selection In the first step, in-domain bilingual data is selected from all the bilingual data to train in-domain TMs. We use the bilingual cross-entropy based approach (Axelrod et al., 2011) to obtain the in-domain data: [HI−src(s)−HG−src(s)]+[HI−tgt(t)−HG−tgt(t)] (1) 1056 where {s,t} is a bilingual sentence pair in the entire bilingual corpus. HI−xxx(·) and HG−xxx(·) represent the cross-entropy of a string according to an indomain LM and a general-domain LM, respectively. ”xxx” denotes either the source language (src) or the target language (tgt). HI−sr,(s) − HG−sr,(s) is the cross-entropy difference of string s between the indomain and general-domain source-side LMs, and HI−tgt(t) − HG−tgt(t) is the cross-entropy difference of string t between the in-domain and generaldomain ta</context>
<context position="29873" citStr="Axelrod et al. (2011)" startWordPosition="4893" endWordPosition="4896">ing approach to improve the performance of SMT on specific domains. Eck 1062 et al. (2004) first decoded the testing data with a general TM, and then used the translation results to train an adapted LM, which was in turn used to re-decode the testing data. L¨u et al. (2007) tried to weight the training data according to the similarity with test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355–362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="31735" citStr="Blitzer et al. (2006)" startWordPosition="5192" endWordPosition="5195">adaptation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking o</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Pannagadatta Shivaswamy</author>
<author>Srinivas Vadrevu</author>
<author>Kilian Weinberger</author>
<author>Ya Zhang</author>
<author>Belle Tseng</author>
</authors>
<date>2011</date>
<booktitle>Boosted multi-task learning. Machine learning,</booktitle>
<pages>85--1</pages>
<contexts>
<context position="3154" citStr="Chapelle et al., 2011" startWordPosition="474" endWordPosition="477">hieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1</context>
<context position="32213" citStr="Chapelle et al., 2011" startWordPosition="5266" endWordPosition="5269"> other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representati</context>
</contexts>
<marker>Chapelle, Shivaswamy, Vadrevu, Weinberger, Zhang, Tseng, 2011</marker>
<rawString>Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas Vadrevu, Kilian Weinberger, Ya Zhang, and Belle Tseng. 2011. Boosted multi-task learning. Machine learning, 85(1-2):149–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="19580" citStr="Chiang (2007)" startWordPosition="3197" endWordPosition="3198">nces for testing in each domain. The details are shown in Table 2. Domain Train Dev Test En Ch En Ch En Ch Business 30M 28M 36K 35K 19K 19K Ent. 25M 22M 21K 18K 13K 12K Health 23M 20M 33K 33K 21K 22K Sci&amp;Tech 28M 26M 46K 45K 27K 27K Sports 19M 16M 18K 14K 10K 9K Politics 28M 24M 19K 17K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to LDC2006E34, LDC2006E85, LDC2006E92. refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="31866" citStr="Collobert and Weston (2008)" startWordPosition="5213" endWordPosition="5216">g, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochast</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Bilingual data cleaning for smt using graph-based random walk.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>340--345</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="17922" citStr="Cui et al., 2013" startWordPosition="2915" endWordPosition="2918">Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science &amp; Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2. In total, the bilingual data 2LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, 1059 Domain English Chinese Docs Words Docs Words Business 21M 10.4B 7.91M 2.73B Ent. 18.3M 8.29B 4.16M 1.31B Health 8.7M 4.73B 0.9M 0.42B Sci&amp;Tech 10.9M 5.33B 5.28M 1.6B Sports 18.9M 9.58B 2.49M 0.59B Politics 10.3M 5.56B 1.67M 0.39B Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). ”M” refers to million </context>
</contexts>
<marker>Cui, Zhang, Liu, Li, Zhou, 2013</marker>
<rawString>Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and Ming Zhou. 2013. Bilingual data cleaning for smt using graph-based random walk. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 340–345, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>689--697</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3114" citStr="Dredze and Crammer, 2008" startWordPosition="466" endWordPosition="470">omain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based 1055 Proceedings of the 2013 Conference on Empirical Methods</context>
<context position="32173" citStr="Dredze and Crammer, 2008" startWordPosition="5259" endWordPosition="5262">ore, what is learned for each task can help other tasks be learned better. MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain model</context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 689–697, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
<author>Masaaki Nagata</author>
</authors>
<title>N-best reranking by multitask learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>375--383</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="32292" citStr="Duh et al. (2010)" startWordPosition="5279" endWordPosition="5282">e Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general </context>
</contexts>
<marker>Duh, Sudoh, Tsukada, Isozaki, Nagata, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki Isozaki, and Masaaki Nagata. 2010. N-best reranking by multitask learning. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 375–383, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Language model adaptation for statistical machine translation based on information retrieval.</title>
<date>2004</date>
<booktitle>In In Proc. of LREC.</booktitle>
<contexts>
<context position="2172" citStr="Eck et al., 2004" startWordPosition="315" endWordPosition="318">ing domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply the</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Language model adaptation for statistical machine translation based on information retrieval. In In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2296" citStr="Foster and Kuhn, 2007" startWordPosition="338" endWordPosition="341">models may come from a variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To levera</context>
<context position="30306" citStr="Foster and Kuhn (2007)" startWordPosition="4959" endWordPosition="4962">inative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same t</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="20651" citStr="Foster et al., 2006" startWordPosition="3358" endWordPosition="3361">y. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 Baseline We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3. We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 Results</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 53–61, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2211" citStr="Foster et al., 2010" startWordPosition="323" endWordPosition="326">is assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individu</context>
<context position="29666" citStr="Foster et al. (2010)" startWordPosition="4860" endWordPosition="4863">MTL](G+I)-TM + (G+I)- LM” just translates ”land” to ”降落在” (fall on), which is more appropriate. 4 Related Work 4.1 Domain Adaptation One direction of domain adaptation explored the data selection and weighting approach to improve the performance of SMT on specific domains. Eck 1062 et al. (2004) first decoded the testing data with a general TM, and then used the translation results to train an adapted LM, which was in turn used to re-decode the testing data. L¨u et al. (2007) tried to weight the training data according to the similarity with test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization o</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Shiquan Yang</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Qingsheng Zhu</author>
</authors>
<title>Mining bilingual data from the web with adaptively learnt patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="17833" citStr="Jiang et al. (2009)" startWordPosition="2901" endWordPosition="2904">addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science &amp; Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC2. In total, the bilingual data 2LDC2003E07, LDC2003E14, LDC2004E12, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, 1059 Domain English Chinese Docs Words Docs Words Business 21M 10.4B 7.91M 2.73B Ent. 18.3M 8.29B 4.16M 1.31B Health 8.7M 4.73B 0.9M 0.42B Sci&amp;Tech 10.9M 5.33B 5.28M 1.6B Sports 18.9M 9.58B 2.49M 0.59B Politics 10.3M 5.56B 1.67M 0.39B Table 1: Statistics of web-crawled m</context>
</contexts>
<marker>Jiang, Yang, Zhou, Liu, Zhu, 2009</marker>
<rawString>Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and Qingsheng Zhu. 2009. Mining bilingual data from the web with adaptively learnt patterns. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 870–878, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="20174" citStr="Kneser and Ney, 1995" startWordPosition="3284" endWordPosition="3287">ettings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to LDC2006E34, LDC2006E85, LDC2006E92. refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 Baseline We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2323" citStr="Koehn and Schroeder, 2007" startWordPosition="342" endWordPosition="345">variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we</context>
<context position="21140" citStr="Koehn and Schroeder, 2007" startWordPosition="3434" endWordPosition="3437">It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3. We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to Google Translation, with certain domains performed even better (Business, Sci&amp;Tech, Sports, Politics). This demonstrates that the translation quality of our baseline is state-of-the-art. Moreover, we can answer three questions according to the experimental results as follow: First, is domain mismatch a significant problem for a real world SMT system? We used the same sy</context>
<context position="30415" citStr="Koehn and Schroeder (2007)" startWordPosition="4977" endWordPosition="4980"> selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same time. One could also simply build multiple SMT systems that were adapted to multiple domains, but they were of</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="20409" citStr="Koehn, 2004" startWordPosition="3320" endWordPosition="3321">tions, and the diag-grow-final heuristic was used to LDC2006E34, LDC2006E85, LDC2006E92. refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 Baseline We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3. We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comp</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--768</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 761–768, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>343--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>L¨u, Huang, Liu, 2007</marker>
<rawString>Yajuan L¨u, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 343–350, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2234" citStr="Moore and Lewis, 2010" startWordPosition="327" endWordPosition="330">t hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult t</context>
<context position="29846" citStr="Moore and Lewis, 2010" startWordPosition="4888" endWordPosition="4891">he data selection and weighting approach to improve the performance of SMT on specific domains. Eck 1062 et al. (2004) first decoded the testing data with a general TM, and then used the translation results to train an adapted LM, which was in turn used to re-decode the testing data. L¨u et al. (2007) tried to weight the training data according to the similarity with test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tu</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19783" citStr="Och and Ney, 2003" startWordPosition="3227" endWordPosition="3230">K Sci&amp;Tech 28M 26M 46K 45K 27K 27K Sports 19M 16M 18K 14K 10K 9K Politics 28M 24M 19K 17K 13K 12K Table 2: Statistics of in-domain training, development and testing data, in number of words. 3.2 Setup An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to LDC2006E34, LDC2006E85, LDC2006E92. refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resam</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="20775" citStr="Och, 2003" startWordPosition="3382" endWordPosition="3383">ey, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 Baseline We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3. We obtained the English-toChinese translations of the testing data from Google Translation to have a more solid comparison. Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains. 3.4 Results The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to Google </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="20310" citStr="Papineni et al., 2002" startWordPosition="3303" endWordPosition="3306">ed over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to LDC2006E34, LDC2006E85, LDC2006E92. refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004). 3.3 Baseline We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service3. We obtained the E</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>George Foster</author>
<author>Baskaran Sankaran</author>
<author>Anoop Sarkar</author>
</authors>
<title>Mixing multiple translation models in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>940--949</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2362" citStr="Razmara et al., 2012" startWordPosition="348" endWordPosition="351"> is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptati</context>
<context position="30656" citStr="Razmara et al. (2012)" startWordPosition="5016" endWordPosition="5019">ller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same time. One could also simply build multiple SMT systems that were adapted to multiple domains, but they were often separated and not tuned together. So far, there has been little research into the multi-domain adaptation problem over mixture models for SMT systems, as proposed in this paper. 4.2 Multi-task Learning In machine learning, MTL is an appr</context>
</contexts>
<marker>Razmara, Foster, Sankaran, Sarkar, 2012</marker>
<rawString>Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models in statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 940–949, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>539--549</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="2339" citStr="Sennrich, 2012" startWordPosition="346" endWordPosition="347">nslation quality is often unsatisfactory when This work was done while the first and second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise </context>
<context position="30480" citStr="Sennrich (2012)" startWordPosition="4990" endWordPosition="4991">. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same time. One could also simply build multiple SMT systems that were adapted to multiple domains, but they were often separated and not tuned together. So far, there has been litt</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539–549, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Ranking and reranking with perceptron.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="12761" citStr="Shen and Joshi, 2005" startWordPosition="2007" endWordPosition="2010">otes the loss between the system’s output and the reference translations. The basic idea of the objective function is to minimize the sum of loss functions for all the domains, rather than one domain at a time. Therefore, by adjusting the in-domain and general-domain featureweights, the translation quality is expected to be good across different domains. To effectively tune SMT systems jointly, we modify the asynchronous Stochastic Gradient Descend (SGD) Algorithm (Simianer et al., 2012) to optimize objective function (4). We follow the pairwise ranking approach with the perceptron algorithm (Shen and Joshi, 2005) to update feature weights. Let a translation candidate be denoted by its feature vector v ∈ ][8D, the pairwise preference for training is constructed by ranking two candidates according to the smoothed sentence-level BLEU (Liang et al., 2006). For a preference pair vU]=(v(1), v(2)) where v(1) is preferred, a hinge loss is used: L(wi) = (−hwi, v(1) − v(2)i)+ (5) where (x)+ = max(0, x) and h·, ·i denotes the inner product of two vectors. With the perceptron algorithm (Shen and Joshi, 2005), the gradient of the hinge loss is: � v(2) − v(1) ifhwi, v(1) − v(2)i ≤ 0 ∇L(wi) = (6) 0 otherwise The tra</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Libin Shen and Aravind K Joshi. 2005. Ranking and reranking with perceptron. Machine Learning, 60(1-3):73–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>11--21</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5447" citStr="Simianer et al., 2012" startWordPosition="843" endWordPosition="846">ased on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual adaptation approach towards each domain. The rest of the paper is organized as follows: The proposed approach is explained in</context>
<context position="12632" citStr="Simianer et al., 2012" startWordPosition="1987" endWordPosition="1990"> matrix, representing [w1|w2 |... |wn,]T . ˆe(Fi, wi) are the best translations obtained for Fi with parameters wi. Loss(·, ·) denotes the loss between the system’s output and the reference translations. The basic idea of the objective function is to minimize the sum of loss functions for all the domains, rather than one domain at a time. Therefore, by adjusting the in-domain and general-domain featureweights, the translation quality is expected to be good across different domains. To effectively tune SMT systems jointly, we modify the asynchronous Stochastic Gradient Descend (SGD) Algorithm (Simianer et al., 2012) to optimize objective function (4). We follow the pairwise ranking approach with the perceptron algorithm (Shen and Joshi, 2005) to update feature weights. Let a translation candidate be denoted by its feature vector v ∈ ][8D, the pairwise preference for training is constructed by ranking two candidates according to the smoothed sentence-level BLEU (Liang et al., 2006). For a preference pair vU]=(v(1), v(2)) where v(1) is preferred, a hinge loss is used: L(wi) = (−hwi, v(1) − v(2)i)+ (5) where (x)+ = max(0, x) and h·, ·i denotes the inner product of two vectors. With the perceptron algorithm </context>
<context position="16042" citStr="Simianer et al., 2012" startWordPosition="2607" endWordPosition="2610"> decoders run in parallel and each decoder updates its feature weights individually using its indomain development data (line 4-15). For each domain, the domain-specific decoder translates each in-domain development sentence and determines the N-best translations (line 4-8). The preference pairs are built and used to update the parameters by gradient descent with q = 0.0001 (line 9-13). Each domain-specific decoder translates its in-domain development data multiple times. After each iteration, feature weights from all decoders are collected (line 16-19). In contrast to the original algorithm (Simianer et al., 2012), we only average the generaldomain feature weights w1 ,... , w�N, but do not average the in-domain feature weights (line 20-25). The reason is we hope to leverage the commonalities among these systems. Meanwhile, general knowledge is enforced to be conveyed equally across different domains. Finally, the algorithm returns all the domain-specific feature weights w1, w2, ... , wN that are used for testing (line 27). After the joint MTL-based tuning, the feature weights tailored for domain-specific SMT systems are used to translate the testing data. We collect indomain testing data for each domai</context>
<context position="32436" citStr="Simianer et al. (2012)" startWordPosition="5303" endWordPosition="5306"> part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD. Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general models. In addition, domain-specific translation knowledge was also better characterized by the in-domain models. 5 Conclusion and Future Work I</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11–21, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2428" citStr="Ueffing et al., 2007" startWordPosition="356" endWordPosition="359">nd second authors were visiting Microsoft Research Asia. translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance. Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L¨u et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adapt</context>
<context position="30214" citStr="Ueffing et al., 2007" startWordPosition="4945" endWordPosition="4948"> test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007). Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods mere</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25–32, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
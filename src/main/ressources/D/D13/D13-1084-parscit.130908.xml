<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.825635">
Word Level Language Identification in Online Multilingual Communication
</title>
<author confidence="0.647219">
Dong Nguyen&apos; A. Seza Do˘gru¨oz23
</author>
<listItem confidence="0.942660666666667">
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Tilburg School of Humanities, Tilburg University, Tilburg, The Netherlands
(3) Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA
</listItem>
<email confidence="0.983529">
dong.p.ng@gmail.com, a.s.dogruoz@gmail.com
</email>
<sectionHeader confidence="0.996412" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998705583333333">
Multilingual speakers switch between lan-
guages in online and spoken communication.
Analyses of large scale multilingual data re-
quire automatic language identification at the
word level. For our experiments with mul-
tilingual online discussions, we first tag the
language of individual words using language
models and dictionaries. Secondly, we incor-
porate context to improve the performance.
We achieve an accuracy of 98%. Besides word
level accuracy, we use two new metrics to
evaluate this task.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98144262745098">
There are more multilingual speakers in the world
than monolingual speakers (Auer and Wei, 2007).
Multilingual speakers switch across languages in
daily communication (Auer, 1999). With the in-
creasing use of social media, multilingual speakers
also communicate with each other in online environ-
ments (Paolillo, 2011). Data from such resources
can be used to study code switching patterns and lan-
guage preferences in online multilingual conversa-
tions. Although most studies on multilingual online
communication rely on manual identification of lan-
guages in relatively small datasets (Danet and Her-
ring, 2007; Androutsopoulos, 2007), there is a grow-
ing demand for automatic language identification in
larger datasets. Such a system would also be useful
for selecting the right parsers to process multilingual
documents and to build language resources for mi-
nority languages (King and Abney, 2013).
In this paper, we identify Dutch (NL) en Turkish
(TR) at the word level in a large online forum for
Turkish-Dutch speakers living in the Netherlands.
The users in the forum frequently switch languages
within posts, for example:
&lt;TR&gt; Sariyi ver &lt;/TR&gt;
&lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt;
So far, language identification has mostly been mod-
eled as a document classification problem. Most ap-
proaches rely on character or byte n-grams, by com-
paring n-gram profiles (Cavnar and Trenkle, 1994),
or using various machine learning classifiers. While
McNamee (2005) argues that language identification
is a solved problem, classification on a more fine-
grained level (instead of document level) remains a
challenge (Hughes et al., 2006). Furthermore, lan-
guage identification is more difficult for short texts
(Baldwin and Lui, 2010; Vatanen et al., 2010), such
as queries and tweets (Bergsma et al., 2012; Carter
et al., 2012; Ceylan and Kim, 2009). Tagging in-
dividual words (without context) has been done us-
ing dictionaries, affix statistics and classifiers us-
ing character n-grams (Hammarstr¨om, 2007; Got-
tron and Lipka, 2010). Although Yamaguchi and
Tanaka-Ishii (2012) segmented text by language,
their data was artificially created by randomly sam-
pling and concatenating text segments (40-160 char-
acters) from monolingual texts. Therefore, the lan-
guage switches do not reflect realistic switches as
they occur in natural texts. Most related to ours is
the work by King and Abney (2013) who labeled
languages of words in multilingual web pages, but
evaluated the task only using word level accuracy.
</bodyText>
<page confidence="0.967337">
857
</page>
<bodyText confidence="0.940849266666667">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Our paper makes the following contributions: 1)
We explore two new ways to evaluate the task for an-
alyzing multilingual communication and show that
only word accuracy gives a limited view 2) We are
the first to apply this task on a conversational and
larger dataset 3) We show that features using the
context improve the performance 4) We present a
new public dataset to support research on language
identification.
In the rest of the paper, we first discuss the related
work and describe our dataset. Secondly, we present
our experiments. We finally conclude with a sum-
mary and suggestions for future work.
</bodyText>
<sectionHeader confidence="0.987298" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999355909090909">
Our data1 comes from one of the largest online
communities in The Netherlands for Turkish-Dutch
speakers. All posts from May 2006 until October
2012 were crawled. Although Dutch and Turkish
dominate the forum, English fixed phrases (e.g. no
comment, come on) are also occasionally observed.
Users switch between languages within and across
posts. Examples 1 and 2 illustrate switches between
Dutch and Turkish within the same post. Example 1
is a switch at sentence level, example 2 is a switch
at word level.
</bodyText>
<figure confidence="0.890132916666666">
Example 1:
&lt;NL&gt;Mijn dag kan niet stuk :) &lt;/NL&gt;
&lt;TR&gt; Cok guzel bir haber aldim &lt;/TR&gt;
Translation: &lt;NL&gt; This made my day:)
&lt;/NL&gt;&lt;TR&gt; I received good news
&lt;/TR&gt;
Example 2:
&lt;TR&gt;kahvalti&lt;/TR&gt;&lt;NL&gt;met
vriendinnen by my thuis &lt;/NL&gt;
Translation: &lt;TR&gt;breakfast &lt;/TR&gt;
&lt;NL&gt; with my girlfriends at my home
&lt;/NL&gt;
</figure>
<bodyText confidence="0.999427714285714">
The data is highly informal with misspellings,
lengthening of characters (e.g. hotttt), replacement
of Turkish characters (kahvalti instead of kahvaltı)
and spelling variations (tankyu instead of thank
you). Dutch and Turkish sometimes share common
spellings (e.g. ben is am in Dutch and I in Turkish),
making this a challenging task.
</bodyText>
<footnote confidence="0.9952125">
1Available at http://www.dongnguyen.nl/data-langid-
emnlp2013.html
</footnote>
<bodyText confidence="0.9283007">
Annotation
For this research, we classify words as either Turkish
or Dutch. Since Dutch and English are typologically
more similar to each other than Turkish, the English
phrases (less than 1%) are classified as Dutch. Posts
were randomly sampled and annotated by a native
Turkish speaker who is also fluent in Dutch. A na-
tive Dutch speaker annotated a random set of 100
posts (Cohen’s kappa = 0.98). The following tokens
were ignored for language identification:
</bodyText>
<listItem confidence="0.997293454545454">
• Smileys (as part of the forum markup, as well
as textual smileys such as “:)” ).
• Numeric tokens and punctuation.
• Forum tags (e.g. [u] to underline text).
• Links, images, embedded videos etc.
• Turkish and Dutch first names and place
names2.
• Usernames when indicated with special forum
markup.
• Chat words, such as hahaha, ooooh and lol rec-
ognized using regular expressions.
</listItem>
<bodyText confidence="0.963862928571429">
Posts for which all tokens are ignored, are not
included in the corpus.
Statistics
The dataset was randomly divided into a training,
development and test set. The statistics are listed
in Table 1. The statistics show that Dutch is the
majority language, although the difference between
Turkish and Dutch is not large. We also find that the
documents (i.e. posts) are short, with on average 18
tokens per document. The data represents realistic
texts found in online multilingual communication.
Compared to previously used datasets (Yamaguchi
and Tanaka-Ishii, 2012; King and Abney, 2013), the
data is noisier and the documents are much shorter.
</bodyText>
<table confidence="0.77048025">
#NL tokens #TR tokens #Posts/(BL%)
Train 14900 (54%) 12737 (46%) 1603 (15%)
Dev 8590 (51%) 8140 (49%) 728 (19%)
Test 5895 (53%) 5293 (47%) 735 (17%)
</table>
<tableCaption confidence="0.99829">
Table 1: Number of tokens and posts for Dutch (NL) and
Turkish (TR), including % of bilingual (BL) posts
</tableCaption>
<footnote confidence="0.716923">
2Based on online name lists and Wikipedia pages
</footnote>
<page confidence="0.996859">
858
</page>
<sectionHeader confidence="0.996218" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.999007">
3.1 Training Corpora
</subsectionHeader>
<bodyText confidence="0.999443">
We used the following corpora to extract dictionaries
and language models.
</bodyText>
<listItem confidence="0.9848804">
• GenCor: Turkish web pages (Sak et al., 2008).
• NLCOW2012: Dutch web pages (Sch¨afer and
Bildhauer, 2012).
• Blog authorship corpus: English blogs (Schler
et al., 2006).
</listItem>
<bodyText confidence="0.999833">
Each corpus was chunked into large segments
which were then selected randomly until 5M tokens
were obtained for each language. We tokenized the
text and kept the punctuation.
</bodyText>
<subsectionHeader confidence="0.99864">
3.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999995583333333">
As baselines, we use langid.py3 (Lui and Bald-
win, 2012) and van Noord’s TextCat implementa-
tion4 of the algorithm by Cavnar and Trenkle (1994).
TextCat is based on the comparison of n-gram pro-
files and langid.py on Naive Bayes with n-gram fea-
tures. For both baselines, words were entered indi-
vidually to each program. Words for which no lan-
guage could be determined were assigned to Dutch.
These models were developed to identify the lan-
guages of the documents instead of words and we
did not retrain them. Therefore, these models are
not expected to perform well on this task.
</bodyText>
<subsectionHeader confidence="0.998209">
3.3 Models
</subsectionHeader>
<bodyText confidence="0.9999964">
We start with models that assign languages based on
only the current word. Next, we explore models and
features that can exploit the context (the other words
in the post). Words with the highest probability for
English were assigned to Dutch for evaluation.
</bodyText>
<sectionHeader confidence="0.876991" genericHeader="method">
Dictionary lookup (DICT)
</sectionHeader>
<bodyText confidence="0.999989">
We extract dictionaries with word frequencies from
the training corpora. This approach looks up the
words in the dictionaries and chooses the language
for which the word has the highest probability. If
the word does not occur in the dictionaries, Dutch is
chosen as the language.
</bodyText>
<footnote confidence="0.9998595">
3https://github.com/saffsd/langid.py
4http://www.let.rug.nl/—vannoord/TextCat/
</footnote>
<subsectionHeader confidence="0.278803">
Language model (LM)
</subsectionHeader>
<bodyText confidence="0.931859875">
We build a character n-gram language model for
each language (max. n-gram length is 5). We use
Witten-Bell smoothing and include word boundaries
for calculating the probabilities.
Dictionary + Language model (DICT+LM)
We first use the dictionary lookup approach (DICT).
If the word does not occur in dictionaries, a decision
is made using the language models (LM).
</bodyText>
<subsectionHeader confidence="0.60741">
Logistic Regression (LR)
</subsectionHeader>
<bodyText confidence="0.999925">
We use a logistic regression model that incorporates
context with the following features:
</bodyText>
<listItem confidence="0.999570666666667">
• (Individual word) Label assigned by the
DICT+LM model.
• (Context) The results of the LM model based on
</listItem>
<bodyText confidence="0.852296142857143">
previous + current token, and current token +
next token (e.g. the sequence “ben thuis” (am
home) as a whole if ben is the current token).
This gives the language model more context for
estimation. We compare the use of the assigned
labels (LAB) with the use of the log probability
values (PROB) as feature values.
</bodyText>
<subsectionHeader confidence="0.467884">
Conditional Random Fields (CRF)
</subsectionHeader>
<bodyText confidence="0.980790666666667">
We treat the task as a sequence labeling problem and
experiment with linear-chain Conditional Random
Fields (Lafferty et al., 2001) in three settings:
</bodyText>
<listItem confidence="0.864629714285714">
• (Individual word) A CRF with only the tags as-
signed by the DICT+LM to the individual to-
kens as a feature (BASE).
• (Context). CRFs using the LAB or PROB as ad-
ditional features (same features as in the logis-
tic regression model) to capture additional con-
text.
</listItem>
<subsectionHeader confidence="0.911932">
3.4 Implementation
</subsectionHeader>
<bodyText confidence="0.992750333333333">
Language identification was not performed for texts
within quotes. To handle the alphabetical length-
ening (e.g. lolllll), words are normalized by trim-
ming same character sequences of three characters
or more. We use the Lingpipe5 and Scikit-learn (Pe-
dregosa et al., 2011) toolkits for our experiments.
</bodyText>
<footnote confidence="0.98179">
5http://alias-i.com/lingpipe/
</footnote>
<page confidence="0.980966">
859
</page>
<table confidence="0.999693153846154">
Run Word classification Acc. P Fraction BL Post classification
TR NL MAE Fl Acc.
P R P R All Mono.
Textcat 0.872 0.647 0.743 0.915 0.788 0.739 0.251 0.264 0.188 0.386 0.396
LangIDPy 0.954 0.387 0.641 0.983 0.701 0.615 0.364 0.371 0.333 0.413 0.475
DICT 0.955 0.733 0.802 0.969 0.858 0.827 0.196 0.200 0.175 0.511 0.531
LM 0.950 0.930 0.938 0.956 0.944 0.926 0.074 0.076 0.065 0.699 0.703
DICT + LM 0.951 0.934 0.942 0.957 0.946 0.943 0.067 0.067 0.063 0.711 0.717
LR + LAB 0.965 0.952 0.958 0.969 0.961 0.917 0.066 0.066 0.068 0.791 0.808
LR + PROB 0.956 0.976 0.978 0.959 0.967 0.945 0.048 0.044 0.064 0.826 0.849
CRF + BASE 0.973 0.974 0.977 0.976 0.975 0.940 0.043 0.027 0.119 0.858 0.898
CRF + LAB 0.964 0.977 0.979 0.967 0.972 0.933 0.046 0.033 0.111 0.855 0.891
CRF + PROB 0.970 0.980 0.982 0.973 0.976 0.946 0.039 0.025 0.103 0.853 0.895
</table>
<tableCaption confidence="0.963657">
Table 2: Results of language identification experiments .
</tableCaption>
<subsectionHeader confidence="0.725442">
3.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.999956888888889">
The assigned labels can be used for computational
analysis of multilingual data in different ways. For
example, these labels can be used to analyze lan-
guage preferences in multilingual communication or
the direction of the switches (from Turkish to Dutch
or the other way around). Therefore, we evaluate the
methods from different perspectives.
The evaluation at word and post levels is done
with the following metrics:
</bodyText>
<listItem confidence="0.999687571428571">
• Word classification precision (P), recall (R) and
accuracy. Although this is the most straightfor-
ward approach to evaluate the task, it ignores
the document boundaries.
• Fraction of language in a post: Pearson’s cor-
relation (p) and Mean Absolute Error (MAE) of
proportion of Turkish in a post. This evaluates
the measured proportion of languages in a post
when the actual tags for individual words are
not needed. For example, such information is
useful for analyzing the language preferences
of users in the online forum. Besides report-
ing the MAE over all posts, we also separate
the performance over monolingual and bilin-
gual posts (BL).
• Post classification: Durham (2003) analyzed
the switch between languages in terms of the
amount of monolingual and bilingual posts.
Our posts are classified as NL, TR or bilingual
(BL) if all words are tagged in the particular
language or both. We report Fi and accuracy.
</listItem>
<sectionHeader confidence="0.999668" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999775625">
The results are presented in Table 2. Significance
tests were done by comparing the results of the word
and post classification measures using McNemar’s
test, and comparing the MAEs using paired t-tests.
All runs were significantly different from each other
based on these tests (p &lt; 0.05), except the MAEs of
the DICT+LM and LR+LAB runs and the MAEs and
post classification metrics between the CRFs runs.
The difficulty of the task is illustrated by exam-
ining the coverage of the tokens by the dictionaries.
24.6% of the tokens (dev + test set) appear in both
dictionaries, 31.1% only in the Turkish dictionary,
30.5% only in the Dutch dictionary and 13.9% in
none of the dictionaries.
The baselines do not perform well. This confirms
that language identification at the word level needs
different approaches than identification at the docu-
ment level. Using language models result in a bet-
ter performance than dictionaries. They can han-
dle unseen words and are more robust against the
noisy spellings. The combination of language mod-
els and dictionaries is more effective than the indi-
vidual models. The results improve when context
was added using a logistic regression model, espe-
cially with the probability values as feature values.
CRFs improve the results but the improvement
on the correlation and MAE is less. More specifi-
cally, CRFs improve the performance on monolin-
gual posts, especially when a single word is tagged
in the wrong language. However, when the influence
of the context is too high, CRFs reduce the perfor-
mance in bilingual posts.
</bodyText>
<page confidence="0.989235">
860
</page>
<bodyText confidence="0.999943466666667">
This is also illustrated with the results of the post
classification. The LR+PROB run has a high recall
(0.905), but a low precision (0.559) for bilingual
posts, while the CRF+PROB approach has a low re-
call (0.611) and a high precision (0.828).
The fraction of Dutch and Turkish in posts varies
widely, providing additional challenges to the use of
CRFs for this task. Classifying posts first as mono-
lingual/bilingual and tagging individual words after-
wards for bilingual posts might improve the perfor-
mance.
The evaluation metrics highlight different aspects
of the task whereas word level accuracy gives a
limited view. We suggest using multiple metrics to
evaluate this task for future research.
</bodyText>
<subsubsectionHeader confidence="0.851063">
Dictionaries versus Language Models
</subsubsectionHeader>
<bodyText confidence="0.999924909090909">
The results reported in Table 2 were obtained by
sampling 5M tokens of each language. To study the
effect of the number of tokens on the performance
of the DICT and LM runs, we vary the amount of
data. The performance of both methods increases
consistently with more data (Figure 1). We also
find that language models achieve good performance
with only a limited amount of data, and consistently
outperform the approach using dictionaries. This is
probably due to the highly informal and noisy nature
of our data.
</bodyText>
<figureCaption confidence="0.999411">
Figure 1: Effect of sampling size
</figureCaption>
<bodyText confidence="0.963627875">
Post classification
We experimented with classifying posts into TR, NL
and bilingual using the results of the word level lan-
guage identification (Table 2: post classification).
Posts were classified as a particular language if all
words were tagged as belonging to that language,
and bilingual otherwise. Runs using CRFs achieved
the best performance.
We now experiment with allowing a margin (e.g.
a margin of 0.10 classifies posts as TR if at least
90% of the words are classified as TR). Allowing
a small margin already increases the results of sim-
pler approaches (such as the LR-PROB run, Table 3)
by making it more robust against errors. However,
allowing a margin reduces the performance of the
CRF runs.
</bodyText>
<table confidence="0.9940365">
Margin 0.0 0.05 0.10 0.15 0.20
Accuracy 0.849 0.873 0.876 0.878 0.865
</table>
<tableCaption confidence="0.9374105">
Table 3: Effect of margin on post classification
(LR-PROB run)
</tableCaption>
<bodyText confidence="0.975894352941177">
Error analysis
The manual analysis of the results revealed three
main challenges: 1) Our data is highly informal
with many spelling variations (e.g. moimoimoi,
goooooooooooolllll) and noise (e.g. asdfghjfgsha-
haha) 2) Words sharing spelling in Dutch and Turk-
ish are difficult to identify especially when there
is no context available (e.g. a post with only one
word). These words are annotated based on their
context. For example, the word super in “Seyma,
super” is annotated as Turkish since Seyma is also a
Turkish word. 3) Named entity recognition is neces-
sary to improve the performance of the system and
decrease the noise in evaluation. Based on precom-
piled lists, our system ignores named entities. How-
ever, some names still remain undetected (e.g. user-
names).
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999948571428571">
We presented experiments on identifying the lan-
guage of individual words in multilingual conversa-
tional data. Our results reveal that language models
are more robust than dictionaries and adding context
improves the performance. We evaluate our methods
from different perspectives based on how language
identification at word level can be used to analyze
multilingual data. The highly informal spelling in
online environments and the occurrences of named
entities pose challenges.
Future work could focus on cases with more than
two languages, and languages that are typologically
less distinct from each other or dialects (Trieschnigg
et al., 2012).
</bodyText>
<figure confidence="0.987291333333333">
LM
DICT
0 2 × 106 4 × 106
Num. sampled tokens
Accuracy
0.6 0.7 0.8 0.9 1.0
</figure>
<page confidence="0.9884">
861
</page>
<sectionHeader confidence="0.995921" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999948857142857">
The first author was supported by the Netherlands
Organization for Scientific Research (NWO) grant
640.005.002 (FACT) and the second author through
a postdoctoral research grant in E-Humanities (Digi-
tal Humanities) by Tilburg University (NL). The au-
thors would like to thank Mari¨et Theune and Dolf
Trieschnigg for feedback.
</bodyText>
<sectionHeader confidence="0.996582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991731225806451">
J. Androutsopoulos, 2007. The multilingual internet.
Language, Culture and communication online, chapter
Language choice and code-switching in German-based
diasporic web-forums., pages 340–361. Oxford: Ox-
ford University Press.
P. Auer and L. Wei. 2007. Introduction: Multilingual-
ism as a problem? Monolingualism as a problem? In
Handbook of Multilingualism and Multilingual Com-
munication, volume 5 of Handbooks of Applied Lin-
guistics, pages 1–14. Mouton de Gruyter.
P. Auer. 1999. From codeswitching via language mix-
ing to fused lects toward a dynamic typology of bilin-
gual speech. International Journal of Bilingualism,
3(4):309–332.
T. Baldwin and M. Lui. 2010. Language identification:
the long and the short of the matter. In Proceedings of
NAACL 2010.
S. Bergsma, P. McNamee, M. Bagdouri, C. Fink, and
T. Wilson. 2012. Language identification for creat-
ing language-specific twitter collections. In Proceed-
ings of the Second Workshop on Language in Social
Media.
S. Carter, W. Weerkamp, and M. Tsagkias. 2012. Mi-
croblog language identification: Overcoming the limi-
tations of short, unedited and idiomatic text. Language
Resources and Evaluation, pages 1–21.
W.B. Cavnar and J. M. Trenkle. 1994. N-gram-based
text categorization. In Proceedings of Third Annual
Symposium on Document Analysis and Information
Retrieval.
H. Ceylan and Y. Kim. 2009. Language identification of
search engine queries. In Proceedings of ACL 2009.
B. Danet and S. C. Herring. 2007. The multilingual In-
ternet: Language, culture, and communication online.
Oxford University Press Oxford.
M. Durham. 2003. Language choice on a Swiss mailing
list. Journal of Computer-Mediated Communication,
9(1).
T. Gottron and N. Lipka. 2010. A comparison of lan-
guage identification approaches on short, query-style
texts. In Proceedings of ECIR 2010.
H. Hammarstr¨om. 2007. A fine-grained model for lan-
guage identification. In Proceedings of iNEWS-07
Workshop at SIGIR 2007.
B. Hughes, T. Baldwin, S. Bird, J. Nicholson, and
A. Mackinlay. 2006. Reconsidering language identifi-
cation for written language resources. In Proceedings
of LREC 2006.
B. King and S. Abney. 2013. Labeling the languages
of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL 2013.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML 2001.
M. Lui and T. Baldwin. 2012. langid.py: an off-the-shelf
language identification tool. In Proceedings of ACL
2012.
P. McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94–101.
J.C. Paolillo. 2011. “Conversational” codeswitching on
Usenet and Internet Relay Chat. Language@Internet,
8(3).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: machine learning in Python.
Journal of Machine Learning Research, 12:2825–
2830.
H. Sak, T. G¨ung¨or, and M. Sarac¸lar. 2008. Turkish lan-
guage resources: Morphological parser, morphologi-
cal disambiguator and web corpus. In GoTAL 2008,
volume 5221 of LNCS, pages 417–427. Springer.
R. Sch¨afer and F. Bildhauer. 2012. Building large cor-
pora from the web using a new efficient tool chain. In
Proceedings of LREC 2012.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Compu-
tational Approaches for Analyzing Weblogs.
D. Trieschnigg, D. Hiemstra, M. Theune, F. Jong, and
T. Meder. 2012. An exploration of language identifi-
cation techniques for the Dutch folktale database. In
Adaptation of Language Resources and Tools for Pro-
cessing Cultural Heritage workshop (LREC 2012).
T. Vatanen, J. J. V¨ayrynen, and S. Virpioja. 2010. Lan-
guage identification of short text segments with n-
gram models. In Proceedings of LREC 2010.
H. Yamaguchi and K. Tanaka-Ishii. 2012. Text segmen-
tation by language using minimum description length.
In Proceedings of ACL 2012.
</reference>
<page confidence="0.997918">
862
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.261124">
<title confidence="0.976711">Word Level Language Identification in Online Multilingual Communication</title>
<author confidence="0.514251">Seza</author>
<address confidence="0.7600915">(1) Human Media Interaction, University of Twente, Enschede, The Netherlands (2) Tilburg School of Humanities, Tilburg University, Tilburg, The Netherlands</address>
<affiliation confidence="0.571054">(3) Language Technologies Institute, Carnegie Mellon University, Pittsburgh,</affiliation>
<email confidence="0.997559">dong.p.ng@gmail.com,a.s.dogruoz@gmail.com</email>
<abstract confidence="0.999454153846154">Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Androutsopoulos</author>
</authors>
<title>The multilingual internet. Language, Culture and communication online, chapter Language choice and code-switching in German-based diasporic web-forums.,</title>
<date>2007</date>
<pages>340--361</pages>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="1536" citStr="Androutsopoulos, 2007" startWordPosition="212" endWordPosition="213">e more multilingual speakers in the world than monolingual speakers (Auer and Wei, 2007). Multilingual speakers switch across languages in daily communication (Auer, 1999). With the increasing use of social media, multilingual speakers also communicate with each other in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification in larger datasets. Such a system would also be useful for selecting the right parsers to process multilingual documents and to build language resources for minority languages (King and Abney, 2013). In this paper, we identify Dutch (NL) en Turkish (TR) at the word level in a large online forum for Turkish-Dutch speakers living in the Netherlands. The users in the forum frequently switch languages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification has mostly been modeled</context>
</contexts>
<marker>Androutsopoulos, 2007</marker>
<rawString>J. Androutsopoulos, 2007. The multilingual internet. Language, Culture and communication online, chapter Language choice and code-switching in German-based diasporic web-forums., pages 340–361. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Auer</author>
<author>L Wei</author>
</authors>
<title>Introduction: Multilingualism as a problem? Monolingualism as a problem?</title>
<date>2007</date>
<booktitle>In Handbook of Multilingualism and Multilingual Communication,</booktitle>
<volume>5</volume>
<pages>1--14</pages>
<note>Mouton de Gruyter.</note>
<contexts>
<context position="1002" citStr="Auer and Wei, 2007" startWordPosition="133" endWordPosition="136">m Abstract Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task. 1 Introduction There are more multilingual speakers in the world than monolingual speakers (Auer and Wei, 2007). Multilingual speakers switch across languages in daily communication (Auer, 1999). With the increasing use of social media, multilingual speakers also communicate with each other in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification </context>
</contexts>
<marker>Auer, Wei, 2007</marker>
<rawString>P. Auer and L. Wei. 2007. Introduction: Multilingualism as a problem? Monolingualism as a problem? In Handbook of Multilingualism and Multilingual Communication, volume 5 of Handbooks of Applied Linguistics, pages 1–14. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Auer</author>
</authors>
<title>From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech.</title>
<date>1999</date>
<journal>International Journal of Bilingualism,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1085" citStr="Auer, 1999" startWordPosition="145" endWordPosition="146">n. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task. 1 Introduction There are more multilingual speakers in the world than monolingual speakers (Auer and Wei, 2007). Multilingual speakers switch across languages in daily communication (Auer, 1999). With the increasing use of social media, multilingual speakers also communicate with each other in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification in larger datasets. Such a system would also be useful for selecting the right pars</context>
</contexts>
<marker>Auer, 1999</marker>
<rawString>P. Auer. 1999. From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech. International Journal of Bilingualism, 3(4):309–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>M Lui</author>
</authors>
<title>Language identification: the long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="2615" citStr="Baldwin and Lui, 2010" startWordPosition="380" endWordPosition="383">anguages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural text</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>T. Baldwin and M. Lui. 2010. Language identification: the long and the short of the matter. In Proceedings of NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>P McNamee</author>
<author>M Bagdouri</author>
<author>C Fink</author>
<author>T Wilson</author>
</authors>
<title>Language identification for creating language-specific twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media.</booktitle>
<contexts>
<context position="2688" citStr="Bergsma et al., 2012" startWordPosition="393" endWordPosition="396">elpunt &lt;/NL&gt; So far, language identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled </context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>S. Bergsma, P. McNamee, M. Bagdouri, C. Fink, and T. Wilson. 2012. Language identification for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language in Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carter</author>
<author>W Weerkamp</author>
<author>M Tsagkias</author>
</authors>
<title>Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>1--21</pages>
<contexts>
<context position="2709" citStr="Carter et al., 2012" startWordPosition="397" endWordPosition="400">anguage identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled languages of words in</context>
</contexts>
<marker>Carter, Weerkamp, Tsagkias, 2012</marker>
<rawString>S. Carter, W. Weerkamp, and M. Tsagkias. 2012. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, pages 1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Cavnar</author>
<author>J M Trenkle</author>
</authors>
<title>N-gram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of Third Annual Symposium on Document Analysis and Information Retrieval.</booktitle>
<contexts>
<context position="2281" citStr="Cavnar and Trenkle, 1994" startWordPosition="331" endWordPosition="334"> for selecting the right parsers to process multilingual documents and to build language resources for minority languages (King and Abney, 2013). In this paper, we identify Dutch (NL) en Turkish (TR) at the word level in a large online forum for Turkish-Dutch speakers living in the Netherlands. The users in the forum frequently switch languages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om</context>
<context position="7835" citStr="Cavnar and Trenkle (1994)" startWordPosition="1215" endWordPosition="1218">s 858 3 Experimental Setup 3.1 Training Corpora We used the following corpora to extract dictionaries and language models. • GenCor: Turkish web pages (Sak et al., 2008). • NLCOW2012: Dutch web pages (Sch¨afer and Bildhauer, 2012). • Blog authorship corpus: English blogs (Schler et al., 2006). Each corpus was chunked into large segments which were then selected randomly until 5M tokens were obtained for each language. We tokenized the text and kept the punctuation. 3.2 Baselines As baselines, we use langid.py3 (Lui and Baldwin, 2012) and van Noord’s TextCat implementation4 of the algorithm by Cavnar and Trenkle (1994). TextCat is based on the comparison of n-gram profiles and langid.py on Naive Bayes with n-gram features. For both baselines, words were entered individually to each program. Words for which no language could be determined were assigned to Dutch. These models were developed to identify the languages of the documents instead of words and we did not retrain them. Therefore, these models are not expected to perform well on this task. 3.3 Models We start with models that assign languages based on only the current word. Next, we explore models and features that can exploit the context (the other w</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W.B. Cavnar and J. M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of Third Annual Symposium on Document Analysis and Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ceylan</author>
<author>Y Kim</author>
</authors>
<title>Language identification of search engine queries.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2732" citStr="Ceylan and Kim, 2009" startWordPosition="401" endWordPosition="404">n has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled languages of words in multilingual web pages</context>
</contexts>
<marker>Ceylan, Kim, 2009</marker>
<rawString>H. Ceylan and Y. Kim. 2009. Language identification of search engine queries. In Proceedings of ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Danet</author>
<author>S C Herring</author>
</authors>
<title>The multilingual Internet: Language, culture, and communication online.</title>
<date>2007</date>
<publisher>Oxford University Press</publisher>
<location>Oxford.</location>
<contexts>
<context position="1512" citStr="Danet and Herring, 2007" startWordPosition="207" endWordPosition="211">. 1 Introduction There are more multilingual speakers in the world than monolingual speakers (Auer and Wei, 2007). Multilingual speakers switch across languages in daily communication (Auer, 1999). With the increasing use of social media, multilingual speakers also communicate with each other in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification in larger datasets. Such a system would also be useful for selecting the right parsers to process multilingual documents and to build language resources for minority languages (King and Abney, 2013). In this paper, we identify Dutch (NL) en Turkish (TR) at the word level in a large online forum for Turkish-Dutch speakers living in the Netherlands. The users in the forum frequently switch languages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification</context>
</contexts>
<marker>Danet, Herring, 2007</marker>
<rawString>B. Danet and S. C. Herring. 2007. The multilingual Internet: Language, culture, and communication online. Oxford University Press Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Durham</author>
</authors>
<title>Language choice on a Swiss mailing list.</title>
<date>2003</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="12662" citStr="Durham (2003)" startWordPosition="2005" endWordPosition="2006">ugh this is the most straightforward approach to evaluate the task, it ignores the document boundaries. • Fraction of language in a post: Pearson’s correlation (p) and Mean Absolute Error (MAE) of proportion of Turkish in a post. This evaluates the measured proportion of languages in a post when the actual tags for individual words are not needed. For example, such information is useful for analyzing the language preferences of users in the online forum. Besides reporting the MAE over all posts, we also separate the performance over monolingual and bilingual posts (BL). • Post classification: Durham (2003) analyzed the switch between languages in terms of the amount of monolingual and bilingual posts. Our posts are classified as NL, TR or bilingual (BL) if all words are tagged in the particular language or both. We report Fi and accuracy. 4 Results The results are presented in Table 2. Significance tests were done by comparing the results of the word and post classification measures using McNemar’s test, and comparing the MAEs using paired t-tests. All runs were significantly different from each other based on these tests (p &lt; 0.05), except the MAEs of the DICT+LM and LR+LAB runs and the MAEs a</context>
</contexts>
<marker>Durham, 2003</marker>
<rawString>M. Durham. 2003. Language choice on a Swiss mailing list. Journal of Computer-Mediated Communication, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gottron</author>
<author>N Lipka</author>
</authors>
<title>A comparison of language identification approaches on short, query-style texts.</title>
<date>2010</date>
<booktitle>In Proceedings of ECIR</booktitle>
<contexts>
<context position="2913" citStr="Gottron and Lipka, 2010" startWordPosition="427" endWordPosition="431">ng various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled languages of words in multilingual web pages, but evaluated the task only using word level accuracy. 857 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862, Seattle, Washingt</context>
</contexts>
<marker>Gottron, Lipka, 2010</marker>
<rawString>T. Gottron and N. Lipka. 2010. A comparison of language identification approaches on short, query-style texts. In Proceedings of ECIR 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hammarstr¨om</author>
</authors>
<title>A fine-grained model for language identification.</title>
<date>2007</date>
<booktitle>In Proceedings of iNEWS-07 Workshop at SIGIR</booktitle>
<marker>Hammarstr¨om, 2007</marker>
<rawString>H. Hammarstr¨om. 2007. A fine-grained model for language identification. In Proceedings of iNEWS-07 Workshop at SIGIR 2007.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Hughes</author>
<author>T Baldwin</author>
<author>S Bird</author>
<author>J Nicholson</author>
</authors>
<location>and</location>
<marker>Hughes, Baldwin, Bird, Nicholson, </marker>
<rawString>B. Hughes, T. Baldwin, S. Bird, J. Nicholson, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mackinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Mackinlay, 2006</marker>
<rawString>A. Mackinlay. 2006. Reconsidering language identification for written language resources. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B King</author>
<author>S Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="1800" citStr="King and Abney, 2013" startWordPosition="252" endWordPosition="255">her in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification in larger datasets. Such a system would also be useful for selecting the right parsers to process multilingual documents and to build language resources for minority languages (King and Abney, 2013). In this paper, we identify Dutch (NL) en Turkish (TR) at the word level in a large online forum for Turkish-Dutch speakers living in the Netherlands. The users in the forum frequently switch languages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved p</context>
<context position="3275" citStr="King and Abney (2013)" startWordPosition="484" endWordPosition="487">s and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled languages of words in multilingual web pages, but evaluated the task only using word level accuracy. 857 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Our paper makes the following contributions: 1) We explore two new ways to evaluate the task for analyzing multilingual communication and show that only word accuracy gives a limited view 2) We are the first to apply this task on a conversational and larger dataset 3) We show that fe</context>
<context position="6852" citStr="King and Abney, 2013" startWordPosition="1052" endWordPosition="1055">gnized using regular expressions. Posts for which all tokens are ignored, are not included in the corpus. Statistics The dataset was randomly divided into a training, development and test set. The statistics are listed in Table 1. The statistics show that Dutch is the majority language, although the difference between Turkish and Dutch is not large. We also find that the documents (i.e. posts) are short, with on average 18 tokens per document. The data represents realistic texts found in online multilingual communication. Compared to previously used datasets (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), the data is noisier and the documents are much shorter. #NL tokens #TR tokens #Posts/(BL%) Train 14900 (54%) 12737 (46%) 1603 (15%) Dev 8590 (51%) 8140 (49%) 728 (19%) Test 5895 (53%) 5293 (47%) 735 (17%) Table 1: Number of tokens and posts for Dutch (NL) and Turkish (TR), including % of bilingual (BL) posts 2Based on online name lists and Wikipedia pages 858 3 Experimental Setup 3.1 Training Corpora We used the following corpora to extract dictionaries and language models. • GenCor: Turkish web pages (Sak et al., 2008). • NLCOW2012: Dutch web pages (Sch¨afer and Bildhauer, 2012). • Blog aut</context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>B. King and S. Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="10008" citStr="Lafferty et al., 2001" startWordPosition="1562" endWordPosition="1565"> context with the following features: • (Individual word) Label assigned by the DICT+LM model. • (Context) The results of the LM model based on previous + current token, and current token + next token (e.g. the sequence “ben thuis” (am home) as a whole if ben is the current token). This gives the language model more context for estimation. We compare the use of the assigned labels (LAB) with the use of the log probability values (PROB) as feature values. Conditional Random Fields (CRF) We treat the task as a sequence labeling problem and experiment with linear-chain Conditional Random Fields (Lafferty et al., 2001) in three settings: • (Individual word) A CRF with only the tags assigned by the DICT+LM to the individual tokens as a feature (BASE). • (Context). CRFs using the LAB or PROB as additional features (same features as in the logistic regression model) to capture additional context. 3.4 Implementation Language identification was not performed for texts within quotes. To handle the alphabetical lengthening (e.g. lolllll), words are normalized by trimming same character sequences of three characters or more. We use the Lingpipe5 and Scikit-learn (Pedregosa et al., 2011) toolkits for our experiments</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lui</author>
<author>T Baldwin</author>
</authors>
<title>langid.py: an off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="7749" citStr="Lui and Baldwin, 2012" startWordPosition="1200" endWordPosition="1204"> including % of bilingual (BL) posts 2Based on online name lists and Wikipedia pages 858 3 Experimental Setup 3.1 Training Corpora We used the following corpora to extract dictionaries and language models. • GenCor: Turkish web pages (Sak et al., 2008). • NLCOW2012: Dutch web pages (Sch¨afer and Bildhauer, 2012). • Blog authorship corpus: English blogs (Schler et al., 2006). Each corpus was chunked into large segments which were then selected randomly until 5M tokens were obtained for each language. We tokenized the text and kept the punctuation. 3.2 Baselines As baselines, we use langid.py3 (Lui and Baldwin, 2012) and van Noord’s TextCat implementation4 of the algorithm by Cavnar and Trenkle (1994). TextCat is based on the comparison of n-gram profiles and langid.py on Naive Bayes with n-gram features. For both baselines, words were entered individually to each program. Words for which no language could be determined were assigned to Dutch. These models were developed to identify the languages of the documents instead of words and we did not retrain them. Therefore, these models are not expected to perform well on this task. 3.3 Models We start with models that assign languages based on only the curren</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>M. Lui and T. Baldwin. 2012. langid.py: an off-the-shelf language identification tool. In Proceedings of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
</authors>
<title>Language identification: a solved problem suitable for undergraduate instruction.</title>
<date>2005</date>
<journal>Journal of Computing Sciences in Colleges,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="2350" citStr="McNamee (2005)" startWordPosition="342" endWordPosition="343">language resources for minority languages (King and Abney, 2013). In this paper, we identify Dutch (NL) en Turkish (TR) at the word level in a large online forum for Turkish-Dutch speakers living in the Netherlands. The users in the forum frequently switch languages within posts, for example: &lt;TR&gt; Sariyi ver &lt;/TR&gt; &lt;NL&gt; Wel mooi doelpunt &lt;/NL&gt; So far, language identification has mostly been modeled as a document classification problem. Most approaches rely on character or byte n-grams, by comparing n-gram profiles (Cavnar and Trenkle, 1994), or using various machine learning classifiers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii</context>
</contexts>
<marker>McNamee, 2005</marker>
<rawString>P. McNamee. 2005. Language identification: a solved problem suitable for undergraduate instruction. Journal of Computing Sciences in Colleges, 20(3):94–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Paolillo</author>
</authors>
<date>2011</date>
<booktitle>Conversational” codeswitching on Usenet and Internet Relay Chat. Language@Internet,</booktitle>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="1222" citStr="Paolillo, 2011" startWordPosition="166" endWordPosition="167">ltilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task. 1 Introduction There are more multilingual speakers in the world than monolingual speakers (Auer and Wei, 2007). Multilingual speakers switch across languages in daily communication (Auer, 1999). With the increasing use of social media, multilingual speakers also communicate with each other in online environments (Paolillo, 2011). Data from such resources can be used to study code switching patterns and language preferences in online multilingual conversations. Although most studies on multilingual online communication rely on manual identification of languages in relatively small datasets (Danet and Herring, 2007; Androutsopoulos, 2007), there is a growing demand for automatic language identification in larger datasets. Such a system would also be useful for selecting the right parsers to process multilingual documents and to build language resources for minority languages (King and Abney, 2013). In this paper, we id</context>
</contexts>
<marker>Paolillo, 2011</marker>
<rawString>J.C. Paolillo. 2011. “Conversational” codeswitching on Usenet and Internet Relay Chat. Language@Internet, 8(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2830</pages>
<contexts>
<context position="10579" citStr="Pedregosa et al., 2011" startWordPosition="1657" endWordPosition="1661">in Conditional Random Fields (Lafferty et al., 2001) in three settings: • (Individual word) A CRF with only the tags assigned by the DICT+LM to the individual tokens as a feature (BASE). • (Context). CRFs using the LAB or PROB as additional features (same features as in the logistic regression model) to capture additional context. 3.4 Implementation Language identification was not performed for texts within quotes. To handle the alphabetical lengthening (e.g. lolllll), words are normalized by trimming same character sequences of three characters or more. We use the Lingpipe5 and Scikit-learn (Pedregosa et al., 2011) toolkits for our experiments. 5http://alias-i.com/lingpipe/ 859 Run Word classification Acc. P Fraction BL Post classification TR NL MAE Fl Acc. P R P R All Mono. Textcat 0.872 0.647 0.743 0.915 0.788 0.739 0.251 0.264 0.188 0.386 0.396 LangIDPy 0.954 0.387 0.641 0.983 0.701 0.615 0.364 0.371 0.333 0.413 0.475 DICT 0.955 0.733 0.802 0.969 0.858 0.827 0.196 0.200 0.175 0.511 0.531 LM 0.950 0.930 0.938 0.956 0.944 0.926 0.074 0.076 0.065 0.699 0.703 DICT + LM 0.951 0.934 0.942 0.957 0.946 0.943 0.067 0.067 0.063 0.711 0.717 LR + LAB 0.965 0.952 0.958 0.969 0.961 0.917 0.066 0.066 0.068 0.791 0.</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: machine learning in Python. Journal of Machine Learning Research, 12:2825– 2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sak</author>
<author>T G¨ung¨or</author>
<author>M Sarac¸lar</author>
</authors>
<title>Turkish language resources: Morphological parser, morphological disambiguator and web corpus.</title>
<date>2008</date>
<booktitle>In GoTAL</booktitle>
<volume>5221</volume>
<pages>417--427</pages>
<publisher>Springer.</publisher>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2008</marker>
<rawString>H. Sak, T. G¨ung¨or, and M. Sarac¸lar. 2008. Turkish language resources: Morphological parser, morphological disambiguator and web corpus. In GoTAL 2008, volume 5221 of LNCS, pages 417–427. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sch¨afer</author>
<author>F Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>R. Sch¨afer and F. Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of LREC 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schler</author>
<author>M Koppel</author>
<author>S Argamon</author>
<author>J Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs.</booktitle>
<contexts>
<context position="7503" citStr="Schler et al., 2006" startWordPosition="1161" endWordPosition="1164">ocuments are much shorter. #NL tokens #TR tokens #Posts/(BL%) Train 14900 (54%) 12737 (46%) 1603 (15%) Dev 8590 (51%) 8140 (49%) 728 (19%) Test 5895 (53%) 5293 (47%) 735 (17%) Table 1: Number of tokens and posts for Dutch (NL) and Turkish (TR), including % of bilingual (BL) posts 2Based on online name lists and Wikipedia pages 858 3 Experimental Setup 3.1 Training Corpora We used the following corpora to extract dictionaries and language models. • GenCor: Turkish web pages (Sak et al., 2008). • NLCOW2012: Dutch web pages (Sch¨afer and Bildhauer, 2012). • Blog authorship corpus: English blogs (Schler et al., 2006). Each corpus was chunked into large segments which were then selected randomly until 5M tokens were obtained for each language. We tokenized the text and kept the punctuation. 3.2 Baselines As baselines, we use langid.py3 (Lui and Baldwin, 2012) and van Noord’s TextCat implementation4 of the algorithm by Cavnar and Trenkle (1994). TextCat is based on the comparison of n-gram profiles and langid.py on Naive Bayes with n-gram features. For both baselines, words were entered individually to each program. Words for which no language could be determined were assigned to Dutch. These models were de</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>J. Schler, M. Koppel, S. Argamon, and J. Pennebaker. 2006. Effects of age and gender on blogging. In Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Trieschnigg</author>
<author>D Hiemstra</author>
<author>M Theune</author>
<author>F Jong</author>
<author>T Meder</author>
</authors>
<title>An exploration of language identification techniques for the Dutch folktale database.</title>
<date>2012</date>
<booktitle>In Adaptation of Language Resources and Tools for Processing Cultural Heritage workshop (LREC</booktitle>
<marker>Trieschnigg, Hiemstra, Theune, Jong, Meder, 2012</marker>
<rawString>D. Trieschnigg, D. Hiemstra, M. Theune, F. Jong, and T. Meder. 2012. An exploration of language identification techniques for the Dutch folktale database. In Adaptation of Language Resources and Tools for Processing Cultural Heritage workshop (LREC 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Vatanen</author>
<author>J J V¨ayrynen</author>
<author>S Virpioja</author>
</authors>
<title>Language identification of short text segments with ngram models.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Vatanen, V¨ayrynen, Virpioja, 2010</marker>
<rawString>T. Vatanen, J. J. V¨ayrynen, and S. Virpioja. 2010. Language identification of short text segments with ngram models. In Proceedings of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamaguchi</author>
<author>K Tanaka-Ishii</author>
</authors>
<title>Text segmentation by language using minimum description length.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2957" citStr="Yamaguchi and Tanaka-Ishii (2012)" startWordPosition="433" endWordPosition="436">iers. While McNamee (2005) argues that language identification is a solved problem, classification on a more finegrained level (instead of document level) remains a challenge (Hughes et al., 2006). Furthermore, language identification is more difficult for short texts (Baldwin and Lui, 2010; Vatanen et al., 2010), such as queries and tweets (Bergsma et al., 2012; Carter et al., 2012; Ceylan and Kim, 2009). Tagging individual words (without context) has been done using dictionaries, affix statistics and classifiers using character n-grams (Hammarstr¨om, 2007; Gottron and Lipka, 2010). Although Yamaguchi and Tanaka-Ishii (2012) segmented text by language, their data was artificially created by randomly sampling and concatenating text segments (40-160 characters) from monolingual texts. Therefore, the language switches do not reflect realistic switches as they occur in natural texts. Most related to ours is the work by King and Abney (2013) who labeled languages of words in multilingual web pages, but evaluated the task only using word level accuracy. 857 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862, Seattle, Washington, USA, 18-21 October 2013. c�2013 Associat</context>
<context position="6829" citStr="Yamaguchi and Tanaka-Ishii, 2012" startWordPosition="1048" endWordPosition="1051">such as hahaha, ooooh and lol recognized using regular expressions. Posts for which all tokens are ignored, are not included in the corpus. Statistics The dataset was randomly divided into a training, development and test set. The statistics are listed in Table 1. The statistics show that Dutch is the majority language, although the difference between Turkish and Dutch is not large. We also find that the documents (i.e. posts) are short, with on average 18 tokens per document. The data represents realistic texts found in online multilingual communication. Compared to previously used datasets (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), the data is noisier and the documents are much shorter. #NL tokens #TR tokens #Posts/(BL%) Train 14900 (54%) 12737 (46%) 1603 (15%) Dev 8590 (51%) 8140 (49%) 728 (19%) Test 5895 (53%) 5293 (47%) 735 (17%) Table 1: Number of tokens and posts for Dutch (NL) and Turkish (TR), including % of bilingual (BL) posts 2Based on online name lists and Wikipedia pages 858 3 Experimental Setup 3.1 Training Corpora We used the following corpora to extract dictionaries and language models. • GenCor: Turkish web pages (Sak et al., 2008). • NLCOW2012: Dutch web pages (Sch¨afer and Bildh</context>
</contexts>
<marker>Yamaguchi, Tanaka-Ishii, 2012</marker>
<rawString>H. Yamaguchi and K. Tanaka-Ishii. 2012. Text segmentation by language using minimum description length. In Proceedings of ACL 2012.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002914">
<title confidence="0.994265">
The Answer is at your Fingertips: Improving Passage Retrieval for Web
Question Answering with Search Behavior Data
</title>
<author confidence="0.97201">
Mikhail Ageev* Dmitry Lagun Eugene Agichtein
</author>
<affiliation confidence="0.982567">
Moscow State University Emory University Emory University
</affiliation>
<email confidence="0.995597">
mageev@yandex.ru dlagun@emory.edu eugene@mathcs.emory.edu
</email>
<sectionHeader confidence="0.997335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99926075">
Passage retrieval is a crucial first step of au-
tomatic Question Answering (QA). While ex-
isting passage retrieval algorithms are effec-
tive at selecting document passages most sim-
ilar to the question, or those that contain
the expected answer types, they do not take
into account which parts of the document the
searchers actually found useful. We propose,
to the best of our knowledge, the first success-
ful attempt to incorporate searcher examina-
tion data into passage retrieval for question an-
swering. Specifically, we exploit detailed ex-
amination data, such as mouse cursor move-
ments and scrolling, to infer the parts of the
document the searcher found interesting, and
then incorporate this signal into passage re-
trieval for QA. Our extensive experiments and
analysis demonstrate that our method signif-
icantly improves passage retrieval, compared
to using textual features alone. As an addi-
tional contribution, we make available to the
research community the code and the search
behavior data used in this study, with the hope
of encouraging further research in this area.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993222490909091">
Automated Question Answering (QA), is an attrac-
tive variation of search where the QA system auto-
matically returns an answer to a user’s question, in-
stead of a list of document results. Passage retrieval
is a first critical step of QA system, where candi-
date passages are identified and scored as likely to
contain an answer. While significant progress has
been made recently on incorporating syntactic and
semantic analysis for improving the QA system per-
formance, this analysis is typically applied only on
the (limited) set of candidate passages retrieved. The
main reason is that it is generally not practical to
perform deep analysis on all documents in a large
collection, and not yet feasible for the Web at large.
*Work done at Emory University.
In the web search setting, automated question
answering presents additional challenges and op-
portunities. On the downside, the questions and
queries from real users are often not grammatical
or well-formed, differing from the questions used
in the traditional TREC Question Answering evalua-
tions (Kelly and Lin, 2007; Sun et al., 2005). On the
upside, by interacting with a search engine, the mil-
lions of searchers implicitly provide additional clues
about usefulness of documents, result ranking, and
other aspects of the search process. In this paper,
we explore making use of the search behavior data
to improve passage retrieval for automated Question
Answering on the web.
Our basic observation is that when a user is at-
tempting to answer a question, he or she will more
carefully examine the parts of the document that
contain an answer. This observation is intuitive,
and is strongly supported by numerous eye track-
ing studies (e.g., Buscher et al. (2008) and Buscher
et al. (2009a)). Based on this, we hypothesize that
the passages containing the answers can be automat-
ically identified from the naturalistic searcher behav-
ior, and this prediction can be subsequently used to
improve passage ranking. To the best of our knowl-
edge, our work is the first to successfully incorporate
searcher examination into passage ranking for Ques-
tion Answering.
Our approach is primarily aimed at recurring (re-
peated) questions, which comprise a large fraction
of the search volume (while the exact statistics vary,
over 50% of search queries are submitted by multi-
ple users). For such questions, a system would track
the clicked result URLs, as well as the user interac-
tions on the landing pages. Then the system would
use this information to present the improved results
to new users who ask the same (or similar) ques-
tion. Intuitively, our method uses the same general
idea of result click data mining, used by the major
search engines to improve result ranking, but takes
</bodyText>
<page confidence="0.951886">
1011
</page>
<note confidence="0.7310585">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1011–1021,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999037833333333">
it a step further to exploit user interactions on the ac-
tual landing pages. A key point to emphasize is that
our approach exploits the natural browsing behavior
of the users, not requiring any additional effort from
the searchers.
Specifically, our contributions include:
</bodyText>
<listItem confidence="0.892169272727273">
• A novel approach to passage retrieval for ques-
tion answering, that naturally integrates textual
and behavioral evidence.
• A robust infrastructure for connecting fine-
grained searcher behavior to precise page con-
tents.
• Thorough experiments over hundreds of search
sessions and thousands of page views, demon-
strating significant improvements to passage
retrieval by harnessing the user’s page exami-
nation data.
</listItem>
<bodyText confidence="0.999296">
Next we describe related work, to place our contri-
bution in context.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999929">
Our work brings together two areas of research: pas-
sage retrieval for question answering, and mining
searcher behavior data.
Passage retrieval has long been recognized as
the first crucial step of automatic question answer-
ing. In some cases, passage retrieval can even serve
as the final product of a Question Answering sys-
tem (Clarke et al., 2000). As another example, re-
dundancy in the retrieved passages has been used by
the AskMSR system (Brill et al., 2002) to select an-
swers. Tellex et al. (2003) report a thorough com-
parison of passage retrieval methods for QA, up to
2003. Additional improvements have been achieved
by using deeper analysis of the text. For exam-
ple, Cui et al. (2005) exploited dependency relations
between the question terms, Aktolga et al. (2011)
incorporated syntactic structure and answer typing,
while Harabagiu et al. (2005) used semantic analy-
sis at all stages of the question answering process. In
this paper, we pursue a complementary direction, by
exploiting searcher examination behavior, with the
assumption that human searchers can easily zoom in
on relevant passages as part of normal searching.
It has been previously recognized that searcher in-
teractions could be valuable for question answering,
and a task on Complex Interactive QA has been ran
as part of TREC 2007 (Kelly and Lin, 2007). Our
work goes much further by considering not only ex-
plicit interactions, but also the searcher examination
behavior (i.e., detailed information on which text
passages were examined) – which, as we show, pro-
vides additional valuable information for passage re-
trieval. Furthermore, it has been recognized that the
questions used in traditional TREC QA evaluation
may not be reflective of the “real” questions, posed
by users (Bernardi and Kirschner, 2010). Our paper
uses a subset of the real questions posted by users on
Community Question Answering (CQA) sites, and
searches and interactions from real users – which
makes our task unique and more challenging than
the previous settings.
In particular, our work builds on the rich his-
tory of using eye tracking technology to identify
areas of interest and attention, and to study read-
ing behavior. In the context of web search docu-
ment examination, Buscher et al. (2008) extracted
sub-documents by tracking eye movements as im-
plicit feedback and expanded search queries to im-
prove the search result ranking. Buscher et al. also
studied the prediction of salient Web page regions
using eye-tracking (Buscher et al., 2009a). This
work, and others, have shown that user attention can
help identify regions of documents of particular rel-
evance or usefulness for the query. While eye track-
ing equipment limits the applicability of these find-
ings to lab studies, these studies served as inspira-
tion to our work to detect the inferred areas of in-
terest. Specifically, we use mouse cursor tracking
as a natural proxy for user’s attention, to replace the
requirement for eye tracking equipment. As origi-
nally reported by Rodden et al. (2008), the authors
discovered the coordination between a user’s eye
movements and mouse movements when scanning
a web search results page. This work was further
extended by Huang et al. (2012) to predict the gaze
position from mouse cursor movement, with mean
error of about 150 px. In summary, there is mount-
ing evidence that the user’s attention in web search
can be approximated using mouse cursor, scrolling,
and other interaction data. In particular, Hijikata
(2004) proposed a method to extract text passages
of Web pages based on the user’s mouse activity and
found that extracted passages based on mouse ac-
</bodyText>
<page confidence="0.993825">
1012
</page>
<bodyText confidence="0.999932866666667">
tivity such as text tracing, link pointing, link clicking
and text selection enable more accurate extraction of
key words of interest than using the whole text of the
page. Recently, White and Buscher (2012) proposed
a method that uses text selections as implicit feed-
back for document ranking. Most closely related to
this work is a contemporaneous effort on improv-
ing web search result summaries, or snippets, by
exploiting searcher behavior on the examined doc-
uments, described by Ageev et al. (2013). How-
ever, to the best of our knowledge, there has been
no prior work on modeling searcher interaction on
result documents to improve Question Answering
performance, and in particular the passage retrieval
step.
</bodyText>
<sectionHeader confidence="0.866359" genericHeader="method">
3 Problem Statement and Approach
</sectionHeader>
<bodyText confidence="0.9997652">
This section first states the problem we are address-
ing more precisely. Then, we describe the key parts
of our approach (Section 3.2), and the required in-
frastructure we had to develop to accomplish the re-
quired data collection (Section 3.4).
</bodyText>
<subsectionHeader confidence="0.999642">
3.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999985470588235">
Our goal is to incorporate the searcher behavior (in
particular, page examination) into passage retrieval.
That is, by analyzing the searcher behavior data, we
aim to identify the parts of the page that contain rel-
evant passages for answering a question. Specifi-
cally, given a question, a set of queries generated
by searchers attempting to answer this question, and
a set of documents retrieved by a search engine for
each of the queries, our goal to retrieve a set of pas-
sages that contain correct answers for the question.
That is, our goal is to identify, from searcher be-
havior, the passages in the documents most likely to
contain correct answers to a question, which could
then be incorporated into a fully automated question
answering system, or returned to the user directly,
for example, by incorporating these passages into
the result abstracts or “snippets”.
</bodyText>
<subsectionHeader confidence="0.998546">
3.2 Approach
</subsectionHeader>
<bodyText confidence="0.999872489795918">
Our approach accomplishes the goal above by in-
corporating both textual and behavioral evidence.
Specifically, we combine together traditional text-
based passage retrieval features, and the inferred
user interest in specific parts of a document based
on searcher behavior.
First, a passage score is obtained from the QA-
SYS system (Ng and Kan, 2010), resulting in a
strong text-only baseline that generates candidate
passages. Separately, examination behavior data is
collected over the landing pages, using our logging
infrastructure described in the next section. Then, a
behavior model is trained to identify the passages
of interest to the user, based on user examination
data (Section 4.2). Finally, the behavior-based pre-
diction of interest in each candidate passage is com-
bined with the original (text-based) passage score,
in order to generate the final behavior-biased pas-
sage ranking (Section 4.3). Note that by decoupling
the behavior modeling from the candidate genera-
tion method, our approach can be used with any
other passage retrieval approach that provides scores
for the candidate passages (that could be combined
with the behavior scores for the final ranking step).
While general and flexible, our approach makes
two key assumptions, resulting in potential limi-
tations. First, our approach is primarily targeted
(and evaluated for) informational questions – that
is, questions for which the user expects to find an
answer in the text of the page. For other question
classes (e.g., opinion), passage retrieval might have
to be optimized differently. We also assume that
the user interactions on landing pages can be col-
lected by a search engine or a third party. This is
not far-fetched: already, browser plug-ins and tool-
bars collect some form of user interactions on web
pages, major organizations can (and sometimes do)
use proxies, and common page widgets like banner
ads and visit counters commonly inject JavaScript to
monitor basic user interactions – and can be easily
extended to collect the examination data described
in this paper. The privacy and security of these meth-
ods are beyond the scope of this paper, we merely
point out that these behavior gathering tools, as-
sumed by our approach, already exist and are al-
ready widely deployed. The interested reader can
obtain an overview of the relevant privacy issues
and proposed solutions in references (Mayer and
Mitchell, 2012; Krishnamurthy and Wills, 2009).
</bodyText>
<subsectionHeader confidence="0.999592">
3.3 Acquiring Search Behavior Data
</subsectionHeader>
<bodyText confidence="0.99988375">
Our infrastructure for acquiring search behavior was
developed with two goals in mind: (1) to obtain be-
havior data similar to real-world search, with the
ability to track fine-grained search behavior such as
</bodyText>
<page confidence="0.949861">
1013
</page>
<bodyText confidence="0.999938842105263">
a mouse cursor movement (as there are no publicly
available data of this kind); (2) to create a controlled
and clean ground truth set, to train our system and
evaluate the effectiveness of our approach.
To collect sufficient amount of search behavior
data, we adapted for our task the publicly available
UFindIt architecture, described in reference Ageev
et al. (2011). The participants played several search
contests, or “games”, each consisting of 12 search
tasks (questions) to solve. The stated goal of the
game was to submit the highest possible number of
correct answers within the allotted time. After the
searcher decided that they found the answer, they
were instructed to type the answer together with the
supporting URL into the corresponding fields in the
game interface. Each search session (for one ques-
tion) was completed by either submitting an answer
or clicking the “skip question” button to pass to the
next question.
Participants were recruited through the Amazon
Mechanical Turk (MTurk) service. As a first step,
the workers had to solve a ReCaptcha puzzle to
verify that they are human and not an automated
“bot”. A browser verification check was performed
to confirm that the browser was compatible with our
JavaScript tracking code. During the data postpro-
cessing stage, we filtered out the users who did not
answer even the easy, trivial questions, as it indi-
cated either poor understanding of the game rules,
or an attempt to make a quick buck without effort.
In order to capture all of the participants’ search
actions, they were instructed to use only our search
interface (and not a separate browser window). The
search interface performed the web searches using
the public API of a popular web search engine, and
showed result pages to the users using the original
page design, layout and stylesheets, so the user’s
search experience is not affected.
</bodyText>
<subsectionHeader confidence="0.998423">
3.4 Page Examination Behavior Logging
</subsectionHeader>
<bodyText confidence="0.999989035714286">
A key part of our system is a mechanism for collect-
ing searcher interactions on web pages, and tying
them precisely to the page content at the word level.
As the HTML page passed through the proxy, a
JavaScript code is embedded to track the user’s inter-
actions, including mouse movements and scrolling,
as well as the properties of the visited page. The be-
havioral (interaction) events are logged by the search
interface proxy and written to the server log.
To connect the tracked mouse cursor positions
to exact text passages we employed the following
trick. After the HTML page is rendered in the
browser window, our JavaScript code modifies the
page DOM tree so that each word is wrapped by a
separate DOM Element. Then for each DOM El-
ement, the window coordinates of that element are
evaluated and saved in an Element’s attribute. The
processed HTML page is then saved to the server by
an asynchronous request. The saved coordinates are
updated if the page layout is changed due to resize
window event or AJAX action.
As a result of this instrumentation, for each page
visit we know the searcher’s intent (question), a
search engine query that the user issued, a URL
and HTML page, the bounding boxes of each word
in the HTML text, and all of the searcher actions,
e.g., mouse movement coordinates, mouse clicks,
and scrolling.
</bodyText>
<sectionHeader confidence="0.977453" genericHeader="method">
4 Behavior-Biased Passage Retrieval
</sectionHeader>
<bodyText confidence="0.999908">
We now present the details of our behavior-biased
passage retrieval algorithm (BePR). First, we de-
scribe the text-only retrieval system. Then, we in-
troduce our method for inferring the most interesting
or useful parts of the document from user behavior
(Section 4.2).
</bodyText>
<subsectionHeader confidence="0.996393">
4.1 Text-Based Passage Retrieval
</subsectionHeader>
<bodyText confidence="0.99993515">
We adopt an open-source question answering frame-
work QANUS (Ng and Kan, 2010) (version
v29Nov2012). The QANUS distribution contains
the fully functional factoid QA system QA-SYS that
we use as a baseline for our experiments. QA-
SYS implements many of the state-of-the-art ques-
tion answering techniques, and is similar to a top-
performing QA system from TREC (Sun et al.,
2005). The QA-SYS distribution is configured for
processing documents and questions in TREC QA
format, and we adopted QA-SYS for answer extrac-
tion from web documents. QA-SYS takes a set of
documents and a question as an input, and processes
the input in three stages: (1) information source
preparation, (2) question processing, and (3) answer
retrieval.
In the first stage, the downloaded HTML pages
are pre-processed with Natural Language Tool Kit
(NLTK, Bird (2006)). Extracted text is divided into
sentences using Punkt unsupervised sentence split-
</bodyText>
<page confidence="0.978051">
1014
</page>
<bodyText confidence="0.99997544">
ter (Kiss and Strunk, 2006). The QA-SYS performs
Part of Speech tagging using Stanford POS tagger
(Toutanova et al., 2003), and Named Entity Recog-
nition using Stanford NER (Finkel et al., 2005), and
then builds a Lucene index over the set of input
documents. In the second stage the QA-SYS per-
forms POS tagging, NE recognition, and question
type classification for an input question.
To answer a question, QA-SYS creates a query
from the question, performs the search over the in-
dexed text collection, and retrieves top 50 docu-
ments. Each document is split by sentences, and
for each sentence a QA-SYS Passage Retrieval Score
(Text5core) is computed as a linear combination
of term frequency score, proximity score, and term
coverage score. After that 40 passages with the high-
est Text5core are retrieved, for each passage QA-
SYS performs pattern based answer extraction based
on the identified expected answer type of the ques-
tion.
As the focus of this paper is to improve Passage
Retrieval performance, we use the Text5core sen-
tence ranking as a baseline, and improve on it by
adding the new search behavioral features indicating
the passage relevance, as described next.
</bodyText>
<subsectionHeader confidence="0.751061">
4.2 Inferring Relevant Passages from Search
Behavior
</subsectionHeader>
<bodyText confidence="0.999834428571429">
To rank passages by their “interestingness” – that is,
to identify the passages that have been carefully ex-
amined by the searcher, we use a learning-to-rank
approach, and apply regression algorithms to predict
the probability that a specific passage is interesting
for a user. A passage is labeled as “interesting”, if
the user submitted an answer in the current session,
and both the passage and the answer have at least
one common word, after stemming and stop-word
removal.
For each passage, a set of behavior features that
could represent passage interestingness is created.
To associate behavioral features with a given doc-
ument passage, we match the sequence of behav-
ior events and the set of bounding boxes for each
word and DOM Element of a page. For efficiency,
we build a spatial R-Tree index of these bounding
boxes, which allows us to quickly find the matching
DOM Elements for each event.
One key feature is the duration of the time in-
terval when a mouse cursor was hovering over the
</bodyText>
<table confidence="0.999367666666667">
Feature Description
MouseOverTime Time duration when the mouse
cursor was over the text passage
MouseNearTime Time duration when the mouse
cursor was close to the text
passage in the window
(x f 100px, y f 70px)
MouseOverEvents The number of mouse events
during MouseOverTime
MouseNearEvents The number of mouse events
during MouseNearTime
DispTime Time duration when the text
passage has been visible in
the browser window
(depends on scrollbar position)
DispMiddleTime Time duration when the text
passage was visible in the middle
part of the browser window
</table>
<tableCaption confidence="0.9998">
Table 1: Behavior features for text passages
</tableCaption>
<bodyText confidence="0.999935965517242">
specific text passage, or very close to the passage.
We also take a scrollbar and event count features
from papers (Buscher et al., 2009b), and (Guo and
Agichtein, 2012) to detect evidence of “reading” vs.
“skimming” behavior, and adopt those features to
represent the behavior near the specific location of
a page. The full set of our passage behavior features
are reported in Table 1.
To implement the passage ranker, we experi-
mented with a variety of learning-to-rank (LTR) al-
gorithms, and chose two implementations of Regres-
sion Trees, due to their strong performance for gen-
eral web search ranking tasks. The first algorithm
is Regression Tree (Friedman et al., 2001), and the
second is Gradient Boosting Regression Tree algo-
rithm (Friedman, 2001). They are named BePR-
BTree, and BePR-GBM respectively.
The dataset consists of a set of questions, with as-
sociated search behavior data collected from all the
users who tried to find an answer to this question,
the answers submitted by the users, and a set of val-
idated answers. These sets are divided into train-
ing, validation, and test, so that the training and val-
idation set URLs are disjoint, and the test set have
no intersection with training and validation set by
URLs, questions, and users. The training set is cre-
ated from only those page visits where the document
text has non-empty intersection with the user’s an-
swer, and the answer is correct. The trained regres-
</bodyText>
<page confidence="0.973486">
1015
</page>
<bodyText confidence="0.999988928571429">
sion algorithm is applied to all page visits in the test
set. When the trained model is applied at test time,
it has no information about the user’s intent, the cor-
rect answer, or the current query, but rather uses only
the behavioral features of the current page visit to
identify the “interesting” passages.
The predicted probability of passage interesting-
ness is averaged over all the users and page visits,
and the resulting passage interestingness is then used
as the B5core of the passage. Note that B5core
is defined for only visited pages; to incorporate the
overall clickthrough information (i.e., the fraction of
the time a page was visited, indicating relevance),
we introduce a generalized version, designated as
B5scoreAll, defined as: -y·CTR+(1−-y)·B5core,
where CTR is the clickthrough rate for the page,
defined as the fraction of time the result was clicked
for all searches. Intuitively, this version reduces the
weight of the behavior score for the pages with in-
sufficient behavior data by “backing off” to the doc-
ument clickthrough rate, according to the parame-
ter -y. For the cases where only the visited pages
are considered (ignoring the searches when the page
was not visited), -y is set to 0, reverting the score
to the original B5core definition. The resulting
behavior-based passage score is then used as the ag-
gregate value of searcher interest in the passage for
the combined passage retrieval step, described next.
</bodyText>
<subsectionHeader confidence="0.943305">
4.3 Combining Textual and Behavioral
Evidence
</subsectionHeader>
<bodyText confidence="0.999676">
The final step in our approach is to combine the
text-based score Text5core(f) for a sentence (Sec-
tion 4.1) with the interestingness score B5core(f)
(Section 4.2), inferred from the examination data. In
our current implementation we combine these scores
by linear combination:
</bodyText>
<equation confidence="0.996028">
F5core(f) =A · B5core(f)
+ (1 − A) · Text5core(f)
</equation>
<bodyText confidence="0.9993035">
Other more sophisticated ways to combine text
and behavior evidence are possible, such as jointly
learning over both text and behavior features. How-
ever, we chose to follow the simpler linear approach
for interpretability of the results (e.g., by varying the
A parameter).
</bodyText>
<sectionHeader confidence="0.916129" genericHeader="method">
5 Data Collection and Experimental Setup
</sectionHeader>
<bodyText confidence="0.999988">
This section presents the methodology used for se-
lecting the questions (Section 5.1), the correspond-
ing search behavior data (Section 5.2), and the ex-
perimental collections and metrics (Section 5.3).
</bodyText>
<subsectionHeader confidence="0.983205">
5.1 Questions
</subsectionHeader>
<bodyText confidence="0.999995333333333">
The search tasks were selected from community
question answering sites such as wiki.answers.com
and Yahoo! Answers by the researchers. The cri-
teria used were that the question should be clearly
stated, had a clear answer, and that finding this an-
swer was not a trivial task, that is, the answer was
not retrieved simply by submitting the question ver-
batim to Google, Bing, or Yahoo! Search engines.
Overall, 36 such questions were selected, posing (as
it turned out) greatly varying levels of difficulty for
participants. These questions were randomly split
into three game rounds of 12 questions each.
</bodyText>
<subsectionHeader confidence="0.999726">
5.2 Browsing Behavior Dataset
</subsectionHeader>
<bodyText confidence="0.999994107142857">
The search behavior data for each of the questions
above was acquired as described in Section 3.3. A
total of 270 participants finished the game. Af-
ter filtering out users who did not follow the game
rules, we have 3047 search sessions performed by
265 users. Our data for these users consists of 7800
queries, 3910 unique queries, 8574 SERP clicks on
1544 distinct URLs. For 5683 page visits (66%)
and 883 distinct URLs the on-page behavioral data
is collected. For the rest 34% of page visits the be-
havioral data were not collected due to conflicts be-
tween our JavaScript tracking code and other code
presented on the page. For each page view there
are about 400 atomic browsing events (mouse move-
ments, scrolling, key pressing) on average. All the
source and derived data are available at http://
ir.mathcs.emory.edu/intent.
The dataset is divided into training, validation,
and test set in the following way. The behavior
dataset for the first game is divided randomly into
equal-sized training and validation sets that are dis-
joint by URLs. The training set was used to train
the regression algorithm for predicting passage at-
tractiveness, and the validation set was used to ex-
plore the influence of behavior weight A on passage
retrieval performance, and to select the parameter A
for using on a test set. The validation set consists
of 254 different URLs spread over 11 questions, and
</bodyText>
<page confidence="0.987663">
1016
</page>
<bodyText confidence="0.9995915">
for each of them there is a collected browsing be-
havior.
The test set consists of 441 URLs spread over 24
questions, and the test set has no intersection with
training and validation set by URLs, questions, and
users.
</bodyText>
<subsectionHeader confidence="0.9977">
5.3 Candidate Document Selection Strategies
</subsectionHeader>
<bodyText confidence="0.999377833333333">
The first step for question answering is a selection
of a candidate document set. In our settings, we
may select a subset of web documents in a differ-
ent way. We explore passage retrieval effectiveness
using three different strategies of document set se-
lection.
</bodyText>
<listItem confidence="0.985679923076923">
• For each question select All documents that
are in top 10 documents returned by a search
engine for any query that was issued during
search for the specific question. For our dataset
this gives around 500 candidate documents per
question on average.
• For each question select only documents that
were Clicked by a user. This restricts a can-
didate document set to set of most promising
documents. For our dataset this gives around
25 candidate documents per question on aver-
age.
• For each pair of question and Relevant docu-
</listItem>
<bodyText confidence="0.854387842105263">
ment apply passage retrieval to the specific doc-
ument. In this experiment we label a document
“Relevant” if a correct answer was extracted
from it. In a real-world scenario, while doc-
ument relevance could be estimated by a va-
riety of click-based methods, we address the
challenge of how to actually extract the cor-
rect answer from the document, automatically,
with the help of the natural behavior data. We
perform this experiment to estimate the perfor-
mance of passage retrieval for the case when
relevant documents are known with high confi-
dence.
Evaluation Metrics: We evaluate passage retrieval
performance by standard Mean Reciprocal Rank
(MRR), and Mean Average Precision (MAP) met-
rics for top 20 retrieved sentences (Voorhees and
Tice, 1999). We also evaluate ROUGE-1 met-
ric (Lin, 2004) for the first retrieved passage.
</bodyText>
<figure confidence="0.9685405">
0 0.1 0.2 0.3 0.4 0.5
fragment score
</figure>
<figureCaption confidence="0.998084">
Figure 1: The actual passage interestingness, measured
by intersection with user’s answer, vs. the passage rele-
vance score BScore predicted from behavior data
</figureCaption>
<sectionHeader confidence="0.999887" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999977714285714">
We now present the empirical results. First, we re-
port the intermediate result of using behavior data to
infer the interesting (useful) passages in the docu-
ment. Then, we report the main results of the paper
where the quality of the generated snippets with and
without using behavior data is compared using hu-
man judgments.
</bodyText>
<subsectionHeader confidence="0.99992">
6.1 Prediction of Passage Interestingness
</subsectionHeader>
<bodyText confidence="0.999882333333333">
This experiment evaluates how well we can predict
interesting passages by observing a user’s on-page
behavior. We suppose that the passage is interesting
if it is related to the answer for the question. For
each visited page, we collect the user’s answer (if
submitted), and all correct answers from all users
who answered this question. Then, we compare
those answers to each text passage in the document
using ROUGE metrics (Lin, 2004).
Figure 1 shows the relationship between the in-
terestingness of a passage and behavior score. The
graph shows that when the score is high (&gt; 0.5),
then average intersection between the passage and
user’s answer is much higher than those when the
passage score is low. All ROUGE-N metrics sig-
nificantly grow when the behavior score grows, al-
though ROUGE-2 over all correct answers are al-
ways very small (it grows from 0.003 to 0.007).
ROUGE-1 is much greater than ROUGE-2 for high
scores, as the interesting passage might contain use-
ful information for the answer, but the user reformu-
lates the obtained information and submits reformu-
lated answer. The ROUGE-N metrics for a user’s
answer are much greater than those for all correct
</bodyText>
<figure confidence="0.995021076923077">
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
user&apos;s answer ROUGE-1
user&apos;s answer ROUGE-2
all correct answers ROUGE-1
all correct answers ROUGE-2
ROUGE
</figure>
<page confidence="0.97448">
1017
</page>
<table confidence="0.999876571428571">
Feature Feature Importance
DispMiddleTime 0.51
MouseOverTime 0.34
DispTime 0.12
MouseNearTime 0.02
MouseOverEvents 0.01
MouseNearEvents 0.01
</table>
<tableCaption confidence="0.994659">
Table 2: Feature importance for behavioral features, as
measured by Gini coefficient
</tableCaption>
<bodyText confidence="0.999788828571428">
answers, as other users might obtain valuable infor-
mation from other documents, and some questions
have distinct correct answers.
Behavior Feature Importance Analysis: To esti-
mate relative importance of behavior features we
evaluated the Gini importance index (Breiman,
1996) for each behavior feature from the Table 1.
The Table 2 shows that the most important features
are the time duration when the text passage was vis-
ible in the middle part of the scrolling window, and
the time duration when the mouse cursor was over
the text passage. The first feature has been shown
to be a good feature for re-ranking search results
in reference (Buscher et al., 2009b), and we have
shown that it is also useful for passage retrieval. The
MouseOverTime feature has been previously shown
to be correlated with examination time, measured
by eye-tracking experiments (Guo and Agichtein,
2010), and it helps us detect local behavior in the
neighborhood of a specific text passage.
Analysis of Searcher Attention: In order to better
understand what characteristics of the textual pas-
sages attract the searcher’s attention, we explored
21 linguistic features for each sentence. Our fea-
tures were designed to estimate text readability, and
the overlap of a passage with the query that was
used to find the document. We implemented the
readability features from (Kanungo and Orr, 2009),
and query matching features from (Metzler and Ka-
nungo, 2008). Table 3 reports the top 10 features
with the highest absolute value of the correlation co-
efficient with passage interestingness score BScore.
Interestingly, the most highly correlated features are
related to readability, while query matching features
are less important.
</bodyText>
<table confidence="0.787913428571429">
Feature description corr
Number of distinct words in the passage 0.31
Total number of words in the passage 0.28
Number of letter ([a-zA-z]) characters 0.27
Relative location of the passage in the document -0.25
Number of unique words in the passage -0.24
divided by total number of words
Number of punctuation characters -0.20
Number of words with first letter capitalized -0.17
Overlap of query terms expanded 0.15
with synonyms and the passage
Absolute count of query terms 0.15
matched in the passage
Average position of query term within the passage -0.14
</table>
<tableCaption confidence="0.9385325">
Table 3: Correlation of passage interestingness BScore
with linguistic properties of a sentence
</tableCaption>
<figureCaption confidence="0.963609666666667">
Figure 2: MRR for passage retrieval for varying behav-
ior weight A and interestingness prediction algorithms
BePR-DTree, and BePR-GBM
</figureCaption>
<subsectionHeader confidence="0.99872">
6.2 Passage Retrieval with Behavior Data
</subsectionHeader>
<bodyText confidence="0.999942533333333">
This section reports the main results of the paper.
First, we describe the parameter tuning, followed by
the main performance results.
Parameter Tuning: To tune the passage retrieval
performance, we use the validation set to find the
optimal value for A. Figure 2 reports the passage
retrieval MRR for varying A, for two learning al-
gorithms BePR-GBM and BePR-DTree. The figure
shows that both BePR-GBM and BePR-BTree im-
prove over the QA-SYS baseline. BePR-GBM algo-
rithm achieves the best performance with A = 0.8,
and also exhibits more robust behavior compared to
BePR-BTree, so we use BePR-GBM with A = 0.8
for the main experiments described next. Similarly,
using the training and validation sets, we optimized
</bodyText>
<page confidence="0.994329">
1018
</page>
<figureCaption confidence="0.996786">
Figure 3: Passage retrieval MRR (a), ROUGE1 (b), and MAP (c) for the BePR and QA-SYS systems, on the test set.
</figureCaption>
<figure confidence="0.998517416666667">
0.7
0.6
0.7
0.6
0.7
MRR@20
0.6
0.5
0.4
0.3
0.2
0.1
0
QA-SYS
BePR
All Clicked Relevant
QA-SYS
ROUGE1@1
0.5
0.4
0.3
0.2
0.1
0
BePR
All Clicked Relevant
QA-SYS
MAP
0.5
0.4
0.3
0.2
0.1
0
BePR
All Clicked Relevant
</figure>
<bodyText confidence="0.998392097560976">
the value of the clickthrough rate weight y = 0.05
(used for the BScoreAll score) for the All document
set only (as for the Clicked and Relevant document
sets, y is always set to 0 by construction).
Main retrieval results: We now compare the base-
line algorithm for passage retrieval implemented in
QA-SYS system and described in section 4.1 with
the BePR algorithm (section 4.2-4.3) that combines
the textual passage score and the behavior score us-
ing the A parameter for the relative weight of the
behavior evidence.
Figure 3 reports the main results of the paper,
namely the MRR, ROUGE-1@1 and MAP pas-
sage retrieval metrics for the baseline QA-SYS al-
gorithm, and BePR-GBM, on the test set. As the
figure shows, BePR achieves higher performance
on all metrics, and for all document sets. The im-
provements are statistically significant (p &lt; 0.01)
for experiments with Clicked and Relevant docu-
ment sets. Not surprisingly, the improvements are
smallest when All documents are considered, as un-
clicked documents do not provide any associated be-
havior data. As the results show, our simple back-off
strategy (using the document clickthrough rate with
they parameter) is moderately successful, but could
be further refined in the future.
Finally, we illustrate how behavior features affect
passage ranking. Let’s consider a question “How
many Swedes speak English as a percentage?”. The
perfect relevant page for this question is a Wikipedia
page “Languages of Sweden”. A sentence ”Main
foreign language(s): English 89%, German 30%,
French 11%.” contains an answer to the question, but
it has only a small intersection with question terms,
and QA-SYS ranks this question in the 13th place.
Other sentences that contain a country name, a num-
ber, or have more terms that match the question are
ranked higher. In contrast, as searchers examined
this sentence carefully to find the answer, BePR is
able to promote this sentence to the second place in
the ranking.
</bodyText>
<sectionHeader confidence="0.997648" genericHeader="evaluation">
7 Resources and Data
</sectionHeader>
<bodyText confidence="0.9999458">
All the code and the collected data used in this
research are available at http://ir.mathcs.
emory.edu/intent/. The dataset contains the
set of questions used for the experiments, and user’s
behavior: queries submitted by users to search
engine, result pages, visited URLs, downloaded
landing pages, on-page browsing behavior (mouse
movements, scrollbar events, resize actions, clicks).
By sharing our code and data, we hope to encourage
further research in this area.
</bodyText>
<sectionHeader confidence="0.997935" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999951888888889">
We presented the first successful approach to incor-
porating naturalistic searcher behavior data into pas-
sage retrieval for question answering. Specifically,
we developed a robust method to infer searcher in-
terest in specific parts of the document, which could
then be combined with more traditional textual fea-
tures used for passage retrieval. Our results show
significant improvements over a strong baseline, de-
rived from a competitive Question Answering sys-
tem.
To implement the proposed method in a real-
world search engine for Web QA, the proposed in-
frastructure and/or the released data could be used
as a training set for the algorithm that predicts frag-
ment interestingness from user behavior. Such a
system would need to track document examination
data. This can already be done by incorporating our
released tracking code or a similar method into a
</bodyText>
<page confidence="0.990501">
1019
</page>
<bodyText confidence="0.999822466666667">
browser toolbar, banner ad system, visit counters or
other JavaScript widgets that already track user vis-
its. While we acknowledge user privacy as an im-
portant concern, it is beyond the scope of this work.
In the future, we plan to extend this work to more
precisely pinpoint the answer location on a page,
and consequently incorporate searcher behavior into
subsequent answer extraction and ranking stages of
question answering. We also plan to further investi-
gate the examination data to better understand how
searchers find correct (and incorrect) answers using
both general web search engines and QA systems –
in order to inform and further improve query sug-
gestion, result snippet generation, and result ranking
algorithms.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9976674">
This work was supported by the National Science
Foundation grant IIS-1018321, the DARPA grant
D11AP00269, the Yahoo! Faculty Research En-
gagement Program, and by the Russian Foundation
for Basic Research Grant 12-07-31225.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997038771084337">
Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene
Agichtein. 2011. Find it if you can: a game for mod-
eling different types of web search success using inter-
action data. In Proceedings of the 34th international
ACM SIGIR conference on Research and development
in Information Retrieval, SIGIR ’11, pages 345–354,
New York, NY, USA. ACM.
Mikhail Ageev, Dmitry Lagun, and Eugene Agichtein.
2013. Improving search result summaries by using
searcher behavior data. In Proceedings of the 36th in-
ternational ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’13.
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In Proceedings
of the 33rd European conference on Advances in in-
formation retrieval, ECIR’11, pages 617–628, Berlin,
Heidelberg. Springer-Verlag.
Raffaella Bernardi and Manuel Kirschner. 2010. From
artificial questions to real user interaction logs: Real
challenges for interactive question answering systems.
In Proceedings of Workshop on Web Logs and Ques-
tion Answering, pages 8–15.
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, COLING-ACL ’06, pages 69–72,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Leo Breiman. 1996. Bagging predictors. Mach. Learn.,
24(2):123–140.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the askmsr question-answering system. In
Proc. of ACL, EMNLP ’02, pages 257–264, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Georg Buscher, Andreas Dengel, and Ludger van Elst.
2008. Query expansion using gaze-based feedback on
the subdocument level. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’08, pages 387–394, New York, NY, USA. ACM.
Georg Buscher, Edward Cutrell, and Meredith Ringel
Morris. 2009a. What do you see when you’re surf-
ing?: using eye tracking to predict salient regions of
web pages. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ’09,
pages 21–30. ACM.
Georg Buscher, Ludger van Elst, and Andreas Dengel.
2009b. Segment-level display time as implicit feed-
back: a comparison to eye tracking. In Proceedings of
the 32nd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’09, pages 67–74, New York, NY, USA. ACM.
Charles Clarke, Gordon Cormack, Derek Kisman, and
Thomas Lynam. 2000. Question answering by pas-
sage selection (multitext experiments for trec-9). In
Proceedings of the Ninth Text REtrieval Conference
(TREC-9).
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ’05, pages 400–407, New York, NY,
USA. ACM.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ’05,
pages 363–370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
2001. The elements of statistical learning, volume 1.
Springer Series in Statistics.
Jerome H. Friedman. 2001. Greedy function approxi-
mation: A gradient boosting machine. The Annals of
Statistics, 29(5):pp. 1189–1232.
Qi Guo and Eugene Agichtein. 2010. Towards predicting
web searcher gaze position from mouse movements.
In CHI ’10 Extended Abstracts on Human Factors in
Computing Systems, CHI EA ’10, pages 3601–3606,
New York, NY, USA. ACM.
</reference>
<page confidence="0.76245">
1020
</page>
<reference confidence="0.981213891566265">
Qi Guo and Eugene Agichtein. 2012. Beyond dwell
time: estimating document relevance from cursor
movements and other post-click searcher behavior. In
Proceedings of the 21st international conference on
World Wide Web, WWW ’12, pages 569–578, New
York, NY, USA. ACM.
Sanda Harabagiu, Dan Moldovan, Christine Clark,
Mitchell Bowden, Andrew Hickl, and Patrick Wang.
2005. Employing two question answering systems in
trec-2005. In Proceedings of the fourteenth text re-
trieval conference.
Yoshinori Hijikata. 2004. Implicit user profiling for on
demand relevance feedback. In Proceedings of the 9th
international conference on Intelligent user interfaces,
IUI ’04, pages 198–205, New York, NY, USA. ACM.
Jeff Huang, Ryen White, and Georg Buscher. 2012. User
see, user point: gaze and cursor alignment in web
search. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’12, pages
1341–1350, New York, NY, USA. ACM.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceedings
of the Second ACM International Conference on Web
Search and Data Mining, WSDM ’09, pages 202–211,
New York, NY, USA. ACM.
Diane Kelly and Jimmy Lin. 2007. Overview of the trec
2006 ciqa task. In ACM SIGIR Forum, volume 41,
pages 107–116. ACM.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multilin-
gual sentence boundary detection. Comput. Linguist.,
32(4):485–525, December.
Balachander Krishnamurthy and Craig Wills. 2009. Pri-
vacy diffusion on the web: a longitudinal perspective.
In Proceedings of the 18th international conference
on World wide web, WWW ’09, pages 541–550, New
York, NY, USA. ACM.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Jonathan R. Mayer and John C. Mitchell. 2012. Third-
party web tracking: Policy and technology. In Pro-
ceedings of the 2012 IEEE Symposium on Security
and Privacy, SP ’12, pages 413–427, Washington, DC,
USA. IEEE Computer Society.
D. Metzler and T. Kanungo. 2008. Machine learned sen-
tence selection strategies for query-biased summariza-
tion. In SIGIR Learning to Rank Workshop.
Jun-Ping Ng and Min-Yen Kan. 2010. Qanus:
An open-source question-answering platform
http://www.comp.nus.edu.sg/ junping/docs/qanus.pdf.
Kerry Rodden, Xin Fu, Anne Aula, and Ian Spiro. 2008.
Eye-mouse coordination patterns on web search re-
sults pages. In CHI ’08 Extended Abstracts on Human
Factors in Computing Systems, CHI EA ’08, pages
2997–3002, New York, NY, USA. ACM.
Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng
Chua, and Min-Yen Kan. 2005. Using syntactic and
semantic relation analysis in question answering. In
TREC.
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval, SIGIR ’03, pages 41–
47, New York, NY, USA. ACM.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ellen Voorhees and Dawn M Tice. 1999. The trec-8
question answering track evaluation. In Proceedings
of The Eighth Text REtrieval Conference (TREC-8),
http://trec. nist. gov/pubs/trec8/t8 proceedings. html.
Ryen W. White and Georg Buscher. 2012. Text se-
lections as implicit relevance feedback. In Proceed-
ings of the 35th international ACM SIGIR conference
on Research and development in information retrieval,
pages 1151–1152, New York, NY, USA. ACM.
</reference>
<page confidence="0.993488">
1021
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827917">
<title confidence="0.9700295">The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data</title>
<author confidence="0.866142">Lagun Eugene Agichtein</author>
<affiliation confidence="0.993991">Moscow State University Emory University Emory University</affiliation>
<email confidence="0.996426">mageev@yandex.rudlagun@emory.edueugene@mathcs.emory.edu</email>
<abstract confidence="0.99910008">Passage retrieval is a crucial first step of automatic Question Answering (QA). While existing passage retrieval algorithms are effective at selecting document passages most similar to the question, or those that contain the expected answer types, they do not take into account which parts of the document the searchers actually found useful. We propose, to the best of our knowledge, the first successful attempt to incorporate searcher examination data into passage retrieval for question answering. Specifically, we exploit detailed examination data, such as mouse cursor movements and scrolling, to infer the parts of the document the searcher found interesting, and then incorporate this signal into passage retrieval for QA. Our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval, compared to using textual features alone. As an additional contribution, we make available to the research community the code and the search behavior data used in this study, with the hope of encouraging further research in this area.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Ageev</author>
<author>Qi Guo</author>
<author>Dmitry Lagun</author>
<author>Eugene Agichtein</author>
</authors>
<title>Find it if you can: a game for modeling different types of web search success using interaction data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR ’11,</booktitle>
<pages>345--354</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13650" citStr="Ageev et al. (2011)" startWordPosition="2181" endWordPosition="2184">). 3.3 Acquiring Search Behavior Data Our infrastructure for acquiring search behavior was developed with two goals in mind: (1) to obtain behavior data similar to real-world search, with the ability to track fine-grained search behavior such as 1013 a mouse cursor movement (as there are no publicly available data of this kind); (2) to create a controlled and clean ground truth set, to train our system and evaluate the effectiveness of our approach. To collect sufficient amount of search behavior data, we adapted for our task the publicly available UFindIt architecture, described in reference Ageev et al. (2011). The participants played several search contests, or “games”, each consisting of 12 search tasks (questions) to solve. The stated goal of the game was to submit the highest possible number of correct answers within the allotted time. After the searcher decided that they found the answer, they were instructed to type the answer together with the supporting URL into the corresponding fields in the game interface. Each search session (for one question) was completed by either submitting an answer or clicking the “skip question” button to pass to the next question. Participants were recruited thr</context>
</contexts>
<marker>Ageev, Guo, Lagun, Agichtein, 2011</marker>
<rawString>Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene Agichtein. 2011. Find it if you can: a game for modeling different types of web search success using interaction data. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR ’11, pages 345–354, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Ageev</author>
<author>Dmitry Lagun</author>
<author>Eugene Agichtein</author>
</authors>
<title>Improving search result summaries by using searcher behavior data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’13.</booktitle>
<contexts>
<context position="9202" citStr="Ageev et al. (2013)" startWordPosition="1470" endWordPosition="1473">xt passages of Web pages based on the user’s mouse activity and found that extracted passages based on mouse ac1012 tivity such as text tracing, link pointing, link clicking and text selection enable more accurate extraction of key words of interest than using the whole text of the page. Recently, White and Buscher (2012) proposed a method that uses text selections as implicit feedback for document ranking. Most closely related to this work is a contemporaneous effort on improving web search result summaries, or snippets, by exploiting searcher behavior on the examined documents, described by Ageev et al. (2013). However, to the best of our knowledge, there has been no prior work on modeling searcher interaction on result documents to improve Question Answering performance, and in particular the passage retrieval step. 3 Problem Statement and Approach This section first states the problem we are addressing more precisely. Then, we describe the key parts of our approach (Section 3.2), and the required infrastructure we had to develop to accomplish the required data collection (Section 3.4). 3.1 Problem Statement Our goal is to incorporate the searcher behavior (in particular, page examination) into pa</context>
</contexts>
<marker>Ageev, Lagun, Agichtein, 2013</marker>
<rawString>Mikhail Ageev, Dmitry Lagun, and Eugene Agichtein. 2013. Improving search result summaries by using searcher behavior data. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Aktolga</author>
<author>James Allan</author>
<author>David A Smith</author>
</authors>
<title>Passage reranking for question answering using syntactic structures and answer types.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR’11,</booktitle>
<pages>617--628</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5844" citStr="Aktolga et al. (2011)" startWordPosition="925" endWordPosition="928">been recognized as the first crucial step of automatic question answering. In some cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human searchers can easily zoom in on relevant passages as part of normal searching. It has been previously recognized that searcher interactions could be valuable for question answering, and a task on Complex Interactive QA has been ran as part of TREC 2007 (Kelly and Lin, 2007). Our work goes much further by considering</context>
</contexts>
<marker>Aktolga, Allan, Smith, 2011</marker>
<rawString>Elif Aktolga, James Allan, and David A. Smith. 2011. Passage reranking for question answering using syntactic structures and answer types. In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR’11, pages 617–628, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raffaella Bernardi</author>
<author>Manuel Kirschner</author>
</authors>
<title>From artificial questions to real user interaction logs: Real challenges for interactive question answering systems.</title>
<date>2010</date>
<booktitle>In Proceedings of Workshop on Web Logs and Question Answering,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="6857" citStr="Bernardi and Kirschner, 2010" startWordPosition="1083" endWordPosition="1086"> recognized that searcher interactions could be valuable for question answering, and a task on Complex Interactive QA has been ran as part of TREC 2007 (Kelly and Lin, 2007). Our work goes much further by considering not only explicit interactions, but also the searcher examination behavior (i.e., detailed information on which text passages were examined) – which, as we show, provides additional valuable information for passage retrieval. Furthermore, it has been recognized that the questions used in traditional TREC QA evaluation may not be reflective of the “real” questions, posed by users (Bernardi and Kirschner, 2010). Our paper uses a subset of the real questions posted by users on Community Question Answering (CQA) sites, and searches and interactions from real users – which makes our task unique and more challenging than the previous settings. In particular, our work builds on the rich history of using eye tracking technology to identify areas of interest and attention, and to study reading behavior. In the context of web search document examination, Buscher et al. (2008) extracted sub-documents by tracking eye movements as implicit feedback and expanded search queries to improve the search result ranki</context>
</contexts>
<marker>Bernardi, Kirschner, 2010</marker>
<rawString>Raffaella Bernardi and Manuel Kirschner. 2010. From artificial questions to real user interaction logs: Real challenges for interactive question answering systems. In Proceedings of Workshop on Web Logs and Question Answering, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06,</booktitle>
<pages>69--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17707" citStr="Bird (2006)" startWordPosition="2852" endWordPosition="2853">ASYS implements many of the state-of-the-art question answering techniques, and is similar to a topperforming QA system from TREC (Sun et al., 2005). The QA-SYS distribution is configured for processing documents and questions in TREC QA format, and we adopted QA-SYS for answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) question processing, and (3) answer retrieval. In the first stage, the downloaded HTML pages are pre-processed with Natural Language Tool Kit (NLTK, Bird (2006)). Extracted text is divided into sentences using Punkt unsupervised sentence split1014 ter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question. To answer a question, QA-SYS creates a query from the question, performs the search over the indexed text collection, and retrie</context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06, pages 69–72, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<location>Mach. Learn.,</location>
<contexts>
<context position="30699" citStr="Breiman, 1996" startWordPosition="4977" endWordPosition="4978">0 user&apos;s answer ROUGE-1 user&apos;s answer ROUGE-2 all correct answers ROUGE-1 all correct answers ROUGE-2 ROUGE 1017 Feature Feature Importance DispMiddleTime 0.51 MouseOverTime 0.34 DispTime 0.12 MouseNearTime 0.02 MouseOverEvents 0.01 MouseNearEvents 0.01 Table 2: Feature importance for behavioral features, as measured by Gini coefficient answers, as other users might obtain valuable information from other documents, and some questions have distinct correct answers. Behavior Feature Importance Analysis: To estimate relative importance of behavior features we evaluated the Gini importance index (Breiman, 1996) for each behavior feature from the Table 1. The Table 2 shows that the most important features are the time duration when the text passage was visible in the middle part of the scrolling window, and the time duration when the mouse cursor was over the text passage. The first feature has been shown to be a good feature for re-ranking search results in reference (Buscher et al., 2009b), and we have shown that it is also useful for passage retrieval. The MouseOverTime feature has been previously shown to be correlated with examination time, measured by eye-tracking experiments (Guo and Agichtein</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Mach. Learn., 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Susan Dumais</author>
<author>Michele Banko</author>
</authors>
<title>An analysis of the askmsr question-answering system.</title>
<date>2002</date>
<booktitle>In Proc. of ACL, EMNLP ’02,</booktitle>
<pages>257--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5533" citStr="Brill et al., 2002" startWordPosition="873" endWordPosition="876">nts to passage retrieval by harnessing the user’s page examination data. Next we describe related work, to place our contribution in context. 2 Related Work Our work brings together two areas of research: passage retrieval for question answering, and mining searcher behavior data. Passage retrieval has long been recognized as the first crucial step of automatic question answering. In some cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human search</context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Eric Brill, Susan Dumais, and Michele Banko. 2002. An analysis of the askmsr question-answering system. In Proc. of ACL, EMNLP ’02, pages 257–264, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg Buscher</author>
<author>Andreas Dengel</author>
<author>Ludger van Elst</author>
</authors>
<title>Query expansion using gaze-based feedback on the subdocument level.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08,</booktitle>
<pages>387--394</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Buscher, Dengel, van Elst, 2008</marker>
<rawString>Georg Buscher, Andreas Dengel, and Ludger van Elst. 2008. Query expansion using gaze-based feedback on the subdocument level. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages 387–394, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg Buscher</author>
<author>Edward Cutrell</author>
<author>Meredith Ringel Morris</author>
</authors>
<title>What do you see when you’re surfing?: using eye tracking to predict salient regions of web pages.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’09,</booktitle>
<pages>21--30</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3115" citStr="Buscher et al. (2009" startWordPosition="489" endWordPosition="492">with a search engine, the millions of searchers implicitly provide additional clues about usefulness of documents, result ranking, and other aspects of the search process. In this paper, we explore making use of the search behavior data to improve passage retrieval for automated Question Answering on the web. Our basic observation is that when a user is attempting to answer a question, he or she will more carefully examine the parts of the document that contain an answer. This observation is intuitive, and is strongly supported by numerous eye tracking studies (e.g., Buscher et al. (2008) and Buscher et al. (2009a)). Based on this, we hypothesize that the passages containing the answers can be automatically identified from the naturalistic searcher behavior, and this prediction can be subsequently used to improve passage ranking. To the best of our knowledge, our work is the first to successfully incorporate searcher examination into passage ranking for Question Answering. Our approach is primarily aimed at recurring (repeated) questions, which comprise a large fraction of the search volume (while the exact statistics vary, over 50% of search queries are submitted by multiple users). For such question</context>
<context position="7572" citStr="Buscher et al., 2009" startWordPosition="1201" endWordPosition="1204"> (CQA) sites, and searches and interactions from real users – which makes our task unique and more challenging than the previous settings. In particular, our work builds on the rich history of using eye tracking technology to identify areas of interest and attention, and to study reading behavior. In the context of web search document examination, Buscher et al. (2008) extracted sub-documents by tracking eye movements as implicit feedback and expanded search queries to improve the search result ranking. Buscher et al. also studied the prediction of salient Web page regions using eye-tracking (Buscher et al., 2009a). This work, and others, have shown that user attention can help identify regions of documents of particular relevance or usefulness for the query. While eye tracking equipment limits the applicability of these findings to lab studies, these studies served as inspiration to our work to detect the inferred areas of interest. Specifically, we use mouse cursor tracking as a natural proxy for user’s attention, to replace the requirement for eye tracking equipment. As originally reported by Rodden et al. (2008), the authors discovered the coordination between a user’s eye movements and mouse move</context>
<context position="20759" citStr="Buscher et al., 2009" startWordPosition="3351" endWordPosition="3354">on when the mouse cursor was close to the text passage in the window (x f 100px, y f 70px) MouseOverEvents The number of mouse events during MouseOverTime MouseNearEvents The number of mouse events during MouseNearTime DispTime Time duration when the text passage has been visible in the browser window (depends on scrollbar position) DispMiddleTime Time duration when the text passage was visible in the middle part of the browser window Table 1: Behavior features for text passages specific text passage, or very close to the passage. We also take a scrollbar and event count features from papers (Buscher et al., 2009b), and (Guo and Agichtein, 2012) to detect evidence of “reading” vs. “skimming” behavior, and adopt those features to represent the behavior near the specific location of a page. The full set of our passage behavior features are reported in Table 1. To implement the passage ranker, we experimented with a variety of learning-to-rank (LTR) algorithms, and chose two implementations of Regression Trees, due to their strong performance for general web search ranking tasks. The first algorithm is Regression Tree (Friedman et al., 2001), and the second is Gradient Boosting Regression Tree algorithm </context>
<context position="31084" citStr="Buscher et al., 2009" startWordPosition="5045" endWordPosition="5048">uable information from other documents, and some questions have distinct correct answers. Behavior Feature Importance Analysis: To estimate relative importance of behavior features we evaluated the Gini importance index (Breiman, 1996) for each behavior feature from the Table 1. The Table 2 shows that the most important features are the time duration when the text passage was visible in the middle part of the scrolling window, and the time duration when the mouse cursor was over the text passage. The first feature has been shown to be a good feature for re-ranking search results in reference (Buscher et al., 2009b), and we have shown that it is also useful for passage retrieval. The MouseOverTime feature has been previously shown to be correlated with examination time, measured by eye-tracking experiments (Guo and Agichtein, 2010), and it helps us detect local behavior in the neighborhood of a specific text passage. Analysis of Searcher Attention: In order to better understand what characteristics of the textual passages attract the searcher’s attention, we explored 21 linguistic features for each sentence. Our features were designed to estimate text readability, and the overlap of a passage with the </context>
</contexts>
<marker>Buscher, Cutrell, Morris, 2009</marker>
<rawString>Georg Buscher, Edward Cutrell, and Meredith Ringel Morris. 2009a. What do you see when you’re surfing?: using eye tracking to predict salient regions of web pages. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’09, pages 21–30. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg Buscher</author>
<author>Ludger van Elst</author>
<author>Andreas Dengel</author>
</authors>
<title>Segment-level display time as implicit feedback: a comparison to eye tracking.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09,</booktitle>
<pages>67--74</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Buscher, van Elst, Dengel, 2009</marker>
<rawString>Georg Buscher, Ludger van Elst, and Andreas Dengel. 2009b. Segment-level display time as implicit feedback: a comparison to eye tracking. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 67–74, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clarke</author>
<author>Gordon Cormack</author>
<author>Derek Kisman</author>
<author>Thomas Lynam</author>
</authors>
<title>Question answering by passage selection (multitext experiments for trec-9).</title>
<date>2000</date>
<booktitle>In Proceedings of the Ninth Text REtrieval Conference (TREC-9).</booktitle>
<contexts>
<context position="5419" citStr="Clarke et al., 2000" startWordPosition="853" endWordPosition="856">rough experiments over hundreds of search sessions and thousands of page views, demonstrating significant improvements to passage retrieval by harnessing the user’s page examination data. Next we describe related work, to place our contribution in context. 2 Related Work Our work brings together two areas of research: passage retrieval for question answering, and mining searcher behavior data. Passage retrieval has long been recognized as the first crucial step of automatic question answering. In some cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pu</context>
</contexts>
<marker>Clarke, Cormack, Kisman, Lynam, 2000</marker>
<rawString>Charles Clarke, Gordon Cormack, Derek Kisman, and Thomas Lynam. 2000. Question answering by passage selection (multitext experiments for trec-9). In Proceedings of the Ninth Text REtrieval Conference (TREC-9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>TatSeng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05,</booktitle>
<pages>400--407</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5763" citStr="Cui et al. (2005)" startWordPosition="914" endWordPosition="917">ion answering, and mining searcher behavior data. Passage retrieval has long been recognized as the first crucial step of automatic question answering. In some cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human searchers can easily zoom in on relevant passages as part of normal searching. It has been previously recognized that searcher interactions could be valuable for question answering, and a task on Complex Interactive QA has been ran as p</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and TatSeng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17988" citStr="Finkel et al., 2005" startWordPosition="2894" endWordPosition="2897">answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) question processing, and (3) answer retrieval. In the first stage, the downloaded HTML pages are pre-processed with Natural Language Tool Kit (NLTK, Bird (2006)). Extracted text is divided into sentences using Punkt unsupervised sentence split1014 ter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question. To answer a question, QA-SYS creates a query from the question, performs the search over the indexed text collection, and retrieves top 50 documents. Each document is split by sentences, and for each sentence a QA-SYS Passage Retrieval Score (Text5core) is computed as a linear combination of term frequency score, proximity score, and term coverage score. After that 40 passages with the highest Text5core ar</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
</authors>
<title>The elements of statistical learning,</title>
<date>2001</date>
<volume>1</volume>
<publisher>Springer</publisher>
<note>Series in Statistics.</note>
<contexts>
<context position="21295" citStr="Friedman et al., 2001" startWordPosition="3438" endWordPosition="3441">e. We also take a scrollbar and event count features from papers (Buscher et al., 2009b), and (Guo and Agichtein, 2012) to detect evidence of “reading” vs. “skimming” behavior, and adopt those features to represent the behavior near the specific location of a page. The full set of our passage behavior features are reported in Table 1. To implement the passage ranker, we experimented with a variety of learning-to-rank (LTR) algorithms, and chose two implementations of Regression Trees, due to their strong performance for general web search ranking tasks. The first algorithm is Regression Tree (Friedman et al., 2001), and the second is Gradient Boosting Regression Tree algorithm (Friedman, 2001). They are named BePRBTree, and BePR-GBM respectively. The dataset consists of a set of questions, with associated search behavior data collected from all the users who tried to find an answer to this question, the answers submitted by the users, and a set of validated answers. These sets are divided into training, validation, and test, so that the training and validation set URLs are disjoint, and the test set have no intersection with training and validation set by URLs, questions, and users. The training set is </context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2001</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2001. The elements of statistical learning, volume 1. Springer Series in Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Greedy function approximation: A gradient boosting machine.</title>
<date>2001</date>
<journal>The Annals of Statistics,</journal>
<volume>29</volume>
<issue>5</issue>
<pages>1189--1232</pages>
<contexts>
<context position="21375" citStr="Friedman, 2001" startWordPosition="3452" endWordPosition="3453">), and (Guo and Agichtein, 2012) to detect evidence of “reading” vs. “skimming” behavior, and adopt those features to represent the behavior near the specific location of a page. The full set of our passage behavior features are reported in Table 1. To implement the passage ranker, we experimented with a variety of learning-to-rank (LTR) algorithms, and chose two implementations of Regression Trees, due to their strong performance for general web search ranking tasks. The first algorithm is Regression Tree (Friedman et al., 2001), and the second is Gradient Boosting Regression Tree algorithm (Friedman, 2001). They are named BePRBTree, and BePR-GBM respectively. The dataset consists of a set of questions, with associated search behavior data collected from all the users who tried to find an answer to this question, the answers submitted by the users, and a set of validated answers. These sets are divided into training, validation, and test, so that the training and validation set URLs are disjoint, and the test set have no intersection with training and validation set by URLs, questions, and users. The training set is created from only those page visits where the document text has non-empty inters</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):pp. 1189–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Guo</author>
<author>Eugene Agichtein</author>
</authors>
<title>Towards predicting web searcher gaze position from mouse movements.</title>
<date>2010</date>
<booktitle>In CHI ’10 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’10,</booktitle>
<pages>3601--3606</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31306" citStr="Guo and Agichtein, 2010" startWordPosition="5078" endWordPosition="5081">x (Breiman, 1996) for each behavior feature from the Table 1. The Table 2 shows that the most important features are the time duration when the text passage was visible in the middle part of the scrolling window, and the time duration when the mouse cursor was over the text passage. The first feature has been shown to be a good feature for re-ranking search results in reference (Buscher et al., 2009b), and we have shown that it is also useful for passage retrieval. The MouseOverTime feature has been previously shown to be correlated with examination time, measured by eye-tracking experiments (Guo and Agichtein, 2010), and it helps us detect local behavior in the neighborhood of a specific text passage. Analysis of Searcher Attention: In order to better understand what characteristics of the textual passages attract the searcher’s attention, we explored 21 linguistic features for each sentence. Our features were designed to estimate text readability, and the overlap of a passage with the query that was used to find the document. We implemented the readability features from (Kanungo and Orr, 2009), and query matching features from (Metzler and Kanungo, 2008). Table 3 reports the top 10 features with the hig</context>
</contexts>
<marker>Guo, Agichtein, 2010</marker>
<rawString>Qi Guo and Eugene Agichtein. 2010. Towards predicting web searcher gaze position from mouse movements. In CHI ’10 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’10, pages 3601–3606, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Guo</author>
<author>Eugene Agichtein</author>
</authors>
<title>Beyond dwell time: estimating document relevance from cursor movements and other post-click searcher behavior.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web, WWW ’12,</booktitle>
<pages>569--578</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20792" citStr="Guo and Agichtein, 2012" startWordPosition="3356" endWordPosition="3359">close to the text passage in the window (x f 100px, y f 70px) MouseOverEvents The number of mouse events during MouseOverTime MouseNearEvents The number of mouse events during MouseNearTime DispTime Time duration when the text passage has been visible in the browser window (depends on scrollbar position) DispMiddleTime Time duration when the text passage was visible in the middle part of the browser window Table 1: Behavior features for text passages specific text passage, or very close to the passage. We also take a scrollbar and event count features from papers (Buscher et al., 2009b), and (Guo and Agichtein, 2012) to detect evidence of “reading” vs. “skimming” behavior, and adopt those features to represent the behavior near the specific location of a page. The full set of our passage behavior features are reported in Table 1. To implement the passage ranker, we experimented with a variety of learning-to-rank (LTR) algorithms, and chose two implementations of Regression Trees, due to their strong performance for general web search ranking tasks. The first algorithm is Regression Tree (Friedman et al., 2001), and the second is Gradient Boosting Regression Tree algorithm (Friedman, 2001). They are named </context>
</contexts>
<marker>Guo, Agichtein, 2012</marker>
<rawString>Qi Guo and Eugene Agichtein. 2012. Beyond dwell time: estimating document relevance from cursor movements and other post-click searcher behavior. In Proceedings of the 21st international conference on World Wide Web, WWW ’12, pages 569–578, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Mitchell Bowden</author>
<author>Andrew Hickl</author>
<author>Patrick Wang</author>
</authors>
<title>Employing two question answering systems in trec-2005.</title>
<date>2005</date>
<booktitle>In Proceedings of the fourteenth text retrieval conference.</booktitle>
<contexts>
<context position="5926" citStr="Harabagiu et al. (2005)" startWordPosition="936" endWordPosition="939">me cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human searchers can easily zoom in on relevant passages as part of normal searching. It has been previously recognized that searcher interactions could be valuable for question answering, and a task on Complex Interactive QA has been ran as part of TREC 2007 (Kelly and Lin, 2007). Our work goes much further by considering not only explicit interactions, but also the searcher examination behavior (i.e.,</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Christine Clark, Mitchell Bowden, Andrew Hickl, and Patrick Wang. 2005. Employing two question answering systems in trec-2005. In Proceedings of the fourteenth text retrieval conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshinori Hijikata</author>
</authors>
<title>Implicit user profiling for on demand relevance feedback.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th international conference on Intelligent user interfaces, IUI ’04,</booktitle>
<pages>198--205</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8551" citStr="Hijikata (2004)" startWordPosition="1364" endWordPosition="1365">acking as a natural proxy for user’s attention, to replace the requirement for eye tracking equipment. As originally reported by Rodden et al. (2008), the authors discovered the coordination between a user’s eye movements and mouse movements when scanning a web search results page. This work was further extended by Huang et al. (2012) to predict the gaze position from mouse cursor movement, with mean error of about 150 px. In summary, there is mounting evidence that the user’s attention in web search can be approximated using mouse cursor, scrolling, and other interaction data. In particular, Hijikata (2004) proposed a method to extract text passages of Web pages based on the user’s mouse activity and found that extracted passages based on mouse ac1012 tivity such as text tracing, link pointing, link clicking and text selection enable more accurate extraction of key words of interest than using the whole text of the page. Recently, White and Buscher (2012) proposed a method that uses text selections as implicit feedback for document ranking. Most closely related to this work is a contemporaneous effort on improving web search result summaries, or snippets, by exploiting searcher behavior on the e</context>
</contexts>
<marker>Hijikata, 2004</marker>
<rawString>Yoshinori Hijikata. 2004. Implicit user profiling for on demand relevance feedback. In Proceedings of the 9th international conference on Intelligent user interfaces, IUI ’04, pages 198–205, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Huang</author>
<author>Ryen White</author>
<author>Georg Buscher</author>
</authors>
<title>User see, user point: gaze and cursor alignment in web search.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12,</booktitle>
<pages>1341--1350</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8272" citStr="Huang et al. (2012)" startWordPosition="1317" endWordPosition="1320">ns of documents of particular relevance or usefulness for the query. While eye tracking equipment limits the applicability of these findings to lab studies, these studies served as inspiration to our work to detect the inferred areas of interest. Specifically, we use mouse cursor tracking as a natural proxy for user’s attention, to replace the requirement for eye tracking equipment. As originally reported by Rodden et al. (2008), the authors discovered the coordination between a user’s eye movements and mouse movements when scanning a web search results page. This work was further extended by Huang et al. (2012) to predict the gaze position from mouse cursor movement, with mean error of about 150 px. In summary, there is mounting evidence that the user’s attention in web search can be approximated using mouse cursor, scrolling, and other interaction data. In particular, Hijikata (2004) proposed a method to extract text passages of Web pages based on the user’s mouse activity and found that extracted passages based on mouse ac1012 tivity such as text tracing, link pointing, link clicking and text selection enable more accurate extraction of key words of interest than using the whole text of the page. </context>
</contexts>
<marker>Huang, White, Buscher, 2012</marker>
<rawString>Jeff Huang, Ryen White, and Georg Buscher. 2012. User see, user point: gaze and cursor alignment in web search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12, pages 1341–1350, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tapas Kanungo</author>
<author>David Orr</author>
</authors>
<title>Predicting the readability of short web summaries.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM ’09,</booktitle>
<pages>202--211</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31794" citStr="Kanungo and Orr, 2009" startWordPosition="5156" endWordPosition="5159">ature has been previously shown to be correlated with examination time, measured by eye-tracking experiments (Guo and Agichtein, 2010), and it helps us detect local behavior in the neighborhood of a specific text passage. Analysis of Searcher Attention: In order to better understand what characteristics of the textual passages attract the searcher’s attention, we explored 21 linguistic features for each sentence. Our features were designed to estimate text readability, and the overlap of a passage with the query that was used to find the document. We implemented the readability features from (Kanungo and Orr, 2009), and query matching features from (Metzler and Kanungo, 2008). Table 3 reports the top 10 features with the highest absolute value of the correlation coefficient with passage interestingness score BScore. Interestingly, the most highly correlated features are related to readability, while query matching features are less important. Feature description corr Number of distinct words in the passage 0.31 Total number of words in the passage 0.28 Number of letter ([a-zA-z]) characters 0.27 Relative location of the passage in the document -0.25 Number of unique words in the passage -0.24 divided by</context>
</contexts>
<marker>Kanungo, Orr, 2009</marker>
<rawString>Tapas Kanungo and David Orr. 2009. Predicting the readability of short web summaries. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM ’09, pages 202–211, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Kelly</author>
<author>Jimmy Lin</author>
</authors>
<title>Overview of the trec</title>
<date>2007</date>
<journal>In ACM SIGIR Forum,</journal>
<volume>41</volume>
<pages>107--116</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2444" citStr="Kelly and Lin, 2007" startWordPosition="376" endWordPosition="379">m performance, this analysis is typically applied only on the (limited) set of candidate passages retrieved. The main reason is that it is generally not practical to perform deep analysis on all documents in a large collection, and not yet feasible for the Web at large. *Work done at Emory University. In the web search setting, automated question answering presents additional challenges and opportunities. On the downside, the questions and queries from real users are often not grammatical or well-formed, differing from the questions used in the traditional TREC Question Answering evaluations (Kelly and Lin, 2007; Sun et al., 2005). On the upside, by interacting with a search engine, the millions of searchers implicitly provide additional clues about usefulness of documents, result ranking, and other aspects of the search process. In this paper, we explore making use of the search behavior data to improve passage retrieval for automated Question Answering on the web. Our basic observation is that when a user is attempting to answer a question, he or she will more carefully examine the parts of the document that contain an answer. This observation is intuitive, and is strongly supported by numerous eye</context>
<context position="6401" citStr="Kelly and Lin, 2007" startWordPosition="1013" endWordPosition="1016">ncy relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human searchers can easily zoom in on relevant passages as part of normal searching. It has been previously recognized that searcher interactions could be valuable for question answering, and a task on Complex Interactive QA has been ran as part of TREC 2007 (Kelly and Lin, 2007). Our work goes much further by considering not only explicit interactions, but also the searcher examination behavior (i.e., detailed information on which text passages were examined) – which, as we show, provides additional valuable information for passage retrieval. Furthermore, it has been recognized that the questions used in traditional TREC QA evaluation may not be reflective of the “real” questions, posed by users (Bernardi and Kirschner, 2010). Our paper uses a subset of the real questions posted by users on Community Question Answering (CQA) sites, and searches and interactions from </context>
</contexts>
<marker>Kelly, Lin, 2007</marker>
<rawString>Diane Kelly and Jimmy Lin. 2007. Overview of the trec 2006 ciqa task. In ACM SIGIR Forum, volume 41, pages 107–116. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>Jan Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Comput. Linguist.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="17822" citStr="Kiss and Strunk, 2006" startWordPosition="2867" endWordPosition="2870">rming QA system from TREC (Sun et al., 2005). The QA-SYS distribution is configured for processing documents and questions in TREC QA format, and we adopted QA-SYS for answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) question processing, and (3) answer retrieval. In the first stage, the downloaded HTML pages are pre-processed with Natural Language Tool Kit (NLTK, Bird (2006)). Extracted text is divided into sentences using Punkt unsupervised sentence split1014 ter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question. To answer a question, QA-SYS creates a query from the question, performs the search over the indexed text collection, and retrieves top 50 documents. Each document is split by sentences, and for each sentence a QA-SYS Passage Retrieval Score (</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Comput. Linguist., 32(4):485–525, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balachander Krishnamurthy</author>
<author>Craig Wills</author>
</authors>
<title>Privacy diffusion on the web: a longitudinal perspective.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web, WWW ’09,</booktitle>
<pages>541--550</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13032" citStr="Krishnamurthy and Wills, 2009" startWordPosition="2082" endWordPosition="2085">nizations can (and sometimes do) use proxies, and common page widgets like banner ads and visit counters commonly inject JavaScript to monitor basic user interactions – and can be easily extended to collect the examination data described in this paper. The privacy and security of these methods are beyond the scope of this paper, we merely point out that these behavior gathering tools, assumed by our approach, already exist and are already widely deployed. The interested reader can obtain an overview of the relevant privacy issues and proposed solutions in references (Mayer and Mitchell, 2012; Krishnamurthy and Wills, 2009). 3.3 Acquiring Search Behavior Data Our infrastructure for acquiring search behavior was developed with two goals in mind: (1) to obtain behavior data similar to real-world search, with the ability to track fine-grained search behavior such as 1013 a mouse cursor movement (as there are no publicly available data of this kind); (2) to create a controlled and clean ground truth set, to train our system and evaluate the effectiveness of our approach. To collect sufficient amount of search behavior data, we adapted for our task the publicly available UFindIt architecture, described in reference A</context>
</contexts>
<marker>Krishnamurthy, Wills, 2009</marker>
<rawString>Balachander Krishnamurthy and Craig Wills. 2009. Privacy diffusion on the web: a longitudinal perspective. In Proceedings of the 18th international conference on World wide web, WWW ’09, pages 541–550, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="28275" citStr="Lin, 2004" startWordPosition="4593" endWordPosition="4594">ument relevance could be estimated by a variety of click-based methods, we address the challenge of how to actually extract the correct answer from the document, automatically, with the help of the natural behavior data. We perform this experiment to estimate the performance of passage retrieval for the case when relevant documents are known with high confidence. Evaluation Metrics: We evaluate passage retrieval performance by standard Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP) metrics for top 20 retrieved sentences (Voorhees and Tice, 1999). We also evaluate ROUGE-1 metric (Lin, 2004) for the first retrieved passage. 0 0.1 0.2 0.3 0.4 0.5 fragment score Figure 1: The actual passage interestingness, measured by intersection with user’s answer, vs. the passage relevance score BScore predicted from behavior data 6 Results We now present the empirical results. First, we report the intermediate result of using behavior data to infer the interesting (useful) passages in the document. Then, we report the main results of the paper where the quality of the generated snippets with and without using behavior data is compared using human judgments. 6.1 Prediction of Passage Interestin</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan R Mayer</author>
<author>John C Mitchell</author>
</authors>
<title>Thirdparty web tracking: Policy and technology.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 IEEE Symposium on Security and Privacy, SP ’12,</booktitle>
<pages>413--427</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="13000" citStr="Mayer and Mitchell, 2012" startWordPosition="2078" endWordPosition="2081">s on web pages, major organizations can (and sometimes do) use proxies, and common page widgets like banner ads and visit counters commonly inject JavaScript to monitor basic user interactions – and can be easily extended to collect the examination data described in this paper. The privacy and security of these methods are beyond the scope of this paper, we merely point out that these behavior gathering tools, assumed by our approach, already exist and are already widely deployed. The interested reader can obtain an overview of the relevant privacy issues and proposed solutions in references (Mayer and Mitchell, 2012; Krishnamurthy and Wills, 2009). 3.3 Acquiring Search Behavior Data Our infrastructure for acquiring search behavior was developed with two goals in mind: (1) to obtain behavior data similar to real-world search, with the ability to track fine-grained search behavior such as 1013 a mouse cursor movement (as there are no publicly available data of this kind); (2) to create a controlled and clean ground truth set, to train our system and evaluate the effectiveness of our approach. To collect sufficient amount of search behavior data, we adapted for our task the publicly available UFindIt archit</context>
</contexts>
<marker>Mayer, Mitchell, 2012</marker>
<rawString>Jonathan R. Mayer and John C. Mitchell. 2012. Thirdparty web tracking: Policy and technology. In Proceedings of the 2012 IEEE Symposium on Security and Privacy, SP ’12, pages 413–427, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>T Kanungo</author>
</authors>
<title>Machine learned sentence selection strategies for query-biased summarization.</title>
<date>2008</date>
<booktitle>In SIGIR Learning to Rank Workshop.</booktitle>
<contexts>
<context position="31856" citStr="Metzler and Kanungo, 2008" startWordPosition="5165" endWordPosition="5169">ination time, measured by eye-tracking experiments (Guo and Agichtein, 2010), and it helps us detect local behavior in the neighborhood of a specific text passage. Analysis of Searcher Attention: In order to better understand what characteristics of the textual passages attract the searcher’s attention, we explored 21 linguistic features for each sentence. Our features were designed to estimate text readability, and the overlap of a passage with the query that was used to find the document. We implemented the readability features from (Kanungo and Orr, 2009), and query matching features from (Metzler and Kanungo, 2008). Table 3 reports the top 10 features with the highest absolute value of the correlation coefficient with passage interestingness score BScore. Interestingly, the most highly correlated features are related to readability, while query matching features are less important. Feature description corr Number of distinct words in the passage 0.31 Total number of words in the passage 0.28 Number of letter ([a-zA-z]) characters 0.27 Relative location of the passage in the document -0.25 Number of unique words in the passage -0.24 divided by total number of words Number of punctuation characters -0.20 </context>
</contexts>
<marker>Metzler, Kanungo, 2008</marker>
<rawString>D. Metzler and T. Kanungo. 2008. Machine learned sentence selection strategies for query-biased summarization. In SIGIR Learning to Rank Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Qanus: An open-source question-answering platform http://www.comp.nus.edu.sg/ junping/docs/qanus.pdf.</title>
<date>2010</date>
<contexts>
<context position="10942" citStr="Ng and Kan, 2010" startWordPosition="1752" endWordPosition="1755"> most likely to contain correct answers to a question, which could then be incorporated into a fully automated question answering system, or returned to the user directly, for example, by incorporating these passages into the result abstracts or “snippets”. 3.2 Approach Our approach accomplishes the goal above by incorporating both textual and behavioral evidence. Specifically, we combine together traditional textbased passage retrieval features, and the inferred user interest in specific parts of a document based on searcher behavior. First, a passage score is obtained from the QASYS system (Ng and Kan, 2010), resulting in a strong text-only baseline that generates candidate passages. Separately, examination behavior data is collected over the landing pages, using our logging infrastructure described in the next section. Then, a behavior model is trained to identify the passages of interest to the user, based on user examination data (Section 4.2). Finally, the behavior-based prediction of interest in each candidate passage is combined with the original (text-based) passage score, in order to generate the final behavior-biased passage ranking (Section 4.3). Note that by decoupling the behavior mod</context>
<context position="16947" citStr="Ng and Kan, 2010" startWordPosition="2729" endWordPosition="2732"> engine query that the user issued, a URL and HTML page, the bounding boxes of each word in the HTML text, and all of the searcher actions, e.g., mouse movement coordinates, mouse clicks, and scrolling. 4 Behavior-Biased Passage Retrieval We now present the details of our behavior-biased passage retrieval algorithm (BePR). First, we describe the text-only retrieval system. Then, we introduce our method for inferring the most interesting or useful parts of the document from user behavior (Section 4.2). 4.1 Text-Based Passage Retrieval We adopt an open-source question answering framework QANUS (Ng and Kan, 2010) (version v29Nov2012). The QANUS distribution contains the fully functional factoid QA system QA-SYS that we use as a baseline for our experiments. QASYS implements many of the state-of-the-art question answering techniques, and is similar to a topperforming QA system from TREC (Sun et al., 2005). The QA-SYS distribution is configured for processing documents and questions in TREC QA format, and we adopted QA-SYS for answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) </context>
</contexts>
<marker>Ng, Kan, 2010</marker>
<rawString>Jun-Ping Ng and Min-Yen Kan. 2010. Qanus: An open-source question-answering platform http://www.comp.nus.edu.sg/ junping/docs/qanus.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerry Rodden</author>
<author>Xin Fu</author>
<author>Anne Aula</author>
<author>Ian Spiro</author>
</authors>
<title>Eye-mouse coordination patterns on web search results pages.</title>
<date>2008</date>
<booktitle>In CHI ’08 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’08,</booktitle>
<pages>2997--3002</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8085" citStr="Rodden et al. (2008)" startWordPosition="1287" endWordPosition="1290">her et al. also studied the prediction of salient Web page regions using eye-tracking (Buscher et al., 2009a). This work, and others, have shown that user attention can help identify regions of documents of particular relevance or usefulness for the query. While eye tracking equipment limits the applicability of these findings to lab studies, these studies served as inspiration to our work to detect the inferred areas of interest. Specifically, we use mouse cursor tracking as a natural proxy for user’s attention, to replace the requirement for eye tracking equipment. As originally reported by Rodden et al. (2008), the authors discovered the coordination between a user’s eye movements and mouse movements when scanning a web search results page. This work was further extended by Huang et al. (2012) to predict the gaze position from mouse cursor movement, with mean error of about 150 px. In summary, there is mounting evidence that the user’s attention in web search can be approximated using mouse cursor, scrolling, and other interaction data. In particular, Hijikata (2004) proposed a method to extract text passages of Web pages based on the user’s mouse activity and found that extracted passages based on</context>
</contexts>
<marker>Rodden, Fu, Aula, Spiro, 2008</marker>
<rawString>Kerry Rodden, Xin Fu, Anne Aula, and Ian Spiro. 2008. Eye-mouse coordination patterns on web search results pages. In CHI ’08 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’08, pages 2997–3002, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renxu Sun</author>
<author>Jing Jiang</author>
<author>Yee Fan Tan</author>
<author>Hang Cui</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
</authors>
<title>Using syntactic and semantic relation analysis in question answering.</title>
<date>2005</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="2463" citStr="Sun et al., 2005" startWordPosition="380" endWordPosition="383">nalysis is typically applied only on the (limited) set of candidate passages retrieved. The main reason is that it is generally not practical to perform deep analysis on all documents in a large collection, and not yet feasible for the Web at large. *Work done at Emory University. In the web search setting, automated question answering presents additional challenges and opportunities. On the downside, the questions and queries from real users are often not grammatical or well-formed, differing from the questions used in the traditional TREC Question Answering evaluations (Kelly and Lin, 2007; Sun et al., 2005). On the upside, by interacting with a search engine, the millions of searchers implicitly provide additional clues about usefulness of documents, result ranking, and other aspects of the search process. In this paper, we explore making use of the search behavior data to improve passage retrieval for automated Question Answering on the web. Our basic observation is that when a user is attempting to answer a question, he or she will more carefully examine the parts of the document that contain an answer. This observation is intuitive, and is strongly supported by numerous eye tracking studies (</context>
<context position="17244" citStr="Sun et al., 2005" startWordPosition="2777" endWordPosition="2780">retrieval algorithm (BePR). First, we describe the text-only retrieval system. Then, we introduce our method for inferring the most interesting or useful parts of the document from user behavior (Section 4.2). 4.1 Text-Based Passage Retrieval We adopt an open-source question answering framework QANUS (Ng and Kan, 2010) (version v29Nov2012). The QANUS distribution contains the fully functional factoid QA system QA-SYS that we use as a baseline for our experiments. QASYS implements many of the state-of-the-art question answering techniques, and is similar to a topperforming QA system from TREC (Sun et al., 2005). The QA-SYS distribution is configured for processing documents and questions in TREC QA format, and we adopted QA-SYS for answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) question processing, and (3) answer retrieval. In the first stage, the downloaded HTML pages are pre-processed with Natural Language Tool Kit (NLTK, Bird (2006)). Extracted text is divided into sentences using Punkt unsupervised sentence split1014 ter (Kiss and Strunk, 2006). The QA-SYS performs </context>
</contexts>
<marker>Sun, Jiang, Tan, Cui, Chua, Kan, 2005</marker>
<rawString>Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua, and Min-Yen Kan. 2005. Using syntactic and semantic relation analysis in question answering. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>41--47</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5573" citStr="Tellex et al. (2003)" startWordPosition="881" endWordPosition="884">the user’s page examination data. Next we describe related work, to place our contribution in context. 2 Related Work Our work brings together two areas of research: passage retrieval for question answering, and mining searcher behavior data. Passage retrieval has long been recognized as the first crucial step of automatic question answering. In some cases, passage retrieval can even serve as the final product of a Question Answering system (Clarke et al., 2000). As another example, redundancy in the retrieved passages has been used by the AskMSR system (Brill et al., 2002) to select answers. Tellex et al. (2003) report a thorough comparison of passage retrieval methods for QA, up to 2003. Additional improvements have been achieved by using deeper analysis of the text. For example, Cui et al. (2005) exploited dependency relations between the question terms, Aktolga et al. (2011) incorporated syntactic structure and answer typing, while Harabagiu et al. (2005) used semantic analysis at all stages of the question answering process. In this paper, we pursue a complementary direction, by exploiting searcher examination behavior, with the assumption that human searchers can easily zoom in on relevant passa</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 41– 47, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17917" citStr="Toutanova et al., 2003" startWordPosition="2882" endWordPosition="2885">sing documents and questions in TREC QA format, and we adopted QA-SYS for answer extraction from web documents. QA-SYS takes a set of documents and a question as an input, and processes the input in three stages: (1) information source preparation, (2) question processing, and (3) answer retrieval. In the first stage, the downloaded HTML pages are pre-processed with Natural Language Tool Kit (NLTK, Bird (2006)). Extracted text is divided into sentences using Punkt unsupervised sentence split1014 ter (Kiss and Strunk, 2006). The QA-SYS performs Part of Speech tagging using Stanford POS tagger (Toutanova et al., 2003), and Named Entity Recognition using Stanford NER (Finkel et al., 2005), and then builds a Lucene index over the set of input documents. In the second stage the QA-SYS performs POS tagging, NE recognition, and question type classification for an input question. To answer a question, QA-SYS creates a query from the question, performs the search over the indexed text collection, and retrieves top 50 documents. Each document is split by sentences, and for each sentence a QA-SYS Passage Retrieval Score (Text5core) is computed as a linear combination of term frequency score, proximity score, and te</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Dawn M Tice</author>
</authors>
<title>The trec-8 question answering track evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec. nist. gov/pubs/trec8/t8</booktitle>
<note>proceedings. html.</note>
<contexts>
<context position="28230" citStr="Voorhees and Tice, 1999" startWordPosition="4583" endWordPosition="4586"> was extracted from it. In a real-world scenario, while document relevance could be estimated by a variety of click-based methods, we address the challenge of how to actually extract the correct answer from the document, automatically, with the help of the natural behavior data. We perform this experiment to estimate the performance of passage retrieval for the case when relevant documents are known with high confidence. Evaluation Metrics: We evaluate passage retrieval performance by standard Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP) metrics for top 20 retrieved sentences (Voorhees and Tice, 1999). We also evaluate ROUGE-1 metric (Lin, 2004) for the first retrieved passage. 0 0.1 0.2 0.3 0.4 0.5 fragment score Figure 1: The actual passage interestingness, measured by intersection with user’s answer, vs. the passage relevance score BScore predicted from behavior data 6 Results We now present the empirical results. First, we report the intermediate result of using behavior data to infer the interesting (useful) passages in the document. Then, we report the main results of the paper where the quality of the generated snippets with and without using behavior data is compared using human ju</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>Ellen Voorhees and Dawn M Tice. 1999. The trec-8 question answering track evaluation. In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec. nist. gov/pubs/trec8/t8 proceedings. html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryen W White</author>
<author>Georg Buscher</author>
</authors>
<title>Text selections as implicit relevance feedback.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>1151--1152</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8906" citStr="White and Buscher (2012)" startWordPosition="1422" endWordPosition="1425">t the gaze position from mouse cursor movement, with mean error of about 150 px. In summary, there is mounting evidence that the user’s attention in web search can be approximated using mouse cursor, scrolling, and other interaction data. In particular, Hijikata (2004) proposed a method to extract text passages of Web pages based on the user’s mouse activity and found that extracted passages based on mouse ac1012 tivity such as text tracing, link pointing, link clicking and text selection enable more accurate extraction of key words of interest than using the whole text of the page. Recently, White and Buscher (2012) proposed a method that uses text selections as implicit feedback for document ranking. Most closely related to this work is a contemporaneous effort on improving web search result summaries, or snippets, by exploiting searcher behavior on the examined documents, described by Ageev et al. (2013). However, to the best of our knowledge, there has been no prior work on modeling searcher interaction on result documents to improve Question Answering performance, and in particular the passage retrieval step. 3 Problem Statement and Approach This section first states the problem we are addressing mor</context>
</contexts>
<marker>White, Buscher, 2012</marker>
<rawString>Ryen W. White and Georg Buscher. 2012. Text selections as implicit relevance feedback. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 1151–1152, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
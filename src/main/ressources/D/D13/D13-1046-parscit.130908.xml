<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.994288">
Building Specialized Bilingual Lexicons Using Large-Scale Background
Knowledge
</title>
<author confidence="0.959847">
Dhouha Bouamor1, Adrian Popescu1, Nasredine Semmar1, Pierre Zweigenbaum2
</author>
<affiliation confidence="0.591163">
1 CEA, LIST, Vision and Content Engineering Laboratory, 91191
</affiliation>
<address confidence="0.445438">
Gif-sur-Yvette CEDEX, France; firstname.lastname@cea.fr
</address>
<email confidence="0.921336">
2LIMSI-CNRS, F-91403 Orsay CEDEX, France; pz@limsi.fr
</email>
<sectionHeader confidence="0.990422" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811458333333">
Bilingual lexicons are central components of
machine translation and cross-lingual infor-
mation retrieval systems. Their manual con-
struction requires strong expertise in both lan-
guages involved and is a costly process. Sev-
eral automatic methods were proposed as an
alternative but they often rely on resources
available in a limited number of languages
and their performances are still far behind
the quality of manual translations. We intro-
duce a novel approach to the creation of spe-
cific domain bilingual lexicon that relies on
Wikipedia. This massively multilingual en-
cyclopedia makes it possible to create lexi-
cons for a large number of language pairs.
Wikipedia is used to extract domains in each
language, to link domains between languages
and to create generic translation dictionaries.
The approach is tested on four specialized do-
mains and is compared to three state of the art
approaches using two language pairs: French-
English and Romanian-English. The newly in-
troduced method compares favorably to exist-
ing methods in all configurations tested.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973414634146">
The plethora of textual information shared on the
Web is strongly multilingual and users’ information
needs often go well beyond their knowledge of for-
eign languages. In such cases, efficient machine
translation and cross-lingual information retrieval
systems are needed. Machine translation already has
a decades long history and an array of commercial
systems were already deployed, including Google
Translate 1 and Systran 2. However, due to the intrin-
sic difficulty of the task, a number of related prob-
lems remain open, including: the gap between text
semantics and statistically derived translations, the
scarcity of resources in a large majority of languages
and the quality of automatically obtained resources
and translations. While the first challenge is general
and inherent to any automatic approach, the second
and the third can be at least partially addressed by
an appropriate exploitation of multilingual resources
that are increasingly available on the Web.
In this paper we focus on the automatic creation of
domain-specific bilingual lexicons. Such resources
play a vital role in Natural Language Processing
(NLP) applications that involve different languages.
At first, research on lexical extraction has relied on
the use of parallel corpora (Och and Ney, 2003).
The scarcity of such corpora, in particular for spe-
cialized domains and for language pairs not involv-
ing English, pushed researchers to investigate the
use of comparable corpora (Fung, 1998; Chiao and
Zweigenbaum, 2003). These corpora include texts
which are not exact translation of each other but
share common features such as domain, genre, sam-
pling period, etc.
The basic intuition that underlies bilingual lexi-
con creation is the distributional hypothesis (Harris,
1954) which puts that words with similar meanings
occur in similar contexts. In a multilingual formu-
lation, this hypothesis states that the translations of
a word are likely to appear in similar lexical envi-
ronments across languages (Rapp, 1995). The stan-
dard approach to bilingual lexicon extraction builds
</bodyText>
<footnote confidence="0.999927">
1http://translate.google.com/
2http://www.systransoft.com/
</footnote>
<page confidence="0.958066">
479
</page>
<note confidence="0.735475">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479–489,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999863235294117">
on the distributional hypothesis and compares con-
text vectors for each word of the source and tar-
get languages. In this approach, the comparison of
context vectors is conditioned by the existence of a
seed bilingual dictionary. A weakness of the method
is that poor results are obtained for language pairs
that are not closely related (Ismail and Manandhar,
2010). Another important problem occurs whenever
the size of the seed dictionary is small due to ignor-
ing many context words. Conversely, when dictio-
naries are detailed, ambiguity becomes an important
drawback.
We introduce a bilingual lexicon extraction ap-
proach that exploits Wikipedia in an innovative
manner in order to tackle some of the problems
mentioned above. Important advantages of using
Wikipedia are:
</bodyText>
<listItem confidence="0.996217888888889">
• The resource is available in hundreds of lan-
guages and it is structured as unambiguous con-
cepts (i.e. articles).
• The languages are explicitly linked through
concept translations proposed by Wikipedia
contributors.
• It covers a large number of domains and is thus
potentially useful in order to mine a wide array
of specialized lexicons.
</listItem>
<bodyText confidence="0.7241135">
Mirroring the advantages, there are a number of
challenges associated with the use of Wikipedia:
</bodyText>
<listItem confidence="0.985556444444444">
• The comparability of concept descriptions in
different languages is highly variable.
• The translation graph is partial since, when
considering any language pair, only a part of
the concepts are available in both languages
and explicitly connected.
• Domains are unequally covered in Wikipedia
(Halavais and Lackaff, 2008) and efficient do-
main targeting is needed.
</listItem>
<bodyText confidence="0.99990732">
The approach introduced in this paper aims to
draw on Wikipedia’s advantages while appropri-
ately addressing associated challenges. Among
the techniques devised to mine Wikipedia content,
we hypothesize that an adequate adaptation of Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is fitted to our application con-
text. ESA was already successfully tested in differ-
ent NLP tasks, such as word relatedness estimation
or text classification, and we modify it to mine spe-
cialized domains, to characterize these domains and
to link them across languages.
The evaluation of the newly introduced approach
is realized on four diversified specialized domains
(Breast Cancer, Corporate Finance, Wind Energy
and Mobile Technology) and for two pairs of lan-
guages: French-English and Romanian-English.
This choice allows us to study the behavior of dif-
ferent approaches for a pair of languages that are
richly represented and for a pair that includes Roma-
nian, a language that has fewer associated resources
than French and English. Experimental results show
that the newly introduced approach outperforms the
three state of the art methods that were implemented
for comparison.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9971875">
In this section, we first give a review of the stan-
dard approach and then introduce methods that build
upon it. Finally, we discuss works that rely on Ex-
plicit Semantic Analysis to solve other NLP tasks.
</bodyText>
<subsectionHeader confidence="0.997449">
2.1 Standard Approach (SA)
</subsectionHeader>
<bodyText confidence="0.9998178">
Most previous approaches that address bilingual lex-
icon extraction from comparable corpora are based
on the standard approach (Fung, 1998; Chiao and
Zweigenbaum, 2002; Laroche and Langlais, 2010).
This approach is composed of three main steps:
</bodyText>
<listItem confidence="0.999373333333333">
1. Building context vectors: Vectors are first
extracted by identifying the words that ap-
pear around the term to be translated Wand
in a window of n words. Generally, asso-
ciation measures such as the mutual infor-
mation (Morin and Daille, 2006), the log-
likelihood (Morin and Prochasson, 2011) or the
Discounted Odds-Ratio (Laroche and Langlais,
2010) are employed to shape the context vec-
tors.
2. Translation of context vectors: To enable the
comparison of source and target vectors, source
vectors are translated intoto the target language
by using a seed bilingual dictionary. When-
ever several translations of a context word exist,
</listItem>
<page confidence="0.994369">
480
</page>
<bodyText confidence="0.909587666666667">
all translation variants are taken into account.
Words not included in the seed dictionary are
simply ignored.
</bodyText>
<listItem confidence="0.476828">
3. Comparison of source and target vectors:
</listItem>
<bodyText confidence="0.90945975">
Given Wand, its automatically translated con-
text vector is compared to the context vectors
of all possible translations from the target lan-
guage. Most often, the cosine similarity is
used to rank translation candidates but alterna-
tive metrics, including the weighted Jaccard in-
dex (Prochasson et al., 2009) and the city-block
distance (Rapp, 1999), were studied.
</bodyText>
<subsectionHeader confidence="0.99834">
2.2 Improvements of the Standard Approach
</subsectionHeader>
<bodyText confidence="0.999973">
Most of the improvements of the standard approach
are based on the observation that the more repre-
sentative the context vectors of a candidate word
are, the better the bilingual lexicon extraction is. At
first, additional linguistic resources, such as special-
ized dictionaries (Chiao and Zweigenbaum, 2002) or
transliterated words (Prochasson et al., 2009), were
combined with the seed dictionary to translate con-
text vectors.
The ambiguities that appear in the seed bilingual
dictionary were taken into account more recently.
(Morin and Prochasson, 2011) modify the standard
approach by weighting the different translations ac-
cording to their frequency in the target corpus. In
(Bouamor et al., 2013), we proposed a method that
adds a word sense disambiguation process relying
on semantic similarity measurement from WordNet
to the standard approach. Given a context vector in
the source language, the most probable translation of
polysemous words is identified and used for build-
ing the corresponding vector in the target language.
The most probable translation is identified using the
monosemic words that appear in the same lexical en-
vironment.
On specialized French-English comparable cor-
pora, this approach outperforms the one proposed in
(Morin and Prochasson, 2011), which is itself bet-
ter than the standard approach. The main weakness
of (Bouamor et al., 2013) is that the approach relies
on WordNet and its application depends on the ex-
istence of this resource in the target language. Also,
the method is highly dependent on the coverage of
the seed bilingual dictionary.
</bodyText>
<subsectionHeader confidence="0.999047">
2.3 Explicit Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999808837837838">
Explicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007) is a method that maps textual
documents onto a structured semantic space using
classical text indexing schemes such as TF-IDF. Ex-
amples of semantic spaces used include Wikipedia
or the Open Directory Project but, due to superior
performances, Wikipedia is most frequently used.
In the original evaluation, ESA outperformed state
of the art methods in a word relatedness estimation
task.
Subsequently, ESA was successfully exploited in
other NLP tasks and in information retrieval. Radin-
sky and al. (2011) added a temporal dimension to
word vectors and showed that this addition improves
the results of word relatedness estimation. (Hassan
and Mihalcea, 2011) introduced Salient Semantic
Analysis (SSA), a development of ESA that relies
on the detection of salient concepts prior to map-
ping words to concepts. SSA and the original ESA
implementation were tested on several word related-
ness datasets and results were mixed. Improvements
were obtained for text classification when compar-
ing SSA with the authors’ in-house representation
of the method. ESA has weak language depen-
dence and was already deployed in multilingual con-
texts. (Sorg and Cimiano, 2012) extended ESA to
other languages and showed that it is useful in cross-
lingual and multilingual retrieval task. Their focus
was on creating a language independent conceptual
space in which documents would be mapped and
then retrieved.
Some open ESA topics related to bilingual lex-
icon creation include: (1) the document represen-
tation which is simply done by summing individ-
ual contributions of words, (2) the adaptation of the
method to specific domains and (3) the coverage of
the underlying resource in different language.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="method">
3 ESA for Bilingual Lexicon Extraction
</sectionHeader>
<bodyText confidence="0.999957714285714">
The main objective of our approach is to devise lex-
icon translation methods that are easily applicable
to a large number of language pairs, while preserv-
ing the overall quality of results. A subordinated
objective is to exploit large scale background mul-
tilingual knowledge, such as the encyclopedic con-
tent available in Wikipedia. As we mentioned, ESA
</bodyText>
<page confidence="0.998117">
481
</page>
<figureCaption confidence="0.9891115">
Figure 1: Overview of the Explicit Semantic
Analysis enabled bilingual lexicon extraction.
</figureCaption>
<bodyText confidence="0.997199">
(Gabrilovich and Markovitch, 2007) was exploited
in a number of NLP tasks but not in bilingual lexi-
con extraction.
Figure 1 shows the overall architecture of the lex-
ical extraction process we propose. The process is
completed in the following three steps:
</bodyText>
<listItem confidence="0.898355">
1. Given a word to be translated and its con-
text vector in the source language, we derive
a ranked list of similar Wikipedia concepts (i.e.
articles) using the ESA inverted index.
2. Then, a translation graph is used to retrieve the
corresponding concepts in the target language.
3. Candidate translations are found through a sta-
tistical processing of concept descriptions from
the ESA direct index in the target language.
</listItem>
<bodyText confidence="0.999634">
In this section, we first introduce the elements of
the original formulation of ESA necessary in our ap-
proach. Then, we detail the three steps that com-
pose the main bilingual lexicon extraction method
illustrated in Figure 1. Finally, as a complement to
the main method we introduce a measure for domain
word specificity and present a method for extracting
generic translation lexicons.
</bodyText>
<subsectionHeader confidence="0.999627">
3.1 ESA Word and Concept Representation
</subsectionHeader>
<bodyText confidence="0.99998275">
Given a semantic space structured using a set of M
concepts and including a dictionary of N words,
a mapping between words and concepts can be
expressed as the following matrix:
</bodyText>
<equation confidence="0.96614225">
w(W1, C1) w(W2, C1) ... w(WN, C1)
w(W1, C2) w(W2, C2) ... w(WN, C2)
... ... ...
w(W1, CM) w(W2, CM) ... w(WN, CM)
</equation>
<bodyText confidence="0.99948284">
When Wikipedia is exploited concepts are
equated to Wikipedia articles and the texts of the ar-
ticles are processed in order to obtain the weights
that link words and concepts. In (Gabrilovich and
Markovitch, 2007), the weights w that link words
and concepts were obtained through a classical TF-
IDF weighting of Wikipedia articles. A series of
tweaks destined to improve the method’s perfor-
mance were used and disclosed later3. For instance,
administration articles, lists, articles that are too
short or have too few links are discarded. Higher
weight is given to words in the article title and
more longer articles are favored over shorter ones.
We implemented a part of these tweaks and tested
our own version of ESA with the Wikipedia ver-
sion used in the original implementation. The cor-
relation with human judgments of word relatedness
was 0.72 against 0.75 reported by (Gabrilovich and
Markovitch, 2007). The ESA matrix is sparse since
the N size of the dictionary, is usually in the range
of hundreds of thousands and each concept is usu-
ally described by hundreds of distinct words. The
direct ESA index from Figure 1 is obtained by read-
ing the matrix horizontally while the inverted ESA
index is obtained by reading the matrix vertically.
</bodyText>
<footnote confidence="0.968028">
3https://github.com/faraday/
wikiprep-esa/wiki/roadmap
</footnote>
<page confidence="0.99002">
482
</page>
<table confidence="0.984598333333333">
Terme Concepts
action ´evaluation d’action, communisme, actionnaire activiste, socialisme,
d´evelopement durable ...
d´eficit crise de la dette dans la zone euro, dette publique, r`egle d’or budg´etaire,
d´eficit, trouble du d´eficit de l’attention ...
cisaillement taux de cisaillement, zone de cisaillement, cisaillement, contrainte de cisaille-
ment, viscoanalyseur ...
turbine ffc turbine potsdam, turbine a` gaz, turbine, urbine hydraulique, cog´en´eration
. . .
cryptage TEMPEST, chiffrement, liaison 16, Windows Vista, transfert de fichiers ...
protocole Ad-hoc On-demand Distance Vector, protocole de Kyoto, optimized link state
routing protocol, liaison 16, IPv6 ...
biopsie biopsie, maladie de Horton, cancer du sein, cancer du poumon, imagerie par
r´esonance magn´etique ...
palpation cancer du sein, cellulite, examen clinique, appendicite, t´enosynovite ...
</table>
<tableCaption confidence="0.570992333333333">
Table 1: The five most similar Wikipedia concepts to the French terms action[share], d´eficit[deficit], ci-
saillement[shear], turbine[turbine], cryptage[encryption], biopsie[biopsie] and palpation[palpation] and
their context vectors.
</tableCaption>
<subsectionHeader confidence="0.999242">
3.2 Source Language Processing
</subsectionHeader>
<bodyText confidence="0.998960909090909">
The objective of the source language processing is
to obtain a ranked list of similar Wikipedia concepts
for each candidate word (Wcand) in a specialized do-
main. To do this, a context vector is first built for
each Wcand from a specialized monolingual corpus.
The association measure between Wcand and context
words is obtained using the Odds-Ratio (defined in
equation 5). Wikipedia concepts in the source lan-
guage Cs that are similar to Wcand and to a part of its
context words are extracted and ranked using equa-
tion 1.
</bodyText>
<equation confidence="0.9991982">
Rank(Cs) = (10 * max(OddsWcand
Wsi )
OddsWcand
Wsi * w(Wsi, Cs)
(1)
</equation>
<bodyText confidence="0.979319884615385">
where max(OddsWcand
Wsi ) is the highest Odds-Ratio
association between Wcand and any of its context
words Wsi; the factor 10 was empirically set to
give more importance to Wcand over context words;
w(Wcand, Cs) is the weight of the association be-
tween Wcand and Cs from the ESA matrix; n is the
total number of words Wsi in the context vector of
Wcand; OddsWcand
Wsi is the association value between
Wcand and Wsi and w(Wsi, Cs) are the weights of
the associations between each context word Wsi and
Cs from the ESA matrix. The use of contextual in-
formation in equation 1 serves to characterize the
candidate word in the target domain.
In table 1, we present the five most similar
Wikipedia concepts to the French terms action,
d´eficit, cisaillement, turbine, cryptage, biopsie and
palpation and their context vectors. These terms are
part of the four specialized domains we are studying
here. From observing these examples, we note that
despite the difference between the specialized do-
mains and word ambiguity (words action and proto-
cole), our method has the advantage of successfully
representing each word to be translated by relevant
conceptual spaces.
</bodyText>
<subsectionHeader confidence="0.998896">
3.3 Translation Graph Construction
</subsectionHeader>
<bodyText confidence="0.999993555555556">
To bridge the gap between the source and target lan-
guages, a concept translation graph that enables the
multilingual extension of ESA is used. This con-
cept translation graph is extracted from the explicit
translation links available in Wikipedia articles and
is exploited in order to connect a word’s conceptual
space in the source language with the correspond-
ing conceptual space in the target language. Only a
part of the articles have translations and the size of
</bodyText>
<equation confidence="0.972435666666667">
n
* w(Wcand, Cs)) +
Z=1
</equation>
<page confidence="0.981705">
483
</page>
<bodyText confidence="0.9999455">
the conceptual space in the target language is usu-
ally smaller than the space in the source language.
For instance, the French-English translation graph
contains 940,215 pairs of concepts while the French
and English Wikipedias contain approximately 1.4
million articles, respectively 4.25 million articles.
</bodyText>
<subsectionHeader confidence="0.986224">
3.4 Target Language Processing
</subsectionHeader>
<bodyText confidence="0.999955666666667">
The third step of the approach takes place in the tar-
get language. Using the translation graph, we select
the 100 most similar concept translations (thresh-
old determined empirically after preliminary exper-
iments) from the target language and use their di-
rect ESA representations in order to retrieve poten-
tial translations for the candidate word Wand from
source language. These candidate translations Wt
are ranked using equation 2.
</bodyText>
<equation confidence="0.990982333333333">
w(Wt, Cti)
avg(Cti) )
* log(count(Wt, S)) (2)
</equation>
<bodyText confidence="0.994540470588235">
with w(Wt, Cti) is the weight of the translation can-
didate WT for concept Cti from the ESA matrix
in the target language; avg(Cti) is the average TF-
IDF score of words that appear in Cti; S is the set
of similar concepts Cti in the target language and
count(Wt, S) accounts for the number of different
concepts from S in which the candidate translation
WT appears.
The accumulation of weights w(Wt, Cti) fol-
lows the way original ESA text representations
are calculated (Gabrilovich and Markovitch, 2007)
and avg(Cti) is used in order to correct the
bias of the TF-IDF scheme towards short articles.
log(count(Wt, S)) is used to favor words that are
associated with a larger number of concepts. log
weighting was chosen after preliminary experiments
with a wide range of functions.
</bodyText>
<subsectionHeader confidence="0.96646">
3.5 Domain Specificity
</subsectionHeader>
<bodyText confidence="0.999950714285714">
In previous works, ESA was usually exploited
in generic tasks that did not require any domain
adaptation. Here we process information from
specific domains and we need to measure the
specificity of words in those domains. The domain
extraction is seeded by using Wikipedia concepts
(noted Cseed) that best describes the domain in
the target language. For instance, in English,
the Corporate Finance domain is seeded with
https://en.wikipedia.org/wiki/Corporate finance.
We extract a set of 10 words with the highest
TF-IDF score from this article (noted 5W) and use
them to retrieve a domain ranking of concepts in the
target language Rankdom(Ct) with equation 3.
</bodyText>
<equation confidence="0.99611675">
n
Rankdom(Ct) = ( w(Wti, Ct)
Z=1
* w(Cseed, Wti)) * count(5W, Ct) (3)
</equation>
<bodyText confidence="0.99999305">
where n is size of the seed list of words (i.e. 10
items), w(Wti, Ct) is the weight of the domain
words in the concept Ct ; w(Cseed, Wti) is the
weight of Wti in Cseed, the seed concept of the do-
main, and count(5W, Ct) is the number of distinct
seed words from 5W that appear in Ct.
The first part of equation 3 sums up the contribu-
tions of different words from 5W that appear in Ct
while the second part is meant to further reinforce
articles that contain a larger number of domain key-
words from 5W.
Domain delimitation is performed by retaining
articles whose Rankdom(Ct) is at least 1% or the
score of the top Rankdom(Ct) score. This threshold
was set up during preliminary experiments. Given
the delimitation obtained with equation 3, we calcu-
late a domain specificity score (specifdom(Wt)) for
each word that occurs in the domain ( equation 4).
specifdom(Wt) estimates how much of a word’s use
in an underlying corpus is related to a target domain.
</bodyText>
<equation confidence="0.998189">
DFdom(Wt)
specifdom(Wt) = (4)
DFgen(Wt)
</equation>
<bodyText confidence="0.999947769230769">
where DFdom and DFgen stand for the domain and
the generic document frequency of the word Wt.
specifdom(Wt) will be used to favor words with
greater domain specificity over more general ones
when several translations are available in a seed
generic translation lexicon. For instance, the French
word action is ambiguous and has English transla-
tions such as action, stock, share etc. In a general
case, the most frequent translation is action whereas
in a corporate finance context, share or stock are
more relevant. The specificity of the three transla-
tions, from highest to lowest, is: share, stock and ac-
tion and is used to rank these potential translations.
</bodyText>
<equation confidence="0.986217333333333">
n
Rank(Wt) = (
Z=1
</equation>
<page confidence="0.994995">
484
</page>
<subsectionHeader confidence="0.866371">
3.6 Generic Dictionaries
</subsectionHeader>
<bodyText confidence="0.999956541666667">
Generic translation dictionaries, already used by ex-
isting bilingual lexicon extraction approaches, can
also be integrated in the newly proposed approach.
The Wikipedia translation graph is transformed into
a translation dictionary by removing the disam-
biguation marks from ambiguous concept titles, as
well as lists, categories and other administration
pages. Moreover, since the approach does not han-
dle multiword units, we retain only translation pairs
that are composed of unigrams in both languages.
When existing, unigram redirections are also added
in each language.
The obtained dictionaries are incomplete since:
(1) Wikipedia focuses on concepts that are most of-
ten nouns, (2) specialized domain terms often do not
have an associated Wikipedia entry and (3) the trans-
lation graph covers only a fraction of the concepts
available in a language. For instance, the result-
ing translation dictionaries have 193,543 entries for
French-English and 136,681 entries for Romanian-
English. They can be used in addition to or instead
of other resources available and are especially useful
when there are only few other resources that link the
pair of languages processed.
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999908375">
The performances of our approach are evaluated
against the standard approach and its developments
proposed by (Morin and Prochasson, 2011) and
(Bouamor et al., 2013). In this section, we first
describe the data and resources we used in our ex-
periments. We then present differents parameters
needed in the implementation of the different meth-
ods tested. Finally, we discuss the obtained results.
</bodyText>
<subsectionHeader confidence="0.7836515">
4.1 Data and Resources
Comparable corpora
</subsectionHeader>
<bodyText confidence="0.999852">
We conducted our experiments on four French-
English and Romanian-English specialized compa-
rable corpora: Corporate Finance, Breast Can-
cer, Wind Energy and Mobile Technology. For
the Romanian-English language pair, we used
Wikipedia to collect comparable corpora for all do-
mains since they were not already available. The
Wikipedia corpora are harvested using a category-
based selection. We consider the topic in the source
</bodyText>
<table confidence="0.9998027">
Domain FR EN
Corporate Finance 402,486 756,840
Breast Cancer 396,524 524,805
Wind Energy 145,019 345,607
Mobile Technology 197,689 144,168
Domain RO EN
Corporate Finance 206,169 524,805
Breast Cancer 22,539 322,507
Wind Energy 121,118 298,165
Mobile Technology 200,670 124,149
</table>
<tableCaption confidence="0.999126">
Table 2: Number of content words in the
comparable corpora.
</tableCaption>
<bodyText confidence="0.99994948">
language (for instance Cancer Mamar [Breast Can-
cer]) as a query to Wikipedia and extract all its sub-
topics (i.e., sub-categories) to construct a domain-
specific category tree. Then, based on the con-
structed tree, we collect all Wikipedia articles be-
longing to at least one of these categories and use
inter-language links to build the comparable cor-
pora.
Concerning the French-English pair, we followed
the strategy described above to extract the compa-
rable corpora related to the Corporate Finance and
Breast Cancer domains since they were otherwise
unavailable. For the two other domains, we used
the corpora released in the TTC project4. All cor-
pora were normalized through the following linguis-
tic preprocessing steps: tokenization, part-of-speech
tagging, lemmatization, and function word removal.
The resulting corpora5 sizes are presented in Table
2. The size of the domain corpora vary within and
across languages, with the corporate finance domain
being the richest in both languages. In Romanian,
Breast Cancer is particularly small, with approxi-
mately 22,000 tokens included. This variability will
allow us to test if there is a correlation between cor-
pus size and quality of results.
</bodyText>
<subsectionHeader confidence="0.813619">
Bilingual dictionary
</subsectionHeader>
<bodyText confidence="0.998933">
The seed generic French-English dictionary used
to translate French context vectors consists of an
in-house manually built resource which contains
approximately 120,000 entries. For Romanian-
</bodyText>
<footnote confidence="0.995799666666667">
4http://www.ttc-project.eu/index.php/
releases-publications
5Comparable corpora will be shared publicly
</footnote>
<page confidence="0.99367">
485
</page>
<table confidence="0.9997402">
Domain FR-EN RO-EN
Corporate Finance 125 69
Breast Cancer 96 38
Wind Energy 89 38
Mobile Technology 142 94
</table>
<tableCaption confidence="0.999751">
Table 3: Sizes of the evaluation lists.
</tableCaption>
<bodyText confidence="0.997637">
English, we used the generic dictionary extracted
following the procedure described in Subsection 3.6.
</bodyText>
<subsectionHeader confidence="0.570563">
Gold standard
</subsectionHeader>
<bodyText confidence="0.99997825">
In bilingual terminology extraction from compara-
ble corpora, a reference list is required to evaluate
the performance of the alignment. Such lists are usu-
ally composed of around 100 single terms (Hazem
and Morin, 2012; Chiao and Zweigenbaum, 2002).
Reference lists6 were created for the four specialized
domains and the two pairs of languages. For the
French-English, reference words from the Corpo-
rate Finance domain were extracted from the glos-
sary of bilingual micro-finance terms7. For Breast
Cancer, the list is derived from the MESH and the
UMLS thesauri8. Concerning Wind Energy and Mo-
bile Technology, lists were extracted from special-
ized glossaries found on the Web. The Romanian-
English gold standard was manually created by a na-
tive speaker starting from the French-English lists.
Table 3 displays the sizes of the obtained lists. Ref-
erence terms pairs were retained if each word com-
posing them appeared at least five times in the com-
parable domain corpora.
</bodyText>
<subsectionHeader confidence="0.992522">
4.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9999285">
Aside from those already mentioned, three param-
eters need to be set up: (1) the window size that
defines contexts, (2) the association measure that
measures the strength of the association between
words and the (3) similarity measure that ranks can-
didate translations for state of the art methods. Con-
text vectors are defined using a seven-word window
which approximates syntactic dependencies. The
association and the similarity measures (Discounted
Log-Odds ratio (equation 5) and the cosine simi-
</bodyText>
<footnote confidence="0.999918666666667">
6Reference lists will be shared publicly
7http://www.microfinance.lu/en/
8http://www.nlm.nih.gov/
</footnote>
<bodyText confidence="0.999087">
larity) were set following Laroche and Langlais
(2010), a comprehensive study of the influence of
these parameters on the bilingual alignment.
</bodyText>
<equation confidence="0.995422666666667">
12)(O22 + 12)
Odds-Ratiodzsc = log (O11 + 2) (5)
(O12 + 1 2)(O21 + 1
</equation>
<bodyText confidence="0.999966454545454">
where Oil are the cells of the 2 x 2 contingency ma-
trix of a token s co-occurring with the term 5 within
a given window size.
The F-measure of the Top 20 results (F-
Measure@20), which measures the harmonic mean
of precision and recall, is used as evaluation metric.
Precision is the total number of correct translations
divided by the number of terms for which the system
returned at least one answer. Recall is equal to the
ratio between the number of correct translation and
the total number of words to translate
</bodyText>
<subsectionHeader confidence="0.975339">
4.3 Results and discussion
</subsectionHeader>
<bodyText confidence="0.9847263">
In addition to the basic approach based on ESA
(denoted ESA), we evaluate the performances of
a method so-called DicoSp, in which the transla-
tions are extracted from a generic dictionary and
a method we called ESASp�, which combine ESA
and DicoSp,,. DICOSp,, is based on the generic
dictionary we presented in subsection 3.6 and pro-
ceeds as follows: we extract a list of translations for
each word to be translated from the generic dictio-
nary. The domain specificity introduced in subsec-
tion 3.5 is then used to rank these translations. For
instance, the french term port referring in the Mobile
Technology domain, to the system that allows com-
puters to receive and transmit information is trans-
lated into port and seaport. According to domain
specificity values, the following ranking is obtained:
the English term port obtain the highest specificity
value (0.48). seaport comes next with a specificity
value of 0.01. In ESASp,,, the translations set out in
the translations lists proposed by both ESA and the
generic dictionary are weighted according to their
domain specificity values. The main intuition be-
hind this method is that by adding the information
about the domain specificity, we obtain a new rank-
ing of the bilingual extraction results.
The obtained results are displayed in table 4. The
comparison of state of the art method shows that
BA13 performs better than STAPP and MP11 for
French-English and has comparable performances
(W..d).
</bodyText>
<page confidence="0.974534">
486
</page>
<table confidence="0.99721925">
Method F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.49 0.17 0.08 0.06
MP11 0.55 0.33 0.24 0.05
BA13 0.61 0.37 0.30 0.24
Dicospee 0.50 0.20 0.36 0.25
ESA 0.74 0.50 0.83 0.72
ESAspee 0.81 0.56 0.86 0.75
Method F-Measure@20
Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology
STAPP 0.21 0.13 0.08 0.16
MP11 0.21 0.13 0.08 0.16
BA13 0.21 0.14 0.08 0.17
Dicospee 0.44 0.11 0.21 0.16
ESA 0.76 0.17 0.58 0.53
ESAspee 0.78 0.24 0.58 0.55
</table>
<tableCaption confidence="0.990699">
Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three
</tableCaption>
<bodyText confidence="0.7391106">
state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve-
ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that
we developed (Bouamor et al., 2013). Dicospee exploits a generic dictionary, combined with the use of do-
main specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see
Figure 1). ESAspee combines the results of Dicospee and ESA.
</bodyText>
<figure confidence="0.9693415">
a) FR-EN
b) RO-EN
</figure>
<bodyText confidence="0.999693975609756">
for RO-EN. Consequently, we will use BA13 as the
main baseline for discussing the newly introduced
approach. The results presented in Table 4 show
that ESAspee clearly outperforms the three base-
lines for the four domains and the two pairs of lan-
guages tested. When comparing ESAspee to BA13
for French-English, improvements range between
0.19 for Corporate Finance and 0.56 for Wind En-
ergy. For RO-EN, the improvements vary from 0.1
for Corporate Finance to 0.5 for Wind Energy. Also,
except for the Corporate Finance domain in Roma-
nian, the performance variation across domains is
much smaller for ESAspee than for the three state
of the art methods. This shows that ESAspee is more
robust to domain change and thus more generic.
The results obtained with ESA are signifi-
cantly better than those obtained with Dicospee and
ESAspee, their combination, further improves the
results. The main contribution to ESAspee perfor-
mances comes from ESA, a finding that validates
our assumption that the adequate use of a rich multi-
lingual resource such as Wikipedia is appropriate for
specialized lexicon translation. Dicospee is a sim-
ple method that ranks the different meanings of a
candidate word available in a generic dictionary but
its average performances are comparable to those
of BA13 for FR-EN and higher for RO-EN. This
finding advocates for the importance of good qual-
ity generic dictionaries in specialized lexicon trans-
lation approaches. However, it is clear that such
dictionaries are far from being sufficient in order
to cover all possible domains. There is no clear
correlation between domain size and quality of re-
sults. Although richer than the other three domains,
Corporate Finance has the lowest associated per-
formances. This finding is probably explained by
the intrinsic difficulty of each domain. When pass-
ing from FR-EN to RO-EN the average performance
drop is more significant for BA13 than for the ESA
based methods. The result indicates that our ap-
proach is more robust to language change.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999485">
We have presented a new approach to the creation
of specialized bilingual lexicons, one of the central
</bodyText>
<page confidence="0.994957">
487
</page>
<bodyText confidence="0.999961433962264">
building blocks of machine translation systems. The
proposed approach directly tackles two of the ma-
jor challenges identified in the Introduction. The
scarcity of resources is addressed by an adequate
exploitation of Wikipedia, a resource that is avail-
able in hundreds of languages. The quality of auto-
matic translations was improved by appropriate do-
main delimitation and linking across languages, as
well as by an adequate statistical processing of con-
cepts similar to a word in a given context.
The main advantages of our approach compared
to state of the art methods come from: the increased
number of languages that can be processed, from
the smaller sensitivity to structured resources and
the appropriate domain delimitation. Experimental
validation is obtained through evaluation with four
different domains and two pairs of languages which
shows consistent performance improvement. For
French-English, two languages that have rich asso-
ciated Wikipedia representations, performances are
very interesting and are starting to approach those of
manual translations for three domains out of four (F-
Measure@20 around 0.8). For Romanian-English, a
pair involving a language with a sparser Wikipedia
representation, the performances of our method drop
compared to French-English . However, they do not
decrease to the same extent as those of the best state
of the art method tested. This finding indicates that
our approach is more general and, given its low lan-
guage dependence, it can be easily extended to a
large number of language pairs.
The results presented here are very encouraging
and we will to pursue work in several directions.
First, we will pursue the integration of our method,
notably through comparable corpora creation using
the data driven domain delimitation technique de-
scribed in Subsection 3.5. Equally important, the
size of the domain can be adapted so as to find
enough context for all the words in domain reference
lists. Second, given a word in a context, we currently
exploit all similar concepts from the target language.
Given that comparability of article versions in the
source and the target language varies, we will eval-
uate algorithms for filtering out concepts from the
target language that have low alignment with their
source language versions. A final line of work is
constituted by the use of distributional properties of
texts in order to automatically rank parts of concept
descriptions (i.e. articles) by their relatedness to the
candidate word. Similar to the second direction, this
process involves finding comparable text blocks but
rather at a paragraph or sentence level than at the
article level.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859386363636">
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambiguation
for bilingual lexicon extraction. In Proceedings of the
51st Association for Computational Linguistics (ACL-
HLT), Sofia, Bulgaria, August.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th international conference on Computational lin-
guistics - Volume 2, COLING ’02, pages 1–5. Associ-
ation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
effect of a general lexicon in corpus-based identifi-
cation of french-english medical word translations.
In Proceedings Medical Informatics Europe, volume
95 of Studies in Health Technology and Informatics,
pages 397–402, Amsterdam.
Pascale Fung. 1998. A statistical view on bilingual lexi-
con extraction: From parallel corpora to non-parallel
corpora. In Parallel Text Processing, pages 1–17.
Springer.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI’07, pages 1606–1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Alexander Halavais and Derek Lackaff. 2008. An Anal-
ysis of Topical Coverage of Wikipedia. Journal of
Computer-Mediated Communication, 13(2):429–440.
Z.S. Harris. 1954. Distributional structure. Word.
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Amir Hazem and Emmanuel Morin. 2012. Adaptive dic-
tionary for bilingual lexicon extraction from compara-
ble corpora. In Proceedings, 8th international confer-
ence on Language Resources and Evaluation (LREC),
Istanbul, Turkey, May.
Azniah Ismail and Suresh Manandhar. 2010. Bilin-
gual lexicon extraction from comparable corpora us-
ing in-domain terms. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ’10, pages 481–489. Association for
Computational Linguistics.
</reference>
<page confidence="0.983806">
488
</page>
<reference confidence="0.999526166666667">
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617–625, Beijing, China, Aug.
Emmanuel Morin and B´eatrice Daille. 2006. Compara-
bilit´e de corpus et fouille terminologique multilingue.
In Traitement Automatique des Langues (TAL).
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Proceedings, 4th
Workshop on Building and Using Comparable Cor-
pora (BUCC), page 27–34, Portland, Oregon, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19–51, March.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Translation
Summit (MT Summit XII), page 284–291, Ottawa, On-
tario, Canada.
Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A word at a time: com-
puting word relatedness using temporal semantic anal-
ysis. In Proceedings of the 20th international confer-
ence on World wide web, WWW ’11, pages 337–346,
New York, NY, USA. ACM.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
ACL ’95, pages 320–322. Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, ACL ’99, pages 519–526. Asso-
ciation for Computational Linguistics.
P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for
cross-lingual and multilingual information retrieval.
Data Knowl. Eng., 74:26–45, April.
</reference>
<page confidence="0.999147">
489
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649442">
<title confidence="0.996852">Building Specialized Bilingual Lexicons Using Large-Scale Background Knowledge</title>
<author confidence="0.967631">Adrian Nasredine Pierre</author>
<affiliation confidence="0.854192">1CEA, LIST, Vision and Content Engineering Laboratory,</affiliation>
<address confidence="0.907576">CEDEX, France; F-91403 Orsay CEDEX, France;</address>
<abstract confidence="0.99895564">Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: French- English and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dhouha Bouamor</author>
<author>Nasredine Semmar</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Context vector disambiguation for bilingual lexicon extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Association for Computational Linguistics (ACLHLT),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8880" citStr="Bouamor et al., 2013" startWordPosition="1346" endWordPosition="1349">hat the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is. At first, additional linguistic resources, such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009), were combined with the seed dictionary to translate context vectors. The ambiguities that appear in the seed bilingual dictionary were taken into account more recently. (Morin and Prochasson, 2011) modify the standard approach by weighting the different translations according to their frequency in the target corpus. In (Bouamor et al., 2013), we proposed a method that adds a word sense disambiguation process relying on semantic similarity measurement from WordNet to the standard approach. Given a context vector in the source language, the most probable translation of polysemous words is identified and used for building the corresponding vector in the target language. The most probable translation is identified using the monosemic words that appear in the same lexical environment. On specialized French-English comparable corpora, this approach outperforms the one proposed in (Morin and Prochasson, 2011), which is itself better tha</context>
<context position="23552" citStr="Bouamor et al., 2013" startWordPosition="3691" endWordPosition="3694">ave an associated Wikipedia entry and (3) the translation graph covers only a fraction of the concepts available in a language. For instance, the resulting translation dictionaries have 193,543 entries for French-English and 136,681 entries for RomanianEnglish. They can be used in addition to or instead of other resources available and are especially useful when there are only few other resources that link the pair of languages processed. 4 Evaluation The performances of our approach are evaluated against the standard approach and its developments proposed by (Morin and Prochasson, 2011) and (Bouamor et al., 2013). In this section, we first describe the data and resources we used in our experiments. We then present differents parameters needed in the implementation of the different methods tested. Finally, we discuss the obtained results. 4.1 Data and Resources Comparable corpora We conducted our experiments on four FrenchEnglish and Romanian-English specialized comparable corpora: Corporate Finance, Breast Cancer, Wind Energy and Mobile Technology. For the Romanian-English language pair, we used Wikipedia to collect comparable corpora for all domains since they were not already available. The Wikipedi</context>
<context position="30986" citStr="Bouamor et al., 2013" startWordPosition="4864" endWordPosition="4867">72 ESAspee 0.81 0.56 0.86 0.75 Method F-Measure@20 Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology STAPP 0.21 0.13 0.08 0.16 MP11 0.21 0.13 0.08 0.16 BA13 0.21 0.14 0.08 0.17 Dicospee 0.44 0.11 0.21 0.16 ESA 0.76 0.17 0.58 0.53 ESAspee 0.78 0.24 0.58 0.55 Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improvement of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that we developed (Bouamor et al., 2013). Dicospee exploits a generic dictionary, combined with the use of domain specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see Figure 1). ESAspee combines the results of Dicospee and ESA. a) FR-EN b) RO-EN for RO-EN. Consequently, we will use BA13 as the main baseline for discussing the newly introduced approach. The results presented in Table 4 show that ESAspee clearly outperforms the three baselines for the four domains and the two pairs of languages tested. When comparing ESAspee to BA13 for French-English, improvements range between 0.19 fo</context>
</contexts>
<marker>Bouamor, Semmar, Zweigenbaum, 2013</marker>
<rawString>Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2013. Context vector disambiguation for bilingual lexicon extraction. In Proceedings of the 51st Association for Computational Linguistics (ACLHLT), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics - Volume 2, COLING ’02,</booktitle>
<pages>1--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6904" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1038" endWordPosition="1041">anguage that has fewer associated resources than French and English. Experimental results show that the newly introduced approach outperforms the three state of the art methods that were implemented for comparison. 2 Related Work In this section, we first give a review of the standard approach and then introduce methods that build upon it. Finally, we discuss works that rely on Explicit Semantic Analysis to solve other NLP tasks. 2.1 Standard Approach (SA) Most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). This approach is composed of three main steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated Wand in a window of n words. Generally, association measures such as the mutual information (Morin and Daille, 2006), the loglikelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source vectors are translated intoto th</context>
<context position="8485" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1286" endWordPosition="1289">vectors of all possible translations from the target language. Most often, the cosine similarity is used to rank translation candidates but alternative metrics, including the weighted Jaccard index (Prochasson et al., 2009) and the city-block distance (Rapp, 1999), were studied. 2.2 Improvements of the Standard Approach Most of the improvements of the standard approach are based on the observation that the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is. At first, additional linguistic resources, such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009), were combined with the seed dictionary to translate context vectors. The ambiguities that appear in the seed bilingual dictionary were taken into account more recently. (Morin and Prochasson, 2011) modify the standard approach by weighting the different translations according to their frequency in the target corpus. In (Bouamor et al., 2013), we proposed a method that adds a word sense disambiguation process relying on semantic similarity measurement from WordNet to the standard approach. Given a context vector in the source language, the mos</context>
<context position="26607" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="4149" endWordPosition="4152"> Romanian4http://www.ttc-project.eu/index.php/ releases-publications 5Comparable corpora will be shared publicly 485 Domain FR-EN RO-EN Corporate Finance 125 69 Breast Cancer 96 38 Wind Energy 89 38 Mobile Technology 142 94 Table 3: Sizes of the evaluation lists. English, we used the generic dictionary extracted following the procedure described in Subsection 3.6. Gold standard In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are usually composed of around 100 single terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Reference lists6 were created for the four specialized domains and the two pairs of languages. For the French-English, reference words from the Corporate Finance domain were extracted from the glossary of bilingual micro-finance terms7. For Breast Cancer, the list is derived from the MESH and the UMLS thesauri8. Concerning Wind Energy and Mobile Technology, lists were extracted from specialized glossaries found on the Web. The RomanianEnglish gold standard was manually created by a native speaker starting from the French-English lists. Table 3 displays the sizes of the obtained lists. Refere</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th international conference on Computational linguistics - Volume 2, COLING ’02, pages 1–5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>The effect of a general lexicon in corpus-based identification of french-english medical word translations.</title>
<date>2003</date>
<booktitle>In Proceedings Medical Informatics Europe, volume 95 of Studies in Health Technology and Informatics,</booktitle>
<pages>397--402</pages>
<location>Amsterdam.</location>
<contexts>
<context position="2908" citStr="Chiao and Zweigenbaum, 2003" startWordPosition="427" endWordPosition="430">y an appropriate exploitation of multilingual resources that are increasingly available on the Web. In this paper we focus on the automatic creation of domain-specific bilingual lexicons. Such resources play a vital role in Natural Language Processing (NLP) applications that involve different languages. At first, research on lexical extraction has relied on the use of parallel corpora (Och and Ney, 2003). The scarcity of such corpora, in particular for specialized domains and for language pairs not involving English, pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis (Harris, 1954) which puts that words with similar meanings occur in similar contexts. In a multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar lexical environments across languages (Rapp, 1995). The standard approach to bilingual lexicon extraction builds 1http://translate.google.com/ 2http://www</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2003</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The effect of a general lexicon in corpus-based identification of french-english medical word translations. In Proceedings Medical Informatics Europe, volume 95 of Studies in Health Technology and Informatics, pages 397–402, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora.</title>
<date>1998</date>
<booktitle>In Parallel Text Processing,</booktitle>
<pages>1--17</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2878" citStr="Fung, 1998" startWordPosition="425" endWordPosition="426"> addressed by an appropriate exploitation of multilingual resources that are increasingly available on the Web. In this paper we focus on the automatic creation of domain-specific bilingual lexicons. Such resources play a vital role in Natural Language Processing (NLP) applications that involve different languages. At first, research on lexical extraction has relied on the use of parallel corpora (Och and Ney, 2003). The scarcity of such corpora, in particular for specialized domains and for language pairs not involving English, pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis (Harris, 1954) which puts that words with similar meanings occur in similar contexts. In a multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar lexical environments across languages (Rapp, 1995). The standard approach to bilingual lexicon extraction builds 1http://tra</context>
<context position="6875" citStr="Fung, 1998" startWordPosition="1036" endWordPosition="1037">omanian, a language that has fewer associated resources than French and English. Experimental results show that the newly introduced approach outperforms the three state of the art methods that were implemented for comparison. 2 Related Work In this section, we first give a review of the standard approach and then introduce methods that build upon it. Finally, we discuss works that rely on Explicit Semantic Analysis to solve other NLP tasks. 2.1 Standard Approach (SA) Most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). This approach is composed of three main steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated Wand in a window of n words. Generally, association measures such as the mutual information (Morin and Daille, 2006), the loglikelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source vec</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In Parallel Text Processing, pages 1–17. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5606" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="832" endWordPosition="835">criptions in different languages is highly variable. • The translation graph is partial since, when considering any language pair, only a part of the concepts are available in both languages and explicitly connected. • Domains are unequally covered in Wikipedia (Halavais and Lackaff, 2008) and efficient domain targeting is needed. The approach introduced in this paper aims to draw on Wikipedia’s advantages while appropriately addressing associated challenges. Among the techniques devised to mine Wikipedia content, we hypothesize that an adequate adaptation of Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) is fitted to our application context. ESA was already successfully tested in different NLP tasks, such as word relatedness estimation or text classification, and we modify it to mine specialized domains, to characterize these domains and to link them across languages. The evaluation of the newly introduced approach is realized on four diversified specialized domains (Breast Cancer, Corporate Finance, Wind Energy and Mobile Technology) and for two pairs of languages: French-English and Romanian-English. This choice allows us to study the behavior of different approaches for a pair of languages</context>
<context position="9859" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1497" endWordPosition="1500">able translation is identified using the monosemic words that appear in the same lexical environment. On specialized French-English comparable corpora, this approach outperforms the one proposed in (Morin and Prochasson, 2011), which is itself better than the standard approach. The main weakness of (Bouamor et al., 2013) is that the approach relies on WordNet and its application depends on the existence of this resource in the target language. Also, the method is highly dependent on the coverage of the seed bilingual dictionary. 2.3 Explicit Semantic Analysis Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) is a method that maps textual documents onto a structured semantic space using classical text indexing schemes such as TF-IDF. Examples of semantic spaces used include Wikipedia or the Open Directory Project but, due to superior performances, Wikipedia is most frequently used. In the original evaluation, ESA outperformed state of the art methods in a word relatedness estimation task. Subsequently, ESA was successfully exploited in other NLP tasks and in information retrieval. Radinsky and al. (2011) added a temporal dimension to word vectors and showed that this addition improves the results </context>
<context position="12063" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1840" endWordPosition="1843">tation of the method to specific domains and (3) the coverage of the underlying resource in different language. 3 ESA for Bilingual Lexicon Extraction The main objective of our approach is to devise lexicon translation methods that are easily applicable to a large number of language pairs, while preserving the overall quality of results. A subordinated objective is to exploit large scale background multilingual knowledge, such as the encyclopedic content available in Wikipedia. As we mentioned, ESA 481 Figure 1: Overview of the Explicit Semantic Analysis enabled bilingual lexicon extraction. (Gabrilovich and Markovitch, 2007) was exploited in a number of NLP tasks but not in bilingual lexicon extraction. Figure 1 shows the overall architecture of the lexical extraction process we propose. The process is completed in the following three steps: 1. Given a word to be translated and its context vector in the source language, we derive a ranked list of similar Wikipedia concepts (i.e. articles) using the ESA inverted index. 2. Then, a translation graph is used to retrieve the corresponding concepts in the target language. 3. Candidate translations are found through a statistical processing of concept descriptions from </context>
<context position="13640" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2107" endWordPosition="2110">main word specificity and present a method for extracting generic translation lexicons. 3.1 ESA Word and Concept Representation Given a semantic space structured using a set of M concepts and including a dictionary of N words, a mapping between words and concepts can be expressed as the following matrix: w(W1, C1) w(W2, C1) ... w(WN, C1) w(W1, C2) w(W2, C2) ... w(WN, C2) ... ... ... w(W1, CM) w(W2, CM) ... w(WN, CM) When Wikipedia is exploited concepts are equated to Wikipedia articles and the texts of the articles are processed in order to obtain the weights that link words and concepts. In (Gabrilovich and Markovitch, 2007), the weights w that link words and concepts were obtained through a classical TFIDF weighting of Wikipedia articles. A series of tweaks destined to improve the method’s performance were used and disclosed later3. For instance, administration articles, lists, articles that are too short or have too few links are discarded. Higher weight is given to words in the article title and more longer articles are favored over shorter ones. We implemented a part of these tweaks and tested our own version of ESA with the Wikipedia version used in the original implementation. The correlation with human jud</context>
<context position="19467" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3023" endWordPosition="3026">d from source language. These candidate translations Wt are ranked using equation 2. w(Wt, Cti) avg(Cti) ) * log(count(Wt, S)) (2) with w(Wt, Cti) is the weight of the translation candidate WT for concept Cti from the ESA matrix in the target language; avg(Cti) is the average TFIDF score of words that appear in Cti; S is the set of similar concepts Cti in the target language and count(Wt, S) accounts for the number of different concepts from S in which the candidate translation WT appears. The accumulation of weights w(Wt, Cti) follows the way original ESA text representations are calculated (Gabrilovich and Markovitch, 2007) and avg(Cti) is used in order to correct the bias of the TF-IDF scheme towards short articles. log(count(Wt, S)) is used to favor words that are associated with a larger number of concepts. log weighting was chosen after preliminary experiments with a wide range of functions. 3.5 Domain Specificity In previous works, ESA was usually exploited in generic tasks that did not require any domain adaptation. Here we process information from specific domains and we need to measure the specificity of words in those domains. The domain extraction is seeded by using Wikipedia concepts (noted Cseed) tha</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Halavais</author>
<author>Derek Lackaff</author>
</authors>
<title>An Analysis of Topical Coverage of Wikipedia.</title>
<date>2008</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="5263" citStr="Halavais and Lackaff, 2008" startWordPosition="783" endWordPosition="786">xplicitly linked through concept translations proposed by Wikipedia contributors. • It covers a large number of domains and is thus potentially useful in order to mine a wide array of specialized lexicons. Mirroring the advantages, there are a number of challenges associated with the use of Wikipedia: • The comparability of concept descriptions in different languages is highly variable. • The translation graph is partial since, when considering any language pair, only a part of the concepts are available in both languages and explicitly connected. • Domains are unequally covered in Wikipedia (Halavais and Lackaff, 2008) and efficient domain targeting is needed. The approach introduced in this paper aims to draw on Wikipedia’s advantages while appropriately addressing associated challenges. Among the techniques devised to mine Wikipedia content, we hypothesize that an adequate adaptation of Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) is fitted to our application context. ESA was already successfully tested in different NLP tasks, such as word relatedness estimation or text classification, and we modify it to mine specialized domains, to characterize these domains and to link them acros</context>
</contexts>
<marker>Halavais, Lackaff, 2008</marker>
<rawString>Alexander Halavais and Derek Lackaff. 2008. An Analysis of Topical Coverage of Wikipedia. Journal of Computer-Mediated Communication, 13(2):429–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Distributional structure. Word. Samer Hassan and Rada Mihalcea.</title>
<date>1954</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="3164" citStr="Harris, 1954" startWordPosition="468" endWordPosition="469">that involve different languages. At first, research on lexical extraction has relied on the use of parallel corpora (Och and Ney, 2003). The scarcity of such corpora, in particular for specialized domains and for language pairs not involving English, pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis (Harris, 1954) which puts that words with similar meanings occur in similar contexts. In a multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar lexical environments across languages (Rapp, 1995). The standard approach to bilingual lexicon extraction builds 1http://translate.google.com/ 2http://www.systransoft.com/ 479 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479–489, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics on the distributional hypothesis and</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z.S. Harris. 1954. Distributional structure. Word. Samer Hassan and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Hazem</author>
<author>Emmanuel Morin</author>
</authors>
<title>Adaptive dictionary for bilingual lexicon extraction from comparable corpora.</title>
<date>2012</date>
<booktitle>In Proceedings, 8th international conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="26577" citStr="Hazem and Morin, 2012" startWordPosition="4145" endWordPosition="4148">ly 120,000 entries. For Romanian4http://www.ttc-project.eu/index.php/ releases-publications 5Comparable corpora will be shared publicly 485 Domain FR-EN RO-EN Corporate Finance 125 69 Breast Cancer 96 38 Wind Energy 89 38 Mobile Technology 142 94 Table 3: Sizes of the evaluation lists. English, we used the generic dictionary extracted following the procedure described in Subsection 3.6. Gold standard In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are usually composed of around 100 single terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Reference lists6 were created for the four specialized domains and the two pairs of languages. For the French-English, reference words from the Corporate Finance domain were extracted from the glossary of bilingual micro-finance terms7. For Breast Cancer, the list is derived from the MESH and the UMLS thesauri8. Concerning Wind Energy and Mobile Technology, lists were extracted from specialized glossaries found on the Web. The RomanianEnglish gold standard was manually created by a native speaker starting from the French-English lists. Table 3 displays the sizes</context>
</contexts>
<marker>Hazem, Morin, 2012</marker>
<rawString>Amir Hazem and Emmanuel Morin. 2012. Adaptive dictionary for bilingual lexicon extraction from comparable corpora. In Proceedings, 8th international conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azniah Ismail</author>
<author>Suresh Manandhar</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using in-domain terms.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>481--489</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4091" citStr="Ismail and Manandhar, 2010" startWordPosition="601" endWordPosition="604">ds 1http://translate.google.com/ 2http://www.systransoft.com/ 479 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479–489, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics on the distributional hypothesis and compares context vectors for each word of the source and target languages. In this approach, the comparison of context vectors is conditioned by the existence of a seed bilingual dictionary. A weakness of the method is that poor results are obtained for language pairs that are not closely related (Ismail and Manandhar, 2010). Another important problem occurs whenever the size of the seed dictionary is small due to ignoring many context words. Conversely, when dictionaries are detailed, ambiguity becomes an important drawback. We introduce a bilingual lexicon extraction approach that exploits Wikipedia in an innovative manner in order to tackle some of the problems mentioned above. Important advantages of using Wikipedia are: • The resource is available in hundreds of languages and it is structured as unambiguous concepts (i.e. articles). • The languages are explicitly linked through concept translations proposed </context>
</contexts>
<marker>Ismail, Manandhar, 2010</marker>
<rawString>Azniah Ismail and Suresh Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using in-domain terms. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 481–489. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for term-translation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>617--625</pages>
<location>Beijing, China,</location>
<contexts>
<context position="6933" citStr="Laroche and Langlais, 2010" startWordPosition="1042" endWordPosition="1045">ated resources than French and English. Experimental results show that the newly introduced approach outperforms the three state of the art methods that were implemented for comparison. 2 Related Work In this section, we first give a review of the standard approach and then introduce methods that build upon it. Finally, we discuss works that rely on Explicit Semantic Analysis to solve other NLP tasks. 2.1 Standard Approach (SA) Most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). This approach is composed of three main steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated Wand in a window of n words. Generally, association measures such as the mutual information (Morin and Daille, 2006), the loglikelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source vectors are translated intoto the target language by using a </context>
<context position="28001" citStr="Laroche and Langlais (2010)" startWordPosition="4363" endWordPosition="4366">y mentioned, three parameters need to be set up: (1) the window size that defines contexts, (2) the association measure that measures the strength of the association between words and the (3) similarity measure that ranks candidate translations for state of the art methods. Context vectors are defined using a seven-word window which approximates syntactic dependencies. The association and the similarity measures (Discounted Log-Odds ratio (equation 5) and the cosine simi6Reference lists will be shared publicly 7http://www.microfinance.lu/en/ 8http://www.nlm.nih.gov/ larity) were set following Laroche and Langlais (2010), a comprehensive study of the influence of these parameters on the bilingual alignment. 12)(O22 + 12) Odds-Ratiodzsc = log (O11 + 2) (5) (O12 + 1 2)(O21 + 1 where Oil are the cells of the 2 x 2 contingency matrix of a token s co-occurring with the term 5 within a given window size. The F-measure of the Top 20 results (FMeasure@20), which measures the harmonic mean of precision and recall, is used as evaluation metric. Precision is the total number of correct translations divided by the number of terms for which the system returned at least one answer. Recall is equal to the ratio between the </context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for term-translation spotting in comparable corpora. In 23rd International Conference on Computational Linguistics (Coling 2010), pages 617–625, Beijing, China, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
</authors>
<title>Comparabilit´e de corpus et fouille terminologique multilingue.</title>
<date>2006</date>
<booktitle>In Traitement Automatique des Langues (TAL).</booktitle>
<contexts>
<context position="7225" citStr="Morin and Daille, 2006" startWordPosition="1092" endWordPosition="1095">t build upon it. Finally, we discuss works that rely on Explicit Semantic Analysis to solve other NLP tasks. 2.1 Standard Approach (SA) Most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). This approach is composed of three main steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated Wand in a window of n words. Generally, association measures such as the mutual information (Morin and Daille, 2006), the loglikelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source vectors are translated intoto the target language by using a seed bilingual dictionary. Whenever several translations of a context word exist, 480 all translation variants are taken into account. Words not included in the seed dictionary are simply ignored. 3. Comparison of source and target vectors: Given Wand, its automatically translated context ve</context>
</contexts>
<marker>Morin, Daille, 2006</marker>
<rawString>Emmanuel Morin and B´eatrice Daille. 2006. Comparabilit´e de corpus et fouille terminologique multilingue. In Traitement Automatique des Langues (TAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Emmanuel Prochasson</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings, 4th Workshop on Building and Using Comparable Corpora (BUCC),</booktitle>
<pages>27--34</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="7273" citStr="Morin and Prochasson, 2011" startWordPosition="1099" endWordPosition="1102">hat rely on Explicit Semantic Analysis to solve other NLP tasks. 2.1 Standard Approach (SA) Most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach (Fung, 1998; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). This approach is composed of three main steps: 1. Building context vectors: Vectors are first extracted by identifying the words that appear around the term to be translated Wand in a window of n words. Generally, association measures such as the mutual information (Morin and Daille, 2006), the loglikelihood (Morin and Prochasson, 2011) or the Discounted Odds-Ratio (Laroche and Langlais, 2010) are employed to shape the context vectors. 2. Translation of context vectors: To enable the comparison of source and target vectors, source vectors are translated intoto the target language by using a seed bilingual dictionary. Whenever several translations of a context word exist, 480 all translation variants are taken into account. Words not included in the seed dictionary are simply ignored. 3. Comparison of source and target vectors: Given Wand, its automatically translated context vector is compared to the context vectors of all p</context>
<context position="8734" citStr="Morin and Prochasson, 2011" startWordPosition="1323" endWordPosition="1326">e (Rapp, 1999), were studied. 2.2 Improvements of the Standard Approach Most of the improvements of the standard approach are based on the observation that the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is. At first, additional linguistic resources, such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009), were combined with the seed dictionary to translate context vectors. The ambiguities that appear in the seed bilingual dictionary were taken into account more recently. (Morin and Prochasson, 2011) modify the standard approach by weighting the different translations according to their frequency in the target corpus. In (Bouamor et al., 2013), we proposed a method that adds a word sense disambiguation process relying on semantic similarity measurement from WordNet to the standard approach. Given a context vector in the source language, the most probable translation of polysemous words is identified and used for building the corresponding vector in the target language. The most probable translation is identified using the monosemic words that appear in the same lexical environment. On spe</context>
<context position="23525" citStr="Morin and Prochasson, 2011" startWordPosition="3686" endWordPosition="3689">lized domain terms often do not have an associated Wikipedia entry and (3) the translation graph covers only a fraction of the concepts available in a language. For instance, the resulting translation dictionaries have 193,543 entries for French-English and 136,681 entries for RomanianEnglish. They can be used in addition to or instead of other resources available and are especially useful when there are only few other resources that link the pair of languages processed. 4 Evaluation The performances of our approach are evaluated against the standard approach and its developments proposed by (Morin and Prochasson, 2011) and (Bouamor et al., 2013). In this section, we first describe the data and resources we used in our experiments. We then present differents parameters needed in the implementation of the different methods tested. Finally, we discuss the obtained results. 4.1 Data and Resources Comparable corpora We conducted our experiments on four FrenchEnglish and Romanian-English specialized comparable corpora: Corporate Finance, Breast Cancer, Wind Energy and Mobile Technology. For the Romanian-English language pair, we used Wikipedia to collect comparable corpora for all domains since they were not alre</context>
<context position="30920" citStr="Morin and Prochasson, 2011" startWordPosition="4852" endWordPosition="4855">3 0.61 0.37 0.30 0.24 Dicospee 0.50 0.20 0.36 0.25 ESA 0.74 0.50 0.83 0.72 ESAspee 0.81 0.56 0.86 0.75 Method F-Measure@20 Breast Cancer Corporate Finance Wind Eenrgy Mobile Technology STAPP 0.21 0.13 0.08 0.16 MP11 0.21 0.13 0.08 0.16 BA13 0.21 0.14 0.08 0.17 Dicospee 0.44 0.11 0.21 0.16 ESA 0.76 0.17 0.58 0.53 ESAspee 0.78 0.24 0.58 0.55 Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improvement of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that we developed (Bouamor et al., 2013). Dicospee exploits a generic dictionary, combined with the use of domain specificity (see Subsection 3.5). ESA stands for the ESA based approach introduced in this paper (see Figure 1). ESAspee combines the results of Dicospee and ESA. a) FR-EN b) RO-EN for RO-EN. Consequently, we will use BA13 as the main baseline for discussing the newly introduced approach. The results presented in Table 4 show that ESAspee clearly outperforms the three baselines for the four domains and the two pairs of languages tested. When comparing ESAs</context>
</contexts>
<marker>Morin, Prochasson, 2011</marker>
<rawString>Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. In Proceedings, 4th Workshop on Building and Using Comparable Corpora (BUCC), page 27–34, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2687" citStr="Och and Ney, 2003" startWordPosition="393" endWordPosition="396">s and the quality of automatically obtained resources and translations. While the first challenge is general and inherent to any automatic approach, the second and the third can be at least partially addressed by an appropriate exploitation of multilingual resources that are increasingly available on the Web. In this paper we focus on the automatic creation of domain-specific bilingual lexicons. Such resources play a vital role in Natural Language Processing (NLP) applications that involve different languages. At first, research on lexical extraction has relied on the use of parallel corpora (Och and Ney, 2003). The scarcity of such corpora, in particular for specialized domains and for language pairs not involving English, pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis (Harris, 1954) which puts that words with similar meanings occur in similar contexts. In a multilingual formulation, this hypothesis stat</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Emmanuel Morin</author>
<author>Kyo Kageura</author>
</authors>
<title>Anchor points for bilingual lexicon extraction from small comparable corpora.</title>
<date>2009</date>
<booktitle>In Proceedings, 12th Conference on Machine Translation Summit (MT Summit XII),</booktitle>
<pages>284--291</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="8080" citStr="Prochasson et al., 2009" startWordPosition="1225" endWordPosition="1228">vectors, source vectors are translated intoto the target language by using a seed bilingual dictionary. Whenever several translations of a context word exist, 480 all translation variants are taken into account. Words not included in the seed dictionary are simply ignored. 3. Comparison of source and target vectors: Given Wand, its automatically translated context vector is compared to the context vectors of all possible translations from the target language. Most often, the cosine similarity is used to rank translation candidates but alternative metrics, including the weighted Jaccard index (Prochasson et al., 2009) and the city-block distance (Rapp, 1999), were studied. 2.2 Improvements of the Standard Approach Most of the improvements of the standard approach are based on the observation that the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is. At first, additional linguistic resources, such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009), were combined with the seed dictionary to translate context vectors. The ambiguities that appear in the seed bilingual dictionary were taken in</context>
</contexts>
<marker>Prochasson, Morin, Kageura, 2009</marker>
<rawString>Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Proceedings, 12th Conference on Machine Translation Summit (MT Summit XII), page 284–291, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web, WWW ’11,</booktitle>
<pages>337--346</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In Proceedings of the 20th international conference on World wide web, WWW ’11, pages 337–346, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95,</booktitle>
<pages>320--322</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3404" citStr="Rapp, 1995" startWordPosition="506" endWordPosition="507"> English, pushed researchers to investigate the use of comparable corpora (Fung, 1998; Chiao and Zweigenbaum, 2003). These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc. The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis (Harris, 1954) which puts that words with similar meanings occur in similar contexts. In a multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar lexical environments across languages (Rapp, 1995). The standard approach to bilingual lexicon extraction builds 1http://translate.google.com/ 2http://www.systransoft.com/ 479 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479–489, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics on the distributional hypothesis and compares context vectors for each word of the source and target languages. In this approach, the comparison of context vectors is conditioned by the existence of a seed bilingual dictionary. A weakness of the method is that poor results ar</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95, pages 320–322. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated english and german corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>519--526</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8121" citStr="Rapp, 1999" startWordPosition="1233" endWordPosition="1234">et language by using a seed bilingual dictionary. Whenever several translations of a context word exist, 480 all translation variants are taken into account. Words not included in the seed dictionary are simply ignored. 3. Comparison of source and target vectors: Given Wand, its automatically translated context vector is compared to the context vectors of all possible translations from the target language. Most often, the cosine similarity is used to rank translation candidates but alternative metrics, including the weighted Jaccard index (Prochasson et al., 2009) and the city-block distance (Rapp, 1999), were studied. 2.2 Improvements of the Standard Approach Most of the improvements of the standard approach are based on the observation that the more representative the context vectors of a candidate word are, the better the bilingual lexicon extraction is. At first, additional linguistic resources, such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009), were combined with the seed dictionary to translate context vectors. The ambiguities that appear in the seed bilingual dictionary were taken into account more recently. (Morin and Proc</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 519–526. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sorg</author>
<author>P Cimiano</author>
</authors>
<title>Exploiting wikipedia for cross-lingual and multilingual information retrieval.</title>
<date>2012</date>
<journal>Data Knowl. Eng.,</journal>
<volume>74</volume>
<contexts>
<context position="11017" citStr="Sorg and Cimiano, 2012" startWordPosition="1676" endWordPosition="1679"> word vectors and showed that this addition improves the results of word relatedness estimation. (Hassan and Mihalcea, 2011) introduced Salient Semantic Analysis (SSA), a development of ESA that relies on the detection of salient concepts prior to mapping words to concepts. SSA and the original ESA implementation were tested on several word relatedness datasets and results were mixed. Improvements were obtained for text classification when comparing SSA with the authors’ in-house representation of the method. ESA has weak language dependence and was already deployed in multilingual contexts. (Sorg and Cimiano, 2012) extended ESA to other languages and showed that it is useful in crosslingual and multilingual retrieval task. Their focus was on creating a language independent conceptual space in which documents would be mapped and then retrieved. Some open ESA topics related to bilingual lexicon creation include: (1) the document representation which is simply done by summing individual contributions of words, (2) the adaptation of the method to specific domains and (3) the coverage of the underlying resource in different language. 3 ESA for Bilingual Lexicon Extraction The main objective of our approach i</context>
</contexts>
<marker>Sorg, Cimiano, 2012</marker>
<rawString>P. Sorg and P. Cimiano. 2012. Exploiting wikipedia for cross-lingual and multilingual information retrieval. Data Knowl. Eng., 74:26–45, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
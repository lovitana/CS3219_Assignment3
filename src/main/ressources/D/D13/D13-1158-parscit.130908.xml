<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.900293">
Single-Document Summarization as a Tree Knapsack Problem
</title>
<author confidence="0.497389">
Tsutomu Hirao† Yasuhisa Yoshida† Masaaki Nishino† Norihito Yasudal Masaaki Nagata††NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
</author>
<affiliation confidence="0.310788">
{hirao.tsutomu,yoshida.y,nishino.masaaki,
</affiliation>
<email confidence="0.968713">
nagata.masaaki}@lab.ntt.co.jp
</email>
<note confidence="0.9232235">
$ Japan Science and Technology Agency
North 14 West 9, Sapporo, Hokkaido, 060-0814, Japan
</note>
<email confidence="0.995928">
yasuda@erato.ist.hokudai.ac.jp
</email>
<sectionHeader confidence="0.99663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999758043478261">
Recent studies on extractive text summariza-
tion formulate it as a combinatorial optimiza-
tion problem such as a Knapsack Problem, a
Maximum Coverage Problem or a Budgeted
Median Problem. These methods successfully
improved summarization quality, but they did
not consider the rhetorical relations between
the textual units of a source document. Thus,
summaries generated by these methods may
lack logical coherence. This paper proposes a
single document summarization method based
on the trimming of a discourse tree. This is
a two-fold process. First, we propose rules
for transforming a rhetorical structure theory-
based discourse tree into a dependency-based
discourse tree, which allows us to take a tree-
trimming approach to summarization. Sec-
ond, we formulate the problem of trimming
a dependency-based discourse tree as a Tree
Knapsack Problem, then solve it with integer
linear programming (ILP). Evaluation results
showed that our method improved ROUGE
scores.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982297581395349">
State-of-the-art extractive text summarization meth-
ods regard a document (or a document set) as a set
of textual units (e.g. sentences, clauses, phrases)
and formulate summarization as a combinatorial op-
timization problem, i.e. selecting a subset of the set
of textual units that maximizes an objective with-
out violating a length constraint. For example, Mc-
Donald (2007) formulated text summarization as a
Knapsack Problem, where he selects a set of textual
units that maximize the sum of significance scores
of each unit. Filatova et al. (2004) proposed a
summarization method based on a Maximum Cov-
erage Problem, in which they select a set of textual
units that maximizes the weighted sum of the con-
ceptual units (e.g. unigrams) contained in the set.
Although, their greedy solution is only an approxi-
mation, Takamura et al. (2009a) extended it to ob-
tain the exact solution. More recently, Takamura et
al. (2009b) regarded summarization as a Budgeted
Median Problem and obtain exact solutions with in-
teger linear programming.
These methods successfully improved ROUGE
(Lin, 2004) scores, but they still have one critical
shortcoming. Since these methods are based on sub-
set selection, the summaries they generate cannot
preserve the rhetorical structure of the textual units
of a source document. Thus, the resulting summary
may lack coherence and may not include significant
textual units from a source document.
One powerful and potential way to overcome the
problem is to include discourse tree constraints in
the summarization procedure. Marcu (1998) re-
garded a document as a Rhetorical Structure The-
ory (RST) (William Charles, Mann and Sandra An-
near, Thompson, 1988)-based discourse tree (RST-
DT) and selected textual units according to a prefer-
ence ranking derived from the tree structure to make
a summary. Daume et al. (2002) proposed a docu-
ment compression method that directly models the
probability of a summary given an RST-DT by us-
ing a noisy-channel model. These methods generate
well-organized summaries, however, since they do
not formulate summarizations as combinatorial op-
</bodyText>
<page confidence="0.9089">
1515
</page>
<note confidence="0.521509">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1515–1520,
</note>
<page confidence="0.270841">
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</page>
<figure confidence="0.999481838709677">
0 Root
12S
N
2 5 13 16
N S N S
1
3 4 6 7
S N N
14 15 17 18
S S N S N
8
9
N N
10
11
N S
Root
N S N S
Elaboration
Example
N S
Elaboration
Elaboration
N N S S N S
Contrast Contrast
N
Evidence
N S
Background
Elaboration
Concession Antithesis
</figure>
<figureCaption confidence="0.99997">
Figure 1: Example RST-DT from (Marcu, 1998). Figure 2: Heads of non-terminal nodes.
</figureCaption>
<bodyText confidence="0.998232318181818">
timization problems, the optimality of the generated
summaries is not guaranteed.
In this paper, we propose a single document sum-
marization method based on the trimming of a dis-
course tree based on the Tree Knapsack Problem. If
a discourse tree explicitly represents parent-child re-
lationships between textual units, we can apply the
well-known tree-trimming approach to a discourse
tree and reap the benefit of combinatorial optimiza-
tion methods. In other words, to apply the tree-
trimming approach, we need a tree whose all nodes
represent textual units. Unfortunately, the RST-DT
does not allow it, because textual units in the RST-
DT are located only on leaf nodes and parent-child
relationship between textual units are represented
implicitly at higher positions in a tree. Therefore, we
first propose rules that transform an RST-DT into a
dependency-based discourse tree (DEP-DT) that ex-
plicitly defines the parent-child relationships. Sec-
ond, we treat it as a rooted subtree selection, in other
words, a Tree Knapsack Problem and formulate the
problem as an ILP.
</bodyText>
<sectionHeader confidence="0.912631" genericHeader="method">
2 From RST-DT to DEP-DT
</sectionHeader>
<subsectionHeader confidence="0.969923">
2.1 RST-DT
</subsectionHeader>
<bodyText confidence="0.99986025">
According to RST, a document is represented as an
RST-DT whose terminal nodes correspond to ele-
mentary discourse units (EDU)sl and whose non-
terminal nodes indicate the role of the contiguous
</bodyText>
<footnote confidence="0.938374">
1EDUs roughly correspond to clauses.
</footnote>
<bodyText confidence="0.999832142857143">
EDUs namely, ‘nucleus (N)’ or ‘satellite (S)’. A nu-
cleus is more important than a satellite in terms of
the writer’s purpose. That is, a satellite is a child of
a nucleus in the RST-DT. Some discourse relations
such as ‘Elaboration’, ‘Contrast’ and ‘Evidence’ be-
tween a nucleus and a satellite or two nuclei are de-
fined. Figure 1 shows an example of an RST-DT.
</bodyText>
<subsectionHeader confidence="0.998089">
2.2 DEP-DT
</subsectionHeader>
<bodyText confidence="0.99995180952381">
An RST-DT is not suitable for tree trimming because
it does not always explicitly define parent-child re-
lationships between textual units. For example, if
we consider how to trim the RST-DT in Figure 1,
when we drop e8, we have to drop e7 because of the
parent-child relationship defined between e7 and e8,
i.e. e7 is a satellite (child) of the nucleus (parent)
e8. On the other hand, we cannot judge whether we
have to drop e9 or e10 because the parent-child rela-
tionships are not explicitly defined between e8 and
e9, e8 and e10. This view motivates us to produce a
discourse tree that explicitly defines parent-child re-
lationships and whose root node represents the most
important EDU in a source document. If we can ob-
tain such a tree, it is easy to formulate summariza-
tion as a Tree Knapsack Problem.
To construct a discourse tree that represents
the parent-child relationships between EDUs, we
propose rules for transforming an RST-DT to a
dependency-based discourse tree (DEP-DT). The
procedure is defined as follows:
</bodyText>
<listItem confidence="0.668854">
1. For each non-terminal node excluding the par-
</listItem>
<page confidence="0.938708">
1516
</page>
<table confidence="0.987148125">
Depth=1 3 Tree Knapsack Model for
Depth=2 Single-Document Summarization
Depth=3 3.1 Formalization
Depth=4 We denote T as a set of all possible rooted subtrees
obtained from a DEP-DT. F(t) is the significance
score for a rooted subtree t E T and L is the maxi-
mum number of words allowed in a summary. The
optimal subtree t* is defined as follows:
</table>
<figureCaption confidence="0.905628">
Figure 3: The DEP-DT obtained from the RST-DT in Fig- t* = arg max F(t) (1)
ure 1. tET
</figureCaption>
<bodyText confidence="0.7877956">
ent of an EDU in the RST-DT, we define a
‘head’. Here, a ‘head’ of a non-terminal node
is the leftmost descendant EDU whose parent is
N. In Figure 2, ‘H’ indicates the ‘head’ of each
node.
</bodyText>
<listItem confidence="0.980549642857143">
2. For each EDU whose parent is N, we pick the
nearest S with a ‘head’ from the EDU’s ances-
tors and we add the EDU to the DEP-DT as a
child of the head of the S’s parent. If there is no
nearest S, the EDU is the root of the DEP-DT.
For example, in Figure 2, the nearest S to e3
that has a head is node 5 and the head of node
5’s parent is e2. Thus, e3 is a child of e2.
3. For each EDU whose parent is S, we pick the
nearest non-terminal with a ‘head’ from the an-
cestors and we add the EDU to the DEP-DT as
a child of the head of the non-terminal node.
For example, the nearest non-terminal node of
e9 that has a head is node 16 and the head of
</listItem>
<equation confidence="0.8146522">
node 16 is e10. Thus, e9 is a child of e10.
s.t. Length(t) &lt;_ L. (2)
Here, we define F(t) as
W(e)(3)
Depth(e).
</equation>
<bodyText confidence="0.99077275">
E(t) is the set of EDUs contained in t, Depth(e)
is the depth of an EDU e within the DEP-DT. For
example, Depth(e2) = 1, Depth(e6) = 4 for the
DEP-DT of Figure 3. W(e) is defined as follows:
</bodyText>
<equation confidence="0.99826">
W(e) = E tf(w, D). (4)
wEW(e)
</equation>
<bodyText confidence="0.854545666666667">
W(e) is the set of words contained in e and
tf(w, D) is the term frequency of word w in a docu-
ment D.
</bodyText>
<subsectionHeader confidence="0.983748">
3.2 ILP Formulation
</subsectionHeader>
<bodyText confidence="0.998404666666667">
We formulate the optimization problem in the pre-
vious section as a Tree Knapsack Problem, which is
a kind of Precedence-Constrained Knapsack Prob-
lem (Samphaiboon and Yamada, 2000) and we can
obtain the optimal rooted subtree by solving the fol-
lowing ILP problem2:
</bodyText>
<equation confidence="0.997324444444444">
E
F(t) =
eEE(t)
maximize
X
Depth(ei)xi (5)
EN
i=1
W(ei)
</equation>
<bodyText confidence="0.999911222222222">
Figure 3 shows the DEP-DT obtained from the
RST-DT in Figure 1. The DEP-DT expresses the
parent-child relationship between the EDUs. There-
fore, we have to drop e7, e9 and e10 when we drop
e8. Note that, by applying the rules, discourse rela-
tions defined between non-terminals of an RST-DT
are eliminated. However, we believe that these re-
lations are no needed for the summarization that we
are attempting to realize.
</bodyText>
<equation confidence="0.9995015">
s.t. EN ℓixi &lt;_ L (6)
i=1
Vi : xparent(i) �! xi (7)
Vi : xi E t0, 11, (8)
</equation>
<footnote confidence="0.633214">
2A similar approach has been applied to sentence compres-
sion (Filippova and Strube, 2008).
</footnote>
<page confidence="0.978074">
1517
</page>
<table confidence="0.999159666666667">
ROUGE-1 ROUGE-2
F R F R
TKP(G) .310H,K,L .321G,H,K,L .108 .112H
TKP(H) .281H .284H .092 .093
Marcu(G) .291H .272H .101 .093
Marcu(H) .236 .219 .073 .068
MCP .279 .295H .073 .077
KP .251 .266H .071 .075
LEAD .255 .240 .092 .086
</table>
<tableCaption confidence="0.94164175">
Table 1: ROUGE scores of the RST discourse treebank
dataset. In the table, G,H,K,L indicate a method sta-
tistically significant against Marcu(G), Marcu(H), KP,
LEAD, respectively.
</tableCaption>
<bodyText confidence="0.999978444444444">
where x is an N-dimensional binary vector that
represents the summary, i.e. xi=1 denotes that the i-
th EDU is included in the summary. N is the number
of EDUs in a document, ti is the length (the number
of words) of the i-th EDU, and parent(i) indicates
the ID of the parent of the i-th EDU in the DEP-DT.
Constraint (6) ensures that the length of a summary
is less than limit L. Constraint (7) ensures that a
summary is a rooted subtree of the DEP-DT. Thus,
xparent(i) is always 1 when the i-th EDU is included
in the summary.
In general, the Tree Knapsack Problem is NP-
hard, but fortunately we can obtain the optimal solu-
tion in a feasible time by using ILP solvers for doc-
uments of practical tree size. In addition, bottom-
up DP (Lukes, 1974) and depth-first DP algorithms
(Cho and Shaw, 1997) are known to find the optimal
solution efficiently.
</bodyText>
<sectionHeader confidence="0.995904" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.986398">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.9999727">
We conducted an experimental evaluation on the test
collection for single document summarization eval-
uation contained in the RST Discourse Treebank
(RST-DTB)(Carlson et al., 2001) distributed by the
Linguistic Data Consortium (LDC)I. The RST-DTB
Corpus includes 385 Wall Street Journal articles
with RST annotation, and 30 of these documents
also have one human-made reference summary. The
average length of the reference summaries corre-
sponds to about 10 % of the words in the source
</bodyText>
<footnote confidence="0.9048285">
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T07
</footnote>
<bodyText confidence="0.993072833333333">
document.
We compared our method (TKP) with Marcu’s
method (Marcu) (Marcu, 1998), a simple knapsack
model (KP), a maximum coverage model (MCP)
and a lead method (LEAD). MCP is known to be a
state-of-the-art method for multiple document sum-
marization and we believe that MCP also performs
well in terms of single document summarization.
LEAD is also a widely used summarizer that simply
takes the first K textual units of the document. Al-
though this is a simple heuristic rule, it is known as
a state-of-the-art summarizer (Nenkova and McKe-
own, 2011).
For our method, we examined two types of
DEP-DT. One was obtained from the gold RST-
DT. The other was obtained from the RST-DT pro-
duced by a state-of-the-art RST parser, HILDA (du-
Verle and Prendinger, 2009; Hernault et al., 2010).
For Marcu’s method, we examined both the gold
RST-DT and HILDA’s RST-DT. We re-implemented
HILDA and re-trained it on the RST-DT Corpus ex-
cluding the 30 documents used in the evaluation.
The F-score of the parser was around 0.5. For KP,
we exclude constraint (7) from the ILP formulation
of TKP and set the depth of all EDUs in equations
(3) and (5) at 1. For MCP, we use tf (equation (4))
as the word weight.
We evaluated the summarization systems with
ROUGE version 1.5.5 4. Performance metrics were
the recall (R) and F-score (F) of ROUGE-1,2.
</bodyText>
<subsectionHeader confidence="0.93186">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.993495">
Table 1 shows the evaluation results. In the ta-
ble, TKP(G) and TKP(H) denote methods with the
DEP-DT obtained from the gold RST-DT and from
HILDA, respectively. Marcu(G) and Marcu(H) de-
note Marcu’s method described in (Marcu, 1998)
with gold RST-DT and with HILDA, respectively.
We performed a multiple comparison test for the dif-
ferences among ROUGE scores, we calculated the p-
values between systems with the Wilcoxon signed-
rank test (Wilcoxon, 1945) and used the False Dis-
covery Rate (FDR) (Benjamini and Hochberg, 1995)
to calculate adjusted p-values, in order to limit false
positive rate to 5%.
From the table, TKP(G) and Marcu(G) achieved
</bodyText>
<footnote confidence="0.961649">
4Options used: -n 2 -s -m -x
</footnote>
<page confidence="0.98515">
1518
</page>
<bodyText confidence="0.631323416666667">
Reference:
The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been
over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets
are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Delicious, it is
much sweeter, less mealy and has a longer shelf life.
TKP(G):
We’ll still have mom and apple pie. A Japanese apple called the Fuji. Some fruit visionaries say the Fuji could someday tumble
the Red Delicious from the top of America’s apple heap. It has a long shelf life. Now, even more radical changes seem afoot. The
Delicious hegemony won’t end anytime soon. New apple trees grow slowly. But the apple industry is ripe for change. There’s a
Fuji apple cult.
Marcu(G):
We’ll still have mom and apple pie. On second thought, make that just mom. The Fuji could someday tumble the Red Delicious
from the top of America’s apple heap. Now, even more radical changes seem afoot. The Delicious hegemony won’t end anytime
soon. More than twice as many Red Delicious apples are grown as the Golden variety, America’s No. 2 apple. But the apple
industry is ripe for change.
MCP:
Called the Fuji. It has a long shelf life. New apple trees grow slowly. Its roots are patriotic. I’m going to have to get another job
this year. Scowls. They still buy apples mainly for big, red good looks. Japanese researchers have bred dozens of strains of Fujis.
Mr. Auvil, the Washington grower, says. Stores sell in summer. The “ big boss ” at a supermarket chain even rejected his Red
Delicious recently. Many growers employ.
LEAD:
Soichiro Honda’s picture now hangs with Henry Ford’s in the U.S. Automotive Hall of Fame, and the game-show “ Jeopardy ” is
soon to be Sony-owned. But no matter how much Japan gets under our skin, we’ll still have mom and apple pie. On second
thought, make that just mom. A Japanese apple called the Fuji is cropping up in orchards the way Hondas did on U.S. roads.
</bodyText>
<figureCaption confidence="0.998202">
Figure 4: Summaries obtained from wsj 1128.
</figureCaption>
<bodyText confidence="0.999594291666667">
better results than MCP, KP and LEAD, although
some of the comparisons are not significant. In par-
ticular, TKP(G) achieved the highest ROUGE scores
on all measures. On ROUGE-1 Recall, TKP(G) sig-
nificantly outperformed Marcu(G), Marcu(H), KP
and LEAD. These results support the effectiveness
of our method that utilizes the discourse structure.
Comparing TKP(H) with Marcu(H), the former
achieved higher scores with statistical significance
on ROUGE-1. In addition, Marcu(H) was outper-
formed by MCP, KP and LEAD. The results confirm
the effectiveness of our summarization model and
trimming proposal for DEP-DT. Moreover, the dif-
ference between TKP(G) and TKP(H) was smaller
than that between Marcu(G) and Marcu(H). This
implies that our method is more robust against dis-
course parser error than Marcu’s method.
Figure 4 shows the example summaries gener-
ated by TKP(G), Marcu(G), MCP and LEAD, re-
spectively for an article, wsj 1128. Since TKP(G)
and Marcu(G) utilize a discourse tree, the summary
generated by TKP(G) is similar to that generated by
Marcu(G) but it is different from those generated by
MCP and LEAD.
</bodyText>
<sectionHeader confidence="0.998739" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999333">
This paper proposed rules for transforming an RST-
DT to a DEP-DT to obtain the parent-child relation-
ships between EDUs. We treated a single document
summarization method as a Tree Knapsack Problem,
i.e. the summarizer selects the best rooted subtree
from a DEP-DT. To demonstrate the effectiveness of
our method, we conducted an experimental evalua-
tion using 30 documents selected from the RST Dis-
course Treebank Corpus. The results showed that
our method achieved the highest ROUGE-1,2 scores.
</bodyText>
<sectionHeader confidence="0.996813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982191636363636">
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and powerful
approach to multiple testing. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 57(1):289–
300.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proc. of
the SIGDIAL01, pages 1–10.
Geon Cho and Dong X Shaw. 1997. A depth-first
dynamic programming algorithm for the tree knap-
</reference>
<page confidence="0.881105">
1519
</page>
<reference confidence="0.999729903846154">
sack problem. INFORMS Journal on Computing,
9(4):431–438.
Hal Daume III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proc. of the 40th
ACL, pages 449–456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proc. of the Joint Conference of the 47th
ACL and 4th IJCNLP, pages 665–673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence extraction. In Proc. of the 20th COLING,
pages 397–403.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proc. of the 5th
International Natural Language Generation Confer-
ence (INLG), pages 25–32.
Hugo Hernault, Helmut Prendinger, David A duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1–33.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Workshop on
Text Summarization Branches Out, pages 74–81.
J. A. Lukes. 1974. Efficient algorithm for the partition-
ing of trees. IBM Journal of Research and Develop-
ment, 18(3):217–224.
Daniel Marcu. 1998. Improving summarization through
rhetorical parsing tuning. In Proc. of the 6th Workshop
on Very Large Corpora, pages 206–215.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Proc.
of the 29th ECIR, pages 557–564.
Ani Nenkova and Kathaleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in In-
formation Retrieval, 5(2-3):103–233.
Natthawut Samphaiboon and Takeo Yamada. 2000.
Heuristic and exact algorithms for the precedence-
constrained knapsack problem. Journal of Optimiza-
tion Theory and Applications, 105(3):659–676.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proc. of the 12th EACL,
pages 781–789.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceedings of the 18th CIKM.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80–83.
William Charles, Mann and Sandra Annear, Thompson.
1988. Rhetorical Structure Theory: Toward a func-
tional theory of text organization. Text, 8(3):243–281.
</reference>
<page confidence="0.989951">
1520
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.219676">
<title confidence="0.765907">Single-Document Summarization as a Tree Knapsack Problem Communication Science Laboratories, NTT</title>
<author confidence="0.249285">Seika-cho Hikaridai</author>
<author confidence="0.249285">Kyoto Soraku-gun</author>
<affiliation confidence="0.839491">Science and Technology</affiliation>
<address confidence="0.969239">North 14 West 9, Sapporo, Hokkaido, 060-0814,</address>
<email confidence="0.990739">yasuda@erato.ist.hokudai.ac.jp</email>
<abstract confidence="0.997593625">Recent studies on extractive text summarization formulate it as a combinatorial optimizaproblem such as a a Coverage Problem a These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming dependency-based discourse tree as a then solve it with integer linear programming (ILP). Evaluation results that our method improved scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Benjamini</author>
<author>Yosef Hochberg</author>
</authors>
<title>Controlling the false discovery rate: A practical and powerful approach to multiple testing.</title>
<date>1995</date>
<journal>Journal of the Royal Statistical Society, Series B (Methodological),</journal>
<volume>57</volume>
<issue>1</issue>
<pages>300</pages>
<contexts>
<context position="13196" citStr="Benjamini and Hochberg, 1995" startWordPosition="2225" endWordPosition="2228"> 1.5.5 4. Performance metrics were the recall (R) and F-score (F) of ROUGE-1,2. 4.2 Results and Discussion Table 1 shows the evaluation results. In the table, TKP(G) and TKP(H) denote methods with the DEP-DT obtained from the gold RST-DT and from HILDA, respectively. Marcu(G) and Marcu(H) denote Marcu’s method described in (Marcu, 1998) with gold RST-DT and with HILDA, respectively. We performed a multiple comparison test for the differences among ROUGE scores, we calculated the pvalues between systems with the Wilcoxon signedrank test (Wilcoxon, 1945) and used the False Discovery Rate (FDR) (Benjamini and Hochberg, 1995) to calculate adjusted p-values, in order to limit false positive rate to 5%. From the table, TKP(G) and Marcu(G) achieved 4Options used: -n 2 -s -m -x 1518 Reference: The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Delicious, it is much sweeter, less mealy and has a longer shelf life. TKP(</context>
</contexts>
<marker>Benjamini, Hochberg, 1995</marker>
<rawString>Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B (Methodological), 57(1):289– 300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proc. of the SIGDIAL01,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="10940" citStr="Carlson et al., 2001" startWordPosition="1855" endWordPosition="1858">DEP-DT. Thus, xparent(i) is always 1 when the i-th EDU is included in the summary. In general, the Tree Knapsack Problem is NPhard, but fortunately we can obtain the optimal solution in a feasible time by using ILP solvers for documents of practical tree size. In addition, bottomup DP (Lukes, 1974) and depth-first DP algorithms (Cho and Shaw, 1997) are known to find the optimal solution efficiently. 4 Experimental Evaluation 4.1 Settings We conducted an experimental evaluation on the test collection for single document summarization evaluation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC)I. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotation, and 30 of these documents also have one human-made reference summary. The average length of the reference summaries corresponds to about 10 % of the words in the source 3http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2002T07 document. We compared our method (TKP) with Marcu’s method (Marcu) (Marcu, 1998), a simple knapsack model (KP), a maximum coverage model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for mul</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proc. of the SIGDIAL01, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geon Cho</author>
<author>Dong X Shaw</author>
</authors>
<title>A depth-first dynamic programming algorithm for the tree knapsack problem.</title>
<date>1997</date>
<journal>INFORMS Journal on Computing,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="10669" citStr="Cho and Shaw, 1997" startWordPosition="1817" endWordPosition="1820"> is the length (the number of words) of the i-th EDU, and parent(i) indicates the ID of the parent of the i-th EDU in the DEP-DT. Constraint (6) ensures that the length of a summary is less than limit L. Constraint (7) ensures that a summary is a rooted subtree of the DEP-DT. Thus, xparent(i) is always 1 when the i-th EDU is included in the summary. In general, the Tree Knapsack Problem is NPhard, but fortunately we can obtain the optimal solution in a feasible time by using ILP solvers for documents of practical tree size. In addition, bottomup DP (Lukes, 1974) and depth-first DP algorithms (Cho and Shaw, 1997) are known to find the optimal solution efficiently. 4 Experimental Evaluation 4.1 Settings We conducted an experimental evaluation on the test collection for single document summarization evaluation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC)I. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotation, and 30 of these documents also have one human-made reference summary. The average length of the reference summaries corresponds to about 10 % of the words in the source 3http://www.ldc.upenn.e</context>
</contexts>
<marker>Cho, Shaw, 1997</marker>
<rawString>Geon Cho and Dong X Shaw. 1997. A depth-first dynamic programming algorithm for the tree knapsack problem. INFORMS Journal on Computing, 9(4):431–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th ACL,</booktitle>
<pages>449--456</pages>
<marker>Daume, Marcu, 2002</marker>
<rawString>Hal Daume III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proc. of the 40th ACL, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David duVerle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proc. of the Joint Conference of the 47th ACL and 4th IJCNLP,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="12074" citStr="duVerle and Prendinger, 2009" startWordPosition="2034" endWordPosition="2038">age model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for multiple document summarization and we believe that MCP also performs well in terms of single document summarization. LEAD is also a widely used summarizer that simply takes the first K textual units of the document. Although this is a simple heuristic rule, it is known as a state-of-the-art summarizer (Nenkova and McKeown, 2011). For our method, we examined two types of DEP-DT. One was obtained from the gold RSTDT. The other was obtained from the RST-DT produced by a state-of-the-art RST parser, HILDA (duVerle and Prendinger, 2009; Hernault et al., 2010). For Marcu’s method, we examined both the gold RST-DT and HILDA’s RST-DT. We re-implemented HILDA and re-trained it on the RST-DT Corpus excluding the 30 documents used in the evaluation. The F-score of the parser was around 0.5. For KP, we exclude constraint (7) from the ILP formulation of TKP and set the depth of all EDUs in equations (3) and (5) at 1. For MCP, we use tf (equation (4)) as the word weight. We evaluated the summarization systems with ROUGE version 1.5.5 4. Performance metrics were the recall (R) and F-score (F) of ROUGE-1,2. 4.2 Results and Discussion </context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>David duVerle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proc. of the Joint Conference of the 47th ACL and 4th IJCNLP, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence extraction.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th COLING,</booktitle>
<pages>397--403</pages>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence extraction. In Proc. of the 20th COLING, pages 397–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proc. of the 5th International Natural Language Generation Conference (INLG),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="9462" citStr="Filippova and Strube, 2008" startWordPosition="1600" endWordPosition="1603">eEE(t) maximize X Depth(ei)xi (5) EN i=1 W(ei) Figure 3 shows the DEP-DT obtained from the RST-DT in Figure 1. The DEP-DT expresses the parent-child relationship between the EDUs. Therefore, we have to drop e7, e9 and e10 when we drop e8. Note that, by applying the rules, discourse relations defined between non-terminals of an RST-DT are eliminated. However, we believe that these relations are no needed for the summarization that we are attempting to realize. s.t. EN ℓixi &lt;_ L (6) i=1 Vi : xparent(i) �! xi (7) Vi : xi E t0, 11, (8) 2A similar approach has been applied to sentence compression (Filippova and Strube, 2008). 1517 ROUGE-1 ROUGE-2 F R F R TKP(G) .310H,K,L .321G,H,K,L .108 .112H TKP(H) .281H .284H .092 .093 Marcu(G) .291H .272H .101 .093 Marcu(H) .236 .219 .073 .068 MCP .279 .295H .073 .077 KP .251 .266H .071 .075 LEAD .255 .240 .092 .086 Table 1: ROUGE scores of the RST discourse treebank dataset. In the table, G,H,K,L indicate a method statistically significant against Marcu(G), Marcu(H), KP, LEAD, respectively. where x is an N-dimensional binary vector that represents the summary, i.e. xi=1 denotes that the ith EDU is included in the summary. N is the number of EDUs in a document, ti is the leng</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Dependency tree based sentence compression. In Proc. of the 5th International Natural Language Generation Conference (INLG), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
</authors>
<title>Helmut Prendinger, David A duVerle, and Mitsuru Ishizuka.</title>
<date>2010</date>
<marker>Hernault, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David A duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proc. of Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="2523" citStr="Lin, 2004" startWordPosition="364" endWordPosition="365">ximize the sum of significance scores of each unit. Filatova et al. (2004) proposed a summarization method based on a Maximum Coverage Problem, in which they select a set of textual units that maximizes the weighted sum of the conceptual units (e.g. unigrams) contained in the set. Although, their greedy solution is only an approximation, Takamura et al. (2009a) extended it to obtain the exact solution. More recently, Takamura et al. (2009b) regarded summarization as a Budgeted Median Problem and obtain exact solutions with integer linear programming. These methods successfully improved ROUGE (Lin, 2004) scores, but they still have one critical shortcoming. Since these methods are based on subset selection, the summaries they generate cannot preserve the rhetorical structure of the textual units of a source document. Thus, the resulting summary may lack coherence and may not include significant textual units from a source document. One powerful and potential way to overcome the problem is to include discourse tree constraints in the summarization procedure. Marcu (1998) regarded a document as a Rhetorical Structure Theory (RST) (William Charles, Mann and Sandra Annear, Thompson, 1988)-based d</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proc. of Workshop on Text Summarization Branches Out, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Lukes</author>
</authors>
<title>Efficient algorithm for the partitioning of trees.</title>
<date>1974</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="10618" citStr="Lukes, 1974" startWordPosition="1811" endWordPosition="1812">y. N is the number of EDUs in a document, ti is the length (the number of words) of the i-th EDU, and parent(i) indicates the ID of the parent of the i-th EDU in the DEP-DT. Constraint (6) ensures that the length of a summary is less than limit L. Constraint (7) ensures that a summary is a rooted subtree of the DEP-DT. Thus, xparent(i) is always 1 when the i-th EDU is included in the summary. In general, the Tree Knapsack Problem is NPhard, but fortunately we can obtain the optimal solution in a feasible time by using ILP solvers for documents of practical tree size. In addition, bottomup DP (Lukes, 1974) and depth-first DP algorithms (Cho and Shaw, 1997) are known to find the optimal solution efficiently. 4 Experimental Evaluation 4.1 Settings We conducted an experimental evaluation on the test collection for single document summarization evaluation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC)I. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotation, and 30 of these documents also have one human-made reference summary. The average length of the reference summaries corresponds to about 10 %</context>
</contexts>
<marker>Lukes, 1974</marker>
<rawString>J. A. Lukes. 1974. Efficient algorithm for the partitioning of trees. IBM Journal of Research and Development, 18(3):217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Improving summarization through rhetorical parsing tuning.</title>
<date>1998</date>
<booktitle>In Proc. of the 6th Workshop on Very Large Corpora,</booktitle>
<pages>206--215</pages>
<contexts>
<context position="2998" citStr="Marcu (1998)" startWordPosition="437" endWordPosition="438">Budgeted Median Problem and obtain exact solutions with integer linear programming. These methods successfully improved ROUGE (Lin, 2004) scores, but they still have one critical shortcoming. Since these methods are based on subset selection, the summaries they generate cannot preserve the rhetorical structure of the textual units of a source document. Thus, the resulting summary may lack coherence and may not include significant textual units from a source document. One powerful and potential way to overcome the problem is to include discourse tree constraints in the summarization procedure. Marcu (1998) regarded a document as a Rhetorical Structure Theory (RST) (William Charles, Mann and Sandra Annear, Thompson, 1988)-based discourse tree (RSTDT) and selected textual units according to a preference ranking derived from the tree structure to make a summary. Daume et al. (2002) proposed a document compression method that directly models the probability of a summary given an RST-DT by using a noisy-channel model. These methods generate well-organized summaries, however, since they do not formulate summarizations as combinatorial op1515 Proceedings of the 2013 Conference on Empirical Methods in </context>
<context position="11399" citStr="Marcu, 1998" startWordPosition="1920" endWordPosition="1921">al evaluation on the test collection for single document summarization evaluation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al., 2001) distributed by the Linguistic Data Consortium (LDC)I. The RST-DTB Corpus includes 385 Wall Street Journal articles with RST annotation, and 30 of these documents also have one human-made reference summary. The average length of the reference summaries corresponds to about 10 % of the words in the source 3http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2002T07 document. We compared our method (TKP) with Marcu’s method (Marcu) (Marcu, 1998), a simple knapsack model (KP), a maximum coverage model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for multiple document summarization and we believe that MCP also performs well in terms of single document summarization. LEAD is also a widely used summarizer that simply takes the first K textual units of the document. Although this is a simple heuristic rule, it is known as a state-of-the-art summarizer (Nenkova and McKeown, 2011). For our method, we examined two types of DEP-DT. One was obtained from the gold RSTDT. The other was obtained from the RST-DT pro</context>
<context position="12905" citStr="Marcu, 1998" startWordPosition="2180" endWordPosition="2181">score of the parser was around 0.5. For KP, we exclude constraint (7) from the ILP formulation of TKP and set the depth of all EDUs in equations (3) and (5) at 1. For MCP, we use tf (equation (4)) as the word weight. We evaluated the summarization systems with ROUGE version 1.5.5 4. Performance metrics were the recall (R) and F-score (F) of ROUGE-1,2. 4.2 Results and Discussion Table 1 shows the evaluation results. In the table, TKP(G) and TKP(H) denote methods with the DEP-DT obtained from the gold RST-DT and from HILDA, respectively. Marcu(G) and Marcu(H) denote Marcu’s method described in (Marcu, 1998) with gold RST-DT and with HILDA, respectively. We performed a multiple comparison test for the differences among ROUGE scores, we calculated the pvalues between systems with the Wilcoxon signedrank test (Wilcoxon, 1945) and used the False Discovery Rate (FDR) (Benjamini and Hochberg, 1995) to calculate adjusted p-values, in order to limit false positive rate to 5%. From the table, TKP(G) and Marcu(G) achieved 4Options used: -n 2 -s -m -x 1518 Reference: The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been over-planted and prices h</context>
</contexts>
<marker>Marcu, 1998</marker>
<rawString>Daniel Marcu. 1998. Improving summarization through rhetorical parsing tuning. In Proc. of the 6th Workshop on Very Large Corpora, pages 206–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proc. of the 29th ECIR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="1812" citStr="McDonald (2007)" startWordPosition="248" endWordPosition="250">ion. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores. 1 Introduction State-of-the-art extractive text summarization methods regard a document (or a document set) as a set of textual units (e.g. sentences, clauses, phrases) and formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of the set of textual units that maximizes an objective without violating a length constraint. For example, McDonald (2007) formulated text summarization as a Knapsack Problem, where he selects a set of textual units that maximize the sum of significance scores of each unit. Filatova et al. (2004) proposed a summarization method based on a Maximum Coverage Problem, in which they select a set of textual units that maximizes the weighted sum of the conceptual units (e.g. unigrams) contained in the set. Although, their greedy solution is only an approximation, Takamura et al. (2009a) extended it to obtain the exact solution. More recently, Takamura et al. (2009b) regarded summarization as a Budgeted Median Problem an</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proc. of the 29th ECIR, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathaleen McKeown</author>
</authors>
<date>2011</date>
<booktitle>Automatic summarization. Foundations and Trends in Information Retrieval,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="11868" citStr="Nenkova and McKeown, 2011" startWordPosition="1997" endWordPosition="2001">rce 3http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2002T07 document. We compared our method (TKP) with Marcu’s method (Marcu) (Marcu, 1998), a simple knapsack model (KP), a maximum coverage model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for multiple document summarization and we believe that MCP also performs well in terms of single document summarization. LEAD is also a widely used summarizer that simply takes the first K textual units of the document. Although this is a simple heuristic rule, it is known as a state-of-the-art summarizer (Nenkova and McKeown, 2011). For our method, we examined two types of DEP-DT. One was obtained from the gold RSTDT. The other was obtained from the RST-DT produced by a state-of-the-art RST parser, HILDA (duVerle and Prendinger, 2009; Hernault et al., 2010). For Marcu’s method, we examined both the gold RST-DT and HILDA’s RST-DT. We re-implemented HILDA and re-trained it on the RST-DT Corpus excluding the 30 documents used in the evaluation. The F-score of the parser was around 0.5. For KP, we exclude constraint (7) from the ILP formulation of TKP and set the depth of all EDUs in equations (3) and (5) at 1. For MCP, we </context>
</contexts>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathaleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval, 5(2-3):103–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natthawut Samphaiboon</author>
<author>Takeo Yamada</author>
</authors>
<title>Heuristic and exact algorithms for the precedenceconstrained knapsack problem.</title>
<date>2000</date>
<journal>Journal of Optimization Theory and Applications,</journal>
<volume>105</volume>
<issue>3</issue>
<contexts>
<context position="8741" citStr="Samphaiboon and Yamada, 2000" startWordPosition="1469" endWordPosition="1472"> 16 is e10. Thus, e9 is a child of e10. s.t. Length(t) &lt;_ L. (2) Here, we define F(t) as W(e)(3) Depth(e). E(t) is the set of EDUs contained in t, Depth(e) is the depth of an EDU e within the DEP-DT. For example, Depth(e2) = 1, Depth(e6) = 4 for the DEP-DT of Figure 3. W(e) is defined as follows: W(e) = E tf(w, D). (4) wEW(e) W(e) is the set of words contained in e and tf(w, D) is the term frequency of word w in a document D. 3.2 ILP Formulation We formulate the optimization problem in the previous section as a Tree Knapsack Problem, which is a kind of Precedence-Constrained Knapsack Problem (Samphaiboon and Yamada, 2000) and we can obtain the optimal rooted subtree by solving the following ILP problem2: E F(t) = eEE(t) maximize X Depth(ei)xi (5) EN i=1 W(ei) Figure 3 shows the DEP-DT obtained from the RST-DT in Figure 1. The DEP-DT expresses the parent-child relationship between the EDUs. Therefore, we have to drop e7, e9 and e10 when we drop e8. Note that, by applying the rules, discourse relations defined between non-terminals of an RST-DT are eliminated. However, we believe that these relations are no needed for the summarization that we are attempting to realize. s.t. EN ℓixi &lt;_ L (6) i=1 Vi : xparent(i) </context>
</contexts>
<marker>Samphaiboon, Yamada, 2000</marker>
<rawString>Natthawut Samphaiboon and Takeo Yamada. 2000. Heuristic and exact algorithms for the precedenceconstrained knapsack problem. Journal of Optimization Theory and Applications, 105(3):659–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proc. of the 12th EACL,</booktitle>
<pages>781--789</pages>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009a. Text summarization model based on maximum coverage problem and its variant. In Proc. of the 12th EACL, pages 781–789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on the budgeted median problem.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th CIKM.</booktitle>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009b. Text summarization model based on the budgeted median problem. In Proceedings of the 18th CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wilcoxon</author>
</authors>
<title>Individual comparisons by ranking methods.</title>
<date>1945</date>
<journal>Biometrics Bulletin,</journal>
<volume>1</volume>
<issue>6</issue>
<contexts>
<context position="13125" citStr="Wilcoxon, 1945" startWordPosition="2215" endWordPosition="2216">We evaluated the summarization systems with ROUGE version 1.5.5 4. Performance metrics were the recall (R) and F-score (F) of ROUGE-1,2. 4.2 Results and Discussion Table 1 shows the evaluation results. In the table, TKP(G) and TKP(H) denote methods with the DEP-DT obtained from the gold RST-DT and from HILDA, respectively. Marcu(G) and Marcu(H) denote Marcu’s method described in (Marcu, 1998) with gold RST-DT and with HILDA, respectively. We performed a multiple comparison test for the differences among ROUGE scores, we calculated the pvalues between systems with the Wilcoxon signedrank test (Wilcoxon, 1945) and used the False Discovery Rate (FDR) (Benjamini and Hochberg, 1995) to calculate adjusted p-values, in order to limit false positive rate to 5%. From the table, TKP(G) and Marcu(G) achieved 4Options used: -n 2 -s -m -x 1518 Reference: The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Deli</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Charles</author>
<author>Mann</author>
<author>Sandra Annear</author>
<author>Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<marker>Charles, Mann, Annear, Thompson, 1988</marker>
<rawString>William Charles, Mann and Sandra Annear, Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
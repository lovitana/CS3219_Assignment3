<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000073">
<title confidence="0.982931">
Orthonormal Explicit Topic Analysis for Cross-lingual Document Matching
</title>
<author confidence="0.998026">
John Philip McCrae Philipp Cimiano Roman Klinger
</author>
<affiliation confidence="0.999201">
University Bielefeld University Bielefeld University Bielefeld
</affiliation>
<address confidence="0.898251">
Inspiration 1 Inspiration 1 Inspiration 1
Bielefeld, Germany Bielefeld, Germany Bielefeld, Germany
</address>
<email confidence="0.912918">
{jmccrae,cimiano,rklinger}@cit-ec.uni-bielefeld.de
</email>
<sectionHeader confidence="0.99786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999709333333333">
Cross-lingual topic modelling has applications
in machine translation, word sense disam-
biguation and terminology alignment. Multi-
lingual extensions of approaches based on la-
tent (LSI), generative (LDA, PLSI) as well as
explicit (ESA) topic modelling can induce an
interlingual topic space allowing documents
in different languages to be mapped into the
same space and thus to be compared across
languages. In this paper, we present a novel
approach that combines latent and explicit
topic modelling approaches in the sense that
it builds on a set of explicitly defined top-
ics, but then computes latent relations between
these. Thus, the method combines the ben-
efits of both explicit and latent topic mod-
elling approaches. We show that on a cross-
lingual mate retrieval task, our model signif-
icantly outperforms LDA, LSI, and ESA, as
well as a baseline that translates every word in
a document into the target language.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999637355555556">
Cross-lingual document matching is the task of,
given a query document in some source language,
estimating the similarity to a document in some tar-
get language. This task has important applications in
machine translation (Palmer et al., 1998; Tam et al.,
2007), word sense disambiguation (Li et al., 2010)
and ontology alignment (Spiliopoulos et al., 2007).
An approach that has become quite popular in re-
cent years for cross-lingual document matching is
Explicit Semantics Analysis (ESA, Gabrilovich and
Markovitch (2007)) and its cross-lingual extension
CL-ESA (Sorg and Cimiano, 2008). ESA indexes
documents by mapping them into a topic space de-
fined by their similarity to predefined explicit top-
ics – generally articles from an encyclopaedia – in
such a way that there is a one-to-one correspondence
between topics and encyclopedic entries. CL-ESA
extends this to the multilingual case by exploiting
a background document collection that is aligned
across languages, such as Wikipedia. A feature of
ESA and its extension CL-ESA is that, in contrast to
latent (e.g. LSI, Deerwester et al. (1990)) or genera-
tive topic models (such as LDA, Blei et al. (2003)),
it requires no training and, nevertheless, has been
demonstrated to outperform LSI and LDA on cross-
lingual retrieval tasks (Cimiano et al., 2009).
A key choice in Explicit Semantic Analysis is the
document space that will act as the topic space. The
standard choice is to regard all articles from a back-
ground document collection – Wikipedia articles are
a typical choice – as the topic space. However, it
is crucial to ensure that these topics cover the se-
mantic space evenly and completely. In this pa-
per, we present an alternative approach where we
remap the semantic space defined by the topics in
such a manner that it is orthonormal. In this way,
each document is mapped to a topic that is distinct
from all other topics. Such a mapping can be con-
sidered as equivalent to a variant of Latent Seman-
tic Indexing (LSI) with the main difference that our
model exploits the matrix that maps topic vectors
back into document space, which is normally dis-
carded in LSI-based approaches. We dub our model
ONETA (OrthoNormal Explicit Topic Analysis) and
empirically show that on a cross-lingual retrieval
</bodyText>
<page confidence="0.945951">
1732
</page>
<note confidence="0.7316775">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1732–1740,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997578">
task it outperforms ESA, LSI, and Latent Dirichlet
Allocation (LDA) as well as a baseline consisting of
translating each word into the target language, thus
reducing the task to a standard monolingual match-
ing task. In particular, we quantify the effect of dif-
ferent approximation techniques for computing the
orthonormal basis and investigate the effect of vari-
ous methods for the normalization of frequency vec-
tors.
The structure of the paper is as follows: we situate
our work in the general context of related work on
topic models for cross-lingual document matching
in Section 2. We present our model in Section 3 and
present our experimental results and discuss these
results in Section 4.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999933932432433">
The idea of applying topic models that map docu-
ments into an interlingual topic space seems a quite
natural and principled approach to tackle several
tasks including the cross-lingual document retrieval
problem.
Topic modelling is the process of finding a rep-
resentation of a document d in a lower dimensional
space RK where each dimension corresponds to one
topic that abstracts from specific words and thus al-
lows us to detect deeper semantic similarities be-
tween documents beyond the computation of the
pure overlap in terms of words.
Three main variants of document models have
been mainly considered for cross-lingual document
matching:
Latent methods such as Latent Semantic Indexing
(LSI, Deerwester et al. (1990)) induce a de-
composition of the term-document matrix in
a way that reduces the dimensionality of the
documents, while minimizing the error in re-
constructing the training data. For example,
in Latent Semantic Indexing, a term-document
matrix is approximated by a partial singu-
lar value decomposition, or in Non-Negative
Matrix Factorization (NMF, Lee and Seung
(1999)) by two smaller non-negative matrices.
If we append comparable or equivalent doc-
uments in multiple languages together before
computing the decomposition as proposed by
Dumais et al. (1997) then the topic model is
essentially cross-lingual allowing to compare
documents in different languages once they
have been mapped into the topic space.
Probabilistic or generative methods instead at-
tempt to induce a (topic) model that has the
highest likelihood of generating the documents
actually observed during training. As with la-
tent methods, these topics are thus interlin-
gual and can generate words/terms in differ-
ent languages. Prominent representatives of
this type of method are Probabilistic Latent Se-
mantic Indexing (PLSI, Hofmann (1999)) or
Latent Dirichlet Allocation (LDA, Blei et al.
(2003)), both of which can be straightforwardly
extended to the cross-lingual case (Mimno et
al., 2009).
Explicit topic models make the assumption that
topics are explicitly given instead of being in-
duced from training data. Typically, a back-
ground document collection is assumed to be
given whereby each document in this corpus
corresponds to one topic. A mapping from doc-
ument to topic space is calculated by comput-
ing the similarity of the document to every doc-
ument in the topic space. A prominent exam-
ple for this kind of topic modelling approach is
Explicit Semantic Analysis (ESA, Gabrilovich
and Markovitch (2007)).
Both latent and generative topic models attempt to
find topics from the data and it has been found that
in some cases they are equivalent (Ding et al., 2006).
However, this approach suffers from the problem
that the topics might be artifacts of the training data
rather than coherent semantic topics. In contrast, ex-
plicit topic methods can use a set of topics that are
chosen to be well-suited to the domain. The princi-
ple drawback of this is that the method for choosing
such explicit topics by selecting documents is com-
paratively crude. In general, these topics may be
overlapping and poorly distributed over the seman-
tic topic space. By comparison, our method takes the
advantage of the pre-specified topics of explicit topic
models, but incorporates a training step to learn la-
tent relations between these topics.
</bodyText>
<page confidence="0.990637">
1733
</page>
<sectionHeader confidence="0.99676" genericHeader="method">
3 Orthonormal explicit topic analysis
</sectionHeader>
<bodyText confidence="0.999979285714286">
Our approach follows Explicit Semantic Analysis in
the sense that it assumes the availability of a back-
ground document collection B = {b1, b2,..., bN}
consisting of textual representations. The map-
ping into the explicit topic space is defined by a
language-specific function Φ that maps documents
into RN such that the jth value in the vector is given
by some association measure Oj(d) for each back-
ground document bj. Typical choices for this associ-
ation measure 0 are the sum of the TF-IDF scores or
an information retrieval relevance scoring function
such as BM-25 (Sorg and Cimiano, 2010).
For the case of TF-IDF, the value of the j-th ele-
ment of the topic vector is given by:
</bodyText>
<equation confidence="0.998855">
Oj(d) = tf-idf(bj)T −−−�
−−−� tf-idf(d)
</equation>
<bodyText confidence="0.9996585">
Thus, the mapping function can be represented as
the product of a TF-IDF vector of document d multi-
plied by an W xN matrix, X, each element of which
contains the TF-IDF value of word i in document bj:
</bodyText>
<equation confidence="0.9992195">
tf-idf(d) = XT · −−−→
−−−→ tf-idf(d)
</equation>
<bodyText confidence="0.999757428571429">
For simplicity, we shall assume from this point on
that all vectors are already converted to a TF-IDF
or similar numeric vector form. In order to com-
pute the similarity between two documents di and
dj, typically the cosine-function (or the normalized
dot product) between the vectors Φ(di) and Φ(dj) is
computed as follows:
</bodyText>
<equation confidence="0.986719">
Φ(di)TΦ(dj)
sim(di, dj) = cos(Φ(di), Φ(dj)) = ||Φ(di) |Φ(dj)||
</equation>
<bodyText confidence="0.984816375">
If we represent the above using our above defined
W x N matrix X then we get:
The key challenge with ESA is choosing a good
background document collection B = {b1, ..., bN}.
A simple minimal criterion for a good background
document collection is that each document in this
collection should be maximally similar to itself and
less similar to any other document:
</bodyText>
<equation confidence="0.983177">
Vi =� j 1 = sim(bj, bj) &gt; sim(bi, bj) &gt; 0
</equation>
<bodyText confidence="0.999839625">
While this criterion is trivially satisfied if we have
no duplicate documents in our collection, our intu-
ition is that we should choose a background collec-
tion that maximizes the slack margin of this inequal-
ity, i.e. |sim(bj, bj) − sim(bi, bj)|. We can see that
maximizing this margin for all i,j is the same as
minimizing the semantic overlap of the background
documents, which is given as follows:
</bodyText>
<equation confidence="0.99556175">
�overlap(B) = sim(bi, bj)
i � 1, . . . ,N
j� 1, . . . ,N
i7�j
</equation>
<bodyText confidence="0.9978108">
We first note that we can, without loss of general-
ity, normalize our background documents such that
||Xbj ||= 1 for all j, and in this case we can re-
define the semantic overlap as the following matrix
expression1
</bodyText>
<equation confidence="0.996913">
overlap(X) = ||XTXXTX − I||1
</equation>
<bodyText confidence="0.994410666666667">
It is trivial to verify that this equation has a mini-
mum when XTXXTX = I. This is the case when
the topics are orthonormal:
</bodyText>
<equation confidence="0.996669">
(XTbi)T(XTbj) = 0 if i =� j
(XTbi)T(XTbi) = 1
</equation>
<bodyText confidence="0.996961555555556">
Unfortunately, this is not typically the case as the
documents have significant word overlap as well as
semantic overlap. Our goal is thus to apply a suitable
transformation to X with the goal of ensuring that
the orthogonality property holds.
Assuming that this transformation of X is done
by multiplication with some other matrix A, we can
define the learning problem as finding that matrix A
such that:
</bodyText>
<equation confidence="0.791457333333333">
(AXTX)T(AXTX) = I
�
1||A||� = Ei,j |aij|� is the p-norm. ||A||., = ||A||2 is
</equation>
<bodyText confidence="0.223499">
the Frobenius norm.
</bodyText>
<figure confidence="0.901076571428572">
Φ(d) = ⎛ tf-idf(b1)T ⎞
⎜ ⎜ ⎝ −−−→ ⎠⎟⎟
...
tf-idf(b�)T
−−−→
sim(di, dj) = cos(XTdi, XTdj) = dT i XXTdj
||XTdi |XTdj||
</figure>
<page confidence="0.984769">
1734
</page>
<bodyText confidence="0.999886">
If we have the case that W &gt; N and that the rank
of X is N, then XTX is invertible and thus A =
(XTX)−1 is the solution to this problem.2
We define the projection function of a document
d, represented as a normalized term frequency vec-
tor, as follows:
</bodyText>
<equation confidence="0.982811">
4bONETA(d) = (XTX)−1XTd
</equation>
<bodyText confidence="0.99995575">
For the cross-lingual case we assume that we have
two sets of background documents of equal size,
B1 = {b11, ... , b1N}, B2 = {bi, ... , b2N} in lan-
guages l1 and l2, respectively and that these doc-
uments are aligned such that for every index i, b1i
and b2i are documents on the same topic in each
language. Using this we can construct a projec-
tion function for each language which maps into the
same topic space. Thus, as in CL-ESA, we obtain
the cross-lingual similarity between a document di
in language l1 and a document dj in language l2 as
follows:
</bodyText>
<equation confidence="0.8874805">
Sim(di, dj) = COS(4bl1ONETA(di), 4bl2
ONETA(dj))
</equation>
<bodyText confidence="0.999954875">
We note here that we assume that 4b could be rep-
resented as a symmetric inner product of two vec-
tors. However, for many common choices of asso-
ciation measures, including BM25, this is not the
case. In this case the expression XTX can be re-
placed with a kernel matrix specifying the associ-
ation of each background document to each other
background document.
</bodyText>
<subsectionHeader confidence="0.99695">
3.1 Relationship to Latent Semantic Indexing
</subsectionHeader>
<bodyText confidence="0.9814716">
In this section we briefly clarify the relationship be-
tween our method ONETA and Latent Semantic In-
dexing. Latent Semantic Indexing defines a map-
ping from a document represented as a term fre-
quency vector to a vector in RK. This transforma-
tion is defined by means of calculating the singu-
lar value decomposition (SVD) of the matrix X as
above, namely
2In the case that the matrix is not invertible we can in-
stead solve ||XTXA − I||,-, which has a minimum at A =
VE−1UT where XTX = UEVT is the singular value de-
composition of XTX.
As usual we do not in fact compute the inverse for our exper-
iments, but instead the LU Decomposition and solve by Gaus-
sian elimination at test time.
</bodyText>
<equation confidence="0.983744">
X = UEVT
</equation>
<bodyText confidence="0.9611708">
Where E is diagonal and U V are the eigenvec-
tors of XXT and XTX., respectively. Let EK de-
note the K x K submatrix containing the largest
eigenvalues, and UK,VK denote the corresponding
eigenvectors. Thus LSI can be defined as:
</bodyText>
<equation confidence="0.9988535">
4bLSI(d) = E−1
K UKd
</equation>
<bodyText confidence="0.995653">
With regards to orthonormalized topics, we see
that using the SVD, we can simply derive the fol-
lowing:
</bodyText>
<equation confidence="0.997101">
(XTX)−1XT = VE−1UT
</equation>
<bodyText confidence="0.999812928571428">
When we set K = N and thus choose the maxi-
mum number of topics, ONETA is equivalent to LSI
modulo the fact that it multiplies the resulting topic
vector by V, thus projecting back into document
space, i.e. into explicit topics.
In practice, both methods differ significantly in
that the approximations they make are quite differ-
ent. Furthermore, in the case that W » N and X
has n non-zeroes, the calculation of the SVD is of
complexity O(nN + WN2) and requires O(WN)
bytes of memory. In contrast, ONETA requires com-
putation time of O(Na) for a &gt; 2, which is the com-
plexity of the matrix inversion algorithm3, and only
O(n + N2) bytes of memory.
</bodyText>
<subsectionHeader confidence="0.996368">
3.2 Approximations
</subsectionHeader>
<bodyText confidence="0.99998675">
The computation of the inverse has a complexity
that, using current practical algorithms, is approxi-
mately cubic and as such the time spent calculating
the inverse can grow very quickly. There are sev-
eral methods for obtaining an approximate inverse.
The most commonly used are based on the SVD or
eigendecomposition of the matrix. As XTX is sym-
metric positive definite, it holds that:
</bodyText>
<sectionHeader confidence="0.619765" genericHeader="method">
XTX = UEUT
</sectionHeader>
<bodyText confidence="0.9970325">
Where U are the eigenvectors of XTX and E is
a diagonal matrix of the eigenvalues. With UK,EK
</bodyText>
<footnote confidence="0.935878">
3Algorithms with a = 2.3727 are known but practical algo-
rithms have a = 2.807 or a = 3 (Coppersmith and Winograd,
1990)
</footnote>
<page confidence="0.987098">
1735
</page>
<bodyText confidence="0.999412">
as the first K eigenvalues and eigenvectors, respec-
tively, we have:
</bodyText>
<equation confidence="0.99715325">
C (ATA)−&apos;AT −(ATA)−&apos;ATBC&apos; A B l I
0 C&apos; / \ 0 C J
(XTX)−1 &apos; UK�−1 K UT (1)
K
</equation>
<bodyText confidence="0.999891">
We call this the orthonormal eigenapproxima-
tion or ON-Eigen. The complexity of calculating
(XTX)−1XT from this is O(N2K + Nn), where
n is the number of non-zeros in X.
Similarly, using the formula derived in the previ-
ous section we can derive an approximation of the
full model as follows:
</bodyText>
<equation confidence="0.996831">
(XTX)−1XT &apos; UK�−1 K VT (2)
K
</equation>
<bodyText confidence="0.999986333333334">
We call this approximation Explicit LSI as it first
maps into the latent topic space and then into the
explicit topic space.
We can consider another approximation by notic-
ing that X is typically very sparse and moreover
some rows of X have significantly fewer non-zeroes
than others (these rows are for terms with low fre-
quency). Thus, if we take the first N1 columns (doc-
uments) in X, it is possible to rearrange the rows
of X with the result that there is some W1 such
that rows with index greater than W1 have only ze-
roes in the columns up to N1. In other words, we
take a subset of N1 documents and enumerate the
words in such a way that the terms occurring in the
first N1 documents are enumerated 1, ... , W1. Let
N2 = N − N1, W2 = W − W1. The result of this
row permutation does not affect the value of XTX
and we can write the matrix X as:
</bodyText>
<equation confidence="0.992323">
X =( A B)
0 C /)
</equation>
<bodyText confidence="0.996228272727273">
where A is a W1 × N1 matrix representing term
frequencies in the first N1 documents, B is a W1 ×
N2 matrix containing term frequencies in the re-
maining documents for terms that are also found in
the first N1 documents, and C is a W2 ×N2 contain-
ing the frequency of all terms not found in the first
N1 documents.
Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for
matrix inversion yields the following easily verifi-
able matrix identity, given that we can find C0 such
</bodyText>
<equation confidence="0.631435">
that C0C = I.
(3)
</equation>
<bodyText confidence="0.9993445">
We denote the above equation using a matrix L
as LTX = I. We note that L =6 (XTX)−1X,
but for any document vector d that is representable
as a linear combination of the background doc-
ument set (i.e., columns of X) we have that
Ld = (XTX)−1XTd and in this sense L &apos;
(XTX)−1XT.
We further relax the assumption so that we only
need to find a C0 such that C0C &apos; I. For this,
we first observe that C is very sparse as it contains
only terms not contained in the first N1 documents
and we notice that very sparse matrices tend to be
approximately orthogonal, hence suggesting that it
should be very easy to find a left-inverse of C. The
following lemma formalizes this intuition:
Lemma: If C is a W × N matrix with M non-
zeros, distributed randomly and uniformly across the
matrix, and all the non-zeros are 1, then DCTC has
an expected value on each non-diagonal value of N2M
and a diagonal value of 1 if D is the diagonal matrix
whose values are given by ||ci||−2, the square of the
norm of the corresponding column of C.
Proof: We simply observe that if D0 = DCTC,
then the (i, j)th element of D0 is given by
</bodyText>
<equation confidence="0.9407368">
cTi cj
dij =
||c1||−2 0
...
0 ||cN2||−2
</equation>
<bodyText confidence="0.992800785714286">
We call this method L-Solve. The complexity
of calculating a left-inverse by this method is of
2
If i =6 j then the cTi cj is the number of non-zeroes
overlapping in the ith and jth column of C and under
a uniform distribution we expect this to be M2
N� . Sim-
ilarly, we expect the column norm to be MN such that
the overall expectation is N2M . The diagonal value is
clearly equal to 1.0
As long as C is very sparse, we can use the fol-
lowing approximation, which can be calculated in
O(M) operations, where M is the number of non-
zeroes.
</bodyText>
<equation confidence="0.8165662">
||ci||
�
�
C0 &apos; �
1 CT
</equation>
<page confidence="0.976198">
1736
</page>
<table confidence="0.997607125">
Precision
Document
Normalization
Frequency Normalization No Yes
TF 0.31 0.78
Relative 0.23 0.42
TFIDF 0.21 0.63
SQRT 0.28 0.66
</table>
<tableCaption confidence="0.9938955">
Table 1: Effect of Term Frequency and Document Nor-
malization on Top-1 Precision
</tableCaption>
<bodyText confidence="0.999801454545454">
order O(Na1 ), being much more efficient than the
eigenvalue methods. However, it is potentially more
error-prone as it requires that a left-inverse of C ex-
ists. On real data this might be violated if we do not
have linear independence of the rows of C, for ex-
ample if W2 &lt; N2 or if we have even one document
which has only words that are also contained in the
first N1 documents and hence there is a row in C
that consists of zeros only. This can be solved by
removing documents from the collection until C is
row-wise linear independent.4
</bodyText>
<subsectionHeader confidence="0.989864">
3.3 Normalization
</subsectionHeader>
<bodyText confidence="0.999982166666667">
A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.
A straightforward approach to normalization is to
normalize each column of X to obtain a matrix as
follows:
</bodyText>
<equation confidence="0.9994535">
� x1 xN
X0 = ||x1 ||... ||xN||
</equation>
<bodyText confidence="0.739175">
If we calculate X0TX0 = Y then we get that the
(i, j)-th element of Y is:
xTi xj
||xi |xj||
</bodyText>
<footnote confidence="0.89203">
4In the experiments in the next section we discarded 4.2% of
documents at Nl = 1000 and 47% of documents at Nl = 5000
</footnote>
<figure confidence="0.420313">
Approximation rate
</figure>
<figureCaption confidence="0.996142">
Figure 1: Effect on Top-1 Precision by various approxi-
mation method
</figureCaption>
<bodyText confidence="0.9810007">
Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have that
|yij |&lt; 1, with the result that the matrix Y is al-
ready close to I. Formally, we can use this to state
a bound on ||X0TX0 − I||F, but in practice it means
that the orthonormalizing matrix has more small or
zero values.
A further option for normalization is to consider
some form of term frequency normalization. For
term frequency normalization, we use TF (tfwn),
Relative (tfwn), TFIDF (tfwn log(d fwN )), and SQRT
Fw( tfwn) Here, tfwn is the term frequency of word w
Fw
in document n, Fw is the total frequency of word
w in the corpus, and dfw is the number of docu-
ments containing the words w. The first three of
these normalizations have been chosen as they are
widely used in the literature. The SQRT normaliza-
tion has been shown to be effective for explicit topic
methods in previous experiments not reported here.
</bodyText>
<sectionHeader confidence="0.999036" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9994212">
For evaluation, we consider a cross-lingual mate re-
trieval task from English/Spanish on the basis of
Wikipedia as aligned corpus. The goal is to, for each
document of a test set, retrieve the aligned document
or mate. For each test document, on the basis of
</bodyText>
<figure confidence="0.99660705882353">
100 200 300 400 500
0.0 0.2 0.4 0.6 0.8
●
●
● ON−Eigen
● L−Solve
Explicit LSI
LSI
− ESA
●
●
●
●
●
●
●
●
● ●●
●
●
●
● ●
●
●
●
●
● ●
● ● ● ●
● ●
●
●
● ●
● ●●
yij =
</figure>
<page confidence="0.971897">
1737
</page>
<table confidence="0.9999145625">
Method Top-1 Prec. Top-5 Prec. Top-10 Prec. MRR Time Memory
ONETA L-Solve (N1 = 1000) 0.290 0.501 0.596 0.390 73s 354MB
ONETA L-Solve (N1 = 2000) 0.328 0.531 0.600 0.423 2m18s 508MB
ONETA L-Solve (N1 = 3000) 0.462 0.662 0.716 0.551 4m12s 718MB
ONETA L-Solve (N1 = 4000) 0.599 0.755 0.781 0.667 7m44s 996MB
ONETA L-Solve (N1 = 5000) 0.695 0.817 0.843 0.750 12m28s 1.30GB
ONETA L-Solve (N1 = 6000) 0.773 0.883 0.905 0.824 18m40s 1.69GB
ONETA L-Solve (N1 = 7000) 0.841 0.928 0.937 0.881 26m31s 2.14GB
ONETA L-Solve (N1 = 8000) 0.896 0.961 0.968 0.927 37m39s 2.65GB
ONETA L-Solve (N1 = 9000) 0.924 0.981 0.987 0.950 52m52s 3.22GB
ONETA (No Approximation) 0.929 0.987 0.990 0.956 57m10s 3.42GB
Word Translation 0.751 0.884 0.916 0.812 n/a n/a
ESA (SQRT Normalization) 0.498 0.769 0.835 0.621 72s 284MB
LDA (K=1000) 0.287 0.568 0.659 0.417 4h12m 8.4GB
LSI (K=4000) 0.615 0.756 0.783 0.676 13h51m 19.7GB
ONETA + Word Translation 0.932 0.987 0.993 0.958 n/a n/a
</table>
<tableCaption confidence="0.999804">
Table 2: Result on large-scale mate-finding studies for English to Spanish matching
</tableCaption>
<bodyText confidence="0.999101285714286">
the similarity of the query document to all indexed
documents, we compute the value ranki indicating
at which position the mate of the ith document oc-
curs. We use two metrics: Top-k Precision, defined
as the percentage of documents for which the mate is
retrieved among the first k elements, and Minimum
Reciprocal Rank, defined as
</bodyText>
<equation confidence="0.811383333333333">
�
MRR =
iEtest
</equation>
<bodyText confidence="0.999853136363637">
For our experiments, we first extracted a subset
of documents (every 20th) from Wikipedia, filtering
this set down to only those that have aligned pages
in both English and Spanish with a minimum length
of 100 words. This gives us 10,369 aligned doc-
uments in total, which form the background docu-
ment collection B. We split this data into a training
and test set of 9,332 and 1,037 documents, respec-
tively. We then removed all words whose total fre-
quencies were below 50. This resulted in corpus of
6.7 millions words in English and 4.2 million words
in Spanish.
Normalization Methods: In order to investigate
the impact of different normalization methods, we
ran small-scale experiments using the first 500 doc-
uments from our dataset to train ONETA and then
evaluate the resulting models on the mate-finding
task on 100 unseen documents. The results are pre-
sented in Table 1, which shows the Top-1 Precision
for the different normalization methods. We see that
the effect of applying document normalization in
all cases improves the quality of the overall result.
Surprisingly, we do not see the same result for fre-
quency normalization yielding the best result for the
case where we do no normalization at all5 . In the re-
maining experiments we thus employ document nor-
malization and no term frequency normalization.
Approximation Methods: In order to evaluate the
different approximation methods, we experimen-
tally compare 4 different approximation methods:
standard LSI, ON-Eigen (Equation 1), Explicit LSI
(Equation 2), L-Solve (Equation 3) on the same
small-scale corpus. For convenience we plot an ap-
proximation rate which is either K or N1 depending
on method; at K = 500 and N1 = 500, these ap-
proximations become exact. This is shown in Figure
1. We also observe the effects of approximation and
see that the performance increases steadily as we
increase the computational factor. We see that the
orthonormal eigenvector (Equation 1) method and
the L-solve (Equation 3) method are clearly simi-
lar in approximation quality. We see that the explicit
LSI method (Equation 2) and the LSI method both
perform significantly worse for most of the approxi-
</bodyText>
<footnote confidence="0.802870333333333">
5A likely explanation for this is that low frequency terms are
less evenly distributed and the effect of calculating the matrix
inverse magnifies the noise from the low frequency terms
</footnote>
<figure confidence="0.4413245">
1
ranki
</figure>
<page confidence="0.974653">
1738
</page>
<bodyText confidence="0.999976530864198">
mation amounts. Explicit LSI is worse than the other
approximations as it first maps the test documents
into a K-dimensional LSI topic space, before map-
ping back into the N-dimensional explicit space. As
expected this performs worse than standard LSI for
all but high values of K as there is significant error
in both mappings. We also see that the (CL-)ESA
baseline, which is very low due to the small number
of documents, is improved upon by even the least ap-
proximation of orthonormalization. In the remain-
ing of this section, we report results using the L-
Solve method as it has a very good performance and
is computationally less expensive than ON-Eigen.
Evaluation and Comparison: We compare
ONETA using the L-Solve method with N1 values
from 1000 to 9000 topics with (CL-)ESA (using
SQRT normalization), LDA (using 1000 topics)
and LSI (using 4000 topics). We choose the largest
topic count for LSI and LDA we could to provide
the best possible comparison. For LSI, the choice of
K was determined on the basis of operating system
memory limits, while for LDA we experimented
with higher values for K without any performance
improvement, likely due to overfitting. We also
stress that for L-Solve ONETA, N1 is not the topic
count but an approximation rate of the mapping. In
all settings we use N topics as with standard ESA,
and so should not be considered directly comparable
to the K values of these methods.
We also compare to a baseline system that re-
lies on word-by-word translation, where we use the
most likely single translation of a word as given by a
phrase table generated by the Moses system (Koehn
et al., 2007) on the EuroParl corpus (Koehn, 2005).
Top 1, Top 5 and Top 10 Precision as well as Mean
Reciprocal Rank are reported in Table 2.
Interestingly, even for a small number of docu-
ments (e.g., N1 = 6000) our results improve both
the word-translation baseline as well as all other
topic models, ESA, LDA and LSI in particular. We
note that at this level the method is still efficiently
computable and calculating the inverse in practice
takes less time than training the Moses system. The
significance for results (N1 &gt; 7000) have been
tested by means of a bootstrap resampling signifi-
cance test, finding out that our results significantly
improve on the translation base line at a 99% level.
Further, we consider a straightforward combina-
tion of our method with the translation system con-
sisting of appending the topic vectors and the trans-
lation frequency vectors, weighted by the relative
average norms of the vectors. We see that in this
case the translations continue to improve the perfor-
mance of the system (albeit not significantly), sug-
gesting a clear potential for this system to help in im-
proving machine translation results. While we have
presented results for English and Spanish here, simi-
lar results were obtained for the German and French
case but are not presented here due to space limita-
tions.
In Table 2 we also include the user time and peak
resident memory of each of these processes, mea-
sured on an 8 Core Intel Xeon 2.50 GHz server.
We do not include the results for Word Translation
as many hours were spent learning a phrase table,
which includes translations for many phrases not in
the test set. We see that the ONETA method signif-
icantly outperforms LSI and LDA in terms of speed
and memory consumption. This is in line with the
theoretical calculations presented earlier where we
argued that inverting the N x N dense matrix XTX
when W » N is computationally lighter than find-
ing an eigendecomposition of the W x W sparse
matrix XXT. In addition, as we do not multiply
(XTX)−1 and XT, we do not need to allocate a
large W x K matrix in memory as with LSI and
LDA.
The implementations of ESA, ONETA,
LSI and LDA used as well as the data
for the experiments are available at
http://github.com/jmccrae/oneta.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991545454545">
We have presented a novel method for cross-lingual
topic modelling, which combines the strengths of
explicit and latent topic models and have demon-
strated its application to cross-lingual document
matching. We have in particular shown that the
method outperforms widely used topic models such
as Explicit Semantic Analysis (ESA), Latent Seman-
tic Indexing (LSI) and Latent Dirichlet Allocation
(LDA). Further, we have shown that it outperforms
a simple baseline relying on word-by-word transla-
tion of the query document into the target language,
</bodyText>
<page confidence="0.978101">
1739
</page>
<bodyText confidence="0.999968166666667">
while the induction of the model takes less time
than training the machine translation system from a
parallel corpus. We have also presented an effec-
tive approximation method, i.e. L-Solve, which sig-
nificantly reduces the computational cost associated
with computing the topic models.
</bodyText>
<sectionHeader confidence="0.99797" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993159666666667">
This work was funded by the Monnet Project
and the Portdial Project under the EC Sev-
enth Framework Programme, Grants No.
248458 and 296170. Roman Klinger has been
funded by the “Its OWL” project (“Intelli-
gent Technical Systems Ostwestfalen-Lippe”,
http://www.its-owl.de/), a leading-edge
cluster of the German Ministry of Education and
Research.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993423076923">
Dennis S Bernstein. 2005. Matrix mathematics, 2nd Edi-
tion. Princeton University Press Princeton.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In IJCAI, volume 9, pages 1513–1518.
Don Coppersmith and Shmuel Winograd. 1990. Matrix
multiplication via arithmetic progressions. Journal of
symbolic computation, 9(3):251–280.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Chris Ding, Tao Li, and Wei Peng. 2006. NMF and
PLSI: equivalence and a hybrid algorithm. In Pro-
ceedings of the 29th annual international ACM SIGIR,
pages 641–642. ACM.
Susan T Dumais, Todd A Letsche, Michael L Littman,
and Thomas K Landauer. 1997. Automatic cross-
language retrieval using latent semantic indexing. In
AAAI spring symposium on cross-language text and
speech retrieval, volume 15, page 21.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
volume 6, page 12.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference, pages 50–57. ACM.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 177–180. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Daniel D Lee and H Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788–791.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1138–1147. Association for
Computational Linguistics.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880–889. Association for
Computational Linguistics.
Martha Palmer, Owen Rambow, and Alexis Nasr. 1998.
Rapid prototyping of domain-specific machine trans-
lation systems. In Machine Translation and the Infor-
mation Soup, pages 95–102. Springer.
Philipp Sorg and Philipp Cimiano. 2008. Cross-lingual
information retrieval with explicit semantic analysis.
In Proceedings of the Cross-language Evaluation Fo-
rum 2008.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36–48. Springer.
Vassilis Spiliopoulos, George A Vouros, and Vangelis
Karkaletsis. 2007. Mapping ontologies elements us-
ing features in a latent space. In IEEE/WIC/ACM
International Conference on Web Intelligence, pages
457–460. IEEE.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187–207.
</reference>
<page confidence="0.988552">
1740
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438398">
<title confidence="0.999932">Orthonormal Explicit Topic Analysis for Cross-lingual Document Matching</title>
<author confidence="0.999995">Philip Philipp Cimiano Roman Klinger</author>
<affiliation confidence="0.997736">University Bielefeld University Bielefeld University Bielefeld</affiliation>
<address confidence="0.624235">Inspiration 1 Inspiration 1 Inspiration 1 Bielefeld, Germany Bielefeld, Germany Bielefeld,</address>
<abstract confidence="0.999269818181818">Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dennis S Bernstein</author>
</authors>
<title>Matrix mathematics, 2nd Edition.</title>
<date>2005</date>
<publisher>Princeton University Press Princeton.</publisher>
<contexts>
<context position="16401" citStr="Bernstein, 2005" startWordPosition="2818" endWordPosition="2819">ay that the terms occurring in the first N1 documents are enumerated 1, ... , W1. Let N2 = N − N1, W2 = W − W1. The result of this row permutation does not affect the value of XTX and we can write the matrix X as: X =( A B) 0 C /) where A is a W1 × N1 matrix representing term frequencies in the first N1 documents, B is a W1 × N2 matrix containing term frequencies in the remaining documents for terms that are also found in the first N1 documents, and C is a W2 ×N2 containing the frequency of all terms not found in the first N1 documents. Application of the well-known divide-andconquer formula (Bernstein, 2005, p. 159) for matrix inversion yields the following easily verifiable matrix identity, given that we can find C0 such that C0C = I. (3) We denote the above equation using a matrix L as LTX = I. We note that L =6 (XTX)−1X, but for any document vector d that is representable as a linear combination of the background document set (i.e., columns of X) we have that Ld = (XTX)−1XTd and in this sense L &apos; (XTX)−1XT. We further relax the assumption so that we only need to find a C0 such that C0C &apos; I. For this, we first observe that C is very sparse as it contains only terms not contained in the first N</context>
</contexts>
<marker>Bernstein, 2005</marker>
<rawString>Dennis S Bernstein. 2005. Matrix mathematics, 2nd Edition. Princeton University Press Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2436" citStr="Blei et al. (2003)" startWordPosition="365" endWordPosition="368">ual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting a background document collection that is aligned across languages, such as Wikipedia. A feature of ESA and its extension CL-ESA is that, in contrast to latent (e.g. LSI, Deerwester et al. (1990)) or generative topic models (such as LDA, Blei et al. (2003)), it requires no training and, nevertheless, has been demonstrated to outperform LSI and LDA on crosslingual retrieval tasks (Cimiano et al., 2009). A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that these topics cover the semantic space evenly and completely. In this paper, we present an alternative approach where we remap the semantic space defined by the topic</context>
<context position="6339" citStr="Blei et al. (2003)" startWordPosition="987" endWordPosition="990">al. (1997) then the topic model is essentially cross-lingual allowing to compare documents in different languages once they have been mapped into the topic space. Probabilistic or generative methods instead attempt to induce a (topic) model that has the highest likelihood of generating the documents actually observed during training. As with latent methods, these topics are thus interlingual and can generate words/terms in different languages. Prominent representatives of this type of method are Probabilistic Latent Semantic Indexing (PLSI, Hofmann (1999)) or Latent Dirichlet Allocation (LDA, Blei et al. (2003)), both of which can be straightforwardly extended to the cross-lingual case (Mimno et al., 2009). Explicit topic models make the assumption that topics are explicitly given instead of being induced from training data. Typically, a background document collection is assumed to be given whereby each document in this corpus corresponds to one topic. A mapping from document to topic space is calculated by computing the similarity of the document to every document in the topic space. A prominent example for this kind of topic modelling approach is Explicit Semantic Analysis (ESA, Gabrilovich and Ma</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit versus latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In IJCAI,</booktitle>
<volume>9</volume>
<pages>1513--1518</pages>
<contexts>
<context position="2584" citStr="Cimiano et al., 2009" startWordPosition="388" endWordPosition="391"> explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting a background document collection that is aligned across languages, such as Wikipedia. A feature of ESA and its extension CL-ESA is that, in contrast to latent (e.g. LSI, Deerwester et al. (1990)) or generative topic models (such as LDA, Blei et al. (2003)), it requires no training and, nevertheless, has been demonstrated to outperform LSI and LDA on crosslingual retrieval tasks (Cimiano et al., 2009). A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that these topics cover the semantic space evenly and completely. In this paper, we present an alternative approach where we remap the semantic space defined by the topics in such a manner that it is orthonormal. In this way, each document is mapped to a topic that is distinct from all other topics. Such a mapping ca</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit versus latent concept models for cross-language information retrieval. In IJCAI, volume 9, pages 1513–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Coppersmith</author>
<author>Shmuel Winograd</author>
</authors>
<title>Matrix multiplication via arithmetic progressions.</title>
<date>1990</date>
<journal>Journal of symbolic computation,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="14683" citStr="Coppersmith and Winograd, 1990" startWordPosition="2479" endWordPosition="2482">. 3.2 Approximations The computation of the inverse has a complexity that, using current practical algorithms, is approximately cubic and as such the time spent calculating the inverse can grow very quickly. There are several methods for obtaining an approximate inverse. The most commonly used are based on the SVD or eigendecomposition of the matrix. As XTX is symmetric positive definite, it holds that: XTX = UEUT Where U are the eigenvectors of XTX and E is a diagonal matrix of the eigenvalues. With UK,EK 3Algorithms with a = 2.3727 are known but practical algorithms have a = 2.807 or a = 3 (Coppersmith and Winograd, 1990) 1735 as the first K eigenvalues and eigenvectors, respectively, we have: C (ATA)−&apos;AT −(ATA)−&apos;ATBC&apos; A B l I 0 C&apos; / \ 0 C J (XTX)−1 &apos; UK�−1 K UT (1) K We call this the orthonormal eigenapproximation or ON-Eigen. The complexity of calculating (XTX)−1XT from this is O(N2K + Nn), where n is the number of non-zeros in X. Similarly, using the formula derived in the previous section we can derive an approximation of the full model as follows: (XTX)−1XT &apos; UK�−1 K VT (2) K We call this approximation Explicit LSI as it first maps into the latent topic space and then into the explicit topic space. We can</context>
</contexts>
<marker>Coppersmith, Winograd, 1990</marker>
<rawString>Don Coppersmith and Shmuel Winograd. 1990. Matrix multiplication via arithmetic progressions. Journal of symbolic computation, 9(3):251–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2375" citStr="Deerwester et al. (1990)" startWordPosition="353" endWordPosition="356">nalysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting a background document collection that is aligned across languages, such as Wikipedia. A feature of ESA and its extension CL-ESA is that, in contrast to latent (e.g. LSI, Deerwester et al. (1990)) or generative topic models (such as LDA, Blei et al. (2003)), it requires no training and, nevertheless, has been demonstrated to outperform LSI and LDA on crosslingual retrieval tasks (Cimiano et al., 2009). A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that these topics cover the semantic space evenly and completely. In this paper, we present an alternative ap</context>
<context position="5176" citStr="Deerwester et al. (1990)" startWordPosition="811" endWordPosition="814">ite natural and principled approach to tackle several tasks including the cross-lingual document retrieval problem. Topic modelling is the process of finding a representation of a document d in a lower dimensional space RK where each dimension corresponds to one topic that abstracts from specific words and thus allows us to detect deeper semantic similarities between documents beyond the computation of the pure overlap in terms of words. Three main variants of document models have been mainly considered for cross-lingual document matching: Latent methods such as Latent Semantic Indexing (LSI, Deerwester et al. (1990)) induce a decomposition of the term-document matrix in a way that reduces the dimensionality of the documents, while minimizing the error in reconstructing the training data. For example, in Latent Semantic Indexing, a term-document matrix is approximated by a partial singular value decomposition, or in Non-Negative Matrix Factorization (NMF, Lee and Seung (1999)) by two smaller non-negative matrices. If we append comparable or equivalent documents in multiple languages together before computing the decomposition as proposed by Dumais et al. (1997) then the topic model is essentially cross-li</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
</authors>
<title>NMF and PLSI: equivalence and a hybrid algorithm.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR,</booktitle>
<pages>641--642</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7114" citStr="Ding et al., 2006" startWordPosition="1117" endWordPosition="1120">explicitly given instead of being induced from training data. Typically, a background document collection is assumed to be given whereby each document in this corpus corresponds to one topic. A mapping from document to topic space is calculated by computing the similarity of the document to every document in the topic space. A prominent example for this kind of topic modelling approach is Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch (2007)). Both latent and generative topic models attempt to find topics from the data and it has been found that in some cases they are equivalent (Ding et al., 2006). However, this approach suffers from the problem that the topics might be artifacts of the training data rather than coherent semantic topics. In contrast, explicit topic methods can use a set of topics that are chosen to be well-suited to the domain. The principle drawback of this is that the method for choosing such explicit topics by selecting documents is comparatively crude. In general, these topics may be overlapping and poorly distributed over the semantic topic space. By comparison, our method takes the advantage of the pre-specified topics of explicit topic models, but incorporates a</context>
</contexts>
<marker>Ding, Li, Peng, 2006</marker>
<rawString>Chris Ding, Tao Li, and Wei Peng. 2006. NMF and PLSI: equivalence and a hybrid algorithm. In Proceedings of the 29th annual international ACM SIGIR, pages 641–642. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Todd A Letsche</author>
<author>Michael L Littman</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic crosslanguage retrieval using latent semantic indexing. In AAAI spring symposium on cross-language text and speech retrieval,</title>
<date>1997</date>
<volume>15</volume>
<pages>21</pages>
<contexts>
<context position="5731" citStr="Dumais et al. (1997)" startWordPosition="895" endWordPosition="898">s such as Latent Semantic Indexing (LSI, Deerwester et al. (1990)) induce a decomposition of the term-document matrix in a way that reduces the dimensionality of the documents, while minimizing the error in reconstructing the training data. For example, in Latent Semantic Indexing, a term-document matrix is approximated by a partial singular value decomposition, or in Non-Negative Matrix Factorization (NMF, Lee and Seung (1999)) by two smaller non-negative matrices. If we append comparable or equivalent documents in multiple languages together before computing the decomposition as proposed by Dumais et al. (1997) then the topic model is essentially cross-lingual allowing to compare documents in different languages once they have been mapped into the topic space. Probabilistic or generative methods instead attempt to induce a (topic) model that has the highest likelihood of generating the documents actually observed during training. As with latent methods, these topics are thus interlingual and can generate words/terms in different languages. Prominent representatives of this type of method are Probabilistic Latent Semantic Indexing (PLSI, Hofmann (1999)) or Latent Dirichlet Allocation (LDA, Blei et al</context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>Susan T Dumais, Todd A Letsche, Michael L Littman, and Thomas K Landauer. 1997. Automatic crosslanguage retrieval using latent semantic indexing. In AAAI spring symposium on cross-language text and speech retrieval, volume 15, page 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>6</volume>
<pages>12</pages>
<contexts>
<context position="1798" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="261" endWordPosition="264">d ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting a background document collection that is aligned across languages, such as Wikipedia. A feature of ESA and its extension CL-ESA is that, in contrast to latent (e.g. LSI, Deerwester et al. (1990)) or generative topic m</context>
<context position="6954" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="1088" endWordPosition="1091">Blei et al. (2003)), both of which can be straightforwardly extended to the cross-lingual case (Mimno et al., 2009). Explicit topic models make the assumption that topics are explicitly given instead of being induced from training data. Typically, a background document collection is assumed to be given whereby each document in this corpus corresponds to one topic. A mapping from document to topic space is calculated by computing the similarity of the document to every document in the topic space. A prominent example for this kind of topic modelling approach is Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch (2007)). Both latent and generative topic models attempt to find topics from the data and it has been found that in some cases they are equivalent (Ding et al., 2006). However, this approach suffers from the problem that the topics might be artifacts of the training data rather than coherent semantic topics. In contrast, explicit topic methods can use a set of topics that are chosen to be well-suited to the domain. The principle drawback of this is that the method for choosing such explicit topics by selecting documents is comparatively crude. In general, these topics may be overlapping and poorly d</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, volume 6, page 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6282" citStr="Hofmann (1999)" startWordPosition="980" endWordPosition="981">computing the decomposition as proposed by Dumais et al. (1997) then the topic model is essentially cross-lingual allowing to compare documents in different languages once they have been mapped into the topic space. Probabilistic or generative methods instead attempt to induce a (topic) model that has the highest likelihood of generating the documents actually observed during training. As with latent methods, these topics are thus interlingual and can generate words/terms in different languages. Prominent representatives of this type of method are Probabilistic Latent Semantic Indexing (PLSI, Hofmann (1999)) or Latent Dirichlet Allocation (LDA, Blei et al. (2003)), both of which can be straightforwardly extended to the cross-lingual case (Mimno et al., 2009). Explicit topic models make the assumption that topics are explicitly given instead of being induced from training data. Typically, a background document collection is assumed to be given whereby each document in this corpus corresponds to one topic. A mapping from document to topic space is calculated by computing the similarity of the document to every document in the topic space. A prominent example for this kind of topic modelling approa</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference, pages 50–57. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26613" citStr="Koehn et al., 2007" startWordPosition="4648" endWordPosition="4651">asis of operating system memory limits, while for LDA we experimented with higher values for K without any performance improvement, likely due to overfitting. We also stress that for L-Solve ONETA, N1 is not the topic count but an approximation rate of the mapping. In all settings we use N topics as with standard ESA, and so should not be considered directly comparable to the K values of these methods. We also compare to a baseline system that relies on word-by-word translation, where we use the most likely single translation of a word as given by a phrase table generated by the Moses system (Koehn et al., 2007) on the EuroParl corpus (Koehn, 2005). Top 1, Top 5 and Top 10 Precision as well as Mean Reciprocal Rank are reported in Table 2. Interestingly, even for a small number of documents (e.g., N1 = 6000) our results improve both the word-translation baseline as well as all other topic models, ESA, LDA and LSI in particular. We note that at this level the method is still efficiently computable and calculating the inverse in practice takes less time than training the Moses system. The significance for results (N1 &gt; 7000) have been tested by means of a bootstrap resampling significance test, finding </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<contexts>
<context position="26650" citStr="Koehn, 2005" startWordPosition="4656" endWordPosition="4657">e for LDA we experimented with higher values for K without any performance improvement, likely due to overfitting. We also stress that for L-Solve ONETA, N1 is not the topic count but an approximation rate of the mapping. In all settings we use N topics as with standard ESA, and so should not be considered directly comparable to the K values of these methods. We also compare to a baseline system that relies on word-by-word translation, where we use the most likely single translation of a word as given by a phrase table generated by the Moses system (Koehn et al., 2007) on the EuroParl corpus (Koehn, 2005). Top 1, Top 5 and Top 10 Precision as well as Mean Reciprocal Rank are reported in Table 2. Interestingly, even for a small number of documents (e.g., N1 = 6000) our results improve both the word-translation baseline as well as all other topic models, ESA, LDA and LSI in particular. We note that at this level the method is still efficiently computable and calculating the inverse in practice takes less time than training the Moses system. The significance for results (N1 &gt; 7000) have been tested by means of a bootstrap resampling significance test, finding out that our results significantly im</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<volume>401</volume>
<issue>6755</issue>
<contexts>
<context position="5542" citStr="Lee and Seung (1999)" startWordPosition="867" endWordPosition="870">documents beyond the computation of the pure overlap in terms of words. Three main variants of document models have been mainly considered for cross-lingual document matching: Latent methods such as Latent Semantic Indexing (LSI, Deerwester et al. (1990)) induce a decomposition of the term-document matrix in a way that reduces the dimensionality of the documents, while minimizing the error in reconstructing the training data. For example, in Latent Semantic Indexing, a term-document matrix is approximated by a partial singular value decomposition, or in Non-Negative Matrix Factorization (NMF, Lee and Seung (1999)) by two smaller non-negative matrices. If we append comparable or equivalent documents in multiple languages together before computing the decomposition as proposed by Dumais et al. (1997) then the topic model is essentially cross-lingual allowing to compare documents in different languages once they have been mapped into the topic space. Probabilistic or generative methods instead attempt to induce a (topic) model that has the highest likelihood of generating the documents actually observed during training. As with latent methods, these topics are thus interlingual and can generate words/ter</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Daniel D Lee and H Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1138--1147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1581" citStr="Li et al., 2010" startWordPosition="230" endWordPosition="233">ese. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting </context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>880--889</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6436" citStr="Mimno et al., 2009" startWordPosition="1002" endWordPosition="1005">fferent languages once they have been mapped into the topic space. Probabilistic or generative methods instead attempt to induce a (topic) model that has the highest likelihood of generating the documents actually observed during training. As with latent methods, these topics are thus interlingual and can generate words/terms in different languages. Prominent representatives of this type of method are Probabilistic Latent Semantic Indexing (PLSI, Hofmann (1999)) or Latent Dirichlet Allocation (LDA, Blei et al. (2003)), both of which can be straightforwardly extended to the cross-lingual case (Mimno et al., 2009). Explicit topic models make the assumption that topics are explicitly given instead of being induced from training data. Typically, a background document collection is assumed to be given whereby each document in this corpus corresponds to one topic. A mapping from document to topic space is calculated by computing the similarity of the document to every document in the topic space. A prominent example for this kind of topic modelling approach is Explicit Semantic Analysis (ESA, Gabrilovich and Markovitch (2007)). Both latent and generative topic models attempt to find topics from the data an</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M Wallach, Jason Naradowsky, David A Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Alexis Nasr</author>
</authors>
<title>Rapid prototyping of domain-specific machine translation systems.</title>
<date>1998</date>
<booktitle>In Machine Translation and the Information Soup,</booktitle>
<pages>95--102</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1517" citStr="Palmer et al., 1998" startWordPosition="219" endWordPosition="222">citly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entr</context>
</contexts>
<marker>Palmer, Rambow, Nasr, 1998</marker>
<rawString>Martha Palmer, Owen Rambow, and Alexis Nasr. 1998. Rapid prototyping of domain-specific machine translation systems. In Machine Translation and the Information Soup, pages 95–102. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Sorg</author>
<author>Philipp Cimiano</author>
</authors>
<title>Cross-lingual information retrieval with explicit semantic analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Cross-language Evaluation Forum</booktitle>
<contexts>
<context position="1863" citStr="Sorg and Cimiano, 2008" startWordPosition="270" endWordPosition="273">the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends this to the multilingual case by exploiting a background document collection that is aligned across languages, such as Wikipedia. A feature of ESA and its extension CL-ESA is that, in contrast to latent (e.g. LSI, Deerwester et al. (1990)) or generative topic models (such as LDA, Blei et al. (2003)), it requires no training </context>
</contexts>
<marker>Sorg, Cimiano, 2008</marker>
<rawString>Philipp Sorg and Philipp Cimiano. 2008. Cross-lingual information retrieval with explicit semantic analysis. In Proceedings of the Cross-language Evaluation Forum 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Sorg</author>
<author>Philipp Cimiano</author>
</authors>
<title>An experimental comparison of explicit semantic analysis implementations for cross-language retrieval.</title>
<date>2010</date>
<booktitle>In Natural Language Processing and Information Systems,</booktitle>
<pages>36--48</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8412" citStr="Sorg and Cimiano, 2010" startWordPosition="1331" endWordPosition="1334">normal explicit topic analysis Our approach follows Explicit Semantic Analysis in the sense that it assumes the availability of a background document collection B = {b1, b2,..., bN} consisting of textual representations. The mapping into the explicit topic space is defined by a language-specific function Φ that maps documents into RN such that the jth value in the vector is given by some association measure Oj(d) for each background document bj. Typical choices for this association measure 0 are the sum of the TF-IDF scores or an information retrieval relevance scoring function such as BM-25 (Sorg and Cimiano, 2010). For the case of TF-IDF, the value of the j-th element of the topic vector is given by: Oj(d) = tf-idf(bj)T −−−� −−−� tf-idf(d) Thus, the mapping function can be represented as the product of a TF-IDF vector of document d multiplied by an W xN matrix, X, each element of which contains the TF-IDF value of word i in document bj: tf-idf(d) = XT · −−−→ −−−→ tf-idf(d) For simplicity, we shall assume from this point on that all vectors are already converted to a TF-IDF or similar numeric vector form. In order to compute the similarity between two documents di and dj, typically the cosine-function (</context>
</contexts>
<marker>Sorg, Cimiano, 2010</marker>
<rawString>Philipp Sorg and Philipp Cimiano. 2010. An experimental comparison of explicit semantic analysis implementations for cross-language retrieval. In Natural Language Processing and Information Systems, pages 36–48. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassilis Spiliopoulos</author>
</authors>
<title>George A Vouros, and Vangelis Karkaletsis.</title>
<date>2007</date>
<booktitle>In IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>457--460</pages>
<publisher>IEEE.</publisher>
<marker>Spiliopoulos, 2007</marker>
<rawString>Vassilis Spiliopoulos, George A Vouros, and Vangelis Karkaletsis. 2007. Mapping ontologies elements using features in a latent space. In IEEE/WIC/ACM International Conference on Web Intelligence, pages 457–460. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1536" citStr="Tam et al., 2007" startWordPosition="223" endWordPosition="226"> but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension CL-ESA (Sorg and Cimiano, 2008). ESA indexes documents by mapping them into a topic space defined by their similarity to predefined explicit topics – generally articles from an encyclopaedia – in such a way that there is a one-to-one correspondence between topics and encyclopedic entries. CL-ESA extends</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual LSA-based adaptation for statistical machine translation. Machine Translation, 21(4):187–207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
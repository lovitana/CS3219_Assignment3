<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.999505">
A Weakly Supervised Model for Sentence-Level Semantic Orientation
Analysis with Multiple Experts
</title>
<author confidence="0.967215">
Lizhen Qu and Rainer Gemulla and Gerhard Weikum
</author>
<affiliation confidence="0.923329">
Max Planck Institute for Informatics
</affiliation>
<address confidence="0.580341">
Saarbr¨ucken, Germany
</address>
<email confidence="0.81182">
{lqu,rgemulla,weikum}@mpi-inf.mpg.de
</email>
<sectionHeader confidence="0.992044" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985367695652174">
We propose the weakly supervised Multi-
Experts Model (MEM) for analyzing the se-
mantic orientation of opinions expressed in
natural language reviews. In contrast to most
prior work, MEM predicts both opinion po-
larity and opinion strength at the level of in-
dividual sentences; such fine-grained analysis
helps to understand better why users like or
dislike the entity under review. A key chal-
lenge in this setting is that it is hard to ob-
tain sentence-level training data for both po-
larity and strength. For this reason, MEM is
weakly supervised: It starts with potentially
noisy indicators obtained from coarse-grained
training data (i.e., document-level ratings), a
small set of diverse base predictors, and, if
available, small amounts of fine-grained train-
ing data. We integrate these noisy indicators
into a unified probabilistic framework using
ideas from ensemble learning and graph-based
semi-supervised learning. Our experiments in-
dicate that MEM outperforms state-of-the-art
methods by a significant margin.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953463414634">
Opinion mining is concerned with analyzing opin-
ions expressed in natural language text. For example,
many internet websites allow their users to provide
both natural language reviews and numerical ratings
to items of interest (such as products or movies).
In this context, opinion mining aims to uncover the
relationship between users and (features of) items.
Preferences of users to items can be well understood
by coarse-grained methods of opinion mining, which
focus on analyzing the semantic orientation of doc-
uments as a whole. To understand why users like or
dislike certain items, however, we need to perform
more fine-grained analysis of the review text itself.
In this paper, we focus on sentence-level analy-
sis of semantic orientation (SO) in online reviews.
The SO consists of polarity (positive, negative, or
other1) and strength (degree to which a sentence is
positive or negative). Both quantities can be ana-
lyzed jointly by mapping them to numerical ratings:
Large negative/positive ratings indicate a strong neg-
ative/positive orientation. A key challenge in fine-
grained rating prediction is that fine-grained train-
ing data for both polarity and strength is hard to
obtain. We thus focus on a weakly supervised set-
ting in which only coarse-level training data (such
as document ratings and subjectivity lexicons) and,
optionally, a small amount of fine-grained training
data (such as sentence polarities) is available.
A number of lexicon-based approaches for phrase-
level rating prediction has been proposed in the liter-
ature (Taboada et al., 2011; Qu et al., 2010). These
methods utilize a subjectivity lexicon of words along
with information about their semantic orientation;
they focus on phrases that contain words from the
lexicon. A key advantage of sentence-level methods
is that they are able to cover all sentences in a review
and that phrase identification is avoided. To the best
of our knowledge, the problem of rating prediction
at the sentence level has not been addressed in the
literature. A naive approach would be to simply aver-
age phrase-level ratings. Such an approach performs
</bodyText>
<footnote confidence="0.922271">
1We assign polarity other to text fragments that are off-topic
or not directly related to the entity under review.
</footnote>
<page confidence="0.956677">
149
</page>
<note confidence="0.787823">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 149–159, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997735875">
poorly, however, since (1) phrases are analyzed out
of context (e.g., modal verbs or conditional clauses),
(2) domain-dependent information about semantic
orientation is not captured in the lexicons, (3) only
phrases that contain lexicon words are covered. Here
(1) and (2) lead to low precision, (3) to low recall.
To address the challenges outlined above, we pro-
pose the weakly supervised Multi-Experts Model
(MEM) for sentence-level rating prediction. MEM
starts with a set of potentially noisy indicators of SO
including phrase-level predictions, language heuris-
tics, and co-occurrence counts. We refer to these
indicators as base predictors; they constitute the set
of experts used in our model. MEM is designed
such that new base predictors can be easily integrated.
Since the information provided by the base predictors
can be contradicting, we use ideas from ensemble
learning (Dietterichl, 2002) to learn the most con-
fident indicators and to exploit domain-dependent
information revealed by document ratings. Thus, in-
stead of averaging base predictors, MEM integrates
their features along with the available coarse-grained
training data into a unified probabilistic model.
The integrated model can be regarded as a Gaus-
sian process (GP) model (Rasmussen, 2004) with
a novel multi-expert prior. The multi-expert prior
decomposes into two component distributions. The
first component distribution integrates sentence-local
information obtained from the base predictors. It
forms a special realization of stacking (Dzeroski and
Zenko, 2004) but uses the features from the base pre-
dictors instead of the actual predictions. The second
component distribution propagates SO information
across similar sentences using techniques from graph-
based semi-supervised learning (GSSL) (Zhu et al.,
2003; Belkin et al., 2006). It aims to improve the
predictions on sentences that are not covered well
enough by our base predictors. Traditional GSSL al-
gorithms support either discrete labels (classification)
or numerical labels (regression); we extend these
techniques to support both types of labels simulta-
neously. We use a novel variant of word sequence
kernels (Cancedda et al., 2003) to measure sentence
similarity. Our kernel takes the relative positions of
words but also their SO and synonymity into account.
Our experiments indicate that MEM significantly
outperforms prior work in both sentence-level rating
prediction and sentence-level polarity classification.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99990345">
There exists a large body of work on analyzing the
semantic orientation of natural language text. Our
approach is unique in that it is weakly supervised,
predicts both polarity and strength, and operates on
the sentence level.
Supervised approaches for sentiment analysis fo-
cus mainly on opinion mining at the document
level (Pang and Lee, 2004; Pang et al., 2002; Pang
and Lee, 2005; Goldberg and Zhu, 2006), but have
also been applied to sentence-level polarity classifi-
cation in specific domains (Mao and Lebanon, 2006;
Pang and Lee, 2004; McDonald et al., 2007). In
these settings, a sufficient amount of training data is
available. In contrast, we focus on opinion mining
tasks with little or no fine-grained training data.
The weakly supervised HCRF model (T¨ackstr¨om
and McDonald, 2011b; T¨ackstr¨om and McDonald,
2011a) for sentence-level polarity classification is per-
haps closest to our work in spirit. Similar to MEM,
HCRF uses coarse-grained training data and, when
available, a small amount of fine-grained sentence
polarities. In contrast to MEM, HCRF does not pre-
dict the strength of semantic orientation and ignores
the order of words within sentences.
There exists a large number of lexicon-based meth-
ods for polarity classification (Ding et al., 2008; Choi
and Cardie, 2009; Hu and Liu, 2004; Zhuang et al.,
2006; Fu and Wang, 2010; Ku et al., 2008). The
lexicon-based methods of (Taboada et al., 2011; Qu
et al., 2010) also predict ratings at the phrase level;
these methods are used as experts in our model.
MEM leverages ideas from ensemble learning (Di-
etterichl, 2002; Bishop, 2006) and GSSL meth-
ods (Zhu et al., 2003; Zhu and Ghahramani, 2002;
Chapelle et al., 2006; Belkin et al., 2006). We extend
GSSL with support for multiple, heterogenous labels.
This allows us to integrate our base predictors as well
as the available training data into a unified model
that exploits that strengths of algorithms from both
families.
</bodyText>
<sectionHeader confidence="0.950512" genericHeader="method">
3 Base Predictors
</sectionHeader>
<bodyText confidence="0.999884">
Each of our base predictors predicts the polarity or
the rating of a single phrase. As indicated above,
we do not use these predictions directly in MEM but
instead integrate the features of the base predictors
</bodyText>
<page confidence="0.99623">
150
</page>
<bodyText confidence="0.9996879375">
(see Sec. 4.4). MEM is designed such that new base
predictors can be integrated easily.
Our base predictors use a diverse set of available
web and linguistic resources. The hope is that this di-
versity increases overall prediction performance (Di-
etterichl, 2002): The statistical polarity predictor fo-
cuses on local syntactic patterns; it is based on corpus
statistics for SO-carrying words and opinion topic
words. The heuristic polarity predictor uses manu-
ally constructed rules to achieve high precision but
low recall. Both the bag-of-opinions rating predictor
and the SO-CAL rating predictor are based on lexi-
cons. The BoO predictor uses a lexicon trained from
a large generic-domain corpus and is recall-oriented;
the SO-CAL predictor uses a different lexicon with
manually assigned weights and is precision-oriented.
</bodyText>
<subsectionHeader confidence="0.999662">
3.1 Statistical Polarity Predictor
</subsectionHeader>
<bodyText confidence="0.99091646">
The polarity of an SO-carrying word strongly de-
pends on its target word. For example, consider the
phrase “I began this novel with the greatest of hopes
[...]”. Here, “greatest” has a positive semantic orien-
tation in all subjectivity lexicons, but the combination
“greatest of hopes” often indicates a negative senti-
ment. We refer to a pair of SO-carrying word (“great-
est”) and a target word (“hopes”) as an opinion-target
pair. Our statistical polarity predictor learns the po-
larity of opinions and targets jointly, which increases
the robustness of its predictions.
Syntactic dependency relations of the form
A R −→ B are a strong indicator for opinion-target
pairs (Qiu et al., 2009; Zhuang et al., 2006); e.g.,
“great” nmod −−−→“product”. To achieve high precision,
we only consider pairs connected by the follow-
ing predefined set of shortest dependency paths:
verbsubj ←−−noun, verb obj
←− noun, adj nmod −−−→ noun,
adj prd −−→ verbsubj ←−−noun. We only retain opinion-
target pairs that are sufficiently frequent.
For each extracted pair z, we count how often it
co-occurs with each document polarity y ∈ Y, where
Y = {positive, negative, other} denotes the set of po-
larities. If z occurs in a document but is preceded by
a negator, we treat it as a co-occurrence of opposite
document polarity. If z occurs in a document with po-
larity other, we count the occurrence with only half
weight, i.e., we increase both #z and #(other, z)
by 0.5. These documents are typically a mixture of
positive and negative opinions so that we want to
reduce their impact. The marginal distribution of
polarity label y given that z occurs in a sentence is
estimated as P(y  |z) = #(y, z)/#z. The predictor
is trained using the text and ratings of the reviews in
the training data, i.e., without relying on fine-grained
annotations.
The statistical polarity predictor can be used to pre-
dict sentence-level polarities by averaging the phrase-
level predictions. As discussed previously, such an
approach is problematic; we use it as a baseline ap-
proach in our experimental study. We also employ
phrase-level averaging to estimate the variance of
base predictors; see Sec. 4.3. Denote by Z(x) the set
of opinion-target pairs in sentence x. To predict the
sentence polarity y ∈ Y, we take the Bayesian aver-
age of the phrase-level predictors: P(y  |Z(x)) =
EZEZ(X) P(y  |z)P(z) = EZEZ(X) P(y, z). Thus
the most likely polarity is the one with the highest
co-occurrence count.
</bodyText>
<subsectionHeader confidence="0.999801">
3.2 Heuristic Polarity Predictor
</subsectionHeader>
<bodyText confidence="0.999925">
Heuristic patterns can also serve as base predictors.
In particular, we found that some authors list positive
and negative aspects separately after keywords such
as “pros” and “cons”. A heuristic that exploits such
patterns achieved a high precision (&gt; 90%) but low
recall (&lt; 5%) in our experiments.
</bodyText>
<subsectionHeader confidence="0.999937">
3.3 Bag-of-Opinions Rating Predictor
</subsectionHeader>
<bodyText confidence="0.999990166666667">
We leverage the bag-of-opinion (BoO) model of Qu et
al. (2010) as a base predictor for phrase-level ratings.
The BoO model was trained from a large generic
corpus without fine-grained annotations.
In BoO, an opinion consists of three components:
an SO-carrying word (e.g., “good”), a set of intensi-
fiers (e.g., “very”) and a set of negators (e.g., “not”).
Each opinion is scored based on these words (repre-
sented as a boolean vector b) and the polarity of the
SO-carrying word (represented as sgn(r) ∈ {−1,1})
as indicated by the MPQA lexicon of Wilson et
al. (2005). In particular, the score is computed as
sgn(r)wTb, where w is the learned weight vector.
The sign function sgn(r) ensures consistent weight
assignment for intensifiers and negators. For exam-
ple, an intensifier like “very” can obtain a large posi-
tive or a large negative weight depending on whether
it is used with a positive or negative SO-carrying
</bodyText>
<page confidence="0.992104">
151
</page>
<bodyText confidence="0.939389">
word, respectively.
</bodyText>
<subsectionHeader confidence="0.992463">
3.4 SO-CAL Rating Predictor
</subsectionHeader>
<bodyText confidence="0.999966916666667">
The Semantic Orientation Calculator (SO-CAL) of
Taboada et al. (2011) also predicts phrase-level rat-
ings via a scoring function similar to the one of BoO.
The SO-CAL predictor uses a manually created lexi-
con, in which each word is classified as either an SO-
carrying word (associated with a numerical score), an
intensifier (associated with a modifier on the numer-
ical score), or a negator. SO-CAL employs various
heuristics to detect irrealis and to correct for the pos-
itive bias inherent in most lexicon-based classifiers.
Compared to BoO, SO-CAL has lower recall but
higher precision.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="method">
4 Multi-Experts Model
</sectionHeader>
<bodyText confidence="0.999952285714286">
Our multi-experts model incorporates features from
the individual base predictors, coarse-grained labels
(i.e., document ratings or polarities), similarities be-
tween sentences, and optionally a small amount of
sentence polarity labels into an unified probabilistic
model. We first give an overview of MEM, and then
describe its components in detail.
</bodyText>
<subsectionHeader confidence="0.999184">
4.1 Model Overview
</subsectionHeader>
<bodyText confidence="0.988082772727273">
Denote by X = {x1, ... , xN} a set of sentences.
We associate each sentence xi with a set of initial
labels yi, which are strong indicators of semantic
orientation: the coarse-grained rating of the corre-
sponding document, the polarity label of our heuristic
polarity predictor, the phrase-level ratings from the
SO-CAL predictor, and optionally a manual polarity
label. Note that the number of initial labels may vary
from sentence to sentence and that initial labels are
heterogeneous in that they refer to either polarities
or ratings. Let Y = {y1, ... , yN}. Our goal is to
predict the unobserved ratings r = {r1, ... , rN} of
each sentence.
Our multi-expert model is a probabilistic model
for X, Y, and r. In particular, we model the rating
vector r via a multi-expert prior PE(r  |X,β) with
parameter β (Sec. 4.2). PE integrates both features
from the base predictors and sentence similarities.
We correlate ratings to initial labels via a set of con-
ditional distributions Pb(yb  |r), where b denotes the
type of initial label (Sec. 4.3). The posterior of r is
then given by
</bodyText>
<equation confidence="0.798678">
P(r  |X, ���, β) ∝ Pb(Yb  |r)PE(r  |X,β).
b
</equation>
<bodyText confidence="0.999951">
Note that the posterior is influenced by both the multi-
expert prior and the set of initial labels.
We use MAP inference to obtain the most likely
rating of each sentence, i.e., we solve
</bodyText>
<equation confidence="0.986377">
log(Pb(�yb  |r)) − log(PE(r  |X,β)),
</equation>
<bodyText confidence="0.999986666666667">
where as before β denotes the model parameters. We
solve the above optimization problem using cyclic
coordinate descent (Friedman et al., 2008).
</bodyText>
<subsectionHeader confidence="0.939578">
4.2 Multi-Expert Prior
</subsectionHeader>
<bodyText confidence="0.999993709677419">
The multi-expert prior PE(r  |X,β) consists of two
component distributions N1 and N2. Distribution
N1 integrates features from the base predictors, N2
incorporates sentence similarities to propagate infor-
mation across sentences.
In a slight abuse of notation, denote by xi the set of
features for the i-th sentence. Vector xi contains the
features of all the base predictors but also includes bi-
gram features for increased coverage of syntactic pat-
terns; see Sec. 4.4 for details about the feature design.
Let m(xi) = βTxi be a linear predictor for ri, where
β is a real weight vector. Assuming Gaussian noise,
ri follows a Gaussian distribution N1(ri  |mi, a2)
with mean mi = m(xi) and variance a2. Note that
predictor m can be regarded as a linear combination
of base predictors because both m and each of the
base predictors are linear functions. By integrating
all features into a single function, the base predictors
are trained jointly so that weight vector β automati-
cally adapts to domain-dependent properties of the
data. This integrated approach significantly outper-
formed the alternative approach of using a weighted
vote of the individual predictions made by the base
predictors. We regularize the weight vector β us-
ing a Laplace prior P(β  |a) with parameter a to
encourage sparsity.
Note that the bigram features in xi partially cap-
ture sentence similarity. However, such features can-
not be extended to longer subsequences such as tri-
grams due to data sparsity: useful features become
as infrequent as noisy terms. Moreover, we would
</bodyText>
<figure confidence="0.4925495">
argmin 1: −
r,β b
</figure>
<page confidence="0.985184">
152
</page>
<bodyText confidence="0.999707789473684">
like to capture sentence similarity using gapped (i.e.,
non-consecutive) subsequences. For example, the
sentences “The book is an easy read.” and “It is easy
to read.” are similar but do not share any consecutive
bigrams. They do share the subsequence “easy read”,
however. To capture this similarity, we make use of a
novel sentiment-augmented variant of word sequence
kernels (Cancedda et al., 2003). Our kernel is used
to construct a similarity matrix W among sentences
and the corresponding regularized Laplacian eL. To
capture the intuition that similar sentences should
have similar ratings, we introduce a Gaussian prior
N2(r  |0, eL−1) as a component into our multi-expert
prior; see Sec. 4.5 for details and a discussion of
why this prior encourages similar ratings for similar
sentences.
Since the two component distributions feature dif-
ferent expertise, we take their product and obtain the
multi-expert prior
</bodyText>
<equation confidence="0.977568">
PE(r  |X,,3) ∝ N1(r  |m, Iσ2)N2(r  |0,eL−1)P(p  |α),
</equation>
<bodyText confidence="0.999870333333333">
where m = (m1, ... , mN). Note that the normal-
izing constant of PE can be ignored during MAP
inference since it does not depend on ,3.
</bodyText>
<subsectionHeader confidence="0.997176">
4.3 Incorporating Initial Labels
</subsectionHeader>
<bodyText confidence="0.988927533333333">
Recall that the initial labels Y are strong indica-
tors of semantic orientation associated with each
sentence; they correspond to either discrete polarity
labels or to continuous rating labels. This hetero-
geneity constitutes the main difficulty for incorporat-
ing the initial labels via the conditional distributions
Pb(ˆyb  |r). We assume independence throughout so
that Pb(ˆyb  |r) = Qi Pb(ˆybi  |ri).
Rating Labels For continuous labels, we assume
Gaussian noise and set Pb(ˆybi  |ri) = N(ˆybi  |ri, ηb i ),
where variance ηbi is a type- and sentence-dependent.
For SO-CAL labels, we simply set ηSO-CAL =
i
ηSO-CAL, where ηSO-CAL is a hyperparameter. The
SO-CAL scores have limited influence in our overall
model; we found that more complex designs lead to
little improvement. We proceed differently for docu-
ment ratings. Our experiment suggests that document
ratings constitute the most important indicator of the
SO of a sentence. Thus sentence ratings should be
close to document ratings unless strong evidence to
the contrary exists. In other words, we want variance
ηDoc ito be small.
When no manually created sentence-level polar-
ity labels are available, we set the value ofηDoc
i de-
pending on the polarity class. In particular, we set
ηDoc i= 1 for both positive and negative documents,
and ηDoc
i = 2 for neutral documents. The reasoning
behind this choice is that sentence ratings in neu-
tral documents express higher variance because these
documents often contain a mixture of positive and
negative sentences.
When a small set of manually created sentence
polarity labels is available, we train a classifier that
predicts whether the sentence polarity coincides with
the document polarity. If so, we set the corresponding
variance ηDoc
i to a small value; otherwise, we choose
a larger value. In particular, we train a logistic regres-
sion classifier (Bishop, 2006) using the following
binary features: (1) an indicator variable for each
document polarity, and (2) an indicator variable for
each triple of base predictor, predicted polarity, and
document polarity (set to 1 if the polarities match).
We then setηDoc
i = (τpi)−1, where pi is the probabil-
ity of matching polarities obtained from the classifier
and τ is a hyperparameter that ensures correct scal-
ing.
Polarity Labels We now describe how to model
the correlation between the polarity of a sentence and
its rating. An simple and effective approach is to
partition the range of ratings into three consecutive
partitions, one for each polarity class. We thus consid-
ering the polarity classes {positive, other, negative}
as ordered and formulate polarity classification as an
ordinal regression problem (Chu and Ghahramani,
2006). We immediately obtain the distribution
</bodyText>
<equation confidence="0.9817475">
ri − b+ !
Pb(ˆyb i = pos  |ri) = Φp
ηb
! b− − ri !
b+ − ri
Pb(ˆyb i = oth  |ri) = Φ p − Φ p
ηb ηb
b− − ri !
Pb(ˆybi = neg  |ri) = Φ p,
ηb
</equation>
<bodyText confidence="0.999378">
where b+ and b− are the partition boundaries between
positive/other and other/negative, respectively,2 Φ(x)
denotes the cumulative distribution function of the
</bodyText>
<footnote confidence="0.8085865">
2We set b+ = 0.3 and b− = −0.3 to calibrate to SO-CAL,
which treats ratings in [−0.3, 0, 3] as polarity other.
</footnote>
<page confidence="0.996302">
153
</page>
<figureCaption confidence="0.84198925">
Figure 1: Distribution of polarity given rating.
Gaussian distribution, and variance 77b is a hyper-
parameter. It is easy to verify that Ei|
yb� ,Y p(yb
</figureCaption>
<bodyText confidence="0.97057425">
ri) = 1. The resulting distribution is shown in Fig. 1.
We can use the same distribution to use MEM for
sentence-level polarity classification; in this case, we
pick the polarity with the highest probability.
</bodyText>
<subsectionHeader confidence="0.994795">
4.4 Incorporating Base Predictors
</subsectionHeader>
<bodyText confidence="0.999992642857143">
Base predictors are integrated into MEM via compo-
nent N1(ri  |mi, Q2) of the multi-expert prior (see
Sec. 4.2). Recall that mi is a linear function of the
features xi of each sentence. In this section, we dis-
cuss how xi is constructed from the features of the
base predictors. New base predictors can be inte-
grated easily by exposing their features to MEM.
Most base predictors operate on the phrase level;
our goal is to construct features for the entire sen-
tence. Denote by nbi the number of phrases in the
i-th sentence covered by base predictor b, and let
ob ijdenote a set of associated features. Features ob ij
may or may not correspond directly to the features
of base predictor b; see the discussion below. A
</bodyText>
<equation confidence="0.854181">
i)−1 �
</equation>
<bodyText confidence="0.984995166666667">
straightforward strategy is to set xb i = (nb j ob ij.
We proceed slightly differently and average the fea-
tures associated with phrases of positive prior polar-
ity separately from those of phrases with negative
prior polarity (Taboada et al., 2011). We then con-
catenate the averaged feature vectors, i.e., we set
xb i = (�ob,pos
ij �ob,neg
ij ), where �ob,p
ij denotes the average
of the feature vectors ob ijassociated with phrases of
prior polarity p. This procedure allows us to learn
a different weight for each feature depending on its
context (e.g., the weight of intensifier “very” may dif-
fer for positive and negative phrases). We construct
xi by concatenating the sentence-level features xbi of
each base predictor and a feature vector of bigrams.
To integrate a base predictor, we only need to
specify the relevant features and, if applicable, prior
phrase polarities. For our choice of base predictors,
we use the following features:
SO-CAL predictor. The prior polarity of a SO-
CAL phrase is given by the polarity of its SO-
carrying word in the SO-CAL lexicon. The feature
vector oSO-CAL consists of the weight of the SO-
ij
carrying word from the lexicon as well the set of
negator words, irrealis marker words, and intensifier
words in the phrase. Moreover, we add the first two
words preceding the SO-carrying word as context
features (skipping nouns, negators, irrealis markers,
and intensifiers, and stopping at clause boundaries).
All words are encoded as binary indicator features.
BoO predictor. Similar to SO-CAL, we deter-
mine the prior polarity of a phrase based on the BoO
dictionary. In contrast to SO-CAL, we directly use
the BoO score as a feature because the BoO predictor
weights have been trained on a very large corpus and
are thus reliable. We also add irrealis marker words
in the form of indicator features.
Statistical polarity predictor. Recall that the sta-
tistical polarity predictor is based on co-occurrence
counts of opinion-topic pairs and document polar-
ities. We treat each opinion-topic pair as a phrase
and use the most frequently co-occurring polarity
as the phrase’s prior polarity. We use the logarithm
of the co-occurrence counts with positive, negative,
and other polarity as features; this set of features per-
formed better than using the co-occurrence counts or
estimated class probabilities directly. We also add
the same type of context features as for SO-CAL, but
rescale each binary feature by the logarithm of the
occurrence count #z of the opinion-topic pair (i.e.,
the features take values in {0, log #z}).
</bodyText>
<subsectionHeader confidence="0.99684">
4.5 Incorporating Sentence Similarities
</subsectionHeader>
<bodyText confidence="0.999995333333333">
The component distribution N2(r  |0, �L−1) in the
multi-expert prior encourages similar sentences to
have similar ratings. The main purpose of N2 is to
propagate information from sentences on which the
base predictors perform well to sentences for which
base prediction is unreliable or unavailable (e.g., be-
</bodyText>
<page confidence="0.999542">
154
</page>
<bodyText confidence="0.9999193">
cause they do not contain SO-carrying words). To
obtain this distribution, we first construct an N x N
sentence similarity matrix W using a sentiment-
augmented word sequence kernel (see below). We
then compute the regularized graph Laplacian L=
L+I/λ2 based on the unnormalized graph Laplacian
L = D − W (Chapelle et al., 2006), where D be a
diagonal matrix with dii = Ej wij and hyperparam-
eter λ2 controls the scale of sentence ratings.
To gain insight into distribution N2, observe that
</bodyText>
<equation confidence="0.749331">
wij(ri − rj)2 − lrl22/λ2) -
</equation>
<bodyText confidence="0.990450341463415">
2).
The left term in the exponent forces the ratings of
similar sentences to be similar: the larger the sen-
tence similarity wij, the more penalty is paid for dis-
similar ratings. For this reason, N2 has a smoothing
effect. The right term is an L2 regularizer and encour-
ages small ratings; it is controlled by hyperparameter
λ2.
The entries wij in the sentence similarity matrix
determine the degree of smoothing for each pair of
sentence ratings. We compute these values by a novel
sentiment-augmented word sequence kernel, which
extends the well-known word sequence kernel of Can-
cedda et al. (2003) by (1) BoO weights to strengthen
the correlation of sentence similarity and rating sim-
ilarity and (2) synonym resolution based on Word-
Net (Miller, 1995).
In general, a word sequence kernel computes a
similarity score of two sequences based on their
shared subsequences. In more detail, we first de-
fine a score function for a pair of shared subse-
quences, and then sum up these scores to obtain
the overall similarity score. Consider for example
the two sentences “The book is an easy read.” (s1)
and “It is easy to read.” (s2) along with the shared
subsequence “is easy read” (u). Observe that the
words “an” and “to” serve as gaps as they are not
part of the subsequence. We represent subsequence
u in sentence s via a real-valued projection function
φu(s). In our example, φu(s1) = υisυganυeasyυread
and φu(s2) = υisυeasyυgtoυread. The decay factors
υw E (0, 1] for matching words characterize the
importance of a word (large values for significant
words). On the contrary, decay factors υgw E (0, 1]
for gap words are penalty terms for mismatches
(small values for significant words). The score of
subsequence u is defined as φu(s1)φu(s2). Thus
two shared subsequences have high similarity if they
share significant words and few gaps. Following Can-
cedda et al. (2003), we define the similarity between
two sequences as
</bodyText>
<equation confidence="0.938231">
�kn(si, sj) = φu(si)φu(sj),
uEΩn
</equation>
<bodyText confidence="0.9999254">
where Ω is a finite set of words and n denotes the
length of the considered subsequences. This sim-
ilarity function can be computed efficiently using
dynamic programming.
To apply the word sequence kernel, we need to
specify the decay factors. A traditional choice is
υw = log(NwN )/ log(N), where Nw is the document
frequency of the word w and N is the total number
of documents. This IDF decay factor is not well-
suited to our setting: Important opinion words such
as “great” have a low IDF value due to their high
document frequency. To overcome this problem,
we incorporate additional weights for SO-carrying
words using the BoO lexicon. To do so, we first
rescale the BoO weights into [0, 1] using the sig-
moid g(w) = (1 + exp(−aωw + b))−1, where ωw
denotes the BoO weight of word w.3 We then set
υw = min(log(NwN )/ log(N) + g(w), 0.9). The de-
cay factor for gaps is given by υgw = 1 − υw. Thus
we strongly penalize gaps that consist of infrequent
words or opinion words.
To address data sparsity, we incorporate synonyms
and hypernyms from WordNet into our kernel. In
particular, we represent words found in WordNet by
their first two synset names (for verbs, adjectives,
nouns) and their direct hypernym (nouns only). Two
words are considered the same when their synsets
overlap. Thus, for example, “writer” has the same
representation as “author”.
To build the similarity matrix W, we construct
a k-nearest-neighbor graph for all sentences.4 We
consider subsequences consisting of three words (i.e.,
wij = k3(si, sj)); longer subsequences are overly
sparse, shorter subsequences are covered by the bi-
grams features in N1.
</bodyText>
<footnote confidence="0.531285333333333">
3We set a = 2 and b = 1 in our experiments.
4We use k = 15 and only consider neighbors with a similar-
ity above 0.001.
</footnote>
<figure confidence="0.8110376">
N2(r  |0, �L−1)
1
a �
exp (-2
i,j
</figure>
<page confidence="0.996663">
155
</page>
<sectionHeader confidence="0.997602" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999903833333333">
We evaluated both MEM and a number of alternative
approaches for both sentence-level polarity classifi-
cation and sentence-level strength prediction across
a number of domains. We found that MEM out-
performs state-of-the-art approaches by a significant
margin.
</bodyText>
<subsectionHeader confidence="0.990509">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999980028571428">
We implemented MEM as well as the HCRF classi-
fier of (T¨ackstr¨om and McDonald, 2011a; T¨ackstr¨om
and McDonald, 2011b), which is the best-performing
estimator of sentence-level polarity in the weakly-
supervised setting reported in the literature. We train
both methods using (1) only coarse labels (MEM-
Coarse, HCRF-Coarse) and (2) additionally a small
number of sentence polarities (MEM-Fine, HCRF-
Fine5). We also implemented a number of baselines
for both polarity classification and strength predic-
tion: a document oracle (DocOracle) that simply uses
the document label for each sentence, the BoO rat-
ing predictor (BaseBoO), and the SO-CAL rating pre-
dictor (BaseSO-CAL). For polarity classification, we
compare our methods also to the statistical polarity
predictor (Basepolarity). To judge on the effectiveness
of our multi-export prior for combining base predic-
tors, we take the majority vote of all base predic-
tors and document polarity as an additional baseline
(Majority-Vote). Similarly, for strength prediction,
we take the arithmetic mean of the document rat-
ing and the phrase-level predictions of BaseBoO and
BaseSO-CAL as a baseline (Mean-Rating). We use the
same hyperparameter setting for MEM across all our
experiments.
We evaluated all methods on Amazon reviews
from different domains using the corpus of Ding et al.
(2008) and the test set of T¨ackstr¨om and McDonald
(2011a). For each domain, we constructed a large bal-
anced dataset by randomly sampling 33,000 reviews
from the corpus of Ding et al. (2008). We chose
the books, electronics, and music domains for our
experiments; the dvd domain was used for develop-
ment. For sentence polarity classification, we use the
test set of T¨ackstr¨om and McDonald (2011a), which
</bodyText>
<footnote confidence="0.862686">
5We used the best-performing model that fuses HCRF-Coarse
and the supervised model (McDonald et al., 2007) by interpola-
tion.
</footnote>
<bodyText confidence="0.999943428571428">
contains roughly 60 reviews per domain (20 for each
polarity). For strength evaluation, we created a test
set of 300 pairs of sentences per domain from the
polarity test set. Each pair consisted of two sentences
of the same polarity; we manually determined which
of the sentences is more positive. We chose this pair-
wise approach because (1) we wanted the evaluation
to be invariant to the scale of the predicted ratings,
and (2) it much easier for human annotators to rank
a pair of sentences than to rank a large collection of
sentences.
We followed T¨ackstr¨om and McDonald (2011b)
and used 3-fold cross-validation, where each fold
consisted of a set of roughly 20 documents from the
test set. In each fold, we merged the test set with the
reviews from the corresponding domain. For MEM-
Fine and HCRF-Fine, we use the data from the other
two folds as fine-grained polarity annotations. For
our experiments on polarity classification, we con-
verted the predicted ratings of MEM, BaseBoO, and
BaseSO-CAL into polarities by the method described
in Sec. 4.3. We compare the performance of each
method in terms of accuracy, which is defined as the
fraction of correct predictions on the test set (correct
label for polarity / correct ranking for strength). All
reported numbers are averages over the three folds. In
our tables, boldface numbers are statistically signifi-
cant against all other methods (t-test, p-value 0.05).
</bodyText>
<subsectionHeader confidence="0.596329">
5.2 Results for Polarity Classification
</subsectionHeader>
<bodyText confidence="0.999912611111111">
Table 1 summarizes the results of our experiments for
sentence polarity classification. The base predictors
perform poorly across all domains, mainly due to
the aforementioned problems associated with aver-
aging phrase-level predictions. In fact, DocOracle
performs almost always better than any of the base
predictors. However, accurracy increases when we
combine base predictors and DocOracle using ma-
jority voting, which indicates that ensemble methods
work well.
When no fine-grained annotations are available
(HCRF-Coarse, MEM-Coarse), both MEM-Coarse
and Majority-Vote outperformed HCRF-Coarse,
which in turn has been shown to outperform a num-
ber of lexicon-based methods as well as classifiers
trained on document labels (T¨ackstr¨om and McDon-
ald, 2011a). MEM-Coarse also performs better than
Majority-Vote. This is because MEM propagates
</bodyText>
<page confidence="0.994613">
156
</page>
<table confidence="0.9997987">
Book Electronics Music Avg
Basepolarity 43.7 40.3 43.8 42.6
BaseBoO 50.9 48.9 52.6 50.8
BaseSO-CAL 44.6 50.2 45.0 46.6
DocOracle 51.9 49.6 59.3 53.6
Majority-Vote 53.7 53.4 58.7 55.2
HCRF-Coarse 52.2 53.4 57.2 54.3
MEM-Coarse 54.4 54.9 64.5 57.9
HCRF-Fine 55.9 61.0 58.7 58.5
MEM-Fine 59.7 59.6 63.8 61.0
</table>
<tableCaption confidence="0.992281">
Table 1: Accuracy of polarity classification per do-
main and averaged across domains.
</tableCaption>
<bodyText confidence="0.998786882352941">
evidence across similar sentences, which is espe-
cially useful when no explicit SO-carrying words
exist. Also, MEM learns weights of features of base
predictors, which leads to a more adaptive integration,
and our ordinal regression formulation for polarity
prediction allows direct competition among positive
and negative evidence for improved accuracy.
When we incorporate a small amount of sentence
polarity labels (HCRF-Fine, MEM-Fine), the accu-
racy of all models greatly improves. HCRF-Fine has
been shown to outperform the strongest supervised
method on the same dataset (McDonald et al., 2007;
T¨ackstr¨om and McDonald, 2011b). MEM-Fine falls
short of HCRF-Fine only in the electronics domain
but performs better on all other domains. In the book
and music domains, where MEM-Fine is particularly
effective, many sentences feature complex syntac-
tic structure and SO-carrying words are often used
without reference to the quality of the product (but to
describe contents, e.g., “a love story” or “a horrible
accident”).
Our models perform especially well when they are
applied to sentences containing no or few opinion
words from lexicons. Table 2 reports the evaluation
results for both sentences containing SO-carrying
words from either MPQA or SO-CAL lexicons and
for sentences containing no such words. The re-
sults explain why our model falls short of HCRF-
Fine in the electronics domain: reviews of electronic
products contain many SO-carrying words, which
almost always express opinions. Nevertheless, MEM-
Fine handles sentences without explicit SO-carrying
words well across all domains; here the propagation
of information across sentences helps to learn the SO
</bodyText>
<table confidence="0.93928925">
Book Electronics Music
op fact op fact op fact
HCRF-Fine 55.7 55.9 63.3 54.6 59.0 57.4
MEM-Fine 58.9 62.4 60.7 56.7 64.5 60.8
</table>
<tableCaption confidence="0.988773666666667">
Table 2: Accuracy of polarity classification for sen-
tences with opinion words (op) and without opinion
words (fact).
</tableCaption>
<bodyText confidence="0.997403333333333">
of facts (such as “short battery life”).
We found that for all methods, most of the errors
are caused by misclassifying positive/negative sen-
tences as other and vice versa. Moreover, sentences
with polarity opposite to the document polarity are
hard cases if they do not feature frequent strong pat-
terns. Another difficulty lies in off-topic sentences,
which may contain explicit SO-carrying words but
are not related to the item under review. This is one
of the main reasons for the poor performance of the
lexicon-based methods.
Overall, we found that MEM-Fine is the method of
choice. Thus our multi-expert model can indeed bal-
ance the strength of the individual experts to obtain
better estimation accuracy.
</bodyText>
<sectionHeader confidence="0.902933" genericHeader="evaluation">
5.3 Results for Strength Prediction
</sectionHeader>
<bodyText confidence="0.9998682">
Table 3 shows the accuracy results for strength pre-
diction. Here our models outperformed all baselines
by a large margin. Although document ratings are
strong indicators in the polarity classification task,
they lead to worse performance than lexicon-based
methods. The main reason for this drop in accuracy
is that the document oracle assigns the same rating
to all sentences within a review. Thus DocOracle
cannot rank sentences from the same review, which
is a severe limitation. This shortage can be partly
compensated by averaging the base predictions and
document rating (Mean-Rating). Note that it is non-
trivial to apply existing ensemble methods for the
weights of individual base predictors because of the
absence of the sentence ratings as training labels. In
contrast, our MEM models use indirect supervision
to adaptively assign weights to the features from base
predictors. Similar to polarity classification, a small
amount of sentence polarity labels often improved
the performance of MEM.
</bodyText>
<page confidence="0.990313">
157
</page>
<table confidence="0.999709714285714">
Book Electronics Music Avg
BaseBoO 58.3 51.6 53.5 54.5
BaseSO-CAL 60.6 57.1 47.6 55.1
DocOracle 45.1 36.2 41.4 40.9
Mean-Rating 70.3 57.0 60.8 62.7
MEM-Coarse 68.7 60.5 69.5 66.2
MEM-Fine 72.4 63.3 67.2 67.6
</table>
<tableCaption confidence="0.999785">
Table 3: Accuracy of strength prediction.
</tableCaption>
<sectionHeader confidence="0.996896" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991818181818">
We proposed the Multi-Experts Model for analyz-
ing both opinion polarity and opinion strength at
the sentence level. MEM is weakly supervised; it
can run without any fine-grained annotations but is
also able to leverage such annotations when avail-
able. MEM is driven by a novel multi-expert prior,
which integrates a number of diverse base predictors
and propagates information across sentences using a
sentiment-augmented word sequence kernel. Our ex-
periments indicate that MEM achieves better overall
accuracy than alternative methods.
</bodyText>
<sectionHeader confidence="0.998181" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874038961039">
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples.
The Journal of Machine Learning Research, 7:2399–
2434.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning, volume 4. Springer New York.
Nicola Cancedda, ´Eric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059–1082.
Oliver Chapelle, Bernhard Sch¨olkopf, and Alexander Zien.
2006. Semi-Supervised Learning. MIT Press.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, volume 2, pages 590–598.
Wei Chu and Zoubin Ghahramani. 2006. Gaussian pro-
cesses for ordinal regression. Journal of Machine
Learning Research, 6(1):1019.
Thomas G. Dietterichl. 2002. Ensemble learning. The
Handbook of Brain Theory and Neural Networks, pages
405–408.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the International Conference on Web
Search and Data Mining, pages 231–240.
Saso Dzeroski and Bernard Zenko. 2004. Is combining
classifiers with stacking better than selecting the best
one? Machine Learning, 54(3):255–273.
Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear mod-
els via coordinate descent. Technical report.
Guohong Fu and Xin Wang. 2010. Chinese sentence-
level sentiment classification based on fuzzy sets. In
Proceedings of the International Conference on Com-
putational Linguistics, pages 312–319. Association for
Computational Linguistics.
Andrew B. Goldberg and Xiaojun Zhu. 2006. Seeing
stars when there aren’t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 168–177.
Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen,
and Hsin-Hsi Chen. 2008. Sentence-level opinion anal-
ysis by copeopi in ntcir-7. In Proceedings of NTCIR-7
Workshop Meeting.
Yi Mao and Guy Lebanon. 2006. Isotonic Conditional
Random Fields and Local Sentiment Flow. Advances
in Neural Information Processing Systems, pages 961–
968.
Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeffrey C. Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In Proceedings of
the Annual Meeting on Association for Computational
Linguistics, volume 45, page 432.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Annual
Meeting on Association for Computational Linguistics,
pages 271–278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 124–131.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 79–86.
</reference>
<page confidence="0.979189">
158
</page>
<reference confidence="0.999354697674419">
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding Domain Sentiment Lexicon through Dou-
ble Propagation. In International Joint Conference on
Artificial Intelligence, pages 1199–1204.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010.
The bag-of-opinions method for review rating predic-
tion from sparse text patterns. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 913–921.
Carl Edward Rasmussen. 2004. Gaussian processes in
machine learning. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267–307.
Oscar T¨ackstr¨om and Ryan T. McDonald. 2011a. Dis-
covering Fine-Grained Sentiment with Latent Variable
Structured Prediction Models. In Proceedings of the
European Conference on Information Retrieval, pages
368–374.
Oscar T¨ackstr¨om and Ryan T. McDonald. 2011b. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 569–574.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Human Language
Technology Conference and the Conference on Empir-
ical Methods in Natural Language Processing, pages
347–354.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propagation.
Technical report.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In Proceedings of the Inter-
national Conference on Machine Learning, pages 912–
919.
Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM international conference on Information and
knowledge management, pages 43–50.
</reference>
<page confidence="0.998835">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224157">
<title confidence="0.863890666666667">A Weakly Supervised Model for Sentence-Level Semantic Analysis with Multiple Experts Lizhen Qu and Rainer Gemulla and Gerhard</title>
<author confidence="0.979769">Max Planck Institute for</author>
<email confidence="0.386994">Saarbr¨ucken,</email>
<abstract confidence="0.999269416666666">propose the weakly supervised Multi- Model for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data. We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>7</volume>
<pages>2434</pages>
<contexts>
<context position="5569" citStr="Belkin et al., 2006" startWordPosition="833" endWordPosition="836">odel can be regarded as a Gaussian process (GP) model (Rasmussen, 2004) with a novel multi-expert prior. The multi-expert prior decomposes into two component distributions. The first component distribution integrates sentence-local information obtained from the base predictors. It forms a special realization of stacking (Dzeroski and Zenko, 2004) but uses the features from the base predictors instead of the actual predictions. The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) (Zhu et al., 2003; Belkin et al., 2006). It aims to improve the predictions on sentences that are not covered well enough by our base predictors. Traditional GSSL algorithms support either discrete labels (classification) or numerical labels (regression); we extend these techniques to support both types of labels simultaneously. We use a novel variant of word sequence kernels (Cancedda et al., 2003) to measure sentence similarity. Our kernel takes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating predict</context>
<context position="7943" citStr="Belkin et al., 2006" startWordPosition="1210" endWordPosition="1213">ength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a single phrase. As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors 150 (see Sec. 4.4). MEM is designed such that new base predictors can be integrated easily. Our base predictors use a diverse set of ava</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research, 7:2399– 2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning,</title>
<date>2006</date>
<volume>4</volume>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="7837" citStr="Bishop, 2006" startWordPosition="1192" endWordPosition="1193">small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a single phrase. As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors 150 (see Sec. 4.4). MEM is des</context>
<context position="20117" citStr="Bishop, 2006" startWordPosition="3198" endWordPosition="3199"> positive and negative documents, and ηDoc i = 2 for neutral documents. The reasoning behind this choice is that sentence ratings in neutral documents express higher variance because these documents often contain a mixture of positive and negative sentences. When a small set of manually created sentence polarity labels is available, we train a classifier that predicts whether the sentence polarity coincides with the document polarity. If so, we set the corresponding variance ηDoc i to a small value; otherwise, we choose a larger value. In particular, we train a logistic regression classifier (Bishop, 2006) using the following binary features: (1) an indicator variable for each document polarity, and (2) an indicator variable for each triple of base predictor, predicted polarity, and document polarity (set to 1 if the polarities match). We then setηDoc i = (τpi)−1, where pi is the probability of matching polarities obtained from the classifier and τ is a hyperparameter that ensures correct scaling. Polarity Labels We now describe how to model the correlation between the polarity of a sentence and its rating. An simple and effective approach is to partition the range of ratings into three consecu</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern recognition and machine learning, volume 4. Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>´Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>JeanMichel Renders</author>
</authors>
<title>Word-sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="5932" citStr="Cancedda et al., 2003" startWordPosition="889" endWordPosition="892"> features from the base predictors instead of the actual predictions. The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) (Zhu et al., 2003; Belkin et al., 2006). It aims to improve the predictions on sentences that are not covered well enough by our base predictors. Traditional GSSL algorithms support either discrete labels (classification) or numerical labels (regression); we extend these techniques to support both types of labels simultaneously. We use a novel variant of word sequence kernels (Cancedda et al., 2003) to measure sentence similarity. Our kernel takes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mini</context>
<context position="17512" citStr="Cancedda et al., 2003" startWordPosition="2776" endWordPosition="2779">e sentence similarity. However, such features cannot be extended to longer subsequences such as trigrams due to data sparsity: useful features become as infrequent as noisy terms. Moreover, we would argmin 1: − r,β b 152 like to capture sentence similarity using gapped (i.e., non-consecutive) subsequences. For example, the sentences “The book is an easy read.” and “It is easy to read.” are similar but do not share any consecutive bigrams. They do share the subsequence “easy read”, however. To capture this similarity, we make use of a novel sentiment-augmented variant of word sequence kernels (Cancedda et al., 2003). Our kernel is used to construct a similarity matrix W among sentences and the corresponding regularized Laplacian eL. To capture the intuition that similar sentences should have similar ratings, we introduce a Gaussian prior N2(r |0, eL−1) as a component into our multi-expert prior; see Sec. 4.5 for details and a discussion of why this prior encourages similar ratings for similar sentences. Since the two component distributions feature different expertise, we take their product and obtain the multi-expert prior PE(r |X,,3) ∝ N1(r |m, Iσ2)N2(r |0,eL−1)P(p |α), where m = (m1, ... , mN). Note t</context>
<context position="26532" citStr="Cancedda et al. (2003)" startWordPosition="4282" endWordPosition="4286">ri − rj)2 − lrl22/λ2) - 2). The left term in the exponent forces the ratings of similar sentences to be similar: the larger the sentence similarity wij, the more penalty is paid for dissimilar ratings. For this reason, N2 has a smoothing effect. The right term is an L2 regularizer and encourages small ratings; it is controlled by hyperparameter λ2. The entries wij in the sentence similarity matrix determine the degree of smoothing for each pair of sentence ratings. We compute these values by a novel sentiment-augmented word sequence kernel, which extends the well-known word sequence kernel of Cancedda et al. (2003) by (1) BoO weights to strengthen the correlation of sentence similarity and rating similarity and (2) synonym resolution based on WordNet (Miller, 1995). In general, a word sequence kernel computes a similarity score of two sequences based on their shared subsequences. In more detail, we first define a score function for a pair of shared subsequences, and then sum up these scores to obtain the overall similarity score. Consider for example the two sentences “The book is an easy read.” (s1) and “It is easy to read.” (s2) along with the shared subsequence “is easy read” (u). Observe that the wo</context>
<context position="27803" citStr="Cancedda et al. (2003)" startWordPosition="4497" endWordPosition="4501">art of the subsequence. We represent subsequence u in sentence s via a real-valued projection function φu(s). In our example, φu(s1) = υisυganυeasyυread and φu(s2) = υisυeasyυgtoυread. The decay factors υw E (0, 1] for matching words characterize the importance of a word (large values for significant words). On the contrary, decay factors υgw E (0, 1] for gap words are penalty terms for mismatches (small values for significant words). The score of subsequence u is defined as φu(s1)φu(s2). Thus two shared subsequences have high similarity if they share significant words and few gaps. Following Cancedda et al. (2003), we define the similarity between two sequences as �kn(si, sj) = φu(si)φu(sj), uEΩn where Ω is a finite set of words and n denotes the length of the considered subsequences. This similarity function can be computed efficiently using dynamic programming. To apply the word sequence kernel, we need to specify the decay factors. A traditional choice is υw = log(NwN )/ log(N), where Nw is the document frequency of the word w and N is the total number of documents. This IDF decay factor is not wellsuited to our setting: Important opinion words such as “great” have a low IDF value due to their high </context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, ´Eric Gaussier, Cyril Goutte, and JeanMichel Renders. 2003. Word-sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Chapelle</author>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Zien</author>
</authors>
<title>Semi-Supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<marker>Chapelle, Sch¨olkopf, Zien, 2006</marker>
<rawString>Oliver Chapelle, Bernhard Sch¨olkopf, and Alexander Zien. 2006. Semi-Supervised Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>2</volume>
<pages>590--598</pages>
<contexts>
<context position="7524" citStr="Choi and Cardie, 2009" startWordPosition="1135" endWordPosition="1138">ining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exp</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 2, pages 590–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Gaussian processes for ordinal regression.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="20940" citStr="Chu and Ghahramani, 2006" startWordPosition="3326" endWordPosition="3329"> (set to 1 if the polarities match). We then setηDoc i = (τpi)−1, where pi is the probability of matching polarities obtained from the classifier and τ is a hyperparameter that ensures correct scaling. Polarity Labels We now describe how to model the correlation between the polarity of a sentence and its rating. An simple and effective approach is to partition the range of ratings into three consecutive partitions, one for each polarity class. We thus considering the polarity classes {positive, other, negative} as ordered and formulate polarity classification as an ordinal regression problem (Chu and Ghahramani, 2006). We immediately obtain the distribution ri − b+ ! Pb(ˆyb i = pos |ri) = Φp ηb ! b− − ri ! b+ − ri Pb(ˆyb i = oth |ri) = Φ p − Φ p ηb ηb b− − ri ! Pb(ˆybi = neg |ri) = Φ p, ηb where b+ and b− are the partition boundaries between positive/other and other/negative, respectively,2 Φ(x) denotes the cumulative distribution function of the 2We set b+ = 0.3 and b− = −0.3 to calibrate to SO-CAL, which treats ratings in [−0.3, 0, 3] as polarity other. 153 Figure 1: Distribution of polarity given rating. Gaussian distribution, and variance 77b is a hyperparameter. It is easy to verify that Ei| yb� ,Y p(</context>
</contexts>
<marker>Chu, Ghahramani, 2006</marker>
<rawString>Wei Chu and Zoubin Ghahramani. 2006. Gaussian processes for ordinal regression. Journal of Machine Learning Research, 6(1):1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterichl</author>
</authors>
<title>Ensemble learning.</title>
<date>2002</date>
<booktitle>The Handbook of Brain Theory and Neural Networks,</booktitle>
<pages>405--408</pages>
<contexts>
<context position="4655" citStr="Dietterichl, 2002" startWordPosition="705" endWordPosition="706">precision, (3) to low recall. To address the challenges outlined above, we propose the weakly supervised Multi-Experts Model (MEM) for sentence-level rating prediction. MEM starts with a set of potentially noisy indicators of SO including phrase-level predictions, language heuristics, and co-occurrence counts. We refer to these indicators as base predictors; they constitute the set of experts used in our model. MEM is designed such that new base predictors can be easily integrated. Since the information provided by the base predictors can be contradicting, we use ideas from ensemble learning (Dietterichl, 2002) to learn the most confident indicators and to exploit domain-dependent information revealed by document ratings. Thus, instead of averaging base predictors, MEM integrates their features along with the available coarse-grained training data into a unified probabilistic model. The integrated model can be regarded as a Gaussian process (GP) model (Rasmussen, 2004) with a novel multi-expert prior. The multi-expert prior decomposes into two component distributions. The first component distribution integrates sentence-local information obtained from the base predictors. It forms a special realizat</context>
<context position="7822" citStr="Dietterichl, 2002" startWordPosition="1189" endWordPosition="1191"> when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a single phrase. As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors 150 (see Sec. 4</context>
</contexts>
<marker>Dietterichl, 2002</marker>
<rawString>Thomas G. Dietterichl. 2002. Ensemble learning. The Handbook of Brain Theory and Neural Networks, pages 405–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Data Mining,</booktitle>
<pages>231--240</pages>
<contexts>
<context position="7501" citStr="Ding et al., 2008" startWordPosition="1131" endWordPosition="1134"> focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a</context>
<context position="31304" citStr="Ding et al. (2008)" startWordPosition="5074" endWordPosition="5077">ods also to the statistical polarity predictor (Basepolarity). To judge on the effectiveness of our multi-export prior for combining base predictors, we take the majority vote of all base predictors and document polarity as an additional baseline (Majority-Vote). Similarly, for strength prediction, we take the arithmetic mean of the document rating and the phrase-level predictions of BaseBoO and BaseSO-CAL as a baseline (Mean-Rating). We use the same hyperparameter setting for MEM across all our experiments. We evaluated all methods on Amazon reviews from different domains using the corpus of Ding et al. (2008) and the test set of T¨ackstr¨om and McDonald (2011a). For each domain, we constructed a large balanced dataset by randomly sampling 33,000 reviews from the corpus of Ding et al. (2008). We chose the books, electronics, and music domains for our experiments; the dvd domain was used for development. For sentence polarity classification, we use the test set of T¨ackstr¨om and McDonald (2011a), which 5We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al., 2007) by interpolation. contains roughly 60 reviews per domain (20 for each polarity). For strengt</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the International Conference on Web Search and Data Mining, pages 231–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saso Dzeroski</author>
<author>Bernard Zenko</author>
</authors>
<title>Is combining classifiers with stacking better than selecting the best one?</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="5297" citStr="Dzeroski and Zenko, 2004" startWordPosition="793" endWordPosition="796">t confident indicators and to exploit domain-dependent information revealed by document ratings. Thus, instead of averaging base predictors, MEM integrates their features along with the available coarse-grained training data into a unified probabilistic model. The integrated model can be regarded as a Gaussian process (GP) model (Rasmussen, 2004) with a novel multi-expert prior. The multi-expert prior decomposes into two component distributions. The first component distribution integrates sentence-local information obtained from the base predictors. It forms a special realization of stacking (Dzeroski and Zenko, 2004) but uses the features from the base predictors instead of the actual predictions. The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) (Zhu et al., 2003; Belkin et al., 2006). It aims to improve the predictions on sentences that are not covered well enough by our base predictors. Traditional GSSL algorithms support either discrete labels (classification) or numerical labels (regression); we extend these techniques to support both types of labels simultaneously. We use a novel variant of word seque</context>
</contexts>
<marker>Dzeroski, Zenko, 2004</marker>
<rawString>Saso Dzeroski and Bernard Zenko. 2004. Is combining classifiers with stacking better than selecting the best one? Machine Learning, 54(3):255–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
<author>Trevor Hastie</author>
<author>Rob Tibshirani</author>
</authors>
<title>Regularization paths for generalized linear models via coordinate descent.</title>
<date>2008</date>
<tech>Technical report.</tech>
<contexts>
<context position="15519" citStr="Friedman et al., 2008" startWordPosition="2451" endWordPosition="2454">predictors and sentence similarities. We correlate ratings to initial labels via a set of conditional distributions Pb(yb |r), where b denotes the type of initial label (Sec. 4.3). The posterior of r is then given by P(r |X, ���, β) ∝ Pb(Yb |r)PE(r |X,β). b Note that the posterior is influenced by both the multiexpert prior and the set of initial labels. We use MAP inference to obtain the most likely rating of each sentence, i.e., we solve log(Pb(�yb |r)) − log(PE(r |X,β)), where as before β denotes the model parameters. We solve the above optimization problem using cyclic coordinate descent (Friedman et al., 2008). 4.2 Multi-Expert Prior The multi-expert prior PE(r |X,β) consists of two component distributions N1 and N2. Distribution N1 integrates features from the base predictors, N2 incorporates sentence similarities to propagate information across sentences. In a slight abuse of notation, denote by xi the set of features for the i-th sentence. Vector xi contains the features of all the base predictors but also includes bigram features for increased coverage of syntactic patterns; see Sec. 4.4 for details about the feature design. Let m(xi) = βTxi be a linear predictor for ri, where β is a real weigh</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2008</marker>
<rawString>Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani. 2008. Regularization paths for generalized linear models via coordinate descent. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Xin Wang</author>
</authors>
<title>Chinese sentencelevel sentiment classification based on fuzzy sets.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>312--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7582" citStr="Fu and Wang, 2010" startWordPosition="1147" endWordPosition="1150">weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 B</context>
</contexts>
<marker>Fu, Wang, 2010</marker>
<rawString>Guohong Fu and Xin Wang. 2010. Chinese sentencelevel sentiment classification based on fuzzy sets. In Proceedings of the International Conference on Computational Linguistics, pages 312–319. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Goldberg</author>
<author>Xiaojun Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="6640" citStr="Goldberg and Zhu, 2006" startWordPosition="998" endWordPosition="1001"> also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of </context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Andrew B. Goldberg and Xiaojun Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="7542" citStr="Hu and Liu, 2004" startWordPosition="1139" endWordPosition="1142"> or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengt</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>I-Chien Liu</author>
<author>Chia-Ying Lee</author>
<author>Kuan hua Chen</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Sentence-level opinion analysis by copeopi in ntcir-7.</title>
<date>2008</date>
<booktitle>In Proceedings of NTCIR-7 Workshop Meeting.</booktitle>
<contexts>
<context position="7600" citStr="Ku et al., 2008" startWordPosition="1151" endWordPosition="1154">CRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Eac</context>
</contexts>
<marker>Ku, Liu, Lee, Chen, Chen, 2008</marker>
<rawString>Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen, and Hsin-Hsi Chen. 2008. Sentence-level opinion analysis by copeopi in ntcir-7. In Proceedings of NTCIR-7 Workshop Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Isotonic Conditional Random Fields and Local Sentiment Flow.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="6753" citStr="Mao and Lebanon, 2006" startWordPosition="1015" endWordPosition="1018">in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation </context>
</contexts>
<marker>Mao, Lebanon, 2006</marker>
<rawString>Yi Mao and Guy Lebanon. 2006. Isotonic Conditional Random Fields and Local Sentiment Flow. Advances in Neural Information Processing Systems, pages 961– 968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeffrey C Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>432</pages>
<contexts>
<context position="6797" citStr="McDonald et al., 2007" startWordPosition="1023" endWordPosition="1026">d sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within senten</context>
<context position="31811" citStr="McDonald et al., 2007" startWordPosition="5156" endWordPosition="5159">periments. We evaluated all methods on Amazon reviews from different domains using the corpus of Ding et al. (2008) and the test set of T¨ackstr¨om and McDonald (2011a). For each domain, we constructed a large balanced dataset by randomly sampling 33,000 reviews from the corpus of Ding et al. (2008). We chose the books, electronics, and music domains for our experiments; the dvd domain was used for development. For sentence polarity classification, we use the test set of T¨ackstr¨om and McDonald (2011a), which 5We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al., 2007) by interpolation. contains roughly 60 reviews per domain (20 for each polarity). For strength evaluation, we created a test set of 300 pairs of sentences per domain from the polarity test set. Each pair consisted of two sentences of the same polarity; we manually determined which of the sentences is more positive. We chose this pairwise approach because (1) we wanted the evaluation to be invariant to the scale of the predicted ratings, and (2) it much easier for human annotators to rank a pair of sentences than to rank a large collection of sentences. We followed T¨ackstr¨om and McDonald (201</context>
<context position="35128" citStr="McDonald et al., 2007" startWordPosition="5669" endWordPosition="5672">eraged across domains. evidence across similar sentences, which is especially useful when no explicit SO-carrying words exist. Also, MEM learns weights of features of base predictors, which leads to a more adaptive integration, and our ordinal regression formulation for polarity prediction allows direct competition among positive and negative evidence for improved accuracy. When we incorporate a small amount of sentence polarity labels (HCRF-Fine, MEM-Fine), the accuracy of all models greatly improves. HCRF-Fine has been shown to outperform the strongest supervised method on the same dataset (McDonald et al., 2007; T¨ackstr¨om and McDonald, 2011b). MEM-Fine falls short of HCRF-Fine only in the electronics domain but performs better on all other domains. In the book and music domains, where MEM-Fine is particularly effective, many sentences feature complex syntactic structure and SO-carrying words are often used without reference to the quality of the product (but to describe contents, e.g., “a love story” or “a horrible accident”). Our models perform especially well when they are applied to sentences containing no or few opinion words from lexicons. Table 2 reports the evaluation results for both sente</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan T. McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeffrey C. Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Proceedings of the Annual Meeting on Association for Computational Linguistics, volume 45, page 432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="26685" citStr="Miller, 1995" startWordPosition="4310" endWordPosition="4311">penalty is paid for dissimilar ratings. For this reason, N2 has a smoothing effect. The right term is an L2 regularizer and encourages small ratings; it is controlled by hyperparameter λ2. The entries wij in the sentence similarity matrix determine the degree of smoothing for each pair of sentence ratings. We compute these values by a novel sentiment-augmented word sequence kernel, which extends the well-known word sequence kernel of Cancedda et al. (2003) by (1) BoO weights to strengthen the correlation of sentence similarity and rating similarity and (2) synonym resolution based on WordNet (Miller, 1995). In general, a word sequence kernel computes a similarity score of two sequences based on their shared subsequences. In more detail, we first define a score function for a pair of shared subsequences, and then sum up these scores to obtain the overall similarity score. Consider for example the two sentences “The book is an easy read.” (s1) and “It is easy to read.” (s2) along with the shared subsequence “is easy read” (u). Observe that the words “an” and “to” serve as gaps as they are not part of the subsequence. We represent subsequence u in sentence s via a real-valued projection function φ</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="6576" citStr="Pang and Lee, 2004" startWordPosition="986" endWordPosition="989">arity. Our kernel takes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coar</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the Annual Meeting on Association for Computational Linguistics, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>124--131</pages>
<contexts>
<context position="6615" citStr="Pang and Lee, 2005" startWordPosition="994" endWordPosition="997">sitions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when avai</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 124–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="6595" citStr="Pang et al., 2002" startWordPosition="990" endWordPosition="993">kes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification. 2 Related Work There exists a large body of work on analyzing the semantic orientation of natural language text. Our approach is unique in that it is weakly supervised, predicts both polarity and strength, and operates on the sentence level. Supervised approaches for sentiment analysis focus mainly on opinion mining at the document level (Pang and Lee, 2004; Pang et al., 2002; Pang and Lee, 2005; Goldberg and Zhu, 2006), but have also been applied to sentence-level polarity classification in specific domains (Mao and Lebanon, 2006; Pang and Lee, 2004; McDonald et al., 2007). In these settings, a sufficient amount of training data is available. In contrast, we focus on opinion mining tasks with little or no fine-grained training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding Domain Sentiment Lexicon through Double Propagation.</title>
<date>2009</date>
<booktitle>In International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1199--1204</pages>
<contexts>
<context position="9954" citStr="Qiu et al., 2009" startWordPosition="1528" endWordPosition="1531">rd. For example, consider the phrase “I began this novel with the greatest of hopes [...]”. Here, “greatest” has a positive semantic orientation in all subjectivity lexicons, but the combination “greatest of hopes” often indicates a negative sentiment. We refer to a pair of SO-carrying word (“greatest”) and a target word (“hopes”) as an opinion-target pair. Our statistical polarity predictor learns the polarity of opinions and targets jointly, which increases the robustness of its predictions. Syntactic dependency relations of the form A R −→ B are a strong indicator for opinion-target pairs (Qiu et al., 2009; Zhuang et al., 2006); e.g., “great” nmod −−−→“product”. To achieve high precision, we only consider pairs connected by the following predefined set of shortest dependency paths: verbsubj ←−−noun, verb obj ←− noun, adj nmod −−−→ noun, adj prd −−→ verbsubj ←−−noun. We only retain opiniontarget pairs that are sufficiently frequent. For each extracted pair z, we count how often it co-occurs with each document polarity y ∈ Y, where Y = {positive, negative, other} denotes the set of polarities. If z occurs in a document but is preceded by a negator, we treat it as a co-occurrence of opposite docum</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding Domain Sentiment Lexicon through Double Propagation. In International Joint Conference on Artificial Intelligence, pages 1199–1204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lizhen Qu</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>The bag-of-opinions method for review rating prediction from sparse text patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>913--921</pages>
<contexts>
<context position="2860" citStr="Qu et al., 2010" startWordPosition="429" endWordPosition="432"> ratings: Large negative/positive ratings indicate a strong negative/positive orientation. A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain. We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available. A number of lexicon-based approaches for phraselevel rating prediction has been proposed in the literature (Taboada et al., 2011; Qu et al., 2010). These methods utilize a subjectivity lexicon of words along with information about their semantic orientation; they focus on phrases that contain words from the lexicon. A key advantage of sentence-level methods is that they are able to cover all sentences in a review and that phrase identification is avoided. To the best of our knowledge, the problem of rating prediction at the sentence level has not been addressed in the literature. A naive approach would be to simply average phrase-level ratings. Such an approach performs 1We assign polarity other to text fragments that are off-topic or n</context>
<context position="7670" citStr="Qu et al., 2010" startWordPosition="1163" endWordPosition="1166">2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a sing</context>
<context position="12151" citStr="Qu et al. (2010)" startWordPosition="1895" endWordPosition="1898">the Bayesian average of the phrase-level predictors: P(y |Z(x)) = EZEZ(X) P(y |z)P(z) = EZEZ(X) P(y, z). Thus the most likely polarity is the one with the highest co-occurrence count. 3.2 Heuristic Polarity Predictor Heuristic patterns can also serve as base predictors. In particular, we found that some authors list positive and negative aspects separately after keywords such as “pros” and “cons”. A heuristic that exploits such patterns achieved a high precision (&gt; 90%) but low recall (&lt; 5%) in our experiments. 3.3 Bag-of-Opinions Rating Predictor We leverage the bag-of-opinion (BoO) model of Qu et al. (2010) as a base predictor for phrase-level ratings. The BoO model was trained from a large generic corpus without fine-grained annotations. In BoO, an opinion consists of three components: an SO-carrying word (e.g., “good”), a set of intensifiers (e.g., “very”) and a set of negators (e.g., “not”). Each opinion is scored based on these words (represented as a boolean vector b) and the polarity of the SO-carrying word (represented as sgn(r) ∈ {−1,1}) as indicated by the MPQA lexicon of Wilson et al. (2005). In particular, the score is computed as sgn(r)wTb, where w is the learned weight vector. The s</context>
</contexts>
<marker>Qu, Ifrim, Weikum, 2010</marker>
<rawString>Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The bag-of-opinions method for review rating prediction from sparse text patterns. In Proceedings of the International Conference on Computational Linguistics, pages 913–921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
</authors>
<title>Gaussian processes in machine learning.</title>
<date>2004</date>
<publisher>Springer.</publisher>
<contexts>
<context position="5020" citStr="Rasmussen, 2004" startWordPosition="759" endWordPosition="760">onstitute the set of experts used in our model. MEM is designed such that new base predictors can be easily integrated. Since the information provided by the base predictors can be contradicting, we use ideas from ensemble learning (Dietterichl, 2002) to learn the most confident indicators and to exploit domain-dependent information revealed by document ratings. Thus, instead of averaging base predictors, MEM integrates their features along with the available coarse-grained training data into a unified probabilistic model. The integrated model can be regarded as a Gaussian process (GP) model (Rasmussen, 2004) with a novel multi-expert prior. The multi-expert prior decomposes into two component distributions. The first component distribution integrates sentence-local information obtained from the base predictors. It forms a special realization of stacking (Dzeroski and Zenko, 2004) but uses the features from the base predictors instead of the actual predictions. The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) (Zhu et al., 2003; Belkin et al., 2006). It aims to improve the predictions on sentences t</context>
</contexts>
<marker>Rasmussen, 2004</marker>
<rawString>Carl Edward Rasmussen. 2004. Gaussian processes in machine learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly D Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2842" citStr="Taboada et al., 2011" startWordPosition="425" endWordPosition="428">ping them to numerical ratings: Large negative/positive ratings indicate a strong negative/positive orientation. A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain. We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available. A number of lexicon-based approaches for phraselevel rating prediction has been proposed in the literature (Taboada et al., 2011; Qu et al., 2010). These methods utilize a subjectivity lexicon of words along with information about their semantic orientation; they focus on phrases that contain words from the lexicon. A key advantage of sentence-level methods is that they are able to cover all sentences in a review and that phrase identification is avoided. To the best of our knowledge, the problem of rating prediction at the sentence level has not been addressed in the literature. A naive approach would be to simply average phrase-level ratings. Such an approach performs 1We assign polarity other to text fragments that </context>
<context position="7652" citStr="Taboada et al., 2011" startWordPosition="1159" endWordPosition="1162">kstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or th</context>
<context position="13127" citStr="Taboada et al. (2011)" startWordPosition="2054" endWordPosition="2057"> boolean vector b) and the polarity of the SO-carrying word (represented as sgn(r) ∈ {−1,1}) as indicated by the MPQA lexicon of Wilson et al. (2005). In particular, the score is computed as sgn(r)wTb, where w is the learned weight vector. The sign function sgn(r) ensures consistent weight assignment for intensifiers and negators. For example, an intensifier like “very” can obtain a large positive or a large negative weight depending on whether it is used with a positive or negative SO-carrying 151 word, respectively. 3.4 SO-CAL Rating Predictor The Semantic Orientation Calculator (SO-CAL) of Taboada et al. (2011) also predicts phrase-level ratings via a scoring function similar to the one of BoO. The SO-CAL predictor uses a manually created lexicon, in which each word is classified as either an SOcarrying word (associated with a numerical score), an intensifier (associated with a modifier on the numerical score), or a negator. SO-CAL employs various heuristics to detect irrealis and to correct for the positive bias inherent in most lexicon-based classifiers. Compared to BoO, SO-CAL has lower recall but higher precision. 4 Multi-Experts Model Our multi-experts model incorporates features from the indiv</context>
<context position="22756" citStr="Taboada et al., 2011" startWordPosition="3659" endWordPosition="3662">o MEM. Most base predictors operate on the phrase level; our goal is to construct features for the entire sentence. Denote by nbi the number of phrases in the i-th sentence covered by base predictor b, and let ob ijdenote a set of associated features. Features ob ij may or may not correspond directly to the features of base predictor b; see the discussion below. A i)−1 � straightforward strategy is to set xb i = (nb j ob ij. We proceed slightly differently and average the features associated with phrases of positive prior polarity separately from those of phrases with negative prior polarity (Taboada et al., 2011). We then concatenate the averaged feature vectors, i.e., we set xb i = (�ob,pos ij �ob,neg ij ), where �ob,p ij denotes the average of the feature vectors ob ijassociated with phrases of prior polarity p. This procedure allows us to learn a different weight for each feature depending on its context (e.g., the weight of intensifier “very” may differ for positive and negative phrases). We construct xi by concatenating the sentence-level features xbi of each base predictor and a feature vector of bigrams. To integrate a base predictor, we only need to specify the relevant features and, if applic</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly D. Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan T McDonald</author>
</authors>
<title>Discovering Fine-Grained Sentiment with Latent Variable Structured Prediction Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the European Conference on Information Retrieval,</booktitle>
<pages>368--374</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan T. McDonald. 2011a. Discovering Fine-Grained Sentiment with Latent Variable Structured Prediction Models. In Proceedings of the European Conference on Information Retrieval, pages 368–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan T McDonald</author>
</authors>
<title>Semisupervised latent variable models for sentence-level sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>569--574</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan T. McDonald. 2011b. Semisupervised latent variable models for sentence-level sentiment analysis. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 569–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="12655" citStr="Wilson et al. (2005)" startWordPosition="1980" endWordPosition="1983">our experiments. 3.3 Bag-of-Opinions Rating Predictor We leverage the bag-of-opinion (BoO) model of Qu et al. (2010) as a base predictor for phrase-level ratings. The BoO model was trained from a large generic corpus without fine-grained annotations. In BoO, an opinion consists of three components: an SO-carrying word (e.g., “good”), a set of intensifiers (e.g., “very”) and a set of negators (e.g., “not”). Each opinion is scored based on these words (represented as a boolean vector b) and the polarity of the SO-carrying word (represented as sgn(r) ∈ {−1,1}) as indicated by the MPQA lexicon of Wilson et al. (2005). In particular, the score is computed as sgn(r)wTb, where w is the learned weight vector. The sign function sgn(r) ensures consistent weight assignment for intensifiers and negators. For example, an intensifier like “very” can obtain a large positive or a large negative weight depending on whether it is used with a positive or negative SO-carrying 151 word, respectively. 3.4 SO-CAL Rating Predictor The Semantic Orientation Calculator (SO-CAL) of Taboada et al. (2011) also predicts phrase-level ratings via a scoring function similar to the one of BoO. The SO-CAL predictor uses a manually creat</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report.</tech>
<contexts>
<context position="7898" citStr="Zhu and Ghahramani, 2002" startWordPosition="1202" endWordPosition="1205">In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a single phrase. As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors 150 (see Sec. 4.4). MEM is designed such that new base predictors can be integrated easily.</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="5547" citStr="Zhu et al., 2003" startWordPosition="829" endWordPosition="832">. The integrated model can be regarded as a Gaussian process (GP) model (Rasmussen, 2004) with a novel multi-expert prior. The multi-expert prior decomposes into two component distributions. The first component distribution integrates sentence-local information obtained from the base predictors. It forms a special realization of stacking (Dzeroski and Zenko, 2004) but uses the features from the base predictors instead of the actual predictions. The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) (Zhu et al., 2003; Belkin et al., 2006). It aims to improve the predictions on sentences that are not covered well enough by our base predictors. Traditional GSSL algorithms support either discrete labels (classification) or numerical labels (regression); we extend these techniques to support both types of labels simultaneously. We use a novel variant of word sequence kernels (Cancedda et al., 2003) to measure sentence similarity. Our kernel takes the relative positions of words but also their SO and synonymity into account. Our experiments indicate that MEM significantly outperforms prior work in both sentenc</context>
<context position="7872" citStr="Zhu et al., 2003" startWordPosition="1198" endWordPosition="1201">tence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from both families. 3 Base Predictors Each of our base predictors predicts the polarity or the rating of a single phrase. As indicated above, we do not use these predictions directly in MEM but instead integrate the features of the base predictors 150 (see Sec. 4.4). MEM is designed such that new base predictors</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the International Conference on Machine Learning, pages 912– 919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM international conference on Information and knowledge management,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="7563" citStr="Zhuang et al., 2006" startWordPosition="1143" endWordPosition="1146">d training data. The weakly supervised HCRF model (T¨ackstr¨om and McDonald, 2011b; T¨ackstr¨om and McDonald, 2011a) for sentence-level polarity classification is perhaps closest to our work in spirit. Similar to MEM, HCRF uses coarse-grained training data and, when available, a small amount of fine-grained sentence polarities. In contrast to MEM, HCRF does not predict the strength of semantic orientation and ignores the order of words within sentences. There exists a large number of lexicon-based methods for polarity classification (Ding et al., 2008; Choi and Cardie, 2009; Hu and Liu, 2004; Zhuang et al., 2006; Fu and Wang, 2010; Ku et al., 2008). The lexicon-based methods of (Taboada et al., 2011; Qu et al., 2010) also predict ratings at the phrase level; these methods are used as experts in our model. MEM leverages ideas from ensemble learning (Dietterichl, 2002; Bishop, 2006) and GSSL methods (Zhu et al., 2003; Zhu and Ghahramani, 2002; Chapelle et al., 2006; Belkin et al., 2006). We extend GSSL with support for multiple, heterogenous labels. This allows us to integrate our base predictors as well as the available training data into a unified model that exploits that strengths of algorithms from</context>
<context position="9976" citStr="Zhuang et al., 2006" startWordPosition="1532" endWordPosition="1535">onsider the phrase “I began this novel with the greatest of hopes [...]”. Here, “greatest” has a positive semantic orientation in all subjectivity lexicons, but the combination “greatest of hopes” often indicates a negative sentiment. We refer to a pair of SO-carrying word (“greatest”) and a target word (“hopes”) as an opinion-target pair. Our statistical polarity predictor learns the polarity of opinions and targets jointly, which increases the robustness of its predictions. Syntactic dependency relations of the form A R −→ B are a strong indicator for opinion-target pairs (Qiu et al., 2009; Zhuang et al., 2006); e.g., “great” nmod −−−→“product”. To achieve high precision, we only consider pairs connected by the following predefined set of shortest dependency paths: verbsubj ←−−noun, verb obj ←− noun, adj nmod −−−→ noun, adj prd −−→ verbsubj ←−−noun. We only retain opiniontarget pairs that are sufficiently frequent. For each extracted pair z, we count how often it co-occurs with each document polarity y ∈ Y, where Y = {positive, negative, other} denotes the set of polarities. If z occurs in a document but is preceded by a negator, we treat it as a co-occurrence of opposite document polarity. If z occ</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiaoyan Zhu. 2006. Movie review mining and summarization. In Proceedings of the ACM international conference on Information and knowledge management, pages 43–50.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997774">
Optimising Incremental Dialogue Decisions Using Information Density for
Interactive Systems
</title>
<author confidence="0.999775">
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
</author>
<affiliation confidence="0.9817955">
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh, Scotland
</affiliation>
<email confidence="0.991776">
n.s.dethlefs  |h.hastie  |v.t.rieser  |o.lemon@hw.ac.uk
</email>
<sectionHeader confidence="0.995505" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999371454545455">
Incremental processing allows system design-
ers to address several discourse phenomena
that have previously been somewhat neglected
in interactive systems, such as backchannels
or barge-ins, but that can enhance the re-
sponsiveness and naturalness of systems. Un-
fortunately, prior work has focused largely
on deterministic incremental decision mak-
ing, rendering system behaviour less flexible
and adaptive than is desirable. We present a
novel approach to incremental decision mak-
ing that is based on Hierarchical Reinforce-
ment Learning to achieve an interactive op-
timisation of Information Presentation (IP)
strategies, allowing the system to generate
and comprehend backchannels and barge-ins,
by employing the recent psycholinguistic hy-
pothesis of information density (ID) (Jaeger,
2010). Results in terms of average rewards
and a human rating study show that our learnt
strategy outperforms several baselines that are
not sensitive to ID by more than 23%.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945363636364">
Recent work on incremental systems has shown
that adapting a system’s turn-taking behaviour to be
more human-like can improve the user’s experience
significantly, based on incremental models of auto-
matic speech recognition (ASR) (Baumann et al.,
2011), dialogue management (Buss et al., 2010), and
speech generation (Skantze and Hjalmarsson, 2010).
All of these approaches are based on the same gen-
eral abstract architecture of incremental processing
(Schlangen and Skantze, 2011). While this archi-
tecture offers inherently incremental mechanisms to
</bodyText>
<page confidence="0.980508">
82
</page>
<bodyText confidence="0.9998584">
update and revise input hypotheses, it is affected
by a number of drawbacks, shared by determinis-
tic models of decision making in general: they rely
on hand-crafted rules which can be time-consuming
and expensive to produce, they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, they are unable to gener-
alise and adapt flexibly to unseen situations, and
they do not use automatic optimisation. Statisti-
cal approaches to incremental processing that ad-
dress some of these problems have been suggested
by Raux and Eskenazi (2009), who use a cost matrix
and decision theoretic principles to optimise turn-
taking in a dialogue system under the constraint that
users prefer no gaps and no overlap at turn bound-
aries. Also, DeVault et al. (2009) use maximum en-
tropy classification to support responsive overlap in
an incremental system by predicting the completions
of user utterances. Selfridge et al. (2011) use logis-
tic regression models to predict the stability and ac-
curacy of incremental speech recognition results to
enhance performance without causing delay. For re-
lated work on (deterministic) incremental language
generation, please see (Kilger and Finkler, 1995;
Purver and Otsuka, 2003).
</bodyText>
<listItem confidence="0.368175">
Recent years have seen a number of data-driven
approaches to interactive systems that automatically
adapt their decisions to the dialogue context us-
ing Reinforcement Learning (Levin et al., 2000;
</listItem>
<note confidence="0.960052571428571">
Walker, 2000; Young, 2000; Singh et al., 2002;
Pietquin and Dutoit, 2006; Henderson et al., 2008;
Cuay´ahuitl et al., 2010; Thomson, 2009; Young et
al., 2010; Lemon, 2011; Janarthanam and Lemon,
2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs,
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.994616896551724">
2011; Dethlefs and Cuay´ahuitl, 2011). While these
approaches have been shown to enhance the perfor-
mance and adaptivity of interactive systems, unfor-
tunately none of them has yet been combined with
incremental processing.
In this paper, we present a novel approach to in-
cremental decision making for output planning that
is based on Hierarchical Reinforcement Learning
(HRL). In particular, we address the problem of op-
timising IP strategies while allowing the system to
generate and comprehend backchannels and barge-
ins based on a partially data-driven reward func-
tion. Generating backchannels can be beneficial for
grounding in interaction. Similarly, barge-ins can
lead to more efficient interactions, e.g. when a sys-
tem can clarify a bad recognition result immediately
before acting based on a misrecognition.
A central concept to our approach is Information
Density (ID) (Jaeger, 2010), a psycholinguistic hy-
pothesis that human utterance production is sensitive
to a uniform distribution of information across the
utterance. This hypothesis has also been adopted for
low level output planning recently, see e.g. Rajku-
mar and White (2011). Our results in terms of av-
erage rewards and a human rating study show that a
learning agent that is sensitive to ID can learn when
it is most beneficial to generate feedback to a user,
and outperforms several other agents that are not
sensitive to ID.
</bodyText>
<sectionHeader confidence="0.995898" genericHeader="method">
2 Incremental Information Presentation
</sectionHeader>
<subsectionHeader confidence="0.499252">
2.1 Information Presentation Strategies
</subsectionHeader>
<bodyText confidence="0.9998542">
Our example domain of application is the Infor-
mation Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al. (2010). This previous work
incrementally constructs IP strategies according to
the predicted user reaction, whereas our approach
focuses on whether and when to generate backchan-
nels and barge-ins and how to react to user barge-
ins in the context of dynamically changing input hy-
potheses. We therefore implement a simplified ver-
sion of Rieser et al.’s model. Their system distin-
guished two steps: the selection of an IP strategy
and the selection of attributes to present to the user.
We assume here that the choice of attributes is deter-
mined by matching the types specified in the user in-
put, so that our system only needs to choose a strat-
egy for presenting its results. Attributes include cui-
sine, food quality, location, price range and service
quality of a restaurant. The system then performs a
database lookup and chooses among three main IP
strategies summary, comparison, recommendation
and several ordered combinations of these. Please
see Rieser et al. (2010) for details. Table 1 shows
examples of the main types of IP strategies that we
generate.
</bodyText>
<subsectionHeader confidence="0.998045">
2.2 Backchannels and Barge-ins
</subsectionHeader>
<bodyText confidence="0.99987709375">
An important advantage of incremental processing
can be the increased reactiveness of systems. In this
paper, we focus on the phenomena of backchannels
and barge-ins that can act as feedback in an interac-
tion for both user and system. Figure 1 shows some
examples. Backchannels can often be interpreted as
signals of grounding. Coming from the user, the sys-
tem may infer that the user is following the presenta-
tion of information or is confirming a piece of infor-
mation without trying to take the turn. Similarly, we
can allow a system to generate backchannels to the
user to confirm that it understands the user’s prefer-
ences, i.e. receives high confidence scores from the
ASR module. An important decision for a dialogue
system is then when to generate a backchannel?
Barge-ins typically occur in different situations.
The user may barge-in on the system to correct an
ASR error (such as ‘Italian’ instead of ‘Indian’ in
Figure 1) or the system may want to barge-in on the
user to confirm a low-confidence ASR hypothesis so
as to be able to start an immediate database look up
for results. In the former case, the user barging-in
on the system, we assume that the system has two
choices: yielding the turn to the user, or trying to
keep the turn. In the latter case, the system barging-
in on the user, the system would have to decide if and
when it would be beneficial to barge-in on a user ut-
terance. In the following sections, we will develop
a model of dialogue optimisation that can address
these question based on Hierarchical RL that opti-
mises system behaviour based on trade-offs defined
in terms of ID.
</bodyText>
<page confidence="0.99795">
83
</page>
<subsectionHeader confidence="0.5871605">
Type Example
Comparison The restaurant Roma is in the medium price range, but does not serve excellent food.
</subsectionHeader>
<bodyText confidence="0.803644666666667">
The restaurants Firenze and Verona both have great food but are more expensive. The
restaurant Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expen-
sive, but has great food and service.
Summary I found 24 Italian restaurants in the city centre that match your query. 11 of them are
in the medium price range, 5 are cheap and 8 are expensive.
</bodyText>
<tableCaption confidence="0.9892335">
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
</tableCaption>
<figure confidence="0.578669222222222">
Backchannel 1 (the system backchannels)
USR I want Italian food [500 ms] in the city centre...
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range, but does not
have great food. The restaurants Firenze and Verona ...
Backchannel 2 (the user backchannels)
USR I want Italian food in the centre of town ...
SYS OK. I found 35 central Italian restaurants ...
USR OK.
SYS The restaurant Verona has great food but is also a bit
expensive. The Roma is cheaper, but not as central as Verona ...
Barge-ins 1 (the user barges-in on system)
USR I want Italian food in the centre of town ...
SYS I found 35 Indian ...
USR Not Indian, I want Italian.
SYS OK, Italian ...
SYS I have 24 Italian restaurants ...
</figure>
<figureCaption confidence="0.5784463">
Barge-ins 2 (the system barges-in on user)
USR I need an Italian restaurant that is located ...
SYS I’m sorry. Did you say
Indian or Italian?
USR I said Italian. And in the centre of town please.
SYS OK, let me see. I have 24 Italian restaurants ...
Figure 1: Example phenomena generated with the learnt
policy. The agent has learnt to produce backchannels
and barge-ins at the appropriate moment and alternative
strategies to deal with user barge-ins.
</figureCaption>
<sectionHeader confidence="0.995693" genericHeader="method">
3 Information Theory
</sectionHeader>
<bodyText confidence="0.999917733333333">
Information Theory as introduced by Shannon
(1948) is based on two main concepts: a communi-
cation channel through which information is trans-
ferred in bits and the information gain, i.e. the in-
formation load that each bit carries. For natural lan-
guage, the assumption is that people aim to com-
municate according to the channel’s capacity, which
corresponds to the hearer’s capacity in terms of cog-
nitive load. If they go beyond that, the cognitive load
of the listener gets too high. If they stay (far) below,
too little information is transferred per bit (i.e., the
utterance is inefficient or uninformative). The in-
formation gain of each word, which is indicative of
how close we are to the channel’s capacity, can be
computed using entropy measures.
</bodyText>
<subsectionHeader confidence="0.994517">
3.1 Information Density
</subsectionHeader>
<bodyText confidence="0.999737941176471">
Psycholinguistic research has presented evidence for
users distributing information across utterances uni-
formly, so that each word is carrying roughly the
same amount of information. This has been ob-
served for phonetic phenomena based on words
(Bell et al., 2003) and syllables (Aylett and Turk,
2004), and for syntactic phenomena (Levy and
Jaeger, 2007; Jaeger, 2010). Relating ID to likeli-
hood, we can say that the less frequent a word is, the
more information it is likely to carry (Jaeger, 2010).
For example the word ‘the’ often has a high corpus
frequency but a low ID.
The ID is defined as the log-probability of an
event (i.e. a word) (Shannon, 1948; Levy and Jaeger,
2007), so that for an utterance u consisting of the
word sequence w1 ... wi−1, we can compute the ID
at each point during the utterance as:
</bodyText>
<equation confidence="0.975029333333333">
1 n 1 (1)
log P(u) _ log P(wi|w1 ... wi−1)
i=1
</equation>
<bodyText confidence="0.9998545">
While typically the context of a word is given by
all preceding words of the utterance, we follow Gen-
zel and Charniak (2002) in restricting our computa-
tion to tri-grams for computability reasons. Given a
</bodyText>
<page confidence="0.996898">
84
</page>
<bodyText confidence="0.999820125">
language model of the domain, we can therefore op-
timise ID in system-generated discourse, where we
treat ID as “an optimal solution to the problem of
rapid yet error-free communication in a noisy envi-
ronment” (Levy and Jaeger (2007), p.2). We will
now transfer the notion of ID to IP and investigate
the distribution of information over user restaurant
queries.
</bodyText>
<subsectionHeader confidence="0.99958">
3.2 Information Density in User Utterances
</subsectionHeader>
<bodyText confidence="0.99989196875">
We aim to use ID for incremental IP in two ways:
(1) to estimate the best moment for generating
backchannels or barge-ins to the user, and (2) to de-
cide whether to yield or keep the current system turn
in case of a user barge-in. While we do not have spe-
cific data on human barge-in behaviour, we know
from the work of (Jaeger, 2010), e.g., that ID influ-
ences human language production. We therefore hy-
pothesise a relationship between ID and incremen-
tal phenomena. A human-human data collection is
planned for the near future.
To compute the ID of user and system utterances
at each time step, we estimated an n−gram lan-
guage model (using Kneser-Ney smoothing) based
on a transcribed corpus of human subjects interact-
ing with a system for restaurant recommendations of
Rieser et al. (2011).1 The corpus contained user ut-
terances as exemplified in Figure 1 and allowed us to
compute the ID at any point during a user utterance.2
In this way, we can estimate points of low density
which may be eligible for a barge-in or a backchan-
nel. Figure 2 shows some example utterances drawn
from the corpus and their ID including the first sen-
tence from Figure 1. These examples were typical
for what could generally be observed from the cor-
pus. We see that while information is transmitted
with varying amounts of density, the main bits of in-
formation are transmitted at a scale between 2 and
7.
Due to a lack of human data for the system utter-
ances, we use the same corpus data to compute the
ID of system utterances.3 The learning agent can use
</bodyText>
<footnote confidence="0.966294">
1Available at http://www.macs.hw.ac.uk/
</footnote>
<bodyText confidence="0.6134222">
ilabarchive/classicproject/data/login.php.
2Note that our model does not currently handle out-of-
domain words. In future work, we will learn when to seek clar-
ification.
3We plan a data collection of such utterances for the future,
</bodyText>
<figure confidence="0.964934">
I want Italian food in the city centre.
Yes, I need a moderately priced restaurant in the New Chesterton area.
I need the address of a Thai restaurant.
14
12
10
8
6
4
2
2 3 4 5 6 7 8 9 10 11 12
Time
</figure>
<figureCaption confidence="0.97067">
Figure 2: Information Density for example utterances,
where peaks indicate places of high density.
</figureCaption>
<bodyText confidence="0.999762">
this information to consider the trade-off of yielding
a current turn to the user or trying to keep it, e.g., in
case of a user barge-in given the ID of its own turn
and of the user’s incoming turn. Such decisions will
be made incrementally in our domain given dynam-
ically changing hypotheses of user input.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="method">
4 Incremental Utterance Optimisation
</sectionHeader>
<bodyText confidence="0.9996875">
To optimise incremental decision making for an in-
teractive system given the optimisation measure of
ID, we formalise the dialogue module as a Hierar-
chical Reinforcement Learning agent and learn an
optimal action policy by mapping states to actions
and optimising a long-term reward signal. The di-
alogue states can be seen as representing the sys-
tem’s knowledge about the task, the user and the
environment. The dialogue actions correspond to
the system’s capabilities, such as present the re-
sults or barge-in on the user. They also handle in-
cremental updates in the system. In addition, we
need a transition function that specifies the way
that actions change the environment (as expressed
in the state representation) and a reward function
which specifies a numeric value for each action
taken. In this way, decision making can be seen
as a finite sequence of states, actions and rewards
{s0, a0, r1, s1, a1, ..., rt−1, st1, where the goal is to
induce an optimal strategy automatically using Rein-
forcement Learning (RL) (Sutton and Barto, 1998).
We used Hierarchical RL, rather than flat RL, be-
cause the latter is affected by the curse of dimen-
sionality, the fact that the state space grows expo-
nentially according to the state variables taken into
account. This affects the scalability of flat RL agents
but for now make the assumption that using the corpus data is
informative since they are from the same domain.
</bodyText>
<figure confidence="0.998029166666667">
Information Density
20
18
16
0
1
</figure>
<page confidence="0.995585">
85
</page>
<bodyText confidence="0.99989732">
and limits their application to small-scale problems.
Since timing is crucial for incremental approaches,
where processing needs to be fast, we choose a hi-
erarchical setting for better scalability. We denote
the hierarchy of RL agents as Mij where the in-
dexes i and j only identify an agent in a unique
way, they do not specify the execution sequence of
subtasks, which is subject to optimisation. Each
agent of the hierarchy is defined as a Semi-Markov
Decision Process (SMDP) consisting of a 4-tuple
&lt; 5ij, Aij, Tji, R� &gt;. Here, 5ij denotes the set of
states, Aij denotes the set of actions, and Tji is a
probabilistic state transition function that determines
the next state s′ from the current state s and the per-
formed action a. k j(s′, 7-|s, a) is a reward function
that specifies the reward that an agent receives for
taking an action a in state s lasting T time steps
(Dietterich, 1999). Since actions in SMDPs may
take a variable number of time steps to complete,
the variable T represents this number of time steps.
The organisation of the learning process into dis-
crete time steps allows us to define incremental hy-
pothesis updates as state updates and transitions in
an SMDP. Whenever conditions in the learning en-
vironment change, such as the recogniser’s best hy-
pothesis of the user input, we represent them as tran-
sitions from one state to another. At each time step,
the agent checks for changes in its state represen-
tation and takes the currently best action according
to the new state. The best action in an incremental
framework can also include generating a backchan-
nel to the user to indicate the status of grounding
or barging-in to confirm an uncertain piece of infor-
mation. Once information has been presented to the
user, it is committed or realised. Realised informa-
tion is represented in the agent’s state, so that it can
monitor its own output.
Actions in a Hierarchical Reinforcement learner
can be either primitive or composite. The former
are single-step actions that yield single rewards, and
the latter are multi-step actions that correspond to
SMDPs and yield cumulative rewards. Decision
making occurs at any time step of an SMDP: after
each single-step action, we check for any updates
of the environment that require a system reaction or
change of strategy. If no system action is required
(e.g. because the user is speaking), the system can
decide to do nothing. The goal of each SMDP is to
find an optimal policy 7r* that maximises the reward
for each visited state, according to
</bodyText>
<equation confidence="0.9860245">
7r*ij(s) = arg max Q*ij (s, a), (2)
aEA
</equation>
<bodyText confidence="0.99932225">
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing 7r*. We use HSMQ-Learning to induce dia-
logue policies, see (Cuay´ahuitl, 2009), p. 92.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="method">
5 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.999849">
5.1 Hierarchy of Learning Agents
</subsectionHeader>
<bodyText confidence="0.999948617647059">
The HRL agent in Figure 3 shows how the tasks of
(1) dealing with incrementally changing input hy-
potheses, (2) choosing a suitable IP strategy and (3)
presenting information, are connected. Note that
we focus on a detailed description of models M01..�
here, which deal with barge-ins and backchannels
and are the core of this paper. Please see Dethlefs et
al. (2012) for details of an RL model that deals with
the remaining decisions.
Briefly, model Mo deals with dynamic input hy-
potheses. It chooses when to listen to an incoming
user utterance (M3) and when and how to present
information (Mo...2) by calling and passing control
to a child subtask. The variable ‘incrementalStatus’
characterises situations in which a particular (incre-
mental) action is triggered, such as a floor holder ‘let
me see’, a correction or self-correction. The variable
‘presStrategy’ indicates whether a strategy for IP has
been chosen or not, and the variable ‘userReaction’
shows the user’s reaction to an IP episode. The
‘userSilence’ variable indicates whether the user is
speaking or not. The detailed state and action space
of the agents is given in Figure 4. We distinguish ac-
tions for Information Presentation (IP), actions for
attribute presentation and ordering (Slot-ordering),
and incremental actions (Incremental).
Models Mo...2 correspond to different ways of
presenting information to the user. They perform
attribute selection and ordering and then call the
child agents M2�...4 for attribute realisation. When-
ever a user barges in over the system, these agents
will decide to either yield the turn to the user or to
try and keep the turn based on information density.
The variables representing the status of the cuisine,
</bodyText>
<page confidence="0.99358">
86
</page>
<figureCaption confidence="0.9982325">
Figure 3: Hierarchy of learning agent for incremental In-
formation Presentation and Slot Ordering.
</figureCaption>
<bodyText confidence="0.999913272727273">
food, location, price and service of restaurants indi-
cate whether the slot is of interest to the user (we as-
sume that 0 means that the user does not care about
this slot), and what input confidence score is cur-
rently associated with the value of the slot. For ex-
ample, if our current best hypothesis is that the user
is interested in Indian restaurants, the variable ’sta-
tusCuisine’ will have a value between 1-3 indicating
the strength of this hypothesis. Once slots have been
presented to the user, they are realised and can only
be changed through a correction or self-correction.
Model M3 is called whenever the user is speak-
ing. The system’s main choice here is to remain
silent and listen to the user or barge-in to request
the desired cuisine, location, or price range of a
restaurant. This can be beneficial in certain situa-
tions, such as when the system is able to increase its
confidence for a slot from ‘low’ to ‘high’ through
barging-in with a direct clarification request, e.g.
Did you say Indian?’ (and thereby saving sev-
eral turns that may be based on a wrong hypoth-
esis). This can also be harmful in certain situa-
tions, though, assuming that users have a general
preference for not being barged-in on. The learning
agent will need to learn to distinguish these situa-
tions. This agent is also responsible for generating
backchannels and will over time learn the best mo-
ments to do this.
Models Mo���4 choose surface forms for presenta-
tion to the user from hand-crafted templates. They
are not the focus of this paper, however, and there-
fore not presented in detail. The state-action space
size of this agent is roughly 1.5 million.4 The agent
</bodyText>
<footnote confidence="0.812803">
4Note that a flat RL agent, in contrast, would need 8 x 1025
million state-actions to represent this problem.
</footnote>
<table confidence="0.971184866666666">
States M00
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
observeUser {0=unfilled,1=filled}
presStrategy {0=unfilled,1=filled}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions M00
IP: compare M11, recommend M12, summarise M10, sum-
mariseCompare, summariseRecommend, summariseCompar-
eRecommend,
Incremental: correct, selfCorrect, holdFloor, observeUser
Goal State M00 0, 1, 1, 0, ?
States M10...2
IDSystem={0=low,1=medium, 2=high}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}
userBargeIn {0=false, 1=true}
Actions M10...2
Slot-ordering: presentCuisine M02, presentQuality M21,
presentLocation M22, presentPrice M32, presentService M24 ,
Incremental: yieldTurn, keepTurn
Goal State M10...2 ?, V 4, 0 V 4, 0 V 4, 0 V 4, 0 V 4, ?, ?
States M13
bargeInOnUser={0=undecided,1=yes, 2=no}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
Actions M13
Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,
bargeInPrice, backchannel
Goal State M13 &gt;0, ?, 0 V 4, 0 V 4, 0 V 4
States M20...4
IDSystem={0=low,1=medium, 2=high}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
surfaceForm {0=unrealised,1=realised}
Actions M20...4
Surface Realisation: [alternative surface realisations]
e.g. ‘$number$ restaurants serve $cuisine$ food’, ‘$number$
places are located in $area$, etc.
Goal State M20...4 ?, ?, 1
</table>
<figureCaption confidence="0.695875285714286">
Figure 4: The state and action space of the HRL agent.
The goal state is reached when all items (that the user
specified in the search query) have been presented. Ques-
tion marks mean that a variable does not affect the goal
state, which can be reached regardless of the variable’s
value.
reaches its goal state (defined w.r.t. the state vari-
</figureCaption>
<figure confidence="0.998955692307692">
Observe
Summary Comparison Recommendation User
Present
Cuisine
Present
Food
Root
Present
Service
Present
Location
Present
Price
</figure>
<page confidence="0.996674">
87
</page>
<bodyText confidence="0.991323">
ables in Fig. 4) when an IP strategy has been chosen
and all information has been presented.
</bodyText>
<subsectionHeader confidence="0.995348">
5.2 The Simulated Environment
</subsectionHeader>
<bodyText confidence="0.999985555555555">
For a policy to converge, a learning agent typically
needs several thousand interactions in which it is ex-
posed to a multitude of different circumstances. For
our domain, we designed a simulated environment
with three main components addressing IP, incre-
mental input hypotheses and ID. Using this simula-
tion, we trained the agent for 10 thousand episodes,
where one episode corresponds to one recommenda-
tion dialogue.
</bodyText>
<subsubsectionHeader confidence="0.902044">
5.2.1 Information Presentation
</subsubsectionHeader>
<bodyText confidence="0.999934210526316">
To learn a good IP strategy, we use a user simula-
tion5 by Rieser et al. (2010) which was estimated
from human data and uses bi-grams of the form
P(au,t|IPs,t), where au,t is the predicted user reac-
tion at time t to the system’s IP strategy IPs,t in state
s at time t. We distinguish the user reactions of se-
lect a restaurant, addMoreInfo to the current query to
constrain the search, and other. The last category is
usually considered an undesirable user reaction that
the system should learn to avoid. The simulation
uses linear smoothing to account for unseen situa-
tions. In this way, we can predict the most likely
user reaction to each system action. Even though
previous work has shown that n-gram-based simu-
lations can lead to dialogue inconsistencies, we as-
sume that for the present study this does not present
a problem, since we focus on generating single utter-
ances and on obtaining user judgements for single,
independent utterances.
</bodyText>
<subsectionHeader confidence="0.60515">
5.2.2 Input Hypothesis Updates
</subsectionHeader>
<bodyText confidence="0.9998214">
While the IP strategies can be used for incremen-
tal and non-incremental dialogue, the second part of
the simulation deals explicitly with the dynamic en-
vironment updates that the system will need to be
sensitive to in an incremental setting. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores for each value are
</bodyText>
<footnote confidence="0.7826665">
5The simulation data are available from www.
classic-project.org.
</footnote>
<bodyText confidence="0.999977263157895">
shown in Table 2. A score of 0 means that the user
does not care about the attribute, 1 means that the
system’s confidence in the attribute’s value is low, 2
that the confidence is medium, and 3 means that the
confidence is high. A value of 4 means that the at-
tribute has already been realised, i.e. communicated
to the user. At the beginning of a learning episode,
we assign each attribute a possible value and con-
fidence score with equal probability. For food and
service quality, we assume that the user is never in-
terested in bad food or service. Subsequently, con-
fidence scores can change at each time step. In fu-
ture work these transition probabilities will be esti-
mated from a data collection, though the following
assumptions are realistic based on our experience.
We assume that a confidence score of 0 changes to
any other value with a likelihood of 0.05. A confi-
dence score of 1 changes with a probability of 0.3,
a confidence score of 2 with a probability of 0.1
and a confidence score of 3 with a probability of
0.03. Once slots have been realised, their value is
set to 4. They cannot be changed then without an ex-
plicit correction. We also assume that realised slots
change with a probability of 0.1. If they change,
we assume that half of the time, the user is the ori-
gin of the change (because they changed their mind)
and half of the time the system is the origin of the
change (because of an ASR or interpretation error).
Each time a confidence score is changed, it has a
probability of 0.5 for also changing its value. The
resulting input to the system are data structures of
the form present(cuisine=Indian), confidence=low.
The probability of observing this data structure in
our simulation is 0.1 (for Indian) x 0.2 (for low
confidence) = 0.02. Its probability of changing
to present(cuisine=italian), confidence=high is 0.1
(for changing from low to medium) x 0.05 (for
changing from Indian to Italian) = 0.005.
</bodyText>
<subsectionHeader confidence="0.556264">
5.2.3 Information Density Updates
</subsectionHeader>
<bodyText confidence="0.999520875">
We simulate ID of user utterances based on proba-
bilistic context-free grammars (PCFG) that were au-
tomatically induced from the corpus data in Section
3.2 using the ABL algorithm (van Zaanen, 2000).
This algorithm takes a set of strings as input and
computes a context-free grammar as output by align-
ing strings based on Minimum Edit Distance. We
use the n−gram language models trained earlier to
</bodyText>
<page confidence="0.998815">
88
</page>
<table confidence="0.906990095238095">
Attribute Values
Cuisine Chinese, French, German, In-,
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Quality bad, adequate, good, very good
Location 7 distinct areas of the city
Price cheap, good-price-for-value,
expensive, very expensive
Service bad, adequate, good, very good
agent receives
+100
0
R= -100
-0.5
{ -1
Confidence
0, 1, 2, 3, 4
0, 1, 2, 3, 4
0, 1, 2, 3, 4
0, 1, 2, 3, 4
0, 1, 2, 3, 4
</table>
<bodyText confidence="0.969281857142857">
if the user selects an item,
if the user adds further con-
straints to the search,
if the user does something else
or a self-correction,
for the system holding a turn,
otherwise.
</bodyText>
<tableCaption confidence="0.9900015">
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
</tableCaption>
<bodyText confidence="0.999946523809524">
add probabilities to grammar rules. We use these
PCFGs to simulate user utterances to which the sys-
tem has to react. They can be meaningful utter-
ances such as ‘Show me restaurants nearby’ or less
meaningful fragments such as ‘um let me see, do
you... hm’. The former type is more frequent in
the data, but both types can be simulated along with
their ID (clearly, the first type is more dense than the
second).
In addition to simulating user utterances, we
hand-crafted context-free grammars of system ut-
terances and augmented them with probabilities es-
timated using the same user corpus data as above
(where again, we make the assumption that this is
to some extent feasible given the shared domain).
We use the simulated system utterances to compute
varying degrees of ID for the system.
Both measures, the ID of user and system utter-
ances, can inform the system during learning to bal-
ance the trade-off between them for generating and
receiving backchannels and barge-ins.
</bodyText>
<subsectionHeader confidence="0.9982755">
5.3 A Reward Function for Incremental
Dialogue Based on Information Density
</subsectionHeader>
<bodyText confidence="0.987532179487179">
To train the HRL agent, we use a partially data-
driven reward function. For incremental IP, we use
rewards that are based on human intuition. The
The agent is encouraged to choose those sequences
of actions that lead to the user selecting a restaurant
as quickly as possible. If the agent is not sure what to
say (because planning has not finished), it can gen-
erate a floor holding marker, but should in any case
avoid a self-correction due to having started speak-
ing too early.
The remaining rewards are based on ID scores
computed incrementally during an interaction. The
agent receives the following rewards, where info-
Density(Usr) and infoDensity(Sys) refer to the ID of
the current user and system utterance, respectively,
as defined in Equation 1.
{ -infoDensity(Usr) for keeping a turn,
barging-in or
a backchannel,
-infoDensity(Sys) for yielding a turn.
These two measures encourage the agent to consider
the trade-offs between its own ID and the one trans-
mitted by an incoming user utterance. Barging-in
on a user utterance at a low ID point then yields a
small negative reward, whereas barging-in on a user
utterance at a high ID point yields a high negative
reward. Both rewards are negative because barging-
in on the user always contains some risk. Similarly,
keeping a turn over a non-dense user utterance re-
ceives a smaller negative reward than keeping it over
a dense user utterance. A reward of −2 is assigned
for barging-in over a user utterance fragment with a
falling ID to reflect results from a qualitative study
of our corpus data: humans tend to barge-in between
information peaks, so that a barge-in to clarify a low-
confidence slot appears immediately before the ID is
rising again for a new slot. The exact best moment
for barge-ins and backchannels to occur will be sub-
ject to optimisation.
</bodyText>
<equation confidence="0.690705">
R=
</equation>
<page confidence="0.999343">
89
</page>
<sectionHeader confidence="0.995193" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.983954222222223">
The agent learns to barge-in or generate backchan-
nels to users at points where the ID is low but rising.
In particular, the agent learns to barge-in right before
information density peaks in an incoming user utter-
ance to clarify or request slots that are still open from
the previous information density peak. If a user has
specified their desired cuisine type but the system
has received a low ASR confidence score for it, it
may barge-in to clarify the slot. This case was illus-
trated in the last example in Figure 1, where the sys-
tem clarified the previous (cuisine) slot (which is as-
sociated with a high ID) just before the user specifies
the location slot (which again would have a high ID).
The main benefit the system can gain through clar-
ification barge-ins is to avoid self-corrections when
having acted based on a low ASR confidence, lead-
ing to more efficient interactions.
The system learns to generate backchannels after
information peaks to confirm newly acquired slots
that have a high confidence. An example is shown
in the first dialogue fragment in Figure 1.
In addition, the system learns to yield its current
turn to a user that is barging-in if its own ID is low,
falling or rising, or if the ID of the incoming user
utterance is high. If the system’s own ID is high, but
the user’s is not, it will try to keep the turn.6 This is
exemplified in the third dialogue fragment in Figure
1.
We compare our learnt policy against two base-
lines. Baseline 1 was designed to always generate
barge-ins after an information peak in a user utter-
ance, i.e. when ID has just switched from high to
falling. We chose this baseline to confirm that users
indeed prefer barge-ins before information peaks
rather than at any point of low ID. Baseline 1 yields
a turn to a user barge-in if its own ID is low and tries
to keep it otherwise. Baseline 2 generates barge-ins
and backchannels randomly and at any point during
a user utterance. The decision of yielding or keeping
a turn in case of a user barge-in is also random. Both
baselines also use HRL to optimise their IP strategy.
We do not compare different IP strategies, which has
been done in detail by Rieser et al. (2010). All re-
6Incidentally, this also helps to prevent the system yielding
its turn to a user backchannel; cf. Example 2 in Fig. 1.
</bodyText>
<figure confidence="0.8800055">
101 102 103 104
Episodes
</figure>
<figureCaption confidence="0.975206">
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the HRL agent and its baselines.
</figureCaption>
<bodyText confidence="0.872865">
sults are summarised in Table 3.
</bodyText>
<subsectionHeader confidence="0.998554">
6.1 Average Rewards over Time
</subsectionHeader>
<bodyText confidence="0.999958275862069">
Figure 5 shows the performance of all systems in
terms of average rewards in simulation. The learnt
policy outperforms both baselines. While the learnt
policy and Baseline 1 appear to achieve similar per-
formance, an absolute comparison of the last 1000
episodes of each behaviour shows that the improve-
ment of the HRL agent over Baseline 1 corresponds
to 23.42%. The difference between the learnt policy
and its baselines is significant at p &lt; 0.0001 accord-
ing to a paired t-test and has a high effect size of
r = 0.85.
The main reason for these different performances
is the moment each system will barge-in. Since
Baseline 1 barges-in on users after an information
peak, when ID may still be high, it continuously re-
ceives a negative reward reflecting the user prefer-
ence for late barge-ins. As a result of this contin-
uous negative reward, the agent will then learn to
avoid barge-ins altogether, which may in turn lead
to less efficient interactions because low confidence
ASR scores are clarified only late in the interaction.
The main problem of the random barge-ins of
Baseline 2 is that users may often have to restart
a turn because the system barged-in too early or
in the middle of an information peak. In addition,
Baseline 2 needs to occasionally self-correct its own
utterances because it started to present information
too early, when input hypotheses were not yet stable
enough to act upon them.
</bodyText>
<figure confidence="0.994896785714286">
60
40
20
0
−20
−40
−60
−80
−100
−120
Learnt
Baseline1
Baseline2
Average Reward
</figure>
<page confidence="0.939476">
90
</page>
<table confidence="0.95342475">
Policy Average Reward User Rating (%)
Learnt 55.54**,* 43%**
Baseline 1 45.0** 26%
Baseline 2 1.47 31%
</table>
<tableCaption confidence="0.996090666666667">
Table 3: Comparison of policies in terms of average re-
wards and user ratings. * indicates a significant improve-
ment over Baseline 1 and ** over Baseline 2.
</tableCaption>
<subsectionHeader confidence="0.99864">
6.2 Human Rating Study
</subsectionHeader>
<bodyText confidence="0.999766194444444">
To confirm our simulation-based results, we con-
ducted a user rating study on the CrowdFlower
crowd sourcing platform.7 Participants were
shown user utterances along with three options of
barging-in over them. For example:  |I want
[OPTION 1] Italian food [OPTION 2] in the
city [OPTION 3] centre|, where OPTION 1 cor-
responds to the learnt policy, OPTION 2 to Baseline
2 and OPTION 3 to Baseline 1.
Users were asked to choose one option which they
considered the best moment for a barge-in. Partici-
pants in the study rated altogether 144 utterances.
They preferred the learnt system 63 times (43%),
Baseline 1 37 times (26%) and Baseline 2 44 times
(31%). This is statistically significant at p &lt; 0.02
according to a Chi-Square test (x2 = 7.542, df =
2). In a separate test, directly comparing the learnt
policy and Baseline 1, learnt was chosen signifi-
cantly more often than Baseline 1; i.e. 79% of the
time (for 127 utterances, using a 1-tailed Sign test,
p &lt; 0.0001). Finally, learnt was directly compared
to Baseline 2 and shown to be significantly more of-
ten chosen; i.e. 59% of the time (138 utterances, 1-
tailed Sign test, p &lt; 0.025). These results provide
evidence that an optimisation of the timing of gener-
ating barge-ins and backchannels in incremental di-
alogue can be sensitive to fine-grained cues in evolv-
ing ID and therefore achieve a high level of adaptiv-
ity. Such sensitivity is difficult to hand-craft as can
be concluded w.r.t. the performance of Baseline 1,
which received similar rewards to learnt in simula-
tion, but is surprisingly beaten by the random Base-
line 2 here. This indicates a strong human dislike
for late barge-ins. The bad performance of Base-
line 2 in terms of average rewards was due to the
random barge-ins leading to less efficient dialogues.
</bodyText>
<footnote confidence="0.926732">
7www.crowdflower.com
</footnote>
<bodyText confidence="0.999584727272727">
Regarding user ratings however, Baseline 2 was pre-
ferred over Baseline 1. This is most likely due to the
timing of barge-ins: since Baseline 2 has a chance
of barging-in at earlier occasions than Baseline 1,
it may have received better ratings. The evaluation
shows that humans care about timing of a barge-in
regarding the density of information that is currently
conveyed and dislike late barge-ins. ID is then useful
in determining when to barge-in. We can therefore
further conclude that ID can be a feasible optimisa-
tion criterion for incremental decision making.
</bodyText>
<sectionHeader confidence="0.990331" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999951714285714">
We have presented a novel approach to incremen-
tal dialogue decision making based on Hierarchical
RL combined with the notion of information den-
sity. We presented a learning agent in the domain of
IP for restaurant recommendations that was able to
generate backchannels and barge-ins for higher re-
sponsiveness in interaction. Results in terms of av-
erage rewards and a human rating study have shown
that a learning agent that is optimised based on a
partially data-driven reward function that addresses
information density can learn to decide when and if
it is beneficial to barge-in or backchannel on user
utterances and to deal with backchannels and barge-
ins from the user. Future work can take several di-
rections. Given that ID is a measure influencing
human language production, we could replace our
template-based surface realiser by an agent that op-
timises the information density of its output. Cur-
rently we learn the agent’s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situa-
tions could be achieved if the agent was able to learn
from ongoing interactions. Finally, we can confirm
the human results obtained from an overhearer-style
evaluation in a real interactive setting and explicitly
extend our language model to discourse phenomena
such as pauses or hesitations to take them into ac-
count in measuring ID.
</bodyText>
<sectionHeader confidence="0.996517" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9756858">
The research leading to this work has received funding
from EC’s FP7 programmes: (FP7/2011-14) under grant
agreement no. 287615 (PARLANCE); (FP7/2007-13) un-
der grant agreement no. 216594 (CLASSiC); (FP7/2011-
14) under grant agreement no. 270019 (SPACEBOOK);
</bodyText>
<page confidence="0.996152">
91
</page>
<bodyText confidence="0.9983055">
and (FP7/2011-16) under grant agreement no. 269427
(STAC). Many thanks to Michael White for discussion
of the original idea of using information density as an op-
timisation metric.
</bodyText>
<sectionHeader confidence="0.988785" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998795232323232">
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for
the relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31–56.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in english conver-
sation. Journal of the Acoustic Society of America,
113(2):1001–1024.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proceedings of 11th An-
nual SIGdial Meeting on Discourse and Dialogue.
Heriberto Cuay´ahuitl and Nina Dethlefs. 2011.
Spatially-aware Dialogue Control Using Hierarchi-
cal Reinforcement Learning. ACM Transactions on
Speech and Language Processing (Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue System), 7(3).
Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395–429.
Heriberto Cuay´ahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD Thesis,
University of Edinburgh, School of Informatics.
Nina Dethlefs and Heriberto Cuay´ahuitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the International
Conference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Pro-
ceedings of the 10th Annual SigDial Meeting on Dis-
course and Dialogue, Queen Mary University, UK.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227–303.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
Rate Constancy in Text. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 199–206.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid Reinforcement/Supervised Learning of
Dialogue Policies from Fixed Data Sets. Computa-
tional Linguistics, 34(4):487–511.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61:23–62.
Srini Janarthanam and Oliver Lemon. 2010. Learning
to Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 69–78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Oliver Lemon. 2011. Learning What to Say and How to
Say It: Joint Optimization of Spoken Dialogue Man-
agement and Natural Language Generation.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A Stochastic Model of Computer-Human Inter-
action for Learning Dialogue Strategies. IEEE Trans-
actions on Speech and Audio Processing, 8:11–23.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19.
Olivier Pietquin and Dutoit. 2006. A Probabilis-
tic Framework for Dialogue Simulation and Optimal
Strategy Learning. IEEE Transactions on Speech and
Audio Processing, 14(2):589–599.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Rajakrishnan Rajkumar and Michael White. 2011. Lin-
guistically Motivated Complementizer Choice in Sur-
face Realization. In Proceedings of the EMNLP-11
Workshop on Using Corpora in NLG, Edinburgh, Scot-
land.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proceedings of the 10th Conference of the
</reference>
<page confidence="0.932922">
92
</page>
<reference confidence="0.999796263157895">
North American Chapter of the Association for Com-
putational Linguistics—Human Language Technolo-
gies (NAACL-HLT), Boulder, Colorado.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
David Schlangen and Gabriel Skantze. 2011. A General,
Abstract Model of Incremental Dialogue Processing.
Dialogue and Discourse, 2(1).
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in In-
cremental Speech Recognition. In Proceedings of the
12th Annual SigDial Meeting on Discourse and Dia-
logue, Portland, Oregon.
Claude Shannon. 1948. A Mathematical Theory of
Communications. Bell Systems Technical Journal,
27(4):623–656.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105–133.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proceedings of the 11th Annual SigDial Meeting on
Discourse and Dialogue, Tokyo, Japan.
Richard Sutton and Andrew Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA.
Blaise Thomson. 2009. Statistical Methods for Spo-
ken Dialogue Management. Ph.D. thesis, University
of Cambridge.
Menno van Zaanen. 2000. Bootstrapping Syntax and
Recursion using Alignment-Based Learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning, ICML ’00, pages 1063–1070.
Marilyn Walker. 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken
Dialogue System for Email. Journal of Artificial In-
telligence Research (JAIR), 12:387–416.
Steve Young, Milica Gasic, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The Hidden Information State Model: A
Practical Framework for POMDP-based Spoken Dia-
logue Management. Computer Speech and Language,
24(2):150–174.
Steve Young. 2000. Probabilistic Methods in Spoken
Dialogue Systems. Philosophical Transactions of the
Royal Society (Series A), 358(1769):1389–1402.
</reference>
<page confidence="0.999169">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.782036">
<title confidence="0.9998695">Optimising Incremental Dialogue Decisions Using Information Density Interactive Systems</title>
<author confidence="0.998434">Nina Dethlefs</author>
<author confidence="0.998434">Helen Hastie</author>
<author confidence="0.998434">Verena Rieser</author>
<author confidence="0.998434">Oliver</author>
<affiliation confidence="0.9985165">School of Mathematical and Computer Heriot-Watt University, Edinburgh,</affiliation>
<email confidence="0.974939">n.s.dethlefs|h.hastie|v.t.rieser|o.lemon@hw.ac.uk</email>
<abstract confidence="0.991483956521739">Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision makthat is based on Reinforce- Learning achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hyof density (ID) 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are sensitive to ID by more than</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Aylett</author>
<author>Alice Turk</author>
</authors>
<title>The smooth signal redundancy hypothesis: A functional explanation for the relationships between redundancy, prosodic prominence, and duration in spontaneous speech.</title>
<date>2004</date>
<journal>Language and Speech,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="11099" citStr="Aylett and Turk, 2004" startWordPosition="1779" endWordPosition="1782">the listener gets too high. If they stay (far) below, too little information is transferred per bit (i.e., the utterance is inefficient or uninformative). The information gain of each word, which is indicative of how close we are to the channel’s capacity, can be computed using entropy measures. 3.1 Information Density Psycholinguistic research has presented evidence for users distributing information across utterances uniformly, so that each word is carrying roughly the same amount of information. This has been observed for phonetic phenomena based on words (Bell et al., 2003) and syllables (Aylett and Turk, 2004), and for syntactic phenomena (Levy and Jaeger, 2007; Jaeger, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 While typically the context of a word </context>
</contexts>
<marker>Aylett, Turk, 2004</marker>
<rawString>Matthew Aylett and Alice Turk. 2004. The smooth signal redundancy hypothesis: A functional explanation for the relationships between redundancy, prosodic prominence, and duration in spontaneous speech. Language and Speech, 47(1):31–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>Okko Buss</author>
<author>David Schlangen</author>
</authors>
<date>2011</date>
<booktitle>Evaluation and Optimisation of Incremental Processors. Dialogue and Discourse,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1527" citStr="Baumann et al., 2011" startWordPosition="209" endWordPosition="212">IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1 Introduction Recent work on incremental systems has shown that adapting a system’s turn-taking behaviour to be more human-like can improve the user’s experience significantly, based on incremental models of automatic speech recognition (ASR) (Baumann et al., 2011), dialogue management (Buss et al., 2010), and speech generation (Skantze and Hjalmarsson, 2010). All of these approaches are based on the same general abstract architecture of incremental processing (Schlangen and Skantze, 2011). While this architecture offers inherently incremental mechanisms to 82 update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by va</context>
</contexts>
<marker>Baumann, Buss, Schlangen, 2011</marker>
<rawString>Timo Baumann, Okko Buss, and David Schlangen. 2011. Evaluation and Optimisation of Incremental Processors. Dialogue and Discourse, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Bell</author>
<author>Dan Jurafsky</author>
<author>Eric Fossler-Lussier</author>
<author>Cynthia Girand</author>
<author>Michelle Gregory</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of disfluencies, predictability, and utterance position on word form variation in english conversation.</title>
<date>2003</date>
<journal>Journal of the Acoustic Society of America,</journal>
<volume>113</volume>
<issue>2</issue>
<contexts>
<context position="11061" citStr="Bell et al., 2003" startWordPosition="1773" endWordPosition="1776">eyond that, the cognitive load of the listener gets too high. If they stay (far) below, too little information is transferred per bit (i.e., the utterance is inefficient or uninformative). The information gain of each word, which is indicative of how close we are to the channel’s capacity, can be computed using entropy measures. 3.1 Information Density Psycholinguistic research has presented evidence for users distributing information across utterances uniformly, so that each word is carrying roughly the same amount of information. This has been observed for phonetic phenomena based on words (Bell et al., 2003) and syllables (Aylett and Turk, 2004), and for syntactic phenomena (Levy and Jaeger, 2007; Jaeger, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 </context>
</contexts>
<marker>Bell, Jurafsky, Fossler-Lussier, Girand, Gregory, Gildea, 2003</marker>
<rawString>Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia Girand, Michelle Gregory, and Daniel Gildea. 2003. Effects of disfluencies, predictability, and utterance position on word form variation in english conversation. Journal of the Acoustic Society of America, 113(2):1001–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko Buss</author>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>Collaborating on Utterances with a Spoken Dialogue Systen Using an ISU-based Approach to Incremental Dialogue Management.</title>
<date>2010</date>
<booktitle>In Proceedings of 11th Annual SIGdial Meeting on Discourse and Dialogue.</booktitle>
<contexts>
<context position="1568" citStr="Buss et al., 2010" startWordPosition="215" endWordPosition="218">ate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1 Introduction Recent work on incremental systems has shown that adapting a system’s turn-taking behaviour to be more human-like can improve the user’s experience significantly, based on incremental models of automatic speech recognition (ASR) (Baumann et al., 2011), dialogue management (Buss et al., 2010), and speech generation (Skantze and Hjalmarsson, 2010). All of these approaches are based on the same general abstract architecture of incremental processing (Schlangen and Skantze, 2011). While this architecture offers inherently incremental mechanisms to 82 update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to </context>
</contexts>
<marker>Buss, Baumann, Schlangen, 2010</marker>
<rawString>Okko Buss, Timo Baumann, and David Schlangen. 2010. Collaborating on Utterances with a Spoken Dialogue Systen Using an ISU-based Approach to Incremental Dialogue Management. In Proceedings of 11th Annual SIGdial Meeting on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Nina Dethlefs</author>
</authors>
<title>Spatially-aware Dialogue Control Using Hierarchical Reinforcement Learning.</title>
<date>2011</date>
<booktitle>ACM Transactions on Speech and Language Processing (Special Issue on Machine Learning for Robust and Adaptive Spoken Dialogue System),</booktitle>
<volume>7</volume>
<issue>3</issue>
<marker>Cuay´ahuitl, Dethlefs, 2011</marker>
<rawString>Heriberto Cuay´ahuitl and Nina Dethlefs. 2011. Spatially-aware Dialogue Control Using Hierarchical Reinforcement Learning. ACM Transactions on Speech and Language Processing (Special Issue on Machine Learning for Robust and Adaptive Spoken Dialogue System), 7(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Steve Renals</author>
<author>Oliver Lemon</author>
<author>Hiroshi Shimodaira</author>
</authors>
<title>Evaluation of a hierarchical reinforcement learning spoken dialogue system.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2010</marker>
<rawString>Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and Hiroshi Shimodaira. 2010. Evaluation of a hierarchical reinforcement learning spoken dialogue system. Computer Speech and Language, 24(2):395–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
</authors>
<title>Hierarchical Reinforcement Learning for Spoken Dialogue Systems.</title>
<date>2009</date>
<tech>PhD Thesis,</tech>
<institution>University of Edinburgh, School of Informatics.</institution>
<marker>Cuay´ahuitl, 2009</marker>
<rawString>Heriberto Cuay´ahuitl. 2009. Hierarchical Reinforcement Learning for Spoken Dialogue Systems. PhD Thesis, University of Edinburgh, School of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Heriberto Cuay´ahuitl</author>
</authors>
<title>Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<marker>Dethlefs, Cuay´ahuitl, 2011</marker>
<rawString>Nina Dethlefs and Heriberto Cuay´ahuitl. 2011. Combining Hierarchical Reinforcement Learning and Bayesian Networks for Natural Language Generation in Situated Dialogue. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Natural Language Generation (INLG),</booktitle>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="19354" citStr="Dethlefs et al. (2012)" startWordPosition="3215" endWordPosition="3218">EA where Qij(s, a) specifies the expected cumulative reward for executing action a in state s and then following 7r*. We use HSMQ-Learning to induce dialogue policies, see (Cuay´ahuitl, 2009), p. 92. 5 Experimental Setting 5.1 Hierarchy of Learning Agents The HRL agent in Figure 3 shows how the tasks of (1) dealing with incrementally changing input hypotheses, (2) choosing a suitable IP strategy and (3) presenting information, are connected. Note that we focus on a detailed description of models M01..� here, which deal with barge-ins and backchannels and are the core of this paper. Please see Dethlefs et al. (2012) for details of an RL model that deals with the remaining decisions. Briefly, model Mo deals with dynamic input hypotheses. It chooses when to listen to an incoming user utterance (M3) and when and how to present information (Mo...2) by calling and passing control to a child subtask. The variable ‘incrementalStatus’ characterises situations in which a particular (incremental) action is triggered, such as a floor holder ‘let me see’, a correction or self-correction. The variable ‘presStrategy’ indicates whether a strategy for IP has been chosen or not, and the variable ‘userReaction’ shows the </context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver Lemon. 2012. Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers. In Proceedings of the International Conference on Natural Language Generation (INLG), Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Can I finish? Learning when to respond to incremental interpretation result in interactive dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th Annual SigDial Meeting on Discourse and Dialogue,</booktitle>
<institution>Queen Mary University, UK.</institution>
<contexts>
<context position="2607" citStr="DeVault et al. (2009)" startWordPosition="380" endWordPosition="383">crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context</context>
</contexts>
<marker>DeVault, Sagae, Traum, 2009</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2009. Can I finish? Learning when to respond to incremental interpretation result in interactive dialogue. In Proceedings of the 10th Annual SigDial Meeting on Discourse and Dialogue, Queen Mary University, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>13--227</pages>
<contexts>
<context position="17070" citStr="Dietterich, 1999" startWordPosition="2829" endWordPosition="2830">tify an agent in a unique way, they do not specify the execution sequence of subtasks, which is subject to optimisation. Each agent of the hierarchy is defined as a Semi-Markov Decision Process (SMDP) consisting of a 4-tuple &lt; 5ij, Aij, Tji, R� &gt;. Here, 5ij denotes the set of states, Aij denotes the set of actions, and Tji is a probabilistic state transition function that determines the next state s′ from the current state s and the performed action a. k j(s′, 7-|s, a) is a reward function that specifies the reward that an agent receives for taking an action a in state s lasting T time steps (Dietterich, 1999). Since actions in SMDPs may take a variable number of time steps to complete, the variable T represents this number of time steps. The organisation of the learning process into discrete time steps allows us to define incremental hypothesis updates as state updates and transitions in an SMDP. Whenever conditions in the learning environment change, such as the recogniser’s best hypothesis of the user input, we represent them as transitions from one state to another. At each time step, the agent checks for changes in its state representation and takes the currently best action according to the n</context>
</contexts>
<marker>Dietterich, 1999</marker>
<rawString>Thomas G. Dietterich. 1999. Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. Journal of Artificial Intelligence Research, 13:227–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Entropy Rate Constancy in Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>199--206</pages>
<contexts>
<context position="11785" citStr="Genzel and Charniak (2002)" startWordPosition="1908" endWordPosition="1912">, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel and Charniak (2002) in restricting our computation to tri-grams for computability reasons. Given a 84 language model of the domain, we can therefore optimise ID in system-generated discourse, where we treat ID as “an optimal solution to the problem of rapid yet error-free communication in a noisy environment” (Levy and Jaeger (2007), p.2). We will now transfer the notion of ID to IP and investigate the distribution of information over user restaurant queries. 3.2 Information Density in User Utterances We aim to use ID for incremental IP in two ways: (1) to estimate the best moment for generating backchannels or </context>
</contexts>
<marker>Genzel, Charniak, 2002</marker>
<rawString>Dmitriy Genzel and Eugene Charniak. 2002. Entropy Rate Constancy in Text. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="3354" citStr="Henderson et al., 2008" startWordPosition="491" endWordPosition="494">ser utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental p</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2008</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets. Computational Linguistics, 34(4):487–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Florian Jaeger</author>
</authors>
<title>Redundancy and reduction: Speakers manage syntactic information density.</title>
<date>2010</date>
<pages>61--23</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="1091" citStr="Jaeger, 2010" startWordPosition="143" endWordPosition="144"> or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1 Introduction Recent work on incremental systems has shown that adapting a system’s turn-taking behaviour to be more human-like can improve the user’s experience significantly, based on incremental models of automatic speech recognition (ASR) (Baumann et al., 2011), dialogue management (Buss et al., 2010), and speech generation (Skantze and Hjalmarsson, 2010). All of these approaches are based on the same general abstract arc</context>
<context position="4633" citStr="Jaeger, 2010" startWordPosition="682" endWordPosition="683">al decision making for output planning that is based on Hierarchical Reinforcement Learning (HRL). In particular, we address the problem of optimising IP strategies while allowing the system to generate and comprehend backchannels and bargeins based on a partially data-driven reward function. Generating backchannels can be beneficial for grounding in interaction. Similarly, barge-ins can lead to more efficient interactions, e.g. when a system can clarify a bad recognition result immediately before acting based on a misrecognition. A central concept to our approach is Information Density (ID) (Jaeger, 2010), a psycholinguistic hypothesis that human utterance production is sensitive to a uniform distribution of information across the utterance. This hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example dom</context>
<context position="11166" citStr="Jaeger, 2010" startWordPosition="1791" endWordPosition="1792"> is transferred per bit (i.e., the utterance is inefficient or uninformative). The information gain of each word, which is indicative of how close we are to the channel’s capacity, can be computed using entropy measures. 3.1 Information Density Psycholinguistic research has presented evidence for users distributing information across utterances uniformly, so that each word is carrying roughly the same amount of information. This has been observed for phonetic phenomena based on words (Bell et al., 2003) and syllables (Aylett and Turk, 2004), and for syntactic phenomena (Levy and Jaeger, 2007; Jaeger, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel </context>
<context position="12606" citStr="Jaeger, 2010" startWordPosition="2055" endWordPosition="2056">lution to the problem of rapid yet error-free communication in a noisy environment” (Levy and Jaeger (2007), p.2). We will now transfer the notion of ID to IP and investigate the distribution of information over user restaurant queries. 3.2 Information Density in User Utterances We aim to use ID for incremental IP in two ways: (1) to estimate the best moment for generating backchannels or barge-ins to the user, and (2) to decide whether to yield or keep the current system turn in case of a user barge-in. While we do not have specific data on human barge-in behaviour, we know from the work of (Jaeger, 2010), e.g., that ID influences human language production. We therefore hypothesise a relationship between ID and incremental phenomena. A human-human data collection is planned for the near future. To compute the ID of user and system utterances at each time step, we estimated an n−gram language model (using Kneser-Ney smoothing) based on a transcribed corpus of human subjects interacting with a system for restaurant recommendations of Rieser et al. (2011).1 The corpus contained user utterances as exemplified in Figure 1 and allowed us to compute the ID at any point during a user utterance.2 In th</context>
</contexts>
<marker>Jaeger, 2010</marker>
<rawString>T. Florian Jaeger. 2010. Redundancy and reduction: Speakers manage syntactic information density. Cognitive Psychology, 61:23–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>69--78</pages>
<contexts>
<context position="3457" citStr="Janarthanam and Lemon, 2010" startWordPosition="507" endWordPosition="510">d accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output plannin</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>Srini Janarthanam and Oliver Lemon. 2010. Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 69–78, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Kilger</author>
<author>Wolfgang Finkler</author>
</authors>
<title>Incremental generation for real-time applications.</title>
<date>1995</date>
<tech>Technical report, DFKI Saarbruecken,</tech>
<contexts>
<context position="3034" citStr="Kilger and Finkler, 1995" startWordPosition="442" endWordPosition="445">st matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language </context>
</contexts>
<marker>Kilger, Finkler, 1995</marker>
<rawString>Anne Kilger and Wolfgang Finkler. 1995. Incremental generation for real-time applications. Technical report, DFKI Saarbruecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
</authors>
<title>Learning What to Say and How to Say It:</title>
<date>2011</date>
<journal>Joint Optimization of Spoken Dialogue Management and Natural Language Generation.</journal>
<contexts>
<context position="3428" citStr="Lemon, 2011" startWordPosition="505" endWordPosition="506"> stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decis</context>
</contexts>
<marker>Lemon, 2011</marker>
<rawString>Oliver Lemon. 2011. Learning What to Say and How to Say It: Joint Optimization of Spoken Dialogue Management and Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>A Stochastic Model of Computer-Human Interaction for Learning Dialogue Strategies.</title>
<date>2000</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>8--11</pages>
<contexts>
<context position="3256" citStr="Levin et al., 2000" startWordPosition="475" endWordPosition="478">tion to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adap</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A Stochastic Model of Computer-Human Interaction for Learning Dialogue Strategies. IEEE Transactions on Speech and Audio Processing, 8:11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>T Florian Jaeger</author>
</authors>
<title>Speakers optimize information density through syntactic reduction.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<contexts>
<context position="11151" citStr="Levy and Jaeger, 2007" startWordPosition="1787" endWordPosition="1790"> too little information is transferred per bit (i.e., the utterance is inefficient or uninformative). The information gain of each word, which is indicative of how close we are to the channel’s capacity, can be computed using entropy measures. 3.1 Information Density Psycholinguistic research has presented evidence for users distributing information across utterances uniformly, so that each word is carrying roughly the same amount of information. This has been observed for phonetic phenomena based on words (Bell et al., 2003) and syllables (Aylett and Turk, 2004), and for syntactic phenomena (Levy and Jaeger, 2007; Jaeger, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 While typically the context of a word is given by all preceding words of the utterance, we</context>
</contexts>
<marker>Levy, Jaeger, 2007</marker>
<rawString>Roger Levy and T. Florian Jaeger. 2007. Speakers optimize information density through syntactic reduction. Advances in Neural Information Processing Systems, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Pietquin</author>
<author>Dutoit</author>
</authors>
<title>A Probabilistic Framework for Dialogue Simulation and Optimal Strategy Learning.</title>
<date>2006</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="3330" citStr="Pietquin and Dutoit, 2006" startWordPosition="487" endWordPosition="490">icting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been com</context>
</contexts>
<marker>Pietquin, Dutoit, 2006</marker>
<rawString>Olivier Pietquin and Dutoit. 2006. A Probabilistic Framework for Dialogue Simulation and Optimal Strategy Learning. IEEE Transactions on Speech and Audio Processing, 14(2):589–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Masayuki Otsuka</author>
</authors>
<title>Incremental Generation by Incremental Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th UK Special-Interesting Group for Computational Linguistics (CLUK) Colloquium.</booktitle>
<contexts>
<context position="3060" citStr="Purver and Otsuka, 2003" startWordPosition="446" endWordPosition="449">oretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jej</context>
</contexts>
<marker>Purver, Otsuka, 2003</marker>
<rawString>Matthew Purver and Masayuki Otsuka. 2003. Incremental Generation by Incremental Parsing. In Proceedings of the 6th UK Special-Interesting Group for Computational Linguistics (CLUK) Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
</authors>
<title>Linguistically Motivated Complementizer Choice in Surface Realization.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP-11 Workshop on Using Corpora in NLG,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="4885" citStr="Rajkumar and White (2011)" startWordPosition="717" endWordPosition="721">ins based on a partially data-driven reward function. Generating backchannels can be beneficial for grounding in interaction. Similarly, barge-ins can lead to more efficient interactions, e.g. when a system can clarify a bad recognition result immediately before acting based on a misrecognition. A central concept to our approach is Information Density (ID) (Jaeger, 2010), a psycholinguistic hypothesis that human utterance production is sensitive to a uniform distribution of information across the utterance. This hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example domain of application is the Information Presentation phase in an interactive system for restaurant recommendations, extending previous work by Rieser et al. (2010). This previous work incrementally constructs IP strategies according to the predicted user</context>
</contexts>
<marker>Rajkumar, White, 2011</marker>
<rawString>Rajakrishnan Rajkumar and Michael White. 2011. Linguistically Motivated Complementizer Choice in Surface Realization. In Proceedings of the EMNLP-11 Workshop on Using Corpora in NLG, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Maxine Eskenazi</author>
</authors>
<title>A FiniteState Turn-Taking Model for Spoken Dialog Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th Conference of the</booktitle>
<contexts>
<context position="2396" citStr="Raux and Eskenazi (2009)" startWordPosition="343" endWordPosition="346">hitecture offers inherently incremental mechanisms to 82 update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation</context>
</contexts>
<marker>Raux, Eskenazi, 2009</marker>
<rawString>Antoine Raux and Maxine Eskenazi. 2009. A FiniteState Turn-Taking Model for Spoken Dialog Systems. In Proceedings of the 10th Conference of the</rawString>
</citation>
<citation valid="false">
<authors>
<author>North American</author>
</authors>
<title>Chapter of the Association for Computational Linguistics—Human Language Technologies (NAACL-HLT),</title>
<location>Boulder, Colorado.</location>
<marker>American, </marker>
<rawString>North American Chapter of the Association for Computational Linguistics—Human Language Technologies (NAACL-HLT), Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>Optimising Information Presentation for Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="3478" citStr="Rieser et al., 2010" startWordPosition="511" endWordPosition="514">ech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output planning that is based on Hi</context>
<context position="5394" citStr="Rieser et al. (2010)" startWordPosition="799" endWordPosition="802">his hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example domain of application is the Information Presentation phase in an interactive system for restaurant recommendations, extending previous work by Rieser et al. (2010). This previous work incrementally constructs IP strategies according to the predicted user reaction, whereas our approach focuses on whether and when to generate backchannels and barge-ins and how to react to user bargeins in the context of dynamically changing input hypotheses. We therefore implement a simplified version of Rieser et al.’s model. Their system distinguished two steps: the selection of an IP strategy and the selection of attributes to present to the user. We assume here that the choice of attributes is determined by matching the types specified in the user input, so that our s</context>
<context position="25545" citStr="Rieser et al. (2010)" startWordPosition="4106" endWordPosition="4109">rategy has been chosen and all information has been presented. 5.2 The Simulated Environment For a policy to converge, a learning agent typically needs several thousand interactions in which it is exposed to a multitude of different circumstances. For our domain, we designed a simulated environment with three main components addressing IP, incremental input hypotheses and ID. Using this simulation, we trained the agent for 10 thousand episodes, where one episode corresponds to one recommendation dialogue. 5.2.1 Information Presentation To learn a good IP strategy, we use a user simulation5 by Rieser et al. (2010) which was estimated from human data and uses bi-grams of the form P(au,t|IPs,t), where au,t is the predicted user reaction at time t to the system’s IP strategy IPs,t in state s at time t. We distinguish the user reactions of select a restaurant, addMoreInfo to the current query to constrain the search, and other. The last category is usually considered an undesirable user reaction that the system should learn to avoid. The simulation uses linear smoothing to account for unseen situations. In this way, we can predict the most likely user reaction to each system action. Even though previous wo</context>
<context position="35143" citStr="Rieser et al. (2010)" startWordPosition="5765" endWordPosition="5768">i.e. when ID has just switched from high to falling. We chose this baseline to confirm that users indeed prefer barge-ins before information peaks rather than at any point of low ID. Baseline 1 yields a turn to a user barge-in if its own ID is low and tries to keep it otherwise. Baseline 2 generates barge-ins and backchannels randomly and at any point during a user utterance. The decision of yielding or keeping a turn in case of a user barge-in is also random. Both baselines also use HRL to optimise their IP strategy. We do not compare different IP strategies, which has been done in detail by Rieser et al. (2010). All re6Incidentally, this also helps to prevent the system yielding its turn to a user backchannel; cf. Example 2 in Fig. 1. 101 102 103 104 Episodes Figure 5: Performance in terms of rewards (averaged over 10 runs) for the HRL agent and its baselines. sults are summarised in Table 3. 6.1 Average Rewards over Time Figure 5 shows the performance of all systems in terms of average rewards in simulation. The learnt policy outperforms both baselines. While the learnt policy and Baseline 1 appear to achieve similar performance, an absolute comparison of the last 1000 episodes of each behaviour sh</context>
</contexts>
<marker>Rieser, Lemon, Liu, 2010</marker>
<rawString>Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising Information Presentation for Spoken Dialogue Systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Simon Keizer</author>
<author>Xingkun Liu</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="13062" citStr="Rieser et al. (2011)" startWordPosition="2127" endWordPosition="2130"> or keep the current system turn in case of a user barge-in. While we do not have specific data on human barge-in behaviour, we know from the work of (Jaeger, 2010), e.g., that ID influences human language production. We therefore hypothesise a relationship between ID and incremental phenomena. A human-human data collection is planned for the near future. To compute the ID of user and system utterances at each time step, we estimated an n−gram language model (using Kneser-Ney smoothing) based on a transcribed corpus of human subjects interacting with a system for restaurant recommendations of Rieser et al. (2011).1 The corpus contained user utterances as exemplified in Figure 1 and allowed us to compute the ID at any point during a user utterance.2 In this way, we can estimate points of low density which may be eligible for a barge-in or a backchannel. Figure 2 shows some example utterances drawn from the corpus and their ID including the first sentence from Figure 1. These examples were typical for what could generally be observed from the corpus. We see that while information is transmitted with varying amounts of density, the main bits of information are transmitted at a scale between 2 and 7. Due </context>
</contexts>
<marker>Rieser, Keizer, Liu, Lemon, 2011</marker>
<rawString>Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver Lemon. 2011. Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A General,</title>
<date>2011</date>
<booktitle>Abstract Model of Incremental Dialogue Processing. Dialogue and Discourse,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1756" citStr="Schlangen and Skantze, 2011" startWordPosition="242" endWordPosition="245"> and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1 Introduction Recent work on incremental systems has shown that adapting a system’s turn-taking behaviour to be more human-like can improve the user’s experience significantly, based on incremental models of automatic speech recognition (ASR) (Baumann et al., 2011), dialogue management (Buss et al., 2010), and speech generation (Skantze and Hjalmarsson, 2010). All of these approaches are based on the same general abstract architecture of incremental processing (Schlangen and Skantze, 2011). While this architecture offers inherently incremental mechanisms to 82 update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have be</context>
</contexts>
<marker>Schlangen, Skantze, 2011</marker>
<rawString>David Schlangen and Gabriel Skantze. 2011. A General, Abstract Model of Incremental Dialogue Processing. Dialogue and Discourse, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethan Selfridge</author>
<author>Iker Arizmendi</author>
<author>Peter Heeman</author>
<author>Jason Williams</author>
</authors>
<title>Stability and Accuracy in Incremental Speech Recognition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Annual SigDial Meeting on Discourse and Dialogue,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="2771" citStr="Selfridge et al. (2011)" startWordPosition="404" endWordPosition="407"> they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et </context>
</contexts>
<marker>Selfridge, Arizmendi, Heeman, Williams, 2011</marker>
<rawString>Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Jason Williams. 2011. Stability and Accuracy in Incremental Speech Recognition. In Proceedings of the 12th Annual SigDial Meeting on Discourse and Dialogue, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Shannon</author>
</authors>
<title>A Mathematical Theory of Communications.</title>
<date>1948</date>
<journal>Bell Systems Technical Journal,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="10072" citStr="Shannon (1948)" startWordPosition="1613" endWordPosition="1614">USR Not Indian, I want Italian. SYS OK, Italian ... SYS I have 24 Italian restaurants ... Barge-ins 2 (the system barges-in on user) USR I need an Italian restaurant that is located ... SYS I’m sorry. Did you say Indian or Italian? USR I said Italian. And in the centre of town please. SYS OK, let me see. I have 24 Italian restaurants ... Figure 1: Example phenomena generated with the learnt policy. The agent has learnt to produce backchannels and barge-ins at the appropriate moment and alternative strategies to deal with user barge-ins. 3 Information Theory Information Theory as introduced by Shannon (1948) is based on two main concepts: a communication channel through which information is transferred in bits and the information gain, i.e. the information load that each bit carries. For natural language, the assumption is that people aim to communicate according to the channel’s capacity, which corresponds to the hearer’s capacity in terms of cognitive load. If they go beyond that, the cognitive load of the listener gets too high. If they stay (far) below, too little information is transferred per bit (i.e., the utterance is inefficient or uninformative). The information gain of each word, which</context>
<context position="11455" citStr="Shannon, 1948" startWordPosition="1846" endWordPosition="1847">ence for users distributing information across utterances uniformly, so that each word is carrying roughly the same amount of information. This has been observed for phonetic phenomena based on words (Bell et al., 2003) and syllables (Aylett and Turk, 2004), and for syntactic phenomena (Levy and Jaeger, 2007; Jaeger, 2010). Relating ID to likelihood, we can say that the less frequent a word is, the more information it is likely to carry (Jaeger, 2010). For example the word ‘the’ often has a high corpus frequency but a low ID. The ID is defined as the log-probability of an event (i.e. a word) (Shannon, 1948; Levy and Jaeger, 2007), so that for an utterance u consisting of the word sequence w1 ... wi−1, we can compute the ID at each point during the utterance as: 1 n 1 (1) log P(u) _ log P(wi|w1 ... wi−1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel and Charniak (2002) in restricting our computation to tri-grams for computability reasons. Given a 84 language model of the domain, we can therefore optimise ID in system-generated discourse, where we treat ID as “an optimal solution to the problem of rapid yet error-free communication in</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Shannon. 1948. A Mathematical Theory of Communications. Bell Systems Technical Journal, 27(4):623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>16--105</pages>
<contexts>
<context position="3303" citStr="Singh et al., 2002" startWordPosition="483" endWordPosition="486">ental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately no</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. 2002. Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16:105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>Anna Hjalmarsson</author>
</authors>
<title>Towards Incremental Speech Generation in Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1623" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="222" endWordPosition="225">ns, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 1 Introduction Recent work on incremental systems has shown that adapting a system’s turn-taking behaviour to be more human-like can improve the user’s experience significantly, based on incremental models of automatic speech recognition (ASR) (Baumann et al., 2011), dialogue management (Buss et al., 2010), and speech generation (Skantze and Hjalmarsson, 2010). All of these approaches are based on the same general abstract architecture of incremental processing (Schlangen and Skantze, 2011). While this architecture offers inherently incremental mechanisms to 82 update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and</context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>Gabriel Skantze and Anna Hjalmarsson. 2010. Towards Incremental Speech Generation in Dialogue Systems. In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sutton</author>
<author>Andrew Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15768" citStr="Sutton and Barto, 1998" startWordPosition="2596" endWordPosition="2599">ions correspond to the system’s capabilities, such as present the results or barge-in on the user. They also handle incremental updates in the system. In addition, we need a transition function that specifies the way that actions change the environment (as expressed in the state representation) and a reward function which specifies a numeric value for each action taken. In this way, decision making can be seen as a finite sequence of states, actions and rewards {s0, a0, r1, s1, a1, ..., rt−1, st1, where the goal is to induce an optimal strategy automatically using Reinforcement Learning (RL) (Sutton and Barto, 1998). We used Hierarchical RL, rather than flat RL, because the latter is affected by the curse of dimensionality, the fact that the state space grows exponentially according to the state variables taken into account. This affects the scalability of flat RL agents but for now make the assumption that using the corpus data is informative since they are from the same domain. Information Density 20 18 16 0 1 85 and limits their application to small-scale problems. Since timing is crucial for incremental approaches, where processing needs to be fast, we choose a hierarchical setting for better scalabi</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard Sutton and Andrew Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
</authors>
<title>Statistical Methods for Spoken Dialogue Management.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="3395" citStr="Thomson, 2009" startWordPosition="499" endWordPosition="500">ic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a no</context>
</contexts>
<marker>Thomson, 2009</marker>
<rawString>Blaise Thomson. 2009. Statistical Methods for Spoken Dialogue Management. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>Bootstrapping Syntax and Recursion using Alignment-Based Learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00,</booktitle>
<pages>1063--1070</pages>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. Bootstrapping Syntax and Recursion using Alignment-Based Learning. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages 1063–1070.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
</authors>
<title>An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email.</title>
<date>2000</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>12--387</pages>
<contexts>
<context position="3270" citStr="Walker, 2000" startWordPosition="479" endWordPosition="480">onsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of inte</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>Marilyn Walker. 2000. An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email. Journal of Artificial Intelligence Research (JAIR), 12:387–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gasic</author>
<author>Simon Keizer</author>
<author>Francois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The Hidden Information State Model: A Practical Framework for POMDP-based Spoken Dialogue Management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="3415" citStr="Young et al., 2010" startWordPosition="501" endWordPosition="504">odels to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incr</context>
</contexts>
<marker>Young, Gasic, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Gasic, Simon Keizer, Francois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The Hidden Information State Model: A Practical Framework for POMDP-based Spoken Dialogue Management. Computer Speech and Language, 24(2):150–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
</authors>
<title>Probabilistic Methods in Spoken Dialogue Systems.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society (Series A),</journal>
<volume>358</volume>
<issue>1769</issue>
<contexts>
<context position="3283" citStr="Young, 2000" startWordPosition="481" endWordPosition="482"> in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive syste</context>
</contexts>
<marker>Young, 2000</marker>
<rawString>Steve Young. 2000. Probabilistic Methods in Spoken Dialogue Systems. Philosophical Transactions of the Royal Society (Series A), 358(1769):1389–1402.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
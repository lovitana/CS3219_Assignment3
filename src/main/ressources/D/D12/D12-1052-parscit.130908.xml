<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000839">
<title confidence="0.999052">
A Beam-Search Decoder for Grammatical Error Correction
</title>
<author confidence="0.992555">
Daniel Dahlmeier&apos; and Hwee Tou Ng&apos; ,2
</author>
<affiliation confidence="0.979797">
&apos;NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
</affiliation>
<email confidence="0.998072">
{danielhe,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999395823529412">
We present a novel beam-search decoder for
grammatical error correction. The decoder
iteratively generates new hypothesis correc-
tions from current hypotheses and scores them
based on features of grammatical correctness
and fluency. These features include scores
from discriminative classifiers for specific er-
ror categories, such as articles and preposi-
tions. Unlike all previous approaches, our
method is able to perform correction of whole
sentences with multiple and interacting er-
rors while still taking advantage of powerful
existing classifier approaches. Our decoder
achieves an Fl correction score significantly
higher than all previous published scores on
the Helping Our Own (HOO) shared task data
set.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979102040816">
Grammatical error correction is an important prob-
lem in natural language processing (NLP) that has
attracted an increasing amount of interest over
the last few years. Grammatical error correction
promises to provide instantaneous accurate feedback
to language learners, e.g., learners of English as a
Second Language (ESL).
The dominant paradigm that underlies most er-
ror correction systems to date is multi-class clas-
sification. A classifier is trained to predict a word
from a confusion set of possible correction choices,
given some feature representation of the surround-
ing sentence context. During testing, the classifier
predicts the most likely correction for each test in-
stance. If the prediction differs from the observed
word used by the writer and the classifier is suffi-
ciently confident in its prediction, the observed word
is replaced by the prediction. Although considerable
progress has been made, the classification approach
suffers from some serious shortcomings. Each clas-
sifier corrects a single word for a specific error cat-
egory individually. This ignores dependencies be-
tween the words in a sentence. Also, by conditioning
on the surrounding context, the classifier implicitly
assumes that the surrounding context is free of gram-
matical errors, which is often not the case. Finally,
the classifier typically has to commit to a single one-
best prediction and is not able to change its deci-
sion later or explore multiple corrections. Instead of
correcting each word individually, we would like to
perform global inference over corrections of whole
sentences which can contain multiple and interact-
ing errors.
An alternative paradigm is to view error correc-
tion as a statistical machine translation (SMT) prob-
lem from “bad” to “good” English. While this ap-
proach can naturally correct whole sentences, a stan-
dard SMT system cannot easily incorporate mod-
els for specific grammatical errors. It also suffers
from the paucity of error-annotated training data for
grammar correction. As a result, applying a stan-
dard SMT system to error correction does not pro-
duce good results, as we show in this work.
In this work, we present a novel beam-search de-
coder for grammatical error correction that com-
bines the advantages of the classification approach
and the SMT approach. Starting from the origi-
nal input sentence, the decoder performs an itera-
tive search over possible sentence-level hypotheses
</bodyText>
<page confidence="0.960205">
568
</page>
<note confidence="0.7644265">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 568–578, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999977214285715">
to find the best sentence-level correction. In each
iteration, a set of proposers generates new hypothe-
ses by making incremental changes to the hypothe-
ses found so far. A set of experts scores the new
hypotheses on criteria of grammatical correctness.
These experts include discriminative classifiers for
specific error categories, such as articles and prepo-
sitions. The decoder model calculates the overall hy-
pothesis score for each hypothesis as a linear com-
bination of the expert scores. The weights of the de-
coder model are discriminatively trained on a devel-
opment set of error-annotated sentences. The high-
est scoring hypotheses are kept in the search beam
for the next iteration. This search procedure contin-
ues until the beam is empty or the maximum number
of iterations has been reached. The highest scoring
hypothesis is returned as the sentence-level correc-
tion. We evaluate our proposed decoder in the con-
text of the Helping Our Own (HOO) shared task on
grammatical error correction (Dale and Kilgarriff,
2011). Our decoder achieves an F1 score of 25.48%
which improves upon the current state of the art.
The remainder of this paper is organized as fol-
lows. The next section gives an overview of related
work. Section 3 describes the proposed beam-search
decoder. Sections 4 and 5 describe the experimental
setup and results, respectively. Section 6 provides
further discussion. Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962159090909">
In this section, we summarize related work in gram-
matical error correction. For a more detailed review,
the readers can refer to (Leacock et al., 2010).
The classification approach to error correction has
mainly focused on correcting article and preposition
errors (Knight and Chander, 1994; Han et al., 2006;
Chodorow et al., 2007; Tetreault and Chodorow,
2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Ro-
zovskaya and Roth, 2011). The advantage of the
classification approach is that it can make use of
powerful machine learning algorithms in connection
with arbitrary features from the sentence context.
Typical features include surrounding N-grams, part-
of-speech (POS) tags, chunks, etc. In fact, a consid-
erable amount of research effort has been invested in
finding better features.
The SMT approach to error corrections has re-
ceived comparatively less attention. Brockett et
al. (2006) use an SMT system to correct errors in-
volving mass noun errors. Because no large anno-
tated learner corpus was available, the training data
was created artificially from non-learner text. Lee
and Seneff (2006) describe a lattice-based correc-
tion system with a domain-specific grammar for spo-
ken utterances from the flight domain. The work in
(D´esilets and Hermet, 2009) uses simple round-trip
translation with a standard SMT system to correct
grammatical errors. Dahlmeier and Ng (2011a) cor-
rect collocation errors using phrase-based SMT and
paraphrases induced from the writer’s native lan-
guage. Park and Levy (2011) propose a noisy chan-
nel model for error correction. While their motiva-
tion to correct whole sentences is similar to ours,
their proposed generative method differs substan-
tially from our discriminative decoder. Park and
Levy’s model does not allow the use of discrim-
inative expert classifiers as our decoder does, but
instead relies on a bigram language model to find
grammatical corrections. Indeed, they point out that
the language model often fails to distinguish gram-
matical and ungrammatical sentences.
To the best of our knowledge, our work is the first
discriminatively trained decoder for whole-sentence
grammatical error correction.
</bodyText>
<sectionHeader confidence="0.999353" genericHeader="method">
3 Decoder
</sectionHeader>
<bodyText confidence="0.999959888888889">
In this section, we describe the proposed beam-
search decoder and its components.
The task of the decoder is to find the best hypoth-
esis (i.e., the best corrected sentence) for a given in-
put sentence. To accomplish this, the decoder needs
to be able to perform two tasks: generating new
hypotheses from current ones, and discriminating
good hypotheses from bad ones. This is achieved
by two groups of modules which we call proposers
and experts, respectively. Proposers take a hypothe-
sis and generate a set of new hypotheses, where each
new hypothesis is the result of making an incremen-
tal change to the current hypothesis. Experts score
hypotheses on particular aspects of grammaticality.
This can be a general language model score, or the
output of classifiers for particular error categories,
for example for article and preposition usage. The
overall score for a hypothesis is a linear combina-
</bodyText>
<page confidence="0.997241">
569
</page>
<bodyText confidence="0.999850666666667">
tion of the expert scores. Note that in our decoder,
each hypothesis corresponds to a complete sentence.
This makes it easy to apply syntactic processing,
like POS tagging, chunking, and dependency pars-
ing, which provides necessary features for the expert
models. The highest scoring hypotheses are kept in
the search beam for the next iteration. The search
ends when the beam is empty or the maximum num-
ber of iterations has been reached. The highest scor-
ing hypothesis found during the search is returned as
the sentence-level correction. The modular design
of the decoder makes it easy to extend the model
to new error categories by adding specific proposers
and experts without having to change the decoding
algorithm.
</bodyText>
<subsectionHeader confidence="0.996404">
3.1 Proposers
</subsectionHeader>
<bodyText confidence="0.9989904">
The proposers generate new hypotheses, given a hy-
pothesis. Because the number of possible hypothe-
ses grows exponentially with the sentence length,
enumerating all possible hypotheses is infeasible.
Instead, each proposer only makes a small incre-
mental change to the hypothesis in each iteration. A
change corresponds to a correction of a single word
or phrase. We experiment with the following pro-
posers in this work. Additional proposers for other
error categories can easily be added to the decoder.
</bodyText>
<listItem confidence="0.9965904375">
• Spelling Generate a set of new hypotheses, by
replacing a misspelled word with each correc-
tion proposed by a spellchecker.
• Articles For each noun phrase (NP), generate
two new hypotheses by changing the observed
article. Possible article choices are a/an, the,
and the empty article E.
• Prepositions For each prepositional
phrase (PP), generate a set of new hy-
potheses by changing the observed preposition.
For each preposition, we define a confusion set
of possible corrections.
• Punctuation insertion Insert commas, peri-
ods, and hyphens based on a set of simple rules.
• Noun number For each noun, change its num-
ber from singular to plural or vice versa.
</listItem>
<subsectionHeader confidence="0.995218">
3.2 Experts
</subsectionHeader>
<bodyText confidence="0.992435133333333">
The experts score hypotheses on particular aspects
of grammaticality to help the decoder to discrim-
inate grammatical hypotheses from ungrammatical
ones. We employ two types of expert models. The
first type of expert model is a standard N-gram lan-
guage model. The language model expert is not spe-
cialized for any particular type of error. The second
type of experts is based on linear classifiers and is
specialized for particular error categories. We use
the following classifier experts in our work. The fea-
tures for the classifier expert models include features
from N-grams, part-of-speech (POS) tags, chunks,
web-scale N-gram counts, and dependency parse
trees. Additional experts can easily be added to the
decoder.
</bodyText>
<listItem confidence="0.987719">
• Article expert Predict the correct article for a
noun phrase.
• Preposition expert Predict the correct preposi-
tion for a prepositional phrase.
• Noun number expert Predict whether a noun
should be in the singular or plural form.
</listItem>
<bodyText confidence="0.998923333333333">
The outputs of the experts are used as hypothesis
features in the decoder, as described in the next sec-
tion.
</bodyText>
<subsectionHeader confidence="0.999518">
3.3 Hypothesis Features
</subsectionHeader>
<bodyText confidence="0.99998">
Each hypothesis is associated with a vector of real-
valued features which are indicators of grammatical-
ity and are computed from the output of the expert
models. We call these features hypothesis features
to distinguish them from the features of the expert
classifiers. The simplest hypothesis feature is the
log probability of the hypothesis under the N-gram
language model expert. To avoid a bias towards
shorter hypotheses, we normalize the probability by
the length of the hypothesis:
</bodyText>
<equation confidence="0.99809">
scorelm = 1
|h |log Pr(h), (1)
</equation>
<bodyText confidence="0.99708825">
where h is a hypothesis sentence and |h |is the hy-
pothesis length in tokens.
For the classifier-based experts, we define two
types of features. The first is the average score of
</bodyText>
<page confidence="0.975834">
570
</page>
<bodyText confidence="0.893158">
the hypothesis under the expert model:
</bodyText>
<equation confidence="0.9707515">
( )
uTf(xh i , yh i ) , (2)
</equation>
<bodyText confidence="0.999981454545454">
where u is the expert classifier weight vector, xhi and
yhi are the feature vector and the observed class, re-
spectively, for the i-th instance extracted from the
hypothesis h (e.g., the i-th NP in the hypothesis
for the article expert), and f is a feature map that
computes the expert classifier features. The average
score reflects how much the expert model “likes” the
hypothesis. The second expert score, which we call
delta score, is the maximum difference between the
highest scoring class and the observed class in any
instance from the hypothesis:
</bodyText>
<equation confidence="0.83816875">
( )
SCOredelta = Max uT f(xhi , y) − uTf(xhi , yhi ) �
i,y
(3)
</equation>
<bodyText confidence="0.999284615384616">
Generally speaking, the delta score measures how
much the model “disagrees” with the hypothesis.
Finally, each hypothesis has a number of correc-
tion count features that keep track of how many cor-
rections have been made to the hypothesis so far. For
example, there is a feature that counts how often the
article correction c —* the has been applied. We also
add aggregated correction count features for each
error category, e.g., how many article corrections
have been applied in total. The correction count fea-
tures allow the decoder to learn a bias against over-
correcting sentences and to learn which types of cor-
rections are more likely and which are less likely.
</bodyText>
<subsectionHeader confidence="0.67355">
3.4 Decoder Model
</subsectionHeader>
<bodyText confidence="0.999966">
The hypothesis features described in the previous
subsection are combined to compute the score of a
hypothesis according to the following linear model:
</bodyText>
<equation confidence="0.998495">
S = wTfE(h), (4)
</equation>
<bodyText confidence="0.979907555555556">
where w is the decoder model weight vector and
fE is a feature map that computes the hypothesis
features described above, given a set of experts E.
The weight vector w is tuned on a development set
of error-annotated sentences using the PRO ranking
optimization algorithm (Hopkins and May, 2011).1
1We also experimented with the MERT algorithm (Och,
2003) but found that PRO achieved better results.
PRO performs decoder parameter tuning through a
pair-wise ranking approach. The algorithm starts by
sampling hypothesis pairs from the N-best list of the
decoder. The metric score for each hypothesis in-
duces a ranking of the two hypotheses in each pair.
The task of finding a weight vector that correctly
ranks hypotheses can then be reduced to a simple bi-
nary classification task. In this work, we use PRO to
optimize the F1 correction score, which is defined in
Section 4.2. PRO requires a sentence-level score for
each hypothesis. As F1 score is not decomposable,
we optimize sentence-level F1 score which serves
as an approximation of the corpus-level F1 score.
Similarly, Hopkins and May optimize a sentence-
level BLEU approximation (Lin and Och, 2004) in-
stead of the corpus-level BLEU score (Papineni et
al., 2002). We observed that optimizing sentence-
level F1 score worked well in practice in our experi-
ments.
</bodyText>
<subsectionHeader confidence="0.805308">
3.5 Decoder Search
</subsectionHeader>
<bodyText confidence="0.999993888888889">
Given a set of proposers, experts, and a tuned de-
coder model, the decoder can be used to correct
new unseen sentences. This is done by performing
a search over possible hypothesis candidates. The
decoder starts with the input sentence as the initial
hypothesis, i.e., assuming that all words are correct.
It then performs a beam search over the space of
possible hypotheses to find the best hypothesis cor-
rection h� for an input sentence e. The search pro-
ceeds in iterations until the beam is empty or the
maximum number of iterations has been reached. In
each iteration, the decoder takes each hypothesis in
the beam and generates new hypothesis candidates
using all the available proposers. The hypotheses
are evaluated by the expert models that compute the
hypothesis features and finally scored using the de-
coder model. As the search space grows exponen-
tially, it is infeasible to perform exhaustive search.
Therefore, we prune the search space by only ac-
cepting the most promising hypotheses to the pool
of hypotheses for future consideration. If a hypothe-
sis has a higher score than the best hypothesis found
in previous iterations, it is definitely added to the
pool. Otherwise, we use a simulated annealing strat-
egy where hypotheses with a lower score can still be
accepted with a certain probability which depends
on the difference between the hypothesis score and
</bodyText>
<equation confidence="0.564075">
1 n
SCOrea„g = n i=1
</equation>
<page confidence="0.882517">
571
</page>
<figureCaption confidence="0.999063666666667">
Figure 1: Example of a search tree produced by the beam-search decoder for the input In other hands, they might be
right. The highest scoring hypothesis found is On the other hand, they might be right. Some hypotheses are omitted
due to space constraints.
</figureCaption>
<figure confidence="0.995430358024691">
In other hands , they might be right .
score = 9.10
� —. an
e —. the
In —. At
In —. For
In the other hands , they might be right .
score = 9.63
At other hands ..
score = 5.34
In an other ..
score = -1.58
At the other
hands ..
score = 6.05
In —. At
For the other
hands ..
score = 9.05
hands —.hand
In the other hand , they might be right
score = 11.69
In —. For
With the other
hands ..
score = 5.25
On an other hand
..
score = 2.48
On other hand..
score = 4.94
About the other
hand..
score = 9.71
By the other
hand ..
score = 5.80
To the other
hand ..
score = 6.32
...
In —. With
With the other
hand ..
score = 8.69
...
In —. Into In —. Of In —. On
the —. an In —. At In —. For
In —. With
In an other hand
..
score = 3.96
At the other
hand..
score = 9.40
For the other
hand..
score = 10.75
Of the other
hand..
score = 8.94
On the other hand , they might be right
score = 15.36
Into the other
hand..
score = 5.47
hand —. hands
With other hands ..
score = 6.31
In other hand..
score = 8.29
...
...
the —. an
the —. e
On —.About
On —. By
On — .To
In —. With
For other hands ..
score = 7.00
</figure>
<bodyText confidence="0.995981333333333">
the score of the best hypothesis and the “tempera-
ture” of the system. We lower the temperature after
each iteration according to an exponential cooling
schedule. Hypotheses that have been explored be-
fore are not considered again to avoid cycles in the
search. From all hypotheses in the pool, we select
the top k hypotheses and add them to the beam for
the next search iteration. The decoding algorithm
is shown in Algorithm 1. The decoder can be con-
sidered an anytime algorithm (Russell and Norvig,
2010), as it has a current best hypothesis correction
available at any point of the search, while gradually
improving the result by searching for better hypothe-
ses. An example of a search tree produced by our
decoder is shown in Figure 1.
The decoding algorithm shares some similarities
with the beam-search algorithm frequently used in
SMT. There is however a difference between SMT
decoding and grammar correction decoding that is
worth pointing out. In SMT decoding, every input
word needs to be translated exactly once. In con-
trast, in grammar correction decoding, the majority
of the words typically do not need any correction
(in the HOO data, for example, there are on aver-
age 6 errors per 100 words). On the other hand,
some words might require multiple corrections, for
example spelling correction followed by noun num-
ber correction. Errors can also be inter-dependent,
where correcting one word makes it necessary to
change another word, for example to preserve agree-
ment. Our decoding algorithm has the option to cor-
rect some words multiple times, while leaving other
words unchanged.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999974111111111">
We evaluate our decoder in the context of the HOO
shared task on grammatical error correction. The
goal of the task is to automatically correct errors in
academic papers from NLP. The readers can refer to
the overview paper (Dale and Kilgarriff, 2011) for
details. We compare our proposed method with two
baselines: a phrase-based SMT system (described in
Section 4.3) and a pipeline of classifiers (described
in Section 4.4).
</bodyText>
<subsectionHeader confidence="0.984955">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999951625">
We split the HOO development data into an equal
sized training (HOO-TRAIN) and tuning (HOO-
TUNE) set. The official HOO test data (HOO-TEST)
is used for evaluation. In the HOO shared task, par-
ticipants were allowed to raise objections regarding
the gold-standard annotations (corrections) of the
test data after the test data was released. As a result,
the gold-standard annotations could be biased in fa-
</bodyText>
<page confidence="0.99395">
572
</page>
<tableCaption confidence="0.7417762">
Algorithm 1 The beam-search decoding algorithm. e:
original sentence, w: decoder weight vector, P: set of
proposers, E: set of experts, k: beam width, M: maxi-
mum number of iterations, T, c: initial temperature and
cooling schedule for simulated annealing (0 &lt; c &lt; 1).
</tableCaption>
<bodyText confidence="0.872792">
procedure decode(e, w, P, E, k, M)
</bodyText>
<listItem confidence="0.961577806451613">
1: beam t- {e1
2: previous t- {e1
3: hbest t- e
4: sbest t- wTfE(hbest)
5: i t -0
6: while beam # 0 n i &lt; M do
7: pool t- {1
8: for all h E beam do
9: for all p E P do
10: for all h&apos; E p.propose(h) do
11: if h&apos; E previous then
12: continue
13: previous t- previous U {h&apos;1
14: sh t- wT fE(h&apos;)
15: if accept(sh , sbest, T) then
16: pool t- pool U {(h&apos;, sh )1
17: beam t- 0
18: for all (h, sh) E nbest(pool, k) do
19: beam t- beam U {h1
20: if sh &gt; sbest then
21: hbest t- h
22: sbest t- sh
23: T t- T x c
24: i t -i + 1
25: return hbest
procedure accept(sh, sbest, T)
1: 6 t- sh — sbest
2: if 6 &gt; 0 then
3: return true
4: if exp(T� ) &gt; random() then
5: return true else return false
</listItem>
<bodyText confidence="0.997005153846154">
vor of specific systems participating in the shared
task. We obtain both the original and the final offi-
cial gold-standard annotations and report evaluation
results on both annotations.
We use the ACL Anthology2 as training data for
the expert models. We crawl all non-OCR docu-
ments from the anthology, except those documents
that overlap with the HOO data. Section headers,
references, etc. are automatically removed. The
Web 1T 5-gram corpus (Brants and Franz, 2006) is
used for language modeling and collecting web N-
gram counts. Table 1 gives an overview of the data
sets.
</bodyText>
<footnote confidence="0.950858">
2http://www.aclweb.org/anthology-new/
</footnote>
<table confidence="0.9914976">
Data Set Sentences Tokens
HOO-TRAIN 467 11,373
HOO-TUNE 472 11,435
HOO-TEST 722 18,790
ACL-ANTHOLOGY 943,965 22,465,690
</table>
<tableCaption confidence="0.999822">
Table 1: Overview of the data sets.
</tableCaption>
<subsectionHeader confidence="0.92326">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.980922333333333">
We evaluate performance by computing precision,
recall, and F1 correction score without bonus as de-
fined in the official HOO report (Dale and Kilgar-
riff, 2011)3. F1 correction score is simply the F1
measure (van Rijsbergen, 1979) between the correc-
tions (called edits in HOO) proposed by a system
and the gold-standard corrections. Let {e1, ... , en}
be a set of test sentences and let {g1, ... , gn} be
the set of gold-standard edits for the sentences. Let
{h1, ... , hn} be the set of corrected sentences out-
put by a system. One difficulty in the evaluation is
that the set of system edits {d1, ... , dn} between
the test sentences and the system outputs is ambigu-
ous. For example, assume that the original test sen-
tence is The data is similar with test set., the system
output is The data is similar to the test set., and the
gold-standard edits are two corrections with —* to,
c —* the that change with to to and insert the be-
fore test set. The official HOO scorer however ex-
tracts a single system edit with —* to the for this
instance. As the extracted system edit is different
from the gold-standard edits, the system would be
considered wrong, although it proposes the exact
same corrected sentence as the gold standard ed-
its. This problem has also been recognized by the
HOO shared task organizers (see (Dale and Kilgar-
riff, 2011), Section 5).
Our MaxMatch (M2) scorer (Dahlmeier and Ng,
2012) overcomes this problem through an efficient
algorithm that computes the set of system edits
which has the maximum overlap with the gold-
standard edits. We use the M2 scorer as the main
evaluation metric in our experiments. Additionally,
we also report results with the official HOO scorer.
Once the set of system edits is extracted, precision,
recall, and F1 measure are computed as follows.
3“Without bonus” means that a system does not receive extra
credit for not making corrections that are considered optional in
the gold standard.
</bodyText>
<page confidence="0.991986">
573
</page>
<table confidence="0.8479732">
P = =
R En i=1 |di ∩ gi|
F1
=
E&apos;1 |di|
En i=1 |di ∩ gi|
Ei1|gi|
P × R
2 ×
P + R
</table>
<bodyText confidence="0.999819833333333">
We note that the M2 scorer and the HOO scorer ad-
here to the same score definition and only differ in
the way the system edits are computed. For statisti-
cal significance testing, we use sign-test with boot-
strap re-sampling (Koehn, 2004) with 1,000 sam-
ples.
</bodyText>
<subsectionHeader confidence="0.99718">
4.3 SMT Baseline
</subsectionHeader>
<bodyText confidence="0.999912">
We build a baseline error correction system, using
the MOSES SMT system (Koehn et al., 2007). Word
alignments are created automatically on “good-bad”
parallel text from HOO-TRAIN using GIZA++ (Och
and Ney, 2003), followed by phrase extraction us-
ing the standard heuristic (Koehn et al., 2003). The
maximum phrase length is 5. Parameter tuning is
done on the HOO-TUNE data with the PRO al-
gorithm (Hopkins and May, 2011) implemented in
MOSES. The optimization objective is sentence-
level BLEU (Lin and Och, 2004). We note that the
objective function is not the same as the final evalu-
ation F1 score. Also, the training and tuning data are
small by SMT standards. The aim for the SMT base-
line is not to achieve a state-of-the-art system, but to
serve as the simplest possible baseline that uses only
off-the-shelf software.
</bodyText>
<subsectionHeader confidence="0.993828">
4.4 Pipeline Baseline
</subsectionHeader>
<bodyText confidence="0.999911">
The second baseline system is a pipeline of
classifier-based and rule-based correction steps.
Each step takes sentence segmented plain text as in-
put, corrects one particular error category, and feeds
the corrected text into the next step. No search or
global inference is applied. The correction steps are:
</bodyText>
<listItem confidence="0.9985354">
1. Spelling errors
2. Article errors
3. Preposition errors
4. Punctuation errors
5. Noun number errors
</listItem>
<bodyText confidence="0.999954">
We use the following tools for syntactic process-
ing: OpenNLP4 for POS tagging, YamCha (Kudo
and Matsumoto, 2003) for constituent chunking, and
the MALT parser (Nivre et al., 2007) for depen-
dency parsing. For language modeling, we use Ran-
dLM (Talbot and Osborne, 2007).
For spelling correction, we use GNU Aspell5.
Words that contain upper-case characters inside the
word or are shorter than four characters are excluded
from spell checking. The spelling dictionary is aug-
mented with all words that appear at least 10 times
in the ACL-ANTHOLOGY data set.
Article correction is cast as a multi-class clas-
sification problem. As the learning algorithm,
we choose multi-class confidence-weighted (CW)
learning (Crammer et al., 2009) which has been
shown to perform well for NLP problems with high
dimensional and sparse feature spaces. The possi-
ble classes are the articles a, the, and the empty ar-
ticle c. The article an is normalized as a and re-
stored later using a rule-based heuristic. We con-
sider all NPs that are not pronouns and do not have a
non-article determiner, e.g., this, that. The classifier
is trained on over 5 million instances from ACL-
ANTHOLOGY. We use a combination of features
proposed by (Rozovskaya et al., 2011) (which in-
clude lexical and POS N-grams, lexical head words,
etc.), web-scale N-gram count features from the
Web 1T 5-gram corpus following (Bergsma et al.,
2009), and dependency head and child features.
During testing, a correction is proposed if the pre-
dicted article is different from the observed article
used by the writer, and the difference between the
confidence score for the predicted article and the
confidence score for the observed article is larger
than a threshold. Threshold parameters are tuned
via a grid-search on the HOO-TUNE data. We tune
a separate threshold value for each class.
Preposition correction and noun number correc-
tion are analogous to article correction. They differ
only in terms of the classes and the features. For
preposition correction, the classes are 36 frequent
English prepositions6. The features are surrounding
</bodyText>
<footnote confidence="0.8933454">
4http://opennlp.sourceforge.net
5http://aspell.net
6about, along, among, around, as, at, beside, besides, be-
tween, by, down, during, except, for, from, in, inside, into, of,
off, on, onto, outside, over, through, to, toward, towards, under,
</footnote>
<page confidence="0.997022">
574
</page>
<bodyText confidence="0.9999029">
lexical N-grams, web-scale N-gram counts, and de-
pendency features following (Tetreault et al., 2010).
The preposition classifier is trained on 1 million
training examples from the ACL-ANTHOLOGY. For
noun number correction, the classes are singular
and plural. The features are lexical N-grams, web-
scale N-gram counts, dependency features, the noun
lemma, and a binary countability feature. The noun
number classifier is trained on over 5 million exam-
ples from ACL-ANTHOLOGY. During testing, the
singular or plural word surface form is generated us-
ing WordNet (Fellbaum, 1998) and simple heuris-
tics. Punctuation correction is done using a set of
simple rules developed on the HOO development
data.
At the end of every correction step, all proposed
corrections are filtered using a 5-gram language
model from the Web 1T 5-gram corpus and only
corrections that strictly increase the normalized lan-
guage model score of the sentence are applied.
</bodyText>
<subsectionHeader confidence="0.977627">
4.5 Decoder
</subsectionHeader>
<bodyText confidence="0.999965739130435">
We experiment with different decoder configura-
tions with different proposers and expert models.
In the simplest configuration, the decoder only has
the spelling proposer and the language model ex-
pert. We then add the article proposer and expert,
the preposition proposer and expert, the punctua-
tion proposer, and finally the noun number proposer
and expert. We refer to the final configuration with
all proposers and experts as the full decoder model.
Note that error categories are corrected jointly and
not in sequential steps as in the pipeline.
To make the results directly comparable to the
pipeline, the decoder uses the same resources as the
pipeline. As the expert models, we use a 5-gram
language model from the Web 1T 5-gram corpus
with the Berkeley LM (Pauls and Klein, 2011)7 in
the decoder and the CW-classifiers described in the
last section. The spelling proposer uses the same
spellchecker as the pipeline, and the punctuation
proposer uses the same rules as the pipeline. The
beam width and the maximum number of iterations
are set to 10. In earlier experiments, we found that
larger values had no effect on the result. The simu-
</bodyText>
<footnote confidence="0.895606666666667">
underneath, until, up, upon, with, within, without
7Berkeley LM is written in Java and was easier to integrate
into our Java-based decoder than RandLM.
</footnote>
<bodyText confidence="0.999893064516129">
lated annealing temperature T is initialized to 10 and
the exponential cooling schedule c is set to 0.9. The
decoder weight vector is initialized as follows. The
weight for the language model score and the weights
for the classifier expert average scores are initialized
to 1.0, and the weights for the classifier expert delta
scores are initialized to −1.0. The weights for the
correction count features are initialized to zero. For
PRO optimization, we use the HOO-TUNE data and
the default PRO parameters from (Hopkins and May,
2011): we sample 5,000 hypothesis pairs from the
N-best list (N = 100) for every input sentence and
keep the top 50 sample pairs with the highest dif-
ference in F1 measure. The weights are optimized
using MegaM (Daum´e III, 2004) and interpolated
with the previous weight vector with an interpola-
tion parameter of 0.1. We normalize feature val-
ues to avoid having features on a larger scale dom-
inate features on a smaller scale. We linearly scale
all hypothesis features to a unit interval [0, 1]. The
minimum and maximum values for each feature are
estimated from the development data. We use an
early stopping criterion that terminates PRO if the
objective function on the tuning data drops. To bal-
ance the skewed data where samples without errors
greatly outnumber samples with errors, we give a
higher weight to sample pairs where the decoder
proposed a valid correction. We found a weight of
20 to work well, based on initial experiments on
the HOO-TUNE data. We keep all these parameters
fixed for all experiments.
</bodyText>
<sectionHeader confidence="0.999895" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999979857142857">
The complete results of our experiments are shown
in Table 2. Each row contains the results for one
error correction system. Each system is scored on
the original and official gold-standard annotations,
both with the M2 scorer and the official HOO scorer.
This results in four sets of precision, recall, and F1
scores for each system. The best published result
to date on this data set is the UI Run1 system from
the HOO shared task. We include their system as a
reference point.
We make the following observations. First, the
scores on the official gold-standard annotations are
higher compared to the original gold-standard an-
notations. We note that the gap between the two
</bodyText>
<page confidence="0.993778">
575
</page>
<table confidence="0.999932944444444">
System Original gold-standard Official gold-standard
M2 scorer HOO scorer M2 scorer HOO scorer
P R F1 P R F1 P R F1 P R F1
UI Run1 40.86 11.21 17.59 38.13 10.42 16.37 54.61 14.57 23.00 50.72 13.34 21.12
P R F1 P R F1 P R F1 P R F1
SMT 9.84 7.77 8.68 15.25 5.31 7.87 23.35 7.38 11.21 15.82 5.30 7.93
Pipeline P R F1 P R F1 P R F1 P R F1
Spelling 50.00 0.79 1.55 40.00 0.64 1.25 50.00 0.76 1.49 40.00 0.61 1.20
+ Articles 30.86 10.23 15.36 28.04 9.55 14.25 34.42 10.97 16.64 31.78 10.41 15.68
+ Prepositions 27.44 11.90 16.60 24.82 11.15 15.38 30.54 12.77 18.01 27.90 12.04 16.82
+ Punctuation 28.91 14.55 19.36 † 26.57 13.91 18.25 † 32.88 15.99 21.51 30.63 15.41 20.50
+ Noun number 28.77 16.13 20.67 † 24.68 14.22 18.04 † 32.34 17.50 22.71 28.36 15.71 20.22
Decoder P R F1 P R F1 P R F1 P R F1
Spelling 36.84 0.69 1.35 22.22 0.41 0.80 36.84 0.66 1.30 22.22 0.42 0.83
+ Articles 19.84 12.59 15.40 17.99 12.00 14.39 22.45 13.72 17.03 * 20.70 13.27 16.16
+ Prepositions 22.62 14.26 17.49 * 19.30 12.95 15.50 24.84 15.14 18.81 * 21.36 13.78 16.74
+ Punctuation 24.27 18.09 20.73 *† 20.40 16.24 18.08 27.13 19.58 22.75 * 23.07 17.65 19.99
+ Noun number 30.28 19.17 23.48 *† 24.29 16.24 19.46 *† 33.59 20.53 25.48 *† 27.30 17.55 21.36 *
</table>
<tableCaption confidence="0.653553">
Table 2: Experimental results on HOO-TEST. Precision, recall, and Fl score are shown in percent. The best Fl score
for each system is highlighted in bold. Statistically significant improvements (p &lt; 0.01) over the pipeline baseline are
marked with an asterisk (*). Statistically significant improvements over the UI Run1 system are marked with a dagger
(†). All improvements of the pipeline and the decoder over the SMT baseline are statistically significant.
</tableCaption>
<bodyText confidence="0.999934228571429">
annotations is the largest for the UI Run1 system
which confirms the suspected bias of the official
gold-standard annotations in favor of participating
systems. Second, the scores computed with the M2
scorer are higher than the scores computed with the
official HOO scorer. With more error categories and
more ambiguity in the edits segmentation, the gap
between the scorers widens. In the case of the full
pipeline and decoder model, the HOO scorer even
shows a decrease in F1 score when the score actu-
ally goes up as shown by the M2 scorer. We there-
fore focus on the scores of the M2 scorer from now
on. The SMT baseline achieves 8.68% and 11.21%
F1 on the original and official gold standard, respec-
tively. Although the worst system in our experi-
ments, it would still have claimed the third place
in the HOO shared task. One problem is certainly
the small amount of training data. Another reason is
that the phrase-based model is unaware of syntactic
structure and cannot express correction rules of the
form NP -* the NP. Instead, it has to have seen
the exact correction rule, e.g., house -* the house,
in the training data. As a result, the model does not
generalize well. The pipeline achieves state-of-the-
art results. Each additional correction step improves
the score. Our proposed decoder achieves the best
result. When only a few error categories are cor-
rected, the pipeline and the decoder are close to each
other. When more error categories are added, the
gap between the pipeline and the decoder becomes
larger. The full decoder model achieves an F1 score
of 23.48% and 25.48% on the original and official
gold standard, respectively, which is statistically sig-
nificantly better than both the pipeline system and
the UI Run1 system.
</bodyText>
<sectionHeader confidence="0.999859" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999971533333333">
As pointed out in Section 3.5, the majority of sen-
tences require zero or few corrections. Therefore,
the depth of the search tree is typically small. In our
experiments, the average depth of the search tree is
only 1.9 (i.e., 0.9 corrections per sentence) on the
test set. Usually, the search depth will be one larger
than the number of corrections made, since the de-
coder will explore the next level of the search tree
before deciding that none of the new hypotheses are
better than the current best one. On the other hand,
there are many possible hypotheses that can be pro-
posed for any sentence. The breadth of the search
tree is therefore quite large. In our experiments, the
decoder explored on average 99 hypotheses per sen-
tence on the test set.
</bodyText>
<page confidence="0.993936">
576
</page>
<table confidence="0.988597555555556">
PRO iteration P R Fl
1 14.13 20.17 16.62
2 19.71 20.85 20.27
3 23.12 21.03 22.02
4 24.35 20.85 22.47
5 25.53 20.51 22.75
6 26.27 20.34 22.93
7 27.25 20.68 23.52
8 26.73 19.83 22.77
</table>
<tableCaption confidence="0.998072">
Table 3: PRO tuning of the full decoder model on HOO-
</tableCaption>
<figure confidence="0.98896975">
TUNE
Feature Weight
a the -1.3660
a e 0.5253
the a -0.9997
the e 0.0532
E a 0.0694
E the -0.0529
</figure>
<tableCaption confidence="0.779452">
Table 4: Example of PRO-tuned weights for article cor-
rection count features for the full decoder model.
</tableCaption>
<bodyText confidence="0.999991484848485">
We found that PRO tuning is very important to
achieve good performance for our decoder. Most
importantly, PRO tunes the correction count features
that bias the decoder against over-correcting sen-
tences thus improving precision. But PRO is also
able to improve recall during tuning. Table 3 shows
the trajectory of the performance for the full decoder
model during PRO tuning on HOO-TUNE. After
PRO tuning has converged, we inspect the learned
weight vector and observe some interpretable pat-
terns learned by PRO. First, the language model
score and all classifier expert average scores receive
positive weights, while all classifier expert delta
scores receive negative weights, in line with our ini-
tial intuition described in Section 3.3. Second, most
correction count features receive negative weights,
thus acting as a bias against correction if it is not
necessary. Finally, the correction count features re-
veal which corrections are more likely and which are
less likely. For example, article replacement errors
are less common in the HOO-TUNE data than arti-
cle insertions or deletions. The weights learned for
the article correction count features shown in Table 4
reflect this.
Although our decoder achieves state-of-the-art re-
sults, there remain many error categories which the
decoder currently cannot correct. This includes, for
example, verb form errors (Much research (have -*
has) been put into ... ) and lexical choice errors (The
(concerned -* relevant) relation ... ). We believe
that our decoder provides a promising framework to
build grammatical error correction systems that in-
clude these types of errors in the future.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999922777777778">
We have presented a novel beam-search decoder for
grammatical error correction. The model performs
end-to-end correction of whole sentences with mul-
tiple, interacting errors, is discriminatively trained,
and incorporates existing classifier-based models for
error correction. Our decoder achieves an F1 correc-
tion score of 25.48% on the HOO shared task which
outperforms the current state of the art on this data
set.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993435">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999595541666667">
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale N-
gram models for lexical disambiguation. In Proceed-
ings of IJCAI.
T. Brants and A. Franz. 2006. Web 1T 5-gram corpus
version 1.1. Technical report, Google Research.
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of COLING-ACL.
M. Chodorow, J. Tetreault, and N.R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions.
K. Crammer, M. Dredze, and A. Kulesza. 2009. Multi-
class confidence weighted algorithms. In Proceedings
of EMNLP.
D. Dahlmeier and H.T. Ng. 2011a. Correcting seman-
tic collocation errors with L1-induced paraphrases. In
Proceedings of EMNLP.
D. Dahlmeier and H.T. Ng. 2011b. Grammatical error
correction with alternating structure optimization. In
Proceedings of ACL.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation for
grammatical error correction. In Proceedings of HLT-
NAACL.
</reference>
<page confidence="0.971272">
577
</page>
<reference confidence="0.999763882352941">
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
2011 European Workshop on Natural Language Gen-
eration.
H. Daum´e III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available at
http://pub.hal3.name#daume04cg-bfgs,
implementation available at http://hal3.name/
megam/.
A. D´esilets and M. Hermet. 2009. Using automatic
roundtrip translation to repair general errors in second
language writing. In Proceedings of MT-Summit XII.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
N.R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(02).
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proceedings of EMNLP.
K. Knight and I. Chander. 1994. Automated postediting
of documents. In Proceedings of AAAI.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP.
T Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan &amp; Claypool Publishers.
J. Lee and S. Seneff. 2006. Automatic grammar correc-
tion for second-language learners. In Proceedings of
Interspeech.
C.-Y. Lin and F.J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proceedings of COLING.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and M. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings of ACL.
Y. A. Park and R. Levy. 2011. Automated whole
sentence grammar correction using a noisy channel
model. In Proceedings of ACL.
A. Pauls and D. Klein. 2011. Faster and smaller N-gram
language models. In Proceedings of ACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
Proceedings of ACL-HLT.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proceedings of the Generation
Challenges Session at the 13th European Workshop on
Natural Language Generation.
S. Russell and P. Norvig, 2010. Artificial Intelligence: A
Modern Approach, chapter 27. Prentice Hall.
D. Talbot and M. Osborne. 2007. Randomised language
modelling for statistical machine translation. In Pro-
ceedings of ACL.
J. Tetreault and M. Chodorow. 2008. The ups and downs
of preposition error detection in ESL writing. In Pro-
ceedings of COLING.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings ofACL.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworth, 2nd edition.
</reference>
<page confidence="0.996706">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593752">
<title confidence="0.991871">A Beam-Search Decoder for Grammatical Error Correction</title>
<author confidence="0.814669">Tou</author>
<affiliation confidence="0.822317">Graduate School for Integrative Sciences and of Computer Science, National University of</affiliation>
<abstract confidence="0.994828055555555">We present a novel beam-search decoder for grammatical error correction. The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency. These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions. Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches. Our decoder an score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Lin</author>
<author>R Goebel</author>
</authors>
<title>Web-scale Ngram models for lexical disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="26613" citStr="Bergsma et al., 2009" startWordPosition="4484" endWordPosition="4487">P problems with high dimensional and sparse feature spaces. The possible classes are the articles a, the, and the empty article c. The article an is normalized as a and restored later using a rule-based heuristic. We consider all NPs that are not pronouns and do not have a non-article determiner, e.g., this, that. The classifier is trained on over 5 million instances from ACLANTHOLOGY. We use a combination of features proposed by (Rozovskaya et al., 2011) (which include lexical and POS N-grams, lexical head words, etc.), web-scale N-gram count features from the Web 1T 5-gram corpus following (Bergsma et al., 2009), and dependency head and child features. During testing, a correction is proposed if the predicted article is different from the observed article used by the writer, and the difference between the confidence score for the predicted article and the confidence score for the observed article is larger than a threshold. Threshold parameters are tuned via a grid-search on the HOO-TUNE data. We tune a separate threshold value for each class. Preposition correction and noun number correction are analogous to article correction. They differ only in terms of the classes and the features. For prepositi</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale Ngram models for lexical disambiguation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<tech>Technical report, Google Research.</tech>
<contexts>
<context position="21357" citStr="Brants and Franz, 2006" startWordPosition="3586" endWordPosition="3589">rn hbest procedure accept(sh, sbest, T) 1: 6 t- sh — sbest 2: if 6 &gt; 0 then 3: return true 4: if exp(T� ) &gt; random() then 5: return true else return false vor of specific systems participating in the shared task. We obtain both the original and the final official gold-standard annotations and report evaluation results on both annotations. We use the ACL Anthology2 as training data for the expert models. We crawl all non-OCR documents from the anthology, except those documents that overlap with the HOO data. Section headers, references, etc. are automatically removed. The Web 1T 5-gram corpus (Brants and Franz, 2006) is used for language modeling and collecting web Ngram counts. Table 1 gives an overview of the data sets. 2http://www.aclweb.org/anthology-new/ Data Set Sentences Tokens HOO-TRAIN 467 11,373 HOO-TUNE 472 11,435 HOO-TEST 722 18,790 ACL-ANTHOLOGY 943,965 22,465,690 Table 1: Overview of the data sets. 4.2 Evaluation We evaluate performance by computing precision, recall, and F1 correction score without bonus as defined in the official HOO report (Dale and Kilgarriff, 2011)3. F1 correction score is simply the F1 measure (van Rijsbergen, 1979) between the corrections (called edits in HOO) propose</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram corpus version 1.1. Technical report, Google Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W B Dolan</author>
<author>M Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="5968" citStr="Brockett et al. (2006)" startWordPosition="912" endWordPosition="915">(Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was available, the training data was created artificially from non-learner text. Lee and Seneff (2006) describe a lattice-based correction system with a domain-specific grammar for spoken utterances from the flight domain. The work in (D´esilets and Hermet, 2009) uses simple round-trip translation with a standard SMT system to correct grammatical errors. Dahlmeier and Ng (2011a) correct collocation errors using phrase-based SMT and paraphrases induced from the writer’s native language. Pa</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>C. Brockett, W. B. Dolan, and M. Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>N R Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.</booktitle>
<contexts>
<context position="5412" citStr="Chodorow et al., 2007" startWordPosition="828" endWordPosition="831"> organized as follows. The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involvi</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>M. Chodorow, J. Tetreault, and N.R. Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>M Dredze</author>
<author>A Kulesza</author>
</authors>
<title>Multiclass confidence weighted algorithms.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="25948" citStr="Crammer et al., 2009" startWordPosition="4368" endWordPosition="4371"> 2003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5. Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL-ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the articles a, the, and the empty article c. The article an is normalized as a and restored later using a rule-based heuristic. We consider all NPs that are not pronouns and do not have a non-article determiner, e.g., this, that. The classifier is trained on over 5 million instances from ACLANTHOLOGY. We use a combination of features proposed by (Rozovskaya et al., 2011) (which include lexical and POS N-grams, lexical head words, etc.), web-scale N-gram count featur</context>
</contexts>
<marker>Crammer, Dredze, Kulesza, 2009</marker>
<rawString>K. Crammer, M. Dredze, and A. Kulesza. 2009. Multiclass confidence weighted algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Correcting semantic collocation errors with L1-induced paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5479" citStr="Dahlmeier and Ng, 2011" startWordPosition="838" endWordPosition="841">ed work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was </context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier and H.T. Ng. 2011a. Correcting semantic collocation errors with L1-induced paraphrases. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5479" citStr="Dahlmeier and Ng, 2011" startWordPosition="838" endWordPosition="841">ed work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was </context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier and H.T. Ng. 2011b. Grammatical error correction with alternating structure optimization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="23075" citStr="Dahlmeier and Ng, 2012" startWordPosition="3885" endWordPosition="3888">t is The data is similar to the test set., and the gold-standard edits are two corrections with —* to, c —* the that change with to to and insert the before test set. The official HOO scorer however extracts a single system edit with —* to the for this instance. As the extracted system edit is different from the gold-standard edits, the system would be considered wrong, although it proposes the exact same corrected sentence as the gold standard edits. This problem has also been recognized by the HOO shared task organizers (see (Dale and Kilgarriff, 2011), Section 5). Our MaxMatch (M2) scorer (Dahlmeier and Ng, 2012) overcomes this problem through an efficient algorithm that computes the set of system edits which has the maximum overlap with the goldstandard edits. We use the M2 scorer as the main evaluation metric in our experiments. Additionally, we also report results with the official HOO scorer. Once the set of system edits is extracted, precision, recall, and F1 measure are computed as follows. 3“Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. 573 P = = R En i=1 |di ∩ gi| F1 = E&apos;1 |di| En i=1 |di ∩ gi| Ei1</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>D. Dahlmeier and H.T. Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="4666" citStr="Dale and Kilgarriff, 2011" startWordPosition="708" endWordPosition="711">the overall hypothesis score for each hypothesis as a linear combination of the expert scores. The weights of the decoder model are discriminatively trained on a development set of error-annotated sentences. The highest scoring hypotheses are kept in the search beam for the next iteration. This search procedure continues until the beam is empty or the maximum number of iterations has been reached. The highest scoring hypothesis is returned as the sentence-level correction. We evaluate our proposed decoder in the context of the Helping Our Own (HOO) shared task on grammatical error correction (Dale and Kilgarriff, 2011). Our decoder achieves an F1 score of 25.48% which improves upon the current state of the art. The remainder of this paper is organized as follows. The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to e</context>
<context position="19314" citStr="Dale and Kilgarriff, 2011" startWordPosition="3196" endWordPosition="3199">rds might require multiple corrections, for example spelling correction followed by noun number correction. Errors can also be inter-dependent, where correcting one word makes it necessary to change another word, for example to preserve agreement. Our decoding algorithm has the option to correct some words multiple times, while leaving other words unchanged. 4 Experiments We evaluate our decoder in the context of the HOO shared task on grammatical error correction. The goal of the task is to automatically correct errors in academic papers from NLP. The readers can refer to the overview paper (Dale and Kilgarriff, 2011) for details. We compare our proposed method with two baselines: a phrase-based SMT system (described in Section 4.3) and a pipeline of classifiers (described in Section 4.4). 4.1 Data We split the HOO development data into an equal sized training (HOO-TRAIN) and tuning (HOOTUNE) set. The official HOO test data (HOO-TEST) is used for evaluation. In the HOO shared task, participants were allowed to raise objections regarding the gold-standard annotations (corrections) of the test data after the test data was released. As a result, the gold-standard annotations could be biased in fa572 Algorithm</context>
<context position="21833" citStr="Dale and Kilgarriff, 2011" startWordPosition="3657" endWordPosition="3661">cuments that overlap with the HOO data. Section headers, references, etc. are automatically removed. The Web 1T 5-gram corpus (Brants and Franz, 2006) is used for language modeling and collecting web Ngram counts. Table 1 gives an overview of the data sets. 2http://www.aclweb.org/anthology-new/ Data Set Sentences Tokens HOO-TRAIN 467 11,373 HOO-TUNE 472 11,435 HOO-TEST 722 18,790 ACL-ANTHOLOGY 943,965 22,465,690 Table 1: Overview of the data sets. 4.2 Evaluation We evaluate performance by computing precision, recall, and F1 correction score without bonus as defined in the official HOO report (Dale and Kilgarriff, 2011)3. F1 correction score is simply the F1 measure (van Rijsbergen, 1979) between the corrections (called edits in HOO) proposed by a system and the gold-standard corrections. Let {e1, ... , en} be a set of test sentences and let {g1, ... , gn} be the set of gold-standard edits for the sentences. Let {h1, ... , hn} be the set of corrected sentences output by a system. One difficulty in the evaluation is that the set of system edits {d1, ... , dn} between the test sentences and the system outputs is ambiguous. For example, assume that the original test sentence is The data is similar with test set</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>R. Dale and A. Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 2011 European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/ megam/.</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>H. Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name#daume04cg-bfgs, implementation available at http://hal3.name/ megam/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D´esilets</author>
<author>M Hermet</author>
</authors>
<title>Using automatic roundtrip translation to repair general errors in second language writing.</title>
<date>2009</date>
<booktitle>In Proceedings of MT-Summit XII.</booktitle>
<marker>D´esilets, Hermet, 2009</marker>
<rawString>A. D´esilets and M. Hermet. 2009. Using automatic roundtrip translation to repair general errors in second language writing. In Proceedings of MT-Summit XII.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press, Cambridge,MA.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press, Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing: A meta-classifier approach.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="5455" citStr="Gamon, 2010" startWordPosition="836" endWordPosition="837">view of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annot</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing: A meta-classifier approach. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>02</issue>
<contexts>
<context position="5389" citStr="Han et al., 2006" startWordPosition="824" endWordPosition="827">r of this paper is organized as follows. The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N.R. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="13672" citStr="Hopkins and May, 2011" startWordPosition="2180" endWordPosition="2183">decoder to learn a bias against overcorrecting sentences and to learn which types of corrections are more likely and which are less likely. 3.4 Decoder Model The hypothesis features described in the previous subsection are combined to compute the score of a hypothesis according to the following linear model: S = wTfE(h), (4) where w is the decoder model weight vector and fE is a feature map that computes the hypothesis features described above, given a set of experts E. The weight vector w is tuned on a development set of error-annotated sentences using the PRO ranking optimization algorithm (Hopkins and May, 2011).1 1We also experimented with the MERT algorithm (Och, 2003) but found that PRO achieved better results. PRO performs decoder parameter tuning through a pair-wise ranking approach. The algorithm starts by sampling hypothesis pairs from the N-best list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sen</context>
<context position="24387" citStr="Hopkins and May, 2011" startWordPosition="4118" endWordPosition="4121">e definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of classifier-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular er</context>
<context position="30339" citStr="Hopkins and May, 2011" startWordPosition="5075" endWordPosition="5078"> written in Java and was easier to integrate into our Java-based decoder than RandLM. lated annealing temperature T is initialized to 10 and the exponential cooling schedule c is set to 0.9. The decoder weight vector is initialized as follows. The weight for the language model score and the weights for the classifier expert average scores are initialized to 1.0, and the weights for the classifier expert delta scores are initialized to −1.0. The weights for the correction count features are initialized to zero. For PRO optimization, we use the HOO-TUNE data and the default PRO parameters from (Hopkins and May, 2011): we sample 5,000 hypothesis pairs from the N-best list (N = 100) for every input sentence and keep the top 50 sample pairs with the highest difference in F1 measure. The weights are optimized using MegaM (Daum´e III, 2004) and interpolated with the previous weight vector with an interpolation parameter of 0.1. We normalize feature values to avoid having features on a larger scale dominate features on a smaller scale. We linearly scale all hypothesis features to a unit interval [0, 1]. The minimum and maximum values for each feature are estimated from the development data. We use an early stop</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>M. Hopkins and J. May. 2011. Tuning as ranking. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="5371" citStr="Knight and Chander, 1994" startWordPosition="820" endWordPosition="823">e of the art. The remainder of this paper is organized as follows. The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) us</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="24261" citStr="Koehn et al., 2003" startWordPosition="4095" endWordPosition="4098">1 = E&apos;1 |di| En i=1 |di ∩ gi| Ei1|gi| P × R 2 × P + R We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of classi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demonstration Session.</booktitle>
<contexts>
<context position="24061" citStr="Koehn et al., 2007" startWordPosition="4065" endWordPosition="4068">are computed as follows. 3“Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. 573 P = = R En i=1 |di ∩ gi| F1 = E&apos;1 |di| En i=1 |di ∩ gi| Ei1|gi| P × R 2 × P + R We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23931" citStr="Koehn, 2004" startWordPosition="4044" endWordPosition="4045"> report results with the official HOO scorer. Once the set of system edits is extracted, precision, recall, and F1 measure are computed as follows. 3“Without bonus” means that a system does not receive extra credit for not making corrections that are considered optional in the gold standard. 573 P = = R En i=1 |di ∩ gi| F1 = E&apos;1 |di| En i=1 |di ∩ gi| Ei1|gi| P × R 2 × P + R We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25333" citStr="Kudo and Matsumoto, 2003" startWordPosition="4272" endWordPosition="4275">o serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of classifier-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular error category, and feeds the corrected text into the next step. No search or global inference is applied. The correction steps are: 1. Spelling errors 2. Article errors 3. Preposition errors 4. Punctuation errors 5. Noun number errors We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5. Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL-ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Cramme</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T Kudo and Y. Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="5232" citStr="Leacock et al., 2010" startWordPosition="801" endWordPosition="804">rammatical error correction (Dale and Kilgarriff, 2011). Our decoder achieves an F1 score of 25.48% which improves upon the current state of the art. The remainder of this paper is organized as follows. The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been inve</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic grammar correction for second-language learners.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="6177" citStr="Lee and Seneff (2006)" startWordPosition="946" endWordPosition="949">s that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was available, the training data was created artificially from non-learner text. Lee and Seneff (2006) describe a lattice-based correction system with a domain-specific grammar for spoken utterances from the flight domain. The work in (D´esilets and Hermet, 2009) uses simple round-trip translation with a standard SMT system to correct grammatical errors. Dahlmeier and Ng (2011a) correct collocation errors using phrase-based SMT and paraphrases induced from the writer’s native language. Park and Levy (2011) propose a noisy channel model for error correction. While their motivation to correct whole sentences is similar to ours, their proposed generative method differs substantially from our disc</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>J. Lee and S. Seneff. 2006. Automatic grammar correction for second-language learners. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="14533" citStr="Lin and Och, 2004" startWordPosition="2320" endWordPosition="2323">est list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentencelevel BLEU approximation (Lin and Och, 2004) instead of the corpus-level BLEU score (Papineni et al., 2002). We observed that optimizing sentencelevel F1 score worked well in practice in our experiments. 3.5 Decoder Search Given a set of proposers, experts, and a tuned decoder model, the decoder can be used to correct new unseen sentences. This is done by performing a search over possible hypothesis candidates. The decoder starts with the input sentence as the initial hypothesis, i.e., assuming that all words are correct. It then performs a beam search over the space of possible hypotheses to find the best hypothesis correction h� for a</context>
<context position="24478" citStr="Lin and Och, 2004" startWordPosition="4132" endWordPosition="4135">ance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf software. 4.4 Pipeline Baseline The second baseline system is a pipeline of classifier-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular error category, and feeds the corrected text into the next step. No search or global inferenc</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F.J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>M Marsi</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and M. Marsi. 2007. MaltParser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24180" citStr="Och and Ney, 2003" startWordPosition="4082" endWordPosition="4085">hat are considered optional in the gold standard. 573 P = = R En i=1 |di ∩ gi| F1 = E&apos;1 |di| En i=1 |di ∩ gi| Ei1|gi| P × R 2 × P + R We note that the M2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed. For statistical significance testing, we use sign-test with bootstrap re-sampling (Koehn, 2004) with 1,000 samples. 4.3 SMT Baseline We build a baseline error correction system, using the MOSES SMT system (Koehn et al., 2007). Word alignments are created automatically on “good-bad” parallel text from HOO-TRAIN using GIZA++ (Och and Ney, 2003), followed by phrase extraction using the standard heuristic (Koehn et al., 2003). The maximum phrase length is 5. Parameter tuning is done on the HOO-TUNE data with the PRO algorithm (Hopkins and May, 2011) implemented in MOSES. The optimization objective is sentencelevel BLEU (Lin and Och, 2004). We note that the objective function is not the same as the final evaluation F1 score. Also, the training and tuning data are small by SMT standards. The aim for the SMT baseline is not to achieve a state-of-the-art system, but to serve as the simplest possible baseline that uses only off-the-shelf s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13732" citStr="Och, 2003" startWordPosition="2191" endWordPosition="2192">ich types of corrections are more likely and which are less likely. 3.4 Decoder Model The hypothesis features described in the previous subsection are combined to compute the score of a hypothesis according to the following linear model: S = wTfE(h), (4) where w is the decoder model weight vector and fE is a feature map that computes the hypothesis features described above, given a set of experts E. The weight vector w is tuned on a development set of error-annotated sentences using the PRO ranking optimization algorithm (Hopkins and May, 2011).1 1We also experimented with the MERT algorithm (Och, 2003) but found that PRO achieved better results. PRO performs decoder parameter tuning through a pair-wise ranking approach. The algorithm starts by sampling hypothesis pairs from the N-best list of the decoder. The metric score for each hypothesis induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is not de</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14596" citStr="Papineni et al., 2002" startWordPosition="2331" endWordPosition="2334">is induces a ranking of the two hypotheses in each pair. The task of finding a weight vector that correctly ranks hypotheses can then be reduced to a simple binary classification task. In this work, we use PRO to optimize the F1 correction score, which is defined in Section 4.2. PRO requires a sentence-level score for each hypothesis. As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentencelevel BLEU approximation (Lin and Och, 2004) instead of the corpus-level BLEU score (Papineni et al., 2002). We observed that optimizing sentencelevel F1 score worked well in practice in our experiments. 3.5 Decoder Search Given a set of proposers, experts, and a tuned decoder model, the decoder can be used to correct new unseen sentences. This is done by performing a search over possible hypothesis candidates. The decoder starts with the input sentence as the initial hypothesis, i.e., assuming that all words are correct. It then performs a beam search over the space of possible hypotheses to find the best hypothesis correction h� for an input sentence e. The search proceeds in iterations until the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Park</author>
<author>R Levy</author>
</authors>
<title>Automated whole sentence grammar correction using a noisy channel model.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6586" citStr="Park and Levy (2011)" startWordPosition="1008" endWordPosition="1011">6) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was available, the training data was created artificially from non-learner text. Lee and Seneff (2006) describe a lattice-based correction system with a domain-specific grammar for spoken utterances from the flight domain. The work in (D´esilets and Hermet, 2009) uses simple round-trip translation with a standard SMT system to correct grammatical errors. Dahlmeier and Ng (2011a) correct collocation errors using phrase-based SMT and paraphrases induced from the writer’s native language. Park and Levy (2011) propose a noisy channel model for error correction. While their motivation to correct whole sentences is similar to ours, their proposed generative method differs substantially from our discriminative decoder. Park and Levy’s model does not allow the use of discriminative expert classifiers as our decoder does, but instead relies on a bigram language model to find grammatical corrections. Indeed, they point out that the language model often fails to distinguish grammatical and ungrammatical sentences. To the best of our knowledge, our work is the first discriminatively trained decoder for who</context>
</contexts>
<marker>Park, Levy, 2011</marker>
<rawString>Y. A. Park and R. Levy. 2011. Automated whole sentence grammar correction using a noisy channel model. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Faster and smaller N-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="29292" citStr="Pauls and Klein, 2011" startWordPosition="4902" endWordPosition="4905">poser and the language model expert. We then add the article proposer and expert, the preposition proposer and expert, the punctuation proposer, and finally the noun number proposer and expert. We refer to the final configuration with all proposers and experts as the full decoder model. Note that error categories are corrected jointly and not in sequential steps as in the pipeline. To make the results directly comparable to the pipeline, the decoder uses the same resources as the pipeline. As the expert models, we use a 5-gram language model from the Web 1T 5-gram corpus with the Berkeley LM (Pauls and Klein, 2011)7 in the decoder and the CW-classifiers described in the last section. The spelling proposer uses the same spellchecker as the pipeline, and the punctuation proposer uses the same rules as the pipeline. The beam width and the maximum number of iterations are set to 10. In earlier experiments, we found that larger values had no effect on the result. The simuunderneath, until, up, upon, with, within, without 7Berkeley LM is written in Java and was easier to integrate into our Java-based decoder than RandLM. lated annealing temperature T is initialized to 10 and the exponential cooling schedule c</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>A. Pauls and D. Klein. 2011. Faster and smaller N-gram language models. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="5508" citStr="Rozovskaya and Roth, 2011" startWordPosition="842" endWordPosition="846">bes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because no large annotated learner corpus was available, the training data </context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>M Sammons</author>
<author>J Gioja</author>
<author>D Roth</author>
</authors>
<title>University of Illinois system in HOO text correction shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="26451" citStr="Rozovskaya et al., 2011" startWordPosition="4458" endWordPosition="4461">cation problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the articles a, the, and the empty article c. The article an is normalized as a and restored later using a rule-based heuristic. We consider all NPs that are not pronouns and do not have a non-article determiner, e.g., this, that. The classifier is trained on over 5 million instances from ACLANTHOLOGY. We use a combination of features proposed by (Rozovskaya et al., 2011) (which include lexical and POS N-grams, lexical head words, etc.), web-scale N-gram count features from the Web 1T 5-gram corpus following (Bergsma et al., 2009), and dependency head and child features. During testing, a correction is proposed if the predicted article is different from the observed article used by the writer, and the difference between the confidence score for the predicted article and the confidence score for the observed article is larger than a threshold. Threshold parameters are tuned via a grid-search on the HOO-TUNE data. We tune a separate threshold value for each clas</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth. 2011. University of Illinois system in HOO text correction shared task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Russell</author>
<author>P Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach, chapter 27.</title>
<date>2010</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="17958" citStr="Russell and Norvig, 2010" startWordPosition="2973" endWordPosition="2976"> hand.. score = 8.29 ... ... the —. an the —. e On —.About On —. By On — .To In —. With For other hands .. score = 7.00 the score of the best hypothesis and the “temperature” of the system. We lower the temperature after each iteration according to an exponential cooling schedule. Hypotheses that have been explored before are not considered again to avoid cycles in the search. From all hypotheses in the pool, we select the top k hypotheses and add them to the beam for the next search iteration. The decoding algorithm is shown in Algorithm 1. The decoder can be considered an anytime algorithm (Russell and Norvig, 2010), as it has a current best hypothesis correction available at any point of the search, while gradually improving the result by searching for better hypotheses. An example of a search tree produced by our decoder is shown in Figure 1. The decoding algorithm shares some similarities with the beam-search algorithm frequently used in SMT. There is however a difference between SMT decoding and grammar correction decoding that is worth pointing out. In SMT decoding, every input word needs to be translated exactly once. In contrast, in grammar correction decoding, the majority of the words typically </context>
</contexts>
<marker>Russell, Norvig, 2010</marker>
<rawString>S. Russell and P. Norvig, 2010. Artificial Intelligence: A Modern Approach, chapter 27. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25488" citStr="Talbot and Osborne, 2007" startWordPosition="4298" endWordPosition="4301">er-based and rule-based correction steps. Each step takes sentence segmented plain text as input, corrects one particular error category, and feeds the corrected text into the next step. No search or global inference is applied. The correction steps are: 1. Spelling errors 2. Article errors 3. Preposition errors 4. Punctuation errors 5. Noun number errors We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al., 2007) for dependency parsing. For language modeling, we use RandLM (Talbot and Osborne, 2007). For spelling correction, we use GNU Aspell5. Words that contain upper-case characters inside the word or are shorter than four characters are excluded from spell checking. The spelling dictionary is augmented with all words that appear at least 10 times in the ACL-ANTHOLOGY data set. Article correction is cast as a multi-class classification problem. As the learning algorithm, we choose multi-class confidence-weighted (CW) learning (Crammer et al., 2009) which has been shown to perform well for NLP problems with high dimensional and sparse feature spaces. The possible classes are the article</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>D. Talbot and M. Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5442" citStr="Tetreault and Chodorow, 2008" startWordPosition="832" endWordPosition="835">The next section gives an overview of related work. Section 3 describes the proposed beam-search decoder. Sections 4 and 5 describe the experimental setup and results, respectively. Section 6 provides further discussion. Section 7 concludes the paper. 2 Related Work In this section, we summarize related work in grammatical error correction. For a more detailed review, the readers can refer to (Leacock et al., 2010). The classification approach to error correction has mainly focused on correcting article and preposition errors (Knight and Chander, 1994; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Rozovskaya and Roth, 2011). The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context. Typical features include surrounding N-grams, partof-speech (POS) tags, chunks, etc. In fact, a considerable amount of research effort has been invested in finding better features. The SMT approach to error corrections has received comparatively less attention. Brockett et al. (2006) use an SMT system to correct errors involving mass noun errors. Because n</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="27653" citStr="Tetreault et al., 2010" startWordPosition="4639" endWordPosition="4642">d value for each class. Preposition correction and noun number correction are analogous to article correction. They differ only in terms of the classes and the features. For preposition correction, the classes are 36 frequent English prepositions6. The features are surrounding 4http://opennlp.sourceforge.net 5http://aspell.net 6about, along, among, around, as, at, beside, besides, between, by, down, during, except, for, from, in, inside, into, of, off, on, onto, outside, over, through, to, toward, towards, under, 574 lexical N-grams, web-scale N-gram counts, and dependency features following (Tetreault et al., 2010). The preposition classifier is trained on 1 million training examples from the ACL-ANTHOLOGY. For noun number correction, the classes are singular and plural. The features are lexical N-grams, webscale N-gram counts, dependency features, the noun lemma, and a binary countability feature. The noun number classifier is trained on over 5 million examples from ACL-ANTHOLOGY. During testing, the singular or plural word surface form is generated using WordNet (Fellbaum, 1998) and simple heuristics. Punctuation correction is done using a set of simple rules developed on the HOO development data. At </context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworth,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworth, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
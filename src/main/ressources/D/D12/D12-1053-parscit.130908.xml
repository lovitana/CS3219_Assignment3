<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000569">
<title confidence="0.999166">
A Statistical Relational Learning Approach to Identifying
Evidence Based Medicine Categories
</title>
<author confidence="0.8579545">
Mathias Verbeke♦ Vincent Van Asch♣ Roser Morante♣
Paolo Frasconi♠ Walter Daelemans♣ Luc De Raedt♦
</author>
<affiliation confidence="0.992876">
♦ Department of Computer Science, Katholieke Universiteit Leuven, Belgium
</affiliation>
<email confidence="0.792217">
{mathias.verbeke, luc.deraedt}@cs.kuleuven.be
</email>
<affiliation confidence="0.652401">
♣ Department of Linguistics, Universiteit Antwerpen, Belgium
</affiliation>
<email confidence="0.958105">
{roser.morante, vincent.vanasch, walter.daelemans}@ua.ac.be
</email>
<author confidence="0.572736">
♠ Dipartimento di Sistemi e Informatica, Universit`a degli Studi di Firenze, Italy
</author>
<email confidence="0.959282">
p-f@dsi.unifi.it
</email>
<sectionHeader confidence="0.998336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992328125">
Evidence-based medicine is an approach
whereby clinical decisions are supported by
the best available findings gained from scien-
tific research. This requires efficient access
to such evidence. To this end, abstracts in
evidence-based medicine can be labeled using
a set of predefined medical categories, the so-
called PICO criteria. This paper presents an
approach to automatically annotate sentences
in medical abstracts with these labels. Since
both structural and sequential information are
important for this classification task, we use
kLog, a new language for statistical relational
learning with kernels. Our results show a clear
improvement with respect to state-of-the-art
systems.
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99862418">
Evidence-based medicine (EBM) or evidence-based
practice (EBP) combines clinical expertise, the pref-
erences and values of the patient and the best
available evidence to make good patient care deci-
sions. Clinical research findings are systematically
reviewed, appraised and used to improve the patient
care, for which efficient access to such evidence is
required. In order to facilitate the search process,
medical documents are labeled using a set of prede-
fined medical categories, the PICO criteria. PICO is
an acronym for the mnemonic concepts that are used
to construct queries when searching for scientific ev-
idence in the EBM process. The need to automatize
the annotation process has initiated research into au-
tomatic approaches to annotate sentences in medical
documents with the PICO labels.
As indicated by Kim et al. (2011), both the struc-
tural information of the words in the sentence, and
that of the sentences in the document are important
features for this task. Furthermore, sequential infor-
mation can leverage the dependencies between dif-
ferent sentences in the text. Therefore we propose an
approach using kLog (Frasconi et al., 2012) to tackle
this problem. kLog is a new language for statistical
relational learning with kernels, that is embedded in
Prolog, and builds upon and links together concepts
from database theory, logic programming and learn-
ing from interpretations. Learning from interpreta-
tions is a logical and relational learning setting (De
Raedt et al., 2008) in which the examples are inter-
pretations, that is, sets of tuples that are true in the
examples. In a sense, each example can be viewed
as a small relational database. kLog is able to trans-
form relational into graph-based representations and
apply kernel methods to extract an extended high-
dimensional feature space.
The choice for kLog was motivated by previous
results (Verbeke et al., 2012), where we showed that
a statistical relational learning approach using kLog
is able to process the contextual aspects of language
improving on state-of-the-art results for hedge cue
detection. However, the current task adds two levels
of complexity. First, next to the relations between
the words in the sentence, now also the relations be-
tween the sentences in the document become impor-
tant. In the proposed approach, we first generate a
feature space with kLog that captures the intrasen-
tential properties and relations. Hereafter, these fea-
tures serve as input for a structured output support
vector machine that can handle sequence tagging
</bodyText>
<page confidence="0.977075">
579
</page>
<note confidence="0.8219545">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 579–589, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.985665263157895">
(Tsochantaridis et al., 2004), in order to take the
intersentential features into account. Second, since
there are more than two categories, and each sen-
tence can have multiple labels, the problem is now a
multiclass multilabel classification task.
The main contribution of this paper is that we
show that kLog’s relational nature and its ability
to declaratively specify and use background knowl-
edge is beneficial for natural language learning prob-
lems. This is shown on the NICTA-PIBOSO corpus,
for which we present results that indicate a clear im-
provement on the state-of-the-art.
The remainder of this paper is organized as fol-
lows. In Section 2, we outline earlier work that is
related to the research presented here. Section 3 de-
scribes the methodology of our method. We present
a thorough evaluation of our method in Section
4. The last section draws conclusions and presents
some ideas for future work.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999878726027397">
EBM is an approach to clinical problem-solving
based on “systematically finding, appraising, and us-
ing contemporaneous research findings as the ba-
sis for clinical decisions” (Rosenberg and Donald,
1995). The evidence-based process consists of four
steps: (1) Formulating a question from a patient’s
problem; (2) Searching the literature for relevant
clinical articles; (3) Evaluating the evidence; And
(4) implementing useful findings in clinical prac-
tice. Given the amounts of medical publications
available in databases such as PubMed, automating
step 2 is crucial to help doctors in their practice.
Efforts in this direction from the NLP community
have so far focused on corpus annotation (Demner-
Fushman and Lin, 2007; Kim et al., 2011), text cate-
gorization (Davis-Desmond and Moll´a, 2012), sum-
marization (Moll´a and Santiago-Martinez, 2011),
and question-anwering (Niuet al., 2003; Demner-
Fushman and Lin, 2007).
The existing corpora are usually annotated with
the PICO mnemonic (Armstrong, 1999) concepts,
that are used to build queries when searching for
literature for EBM purposes. The PICO concepts
are: primary Problem (P) or population, main Inter-
vention (I), main intervention Comparison (C), and
Outcome of intervention (O). PICO helps determin-
ing what terms are important in a query and there-
fore it helps building the query, which is sent to the
search repositories. Once the documents are found,
they need to be read by a person who eliminates ir-
relevant documents.
The first attempt to classify PICO concepts is pre-
sented in Demner-Fushman and Lin (2007), who
apply a rule-based approach to identify sentences
where PICO concepts occur and a supervised ap-
proach to classify sentences that contain an Out-
come. The features used by this classifier are n-
grams, position, and semantic information from the
parser used to process the data. The system is trained
on 275 abstracts manually annotated. The accura-
cies reported range from 80% for Population, 86%
for Problem, 80% for Intervention, and, from 64%
to 95% for Outcome depending on the test set of ab-
stracts.
Kim et al. (2011) perform a similar classification
task in two steps. First a classifier identifies the sen-
tences that contain PICO concepts, and then another
classifier assigns PICO tags to the sentences found
to be relevant by the previous classifier. The sys-
tem is based on a CRF algorithm and is trained on
the NICTA-PIBOSO corpus. This dataset contains
1,000 medical abstracts manually annotated with an
extension of the PICO tagset, for which the defini-
tions are listed in Table 1. The annotation is per-
formed at sentence level and one sentence may have
more than one tag. An example of an annotated
abstract from the corpus can be found in the sup-
plementary material. The features used by the al-
gorithm include features derived from the context,
semantic relations, structure and sequencing of the
text. The system is evaluated for 5-way and 6-way
classification and results are provided apart from
structured and unstructured abstracts. The F-scores
for structured abstracts is 89.32% for 5-way classifi-
cation and 80.88% for 6-way classification, whereas
for unstructured abstracts it is 71.54% for 5-way
classification and 64.66% for 6-way classification.
Chung (2009) uses CRF to classify PICO con-
cepts by combining them with general categories as-
sociated with rhetorical roles: Aim, Method, Results
and Conclusion. Her system is tested on corpora of
abstracts of randomized control trials. First struc-
tured abstracts with headings labeled with PICO
</bodyText>
<page confidence="0.996191">
580
</page>
<table confidence="0.6588479">
Background Material that informs and may place the current study in perspective, e.g. work that preceded the
current; information about disease prevalence; etc.
Population The group of individual persons, objects or items comprising the study’s sample, or from which
the sample was taken for statistical measurement
Intervention The act of interfering with a condition to modify it or with a process to change its course (includes
prevention)
Outcome The sentence(s) that best summarizes the consequences of an intervention
Study Design The type of study that is described in the abstract
Other Any sentence not falling into one of the other categories and presumed to provide little help with
clinical decision making, i.e. non-key or irrelevant sentences
</table>
<tableCaption confidence="0.999382">
Table 1: Definitions of the semantic tags used as annotation categories (taken from Kim et al. (2011)).
</tableCaption>
<bodyText confidence="0.99718103030303">
concepts are used. A sentence level classification
task is performed, assigning only one rhetorical role
per sentence. The F-scores obtained range from 0.93
to 0.98. Then another sentence level classification
task is performed to automatically assign the labels
Intervention, Participant and Outcome Measures to
sentences in unstructured and structured abstracts
without headings. F-scores of up to 0.83 and 0.84
are obtained for Intervention and Outcome Measure
sentences.
Other work aimed at identifying rhetorical zones
in biomedical articles. In this case areas of text are
classified in terms of the rhetorical categories In-
troduction, Methods, Results and Discussion (IM-
RAD) (Agarwal and Yu, 2009) or richer categories,
such as problem-setting or insight (Mizuta et al.,
2006).
There exists a wide range of statistical relational
learning systems (Getoor and Taskar, 2007; De
Raedt et al., 2008), and many of these systems
are in principle useful for natural language process-
ing. The most popular formalism today is Markov
Logic, which has already been used for natural lan-
guage processing tasks such as semantic role label-
ing (Riedel and Meza-Ruiz, 2008) and coreference
resolution (Poon and Domingos, 2008). With re-
spect to Markov Logic, two distinguishing features
of kLog are that 1) it employs kernel based meth-
ods grounded in statistical learning theory, and 2) it
employs a Prolog like language for defining and us-
ing background knowledge. As Prolog is a program-
ming language, this is more flexible that the formal-
ism used by Markov Logic.
</bodyText>
<sectionHeader confidence="0.998014" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999974176470588">
In learning from examples, or interpretations (De
Raedt et al., 2008), the instances are sampled iden-
tically and independently from some unknown but
fixed distribution. They can be represented as pairs
z = (x, y), in which x represents the inputs and y
the outputs. An example interpretation can be found
in Figure 3, where the hasCategory relation repre-
sents y in this case, since it is the target relation we
want to predict. The inputs x are formed by all other
facts. The task is now to learn a function h : X —* Y
that maps the inputs to the outputs. Sentences may
have multiple labels. Hence this is a structured out-
put task where the output is a sequence of sets of
labels attached to the sentences in a given document.
kLog is the new statistical relational language for
learning with kernels that we use to tackle the PICO
categories classification task. The novelty of kLog
is that, based on the regular, linguistic features, it
allows to define an extended high-dimensional fea-
ture space that is also able to take relational features
into account in a principled manner. Furthermore,
its declarative approach offers a flexible and inter-
pretable way to construct features.
The choice of kLog is motivated by our previous
results (Verbeke et al., 2012), where we showed that
the relational representation of the domain as used
by kLog is able to take the contextual aspects of lan-
guage into account. Whereas there we only used
the relations at the sentence level, the current task
adds a new level of complexity, since the identifica-
tion of PICO categories in abstracts also requires to
take into account various relations between the sen-
tences of an abstract. The general workflow of our
approach is depicted in Figure 1, which will be de-
</bodyText>
<page confidence="0.977762">
581
</page>
<figure confidence="0.99960975">
Feature extraction
(lemma, POS,É)
Raw data
(sentence)
kLog
Database
(Fig. 3)
Declarative feature Feature
Graphicalization
construction generation
Extensionalized
database
Graph
(Fig. 4)
Graph kernel
(NSPDK)
Kernel matrix/
feature vectors
Statistical
learner
</figure>
<figureCaption confidence="0.999978">
Figure 1: General kLog workflow.
</figureCaption>
<bodyText confidence="0.999771057142857">
scribed step by step in the following paragraphs.
Preprocessing The sentences have been prepro-
cessed with a named entity tagger and a dependency
parser.
Named entity tagging has been performed with
the BiogaphTA named entity module, which
matches token sequences with entries in the UMLS
database1. UMLS integrates over 2 million names
for some 900,000 concepts from more than 60 fami-
lies of biomedical vocabularies (Bodenreider, 2004).
The tagger matches sequences with a length of max-
imum 4 tokens. This covers 66.2% of the UMLS
entries. By using UMLS, different token sequences
referring to the same concept can be mapped to
the same concept identifier (CID). The BiographTA
named entity tagger has been evaluated on the
BioInfer corpus (Pyysalo et al., 2007) obtaining a
72.02 F1 score.
Dependency parsing has been performed with the
GENIA dependency parser GDep (Sagae and Tsu-
jii, 2007), which uses a best-first probabilistic shift-
reduce algorithm based on the LR algorithm (Knuth,
1965) and extended by the pseudo-projective pars-
ing technique. This parser is a version of the KSDep
dependency parser trained on the GENIA Treebank
for parsing biomedical text. KSDep was evaluated
in the CoNLL Shared Task 2007 obtaining a La-
beled Attachment Score of 89.01% for the English
dataset. GDEP outputs the lemmas, chunks, Genia
named entities and dependency relations of the to-
kens in a sentence.
This information can be represented as an
Entity/Relationship (E/R) diagram, a modeling
paradigm that is frequently used in database theory
(Garcia-Molina et al., 2008). The E/R-model for the
</bodyText>
<footnote confidence="0.9266805">
1From UMLS only the MRCONSO.RRF and MRSTY.RRF
files are used.
</footnote>
<bodyText confidence="0.999588">
problem under consideration is shown in Figure 2,
which provides an abstract representation of the ex-
amples, i.e. medical abstracts in this case. We will
show later how this abstract representation can be
unrolled for each example, resulting in a graph; cf.
also Figure 4 for our example sentence. This rela-
tional database representation will serve as the input
for kLog.
</bodyText>
<figureCaption confidence="0.9955415">
Figure 2: E/R-diagram modeling the sentence identifica-
tion task.
</figureCaption>
<bodyText confidence="0.9999163">
The entities are the words and sentences in the
abstract. They are represented by the rectangles in
the E/R-model. Each entity can have a number of
properties attached to it, depicted by the ovals and
has a unique identifier (underlined properties). As
in database theory, each entity corresponds with a
tuple, or fact, in the database.
Figure 3 shows a part of an example interpretation
z. For example, w(w4 1,‘Surgical’,‘Surgical’,b-
np,jj,‘O’,‘O’) specifies a word entity, with w4 1 as
identifier and the other arguments as properties. As
indicated before, as lexical information we take the
token string itself, its lemma, the part-of-speech tag
and the chunk tag into account. We also include
some semantic information, namely two binary val-
ues indicating if the word is a (biological) named
entity. sentence(s4,4) represents a sentence entity,
with its index in the abstract as a property.
Furthermore, the E/R-diagram also contains a
number of relationships, which are represented by
</bodyText>
<figure confidence="0.997075647058823">
sentID
hasCategory
sentence
class
nextS
hasWord
wordID
depHead
depRel
next
w
NEUNILS
wordString
NEGenia
lemma
POS-tag
chunktag
</figure>
<page confidence="0.94973">
582
</page>
<equation confidence="0.956862631578947">
sentence(s4,4)
hasCategory(s4,‘background’)
w(w4_1,‘Surgical’,‘Surgical’,b-np,
jj,‘O’,‘O’) hasWord(s4,w4_1)
dh(w4_1,w4_2,nmod)
nextW(w4_2,w4_1)
w(w4_2,‘excision’,‘excision’,i-np,
nn,‘O’,‘O’) hasWord(s4,w4_2)
dh(w4_2,w4_5,sub)
nextW(w4_3,w4_2)
w(w4_3,‘of’,‘of’,b-pp,in,’O’,’O’)
hasWord(s4,w4_3)
dh(w4_3,w4_2,nmod)
nextW(w4_4,w4_3)
w(w4_4,‘CNV’,‘CNV’,b-np,nn,
‘B-protein’,‘O’) hasWord(s4,w4_4)
dh(w4_4,w4_3,pmod)
nextW(w4_5,w4_4)
...
</equation>
<figureCaption confidence="0.998258">
Figure 3: Part of an example interpretation z, represent-
ing the example sentence in Figure 4.
</figureCaption>
<bodyText confidence="0.999640361111111">
the diamonds. They are linked to the entities that
participate in the relationship, or stand alone if they
characterize general properties of the interpretation.
An example relation is nextW(w4 2,w4 1), which
indicates the sequence of the words in the sentence.
dh(w4 1,w4 2,nmod) specifies that word w4 1 is
a noun modifier of word w4 2, and thus serves to
incorporate the dependency relationships between
the words. hasCategory(s4,‘background’) signi-
fies that sentence s4 is a sentence describing back-
ground information. This relation is the target re-
lation that we want to predict for this task and will
not be taken into account as a feature, but is listed in
the database and only used during the training of the
model.
Since the previously described entities and rela-
tionships are listed explicitly in the database, these
are called extensional relations, in contrast to the in-
tensional relations, as we will describe next.
Declarative feature construction A strength of
kLog is that it is also capable of constructing fea-
tures declaratively, by using intensional relations.
This enables one to encode additional background
knowledge based on a small set of preprocessed fea-
tures, which renders experimentation very flexible
and makes the results more interpretable. It further-
more allows one to limit the required features to the
core discriminative ones. These intensional features
are defined through definite clauses, and is done us-
ing an extension of the declarative programming lan-
guage Prolog. The following features were used.
We make a distinction between the features used for
structured and unstructured abstracts.
For structured abstracts, four intensional relations
were defined. The relation lemmaRoot(S,L) is
specified as:
</bodyText>
<equation confidence="0.910718">
lemmaRoot(S,L) +-
hasWord(S, I),
w(I,_,L,_,_,_,_),
dh(I,_,root).
</equation>
<bodyText confidence="0.998297545454545">
For each sentence, it only selects the lemmas
of the root word in the dependency tree, which
markedly limits the number of word features used.
The following relations are related to, and try to
capture the document structure imposed by the sec-
tion headers present in the structured abstracts.
hasHeaderWord(S,X) identifies whether a sen-
tence is a header of a section. In order to realize this,
it selects the words of a sentence that count more
than four characters (to discard short names of bio-
logical entities), which all need to be uppercase.
</bodyText>
<equation confidence="0.953634285714286">
hasHeaderWord(S,X) +—
w(W,X, , , , , ),
hasWord(S,W),
(atom(X) -&gt; name(X,C) ; C = X),
length(C,Len),
Len &gt; 4,
all_upper(C).
</equation>
<bodyText confidence="0.952542636363637">
Also the sentences below a certain section header
need to be marked as belonging to this sec-
tion, which is done by the relation hasSection-
Header(S,X).
hasSectionHeader(S,X) �
nextS(S1,S),
hasHeaderWord(S1,X).
hasSectionHeader(S,X) �
nextS(S1,S),
not isHeaderSentence(S),
once(hasSectionHeader(S1,X)).
</bodyText>
<page confidence="0.995111">
583
</page>
<bodyText confidence="0.969141911111111">
For the unstructured abstracts, also the lemma-
Root relation is used, but next to the lemma, now
also the part-of-speech tag of the root word is taken
into account. Since the unstructured abstracts lack
section headers, other features were needed to dis-
tinguish between the different sections, for which
the relation prevLemmaRoot proved to be very in-
formative. It adds the lemma of the root word in the
previous sentence as a property to the current sen-
tence under consideration.
prevLemmaRoot(S,L) �
nextS(S1,S),
lemmaRoot(S1,L,_).
The intensional predicates are grounded. This is
a proces similar to materialization in databases, that
is, the atoms implied by the background knowledge
and the facts in the example are all computed using
Prolog’s deduction mechanism. This leads to the
extensionalized database, in which both the exten-
sional as well as the grounded intensional predicates
are listed.
Graphicalization and feature generation In the
third step, the interpretations are graphicalized, i.e.
transformed into graphs. Since the facts that form
the interpretation still conform to the E/R-diagram,
this can be interpreted as unfolding the E/R-diagram
over the data. An example illustrating this process
is given in Figure 4. Each interpretation is converted
into a bipartite graph, for which there is a vertex for
every ground atom of every E-relation, one for every
ground atom of every R-relation, and an undirected
edge {e, r} if an entity e participates in relationship
r.
The obtained graphs can then be used in the next
step for feature generation. This is done by means
of a graph kernel n, which calculates the similar-
ity between two graphicalized interpretations. Any
graph kernel that allows fast computations on large
graphs and has a flexible bias to enable heteroge-
neous features can in theory be applied. In the cur-
rent implementation, an extension of the Neighbor-
hood Subgraph Pairwise Distance Kernel (NSPDK)
(Costa and De Grave, 2010) is used.
NSPDK is a decomposition kernel (Haussler,
1999), in which pairs of subgraphs are compared
</bodyText>
<figureCaption confidence="0.996502">
Figure 4: Graphicalization G, of interpretation z.
</figureCaption>
<bodyText confidence="0.99974048">
to each other in order to calculate the similarity be-
tween two graphs. These subgraphs can be seen as
circles in the graph, and are defined by three hyper-
parameters. First of all, there is the center of the
subgraph, the kernel point, which can be any entity
or relation in the graph. The entities and relations
to be taken into account as kernel points are marked
beforehand as a subset of the intensional and exten-
sional domain relations. The radius r determines
the size of the subgraphs and defines which entities
or relations around the kernel point are taken into ac-
count. Each entity or relation that is within a number
of r edges away from the kernel point is considered
to be part of the subgraph. The third hyperparam-
eter, the distance d, determines how far apart from
each other the kernel points can be. Each subgraph
around a kernel point that is within a distance d or
less from the current kernel point will be considered.
This is captured by the relation R,.,d(A,,, Bu, G) be-
tween two rooted subgraphs A,,, Bu and a graph G,
which selects all pairs of neighborhood graphs of ra-
dius r whose roots are at distance d in a given graph
G.
The kernel n,.,d(G, G0) between graphs G and G0
on the relation R,.,d is then defined as:
</bodyText>
<equation confidence="0.976563166666667">
�n,.,d(G, G0) =
Av, Bu ∈ R−&apos;
r,d(G)
A0v,, B0u, ∈ R−&apos;
r,d(G0)
(1)
</equation>
<figure confidence="0.976936583333333">
Surgical excision of CNV may allow stabilisation or improvement
dh(nmod)
next next
title
nextS
s0
dh(sub)
title
nextS
dh(pmod)
s1
s3
next
s2
hasWord
background
s4
s6
s5
s7
s8
s9
of vision.
s(A,,, A0�0)s(Bu, B0u0)
</figure>
<page confidence="0.996908">
584
</page>
<bodyText confidence="0.999913666666667">
For efficiency reasons, an upper bound is imposed
on the radius and distance parameters, which leads
to the following kernel definition:
</bodyText>
<equation confidence="0.964317">
nr,d(G, G&apos;) (2)
</equation>
<bodyText confidence="0.999990810810811">
We hereby limit the sum of the nr,d kernels for all
increasing values of the radius and distance parame-
ter up to a maximum given value of r*, respectively
d*.
The result of this graphicalization and feature
generation process is an extended, high-dimensional
feature space, which serves as input for the statisti-
cal learner in the next step.
Learning The constructed feature space contains
one feature vector per sentence. This implies that
the sequence information of the sentences at the doc-
ument level is not taken into account yet. Since the
order of the sentences in the abstract is a valuable
feature for this prediction problem, a learner that
reflects this in the learning process is needed, al-
though in principle any statistical learner can be used
on the feature space constructed by kLog. There-
fore we opted for SVM-HMM2 (Tsochantaridis et
al., 2004), which is an implementation of structural
support vector machines for sequence tagging. In
contrast to a conventional Hidden Markov Model,
SVM-HMM is able to take these entire feature vec-
tors as observations, and not just atomic tokens.
In our case, the instances to be tagged are formed
by the sentences for which feature vectors were cre-
ated in the previous step. The qid is a special fea-
ture that is used in the structured SVM to restrict
the generation of constraints. Since every document
needs to be represented as a sequence of sentences,
in SVM-HMM, the qid’s are used to obtain the doc-
ument structure. The order of the HMM was set
to 2, which means that the two previous sentences
were considered for collective classification. The
cost value was set to 500, and was determined via
cross-validation. For epsilon, the default value, 0.5,
was kept, since this mainly only influences the run-
ning time and memory consumption during training.
</bodyText>
<footnote confidence="0.927199">
2http://www.cs.cornell.edu/people/tj/
svm_light/svm_hmm.html
</footnote>
<table confidence="0.999820222222222">
All S U
Nb. Abstracts 1000 376 624
Nb. Sentences 10379 4774 5605
- Background 2557 669 1888
- Intervention 690 313 377
- Outcome 4523 2240 2283
- Population 812 369 443
- Study Design 233 149 84
- Other 1564 1034 530
</table>
<tableCaption confidence="0.93329975">
Table 2: Number of abstracts and sentences for Struc-
tured (S) and Unstructured (U) abstract sets, including
number of sentences per class (taken from (Kim et al.,
2011)).
</tableCaption>
<sectionHeader confidence="0.998758" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999942">
We evaluate the performance of kLog against a base-
line system and a memory-based tagger (Daelemans
and van den Bosch, 2005). The results are also com-
pared against those from Kim et al. (2011), which is
the state-of-the-art system for this task.
</bodyText>
<subsectionHeader confidence="0.937032">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999987846153846">
We perform our experiments on the NICTA-
PIBOSO dataset from Kim et al. (2011) (kindly pro-
vided by the authors). It contains 1,000 abstracts of
which 500 were retrieved from MEDLINE by query-
ing for diverse aspects in the traumatic brain injury
and spinal cord injury domain. The dataset consists
of two types of abstracts. If the abstract contains
section headings (e.g. Background, Methodology,
Results, etc.), it is considered to be structured. This
information can be used as a feature in the model.
The other abstracts are regarded unstructured.
The definitions of the semantic tags used as an-
notations categories are a variation on the PICO tag
set, with the addition of two additional categories
(see Table 1 in Section 2). Each sentence can be an-
notated with multiple classes. This renders the task
a multiclass multilabel classification problem. The
statistics on this dataset can be found in Table 2.
In order to apply the same evaluation setting as
Kim et al. (2011), we used the dataset from Demner-
Fushman et al. (2005) as external dataset. It con-
sists of 100 sentences of which 51 are structured.
Because the semantic tag set used for annotation
slightly differs from the one presented in Table 1,
and to make our results comparable, we will use the
same mapping as used in Kim et al. (2011).
</bodyText>
<equation confidence="0.945416666666667">
Kr*,d*(G, G&apos;) =
� d*
d=0
r*
E
r=0
</equation>
<page confidence="0.996959">
585
</page>
<subsectionHeader confidence="0.962383">
4.2 Baseline and benchmarks
</subsectionHeader>
<bodyText confidence="0.999995155555556">
We compare the kLog system to three other systems:
a baseline system, a memory-based system, and the
scores reported by Kim et al. (2011).
The memory-based system that we use is based
on the memory-based tagger MBT3 (Daelemans and
van den Bosch, 2005). This machine learner is orig-
inally designed for part-of-speech tagging. It pro-
cesses data on a sentence basis by carrying out se-
quential tagging, viz. the class label or other features
from previously tagged tokens can be used when
classifying a new token. In our setup, the sentences
of an abstract are taken as the processing unit and
the collection of all sentences in an abstract is taken
as one sequence.
The features that are used to label a sentence are
the class labels of four previous sentences, the am-
bitags of the following two sentences, the lemma of
the dependency root of the sentence, the position of
the sentence in the abstract, the lemma of the root
of the previous sentence, and section information.
For each root lemma, all possible class labels, as ob-
served in the training data, are concatenated into one
ambitag. These tags are stored in a list. An am-
bitag for a sentence is retrieved by looking up the
root lemma in this list. The position of the sentence
is expressed by a number. Section information is ob-
tained by looking for a previous sentence that con-
sists of only one token in uppercase. Finally, basic
lemmatization is carried out by removing a final S.
All other settings of MBT are the default settings
and no feature optimization nor feature selection has
been carried out to prevent overfitting.
When a class label contains multiple labels, like
e.g. population and study design, these labels are
concatenated in an alphabetically sorted manner.
This method of working reduces the multilabel prob-
lem to a problem with many different labels, i.e. the
label powerset method of Tsoumakas et al. (2010).
The baseline system is exactly the same as
the memory-based system except that no machine
learner is included. The most frequent class label in
the training data, i.e. Outcome, is assigned to each
instance. The memory-based system enables us to
compare kLog against a basic machine learning ap-
proach, using few features. The majority baseline
</bodyText>
<footnote confidence="0.96443">
3http://ilk.uvt.nl/mbt [16 March 2012]
</footnote>
<bodyText confidence="0.999189">
system enables us to compare the memory-based
system and kLog against a baseline in which no in-
formation about the observations is used.
</bodyText>
<subsectionHeader confidence="0.999043">
4.3 Parametrization
</subsectionHeader>
<bodyText confidence="0.99998575862069">
From the kernel definition it might be clear that
the kLog hyperparameters, namely the distance d
and radius r, can have a strong influence on the
results. This requires a deliberate choice during
parametrization. From a linguistic perspective, the
use of unigrams and bigrams is justifiable, since
most phrases that reveal clues on the structure of the
abstract (e.g. evaluation measures, methodolody, fu-
ture work) can be expressed with single or pairs of
words. This is reflected by a distance and radius both
set to 1, which enables to take all possible combina-
tions of consecutive words into account and captures
the relational information attached to the word in fo-
cus, i.e. the current kernel point. This is confirmed
by cross-validation on other settings for the hyper-
parameters.
Since kLog generates a feature vector, only the
sequence information at word level is taken into ac-
count by kLog. Since we use a sequence labeling
approach as statistical learner, i.e. SVM-HMM, at
the level of the abstract this information is however
implicitly taken into account during learning. For
SVM-HMM, only the cost parameter C, which reg-
ulates the trade-off between the slack and the mag-
nitude of the weight-vector, and c, that specifies the
precision to which constraints are required to be sat-
isfied by the solution, were optimized by means of
cross-validation. For the other parameters, the de-
fault values were used.
</bodyText>
<subsectionHeader confidence="0.87254">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999976090909091">
Experiments are run on structured and unstructured
abstracts separately. On the NICTA-PIBOSO cor-
pus, we performed 10-fold cross-validation. Over
all folds, all labels, i.e. the parts of the multilabels,
are compared in a binary way between gold standard
and prediction. Summing all true positives, false
positives, and false negatives over all folds leads to
micro-averaged F-scores. This was done for two dif-
ferent settings. In one setting, CV/6-way, we com-
bined the labeling of the sentences with the identifi-
cation of irrelevant information, by adding the Other
</bodyText>
<page confidence="0.996266">
586
</page>
<bodyText confidence="0.9981495">
label as an extra class in the classification. The re-
sults are listed in Table 3.
</bodyText>
<note confidence="0.8813225">
CV/6-way MBT Kim et al. kLog
Label S U S U S U
</note>
<table confidence="0.999343230769231">
Background 71.0 61.3 81.84 68.46 86.19 76.90
Intervention 24.3 6.4 20.25 12.68 26.05 16.14
Outcome 87.9 70.4 92.32 72.94 92.99 77.69
Population 50.6 15.9 56.25 39.80 35.62 21.58
Study Design 45.9 13.10 43.95 4.40 45.5 6.67
Other 86.1 20.9 69.98 24.28 87.98 24.42
CV/5-way MBT Kim et al. kLog
Label S U S U S U
Background 87.1 64.9 87.92 70.67 91.45 80.06
Intervention 48.0 6.9 48.08 21.39 45.58 22.65
Outcome 95.8 75.9 96.03 80.51 96.21 83.04
Population 70.9 21.4 63.88 43.15 63.96 23.32
Study Design 50.0 7.4 47.44 8.6 48.08 4.50
</table>
<tableCaption confidence="0.975965">
Table 5: F-scores per class for 5-way classification over
structured (S) and unstructured (U) abstracts.
</tableCaption>
<note confidence="0.967614">
MBT Kim et al. kLog
</note>
<tableCaption confidence="0.993068">
Table 3: F-scores per class for structured (S) and unstruc-
tured (U) abstracts.
</tableCaption>
<bodyText confidence="0.99987712">
For this setting, kLog is able to outperform both
MBT and the system of Kim et al. (2011), for both
structured and unstructured abstracts on all classes
except Population. From Table 4, where the micro-
average F-scores over all classes and for all settings
are listed, it can be observed that kLog performs up
to 3.73% better than MBT over structured abstracts,
and 9.67% better over unstructured ones.
Although to a lesser extent for the structured ab-
stracts, the same pattern can be observed for the
CV/5-way setting, where we tried to classify the sen-
tences only, without considering the irrelevant ones.
The per-class results for this setting are shown in Ta-
ble 5. Now the scores for Population are comparable
to the other systems, due to which we assume these
sentences are similar in structure to the ones labeled
with Other.
For the external corpus, the results are listed in Ta-
ble 6. Although kLog performs comparably for the
individual classes Background and Intervention, its
overall performance is worse on the structured ab-
stracts. In case of the unstructured abstracts, kLog
performs better on the majority of the individual
classes and in overall performance for the 5-way set-
ting, and comparable for the 4-way setting.
</bodyText>
<table confidence="0.998331666666667">
Baseline MBT kLog
Method S U S U S U
CV/6-way 43.90 41.87 80.56 57.47 84.29 67.14
CV/5-way 61.79 46.66 86.96 64.37 87.67 72.95
Ext/5-way 66.18 6.76 36.34 11.56 20.50 14.00
Ext/4-way 30.11 27.23 67.29 55.96 50.40 50.50
</table>
<tableCaption confidence="0.990467333333333">
Table 4: Micro-averaged F1-score obtained for structured
(S) and unstructured (U) abstracts, both for 10-fold cross-
validation (CV) and on the external corpus (Ext).
</tableCaption>
<table confidence="0.9999549">
Label S U
Background 58.9 15.7
Intervention 21.5 13.8
Outcome 29.3 17.8
Population 10.7 17.8
Other 40.7 3.5
Background 90.4 67.5
Intervention 29 23.1
Outcome 74.1 74.6
Population 48.7 23.8
</table>
<tableCaption confidence="0.965259333333333">
Table 6: F-scores per class for 5-way and 4-way classifi-
cation over structured (S) and unstructured (U) abstracts
on the external corpus.
</tableCaption>
<bodyText confidence="0.999992">
As a general observation, it is important to note
that there is a high variability between the different
labels. Due to kLog’s ability to take the structured
input into account, we assume a correlation between
the sentence structure of the label and the predic-
tion quality. We intend to perform an extensive error
analysis, in order to detect patterns which may allow
us to incorporate additional declarative background
knowledge into our model.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999964">
We presented a statistical relational learning ap-
proach for the automatic identification of PICO cat-
egories in medical abstracts. To this extent, we used
kLog, a new framework for logical and relational
learning with kernels. Due to its graphical approach,
it is able to exploit the full relational representation,
that is often inherent in language structure. Since
contextual features are often essential and relations
are prevalent, the aim of this paper was to show
that statistical relational learning in general, and the
graph kernel-based approach of kLog in particular,
is specifically suited for problems in natural lan-
</bodyText>
<figure confidence="0.999861454545455">
S U
Ext/5-way
56.18 15.67
15.38 28.57
81.34 60.45
35.62 28.07
46.32 15.77
Ext/4-way
77.27 37.5
28.17 8.33
90.5 78.77
42.86 28.57
S U
58.30 29.10
40.00 34.30
27.80 24.10
5.60 28.60
11.40 8.50
65 68.6
28.1 32.3
72.4 72.7
11.8 15.4
</figure>
<page confidence="0.983646">
587
</page>
<bodyText confidence="0.997816666666667">
guage learning.
In future work, we intend to explore additional
ways to incorporate background knowledge in a
declarative way, since it renders the language learn-
ing problem more intuitive and gives a better under-
standing of feature contribution. Furthermore, we
also want to investigate the use of SRL approaches
for high-relational domains, and make a clear com-
parison with related techniques.
</bodyText>
<sectionHeader confidence="0.99912" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999860923076923">
This research is funded by the Research Foundation
Flanders (FWO project G.0478.10 - Statistical Re-
lational Learning of Natural Language), and made
possible through financial support from the KU Leu-
ven Research Fund (GOA project 2008/08 Proba-
bilistic Logic Learning), the University of Antwerp
(GOA project BIOGRAPH) and the Italian Min-
istry of Education, University, and Research (PRIN
project 2009LNP494 - Statistical Relational Learn-
ing: Algorithms and Applications). The authors
would like to thank Fabrizio Costa, Kurt De Grave
and the anonymous reviewers for their valuable
feedback.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880146666667">
Shashank Agarwal and Hong Yu. 2009. Automatically
Classifying Sentences in Full-text Biomedical Articles
into Introduction, Methods, Results and Discussion.
Bioinformatics, 25(23):3174–3180.
E. C. Armstrong. 1999. The Well-built Clinical Ques-
tion: the Key to Finding the Best Evidence Efficiently.
WMJ, 98(2):25–28.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): Integrating Biomed-
ical Terminology. Nucleic Acids Research,
32(Suppl.1):D267–D270.
Grace Y Chung. 2009. Sentence Retrieval for Abstracts
of Randomized Controlled Trials. BMC Medical In-
formatics and Decision Making, 9(10).
Fabrizio Costa and Kurt De Grave. 2010. Fast Neighbor-
hood Subgraph Pairwise Distance Kernel. Proceed-
ings of the 26th International Conference on Machine
Learning, 255–262, Haifa, Israel. Omnipress.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing.. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
P. Davis-Desmond and Diego Moll´a. 2012. Detection of
Evidence in Clinical Research Papers. Proceedings of
the Australasian Workshop On Health Informatics and
Knowledge Management (HIKM 2012), Melbourne,
Australia, 129:13–20. Australian Computer Society,
Inc.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2005. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion (JAMIA), 13:52–60.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge Based
and Statistical Techniques. Computational Linguis-
tics, 33(1):63–103.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic In-
ductive Logic Programming. In: Lecture Notes in
Computer Science (LNCS), 4911. Springer-Verlag,
Heidelberg, Germany.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. kLog - a Language
for Logical and Relational Learning with Kernels.
arXiv:1205.3981v2.
Hector Garcia-Molina, Jeff Ullman, and Jennifer Widom.
2008. Database Systems: The Complete Book. Pren-
tice Hall Press, Englewood Cliffs, NJ, USA.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press, Cambridge,
MA, USA.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report (UCSC-CRL-99-10), Uni-
versity of California at Santa Cruz.
Su Nam Kim, David Martinez, Lawrence Cavedon, and
Lars Yencken. 2011. Automatic Classification of Sen-
tences to Support Evidence Based Medicine. BMC
Bioinformatics, 12(2):S5.
Donald E. Knuth. 1965. On the Translation of Lan-
guages from Left to Right. Information and Control,
8: 607–639.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2006. Zone Analysis in Biology Articles as a Basis
for Information Extraction. International Journal of
Medical Informatics, 75(6):468–487.
Diego Moll´a and Mara Elena Santiago-Martinez. 2011.
Development of a Corpus for Evidence Medicine
Summarisation. Proceedings of the 2011 Australasian
Language Technology Workshop (ALTA 2011), Can-
berra, Australia, 86–94. Association for Computa-
tional Linguistics.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patri-
cia Rodriguez-Gianolli. 2003. Answering Clinical
</reference>
<page confidence="0.979998">
588
</page>
<reference confidence="0.999758229166667">
Questions with Role Identification. Proceedings of the
ACL, Workshop on Natural Language Processing in
Biomedicine. Sapporo, Japan, 73–80. Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov logic.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2008. Hon-
olulu, Hawaii, 650–659. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007. BioInfer: a Corpus for Information
Extraction in the Biomedical Domain. BMC Bioinfor-
matics, 8:50.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with Markov logic. Proceed-
ings of the Twelfth Conference on Computational Nat-
ural Language Learning (CoNLL 2008. Manchester,
United Kingdom, 193–197. Association for Computa-
tional Linguistics.
William Rosenberg and Anna Donald. 1995. Evidence
Based Medicine: an Approach to Clinical Problem
Solving. British Medical Journal, 310(6987):1122–
1126.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensembles. Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, Prague, Czech
Republic, 1044–1050. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support Vector
Machine Learning for Interdependent and Structured
Output Spaces. Proceedings of the twenty-first inter-
national conference on Machine learning (ICML), Al-
berta, Canada, 104–111. ACM.
Grigorios Tsoumakas and Ioannis Katakis and Ioannis P.
Vlahavas. Oded Maimon and Lior Rokach, editors.
2010. Mining Multi-label Data. In: Data Mining and
Knowledge Discovery Handbook, 2nd ed., 667–685.
Springer-Verlag, Heidelberg, Germany.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc De Raedt.
2012. Kernel-based Logical and Relational Learning
with kLog for Hedge Cue Detection. Proceedings of
the 21th International Conference on Inductive Logic
Programming, in press.
</reference>
<page confidence="0.998842">
589
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271837">
<title confidence="0.9990415">A Statistical Relational Learning Approach to Evidence Based Medicine Categories</title>
<author confidence="0.999968">Vincent Van_Roser</author>
<affiliation confidence="0.692216">Walter Luc De ♦ Department of Computer Science, Katholieke Universiteit Leuven, Belgium</affiliation>
<address confidence="0.918028">of Linguistics, Universiteit Antwerpen, Belgium</address>
<email confidence="0.808054">vincent.vanasch,</email>
<author confidence="0.795219">di_Sistemi e Informatica</author>
<author confidence="0.795219">Universit`a degli Studi di_Firenze</author>
<author confidence="0.795219">Italy</author>
<email confidence="0.996498">p-f@dsi.unifi.it</email>
<abstract confidence="0.997678823529412">medicine an approach whereby clinical decisions are supported by the best available findings gained from scientific research. This requires efficient access to such evidence. To this end, abstracts in evidence-based medicine can be labeled using a set of predefined medical categories, the so- This paper presents an approach to automatically annotate sentences in medical abstracts with these labels. Since both structural and sequential information are important for this classification task, we use a new language for statistical relational learning with kernels. Our results show a clear improvement with respect to state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shashank Agarwal</author>
<author>Hong Yu</author>
</authors>
<title>Automatically Classifying Sentences in Full-text Biomedical Articles into Introduction, Methods, Results and Discussion.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>23</issue>
<contexts>
<context position="10057" citStr="Agarwal and Yu, 2009" startWordPosition="1539" endWordPosition="1542">ly one rhetorical role per sentence. The F-scores obtained range from 0.93 to 0.98. Then another sentence level classification task is performed to automatically assign the labels Intervention, Participant and Outcome Measures to sentences in unstructured and structured abstracts without headings. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. Other work aimed at identifying rhetorical zones in biomedical articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employ</context>
</contexts>
<marker>Agarwal, Yu, 2009</marker>
<rawString>Shashank Agarwal and Hong Yu. 2009. Automatically Classifying Sentences in Full-text Biomedical Articles into Introduction, Methods, Results and Discussion. Bioinformatics, 25(23):3174–3180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Armstrong</author>
</authors>
<title>The Well-built Clinical Question: the Key to Finding the Best Evidence Efficiently.</title>
<date>1999</date>
<journal>WMJ,</journal>
<volume>98</volume>
<issue>2</issue>
<contexts>
<context position="5951" citStr="Armstrong, 1999" startWordPosition="891" endWordPosition="892">the evidence; And (4) implementing useful findings in clinical practice. Given the amounts of medical publications available in databases such as PubMed, automating step 2 is crucial to help doctors in their practice. Efforts in this direction from the NLP community have so far focused on corpus annotation (DemnerFushman and Lin, 2007; Kim et al., 2011), text categorization (Davis-Desmond and Moll´a, 2012), summarization (Moll´a and Santiago-Martinez, 2011), and question-anwering (Niuet al., 2003; DemnerFushman and Lin, 2007). The existing corpora are usually annotated with the PICO mnemonic (Armstrong, 1999) concepts, that are used to build queries when searching for literature for EBM purposes. The PICO concepts are: primary Problem (P) or population, main Intervention (I), main intervention Comparison (C), and Outcome of intervention (O). PICO helps determining what terms are important in a query and therefore it helps building the query, which is sent to the search repositories. Once the documents are found, they need to be read by a person who eliminates irrelevant documents. The first attempt to classify PICO concepts is presented in Demner-Fushman and Lin (2007), who apply a rule-based appr</context>
</contexts>
<marker>Armstrong, 1999</marker>
<rawString>E. C. Armstrong. 1999. The Well-built Clinical Question: the Key to Finding the Best Evidence Efficiently. WMJ, 98(2):25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The Unified Medical Language System (UMLS): Integrating Biomedical Terminology. Nucleic Acids Research,</title>
<date>2004</date>
<contexts>
<context position="13399" citStr="Bodenreider, 2004" startWordPosition="2091" endWordPosition="2092">phicalization construction generation Extensionalized database Graph (Fig. 4) Graph kernel (NSPDK) Kernel matrix/ feature vectors Statistical learner Figure 1: General kLog workflow. scribed step by step in the following paragraphs. Preprocessing The sentences have been preprocessed with a named entity tagger and a dependency parser. Named entity tagging has been performed with the BiogaphTA named entity module, which matches token sequences with entries in the UMLS database1. UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies (Bodenreider, 2004). The tagger matches sequences with a length of maximum 4 tokens. This covers 66.2% of the UMLS entries. By using UMLS, different token sequences referring to the same concept can be mapped to the same concept identifier (CID). The BiographTA named entity tagger has been evaluated on the BioInfer corpus (Pyysalo et al., 2007) obtaining a 72.02 F1 score. Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shiftreduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective pars</context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): Integrating Biomedical Terminology. Nucleic Acids Research, 32(Suppl.1):D267–D270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Y Chung</author>
</authors>
<title>Sentence Retrieval for Abstracts of Randomized Controlled Trials.</title>
<date>2009</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>9</volume>
<issue>10</issue>
<contexts>
<context position="8206" citStr="Chung (2009)" startWordPosition="1260" endWordPosition="1261">than one tag. An example of an annotated abstract from the corpus can be found in the supplementary material. The features used by the algorithm include features derived from the context, semantic relations, structure and sequencing of the text. The system is evaluated for 5-way and 6-way classification and results are provided apart from structured and unstructured abstracts. The F-scores for structured abstracts is 89.32% for 5-way classification and 80.88% for 6-way classification, whereas for unstructured abstracts it is 71.54% for 5-way classification and 64.66% for 6-way classification. Chung (2009) uses CRF to classify PICO concepts by combining them with general categories associated with rhetorical roles: Aim, Method, Results and Conclusion. Her system is tested on corpora of abstracts of randomized control trials. First structured abstracts with headings labeled with PICO 580 Background Material that informs and may place the current study in perspective, e.g. work that preceded the current; information about disease prevalence; etc. Population The group of individual persons, objects or items comprising the study’s sample, or from which the sample was taken for statistical measureme</context>
</contexts>
<marker>Chung, 2009</marker>
<rawString>Grace Y Chung. 2009. Sentence Retrieval for Abstracts of Randomized Controlled Trials. BMC Medical Informatics and Decision Making, 9(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Kurt De Grave</author>
</authors>
<title>Fast Neighborhood Subgraph Pairwise Distance Kernel.</title>
<date>2010</date>
<booktitle>Proceedings of the 26th International Conference on Machine Learning, 255–262,</booktitle>
<location>Haifa, Israel. Omnipress.</location>
<marker>Costa, De Grave, 2010</marker>
<rawString>Fabrizio Costa and Kurt De Grave. 2010. Fast Neighborhood Subgraph Pairwise Distance Kernel. Proceedings of the 26th International Conference on Machine Learning, 255–262, Haifa, Israel. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<date>2005</date>
<booktitle>Memory-Based Language Processing.. Studies in Natural Language Processing.</booktitle>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Walter Daelemans and Antal van den Bosch. 2005. Memory-Based Language Processing.. Studies in Natural Language Processing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Davis-Desmond</author>
<author>Diego Moll´a</author>
</authors>
<title>Detection of Evidence in Clinical Research Papers.</title>
<date>2012</date>
<booktitle>Proceedings of the Australasian Workshop On Health Informatics and Knowledge Management (HIKM 2012),</booktitle>
<publisher>Australian Computer Society, Inc.</publisher>
<location>Melbourne, Australia,</location>
<marker>Davis-Desmond, Moll´a, 2012</marker>
<rawString>P. Davis-Desmond and Diego Moll´a. 2012. Detection of Evidence in Clinical Research Papers. Proceedings of the Australasian Workshop On Health Informatics and Knowledge Management (HIKM 2012), Melbourne, Australia, 129:13–20. Australian Computer Society, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Barbara Few</author>
<author>Susan E Hauser</author>
<author>George Thoma</author>
</authors>
<title>Automatically Identifying Health Outcome Information in MEDLINE Records.</title>
<date>2005</date>
<journal>Journal of the American Medical Informatics Association (JAMIA),</journal>
<pages>13--52</pages>
<marker>Demner-Fushman, Few, Hauser, Thoma, 2005</marker>
<rawString>Dina Demner-Fushman, Barbara Few, Susan E. Hauser, and George Thoma. 2005. Automatically Identifying Health Outcome Information in MEDLINE Records. Journal of the American Medical Informatics Association (JAMIA), 13:52–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dina Demner-Fushman</author>
<author>Jimmy Lin</author>
</authors>
<date>2007</date>
<booktitle>Answering Clinical Questions with Knowledge Based and Statistical Techniques. Computational Linguistics,</booktitle>
<pages>33--1</pages>
<contexts>
<context position="6522" citStr="Demner-Fushman and Lin (2007)" startWordPosition="984" endWordPosition="987">usually annotated with the PICO mnemonic (Armstrong, 1999) concepts, that are used to build queries when searching for literature for EBM purposes. The PICO concepts are: primary Problem (P) or population, main Intervention (I), main intervention Comparison (C), and Outcome of intervention (O). PICO helps determining what terms are important in a query and therefore it helps building the query, which is sent to the search repositories. Once the documents are found, they need to be read by a person who eliminates irrelevant documents. The first attempt to classify PICO concepts is presented in Demner-Fushman and Lin (2007), who apply a rule-based approach to identify sentences where PICO concepts occur and a supervised approach to classify sentences that contain an Outcome. The features used by this classifier are ngrams, position, and semantic information from the parser used to process the data. The system is trained on 275 abstracts manually annotated. The accuracies reported range from 80% for Population, 86% for Problem, 80% for Intervention, and, from 64% to 95% for Outcome depending on the test set of abstracts. Kim et al. (2011) perform a similar classification task in two steps. First a classifier iden</context>
</contexts>
<marker>Demner-Fushman, Lin, 2007</marker>
<rawString>Dina Demner-Fushman and Jimmy Lin. 2007. Answering Clinical Questions with Knowledge Based and Statistical Techniques. Computational Linguistics, 33(1):63–103.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>Probabilistic Inductive Logic Programming. In: Lecture Notes in Computer Science (LNCS),</booktitle>
<pages>4911</pages>
<editor>Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors.</editor>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg, Germany.</location>
<marker>2008</marker>
<rawString>Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors. 2008. Probabilistic Inductive Logic Programming. In: Lecture Notes in Computer Science (LNCS), 4911. Springer-Verlag, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Frasconi</author>
<author>Fabrizio Costa</author>
<author>Luc De Raedt</author>
<author>Kurt De Grave</author>
</authors>
<title>kLog - a Language for Logical and Relational Learning with Kernels.</title>
<date>2012</date>
<tech>arXiv:1205.3981v2.</tech>
<marker>Frasconi, Costa, De Raedt, De Grave, 2012</marker>
<rawString>Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and Kurt De Grave. 2012. kLog - a Language for Logical and Relational Learning with Kernels. arXiv:1205.3981v2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Garcia-Molina</author>
<author>Jeff Ullman</author>
<author>Jennifer Widom</author>
</authors>
<title>Database Systems: The Complete Book.</title>
<date>2008</date>
<publisher>Prentice Hall Press,</publisher>
<location>Englewood Cliffs, NJ, USA.</location>
<contexts>
<context position="14527" citStr="Garcia-Molina et al., 2008" startWordPosition="2270" endWordPosition="2273">reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. This parser is a version of the KSDep dependency parser trained on the GENIA Treebank for parsing biomedical text. KSDep was evaluated in the CoNLL Shared Task 2007 obtaining a Labeled Attachment Score of 89.01% for the English dataset. GDEP outputs the lemmas, chunks, Genia named entities and dependency relations of the tokens in a sentence. This information can be represented as an Entity/Relationship (E/R) diagram, a modeling paradigm that is frequently used in database theory (Garcia-Molina et al., 2008). The E/R-model for the 1From UMLS only the MRCONSO.RRF and MRSTY.RRF files are used. problem under consideration is shown in Figure 2, which provides an abstract representation of the examples, i.e. medical abstracts in this case. We will show later how this abstract representation can be unrolled for each example, resulting in a graph; cf. also Figure 4 for our example sentence. This relational database representation will serve as the input for kLog. Figure 2: E/R-diagram modeling the sentence identification task. The entities are the words and sentences in the abstract. They are represente</context>
</contexts>
<marker>Garcia-Molina, Ullman, Widom, 2008</marker>
<rawString>Hector Garcia-Molina, Jeff Ullman, and Jennifer Widom. 2008. Database Systems: The Complete Book. Prentice Hall Press, Englewood Cliffs, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lise Getoor</author>
<author>Ben Taskar</author>
</authors>
<title>Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning).</title>
<date>2007</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10231" citStr="Getoor and Taskar, 2007" startWordPosition="1565" endWordPosition="1568"> labels Intervention, Participant and Outcome Measures to sentences in unstructured and structured abstracts without headings. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. Other work aimed at identifying rhetorical zones in biomedical articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programmi</context>
</contexts>
<marker>Getoor, Taskar, 2007</marker>
<rawString>Lise Getoor and Ben Taskar. 2007. Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning). The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report (UCSC-CRL-99-10),</tech>
<institution>University of California at Santa Cruz.</institution>
<contexts>
<context position="21501" citStr="Haussler, 1999" startWordPosition="3317" endWordPosition="3318">tion, and an undirected edge {e, r} if an entity e participates in relationship r. The obtained graphs can then be used in the next step for feature generation. This is done by means of a graph kernel n, which calculates the similarity between two graphicalized interpretations. Any graph kernel that allows fast computations on large graphs and has a flexible bias to enable heterogeneous features can in theory be applied. In the current implementation, an extension of the Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) (Costa and De Grave, 2010) is used. NSPDK is a decomposition kernel (Haussler, 1999), in which pairs of subgraphs are compared Figure 4: Graphicalization G, of interpretation z. to each other in order to calculate the similarity between two graphs. These subgraphs can be seen as circles in the graph, and are defined by three hyperparameters. First of all, there is the center of the subgraph, the kernel point, which can be any entity or relation in the graph. The entities and relations to be taken into account as kernel points are marked beforehand as a subset of the intensional and extensional domain relations. The radius r determines the size of the subgraphs and defines whi</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report (UCSC-CRL-99-10), University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>David Martinez</author>
<author>Lawrence Cavedon</author>
<author>Lars Yencken</author>
</authors>
<title>Automatic Classification of Sentences to Support Evidence Based Medicine.</title>
<date>2011</date>
<journal>BMC Bioinformatics,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="2080" citStr="Kim et al. (2011)" startWordPosition="287" endWordPosition="290">rch findings are systematically reviewed, appraised and used to improve the patient care, for which efficient access to such evidence is required. In order to facilitate the search process, medical documents are labeled using a set of predefined medical categories, the PICO criteria. PICO is an acronym for the mnemonic concepts that are used to construct queries when searching for scientific evidence in the EBM process. The need to automatize the annotation process has initiated research into automatic approaches to annotate sentences in medical documents with the PICO labels. As indicated by Kim et al. (2011), both the structural information of the words in the sentence, and that of the sentences in the document are important features for this task. Furthermore, sequential information can leverage the dependencies between different sentences in the text. Therefore we propose an approach using kLog (Frasconi et al., 2012) to tackle this problem. kLog is a new language for statistical relational learning with kernels, that is embedded in Prolog, and builds upon and links together concepts from database theory, logic programming and learning from interpretations. Learning from interpretations is a lo</context>
<context position="5690" citStr="Kim et al., 2011" startWordPosition="854" endWordPosition="857">earch findings as the basis for clinical decisions” (Rosenberg and Donald, 1995). The evidence-based process consists of four steps: (1) Formulating a question from a patient’s problem; (2) Searching the literature for relevant clinical articles; (3) Evaluating the evidence; And (4) implementing useful findings in clinical practice. Given the amounts of medical publications available in databases such as PubMed, automating step 2 is crucial to help doctors in their practice. Efforts in this direction from the NLP community have so far focused on corpus annotation (DemnerFushman and Lin, 2007; Kim et al., 2011), text categorization (Davis-Desmond and Moll´a, 2012), summarization (Moll´a and Santiago-Martinez, 2011), and question-anwering (Niuet al., 2003; DemnerFushman and Lin, 2007). The existing corpora are usually annotated with the PICO mnemonic (Armstrong, 1999) concepts, that are used to build queries when searching for literature for EBM purposes. The PICO concepts are: primary Problem (P) or population, main Intervention (I), main intervention Comparison (C), and Outcome of intervention (O). PICO helps determining what terms are important in a query and therefore it helps building the query,</context>
<context position="7046" citStr="Kim et al. (2011)" startWordPosition="1073" endWordPosition="1076">ts. The first attempt to classify PICO concepts is presented in Demner-Fushman and Lin (2007), who apply a rule-based approach to identify sentences where PICO concepts occur and a supervised approach to classify sentences that contain an Outcome. The features used by this classifier are ngrams, position, and semantic information from the parser used to process the data. The system is trained on 275 abstracts manually annotated. The accuracies reported range from 80% for Population, 86% for Problem, 80% for Intervention, and, from 64% to 95% for Outcome depending on the test set of abstracts. Kim et al. (2011) perform a similar classification task in two steps. First a classifier identifies the sentences that contain PICO concepts, and then another classifier assigns PICO tags to the sentences found to be relevant by the previous classifier. The system is based on a CRF algorithm and is trained on the NICTA-PIBOSO corpus. This dataset contains 1,000 medical abstracts manually annotated with an extension of the PICO tagset, for which the definitions are listed in Table 1. The annotation is performed at sentence level and one sentence may have more than one tag. An example of an annotated abstract fr</context>
<context position="9351" citStr="Kim et al. (2011)" startWordPosition="1436" endWordPosition="1439">y’s sample, or from which the sample was taken for statistical measurement Intervention The act of interfering with a condition to modify it or with a process to change its course (includes prevention) Outcome The sentence(s) that best summarizes the consequences of an intervention Study Design The type of study that is described in the abstract Other Any sentence not falling into one of the other categories and presumed to provide little help with clinical decision making, i.e. non-key or irrelevant sentences Table 1: Definitions of the semantic tags used as annotation categories (taken from Kim et al. (2011)). concepts are used. A sentence level classification task is performed, assigning only one rhetorical role per sentence. The F-scores obtained range from 0.93 to 0.98. Then another sentence level classification task is performed to automatically assign the labels Intervention, Participant and Outcome Measures to sentences in unstructured and structured abstracts without headings. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. Other work aimed at identifying rhetorical zones in biomedical articles. In this case areas of text are classified in terms</context>
<context position="25525" citStr="Kim et al., 2011" startWordPosition="4010" endWordPosition="4013">determined via cross-validation. For epsilon, the default value, 0.5, was kept, since this mainly only influences the running time and memory consumption during training. 2http://www.cs.cornell.edu/people/tj/ svm_light/svm_hmm.html All S U Nb. Abstracts 1000 376 624 Nb. Sentences 10379 4774 5605 - Background 2557 669 1888 - Intervention 690 313 377 - Outcome 4523 2240 2283 - Population 812 369 443 - Study Design 233 149 84 - Other 1564 1034 530 Table 2: Number of abstracts and sentences for Structured (S) and Unstructured (U) abstract sets, including number of sentences per class (taken from (Kim et al., 2011)). 4 Evaluation We evaluate the performance of kLog against a baseline system and a memory-based tagger (Daelemans and van den Bosch, 2005). The results are also compared against those from Kim et al. (2011), which is the state-of-the-art system for this task. 4.1 Datasets We perform our experiments on the NICTAPIBOSO dataset from Kim et al. (2011) (kindly provided by the authors). It contains 1,000 abstracts of which 500 were retrieved from MEDLINE by querying for diverse aspects in the traumatic brain injury and spinal cord injury domain. The dataset consists of two types of abstracts. If th</context>
<context position="26773" citStr="Kim et al. (2011)" startWordPosition="4219" endWordPosition="4222">dings (e.g. Background, Methodology, Results, etc.), it is considered to be structured. This information can be used as a feature in the model. The other abstracts are regarded unstructured. The definitions of the semantic tags used as annotations categories are a variation on the PICO tag set, with the addition of two additional categories (see Table 1 in Section 2). Each sentence can be annotated with multiple classes. This renders the task a multiclass multilabel classification problem. The statistics on this dataset can be found in Table 2. In order to apply the same evaluation setting as Kim et al. (2011), we used the dataset from DemnerFushman et al. (2005) as external dataset. It consists of 100 sentences of which 51 are structured. Because the semantic tag set used for annotation slightly differs from the one presented in Table 1, and to make our results comparable, we will use the same mapping as used in Kim et al. (2011). Kr*,d*(G, G&apos;) = � d* d=0 r* E r=0 585 4.2 Baseline and benchmarks We compare the kLog system to three other systems: a baseline system, a memory-based system, and the scores reported by Kim et al. (2011). The memory-based system that we use is based on the memory-based t</context>
<context position="32548" citStr="Kim et al. (2011)" startWordPosition="5195" endWordPosition="5198"> 4.40 45.5 6.67 Other 86.1 20.9 69.98 24.28 87.98 24.42 CV/5-way MBT Kim et al. kLog Label S U S U S U Background 87.1 64.9 87.92 70.67 91.45 80.06 Intervention 48.0 6.9 48.08 21.39 45.58 22.65 Outcome 95.8 75.9 96.03 80.51 96.21 83.04 Population 70.9 21.4 63.88 43.15 63.96 23.32 Study Design 50.0 7.4 47.44 8.6 48.08 4.50 Table 5: F-scores per class for 5-way classification over structured (S) and unstructured (U) abstracts. MBT Kim et al. kLog Table 3: F-scores per class for structured (S) and unstructured (U) abstracts. For this setting, kLog is able to outperform both MBT and the system of Kim et al. (2011), for both structured and unstructured abstracts on all classes except Population. From Table 4, where the microaverage F-scores over all classes and for all settings are listed, it can be observed that kLog performs up to 3.73% better than MBT over structured abstracts, and 9.67% better over unstructured ones. Although to a lesser extent for the structured abstracts, the same pattern can be observed for the CV/5-way setting, where we tried to classify the sentences only, without considering the irrelevant ones. The per-class results for this setting are shown in Table 5. Now the scores for Po</context>
</contexts>
<marker>Kim, Martinez, Cavedon, Yencken, 2011</marker>
<rawString>Su Nam Kim, David Martinez, Lawrence Cavedon, and Lars Yencken. 2011. Automatic Classification of Sentences to Support Evidence Based Medicine. BMC Bioinformatics, 12(2):S5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>On the Translation of Languages from Left to Right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<pages>607--639</pages>
<contexts>
<context position="13956" citStr="Knuth, 1965" startWordPosition="2182" endWordPosition="2183">families of biomedical vocabularies (Bodenreider, 2004). The tagger matches sequences with a length of maximum 4 tokens. This covers 66.2% of the UMLS entries. By using UMLS, different token sequences referring to the same concept can be mapped to the same concept identifier (CID). The BiographTA named entity tagger has been evaluated on the BioInfer corpus (Pyysalo et al., 2007) obtaining a 72.02 F1 score. Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shiftreduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. This parser is a version of the KSDep dependency parser trained on the GENIA Treebank for parsing biomedical text. KSDep was evaluated in the CoNLL Shared Task 2007 obtaining a Labeled Attachment Score of 89.01% for the English dataset. GDEP outputs the lemmas, chunks, Genia named entities and dependency relations of the tokens in a sentence. This information can be represented as an Entity/Relationship (E/R) diagram, a modeling paradigm that is frequently used in database theory (Garcia-Molina et al., 2008). The E/R-model for the 1From</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>Donald E. Knuth. 1965. On the Translation of Languages from Left to Right. Information and Control, 8: 607–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mizuta</author>
<author>A Korhonen</author>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Zone Analysis in Biology Articles as a Basis for Information Extraction.</title>
<date>2006</date>
<journal>International Journal of Medical Informatics,</journal>
<volume>75</volume>
<issue>6</issue>
<contexts>
<context position="10136" citStr="Mizuta et al., 2006" startWordPosition="1551" endWordPosition="1554">98. Then another sentence level classification task is performed to automatically assign the labels Intervention, Participant and Outcome Measures to sentences in unstructured and structured abstracts without headings. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. Other work aimed at identifying rhetorical zones in biomedical articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it emplo</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2006</marker>
<rawString>Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006. Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics, 75(6):468–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Moll´a</author>
<author>Mara Elena Santiago-Martinez</author>
</authors>
<title>Development of a Corpus for Evidence Medicine Summarisation.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Australasian Language Technology Workshop (ALTA 2011),</booktitle>
<location>Canberra, Australia,</location>
<marker>Moll´a, Santiago-Martinez, 2011</marker>
<rawString>Diego Moll´a and Mara Elena Santiago-Martinez. 2011. Development of a Corpus for Evidence Medicine Summarisation. Proceedings of the 2011 Australasian Language Technology Workshop (ALTA 2011), Canberra, Australia, 86–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Niu</author>
<author>Graeme Hirst</author>
<author>Gregory McArthur</author>
<author>Patricia Rodriguez-Gianolli</author>
</authors>
<title>Answering Clinical Questions with Role Identification.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL, Workshop on Natural Language Processing in Biomedicine. Sapporo, Japan, 73–80. Association for Computational Linguistics.</booktitle>
<marker>Niu, Hirst, McArthur, Rodriguez-Gianolli, 2003</marker>
<rawString>Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-Gianolli. 2003. Answering Clinical Questions with Role Identification. Proceedings of the ACL, Workshop on Natural Language Processing in Biomedicine. Sapporo, Japan, 73–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008.</booktitle>
<pages>650--659</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="10568" citStr="Poon and Domingos, 2008" startWordPosition="1620" endWordPosition="1623">n terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represented as pairs z = (x, y), in which x represents the inputs </context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008. Honolulu, Hawaii, 650–659. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni J¨arvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>BioInfer: a Corpus for Information Extraction in the Biomedical Domain.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio Salakoski. 2007. BioInfer: a Corpus for Information Extraction in the Biomedical Domain. BMC Bioinformatics, 8:50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Ivan Meza-Ruiz</author>
</authors>
<title>Collective semantic role labelling with Markov logic.</title>
<date>2008</date>
<booktitle>Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<location>Manchester, United Kingdom,</location>
<contexts>
<context position="10515" citStr="Riedel and Meza-Ruiz, 2008" startWordPosition="1613" endWordPosition="1616">al articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represented a</context>
</contexts>
<marker>Riedel, Meza-Ruiz, 2008</marker>
<rawString>Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective semantic role labelling with Markov logic. Proceedings of the Twelfth Conference on Computational Natural Language Learning (CoNLL 2008. Manchester, United Kingdom, 193–197. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Rosenberg</author>
<author>Anna Donald</author>
</authors>
<title>Evidence Based Medicine: an Approach to Clinical Problem Solving.</title>
<date>1995</date>
<journal>British Medical Journal,</journal>
<volume>310</volume>
<issue>6987</issue>
<pages>1126</pages>
<contexts>
<context position="5153" citStr="Rosenberg and Donald, 1995" startWordPosition="771" endWordPosition="774">hich we present results that indicate a clear improvement on the state-of-the-art. The remainder of this paper is organized as follows. In Section 2, we outline earlier work that is related to the research presented here. Section 3 describes the methodology of our method. We present a thorough evaluation of our method in Section 4. The last section draws conclusions and presents some ideas for future work. 2 Related Work EBM is an approach to clinical problem-solving based on “systematically finding, appraising, and using contemporaneous research findings as the basis for clinical decisions” (Rosenberg and Donald, 1995). The evidence-based process consists of four steps: (1) Formulating a question from a patient’s problem; (2) Searching the literature for relevant clinical articles; (3) Evaluating the evidence; And (4) implementing useful findings in clinical practice. Given the amounts of medical publications available in databases such as PubMed, automating step 2 is crucial to help doctors in their practice. Efforts in this direction from the NLP community have so far focused on corpus annotation (DemnerFushman and Lin, 2007; Kim et al., 2011), text categorization (Davis-Desmond and Moll´a, 2012), summari</context>
</contexts>
<marker>Rosenberg, Donald, 1995</marker>
<rawString>William Rosenberg and Anna Donald. 1995. Evidence Based Medicine: an Approach to Clinical Problem Solving. British Medical Journal, 310(6987):1122– 1126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles.</title>
<date>2007</date>
<booktitle>Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>1044--1050</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13855" citStr="Sagae and Tsujii, 2007" startWordPosition="2164" endWordPosition="2168">entries in the UMLS database1. UMLS integrates over 2 million names for some 900,000 concepts from more than 60 families of biomedical vocabularies (Bodenreider, 2004). The tagger matches sequences with a length of maximum 4 tokens. This covers 66.2% of the UMLS entries. By using UMLS, different token sequences referring to the same concept can be mapped to the same concept identifier (CID). The BiographTA named entity tagger has been evaluated on the BioInfer corpus (Pyysalo et al., 2007) obtaining a 72.02 F1 score. Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shiftreduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. This parser is a version of the KSDep dependency parser trained on the GENIA Treebank for parsing biomedical text. KSDep was evaluated in the CoNLL Shared Task 2007 obtaining a Labeled Attachment Score of 89.01% for the English dataset. GDEP outputs the lemmas, chunks, Genia named entities and dependency relations of the tokens in a sentence. This information can be represented as an Entity/Relationship (E/R) diagram, a modeling paradigm</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles. Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, Prague, Czech Republic, 1044–1050. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support Vector Machine Learning for Interdependent and Structured Output Spaces.</title>
<date>2004</date>
<booktitle>Proceedings of the twenty-first international conference on Machine learning (ICML),</booktitle>
<publisher>ACM.</publisher>
<location>Alberta, Canada,</location>
<contexts>
<context position="4053" citStr="Tsochantaridis et al., 2004" startWordPosition="594" endWordPosition="597">he words in the sentence, now also the relations between the sentences in the document become important. In the proposed approach, we first generate a feature space with kLog that captures the intrasentential properties and relations. Hereafter, these features serve as input for a structured output support vector machine that can handle sequence tagging 579 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 579–589, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics (Tsochantaridis et al., 2004), in order to take the intersentential features into account. Second, since there are more than two categories, and each sentence can have multiple labels, the problem is now a multiclass multilabel classification task. The main contribution of this paper is that we show that kLog’s relational nature and its ability to declaratively specify and use background knowledge is beneficial for natural language learning problems. This is shown on the NICTA-PIBOSO corpus, for which we present results that indicate a clear improvement on the state-of-the-art. The remainder of this paper is organized as </context>
<context position="24131" citStr="Tsochantaridis et al., 2004" startWordPosition="3776" endWordPosition="3779">xtended, high-dimensional feature space, which serves as input for the statistical learner in the next step. Learning The constructed feature space contains one feature vector per sentence. This implies that the sequence information of the sentences at the document level is not taken into account yet. Since the order of the sentences in the abstract is a valuable feature for this prediction problem, a learner that reflects this in the learning process is needed, although in principle any statistical learner can be used on the feature space constructed by kLog. Therefore we opted for SVM-HMM2 (Tsochantaridis et al., 2004), which is an implementation of structural support vector machines for sequence tagging. In contrast to a conventional Hidden Markov Model, SVM-HMM is able to take these entire feature vectors as observations, and not just atomic tokens. In our case, the instances to be tagged are formed by the sentences for which feature vectors were created in the previous step. The qid is a special feature that is used in the structured SVM to restrict the generation of constraints. Since every document needs to be represented as a sequence of sentences, in SVM-HMM, the qid’s are used to obtain the document</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support Vector Machine Learning for Interdependent and Structured Output Spaces. Proceedings of the twenty-first international conference on Machine learning (ICML), Alberta, Canada, 104–111. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Ioannis Katakis</author>
<author>Ioannis P Vlahavas</author>
</authors>
<booktitle>Oded Maimon and Lior Rokach, editors. 2010. Mining Multi-label Data. In: Data Mining and Knowledge Discovery Handbook, 2nd ed.,</booktitle>
<pages>667--685</pages>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg, Germany.</location>
<marker>Tsoumakas, Katakis, Vlahavas, </marker>
<rawString>Grigorios Tsoumakas and Ioannis Katakis and Ioannis P. Vlahavas. Oded Maimon and Lior Rokach, editors. 2010. Mining Multi-label Data. In: Data Mining and Knowledge Discovery Handbook, 2nd ed., 667–685. Springer-Verlag, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Verbeke</author>
<author>Paolo Frasconi</author>
<author>Vincent Van Asch</author>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
<author>Luc De Raedt</author>
</authors>
<title>Kernel-based Logical and Relational Learning with kLog for Hedge Cue Detection.</title>
<date>2012</date>
<booktitle>Proceedings of the 21th International Conference on Inductive Logic Programming,</booktitle>
<note>in press.</note>
<marker>Verbeke, Frasconi, Van Asch, Morante, Daelemans, De Raedt, 2012</marker>
<rawString>Mathias Verbeke, Paolo Frasconi, Vincent Van Asch, Roser Morante, Walter Daelemans, and Luc De Raedt. 2012. Kernel-based Logical and Relational Learning with kLog for Hedge Cue Detection. Proceedings of the 21th International Conference on Inductive Logic Programming, in press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
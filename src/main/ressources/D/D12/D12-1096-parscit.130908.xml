<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000150">
<title confidence="0.9995745">
Parser Showdown at the Wall Street Corral:
An Empirical Investigation of Error Types in Parser Output
</title>
<author confidence="0.998864">
Jonathan K. Kummerfeld† David Hall† James R. Curran‡ Dan Klein†
</author>
<affiliation confidence="0.873102666666667">
†Computer Science Division ‡-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
</affiliation>
<email confidence="0.99769">
{jkk,dlwh,klein}@cs.berkeley.edu james@it.usyd.edu.au
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791923076923">
Constituency parser performance is primarily
interpreted through a single metric, F-score
on WSJ section 23, that conveys no linguis-
tic information regarding the remaining errors.
We classify errors within a set of linguisti-
cally meaningful types using tree transforma-
tions that repair groups of errors together. We
use this analysis to answer a range of ques-
tions about parser behaviour, including what
linguistic constructions are difficult for state-
of-the-art parsers, what types of errors are be-
ing resolved by rerankers, and what types are
introduced when parsing out-of-domain text.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941339622642">
Parsing has been a major area of research within
computational linguistics for decades, and con-
stituent parser F-scores on WSJ section 23 have ex-
ceeded 90% (Petrov and Klein, 2007), and 92%
when using self-training and reranking (McClosky
et al., 2006; Charniak and Johnson, 2005). While
these results give a useful measure of overall per-
formance, they provide no information about the na-
ture, or relative importance, of the remaining errors.
Broad investigations of parser errors beyond the
PARSEVAL metric (Abney et al., 1991) have either
focused on specific parsers, e.g. Collins (2003), or
have involved conversion to dependencies (Carroll
et al., 1998; King et al., 2003). In all of these cases,
the analysis has not taken into consideration how a
set of errors can have a common cause, e.g. a single
mis-attachment can create multiple node errors.
We propose a new method of error classifica-
tion using tree transformations. Errors in the parse
tree are repaired using subtree movement, node cre-
ation, and node deletion. Each step in the process is
then associated with a linguistically meaningful er-
ror type, based on factors such as the node that is
moved, its siblings, and parents.
Using our method we analyse the output of thir-
teen constituency parsers on newswire. Some of
the frequent error types that we identify are widely
recognised as challenging, such as prepositional
phrase (PP) attachment. However, other significant
types have not received as much attention, such as
clause attachment and modifier attachment.
Our method also enables us to investigate where
reranking and self-training improve parsing. Pre-
viously, these developments were analysed only in
terms of their impact on F-score. Similarly, the chal-
lenge of out-of-domain parsing has only been ex-
pressed in terms of this single objective. We are able
to decompose the drop in performance and show that
a disproportionate number of the extra errors are due
to coordination and clause attachment.
This work presents a comprehensive investigation
of parser behaviour in terms of linguistically mean-
ingful errors. By applying our method to multiple
parsers and domains we are able to answer questions
about parser behaviour that were previously only ap-
proachable through approximate measures, such as
counts of node errors. We show which errors have
been reduced over the past fifteen years of parsing
research; where rerankers are making their gains and
where they are not exploiting the full potential of k-
best lists; and what types of errors arise when mov-
ing out-of-domain. We have released our system1 to
enable future work to apply our methodology.
</bodyText>
<footnote confidence="0.982986">
1http://code.google.com/p/berkeley-parser-analyser/
</footnote>
<page confidence="0.910452">
1048
</page>
<note confidence="0.940677">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1048–1059, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.967402" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999662440677966">
Most attempts to understand the behaviour of con-
stituency parsers have focused on overall evaluation
metrics. The three main methods are intrinsic eval-
uation with PARSEVAL, evaluation on dependencies
extracted from the constituency parse, and evalua-
tion on downstream tasks that rely on parsing.
Intrinsic evaluation with PARSEVAL, which calcu-
lates precision and recall over labeled tree nodes, is
a useful indicator of overall performance, but does
not pinpoint which structures the parser has most
difficulty with. Even when the breakdown for par-
ticular node types is presented (e.g. Collins, 2003),
the interaction between node errors is not taken into
account. For example, a VP node could be missing
because of incorrect PP attachment, a coordination
error, or a unary production mistake. There has been
some work that addresses these issues by analysing
the output of constituency parsers on linguistically
motivated error types, but only by hand on sets of
around 100 sentences (Hara et al., 2007; Yu et al.,
2011). By automatically classifying parse errors we
are able to consider the output of multiple parsers on
thousands of sentences.
The second major parser evaluation method in-
volves extraction of grammatical relations (King et
al., 2003; Briscoe and Carroll, 2006) or dependen-
cies (Lin, 1998; Briscoe et al., 2002). These met-
rics have been argued to be more informative and
generally applicable (Carroll et al., 1998), and have
the advantage that the breakdown over dependency
types is more informative than over node types.
There have been comparisons of multiple parsers
(Foster and van Genabith, 2008; Nivre et al., 2010;
Cer et al., 2010), as well as work on finding rela-
tions between errors (Hara et al., 2009), and break-
ing down errors by a range of factors (McDonald and
Nivre, 2007). However, one challenge is that results
for constituency parsers are strongly influenced by
the dependency scheme being used and how easy it
is to extract the dependencies from a given parser’s
output (Clark and Hockenmaier, 2002). Our ap-
proach does not have this disadvantage, as we anal-
yse parser output directly.
The third major approach involves extrinsic eval-
uation, where the parser’s output is used in a down-
stream task, such as machine translation (Quirk
and Corston-Oliver, 2006), information extraction
(Miyao et al., 2008), textual entailment (Yuret et
al., 2010), or semantic dependencies (Dridan and
Oepen, 2011). While some of these approaches give
a better sense of the impact of parse errors, they re-
quire integration into a larger system, making it less
clear where a given error originates.
The work we present here differs from existing
approaches by directly and automatically classifying
errors into meaningful types. This enables the first
very broad, yet detailed, study of parser behaviour,
evaluating the output of thirteen parsers over thou-
sands of sentences.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="method">
3 Parsers
</sectionHeader>
<bodyText confidence="0.952807965517241">
Our evaluation is over a wide range of PTB con-
stituency parsers and their variants from the past fif-
teen years. For all parsers we used the publicly avail-
able version, with the standard parameter settings.
Berkeley (Petrov et al., 2006; Petrov and Klein,
2007). An unlexicalised parser with a grammar
constructed with automatic state splitting.
Bikel (2004) implementation of Collins (1997).
BUBS (Dunlop et al., 2011; Bodenstab et al.,
2011). A ‘grammar-agnostic constituent
parser,’ which uses a Berkeley Parser grammar,
but parses with various pruning techniques to
improve speed, at the cost of accuracy.
Charniak (2000). A generative parser with a max-
imum entropy-inspired model. We also use the
reranker (Charniak and Johnson, 2005), and the
self-trained model (McClosky et al., 2006).
Collins (1997). A generative lexicalised parser,
with three models, a base model, a model that
uses subcategorisation frames for head words,
and a model that takes into account traces.
SSI (Henderson, 2003; Henderson, 2004). A sta-
tistical left-corner parser, with probabilities es-
timated by a neural network.
Stanford (Klein and Manning, 2003a; Klein and
Manning, 2003b). We consider both the un-
lexicalised PCFG parser (-U) and the factored
parser (-F), which combines the PCFG parser
with a lexicalised dependency parser.
</bodyText>
<page confidence="0.991146">
1049
</page>
<table confidence="0.987996125">
System F P R Exact Speed
ENHANCED TRAINING / SYSTEMS
Charniak-SR 92.07 92.44 91.70 44.87 1.8
Charniak-R 91.41 91.78 91.04 44.04 1.8
Charniak-S 91.02 91.16 90.89 40.77 1.8
STANDARD PARSERS
Berkeley 90.06 90.30 89.81 36.59 4.2
Charniak 89.71 89.88 89.55 37.25 1.8
SSN 89.42 89.96 88.89 32.74 1.8
BUBS 88.50 88.57 88.43 31.62 27.6
Bikel 88.16 88.23 88.10 32.33 0.8
Collins-3 87.66 87.82 87.50 32.22 2.0
Collins-2 87.62 87.77 87.48 32.51 2.2
Collins-1 87.09 87.29 86.90 30.35 3.3
Stanford-L 86.42 86.35 86.49 27.65 0.7
Stanford-U 85.78 86.48 85.09 28.35 2.7
</table>
<tableCaption confidence="0.563371">
Table 1: PARSEVAL results on WSJ section 23 for the
parsers we consider. The columns are F-score, precision,
recall, exact sentence match, and speed (sents/sec). Cov-
erage was left out as it was above 99.8% for all parsers.
In the ENHANCED TRAINING / SYSTEMS section we in-
clude the Charniak parser with reranking (R), with a self-
trained model (S), and both (SR).
</tableCaption>
<bodyText confidence="0.99589">
Table 1 shows the standard performance metrics,
measured on section 23 of the WSJ, using all sen-
tences. Speeds were measured using a Quad-Core
Xeon CPU (2.33GHz 4MB L2 cache) with 16GB
of RAM. These results clearly show the variation in
parsing performance, but they do not show which
constructions are the source of those variations.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="method">
4 Error Classification
</sectionHeader>
<bodyText confidence="0.9998719375">
While the statistics in Table 1 give a sense of over-
all parser performance they do not provide linguisti-
cally meaningful intuition for the source of remain-
ing errors. Breaking down the remaining errors by
node type is not particularly informative, as a sin-
gle attachment error can cause multiple node errors,
many of which are for unrelated node types. For
example, in Figure 1 there is a PP attachment error
that causes seven bracket errors (extra S, NP, PP, and
NP, missing S, NP, and PP). Determining that these
correspond to a PP attachment error from just the la-
bels of the missing and extra nodes is difficult. In
contrast, the approach we describe below takes into
consideration the relations between errors, grouping
them into linguistically meaningful sets.
We classify node errors in two phases. First, we
</bodyText>
<figure confidence="0.995906">
S
(a) Parser output
Applied
(b) Gold tree
</figure>
<figureCaption confidence="0.99826375">
Figure 1: Grouping errors by node type is of limited use-
fulness. In this figure and those that follow the top tree
is the incorrect parse and the bottom tree is the correct
parse. Bold, boxed nodes are either extra (marked in the
incorrect tree) or missing (marked in the correct tree).
This is an example of PP Attachment (in 1986 is too
low), but that is not at all clear from the set of incorrect
nodes (extra S, NP, PP, and NP, missing S, NP, and PP).
</figureCaption>
<bodyText confidence="0.999842">
find a set of tree transformations that convert the out-
put tree into the gold tree. Second, the transforma-
tion are classified into error types such as PP attach-
ment and coordination. Pseudocode for our method
is shown in Algorithm 1. The tree transformation
stage corresponds to the main loop, while the sec-
ond stage corresponds to the final loop.
</bodyText>
<subsectionHeader confidence="0.995577">
4.1 Tree Transformation
</subsectionHeader>
<bodyText confidence="0.99979025">
The core of our transformation process is a set of op-
erations that move subtrees, create nodes, and delete
nodes. Searching for the shortest path to transform
one tree into another is prohibitively slow.2 We find
</bodyText>
<footnote confidence="0.997823">
2We implemented various search procedures and found sim-
ilar results on the sentences that could be processed in a reason-
</footnote>
<figure confidence="0.974501123076923">
NP
PRP
He was
VP
VBD
VP
VBN
named
PP
NP
chief executive officer
IN
of
NP
NNP
Applied
NP
PP
in 1986
S
NP
S
NP
PRP
He
VBD
was
named
VBN
chief executive officer
VP
NP
VP
NP
S
IN
of
PP
NP
NNP
in 1986
PP
1050
Algorithm 1 Tree transformation error classification
U = initial set of node errors
Sort U by the depth of the error in the tree, deepest first
G=0
repeat
for all errors e E U do
if e fits an environment template t then
g = new error group
Correct e as specified by t
for all errors f that t corrects do
Remove f from U
Insert f into g
end for
Add g to G
end if
end for
until unable to correct any further errors
for all remaining errors e E U do
Insert a group into G containing e
end for
for all groups g E G do
Classify g based on properties of the group
</figure>
<subsectionHeader confidence="0.894933">
end for
</subsectionHeader>
<bodyText confidence="0.931833163265306">
a path by applying a greedy bottom–up approach,
iterating through the errors in order of tree depth.
We match each error with a template based on
nearby tree structure and errors. For example, in
Figure 1 there are four extra nodes that all cover
spans ending at Applied in 1986: S, NP, PP, NP.
There are also three missing nodes with spans end-
ing between Applied and in: PP, NP, and S. Figure 2
depicts these errors as spans, showing that this case
fits three criteria: (1) there are a set of extra spans all
ending at the same point, (2) there are a set of miss-
ing spans all ending at the same point, and (3) the ex-
tra spans cross the missing spans, extending beyond
their end-point. This indicates that the node start-
ing after Applied is attaching too low and should be
moved up, outside all of the extra nodes. Together,
the criteria and transformation form a template.
Once a suitable template is identified we correct
the error by moving subtrees, adding nodes and re-
moving nodes. In the example this is done by mov-
ing the node spanning in 1986 up in the tree until it
is outside of all the extra spans. Since moving the PP
leaves a unary production from an NP to an NP, we
also collapse that level. In total this corrects seven
able amount of time.
named chief executive officer of Applied in 1986
Figure 2: Templates are defined in terms of extra and
missing spans, shown here with unbroken lines above and
dashed lines below, respectively. This is an example of a
set of extra spans that cross a set of missing spans (which
in both cases all end at the same position). If the last two
words are moved, two of the extra spans will match the
two missing spans. The other extra span is deleted during
the move as it creates an NP→NP unary production.
errors, as there are three cases in which an extra node
is present that matches a missing node once the PP
is moved. All of these errors are placed in a single
group and information about the nearby tree struc-
ture before and after the transformation is recorded.
We continue to make passes through the list until
no errors are corrected on a pass. For each remaining
node error an individual error group is created.
The templates were constructed by hand based on
manual analysis of parser output. They cover a range
of combinations of extra and missing spans, with
further variation for whether crossing is occurring
and if so whether the crossing bracket starts or ends
in the middle of the correct bracket. Errors that do
not match any of our templates are left uncorrected.
</bodyText>
<subsectionHeader confidence="0.986096">
4.2 Transformation Classification
</subsectionHeader>
<bodyText confidence="0.99334725">
We began with a large set of node errors, in the first
stage they were placed into groups, one group per
tree transformation used to get from the test tree to
the gold tree. Next we classify each group as one of
the error types below.
PP Attachment Any case in which the transforma-
tion involved moving a Prepositional Phrase, or
the incorrect bracket is over a PP, e.g.
He was (VP named chief executive officer of
(NP Applied (PP in 1986)))
where (PP in 1986) should modify the entire
VP, rather than just Applied.
</bodyText>
<subsectionHeader confidence="0.456702">
IP Attachment Several cases in which NPs had to
</subsectionHeader>
<bodyText confidence="0.9996642">
. be moved, particularly for mistakes in appos-
itive constructions and incorrect attachments
within a verb phrase, e.g.
The bonds (VP go (PP on sale (NP Oct. 19)))
where Oct. 19 should be an argument of go.
</bodyText>
<page confidence="0.964178">
1051
</page>
<figure confidence="0.997897816901409">
(a) Parser output
NP
VP
VBZ
S
SBAR
TO
VP
1052
another
ad
new
(a) Parser output
the RTC to ...
unless the agency ...
NN
ad appearing
PP
VB
think
about it
VBD
had
S
VP
TO
VP
NP
VB
restrict
the RTC to ...
a breakdown
of major market activity
think
about it
ahead of time
VB
to
VP
VBD
NP
NP
NN
today
S
VBZ
intends
wrote
NP
VBG
appearing
DT
NN
JJ
VP
VP
TO
VP
to
VB
restrict
NP
SBAR
VP
another
new
intends
unless the agency...
VP
today
(b) Gold tree
</figure>
<figureCaption confidence="0.994887666666667">
Figure 3: NP Attachment: today is too high, it should
be the argument of appearing, rather than wrote. This
causes three node errors (extra NP, missing NP and VP).
</figureCaption>
<figure confidence="0.999634125">
VP
S
VBD
ADVP
ahead of time
(a) Parser output
VP
(b) Gold tree
</figure>
<figureCaption confidence="0.986749">
Figure 5: Clause Attachment: unless the agency re-
ceives specific congressional authorization is attaching
too low. This causes six node errors (extra S, VP, and
VP, missing S, VP and VP).
</figureCaption>
<figure confidence="0.999263">
SINV
to
to
ADVP
(b) Gold tree
</figure>
<figureCaption confidence="0.892393333333333">
Figure 4: Modifier Attachment: ahead of time is too
high, it should modify think, not had. This causes six
node errors (extra S, VP, and VP, missing S, VP, and VP).
</figureCaption>
<bodyText confidence="0.925200222222222">
Modifier Attachment Cases involving incorrectly
placed adjectives and adverbs, including errors
corrected by subtree movement and errors re-
quiring only creation of a node, e.g.
(NP (ADVP even more) severe setbacks)
where there should be an extra ADVP node
over even more severe.
Clause Attachment Any group that involves move-
ment of some form of S node.
</bodyText>
<figure confidence="0.997297">
PP
VBD
wrote
NP
VBG
NN
JJ
DT
VP
NP
had
VP
VP
TO
SINV
(b) Gold tree
SINV
S-ADV
VBZ
NP-SBJ-1
(c) Gold tree with traces and function tags
</figure>
<figureCaption confidence="0.996208">
Figure 6: Two Unary errors, a missing S and a missing
NP. The third tree is the PTB tree before traces and func-
tion tags are removed. Note that the missing NP is over
another NP, a production that does occur widely in the
treebank, particularly over the word it.
</figureCaption>
<figure confidence="0.999895661016949">
a breakdown
of major market activity
VP
VBG
Following
VBZ
is
NP
NP
PP
(a) Parser output
S
VBZ
NP
is
VP
NP
PP
VBG
NP-SBJ
-NONE-
*-1
Following
a breakdown
NP
of major market activity
PP
is
VP
VBG
Following
NP
NP
NP NP
State
Baker
NP
PP
Mannesmann AG
and
Dresdner AG’s 10% decline
A 16% drop
IN
for
NP
CC
NP
NP
PP
NNP
IN
of
Secretary
NP
NNP
NNP
(a) Parser output
NP
(b) Gold tree
</figure>
<figureCaption confidence="0.999336666666667">
Figure 7: Coordination: and Dresdner AG’s 10% de-
cline is too low. This causes four node errors (extra PP
and NP, missing NP and PP).
</figureCaption>
<bodyText confidence="0.999444464285714">
Unary Mistakes involving unary productions that
are not linked to a nearby error such as a match-
ing extra or missing node. We do not include a
breakdown by unary type, though we did find
that clause labeling (S, SINV, etc) accounted
for a large proportion of the errors.
Coordination Cases in which a conjunction is an
immediate sibling of the nodes being moved, or
is the leftmost or rightmost node being moved.
NP Internal Structure While most NP structure is
not annotated in the PTB, there is some use of
ADJP, NX, NAC and QP nodes. We form a
single group for each NP that has one or more
errors involving these types of nodes.
Different label In many cases a node is present in
the tree that spans the correct set of words, but
has the wrong label, in which case we group the
two node errors, (one extra, one missing), as a
single error.
Single word phrase A range of node errors that
span a single word, with checks to ensure this
is not linked to another error (e.g. one part of a
set of internal noun phrase errors).
Other There is a long tail of other errors. Some
could be placed within the categories above,
but would require far more specific rules.
For many of these error types it would be diffi-
cult to extract a meaningful understanding from only
</bodyText>
<figure confidence="0.999433">
(a) Parser output
NP
of NNP
State
(b) Gold tree
</figure>
<figureCaption confidence="0.9988205">
Figure 8: NP Internal Structure: Baker is too low, caus-
ing four errors (extra PP and NP, missing PP and NP).
</figureCaption>
<bodyText confidence="0.999028714285714">
the list of node errors involved. Even for error types
that can be measured by counting node errors or rule
production errors, our approach has the advantage
that we identify groups of errors with a single cause.
For example, a missing unary production may corre-
spond to an extra bracket that contains a subtree that
attached incorrectly.
</bodyText>
<subsectionHeader confidence="0.99156">
4.3 Methodology
</subsectionHeader>
<bodyText confidence="0.999929538461539">
We used sections 00 and 24 as development data
while constructing the tree transformation and error
group classification methods. All of our examples
in text come from these sections as well, but for all
tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is
the only section we are sure was not used in the de-
velopment of any of the parsers, either for tuning or
feature development. Our evaluation is entirely fo-
cused on the errors of the parsers, so unless there is
a particular construction that is unusually prevalent
in section 23, we are not revealing any information
about the test set that could bias future work.
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.997266142857143">
Our system enables us to answer questions about
parser behaviour that could previously only be
probed indirectly. We demonstrate its usefulness by
applying it to a range of parsers (here), to reranked
K-best lists of various lengths, and to output for out-
of-domain parsing (following sections).
In Table 2 we consider the breakdown of parser
</bodyText>
<figure confidence="0.994688294117647">
NP
CC
NP
and
Dresdner AG’s 10% decline
for
Mannesmann AG
NP
PP
IN
NP
A 16% drop
Secretary
NNP
PP
IN NP Baker
NNP
</figure>
<page confidence="0.913525">
1053
</page>
<table confidence="0.9983494">
PP Clause Diff Mod NP 1-Word NP
Attach Attach Label Attach Attach Co-ord Span Unary Int. Other
0.60 0.38 0.31 0.25 0.25 0.23 0.20 0.14 0.14 0.50
1.12 0.61 0.51 0.39 0.45 0.40 0.42 0.27 0.27 1.13
Parser F-score
Best
Charniak-RS 92.07
Charniak-R 91.41
Charniak-S 91.02
Berkeley 90.06
Charniak 89.71
SSN 89.42
BUBS 88.63
Bikel 88.16
Collins-3 87.66
Collins-2 87.62
Collins-1 87.09
Stanford-F 86.42
Stanford-U 85.78
Worst
</table>
<tableCaption confidence="0.995640333333333">
Table 2: Average number of bracket errors per sentence due to the top ten error types. For instance, Stanford-U
produces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment. The scale for
each column is indicated by the Best and Worst values.
</tableCaption>
<table confidence="0.9969994375">
Nodes
Error Type Occurrences Involved Ratio
PP Attachment 846 1455 1.7
Single word phrase 490 490 1.0
Clause Attachment 385 913 2.4
Modifier Attachment 383 599 1.6
Different Label 377 754 2.0
Unary 347 349 1.0
NP Attachment 321 597 1.9
NP Internal Structure 299 352 1.2
Coordination 209 557 2.7
Unary Clause Label 185 200 1.1
VP Attachment 64 159 2.5
Parenthetical Attachment 31 74 2.4
Missing Parenthetical 12 17 1.4
Unclassified 655 734 1.1
</table>
<tableCaption confidence="0.9508576">
Table 3: Breakdown of errors on section 23 for the Char-
niak parser with self-trained model and reranker. Errors
are sorted by the number of times they occur. Ratio is the
average number of node errors caused by each error we
identify (i.e. Nodes Involved / Occurrences).
</tableCaption>
<bodyText confidence="0.950363973684211">
errors on WSJ section 23. The shaded area of
each bar indicates the frequency of parse errors (i.e.
empty means fewest errors). The area filled in is
determined by the expected number of node errors
per sentence that are attributed to that type of error.
The average number of node errors per sentence for
a completely full bar is indicated by the Worst row,
and the value for a completely empty bar is indicated
by the Best row. Exact error counts are available at
http://code.google.com/p/berkeley-parser-analyser/.
We use counts of node errors to make the con-
tributions of each type of error more interpretable.
As Table 3 shows, some errors typically cause only
a single node error, where as others, such as co-
ordination, generally cause several. This means
that considering counts of error groups would over-
emphasise some error types, e.g. single word phrase
errors are second most important by number of
groups (in Table 3), but seventh by total number of
node errors (in Table 2).
As expected, PP attachment is the largest contrib-
utor to errors, across all parsers. Interestingly, coor-
dination is sixth on the list, though that is partly due
to the fact that there are fewer coordination decisions
to be made in the treebank.3
By looking at the performance of the Collins
parser we can see the development over the past
fifteen years. There has been improvement across
the board, but in some cases, e.g. clause attach-
ment errors and different label errors, the change has
been more limited (24% and 29% reductions respec-
tively). We investigated the breakdown of the differ-
ent label errors by label, but no particular cases of la-
3This is indicated by the frequency of CCs and PPs in sec-
tions 02–21 of the treebank, 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions
as the nodes can be used in ways that do not involve a decision,
such as sentences that start with a conjunction.
</bodyText>
<page confidence="0.983662">
1054
</page>
<table confidence="0.998924315789474">
PP Clause Diff Mod NP 1-Word NP
System K F-score Attach Attach Label Attach Attach Co-ord Span Unary Int. Other
Best 0.08 0.04 0.08 0.05 0.06 0.04 0.08 0.04 0.04 0.11
1000 98.30
100 97.54
50 97.18
Oracle 20 96.40
10 95.66
5 94.61
2 92.59
1000 92.07
100 92.08
50 92.07
Charniak 20 92.05
10 92.16
5 91.94
2 91.56
1 91.02
Worst 0.66 0.43 0.33 0.26 0.28 0.26 0.23 0.16 0.19 0.60
</table>
<tableCaption confidence="0.997483">
Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser
</tableCaption>
<bodyText confidence="0.9719856">
with reranking and the self-trained model. The oracle results are determined by taking the parse in each K-best list
with the highest F-score.
bel confusion stand out, and we found that the most
common cases remained the same between Collins
and the top results.
It is also interesting to compare pairs of parsers
that share aspects of their architecture. One such
pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised
dependency parser. The main sources of the 0.64
gain in F-score are PP attachment and coordination.
Another interesting pair is the Berkeley parser and
the BUBS parser, which uses a Berkeley grammar,
but improves speed by pruning. The pruning meth-
ods used in BUBS are particularly damaging for PP
attachment errors and unary errors.
Various comparisons can be made between Char-
niak parser variants. We discuss the reranker be-
low. For the self-trained model McClosky et al.
(2006) performed some error analysis, considering
variations in F-score depending on the frequency of
tags such as PP, IN and CC in sentences. Here we
see gains on all error types, though particularly for
clause attachment, modifier attachment and coordi-
nation, which fits with their observations.
</bodyText>
<subsectionHeader confidence="0.994088">
5.1 Reranking
</subsectionHeader>
<bodyText confidence="0.999980551724138">
The standard dynamic programming approach to
parsing limits the range of features that can be em-
ployed. One way to deal with this issue is to mod-
ify the parser to produce the top K parses, rather
than just the 1-best, then use a model with more so-
phisticated features to choose the best parse from
this list (Collins, 2000). While re-ranking has led to
gains in performance (Charniak and Johnson, 2005),
there has been limited analysis of how effectively
rerankers are using the set of available options. Re-
cent work has explored this question in more depth,
but focusing on how variation in the parameters
impacts performance on standard metrics (Huang,
2008; Ng et al., 2010; Auli and Lopez, 2011; Ng
and Curran, 2012).
In Table 4 we present a breakdown over error
types for the Charniak parser, using the self-trained
model and reranker. The oracle results use the parse
in each K-best list with the highest F-score. While
this may not give the true oracle result, as F-score
does not factor over sentences, it gives a close ap-
proximation. The table has the same columns as Ta-
ble 2, but the ranges on the bars now reflect the min
and max for these sets.
While there is improvement on all errors when us-
ing the reranker, there is very little additional gain
beyond the first 5-10 parses. Even for the oracle
results, most of the improvement occurs within the
first 5-10 parses. The limited utility of extra parses
</bodyText>
<page confidence="0.965299">
1055
</page>
<table confidence="0.999811833333334">
Corpus F-score
Best
WSJ 23 92.07
Brown-F 85.91
Brown-G 84.56
Brown-K 84.09
Brown-L 83.95
Brown-M 84.65
Brown-N 85.20
Brown-P 84.09
Brown-R 83.60
G-Web Blogs 84.15
G-Web Email 81.18
Worst
PP Clause Diff Mod NP 1-Word NP
Attach Attach Label Attach Attach Co-ord Span Unary Int. Other
0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021
0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073
</table>
<tableCaption confidence="0.894336333333333">
Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and
the self-trained model. We use per word error rates here rather than per sentence as there is great variation in average
sentence length across the domains, skewing the per sentence results.
</tableCaption>
<bodyText confidence="0.99998375">
for the reranker may be due to the importance of
the base parser output probability feature (which, by
definition, decreases within the K-best list).
Interestingly, the oracle performance improves
across all error types, even at the 2-best level. This
indicates that the base parser model is not particu-
larly biased against a single error. Focusing on the
rows for K = 2 we can also see two interesting out-
liers. The PP attachment improvement of the ora-
cle is considerably higher than that of the reranker,
particularly compared to the differences for other er-
rors, suggesting that the reranker lacks the features
necessary to make the decision better than the parser.
The other interesting outlier is NP internal structure,
which continues to make improvements for longer
lists, unlike the other error types.
</bodyText>
<subsectionHeader confidence="0.999287">
5.2 Out-of-Domain
</subsectionHeader>
<bodyText confidence="0.999879583333333">
Parsing performance drops considerably when shift-
ing outside of the domain a parser was trained on
(Gildea, 2001). Clegg and Shepherd (2005) evalu-
ated parsers qualitatively on node types and rule pro-
ductions. Bender et al. (2011) designed a Wikipedia
test set to evaluate parsers on dependencies repre-
senting ten specific linguistic phenomena.
To provide a deeper understanding of the er-
rors arising when parsing outside of the newswire
domain, we analyse performance of the Charniak
parser with reranker and self-trained model on the
eight parts of the Brown corpus (Marcus et al.,
</bodyText>
<subsubsectionHeader confidence="0.631397">
Corpus Description Sentences Av. Length
</subsubsectionHeader>
<tableCaption confidence="0.9958125">
Table 6: Variation in size and contents of the domains we
consider. The variation in average sentence lengths skews
the results for errors per sentences, and so in Table 5 we
consider errors per word.
</tableCaption>
<bodyText confidence="0.999899066666667">
1993), and two parts of the Google Web corpus
(Petrov and McDonald, 2012). Table 6 shows statis-
tics for the corpora. The variation in average sen-
tence lengths skew the results for errors per sen-
tence. To handle this we divide by the number of
words to determine the results in Table 5, rather than
by the number of sentences, as in previous figures.
There are several interesting features in the table.
First, on the Brown datasets, while the general trend
is towards worse performance on all errors, NP in-
ternal structure is a notable exception and in some
cases PP attachment and unaries are as well.
In the other errors we see similar patterns across
the corpora, except humour (Brown R), on which the
parser is particularly bad at coordination and clause
</bodyText>
<figure confidence="0.746060941176471">
WSJ 23 Newswire 2416 23.5
Brown F Popular 3164 23.4
Brown G Biographies 3279 25.5
Brown K General 3881 17.2
Brown L Mystery 3714 15.7
881 16.6
Brown M Science
Brown N Adventure
Brown P Romance
Brown R Humour
G-Web Blogs Blogs
G-Web Email E-mail
4415 16.0
3942 17.4
967 22.7
1016 23.6
2450 11.9
</figure>
<page confidence="0.987163">
1056
</page>
<bodyText confidence="0.998486482758621">
attachment. This makes sense, as the colloquial na-
ture of the text includes more unusual uses of con-
junctions, for example:
She was a living doll and no mistake – the ...
Comparing the Brown corpora and the Google
Web corpora, there are much larger divergences. We
see a particularly large decrease in NP internal struc-
ture. Looking at some of the instances of this error, it
appears to be largely caused by incorrect handling of
structures such as URLs and phone numbers, which
do not appear in the PTB. There are also some more
difficult cases, for example:
... going up for sale in the next month or do .
where or do is a QP. This typographical error is ex-
tremely difficult to handle for a parser trained only
on well-formed text.
For e-mail there is a substantial drop on single
word phrases. Breaking the errors down by label we
found that the majority of the new errors are miss-
ing or extra NPs over single words. Here the main
problem appears to be temporal expressions, though
there also appear to be a substantial number of errors
that are also at the POS level, such as when NNP is
assigned to ta in this case:
... let you know that I ’m out ta here !
Some of these issues, such as URL handling,
could be resolved with suitable training data. Other
issues, such as ungrammatical language and uncon-
ventional use of words, pose a greater challenge.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977769230769">
The single F-score objective over brackets or depen-
dencies obscures important differences between sta-
tistical parsers. For instance, a single attachment er-
ror can lead to one or many mismatched brackets.
We have created a novel tree-transformation
methodology for evaluating parsers that categorises
errors into linguistically meaningful types. Using
this approach, we presented the first detailed exam-
ination of the errors produced by a wide range of
constituency parsers for English. We found that PP
attachment and clause attachment are the most chal-
lenging constructions, while coordination turns out
to be less problematic than previously thought. We
also noted interesting variations in error types for
parsers variants.
We investigated the errors resolved in reranking,
and introduced by changing domains. We found that
the Charniak rerankers improved most error types,
but made little headway on improving PP attach-
ment. Changing domain has an impact on all error
types, except NP internal structure.
We have released our system so that future con-
stituent parsers can be evaluated using our method-
ology. Our analysis provides new insight into the
development of parsers over the past fifteen years,
and the challenges that remain.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999824375">
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was par-
tially supported by a General Sir John Monash Fel-
lowship to the first author, the Office of Naval Re-
search under MURI Grant No. N000140911081, an
NSF Fellowship to the second author, ARC Discov-
ery grant DP1097291, the Capital Markets CRC, and
the NSF under grant 0643742.
</bodyText>
<sectionHeader confidence="0.997547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984437333333333">
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306–311, Pacific Grove,
California, USA, February.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings ofACL, pages 470–480, Portland, Oregon, USA,
June.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Proceed-
ings of EMNLP, pages 397–408, Edinburgh, United
Kingdom, July.
Daniel M. Bikel. 2004. Intricacies of collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of ACL, pages
440–449, Portland, Oregon, USA, June.
</reference>
<page confidence="0.923117">
1057
</page>
<reference confidence="0.999657219047619">
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of ACL, pages 41–
48, Sydney, Australia, July.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake, 2002. Relational Evaluation Schemes,
pages 4–8. Las Palmas, Canary Islands, Spain, May.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal.
In Proceedings of LREC, pages 447–454, Granada,
Spain, May.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC, Valletta, Malta,
May.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings ofACL, pages 173–180, Ann Ar-
bor, Michigan, USA, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132–139,
Seattle, Washington, USA, April.
Stephen Clark and Julia Hockenmaier. 2002. Evaluat-
ing a wide-coverage ccg parser. In Proceedings of the
LREC Beyond Parseval Workshop, Las Palmas, Ca-
nary Islands, Spain, May.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedical
corpus. In Proceedings of the ACL Workshop on Soft-
ware, pages 14–33, Ann Arbor, Michigan, USA, June.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings ofACL,
pages 16–23, Madrid, Spain, July.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings ofICML, pages
175–182, Palo Alto, California, USA, June.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Rebecca Dridan and Stephan Oepen. 2011. Parser evalu-
ation using elementary dependency matching. In Pro-
ceedings of IWPT, pages 225–230, Dublin, Ireland,
October.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2011. Efficient matrix-encoded grammars and low la-
tency parallelization strategies for cyk. In Proceedings
ofIWPT, pages 163–174, Dublin, Ireland, October.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings ofLREC, Mar-
rakech, Morocco, May.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings ofEMNLP, pages 167–202,
Pittsburgh, Pennsylvania, USA, June.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings ofIWPT, pages 11–22, Prague,
Czech Republic, June.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2009. Descriptive and empirical approaches to captur-
ing underlying dependencies among parsing errors. In
Proceedings ofEMNLP, pages 1162–1171, Singapore,
August.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings ofNAACL, pages 24–31, Edmonton, Canada,
May.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
ACL, pages 95–102, Barcelona, Spain, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586–594, Columbus, Ohio, USA, June.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Cor-
pora at EACL, Budapest, Hungary, April.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of ACL,
pages 423–430, Sapporo, Japan, July.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Proceedings of NIPS, pages 3–10,
Vancouver, British Columbia, Canada, December.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97–114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings ofNAACL, pages 152–159, New York, New York,
USA, June.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of EMNLP, pages 122–131,
Prague, Czech Republic, June.
Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
</reference>
<page confidence="0.8084">
1058
</page>
<reference confidence="0.999914166666667">
Proceedings of ACL, pages 46–54, Columbus, Ohio,
USA, June.
Dominick Ng and James R. Curran. 2012. N-best CCG
parsing and reranking. In Proceedings of ACL, Jeju,
South Korea, July.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage ccg parser. In Pro-
ceedings ofALTA, pages 90–98, Melbourne, Australia,
December.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
G´omez-Rodriguez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of Coling, pages 833–841, Beijing, China, August.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404–411, Rochester, New York, USA, April.
Slav Petrov and Ryan McDonald. 2012. SANCL Shared
Task. LDC2012E43. Linguistic Data Consortium.
Philadelphia, Philadelphia, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings ofACL, pages
433–440, Sydney, Australia, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP,
pages 62–69, Sydney, Australia, July.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
IWPT, pages 48–57, Dublin, Ireland, October.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using textual
entailments. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 51–56, Up-
psala, Sweden, July.
</reference>
<page confidence="0.996081">
1059
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981118">
<title confidence="0.996969">Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output</title>
<author confidence="0.998325">K R</author>
<affiliation confidence="0.9997745">Science Division School of IT University of California, Berkeley University of Sydney</affiliation>
<address confidence="0.99966">Berkeley, CA 94720, USA Sydney, NSW 2006, Australia</address>
<email confidence="0.99892">james@it.usyd.edu.au</email>
<abstract confidence="0.999292142857143">Constituency parser performance is primarily interpreted through a single metric, F-score 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>S Flickenger</author>
<author>C Gdaniec</author>
<author>C Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<location>Pacific Grove, California, USA,</location>
<contexts>
<context position="1516" citStr="Abney et al., 1991" startWordPosition="222" endWordPosition="225">y rerankers, and what types are introduced when parsing out-of-domain text. 1 Introduction Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors. We propose a new method of error classification using tree transformations. Errors in the parse tree are repaired using subtree movement, node creation, and node deletion. Each step in the process is then associated with a linguistically meaningful error type, based on fact</context>
</contexts>
<marker>Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>S. Abney, S. Flickenger, C. Gdaniec, C. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the workshop on Speech and Natural Language, pages 306–311, Pacific Grove, California, USA, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated ccg supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>470--480</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="26585" citStr="Auli and Lopez, 2011" startWordPosition="4575" endWordPosition="4578">res that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. Even for</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated ccg supertagging and parsing. In Proceedings ofACL, pages 470–480, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Yi Zhang</author>
</authors>
<title>Parser evaluation over local and nonlocal deep dependencies in a large corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>397--408</pages>
<location>Edinburgh, United Kingdom,</location>
<contexts>
<context position="29074" citStr="Bender et al. (2011)" startWordPosition="4994" endWordPosition="4997">ement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., Corpus Description Sentences Av. Length Table 6: Variation in size and contents of the domains we consider. The variation in average sentence lengths skews the results for errors per sentences, and so in Table 5 we consider errors per word. 1993)</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Zhang, 2011</marker>
<rawString>Emily M. Bender, Dan Flickinger, Stephan Oepen, and Yi Zhang. 2011. Parser evaluation over local and nonlocal deep dependencies in a large corpus. In Proceedings of EMNLP, pages 397–408, Edinburgh, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins’ parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7191" citStr="Bikel (2004)" startWordPosition="1119" endWordPosition="1120"> from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes i</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Bodenstab</author>
<author>Aaron Dunlop</author>
<author>Keith Hall</author>
<author>Brian Roark</author>
</authors>
<title>Beam-width prediction for efficient context-free parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--449</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7276" citStr="Bodenstab et al., 2011" startWordPosition="1130" endWordPosition="1133"> into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner</context>
</contexts>
<marker>Bodenstab, Dunlop, Hall, Roark, 2011</marker>
<rawString>Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian Roark. 2011. Beam-width prediction for efficient context-free parsing. In Proceedings of ACL, pages 440–449, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="5211" citStr="Briscoe and Carroll, 2006" startWordPosition="796" endWordPosition="799">o account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are stro</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of ACL, pages 41– 48, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Jonathan Graham</author>
<author>Ann Copestake</author>
</authors>
<title>Relational Evaluation Schemes,</title>
<date>2002</date>
<pages>4--8</pages>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="5261" citStr="Briscoe et al., 2002" startWordPosition="805" endWordPosition="808">use of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being use</context>
</contexts>
<marker>Briscoe, Carroll, Graham, Copestake, 2002</marker>
<rawString>Ted Briscoe, John Carroll, Jonathan Graham, and Ann Copestake, 2002. Relational Evaluation Schemes, pages 4–8. Las Palmas, Canary Islands, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>447--454</pages>
<location>Granada, Spain,</location>
<contexts>
<context position="1644" citStr="Carroll et al., 1998" startWordPosition="241" endWordPosition="244">earch within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors. We propose a new method of error classification using tree transformations. Errors in the parse tree are repaired using subtree movement, node creation, and node deletion. Each step in the process is then associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents. Using our method we analyse the output of thirteen constituency p</context>
<context position="5364" citStr="Carroll et al., 1998" startWordPosition="822" endWordPosition="825">e work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of LREC, pages 447–454, Granada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to stanford dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Valletta, Malta,</location>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to stanford dependencies: Trade-offs between speed and accuracy. In Proceedings of LREC, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="1268" citStr="Charniak and Johnson, 2005" startWordPosition="183" endWordPosition="186"> transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text. 1 Introduction Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors. We propose a new method of</context>
<context position="7572" citStr="Charniak and Johnson, 2005" startWordPosition="1174" endWordPosition="1177"> For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact</context>
<context position="26289" citStr="Charniak and Johnson, 2005" startWordPosition="4526" endWordPosition="4529">e frequency of tags such as PP, IN and CC in sentences. Here we see gains on all error types, though particularly for clause attachment, modifier attachment and coordination, which fits with their observations. 5.1 Reranking The standard dynamic programming approach to parsing limits the range of features that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over se</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180, Ann Arbor, Michigan, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="7458" citStr="Charniak (2000)" startWordPosition="1158" endWordPosition="1159">ation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL, pages 132–139, Seattle, Washington, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Evaluating a wide-coverage ccg parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC Beyond Parseval Workshop,</booktitle>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="5968" citStr="Clark and Hockenmaier, 2002" startWordPosition="923" endWordPosition="926">e (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present h</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>Stephen Clark and Julia Hockenmaier. 2002. Evaluating a wide-coverage ccg parser. In Proceedings of the LREC Beyond Parseval Workshop, Las Palmas, Canary Islands, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Clegg</author>
<author>Adrian J Shepherd</author>
</authors>
<title>Evaluating and integrating treebank parsers on a biomedical corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Software,</booktitle>
<pages>14--33</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="28985" citStr="Clegg and Shepherd (2005)" startWordPosition="4979" endWordPosition="4982">using on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., Corpus Description Sentences Av. Length Table 6: Variation in size and contents of the domains we consider. The variation in average sentence lengths skews t</context>
</contexts>
<marker>Clegg, Shepherd, 2005</marker>
<rawString>Andrew B. Clegg and Adrian J. Shepherd. 2005. Evaluating and integrating treebank parsers on a biomedical corpus. In Proceedings of the ACL Workshop on Software, pages 14–33, Ann Arbor, Michigan, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>16--23</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="7224" citStr="Collins (1997)" startWordPosition="1123" endWordPosition="1124">rectly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderso</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings ofACL, pages 16–23, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>175--182</pages>
<location>Palo Alto, California, USA,</location>
<contexts>
<context position="26210" citStr="Collins, 2000" startWordPosition="4516" endWordPosition="4517"> error analysis, considering variations in F-score depending on the frequency of tags such as PP, IN and CC in sentences. Here we see gains on all error types, though particularly for clause attachment, modifier attachment and coordination, which fits with their observations. 5.1 Reranking The standard dynamic programming approach to parsing limits the range of features that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. Whi</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings ofICML, pages 175–182, Palo Alto, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1577" citStr="Collins (2003)" startWordPosition="233" endWordPosition="234">ain text. 1 Introduction Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors. We propose a new method of error classification using tree transformations. Errors in the parse tree are repaired using subtree movement, node creation, and node deletion. Each step in the process is then associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents</context>
<context position="4531" citStr="Collins, 2003" startWordPosition="689" endWordPosition="690"> 2 Background Most attempts to understand the behaviour of constituency parsers have focused on overall evaluation metrics. The three main methods are intrinsic evaluation with PARSEVAL, evaluation on dependencies extracted from the constituency parse, and evaluation on downstream tasks that rely on parsing. Intrinsic evaluation with PARSEVAL, which calculates precision and recall over labeled tree nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves ex</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
<author>Stephan Oepen</author>
</authors>
<title>Parser evaluation using elementary dependency matching.</title>
<date>2011</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>225--230</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="6363" citStr="Dridan and Oepen, 2011" startWordPosition="984" endWordPosition="987">owever, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers w</context>
</contexts>
<marker>Dridan, Oepen, 2011</marker>
<rawString>Rebecca Dridan and Stephan Oepen. 2011. Parser evaluation using elementary dependency matching. In Proceedings of IWPT, pages 225–230, Dublin, Ireland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Dunlop</author>
<author>Nathan Bodenstab</author>
<author>Brian Roark</author>
</authors>
<title>Efficient matrix-encoded grammars and low latency parallelization strategies for cyk.</title>
<date>2011</date>
<booktitle>In Proceedings ofIWPT,</booktitle>
<pages>163--174</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="7251" citStr="Dunlop et al., 2011" startWordPosition="1126" endWordPosition="1129">ly classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). </context>
</contexts>
<marker>Dunlop, Bodenstab, Roark, 2011</marker>
<rawString>Aaron Dunlop, Nathan Bodenstab, and Brian Roark. 2011. Efficient matrix-encoded grammars and low latency parallelization strategies for cyk. In Proceedings ofIWPT, pages 163–174, Dublin, Ireland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Parser evaluation and the bnc: Evaluating 4 constituency parsers with 3 metrics.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<location>Marrakech, Morocco,</location>
<marker>Foster, van Genabith, 2008</marker>
<rawString>Jennifer Foster and Josef van Genabith. 2008. Parser evaluation and the bnc: Evaluating 4 constituency parsers with 3 metrics. In Proceedings ofLREC, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>167--202</pages>
<location>Pittsburgh, Pennsylvania, USA,</location>
<contexts>
<context position="28958" citStr="Gildea, 2001" startWordPosition="4977" endWordPosition="4978">ngle error. Focusing on the rows for K = 2 we can also see two interesting outliers. The PP attachment improvement of the oracle is considerably higher than that of the reranker, particularly compared to the differences for other errors, suggesting that the reranker lacks the features necessary to make the decision better than the parser. The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types. 5.2 Out-of-Domain Parsing performance drops considerably when shifting outside of the domain a parser was trained on (Gildea, 2001). Clegg and Shepherd (2005) evaluated parsers qualitatively on node types and rule productions. Bender et al. (2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., Corpus Description Sentences Av. Length Table 6: Variation in size and contents of the domains we consider. The variation in avera</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings ofEMNLP, pages 167–202, Pittsburgh, Pennsylvania, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an hpsg parser.</title>
<date>2007</date>
<booktitle>In Proceedings ofIWPT,</booktitle>
<pages>11--22</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4934" citStr="Hara et al., 2007" startWordPosition="753" endWordPosition="756">e nodes, is a useful indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and va</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2007</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an hpsg parser. In Proceedings ofIWPT, pages 11–22, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Descriptive and empirical approaches to capturing underlying dependencies among parsing errors.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>1162--1171</pages>
<location>Singapore,</location>
<contexts>
<context position="5662" citStr="Hara et al., 2009" startWordPosition="873" endWordPosition="876">arsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et a</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2009</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2009. Descriptive and empirical approaches to capturing underlying dependencies among parsing errors. In Proceedings ofEMNLP, pages 1162–1171, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>24--31</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="7831" citStr="Henderson, 2003" startWordPosition="1216" endWordPosition="1217">s (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 STANDARD PARSERS Berkeley 90.06 90.30 89.81 36.59 4.2 Charniak 89.71 89.88 89.55 37.25 1.8 SSN 89.42 89.96</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proceedings ofNAACL, pages 24–31, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>95--102</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7849" citStr="Henderson, 2004" startWordPosition="1218" endWordPosition="1219">unlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 STANDARD PARSERS Berkeley 90.06 90.30 89.81 36.59 4.2 Charniak 89.71 89.88 89.55 37.25 1.8 SSN 89.42 89.96 88.89 32.74 1.8 B</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proceedings of ACL, pages 95–102, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="26546" citStr="Huang, 2008" startWordPosition="4569" endWordPosition="4570">sing limits the range of features that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL, pages 586–594, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 dependency bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora at EACL,</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="1664" citStr="King et al., 2003" startWordPosition="245" endWordPosition="248">onal linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors. We propose a new method of error classification using tree transformations. Errors in the parse tree are repaired using subtree movement, node creation, and node deletion. Each step in the process is then associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents. Using our method we analyse the output of thirteen constituency parsers on newswire. </context>
<context position="5183" citStr="King et al., 2003" startWordPosition="792" endWordPosition="795">rs is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for c</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 dependency bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora at EACL, Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7968" citStr="Klein and Manning, 2003" startWordPosition="1234" endWordPosition="1237">er grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 STANDARD PARSERS Berkeley 90.06 90.30 89.81 36.59 4.2 Charniak 89.71 89.88 89.55 37.25 1.8 SSN 89.42 89.96 88.89 32.74 1.8 BUBS 88.50 88.57 88.43 31.62 27.6 Bikel 88.16 88.23 88.10 32.33 0.8 Collins-3 87.66 87.82 87.50 32.22 2.0 Collins-2 87.6</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3--10</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="7968" citStr="Klein and Manning, 2003" startWordPosition="1234" endWordPosition="1237">er grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07 92.44 91.70 44.87 1.8 Charniak-R 91.41 91.78 91.04 44.04 1.8 Charniak-S 91.02 91.16 90.89 40.77 1.8 STANDARD PARSERS Berkeley 90.06 90.30 89.81 36.59 4.2 Charniak 89.71 89.88 89.55 37.25 1.8 SSN 89.42 89.96 88.89 32.74 1.8 BUBS 88.50 88.57 88.43 31.62 27.6 Bikel 88.16 88.23 88.10 32.33 0.8 Collins-3 87.66 87.82 87.50 32.22 2.0 Collins-2 87.6</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. In Proceedings of NIPS, pages 3–10, Vancouver, British Columbia, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="5238" citStr="Lin, 1998" startWordPosition="803" endWordPosition="804">issing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the depe</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. A dependency-based method for evaluating broad-coverage parsers. Natural Language Engineering, 4(2):97–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>152--159</pages>
<location>New York, New York, USA,</location>
<contexts>
<context position="1239" citStr="McClosky et al., 2006" startWordPosition="179" endWordPosition="182">ingful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text. 1 Introduction Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node error</context>
<context position="7624" citStr="McClosky et al., 2006" startWordPosition="1182" endWordPosition="1185">with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, a base model, a model that uses subcategorisation frames for head words, and a model that takes into account traces. SSI (Henderson, 2003; Henderson, 2004). A statistical left-corner parser, with probabilities estimated by a neural network. Stanford (Klein and Manning, 2003a; Klein and Manning, 2003b). We consider both the unlexicalised PCFG parser (-U) and the factored parser (-F), which combines the PCFG parser with a lexicalised dependency parser. 1049 System F P R Exact Speed ENHANCED TRAINING / SYSTEMS Charniak-SR 92.07</context>
<context position="25581" citStr="McClosky et al. (2006)" startWordPosition="4407" endWordPosition="4410">ts of their architecture. One such pair is the Stanford parser, where the factored parser combines the unlexicalised parser with a lexicalised dependency parser. The main sources of the 0.64 gain in F-score are PP attachment and coordination. Another interesting pair is the Berkeley parser and the BUBS parser, which uses a Berkeley grammar, but improves speed by pruning. The pruning methods used in BUBS are particularly damaging for PP attachment errors and unary errors. Various comparisons can be made between Charniak parser variants. We discuss the reranker below. For the self-trained model McClosky et al. (2006) performed some error analysis, considering variations in F-score depending on the frequency of tags such as PP, IN and CC in sentences. Here we see gains on all error types, though particularly for clause attachment, modifier attachment and coordination, which fits with their observations. 5.1 Reranking The standard dynamic programming approach to parsing limits the range of features that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse f</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings ofNAACL, pages 152–159, New York, New York, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>122--131</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5737" citStr="McDonald and Nivre, 2007" startWordPosition="887" endWordPosition="890"> method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al., 2010; Cer et al., 2010), as well as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencie</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMNLP, pages 122–131, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Sætre</author>
<author>Kenji Sagae</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>46--54</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="6271" citStr="Miyao et al., 2008" startWordPosition="971" endWordPosition="974">al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range o</context>
</contexts>
<marker>Miyao, Sætre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of ACL, pages 46–54, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>James R Curran</author>
</authors>
<title>N-best CCG parsing and reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL, Jeju, South Korea,</booktitle>
<contexts>
<context position="26607" citStr="Ng and Curran, 2012" startWordPosition="4579" endWordPosition="4582">ed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses. Even for the oracle results, m</context>
</contexts>
<marker>Ng, Curran, 2012</marker>
<rawString>Dominick Ng and James R. Curran. 2012. N-best CCG parsing and reranking. In Proceedings of ACL, Jeju, South Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Reranking a wide-coverage ccg parser.</title>
<date>2010</date>
<booktitle>In Proceedings ofALTA,</booktitle>
<pages>90--98</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="26563" citStr="Ng et al., 2010" startWordPosition="4571" endWordPosition="4574">he range of features that can be employed. One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000). While re-ranking has led to gains in performance (Charniak and Johnson, 2005), there has been limited analysis of how effectively rerankers are using the set of available options. Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008; Ng et al., 2010; Auli and Lopez, 2011; Ng and Curran, 2012). In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker. The oracle results use the parse in each K-best list with the highest F-score. While this may not give the true oracle result, as F-score does not factor over sentences, it gives a close approximation. The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets. While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first</context>
</contexts>
<marker>Ng, Honnibal, Curran, 2010</marker>
<rawString>Dominick Ng, Matthew Honnibal, and James R. Curran. 2010. Reranking a wide-coverage ccg parser. In Proceedings ofALTA, pages 90–98, Melbourne, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez-Rodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>833--841</pages>
<location>Beijing, China,</location>
<marker>Nivre, Rimell, McDonald, G´omez-Rodriguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez-Rodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of Coling, pages 833–841, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York, USA,</location>
<contexts>
<context position="1168" citStr="Petrov and Klein, 2007" startWordPosition="168" endWordPosition="171"> remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text. 1 Introduction Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007), and 92% when using self-training and reranking (McClosky et al., 2006; Charniak and Johnson, 2005). While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors. Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al., 1991) have either focused on specific parsers, e.g. Collins (2003), or have involved conversion to dependencies (Carroll et al., 1998; King et al., 2003). In all of these cases, the analysis has not taken into consideration how a set of errors can have a co</context>
<context position="7094" citStr="Petrov and Klein, 2007" startWordPosition="1104" endWordPosition="1107"> into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised parser, with three models, </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL, pages 404–411, Rochester, New York, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>SANCL Shared Task. LDC2012E43. Linguistic Data Consortium.</booktitle>
<location>Philadelphia, Philadelphia, USA.</location>
<contexts>
<context position="29742" citStr="Petrov and McDonald, 2012" startWordPosition="5103" endWordPosition="5106"> parsers on dependencies representing ten specific linguistic phenomena. To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al., Corpus Description Sentences Av. Length Table 6: Variation in size and contents of the domains we consider. The variation in average sentence lengths skews the results for errors per sentences, and so in Table 5 we consider errors per word. 1993), and two parts of the Google Web corpus (Petrov and McDonald, 2012). Table 6 shows statistics for the corpora. The variation in average sentence lengths skew the results for errors per sentence. To handle this we divide by the number of words to determine the results in Table 5, rather than by the number of sentences, as in previous figures. There are several interesting features in the table. First, on the Brown datasets, while the general trend is towards worse performance on all errors, NP internal structure is a notable exception and in some cases PP attachment and unaries are as well. In the other errors we see similar patterns across the corpora, except</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. SANCL Shared Task. LDC2012E43. Linguistic Data Consortium. Philadelphia, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="7069" citStr="Petrov et al., 2006" startWordPosition="1100" endWordPosition="1103">y require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years. For all parsers we used the publicly available version, with the standard parameter settings. Berkeley (Petrov et al., 2006; Petrov and Klein, 2007). An unlexicalised parser with a grammar constructed with automatic state splitting. Bikel (2004) implementation of Collins (1997). BUBS (Dunlop et al., 2011; Bodenstab et al., 2011). A ‘grammar-agnostic constituent parser,’ which uses a Berkeley Parser grammar, but parses with various pruning techniques to improve speed, at the cost of accuracy. Charniak (2000). A generative parser with a maximum entropy-inspired model. We also use the reranker (Charniak and Johnson, 2005), and the self-trained model (McClosky et al., 2006). Collins (1997). A generative lexicalised pa</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings ofACL, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>62--69</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6226" citStr="Quirk and Corston-Oliver, 2006" startWordPosition="965" endWordPosition="968">ell as work on finding relations between errors (Hara et al., 2009), and breaking down errors by a range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 </context>
</contexts>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical machine translation. In Proceedings of EMNLP, pages 62–69, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Yusuke Miyao</author>
<author>Takuya Matsuzaki</author>
<author>Xiangli Wang</author>
<author>Junichi Tsujii</author>
</authors>
<title>Analysis of the difficulties in chinese deep parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>48--57</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4952" citStr="Yu et al., 2011" startWordPosition="757" endWordPosition="760">l indicator of overall performance, but does not pinpoint which structures the parser has most difficulty with. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake. There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al., 2007; Yu et al., 2011). By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences. The second major parser evaluation method involves extraction of grammatical relations (King et al., 2003; Briscoe and Carroll, 2006) or dependencies (Lin, 1998; Briscoe et al., 2002). These metrics have been argued to be more informative and generally applicable (Carroll et al., 1998), and have the advantage that the breakdown over dependency types is more informative than over node types. There have been comparisons of multiple parsers (Foster and van Genabith, 2008; </context>
</contexts>
<marker>Yu, Miyao, Matsuzaki, Wang, Tsujii, 2011</marker>
<rawString>Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli Wang, and Junichi Tsujii. 2011. Analysis of the difficulties in chinese deep parsing. In Proceedings of IWPT, pages 48–57, Dublin, Ireland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Aydin Han</author>
<author>Zehra Turgut</author>
</authors>
<title>Semeval-2010 task 12: Parser evaluation using textual entailments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>51--56</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6312" citStr="Yuret et al., 2010" startWordPosition="977" endWordPosition="980"> range of factors (McDonald and Nivre, 2007). However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parser’s output (Clark and Hockenmaier, 2002). Our approach does not have this disadvantage, as we analyse parser output directly. The third major approach involves extrinsic evaluation, where the parser’s output is used in a downstream task, such as machine translation (Quirk and Corston-Oliver, 2006), information extraction (Miyao et al., 2008), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011). While some of these approaches give a better sense of the impact of parse errors, they require integration into a larger system, making it less clear where a given error originates. The work we present here differs from existing approaches by directly and automatically classifying errors into meaningful types. This enables the first very broad, yet detailed, study of parser behaviour, evaluating the output of thirteen parsers over thousands of sentences. 3 Parsers Our evaluation is over a wide range of PTB constituency parsers and their vari</context>
</contexts>
<marker>Yuret, Han, Turgut, 2010</marker>
<rawString>Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. Semeval-2010 task 12: Parser evaluation using textual entailments. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 51–56, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
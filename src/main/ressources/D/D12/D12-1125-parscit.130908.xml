<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000139">
<title confidence="0.618503">
Learning to Map into a Universal POS Tagset
</title>
<author confidence="0.970232">
Yuan Zhang, Roi Reichart, Regina Barzilay Amir Globerson
</author>
<affiliation confidence="0.98396">
Massachusetts Institute of Technology The Hebrew University
</affiliation>
<email confidence="0.991273">
{yuanzh, roiri, regina}@csail.mit.edu gamir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.998568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980454545455">
We present an automatic method for mapping
language-specific part-of-speech tags to a set
of universal tags. This unified representation
plays a crucial role in cross-lingual syntactic
transfer of multilingual dependency parsers.
Until now, however, such conversion schemes
have been created manually. Our central hy-
pothesis is that a valid mapping yields POS
annotations with coherent linguistic proper-
ties which are consistent across source and
target languages. We encode this intuition
in an objective function that captures a range
of distributional and typological characteris-
tics of the derived mapping. Given the ex-
ponential size of the mapping space, we pro-
pose a novel method for optimizing over soft
mappings, and use entropy regularization to
drive those towards hard mappings. Our re-
sults demonstrate that automatically induced
mappings rival the quality of their manually
designed counterparts when evaluated in the
context of multilingual parsing.1
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998695">
In this paper, we explore an automatic method for
mapping language-specific part-of-speech tags to a
universal tagset. In multilingual parsing, this uni-
fied input representation is required for cross-lingual
syntactic transfer. Specifically, the universal tagset
annotations enable an unlexicalized parser to capi-
talize on annotations from one language when learn-
ing a model for another.
</bodyText>
<footnote confidence="0.967112">
1The source code and data for the work presented in this
paper is available at http://groups.csail.mit.edu/
rbg/code/unitag/emnlp2012
</footnote>
<bodyText confidence="0.999754411764706">
While the notion of a universal POS tagset is
widely accepted, in practice it is hardly ever used
for annotation of monolingual resources. In fact,
available POS annotations are designed to capture
language-specific idiosyncrasies and therefore are
substantially more detailed than a coarse universal
tagset. To reconcile these cross-lingual annotation
differences, a number of mapping schemes have
been proposed in the parsing community (Zeman
and Resnik, 2008; Petrov et al., 2011; Naseem et
al., 2010). In all of these cases, the conversion is
performed manually and has to be repeated for each
language and annotation scheme anew.
Despite the apparent simplicity, deriving a map-
ping is by no means easy, even for humans. In fact,
the universal tagsets manually induced by Petrov
et al. (2011) and by Naseem et al. (2010) disagree
on 10% of the tags. An example of such discrep-
ancy is the mapping of the Japanese tag “PVfin” to
the universal tag “particle” according to one scheme,
and to “verb” according to another. Moreover, the
quality of this conversion has a direct implication on
the parsing performance. In the Japanese example
above, this difference in mapping yields a 6.7% dif-
ference in parsing accuracy.
The goal of our work is to induce the mapping
for a new language, utilizing existing manually-
constructed mappings as training data. The exist-
ing mappings developed in the parsing community
rely on gold POS tags for the target language. A
more realistic scenario is to employ the mapping
technique to resource-poor languages where gold
POS annotations are lacking. In such cases, a map-
ping algorithm has to operate over automatically in-
</bodyText>
<page confidence="0.935275">
1368
</page>
<note confidence="0.7633235">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1368–1378, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.98832768852459">
duced clusters on the target language (e.g., using
the Brown algorithm) and convert them to universal
tags. We are interested in a mapping approach that
can effectively handle both gold tags and induced
clusters.
Our central hypothesis is that a valid mapping
yields POS annotations with coherent linguistic
properties which are consistent across languages.
Since universal tags play the same linguistic role
in source and target languages, we expect similar-
ity in their global distributional statistics. Figure 1a
shows statistics for two close languages, English and
German. We can see that their unigram frequencies
on the five most common tags are very close. Other
properties concern POS tag per sentence statistics –
e.g., every sentence has to have at least one verb. Fi-
nally, the mappings can be further constrained by ty-
pological properties of the target language that spec-
ify likely tag sequences. This information is readily
available even for resource poor language (Haspel-
math et al., 2005). For instance, since English and
German are prepositional languages, we expect to
observe adposition-noun sequences but not the re-
verse (see Figure 1b for sample sentences). We en-
code these heterogeneous properties into an objec-
tive function that guides the search for the optimal
mapping.
Having defined a quality measure for mappings,
our goal is to find the optimal mapping. However,
such partition optimization problems2 are NP hard
(Garey and Johnson, 1979). A naive approach to
the problem is to greedily improve the map, but it
turns out that this approach yields poor quality map-
pings. We therefore develop a method for optimiz-
ing over soft mappings, and use entropy regulariza-
tion to drive those towards hard mappings. We con-
struct the objective in a way that facilitates simple
monotonically improving updates corresponding to
solving convex optimization problems.
We evaluate our mapping approach on 19
languages that include representatives of Indo-
European, Semitic, Basque, Japonic and Turkic fam-
ilies. We measure mapping quality based on the
target language parsing accuracy. In addition to
considering gold POS tags for the target language,
2Instances of related hard problems are 3-partition and
subset-sum.
we also evaluate the mapping algorithm on auto-
matically induced POS tags. In all evaluation sce-
narios, our model consistently rivals the quality
of manually induced mappings. We also demon-
strate that the proposed inference procedure outper-
forms greedy methods by a large margin, highlight-
ing the importance of good optimization techniques.
We further show that while all characteristics of
the mapping contribute to the objective, our largest
gain comes from distributional features that capture
global statistics. Finally, we establish that the map-
ping quality has a significant impact on the accuracy
of syntactic transfer, which motivates further study
of this topic.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996891709677419">
Multilingual Parsing Early approaches for multi-
lingual parsing used parallel data to bridge the gap
between languages when modeling syntactic trans-
fer. In this setup, finding the mapping between var-
ious POS annotation schemes was not essential; in-
stead, the transfer algorithm could induce it directly
from the parallel data (Hwa et al., 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008). However,
more recent transfer approaches relinquish this data
requirement, learning to transfer from non-parallel
data (Zeman and Resnik, 2008; McDonald et al.,
2011; Cohen et al., 2011; Naseem et al., 2010).
These approaches assume access to a common input
representation in the form of universal tags, which
enables the model to connect patterns observed in
the source language to their counterparts in the tar-
get language.
Despite ongoing efforts to standardize POS tags
across languages (e.g., EAGLES initiative (Eynde,
2004)), many corpora are still annotated with
language-specific tags. In previous work, their map-
ping to universal tags was performed manually. Yet,
even though some of these mappings have been de-
veloped for the same CoNLL dataset (Buchholz and
Marsi, 2006; Nivre et al., 2007), they are not identi-
cal and yield different parsing performance (Zeman
and Resnik, 2008; Petrov et al., 2011; Naseem et al.,
2010). The goal of our work is to automate this pro-
cess and construct mappings that are optimized for
performance on downstream tasks (here we focus on
parsing). As our results show, we achieve this goal
</bodyText>
<page confidence="0.993104">
1369
</page>
<figure confidence="0.936691190476191">
0.35
0.3
Unigram Frequency
0.25
0.2
0.15
0.1
0.05
0
English
German
Noun Verb Det. Prep. Adj.
-Investors [are appealing] to the Securities
and Exchange Commission not to [limit] their
access to information [about stock purchases]
and sales [by corporate insiders]
-Einer der sich [für den Milliardär] [ausspricht]
[ist] Steve Jobs dem Perot [für den aufbau]
der Computerfirma Next 20 Millionen Dollar
[bereitstellte]
(a) (b)
</figure>
<figureCaption confidence="0.69476">
Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five
tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in
blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions.
</figureCaption>
<bodyText confidence="0.999825105263158">
on a broad range of languages and evaluation sce-
narios.
Syntactic Category Refinement Our work also
relates to work in syntactic category refinement in
which POS categories and parse tree non-terminals
are refined in order to improve parsing perfor-
mance (Finkel et al., 2007; Klein and Manning,
2003; Matsuzaki et al., 2005; Petrov et al., 2006;
Petrov and Klein, 2007; Liang et al., 2007). Our
work differs from these approaches in two ways.
First, these methods have been developed in the
monolingual setting, while our mapping algorithm is
designed for multilingual parsing. Second, these ap-
proaches are trained on the syntactic trees of the tar-
get language, which enables them to directly link the
quality of newly induced categories with the quality
of syntactic parsing. In contrast, we are not given
trees in the target language. Instead, our model is
informed by mappings derived for other languages.
</bodyText>
<sectionHeader confidence="0.987872" genericHeader="method">
3 Task Formulation
</sectionHeader>
<bodyText confidence="0.999954523809524">
The input to our task consists of a target corpus writ-
ten in a language T, and a set of non-parallel source
corpora written in languages {51, ... , 5n}. In the
source corpora, each word is annotated with both
a language-specific POS tag and a universal POS
tag (Petrov et al., 2011). In the target corpus each
word is annotated only with a language-specific POS
tag, either gold or automatically induced.
Our goal is to find a map from the set of LT target
language tags to the set of K universal tags. We as-
sume that each language-specific tag is only mapped
to one universal tag, which means we never split a
language-specific tag and LT &gt; K holds for every
language. We represent the map by a matrix A of
size K x LT where A(c|f) = 1 if the target lan-
guage tag f is mapped to the universal tag c, and
A(c|f) = 0 otherwise.3 Note that each column of
A should contain a single value of 1. We will later
relax the requirement that A(c|f) E 10, 11. A candi-
date mapping A can be applied to the target language
to produce sentences labeled with universal tags.
</bodyText>
<sectionHeader confidence="0.995678" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.958368833333333">
In this section we describe an objective that reflects
the quality of an automatic mapping.
Our key insight is that for a good mapping, the
statistics over the universal tags should be similar for
source and target languages because these tags play
the same role cross-linguistically. For example, we
should expect the frequency of a particular universal
tag to be similar in the source and target languages.
One choice to make when constructing an objec-
tive is the source languages to which we want to be
similar. It is clear that choosing all languages is not a
good idea, since they are not all expected to have dis-
tributional properties similar to the target language.
There is strong evidence that projecting from sin-
gle languages can lead to good parsing performance
3We use c and f to reflect the fact that universal tags are
a coarse version (hence c) of the language specific fine tags
(hence f).
</bodyText>
<page confidence="0.955687">
1370
</page>
<bodyText confidence="0.999727916666667">
(McDonald et al., 2011). Therefore, our strategy is
to choose a single source language for comparison.
The choice of the source language is based on sim-
ilarity between typological properties; we describe
this in detail in Section 5.
We must also determine which statistical proper-
ties we expect to be preserved across languages. Our
model utilizes three linguistic phenomena which are
consistent across languages: POS tag global distri-
butional statistics, POS tag per sentence statistics,
and typology-based ordering statistics. We define
each of these below.
</bodyText>
<subsectionHeader confidence="0.999297">
4.1 Mapping Characterization
</subsectionHeader>
<bodyText confidence="0.999888214285714">
We focus on three categories of mapping properties.
For each of the relevant statistics we define a func-
tion Fi(A) that has low values if the source and tar-
get statistics are similar.
Global distributional statistics: The unigram and
bigram statistics of the universal tags are expected
to be similar across languages with close typological
profiles. We use pS(c1, c2) to denote the bigram dis-
tribution over universal tags in the source language,
and pT (f1, f2) to denote the bigram distribution over
language specific tags in the target language. The
bigram distribution over universal tags in the target
language depends on A and pT (f1, f2) and is given
by:
</bodyText>
<equation confidence="0.971167666666667">
�pT (c1, c2; A) = A(c1|f1)A(c2|f2)pT(f1, f2)
AJ2
(1)
</equation>
<bodyText confidence="0.999171333333333">
To enforce similarity between source and target dis-
tributions, we wish to minimize the KL divergence
between the two: 4
</bodyText>
<equation confidence="0.998958">
Fbi(A) = DKL[pS(c1,c2)|pT(c1,c2;A)] (2)
</equation>
<bodyText confidence="0.998743363636364">
We similarly define Funi(A) as the distance be-
tween unigram distributions.
Per sentence statistics: Another defining property
of POS tags is their average count per sentence.
Specifically, we focus on the verb count per sen-
tence, which we expect be similar across languages.
4We use the KL divergence because it assigns low weights
to infrequent universal tags. Furthermore, this choice results in
a simple, EM-like parameter estimation algorithm as discussed
in Section 5.
To express this constraint, we use n„(s, A) to
denote the number of verbs (i.e., the universal
tags corresponding to verbs according to A) in
sentence s. This is a linear function of A. We also
use E[n,(s, A)] to denote the average number of
verbs per sentence, and V [n,(s, A)] to denote the
variance. We estimate these two statistics from
the source language and denote them by ES,,, VS,.
Good mappings are expected to follow these
patterns by having a variance upper bounded by
VS„ and an average lower bounded by ES,.5 This
corresponds to minimizing the following objectives:
</bodyText>
<equation confidence="0.884179">
FE,(A) = max [0, ESq, − E[n,(s, A)]]
FV ,(A) = max [0, V [n„(s, A)] − VS„]
</equation>
<bodyText confidence="0.999469105263158">
Note that the above objectives are convex in A,
which will make optimization simpler. We refer to
the two terms jointly as FT.,.b(A).
Typology-based ordering statistics: Typolog-
ical features can be useful for determining the
relative order of different tags. If we know that
the target language has a particular typological
feature, we expect its universal tags to obey the
given relative ordering. Specifically, we expect it to
agree with ordering statistics for source languages
with a similar typology. We consider two such
features here. First, in pre-position languages the
preposition is followed by the noun phrase. Thus, if
T is such a language, we expect the probability of
a noun phrase following the adposition to be high,
i.e., cross some threshold. Formally, we define C1 =
{noun, adj, num, pron, det} and consider the set of
bigram distributions Spre that satisfy the following
constraint:
</bodyText>
<equation confidence="0.97052">
� pT(adp,c) &gt; apre (3)
cEC1
</equation>
<bodyText confidence="0.99758975">
where apre = EcEC1 pS(adp,c) is calculated from
the source language. This constraint set is non-
convex in A due to the bilinearity of the bi-
gram term. To simplify optimization6 we take an
</bodyText>
<footnote confidence="0.981766571428571">
5The rationale is that we want to put a lower bound on the
number of verbs per sentence, and induce it from the source
language. Furthermore, we expect the number of verbs to be
well concentrated, and we induce its maximal variance from
the source language.
6In Section 5 we shall see that this makes optimization eas-
ier.
</footnote>
<page confidence="0.992097">
1371
</page>
<bodyText confidence="0.872861428571429">
The overall objective is then: F(A) = Fα(A) +
A•H[A], where A is the weight of the entropy term.7
The resulting optimization problem is:
min F(A) (7)
AEA
where A is the set of non-negative matrices whose
columns sum to one:
</bodyText>
<equation confidence="0.974671333333333">
� �
A = A : A�K f) ? 0 bc, f (8)
Pc=1 A(c|f) = 1 bf
</equation>
<sectionHeader confidence="0.985721" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999496">
In this section we describe the parameter estimation
process for our model. We start by describing how
to optimize A. Next, we discuss the weight selec-
tion algorithm, and finally the method for choosing
source languages.
</bodyText>
<subsectionHeader confidence="0.991537">
5.1 Optimizing the Mapping A
</subsectionHeader>
<bodyText confidence="0.99956795">
Recall that our goal is to solve the optimization
problem in Eq. (7). This objective is non convex
since the function H[A] is concave, and the objec-
tive F(A) involves bilinear terms in A and loga-
rithms of their sums (see Equations (1) and (2)).
While we do not attempt to solve the problem
globally, we do have a simple update scheme that
monotonically decreases the objective. The update
can be derived in a similar manner to expectation
maximization (EM) (Neal and Hinton, 1999) and
convex concave procedures (Yuille and Rangarajan,
2003). Figure 2 describes our optimization algo-
rithm. The key ideas in deriving it are using pos-
terior distributions as in EM, and using a variational
formulation of entropy. The term Fc(A) is handled
in a similar way to the posterior regularization algo-
rithm derivation. A detailed derivation is provided
in the supplementary file. 8
The kth iteration of the algorithm involves several
steps:
</bodyText>
<listItem confidence="0.993521666666667">
• In step 1, we calculate the current esti-
mate of the bigram distribution over tags,
pT(c1, c2; Ak).
</listItem>
<footnote confidence="0.99643925">
7Note that as A --+ oo, only valid maps will be selected by
the objective.
8The supplementary file is available at http://groups.
csail.mit.edu/rbg/code/unitag/emnlp2012.
</footnote>
<bodyText confidence="0.8426745">
approach inspired by the posterior regularization
method (Ganchev et al., 2010) and use the objective:
</bodyText>
<equation confidence="0.877949333333333">
Fc(A) = min
r(c1,c2)ESpre DKL[r(c1, c2)|pT (c1, c2; A)]
(4)
</equation>
<bodyText confidence="0.9990325">
The above objective will attain lower values for A
such that pT(c1, c2; A) is close to the constraint set.
Specifically, it will have a value of zero when the
bigram distribution induced by A has the property
specified in Spre. We similarly define a set Spost
for post-positional languages.
As a second typological feature, we consider the
Demonstrative-Noun ordering. In DN languages we
want the probability of a determiner to come be-
fore C2 = {noun, adj, num}, (i.e., frequent universal
noun-phrase tags), to cross a threshold. This con-
straint translates to:
</bodyText>
<equation confidence="0.995623">
X pT(det, c) ? adet (5)
cEC2
</equation>
<bodyText confidence="0.9999474">
where adet = PcEC2 pS(det, c) is a threshold de-
termined from the source language. We denote the
set of distributions that have this property by SDN,
and add them to the constraint in (4). The overall
constraint set is denoted by S.
</bodyText>
<subsectionHeader confidence="0.865773">
4.2 The Overall Objective
</subsectionHeader>
<bodyText confidence="0.992628">
We have defined a set of functions Fi(A) that are
expected to have low values for good mappings. To
combine those, we use a weighted sum: Fα(A) =
</bodyText>
<equation confidence="0.530115">
P
</equation>
<bodyText confidence="0.995353">
i αi • Fi(A). (The weights in this equation are
learned; we discussed the procedure in Section 5)
Optimizing over the set of mappings is difficult
since each mapping is a discrete set whose size is
exponential size in LT. Technically, the difficulty
comes from the requirement that elements of A are
integral and its columns sum to one. To relax this
restriction, we will allow A(c|f) E [0, 1] and en-
courage A to correspond to a mapping by adding an
entropy regularization term:
</bodyText>
<equation confidence="0.9008695">
H[A] = − X X A(c|f) log A(c|f) (6)
f c
</equation>
<bodyText confidence="0.9987585">
This term receives its minimal value when the con-
ditional probability of the universal tags given a
language-specific tag is 1 for one universal tag and
zero for the others.
</bodyText>
<page confidence="0.960229">
1372
</page>
<listItem confidence="0.992738681818182">
• In step 2, we find the bigram distribution in
the constraint set S that is closest in KL di-
vergence to pT(c1, c2i Ak), and denote it by
rk(c1, c2). This optimization problem is con-
vex in r(c1, c2).
• In step 3, we calculate the bigram posterior
over language specific tags given a pair of uni-
versal tags. This is analogous to the standard
E-step in EM.
• In step 4, we use the posterior in step 3 and the
bigram distributions pS(c1, c2) and rk(c1, c2)
to obtain joint counts over language specific
and universal bigrams.
• In step 5, we use the joint counts from step 4
to obtain counts over pairs of language specific
and universal tags.
• In step 6, analogous to the M-step in EM, we
optimize over the mapping matrix A. The ob-
jective is similar to the Q function in EM, and
also includes the FTerb(A) term, and a linear
upper bound on the entropy term. The objec-
tive can be seen to be convex in A.
</listItem>
<bodyText confidence="0.999985928571429">
As mentioned above, each of the optimization prob-
lems in steps 2 and 6 is convex, and can therefore be
solved using standard convex optimization solvers.
Here, we use the CVX package (Grant and Boyd,
2008; Grant and Boyd, 2011). It can be shown that
the algorithm improves F (A) at every iteration and
converges to a local optimum.
The above algorithm generates a mapping A that
may contain fractional entries. To turn it into a hard
mapping we round A by mapping each f to the c
that maximizes A(cIf) and then perform greedy im-
provement steps (one f at a time) to further improve
the objective. The regularization constant λ is tuned
to minimize the Fα(A) value of the rounded A.
</bodyText>
<subsectionHeader confidence="0.999029">
5.2 Learning the Objective Weights
</subsectionHeader>
<bodyText confidence="0.998741833333333">
Our Fα(A) objective is a weighted sum of the in-
dividual Fi(A) functions. In the following, we de-
scribe how to learn the αi weights for every target
language. We would like Fα(A) to have low values
when A is a good map. Since our performance goal
is parsing accuracy, we consider a map to be good
</bodyText>
<figureCaption confidence="0.99851775">
Figure 2: An iterative algorithm for minimizing our ob-
jective in Eq. (7). For simplicity we assume that all the
weights αi and λ are equal to one. It can be shown that
the objective monotonically decreases in every iteration.
</figureCaption>
<bodyText confidence="0.9996305">
if it results in high parsing accuracy, as measured
when projecting a parser from to S to T.
Since we do not have annotated parses in T, we
use the other source languages S = {S1, ... , Sn}
to learn the weight. For each Si as the target, we
first train a parser for each language in S \ {Si} as
if it was the source, using the map of Petrov et al.
(2011), and choose S∗i E S \ {Si} which gives the
highest parsing accuracy on Si. Next we generate
7000 candidate mappings for Si by randomly per-
turbing the map of (Petrov et al., 2011). We evalu-
ate the quality of each candidate A by projecting the
parser of S∗i to Si, and recording the parsing accu-
racy. Among all the candidates we choose the high-
est accuracy one and denote it by A∗(Si). We now
want the score F (A∗(Si)) to be lower than that of all
other candidates. To achieve this, we train a ranking
SVM whose inputs are pairs of maps A∗(Si) and an-
</bodyText>
<figure confidence="0.968176222222222">
Initialize A0.
Repeat
Step 1(calculate current bigram estimate):
�pT (c1, c2; Ak) = Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
f1,f2
Step 2 (incorporate constraints):
rk(c1, c2) = arg min DKL[r(c1, c2)|pT (c1, c2; Ak)]
rES
Step 3 (calculate model posterior):
p(f1, f2|c1, c2; Ak) ∝ Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 4: (complete joint counts):
( )
Nk(c1, c2, f1, f2) = p(f1, f2|c1, c2; Ak) rk(c1, c2) + pS(c1, c2)
Step 5 (obtain pairwise):
Mk(c, f) = Nk1 (c, f) + Nk2 (c, f)
where Nk1 (c, f) = Ec2,f2 Nk(c,c2, f, f2) and similarly for
Nk2 (c,
f).
Step 6 (M step with entropy linearization): Set Ak+1 to be the
solution of
[ ]
Mk(c, f) log A(c|f) + A(c|f) log Ak(c|f) + F—b(A)
Until Convergence of Ak
min
AEΔ
F−
c,f
</figure>
<page confidence="0.98503">
1373
</page>
<bodyText confidence="0.999990857142857">
other worse A(Si). These map pairs are taken from
many different traget languages, i.e. many different
Si. The features given to the SVM are the terms of
the score Fi(A). The goal of the SVM is to weight
these terms such that the better map A*(Si) has a
lower score. The weights assigned by the SVM are
taken as αi.
</bodyText>
<subsectionHeader confidence="0.999444">
5.3 Source Language Selection
</subsectionHeader>
<bodyText confidence="0.999919857142857">
As noted in Section 4 we construct F(A) by choos-
ing a single source language S. Here we describe the
method for choosing S. Our goal is to choose S that
is closest to T in terms of typology. Assume that
languages are described by binary typological vec-
tors vL. We would like to learn a diagonal matrix
D such that d(S, T; D) = (vS − vT)T D(vS − vT)
reflects the similarity between the languages. In our
context, a good measure of similarity is the perfor-
mance of a parser trained on S and projected on T
(using the optimal map A). We thus seek a matrix
D such that d(S, T; D) is ranked according to the
parsing accuracy. The matrix D is trained using an
SVM ranking algorithm that tries to follow the rank-
ing of parsing accuracy. Similar to the technique for
learning the objective weights, we train across many
pairs of source languages.9
The typological features we use are a subset
of the features described in “The World Atlas of
Languages Structure” (WALS, (Haspelmath et al.,
2005)), and are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="method">
6 Evaluation Set-Up
</sectionHeader>
<bodyText confidence="0.999890785714286">
Datasets We test our model on 19 languages: Ara-
bic, Basque, Bulgarian, Catalan, Chinese, Czech,
Danish, Dutch, English, German, Greek, Hungar-
ian, Italian, Japanese, Portuguese, Slovene, Span-
ish, Swedish, and Turkish. Our data is taken from
the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al., 2007). The
CoNLL datasets consist of manually created depen-
dency trees and language-specific POS tags. Fol-
lowing Petrov et al. (2011), our model maps these
language-specific tags to a set of 12 universal tags:
noun, verb, adjective, adverb, pronoun, determiner,
adposition, numeral, conjunction, particle, punctua-
tion mark and X (a general tag).
</bodyText>
<footnote confidence="0.569817">
9Ties are broken using the F(A) objective.
</footnote>
<bodyText confidence="0.999857857142857">
Evaluation Procedure We perform a separate ex-
periment for each of the 19 languages as the tar-
get and a source language chosen from the rest (us-
ing the method from Section 5.3). For the selected
source language, we assume access to the mapping
of Petrov et al. (2011).
Evaluation Measures We evaluate the quality of
the derived mapping in the context of the target lan-
guage parsing accuracy. In both the training and
test data, the language-specific tags are replaced
with universal tags: Petrov’s tags for the source lan-
guages and learned tags for the target language. We
train two non-lexicalized parsers using source anno-
tations and apply them to the target language. The
first parser is a non-lexicalized version of the MST
parser (McDonald et al., 2005) successfully used in
the multilingual context (McDonald et al., 2011). In
the second parser, parameters of the target language
are estimated as a weighted mixture of parameters
learned from supervised source languages (Cohen et
al., 2011). For the parser of Cohen et al. (2011), we
trained the model on the four languages used in the
original paper — English, German, Czech and Ital-
ian. When measuring the performance on each of
these four languages, we selected another set of four
languages with a similar level of diversity.10
Following the standard evaluation practice in
parsing, we use directed dependency accuracy as our
measure of performance.
Baselines We compare mappings induced by our
model against three baselines: the manually con-
structed mapping of Petrov et al. (2011), a randomly
constructed mapping and a greedy mapping. The
greedy mapping uses the same objective as our full
model, but optimizes it using a greedy method. In
each iteration, this method makes |LT |passes over
the language-specific tags, selecting a substitution
that contributes the most to the objective.
Initialization To reduce the dimension of our al-
gorithm’s search space and speed up our method, we
start by clustering the language-specific POS tags of
the target into |K |= 12 clusters using an unsuper-
</bodyText>
<footnote confidence="0.69929275">
10We also experimented with a version of the Cohen et al.
(2011) model trained on all the source languages. This set-up
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
</footnote>
<page confidence="0.993006">
1374
</page>
<note confidence="0.998844166666667">
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and after
</note>
<tableCaption confidence="0.987745">
Table 1: The set of typological features that we use for source language selection. The first column gives the ID of
the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable
values for each feature; besides these values each feature can also have a value of ‘No dominant order’.
</tableCaption>
<bodyText confidence="0.998801833333333">
vised POS induction algorithm (Lee et al., 2010).11
Our mapping algorithm then learns the connection
between these clusters and universal tags.
For initialization, we perform multiple random
restarts and select the one with the lowest final ob-
jective score.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999976777777778">
We first present the results of our model using the
gold POS tags for the target language. Table 2 sum-
marizes the performance of our model and the base-
lines.
Comparison against Baselines On average, the
mapping produced by our model yields parsers with
higher accuracy than all of the baselines. These re-
sults are consistent for both parsers (McDonald et
al., 2011; Cohen et al., 2011). As expected, random
mappings yield abysmal results — 20.2% and 12.7%
for the two parsers. The low accuracy of parsers that
rely on the Greedy mapping — 29.9% and 25.4% —
show that a greedy approach is a poor strategy for
mapping optimization.
Surprisingly, our model slightly outperforms the
mapping of (Petrov et al., 2011), yielding an aver-
age accuracy of 56.7% as compared to the 55.4%
achieved by its manually constructed counterpart for
the direct transfer method (McDonald et al., 2011).
Similar results are observed for the mixture weights
parser (Cohen et al., 2011). The main reason for
these differences comes from mistakes introduced in
the manual mapping. For example, in Czech tag “R”
is labeled as “pronoun”, while actually it should be
mapped to “adposition”. By correcting this mistake,
we gain 5% in parsing accuracy for the direct trans-
fer parser.
</bodyText>
<footnote confidence="0.891073">
11This pre-clustering results in about 3% improvement, pre-
sumably since it uses contextual information beyond what our
algorithm does.
</footnote>
<bodyText confidence="0.98983527027027">
Overall, the manually constructed mapping and
our model’s output disagree on 21% of the assign-
ments (measured on the token level). However,
the extent of disagreement is not necessarily predic-
tive of the difference in parsing performance. For
instance, the manual and automatic mappings for
Catalan disagree on 8% of the tags and their pars-
ing accuracy differs by 5%. For Greek on the other
hand, the disagreement between mappings is much
higher — 17%, yet the parsing accuracy is very
close. This phenomenon shows that not all mistakes
have equal weight. For instance, a confusion be-
tween “pronoun” and “noun” is less severe in the
parsing context than a confusion between “pronoun”
and “adverb”.
Impact of Language Selection To assess the
quality of our language selection method, we com-
pare the model against an oracle that selects the best
source for a given target language. As Table 2 shows
our method is very close to the oracle performance,
with only 0.7% gap between the two. In fact, for
10 languages our method correctly predicts the best
pairing. This result is encouraging in other contexts
as well. Specifically, McDonald et al. (2011) have
demonstrated that projecting from a single oracle-
chosen language can lead to good parsing perfor-
mance, and our technique may allow such projection
without an oracle.
Relations between Objective Values and Opti-
mization Performance The suboptimal perfor-
mance of the Greedy method shows that choosing
a good optimization strategy plays a critical role in
finding the desired mapping. A natural question to
ask is whether the objective value is predictive of the
end goal parsing performance. Figure 3 shows the
objective values for the mappings computed by our
method and the baselines for four languages. Over-
</bodyText>
<page confidence="0.964025">
1375
</page>
<table confidence="0.999963580645161">
Direct Transfer Parser (Accuracy) Mixture Weight Parser (Accuracy) Tag Diff.
Random Greedy Petrov Model Best Pair Random Greedy Petrov Model.
Catalan 15.9 32.5 74.8 79.3 79.3 12.6 24.6 65.6 73.9 8.8
Italian
Portuguese
Spanish
16.4 41.0 68.7 68.3 71.4 11.7 33.5 64.2 61.9 6.7
15.8 24.6 72.0 75.1 75.1 10.7 14.1 70.4 72.6 12.2
11.5 27.4 72.1 68.9 68.9 6.4 26.5 58.8 62.8 7.5
Danish 35.5 23.7 46.6 46.5 49.2 4.2 23.7 51.4 51.7 5.0
Dutch
English
German
Swedish
18.0 22.1 58.2 56.8 57.3 7.1 15.3 54.9 53.2 4.9
14.7 19.0 51.6 49.0 49.0 13.3 15.1 47.5 41.8 17.7
15.8 24.3 55.7 50.4 51.6 20.9 18.7 52.4 51.8 15.0
15.1 26.3 63.1 63.1 63.1 9.1 36.5 55.7 55.9 8.2
Bulgarian 17.4 28.0 51.6 63.4 63.4 22.6 39.9 64.6 60.4 35.7
Czech
Slovene
19.0 34.4 47.7 57.3 57.3 12.7 26.2 48.3 55.7 28.5
15.6 21.8 43.5 51.4 52.8 11.3 20.7 42.2 53.0 38.8
Greek 17.3 19.5 62.3 59.7 59.8 22.0 15.2 56.2 57.0 17.0
Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1
Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7
Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8
Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1
Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8
Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9
Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3
</table>
<tableCaption confidence="0.994965">
Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.
</tableCaption>
<bodyText confidence="0.956690243902439">
The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011). The second section
is for the weighted mixture parsing model (Cohen et al., 2011). The first two columns (Random and Greedy) of each
section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the
results when the mapping of Petrov et al. (2011) is used. The fourth column (Model) shows the results when our
mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best
source language is selected for every target language. The last column (Tag Diff.) presents the difference between our
mapping and the mapping of Petrov et al. (2011) by showing the percentage of target language tokens for which the
two mappings select a different universal tag.
all, our method and the manual mappings reach sim-
ilar values, both considerably better than other base-
lines. While the parsing performance correlates with
the objective, the correlation is not perfect. For in-
stance, on Greek our mapping has a better objective
value, but lower parsing performance.
Ablation Analysis We next analyze the contribu-
tion of each component of our objective to the result-
ing performance.12 The strongest factor in our ob-
jective is the distributional features capturing global
statistics. Using these features alone achieves an
average accuracy of 51.1%, only 5.6% less than
the full model score. Adding just the verb-related
constraints to the distributional similarity objectives
improves the average model performance by 2.1%.
12The results are consistent for both parsers, here we report
the accuracy for the direct transfer method (McDonald et al.,
2011).
Adding just the typological constraints yields a very
modest performance gain of 0.5%. This is not sur-
prising — the source language is selected to be typo-
logically similar to the target language, and thus its
distributional properties are consistent with typolog-
ical features. However, adding both the verb-related
constraints and the typological constraints results in
a synergistic performance gain of 5.6% over the dis-
tributional similarity objective, a gain which is much
better than the sum of the two individual gains.
Application to Automatically Induced POS Tags
A potential benefit of the proposed method is to re-
late automatically induced clusters in the target lan-
guage to universal tags. In our experiments, we in-
duce such clusters using Brown clustering,13 which
</bodyText>
<footnote confidence="0.971681666666667">
13In our experiments, we employ Liang’s implementation
http://cs.stanford.edu/∼pliang/software/. The number of clus-
ters is set to 30.
</footnote>
<page confidence="0.986252">
1376
</page>
<figureCaption confidence="0.9984155">
Figure 3: Objective values for the different mappings
used in our experiments for four languages. Note that
the goal of the optimization procedure is to minimize the
objective value.
</figureCaption>
<bodyText confidence="0.999178375">
has been successfully used for similar purposes in
parsing research (Koo et al., 2008). We then map
these clusters to the universal tags using our algo-
rithm.
The average parsing accuracy on the 19 languages
is 45.5%. Not surprisingly, automatically induced
tags negatively impact parsing performance, yield-
ing a decrease of 11% when compared to mappings
obtained using manual POS annotations (see Ta-
ble 2). To further investigate the impact of inaccu-
rate tags on the mapping performance, we compare
our model against the oracle mapping model that
maps each cluster to the most common universal tag
of its members. Parsing accuracy obtained using this
method is 45.1%, closely matching the performance
of our mapping algorithm.
An alternative approach to mapping words into
universal tags is to directly partition words into K
clusters (without passing through language specific
tags). In order for these clusters to be meaningful
as universal tags, we can provide several prototypes
for each cluster (e.g., “walk” is a verb etc.). To test
this approach we used the prototype driven tagger of
Haghighi and Klein (2006) with 15 prototypes per
universal tag.14 The resulting universal tags yield
an average parsing accuracy of 40.5%. Our method
(using Brown clustering as above) outperforms this
14Oracle prototypes were obtained by taking the 15 most
frequent words for each universal tag. This yields almost the
same total number of prototypes as those in the experiment of
(Haghighi and Klein, 2006).
baseline by about 5%.
</bodyText>
<sectionHeader confidence="0.999072" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999980928571429">
We present an automatic method for mapping
language-specific part-of-speech tags to a set of uni-
versal tags. Our work capitalizes on manually de-
signed conversion schemes to automatically create
mappings for new languages. Our experimental re-
sults demonstrate that automatically induced map-
pings rival the quality of their hand-crafted coun-
terparts. We also establish that the mapping quality
has a significant impact on the accuracy of syntactic
transfer, which motivates further study of this topic.
Finally, our experiments show that the choice of
mapping optimization scheme plays a crucial role in
the quality of the derived mapping, highlighting the
importance of optimization for the mapping task.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999988333333333">
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-1-
0533) and the DARPA BOLT program. We thank
Tommi Jaakkola, the members of the MIT NLP
group and the ACL reviewers for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999019842105263">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149–164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877–886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50–61.
Frank Van Eynde. 2004. Part of speech tagging en lem-
matisering van het corpus gesproken nederlands. In
Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272–279.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 11:2001–2049.
</reference>
<figure confidence="0.9905326875">
Catalan German Greek
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
Arabic
Model
Petrov
Greedy
Random
</figure>
<page confidence="0.955875">
1377
</page>
<reference confidence="0.999881376623377">
Michael Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman &amp; Co.
Michael C. Grant and Stephen P. Boyd. 2008. Graph im-
plementations for nonsmooth convex programs. In Re-
cent Advances in Learning and Control, Lecture Notes
in Control and Information Sciences, pages 95–110.
Springer-Verlag Limited.
Michael C. Grant and Stephen P. Boyd. 2011. CVX:
Matlab software for disciplined convex programming,
version 1.21.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320–327.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Journal of Natural Language Engineering, 11:311–
325.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423–430.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595–603.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging. In
Proceedings of EMNLP, pages 853–861.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchi-
cal dirichlet processes. In Proceedings of EMNLP-
CoNLL, pages 688–697.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of ACL, pages 75–82.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP, pages 523–530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP, pages 62–72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP, pages 1234–1244.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view
of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models, pages 355–368. MIT Press.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915–932.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404–411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
COLING, pages 433–440.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851–858.
Alan Yuille and Anand Rangarajan. 2003. The concave-
convex procedure (cccp). In Proceedings of Neural
Computation, volume 15, pages 915–936.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35–42.
</reference>
<page confidence="0.993882">
1378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418948">
<title confidence="0.999849">Learning to Map into a Universal POS Tagset</title>
<author confidence="0.978871">Yuan Zhang</author>
<author confidence="0.978871">Roi Reichart</author>
<author confidence="0.978871">Regina Barzilay Amir Globerson</author>
<affiliation confidence="0.588832">Massachusetts Institute of Technology The Hebrew</affiliation>
<email confidence="0.982849">roiri,gamir@cs.huji.ac.il</email>
<abstract confidence="0.986750260869565">We present an automatic method for mapping tags to a set This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the of multilingual</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="7711" citStr="Buchholz and Marsi, 2006" startWordPosition="1177" endWordPosition="1180">al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they are not identical and yield different parsing performance (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks (here we focus on parsing). As our results show, we achieve this goal 1369 0.35 0.3 Unigram Frequency 0.25 0.2 0.15 0.1 0.05 0 English German Noun Verb Det. Prep. Adj. -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases] and sales [by c</context>
<context position="24820" citStr="Buchholz and Marsi, 2006" startWordPosition="4183" endWordPosition="4187">rsing accuracy. Similar to the technique for learning the objective weights, we train across many pairs of source languages.9 The typological features we use are a subset of the features described in “The World Atlas of Languages Structure” (WALS, (Haspelmath et al., 2005)), and are shown in Table 1. 6 Evaluation Set-Up Datasets We test our model on 19 languages: Arabic, Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovene, Spanish, Swedish, and Turkish. Our data is taken from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The CoNLL datasets consist of manually created dependency trees and language-specific POS tags. Following Petrov et al. (2011), our model maps these language-specific tags to a set of 12 universal tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark and X (a general tag). 9Ties are broken using the F(A) objective. Evaluation Procedure We perform a separate experiment for each of the 19 languages as the target and a source language chosen from the rest (using the method from Section 5.3). For the selected s</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>877--886</pages>
<contexts>
<context position="6929" citStr="Burkett and Klein, 2008" startWordPosition="1057" endWordPosition="1060">comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In p</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of EMNLP, pages 877–886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>50--61</pages>
<contexts>
<context position="7116" citStr="Cohen et al., 2011" startWordPosition="1084" endWordPosition="1087"> further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Niv</context>
<context position="26221" citStr="Cohen et al., 2011" startWordPosition="4411" endWordPosition="4414">ing accuracy. In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. We train two non-lexicalized parsers using source annotations and apply them to the target language. The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011). In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directed dependency accuracy as our measure of performance. Baselines We compare mappings induced by our model against three baselines: the manually constructed mapping of Petrov et al. (2011), a randomly constructed mapping and a greedy mapping.</context>
<context position="28884" citStr="Cohen et al., 2011" startWordPosition="4841" endWordPosition="4844"> et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags. For initialization, we perform multiple random restarts and select the one with the lowest final objective score. 7 Results We first present the results of our model using the gold POS tags for the target language. Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al., 2011). As expected, random mappings yield abysmal results — 20.2% and 12.7% for the two parsers. The low accuracy of parsers that rely on the Greedy mapping — 29.9% and 25.4% — show that a greedy approach is a poor strategy for mapping optimization. Surprisingly, our model slightly outperforms the mapping of (Petrov et al., 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al., 2011). Similar results are observed for the mixture weights parser (Cohen et al., 2011). The main reason for th</context>
<context position="33235" citStr="Cohen et al., 2011" startWordPosition="5585" endWordPosition="5588">5 51.2 52.9 3.9 40.9 48.3 51.1 15.7 Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8 Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1 Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8 Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9 Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3 Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language. The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011). The second section is for the weighted mixture parsing model (Cohen et al., 2011). The first two columns (Random and Greedy) of each section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the results when the mapping of Petrov et al. (2011) is used. The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language. The last column (Tag Diff.) presents the difference between our mapping and the mapping of Petrov et al. (2011) by showing the percentage of target lang</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of EMNLP, pages 50–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Van Eynde</author>
</authors>
<title>Part of speech tagging en lemmatisering van het corpus gesproken nederlands. In</title>
<date>2004</date>
<tech>Technical report.</tech>
<marker>Van Eynde, 2004</marker>
<rawString>Frank Van Eynde. 2004. Part of speech tagging en lemmatisering van het corpus gesproken nederlands. In Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="9109" citStr="Finkel et al., 2007" startWordPosition="1403" endWordPosition="1406"> Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by map</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of ACL, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>JMLR,</journal>
<pages>11--2001</pages>
<contexts>
<context position="17608" citStr="Ganchev et al., 2010" startWordPosition="2851" endWordPosition="2854">M, and using a variational formulation of entropy. The term Fc(A) is handled in a similar way to the posterior regularization algorithm derivation. A detailed derivation is provided in the supplementary file. 8 The kth iteration of the algorithm involves several steps: • In step 1, we calculate the current estimate of the bigram distribution over tags, pT(c1, c2; Ak). 7Note that as A --+ oo, only valid maps will be selected by the objective. 8The supplementary file is available at http://groups. csail.mit.edu/rbg/code/unitag/emnlp2012. approach inspired by the posterior regularization method (Ganchev et al., 2010) and use the objective: Fc(A) = min r(c1,c2)ESpre DKL[r(c1, c2)|pT (c1, c2; A)] (4) The above objective will attain lower values for A such that pT(c1, c2; A) is close to the constraint set. Specifically, it will have a value of zero when the bigram distribution induced by A has the property specified in Spre. We similarly define a set Spost for post-positional languages. As a second typological feature, we consider the Demonstrative-Noun ordering. In DN languages we want the probability of a determiner to come before C2 = {noun, adj, num}, (i.e., frequent universal noun-phrase tags), to cross</context>
</contexts>
<marker>Ganchev, Graca, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. JMLR, 11:2001–2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Garey</author>
<author>David S Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NPCompleteness.</title>
<date>1979</date>
<contexts>
<context position="5095" citStr="Garey and Johnson, 1979" startWordPosition="772" endWordPosition="775">perties of the target language that specify likely tag sequences. This information is readily available even for resource poor language (Haspelmath et al., 2005). For instance, since English and German are prepositional languages, we expect to observe adposition-noun sequences but not the reverse (see Figure 1b for sample sentences). We encode these heterogeneous properties into an objective function that guides the search for the optimal mapping. Having defined a quality measure for mappings, our goal is to find the optimal mapping. However, such partition optimization problems2 are NP hard (Garey and Johnson, 1979). A naive approach to the problem is to greedily improve the map, but it turns out that this approach yields poor quality mappings. We therefore develop a method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. We construct the objective in a way that facilitates simple monotonically improving updates corresponding to solving convex optimization problems. We evaluate our mapping approach on 19 languages that include representatives of IndoEuropean, Semitic, Basque, Japonic and Turkic families. We measure mapping quality based on the target</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Michael Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman &amp; Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Grant</author>
<author>Stephen P Boyd</author>
</authors>
<title>Graph implementations for nonsmooth convex programs.</title>
<date>2008</date>
<booktitle>In Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,</booktitle>
<pages>95--110</pages>
<publisher>Springer-Verlag Limited.</publisher>
<contexts>
<context position="20491" citStr="Grant and Boyd, 2008" startWordPosition="3380" endWordPosition="3383">uage specific and universal bigrams. • In step 5, we use the joint counts from step 4 to obtain counts over pairs of language specific and universal tags. • In step 6, analogous to the M-step in EM, we optimize over the mapping matrix A. The objective is similar to the Q function in EM, and also includes the FTerb(A) term, and a linear upper bound on the entropy term. The objective can be seen to be convex in A. As mentioned above, each of the optimization problems in steps 2 and 6 is convex, and can therefore be solved using standard convex optimization solvers. Here, we use the CVX package (Grant and Boyd, 2008; Grant and Boyd, 2011). It can be shown that the algorithm improves F (A) at every iteration and converges to a local optimum. The above algorithm generates a mapping A that may contain fractional entries. To turn it into a hard mapping we round A by mapping each f to the c that maximizes A(cIf) and then perform greedy improvement steps (one f at a time) to further improve the objective. The regularization constant λ is tuned to minimize the Fα(A) value of the rounded A. 5.2 Learning the Objective Weights Our Fα(A) objective is a weighted sum of the individual Fi(A) functions. In the followin</context>
</contexts>
<marker>Grant, Boyd, 2008</marker>
<rawString>Michael C. Grant and Stephen P. Boyd. 2008. Graph implementations for nonsmooth convex programs. In Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95–110. Springer-Verlag Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Grant</author>
<author>Stephen P Boyd</author>
</authors>
<title>CVX: Matlab software for disciplined convex programming, version 1.21.</title>
<date>2011</date>
<contexts>
<context position="20514" citStr="Grant and Boyd, 2011" startWordPosition="3384" endWordPosition="3387">ersal bigrams. • In step 5, we use the joint counts from step 4 to obtain counts over pairs of language specific and universal tags. • In step 6, analogous to the M-step in EM, we optimize over the mapping matrix A. The objective is similar to the Q function in EM, and also includes the FTerb(A) term, and a linear upper bound on the entropy term. The objective can be seen to be convex in A. As mentioned above, each of the optimization problems in steps 2 and 6 is convex, and can therefore be solved using standard convex optimization solvers. Here, we use the CVX package (Grant and Boyd, 2008; Grant and Boyd, 2011). It can be shown that the algorithm improves F (A) at every iteration and converges to a local optimum. The above algorithm generates a mapping A that may contain fractional entries. To turn it into a hard mapping we round A by mapping each f to the c that maximizes A(cIf) and then perform greedy improvement steps (one f at a time) to further improve the objective. The regularization constant λ is tuned to minimize the Fα(A) value of the rounded A. 5.2 Learning the Objective Weights Our Fα(A) objective is a weighted sum of the individual Fi(A) functions. In the following, we describe how to l</context>
</contexts>
<marker>Grant, Boyd, 2011</marker>
<rawString>Michael C. Grant and Stephen P. Boyd. 2011. CVX: Matlab software for disciplined convex programming, version 1.21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="37009" citStr="Haghighi and Klein (2006)" startWordPosition="6183" endWordPosition="6186">ompare our model against the oracle mapping model that maps each cluster to the most common universal tag of its members. Parsing accuracy obtained using this method is 45.1%, closely matching the performance of our mapping algorithm. An alternative approach to mapping words into universal tags is to directly partition words into K clusters (without passing through language specific tags). In order for these clusters to be meaningful as universal tags, we can provide several prototypes for each cluster (e.g., “walk” is a verb etc.). To test this approach we used the prototype driven tagger of Haghighi and Klein (2006) with 15 prototypes per universal tag.14 The resulting universal tags yield an average parsing accuracy of 40.5%. Our method (using Brown clustering as above) outperforms this 14Oracle prototypes were obtained by taking the 15 most frequent words for each universal tag. This yields almost the same total number of prototypes as those in the experiment of (Haghighi and Klein, 2006). baseline by about 5%. 8 Conclusions We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. Our work capitalizes on manually designed conversion schemes to automat</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Haspelmath</author>
<author>Matthew S Dryer</author>
<author>David Gil</author>
<author>Bernard Comrie</author>
<author>editors</author>
</authors>
<title>The World Atlas of Language Structures.</title>
<date>2005</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4632" citStr="Haspelmath et al., 2005" startWordPosition="699" endWordPosition="703">play the same linguistic role in source and target languages, we expect similarity in their global distributional statistics. Figure 1a shows statistics for two close languages, English and German. We can see that their unigram frequencies on the five most common tags are very close. Other properties concern POS tag per sentence statistics – e.g., every sentence has to have at least one verb. Finally, the mappings can be further constrained by typological properties of the target language that specify likely tag sequences. This information is readily available even for resource poor language (Haspelmath et al., 2005). For instance, since English and German are prepositional languages, we expect to observe adposition-noun sequences but not the reverse (see Figure 1b for sample sentences). We encode these heterogeneous properties into an objective function that guides the search for the optimal mapping. Having defined a quality measure for mappings, our goal is to find the optimal mapping. However, such partition optimization problems2 are NP hard (Garey and Johnson, 1979). A naive approach to the problem is to greedily improve the map, but it turns out that this approach yields poor quality mappings. We th</context>
<context position="24469" citStr="Haspelmath et al., 2005" startWordPosition="4127" endWordPosition="4130">imilarity between the languages. In our context, a good measure of similarity is the performance of a parser trained on S and projected on T (using the optimal map A). We thus seek a matrix D such that d(S, T; D) is ranked according to the parsing accuracy. The matrix D is trained using an SVM ranking algorithm that tries to follow the ranking of parsing accuracy. Similar to the technique for learning the objective weights, we train across many pairs of source languages.9 The typological features we use are a subset of the features described in “The World Atlas of Languages Structure” (WALS, (Haspelmath et al., 2005)), and are shown in Table 1. 6 Evaluation Set-Up Datasets We test our model on 19 languages: Arabic, Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovene, Spanish, Swedish, and Turkish. Our data is taken from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The CoNLL datasets consist of manually created dependency trees and language-specific POS tags. Following Petrov et al. (2011), our model maps these language-specific tags to a set of 12 universal tags: noun, verb, adjective, </context>
</contexts>
<marker>Haspelmath, Dryer, Gil, Comrie, editors, 2005</marker>
<rawString>Martin Haspelmath, Matthew S. Dryer, David Gil, and Bernard Comrie, editors. 2005. The World Atlas of Language Structures. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<pages>325</pages>
<contexts>
<context position="6885" citStr="Hwa et al., 2005" startWordPosition="1049" endWordPosition="1052"> to the objective, our largest gain comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Journal of Natural Language Engineering, 11:311– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="9134" citStr="Klein and Manning, 2003" startWordPosition="1407" endWordPosition="1410">on of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other l</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="35978" citStr="Koo et al., 2008" startWordPosition="6018" endWordPosition="6021">ags A potential benefit of the proposed method is to relate automatically induced clusters in the target language to universal tags. In our experiments, we induce such clusters using Brown clustering,13 which 13In our experiments, we employ Liang’s implementation http://cs.stanford.edu/∼pliang/software/. The number of clusters is set to 30. 1376 Figure 3: Objective values for the different mappings used in our experiments for four languages. Note that the goal of the optimization procedure is to minimize the objective value. has been successfully used for similar purposes in parsing research (Koo et al., 2008). We then map these clusters to the universal tags using our algorithm. The average parsing accuracy on the 19 languages is 45.5%. Not surprisingly, automatically induced tags negatively impact parsing performance, yielding a decrease of 11% when compared to mappings obtained using manual POS annotations (see Table 2). To further investigate the impact of inaccurate tags on the mapping performance, we compare our model against the oracle mapping model that maps each cluster to the most common universal tag of its members. Parsing accuracy obtained using this method is 45.1%, closely matching t</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised pos tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>853--861</pages>
<contexts>
<context position="28279" citStr="Lee et al., 2010" startWordPosition="4741" endWordPosition="4744">s, Inpositions 86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive 87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective 88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and after Table 1: The set of typological features that we use for source language selection. The first column gives the ID of the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable values for each feature; besides these values each feature can also have a value of ‘No dominant order’. vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags. For initialization, we perform multiple random restarts and select the one with the lowest final objective score. 7 Results We first present the results of our model using the gold POS tags for the target language. Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al., </context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised pos tagging. In Proceedings of EMNLP, pages 853–861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite pcfg using hierarchical dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="9224" citStr="Liang et al., 2007" startWordPosition="1423" endWordPosition="1426">s on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a target corpus written in </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The infinite pcfg using hierarchical dirichlet processes. In Proceedings of EMNLPCoNLL, pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="9158" citStr="Matsuzaki et al., 2005" startWordPosition="1411" endWordPosition="1414"> tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formula</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Proceedings of ACL, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="25982" citStr="McDonald et al., 2005" startWordPosition="4375" endWordPosition="4378">rest (using the method from Section 5.3). For the selected source language, we assume access to the mapping of Petrov et al. (2011). Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy. In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. We train two non-lexicalized parsers using source annotations and apply them to the target language. The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011). In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directe</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>62--72</pages>
<contexts>
<context position="7096" citStr="McDonald et al., 2011" startWordPosition="1080" endWordPosition="1083">ansfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz </context>
<context position="11763" citStr="McDonald et al., 2011" startWordPosition="1876" endWordPosition="1879">ct the frequency of a particular universal tag to be similar in the source and target languages. One choice to make when constructing an objective is the source languages to which we want to be similar. It is clear that choosing all languages is not a good idea, since they are not all expected to have distributional properties similar to the target language. There is strong evidence that projecting from single languages can lead to good parsing performance 3We use c and f to reflect the fact that universal tags are a coarse version (hence c) of the language specific fine tags (hence f). 1370 (McDonald et al., 2011). Therefore, our strategy is to choose a single source language for comparison. The choice of the source language is based on similarity between typological properties; we describe this in detail in Section 5. We must also determine which statistical properties we expect to be preserved across languages. Our model utilizes three linguistic phenomena which are consistent across languages: POS tag global distributional statistics, POS tag per sentence statistics, and typology-based ordering statistics. We define each of these below. 4.1 Mapping Characterization We focus on three categories of ma</context>
<context position="26052" citStr="McDonald et al., 2011" startWordPosition="4385" endWordPosition="4388">uage, we assume access to the mapping of Petrov et al. (2011). Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy. In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. We train two non-lexicalized parsers using source annotations and apply them to the target language. The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011). In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directed dependency accuracy as our measure of performance. Baselines We comp</context>
<context position="28863" citStr="McDonald et al., 2011" startWordPosition="4837" endWordPosition="4840">nduction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags. For initialization, we perform multiple random restarts and select the one with the lowest final objective score. 7 Results We first present the results of our model using the gold POS tags for the target language. Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al., 2011). As expected, random mappings yield abysmal results — 20.2% and 12.7% for the two parsers. The low accuracy of parsers that rely on the Greedy mapping — 29.9% and 25.4% — show that a greedy approach is a poor strategy for mapping optimization. Surprisingly, our model slightly outperforms the mapping of (Petrov et al., 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al., 2011). Similar results are observed for the mixture weights parser (Cohen et al., 2011). T</context>
<context position="31036" citStr="McDonald et al. (2011)" startWordPosition="5195" endWordPosition="5198"> mistakes have equal weight. For instance, a confusion between “pronoun” and “noun” is less severe in the parsing context than a confusion between “pronoun” and “adverb”. Impact of Language Selection To assess the quality of our language selection method, we compare the model against an oracle that selects the best source for a given target language. As Table 2 shows our method is very close to the oracle performance, with only 0.7% gap between the two. In fact, for 10 languages our method correctly predicts the best pairing. This result is encouraging in other contexts as well. Specifically, McDonald et al. (2011) have demonstrated that projecting from a single oraclechosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. Relations between Objective Values and Optimization Performance The suboptimal performance of the Greedy method shows that choosing a good optimization strategy plays a critical role in finding the desired mapping. A natural question to ask is whether the objective value is predictive of the end goal parsing performance. Figure 3 shows the objective values for the mappings computed by our method and the baselines for four la</context>
<context position="33152" citStr="McDonald et al., 2011" startWordPosition="5571" endWordPosition="5574">0 17.0 Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1 Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7 Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8 Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1 Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8 Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9 Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3 Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language. The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011). The second section is for the weighted mixture parsing model (Cohen et al., 2011). The first two columns (Random and Greedy) of each section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the results when the mapping of Petrov et al. (2011) is used. The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language. The last column (Tag Diff.) presents the difference between our mappi</context>
<context position="34792" citStr="McDonald et al., 2011" startWordPosition="5835" endWordPosition="5838">lue, but lower parsing performance. Ablation Analysis We next analyze the contribution of each component of our objective to the resulting performance.12 The strongest factor in our objective is the distributional features capturing global statistics. Using these features alone achieves an average accuracy of 51.1%, only 5.6% less than the full model score. Adding just the verb-related constraints to the distributional similarity objectives improves the average model performance by 2.1%. 12The results are consistent for both parsers, here we report the accuracy for the direct transfer method (McDonald et al., 2011). Adding just the typological constraints yields a very modest performance gain of 0.5%. This is not surprising — the source language is selected to be typologically similar to the target language, and thus its distributional properties are consistent with typological features. However, adding both the verb-related constraints and the typological constraints results in a synergistic performance gain of 5.6% over the distributional similarity objective, a gain which is much better than the sum of the two individual gains. Application to Automatically Induced POS Tags A potential benefit of the </context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of EMNLP, pages 62–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1234--1244</pages>
<contexts>
<context position="2233" citStr="Naseem et al., 2010" startWordPosition="315" endWordPosition="318">nd data for the work presented in this paper is available at http://groups.csail.mit.edu/ rbg/code/unitag/emnlp2012 While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. Despite the apparent simplicity, deriving a mapping is by no means easy, even for humans. In fact, the universal tagsets manually induced by Petrov et al. (2011) and by Naseem et al. (2010) disagree on 10% of the tags. An example of such discrepancy is the mapping of the Japanese tag “PVfin” to the universal tag “particle” according to one scheme, and to “verb” according to another. Moreover, the quality of this conversion has a direct implication on the parsing pe</context>
<context position="7138" citStr="Naseem et al., 2010" startWordPosition="1088" endWordPosition="1091">is topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of EMNLP, pages 1234–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the em algorithm that justifies incremental, sparse, and other variants.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models,</booktitle>
<pages>355--368</pages>
<editor>In Michael I. Jordan, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16808" citStr="Neal and Hinton, 1999" startWordPosition="2727" endWordPosition="2730">timize A. Next, we discuss the weight selection algorithm, and finally the method for choosing source languages. 5.1 Optimizing the Mapping A Recall that our goal is to solve the optimization problem in Eq. (7). This objective is non convex since the function H[A] is concave, and the objective F(A) involves bilinear terms in A and logarithms of their sums (see Equations (1) and (2)). While we do not attempt to solve the problem globally, we do have a simple update scheme that monotonically decreases the objective. The update can be derived in a similar manner to expectation maximization (EM) (Neal and Hinton, 1999) and convex concave procedures (Yuille and Rangarajan, 2003). Figure 2 describes our optimization algorithm. The key ideas in deriving it are using posterior distributions as in EM, and using a variational formulation of entropy. The term Fc(A) is handled in a similar way to the posterior regularization algorithm derivation. A detailed derivation is provided in the supplementary file. 8 The kth iteration of the algorithm involves several steps: • In step 1, we calculate the current estimate of the bigram distribution over tags, pT(c1, c2; Ak). 7Note that as A --+ oo, only valid maps will be se</context>
</contexts>
<marker>Neal, Hinton, 1999</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1999. A view of the em algorithm that justifies incremental, sparse, and other variants. In Michael I. Jordan, editor, Learning in Graphical Models, pages 355–368. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="9203" citStr="Petrov and Klein, 2007" startWordPosition="1419" endWordPosition="1422">gram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a targ</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACLCOLING,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="9179" citStr="Petrov et al., 2006" startWordPosition="1415" endWordPosition="1418">anguages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACLCOLING, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<booktitle>In ArXiv,</booktitle>
<contexts>
<context position="2211" citStr="Petrov et al., 2011" startWordPosition="311" endWordPosition="314">r. 1The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/ rbg/code/unitag/emnlp2012 While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. Despite the apparent simplicity, deriving a mapping is by no means easy, even for humans. In fact, the universal tagsets manually induced by Petrov et al. (2011) and by Naseem et al. (2010) disagree on 10% of the tags. An example of such discrepancy is the mapping of the Japanese tag “PVfin” to the universal tag “particle” according to one scheme, and to “verb” according to another. Moreover, the quality of this conversion has a direct implica</context>
<context position="7841" citStr="Petrov et al., 2011" startWordPosition="1199" endWordPosition="1202">versal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they are not identical and yield different parsing performance (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks (here we focus on parsing). As our results show, we achieve this goal 1369 0.35 0.3 Unigram Frequency 0.25 0.2 0.15 0.1 0.05 0 English German Noun Verb Det. Prep. Adj. -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases] and sales [by corporate insiders] -Einer der sich [für den Milliardär] [ausspricht] [ist] Steve Jobs dem Perot [für den aufbau] der Computerfirma</context>
<context position="10045" citStr="Petrov et al., 2011" startWordPosition="1562" endWordPosition="1565">e approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a target corpus written in a language T, and a set of non-parallel source corpora written in languages {51, ... , 5n}. In the source corpora, each word is annotated with both a language-specific POS tag and a universal POS tag (Petrov et al., 2011). In the target corpus each word is annotated only with a language-specific POS tag, either gold or automatically induced. Our goal is to find a map from the set of LT target language tags to the set of K universal tags. We assume that each language-specific tag is only mapped to one universal tag, which means we never split a language-specific tag and LT &gt; K holds for every language. We represent the map by a matrix A of size K x LT where A(c|f) = 1 if the target language tag f is mapped to the universal tag c, and A(c|f) = 0 otherwise.3 Note that each column of A should contain a single valu</context>
<context position="21879" citStr="Petrov et al. (2011)" startWordPosition="3644" endWordPosition="3647"> accuracy, we consider a map to be good Figure 2: An iterative algorithm for minimizing our objective in Eq. (7). For simplicity we assume that all the weights αi and λ are equal to one. It can be shown that the objective monotonically decreases in every iteration. if it results in high parsing accuracy, as measured when projecting a parser from to S to T. Since we do not have annotated parses in T, we use the other source languages S = {S1, ... , Sn} to learn the weight. For each Si as the target, we first train a parser for each language in S \ {Si} as if it was the source, using the map of Petrov et al. (2011), and choose S∗i E S \ {Si} which gives the highest parsing accuracy on Si. Next we generate 7000 candidate mappings for Si by randomly perturbing the map of (Petrov et al., 2011). We evaluate the quality of each candidate A by projecting the parser of S∗i to Si, and recording the parsing accuracy. Among all the candidates we choose the highest accuracy one and denote it by A∗(Si). We now want the score F (A∗(Si)) to be lower than that of all other candidates. To achieve this, we train a ranking SVM whose inputs are pairs of maps A∗(Si) and anInitialize A0. Repeat Step 1(calculate current bigr</context>
<context position="24969" citStr="Petrov et al. (2011)" startWordPosition="4208" endWordPosition="4211"> use are a subset of the features described in “The World Atlas of Languages Structure” (WALS, (Haspelmath et al., 2005)), and are shown in Table 1. 6 Evaluation Set-Up Datasets We test our model on 19 languages: Arabic, Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovene, Spanish, Swedish, and Turkish. Our data is taken from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The CoNLL datasets consist of manually created dependency trees and language-specific POS tags. Following Petrov et al. (2011), our model maps these language-specific tags to a set of 12 universal tags: noun, verb, adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark and X (a general tag). 9Ties are broken using the F(A) objective. Evaluation Procedure We perform a separate experiment for each of the 19 languages as the target and a source language chosen from the rest (using the method from Section 5.3). For the selected source language, we assume access to the mapping of Petrov et al. (2011). Evaluation Measures We evaluate the quality of the derived mapping in the co</context>
<context position="26767" citStr="Petrov et al. (2011)" startWordPosition="4500" endWordPosition="4503">f parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directed dependency accuracy as our measure of performance. Baselines We compare mappings induced by our model against three baselines: the manually constructed mapping of Petrov et al. (2011), a randomly constructed mapping and a greedy mapping. The greedy mapping uses the same objective as our full model, but optimizes it using a greedy method. In each iteration, this method makes |LT |passes over the language-specific tags, selecting a substitution that contributes the most to the objective. Initialization To reduce the dimension of our algorithm’s search space and speed up our method, we start by clustering the language-specific POS tags of the target into |K |= 12 clusters using an unsuper10We also experimented with a version of the Cohen et al. (2011) model trained on all the</context>
<context position="29210" citStr="Petrov et al., 2011" startWordPosition="4896" endWordPosition="4899">Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al., 2011). As expected, random mappings yield abysmal results — 20.2% and 12.7% for the two parsers. The low accuracy of parsers that rely on the Greedy mapping — 29.9% and 25.4% — show that a greedy approach is a poor strategy for mapping optimization. Surprisingly, our model slightly outperforms the mapping of (Petrov et al., 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al., 2011). Similar results are observed for the mixture weights parser (Cohen et al., 2011). The main reason for these differences comes from mistakes introduced in the manual mapping. For example, in Czech tag “R” is labeled as “pronoun”, while actually it should be mapped to “adposition”. By correcting this mistake, we gain 5% in parsing accuracy for the direct transfer parser. 11This pre-clustering results in about 3% improvement, pre</context>
<context position="33446" citStr="Petrov et al. (2011)" startWordPosition="5621" endWordPosition="5624">rkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9 Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3 Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language. The first section of the table is for the direct transfer of the MST parser (McDonald et al., 2011). The second section is for the weighted mixture parsing model (Cohen et al., 2011). The first two columns (Random and Greedy) of each section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the results when the mapping of Petrov et al. (2011) is used. The fourth column (Model) shows the results when our mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best source language is selected for every target language. The last column (Tag Diff.) presents the difference between our mapping and the mapping of Petrov et al. (2011) by showing the percentage of target language tokens for which the two mappings select a different universal tag. all, our method and the manual mappings reach similar values, both considerably better than other baselines. While the parsing performance</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. In ArXiv, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhai Xi</author>
<author>Rebecca Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-english languages.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>851--858</pages>
<contexts>
<context position="6903" citStr="Xi and Hwa, 2005" startWordPosition="1053" endWordPosition="1056"> our largest gain comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with la</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>Chenhai Xi and Rebecca Hwa. 2005. A backoff model for bootstrapping resources for non-english languages. In Proceedings of EMNLP, pages 851–858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Yuille</author>
<author>Anand Rangarajan</author>
</authors>
<title>The concaveconvex procedure (cccp).</title>
<date>2003</date>
<booktitle>In Proceedings of Neural Computation,</booktitle>
<volume>15</volume>
<pages>915--936</pages>
<contexts>
<context position="16868" citStr="Yuille and Rangarajan, 2003" startWordPosition="2735" endWordPosition="2738">thm, and finally the method for choosing source languages. 5.1 Optimizing the Mapping A Recall that our goal is to solve the optimization problem in Eq. (7). This objective is non convex since the function H[A] is concave, and the objective F(A) involves bilinear terms in A and logarithms of their sums (see Equations (1) and (2)). While we do not attempt to solve the problem globally, we do have a simple update scheme that monotonically decreases the objective. The update can be derived in a similar manner to expectation maximization (EM) (Neal and Hinton, 1999) and convex concave procedures (Yuille and Rangarajan, 2003). Figure 2 describes our optimization algorithm. The key ideas in deriving it are using posterior distributions as in EM, and using a variational formulation of entropy. The term Fc(A) is handled in a similar way to the posterior regularization algorithm derivation. A detailed derivation is provided in the supplementary file. 8 The kth iteration of the algorithm involves several steps: • In step 1, we calculate the current estimate of the bigram distribution over tags, pT(c1, c2; Ak). 7Note that as A --+ oo, only valid maps will be selected by the objective. 8The supplementary file is availabl</context>
</contexts>
<marker>Yuille, Rangarajan, 2003</marker>
<rawString>Alan Yuille and Anand Rangarajan. 2003. The concaveconvex procedure (cccp). In Proceedings of Neural Computation, volume 15, pages 915–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="2190" citStr="Zeman and Resnik, 2008" startWordPosition="307" endWordPosition="310">rning a model for another. 1The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/ rbg/code/unitag/emnlp2012 While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. Despite the apparent simplicity, deriving a mapping is by no means easy, even for humans. In fact, the universal tagsets manually induced by Petrov et al. (2011) and by Naseem et al. (2010) disagree on 10% of the tags. An example of such discrepancy is the mapping of the Japanese tag “PVfin” to the universal tag “particle” according to one scheme, and to “verb” according to another. Moreover, the quality of this conversion</context>
<context position="7073" citStr="Zeman and Resnik, 2008" startWordPosition="1076" endWordPosition="1079">accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same C</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In Proceedings of IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35–42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
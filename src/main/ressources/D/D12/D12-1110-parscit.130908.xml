<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.987864">
Semantic Compositionality through Recursive Matrix-Vector Spaces
</title>
<author confidence="0.896295">
Richard Socher Brody Huval Christopher D. Manning Andrew Y. Ng
</author>
<email confidence="0.838316">
richard@socher.org,{brodyh,manning,ang}@stanford.edu
</email>
<affiliation confidence="0.621303">
Computer Science Department, Stanford University
</affiliation>
<sectionHeader confidence="0.986622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999063208333333">
Single-word vector space models have been
very successful at learning lexical informa-
tion. However, they cannot capture the com-
positional meaning of longer phrases, prevent-
ing them from a deeper understanding of lan-
guage. We introduce a recursive neural net-
work (RNN) model that learns compositional
vector representations for phrases and sen-
tences of arbitrary syntactic type and length.
Our model assigns a vector and a matrix to ev-
ery node in a parse tree: the vector captures
the inherent meaning of the constituent, while
the matrix captures how it changes the mean-
ing of neighboring words or phrases. This
matrix-vector RNN can learn the meaning of
operators in propositional logic and natural
language. The model obtains state of the art
performance on three different experiments:
predicting fine-grained sentiment distributions
of adverb-adjective pairs; classifying senti-
ment labels of movie reviews and classifying
semantic relationships such as cause-effect or
topic-message between nouns using the syn-
tactic path between them.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998391333333334">
Semantic word vector spaces are at the core of many
useful natural language applications such as search
query expansions (Jones et al., 2006), fact extrac-
tion for information retrieval (Pas¸ca et al., 2006)
and automatic annotation of text with disambiguated
Wikipedia links (Ratinov et al., 2011), among many
others (Turney and Pantel, 2010). In these mod-
els the meaning of a word is encoded as a vector
computed from co-occurrence statistics of a word
and its neighboring words. Such vectors have been
shown to correlate well with human judgments of
word similarity (Griffiths et al., 2007).
</bodyText>
<figureCaption confidence="0.9910435">
Figure 1: A recursive neural network which learns se-
mantic vector representations of phrases in a tree struc-
ture. Each word and phrase is represented by a vector
and a matrix, e.g., very = (a, A). The matrix is applied
to neighboring vectors. The same function is repeated to
combine the phrase very good with movie.
</figureCaption>
<bodyText confidence="0.999852954545454">
Despite their success, single word vector models
are severely limited since they do not capture com-
positionality, the important quality of natural lan-
guage that allows speakers to determine the meaning
of a longer expression based on the meanings of its
words and the rules used to combine them (Frege,
1892). This prevents them from gaining a deeper
understanding of the semantics of longer phrases or
sentences. Recently, there has been much progress
in capturing compositionality in vector spaces, e.g.,
(Mitchell and Lapata, 2010; Baroni and Zamparelli,
2010; Zanzotto et al., 2010; Yessenalina and Cardie,
2011; Socher et al., 2011c) (see related work). We
extend these approaches with a more general and
powerful model of semantic composition.
We present a novel recursive neural network
model for semantic compositionality. In our context,
compositionality is the ability to learn compositional
vector representations for various types of phrases
and sentences of arbitrary length. Fig. 1 shows an
illustration of the model in which each constituent
(a word or longer phrase) has a matrix-vector (MV)
</bodyText>
<figure confidence="0.994053375">
... very good movie ...
( a , A ) ( b , B ) ( c , C )
f(Ba, Ab)=
Ba= Ab=
Recursive Matrix-Vector Model
...
- vector
- matrix
</figure>
<page confidence="0.933215">
1201
</page>
<note confidence="0.8341045">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1201–1211, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999918538461538">
representation. The vector captures the meaning of
that constituent. The matrix captures how it modifies
the meaning of the other word that it combines with.
A representation for a longer phrase is computed
bottom-up by recursively combining the words ac-
cording to the syntactic structure of a parse tree.
Since the model uses the MV representation with a
neural network as the final merging function, we call
our model a matrix-vector recursive neural network
(MV-RNN).
We show that the ability to capture semantic com-
positionality in a syntactically plausible way trans-
lates into state of the art performance on various
tasks. The first experiment demonstrates that our
model can learn fine-grained semantic composition-
ality. The task is to predict a sentiment distribution
over movie reviews of adverb-adjective pairs such as
unbelievably sad or really awesome. The MV-RNN
is the only model that is able to properly negate sen-
timent when adjectives are combined with not. The
MV-RNN outperforms previous state of the art mod-
els on full sentence sentiment prediction of movie
reviews. The last experiment shows that the MV-
RNN can also be used to find relationships between
words using the learned phrase vectors. The rela-
tionship between words is recursively constructed
and composed by words of arbitrary type in the
variable length syntactic path between them. On
the associated task of classifying relationships be-
tween nouns in arbitrary positions of a sentence the
model outperforms all previous approaches on the
SemEval-2010 Task 8 competition (Hendrickx et al.,
2010). It outperforms all but one of the previous ap-
proaches without using any hand-designed semantic
resources such as WordNet or FrameNet. By adding
WordNet hypernyms, POS and NER tags our model
outperforms the state of the art that uses significantly
more resources. The code for our model is available
at www.socher.org.
</bodyText>
<sectionHeader confidence="0.992222" genericHeader="introduction">
2 MV-RNN: A Recursive Matrix-Vector
Model
</sectionHeader>
<bodyText confidence="0.988525947368421">
The dominant approach for building representations
of multi-word units from single word vector repre-
sentations has been to form a linear combination of
the single word representations, such as a sum or
weighted average. This happens in information re-
trieval and in various text similarity functions based
on lexical similarity. These approaches can work
well when the meaning of a text is literally “the sum
of its parts”, but fails when words function as oper-
ators that modify the meaning of another word: the
meaning of “extremely strong” cannot be captured
as the sum of word representations for “extremely”
and “strong.”
The model of Socher et al. (2011c) provided a
new possibility for moving beyond a linear combi-
nation, through use of a matrix W that multiplied
the word vectors (a, b), and a nonlinearity function
g (such as a sigmoid or tanh). They compute the
parent vector p that describes both words as
</bodyText>
<equation confidence="0.984333">
[ a
p = gCW b J/
</equation>
<bodyText confidence="0.999784129032258">
and apply this function recursively inside a binarized
parse tree so that it can compute vectors for multi-
word sequences. Even though the nonlinearity al-
lows to express a wider range of functions, it is al-
most certainly too much to expect a single fixed W
matrix to be able to capture the meaning combina-
tion effects of all natural language operators. After
all, inside the function g, we have the same linear
transformation for all possible pairs of word vectors.
Recent work has started to capture the behavior
of natural language operators inside semantic vec-
tor spaces by modeling them as matrices, which
would allow a matrix for “extremely” to appropri-
ately modify vectors for “smelly” or “strong” (Ba-
roni and Zamparelli, 2010; Zanzotto et al., 2010).
These approaches are along the right lines but so
far have been restricted to capture linear functions
of pairs of words whereas we would like nonlinear
functions to compute compositional meaning repre-
sentations for multi-word phrases or full sentences.
The MV-RNN combines the strengths of both of
these ideas by (i) assigning a vector and a matrix to
every word and (ii) learning an input-specific, non-
linear, compositional function for computing vector
and matrix representations for multi-word sequences
of any syntactic type. Assigning vector-matrix rep-
resentations to all words instead of only to words of
one part of speech category allows for greater flex-
ibility which benefits performance. If a word lacks
operator semantics, its matrix can be an identity ma-
trix. However, if a word acts mainly as an operator,
</bodyText>
<equation confidence="0.934794">
(1)
</equation>
<page confidence="0.928548">
1202
</page>
<bodyText confidence="0.999973875">
such as “extremely”, its vector can become close to
zero, while its matrix gains a clear operator mean-
ing, here magnifying the meaning of the modified
word in both positive and negative directions.
In this section we describe the initial word rep-
resentations, the details of combining two words as
well as the multi-word extensions. This is followed
by an explanation of our training procedure.
</bodyText>
<subsectionHeader confidence="0.986045">
2.1 Matrix-Vector Neural Word Representation
</subsectionHeader>
<bodyText confidence="0.999906090909091">
We represent a word as both a continuous vector
and a matrix of parameters. We initialize all word
vectors x E Rn with pre-trained 50-dimensional
word vectors from the unsupervised model of Col-
lobert and Weston (2008). Using Wikipedia text,
their model learns word vectors by predicting how
likely it is for each word to occur in its context. Sim-
ilar to other local co-occurrence based vector space
models, the resulting word vectors capture syntactic
and semantic information. Every word is also asso-
ciated with a matrix X. In all experiments, we ini-
tialize matrices as X = I + E, i.e., the identity plus a
small amount of Gaussian noise. If the vectors have
dimensionality n, then each word’s matrix has di-
mensionality X E Rnxn. While the initialization is
random, the vectors and matrices will subsequently
be modified to enable a sequence of words to com-
pose a vector that can predict a distribution over se-
mantic labels. Henceforth, we represent any phrase
or sentence of length m as an ordered list of vector-
matrix pairs ((a, A), ... , (m, M)), where each pair
is retrieved based on the word at that position.
</bodyText>
<subsectionHeader confidence="0.998955">
2.2 Composition Models for Two Words
</subsectionHeader>
<bodyText confidence="0.999954791666667">
We first review composition functions for two
words. In order to compute a parent vector p from
two consecutive words and their respective vectors
a and b, Mitchell and Lapata (2010) give as their
most general function: p = f(a, b, R, K),where R
is the a-priori known syntactic relation and K is
background knowledge.
There are many possible functions f. For our
models, there is a constraint on p which is that it
has the same dimensionality as each of the input
vectors. This way, we can compare p easily with
its children and p can be the input to a composition
with another word. The latter is a requirement that
will become clear in the next section. This excludes
tensor products which were outperformed by sim-
pler weighted addition and multiplication methods
in (Mitchell and Lapata, 2010).
We will explore methods that do not require
any manually designed semantic resources as back-
ground knowledge K. No explicit knowledge about
the type of relation R is used. Instead we want the
model to capture this implicitly via the learned ma-
trices. We propose the following combination func-
tion which is input dependent:
</bodyText>
<equation confidence="0.9933325">
p = fA,B (a, b) = f (Ba, Ab) = g (W L Ab
L /(2)
</equation>
<bodyText confidence="0.998886757575758">
where A, B are matrices for single words, the global
W E Rnx2n is a matrix that maps both transformed
words back into the same n-dimensional space. The
element-wise function g could be simply the identity
function but we use instead a nonlinearity such as
the sigmoid or hyperbolic tangent tanh. Such a non-
linearity will allow us to approximate a wider range
of functions beyond purely linear functions. We can
also add a bias term before applying g but omit this
for clarity. Rewriting the two transformed vectors as
one vector z, we get p = g(Wz) which is a single
layer neural network. In this model, the word ma-
trices can capture compositional effects specific to
each word, whereas W captures a general composi-
tion function.
This function builds upon and generalizes several
recent models in the literature. The most related
work is that of (Mitchell and Lapata, 2010; Zan-
zotto et al., 2010) who introduced and explored the
composition function p = Ba + Ab for word pairs.
This model is a special case of Eq. 2 when we set
W = [II] (i.e. two concatenated identity matri-
ces) and g(x) = x (the identity function). Baroni
and Zamparelli (2010) computed the parent vector
of adjective-noun pairs by p = Ab, where A is an
adjective matrix and b is a vector for a noun. This
cannot capture nouns modifying other nouns, e.g.,
disk drive. This model too is a special case of the
above model with B = 0nxn. Lastly, the models of
(Socher et al., 2011b; Socher et al., 2011c; Socher et
al., 2011a) as described above are also special cases
with both A and B set to the identity matrix. We will
compare to these special cases in our experiments.
</bodyText>
<page confidence="0.988919">
1203
</page>
<figureCaption confidence="0.938052">
Figure 2: Example of how the MV-RNN merges a phrase
with another word at a nonterminal node of a parse tree.
</figureCaption>
<subsectionHeader confidence="0.9935525">
2.3 Recursive Compositions of Multiple Words
and Phrases
</subsectionHeader>
<bodyText confidence="0.99999725">
This section describes how we extend a word-pair
matrix-vector-based compositional model to learn
vectors and matrices for longer sequences of words.
The main idea is to apply the same function f to
pairs of constituents in a parse tree. For this to
work, we need to take as input a binary parse tree
of a phrase or sentence and also compute matrices at
each nonterminal parent node. The function f can
be readily used for phrase vectors since it is recur-
sively compatible (p has the same dimensionality as
its children). For computing nonterminal phrase ma-
trices, we define the function
</bodyText>
<equation confidence="0.996691">
P = fM(A, B) = WM [ B ] , (3)
</equation>
<bodyText confidence="0.999419043478261">
where WM E Rn,2n, so P E Rn�n just like each
input matrix.
After two words form a constituent in the parse
tree, this constituent can now be merged with an-
other one by applying the same functions f and
fM. For instance, to compute the vectors and ma-
trices depicted in Fig. 2, we first merge words a
and b and their matrices: p1 = f(Ba, Ab), P1 =
fM(A, B). The resulting vector-matrix pair (p1, P1)
can now be used to compute the full phrase when
combining it with word c and computing p2 =
f(Cp1, P1c), P2 = fM(P1, C). The model com-
putes vectors and matrices in a bottom-up fashion,
applying the functions f, fM to its own previous out-
put (i.e. recursively) until it reaches the top node of
the tree which represents the entire sentence.
For experiments with longer sequences we will
compare to standard RNNs and the special case of
the MV-RNN that computes the parent by p = Ab +
Ba, which we name the linear Matrix-Vector Re-
cursion model (linear MVR). Previously, this model
had not been trained for multi-word sequences. Sec.
6 talks about alternatives for compositionality.
</bodyText>
<subsectionHeader confidence="0.995832">
2.4 Objective Functions for Training
</subsectionHeader>
<bodyText confidence="0.999964">
One of the advantages of RNN-based models is that
each node of a tree has associated with it a dis-
tributed vector representation (the parent vector p)
which can also be seen as features describing that
phrase. We train these representations by adding on
top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or
relationship classes: d(p) = softmax(Wlabelp). If
there are K labels, then d E RK is a K-dimensional
multinomial distribution. For the applications below
(excluding logic), the corresponding error function
E(s, t, θ) that we minimize for a sentence s and its
tree t is the sum of cross-entropy errors at all nodes.
The only other methods that use this type of ob-
jective function are (Socher et al., 2011b; Socher
et al., 2011c), who also combine it with either a
score or reconstruction error. Hence, for compar-
isons to other related work, we need to merge vari-
ations of computing the parent vector p with this
classifier. The main difference is that the MV-RNN
has more flexibility since it has an input specific re-
cursive function fA,B to compute each parent. In
the following applications, we will use the softmax
classifier to predict both sentiment distributions and
noun-noun relationships.
</bodyText>
<subsectionHeader confidence="0.992717">
2.5 Learning
</subsectionHeader>
<bodyText confidence="0.9999186">
Let θ = (W, WM, Wlabel, L, LM) be our model pa-
rameters and λ a vector with regularization hyperpa-
rameters for all model parameters. L and LM are the
sets of all word vectors and word matrices. The gra-
dient of the overall objective function J becomes:
</bodyText>
<equation confidence="0.857245666666667">
∂E(x, t; θ) + AO. 4
( )
ae
</equation>
<bodyText confidence="0.9999465">
To compute this gradient, we first compute all tree
nodes (pi, Pi) from the bottom-up and then take
derivatives of the softmax classifiers at each node
in the tree from the top down. Derivatives are com-
puted efficiently via backpropagation through struc-
ture (Goller and K¨uchler, 1996). Even though the
</bodyText>
<figure confidence="0.364594666666667">
... very good movie ...
(a , A) (b , B) (c , C)
Matrix-Vector Recursive Neural Network
</figure>
<equation confidence="0.986683454545455">
(p1 , P1)
( p2, P2 ) p2 = g(W )
LP1c
Cp1
P2 = WM
LC ]
P1
1 N
(x,t)
∂J
∂θ =
</equation>
<page confidence="0.93584">
1204
</page>
<bodyText confidence="0.9999066">
objective is not convex, we found that L-BFGS run
over the complete training data (batch mode) mini-
mizes the objective well in practice and convergence
is smooth. For more information see (Socher et al.,
2010).
</bodyText>
<subsectionHeader confidence="0.997025">
2.6 Low-Rank Matrix Approximations
</subsectionHeader>
<bodyText confidence="0.998997142857143">
If every word is represented by an n-dimensional
vector and additionally by an n x n matrix, the di-
mensionality of the whole model may become too
large with commonly used vector sizes of n = 100.
In order to reduce the number of parameters, we rep-
resent word matrices by the following low-rank plus
diagonal approximation:
</bodyText>
<equation confidence="0.992469">
A = UV + diag(a), (5)
</equation>
<bodyText confidence="0.998472">
where U E Rnxr, V E Rrxn, a E Rn and we set
the rank for all experiments to r = 3.
</bodyText>
<subsectionHeader confidence="0.990059">
2.7 Discussion: Evaluation and Generality
</subsectionHeader>
<bodyText confidence="0.999991692307692">
Evaluation of compositional vector spaces is a com-
plex task. Most related work compares similarity
judgments of unsupervised models to those of hu-
man judgments and aims at high correlation. These
evaluations can give important insights. However,
even with good correlation the question remains
how these models would perform on downstream
NLP tasks such as sentiment detection. We ex-
perimented with unsupervised learning of general
vector-matrix representations by having the MV-
RNN predict words in their correct context. Ini-
tializing the models with these general representa-
tions, did not improve the performance on the tasks
we consider. For sentiment analysis, this is not sur-
prising since antonyms often get similar vectors dur-
ing unsupervised learning from co-occurrences due
to high similarity of local syntactic contexts. In our
experiments, the high prediction performance came
from supervised learning of meaning representations
using labeled data. While these representations are
task-specific, they could be used across tasks in a
multi-task learning setup. However, in order to fairly
compare to related work, we use only the super-
vised data of each task. Before we describe our full-
scale experiments, we analyze the model’s expres-
sive powers.
</bodyText>
<sectionHeader confidence="0.974706" genericHeader="method">
3 Model Analysis
</sectionHeader>
<bodyText confidence="0.999963769230769">
This section analyzes the model with two proof-of-
concept studies. First, we examine its ability to learn
operator semantics for adverb-adjective pairs. If a
model cannot correctly capture how an adverb op-
erates on the meaning of adjectives, then there’s lit-
tle chance it can learn operators for more complex
relationships. The second study analyzes whether
the MV-RNN can learn simple boolean operators of
propositional logic such as conjunctives or negation
from truth values. Again, if a model did not have this
ability, then there’s little chance it could learn these
frequently occurring phenomena from the noisy lan-
guage of real texts such as movie reviews.
</bodyText>
<subsectionHeader confidence="0.8568135">
3.1 Predicting Sentiment Distributions of
Adverb-Adjective Pairs
</subsectionHeader>
<bodyText confidence="0.999968689655172">
The first study considers the prediction of fine-
grained sentiment distributions of adverb-adjective
pairs and analyzes different possibilities for com-
puting the parent vector p. The results show that
the MV-RNN operators are powerful enough to cap-
ture the operational meanings of various types of ad-
verbs. For example, very is an intensifier, pretty is an
attenuator, and not can negate or strongly attenuate
the positivity of an adjective. For instance not great
is still pretty good and not terrible; see Potts (2010)
for details.
We use a publicly available IMDB dataset of ex-
tracted adverb-adjective pairs from movie reviews.1
The dataset provides the distribution over star rat-
ings: Each consecutive word pair appears a certain
number of times in reviews that have also associ-
ated with them an overall rating of the movie. After
normalizing by the total number of occurrences, one
gets a multinomial distribution over ratings. Only
word pairs that appear at least 50 times are kept. Of
the remaining pairs, we use 4211 randomly sampled
ones for training and a separate set of 1804 for test-
ing. We never give the algorithm sentiment distribu-
tions for single words, and, while single words over-
lap between training and testing, the test set consists
of never before seen word pairs.
The softmax classifier is trained to minimize the
cross entropy error. Hence, an evaluation in terms of
KL-divergence is the most reasonable choice. It is
</bodyText>
<footnote confidence="0.984644">
1http://compprag.christopherpotts.net/reviews.html
</footnote>
<page confidence="0.809943">
1205
</page>
<table confidence="0.9295512">
Method Avg KL
Uniform 0.327
Mean train 0.193
p= 1�(a+b) 0.103
p = a ® b 0.103
p = [a; b] 0.101
p = Ab 0.103
RNN 0.093
Linear MVR 0.092
MV-RNN 0.091
</table>
<figure confidence="0.994883314606742">
unbelievably annoying
unbelievably awesome
unbelievably sad
fairly annoying
fairly awesome
fairly sad
not annoying
not awesome
not sad
0.5
0.5
0.5
0.4
MV−RNN
RNN
0.4
MV−RNN
RNN
0.4
MV−RNN
RNN
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
0.5
0.5
0.5
0.4
MV−RNN
RNN
0.4
MV−RNN
RNN
0.4
Training Pair
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
0.5
0.5
0.5
0.4
MV−RNN
RNN
0.4
MV−RNN
RNN
0.4
MV−RNN
RNN
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
0
1 2 3 4 5 6 7 8 9 10
</figure>
<figureCaption confidence="0.996966142857143">
Figure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of the
test set. See text for p descriptions. Lower is better. The main difference in the KL divergence comes from the few
negation pairs in the test set. Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverb-
adjective pairs. Each row has the same adverb and each column the same adjective. Many predictions are similar
between the two models. The RNN and linear MVR are not able to modify the sentiment correctly: not awesome is
more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. Predictions of the
linear MVR model are almost identical to the standard RNN for these examples.
</figureCaption>
<bodyText confidence="0.815813166666667">
defined as KL(g||p) = EZ gz log(gz/pz), where g is
the gold distribution and p is the predicted one.
We compare to several baselines and ablations of
the MV-RNN model. An (adverb,adjective) pair is
described by its vectors (a, b) and matrices (A, B).
1 p = 0.5(a + b), vector average
</bodyText>
<listItem confidence="0.996030571428572">
2. p = a ® b, element-wise vector multiplication
3. p = [a; b], vector concatenation
4. p = Ab, similar to (Baroni and Lenci, 2010)
5. p = g(W [a; b]), RNN, similar to Socher et al.
6. p = Ab + Ba, Linear MVR, similar to (Mitchell
and Lapata, 2010; Zanzotto et al., 2010)
7. p = g(W [Ba; Ab]), MV-RNN
</listItem>
<bodyText confidence="0.999932459459459">
The final distribution is always predicted by a
softmax classifier whose inputs p vary for each of
the models. This objective function (see Sec. 2.4)
is different to all previously published work except
that of (Socher et al., 2011c).
We cross-validated all models over regulariza-
tion parameters for word vectors, the softmax clas-
sifier, the RNN parameter W and the word op-
erators (10−4,10−3) and word vector sizes (n =
6, 8,10,12,15, 20). All models performed best at
vector sizes of below 12. Hence, it is the model’s
power and not the number of parameters that deter-
mines the performance. The table in Fig. 3 shows
the average KL-divergence on the test set. It shows
that the idea of matrix-vector representations for all
words and having a nonlinearity are both impor-
tant. The MV-RNN which combines these two ideas
is best able to learn the various compositional ef-
fects. The main difference in KL divergence comes
from the few negation cases in the test set. Fig. 3
shows examples of predicted distributions. Many
of the predictions are accurate and similar between
the top models. However, only the MV-RNN has
enough expressive power to allow negation to com-
pletely shift the sentiment with respect to an adjec-
tive. A negated adjective carrying negative senti-
ment becomes slightly positive, whereas not awe-
some is correctly attenuated. All three top models
correctly capture the U-shape of unbelievably sad.
This pair peaks at both the negative and positive
spectrum because it is ambiguous. When referring
to the performance of actors, it is very negative, but,
when talking about the plot, many people enjoy sad
and thought-provoking movies. The p = Ab model
does not perform well because it cannot model the
fact that for an adjective like “sad,” the operator of
“unbelievably” behaves differently.
</bodyText>
<figure confidence="0.998509928571429">
false
false
true
false
¬ true
¬ false
false
false ∧ true
true
true ∧ true
false
∧ false
true
∧ false
</figure>
<figureCaption confidence="0.752731333333333">
Figure 4: Training trees for the MV-RNN to learn propositional operators. The model learns vectors and operators for
∧ (and) and ¬ (negation). The model outputs the exact representations of false and true respectively at the top node.
Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions.
</figureCaption>
<subsectionHeader confidence="0.998435">
3.2 Logic- and Vector-based Compositionality
</subsectionHeader>
<bodyText confidence="0.99991703125">
Another natural question is whether the MV-RNN
can, in general, capture some of the simple boolean
logic that is sometimes found in language. In other
words, can it learn some of the propositional logic
operators such as and, or, not in terms of vectors and
matrices from a few examples. Answering this ques-
tion can also be seen as a first step towards bridg-
ing the gap between logic-based, formal semantics
(Montague, 1974) and vector space models.
The logic-based view of language accounts nicely
for compositionality by directly mapping syntac-
tic constituents to lambda calculus expressions. At
the word level, the focus is on function words, and
nouns and adjectives are often defined only in terms
of the sets of entities they denote in the world. Most
words are treated as atomic symbols with no rela-
tion to each other. There have been many attempts
at automatically parsing natural language to a logi-
cal form using recursive compositional rules.
Conversely, vector space models have the attrac-
tive property that they can automatically extract
knowledge from large corpora without supervision.
Unlike logic-based approaches, these models allow
us to make fine-grained statements about the seman-
tic similarity of words which correlate well with hu-
man judgments (Griffiths et al., 2007). Logic-based
approaches are often seen as orthogonal to distribu-
tional vector-based approaches. However, Garrette
et al. (2011) recently introduced a combination of a
vector space model inside a Markov Logic Network.
One open question is whether vector-based mod-
els can learn some of the simple logic encountered
in language such as negation or conjunctives. To
this end, we illustrate in a simple example that our
MV-RNN model and its learned word matrices (op-
erators) have the ability to learn propositional logic
operators such as ∧, ∨, ¬ (and, or, not). This is a
necessary (though not sufficient) condition for the
ability to pick up these phenomena in real datasets
and tasks such as sentiment detection which we fo-
cus on in the subsequent sections.
Our setup is as follows. We train on 6 strictly
right-branching trees as in Fig. 4. We consider the 1-
dimensional case and fix the representation for true
to (t = 1, T = 1) and false to (f = 0, F = 1).
Fixing the operators to the 1 × 1 identity matrix 1
is essentially ignoring them. The objective is then
to create a perfect reconstruction of (t, T) or (f, F)
(depending on the formula), which we achieve by
the least squares error between the top vector’s rep-
resentation and the corresponding truth value, e.g.
for ¬false: min ||ptop − t||2 + ||Ptop − T||2.
As our function g (see Eq. 2), we use a linear
threshold unit: g(x) = max(min(x, 1), 0). Giving
the derivatives computed for the objective function
for the examples in Fig. 4 to a standard L-BFGS op-
timizer quickly yields a training error of 0. Hence,
the output of these 6 examples has exactly one of the
truth representations, making it recursively compati-
ble with further combinations of operators. Thus, we
can combine these operators to construct any propo-
sitional logic function of any number of inputs (in-
cluding xor). Hence, this MV-RNN is complete in
terms of propositional logic.
</bodyText>
<sectionHeader confidence="0.946743" genericHeader="method">
4 Predicting Movie Review Ratings
</sectionHeader>
<bodyText confidence="0.999869416666667">
In this section, we analyze the model’s performance
on full length sentences. We compare to previous
state of the art methods on a standard benchmark
dataset of movie reviews (Pang and Lee, 2005; Nak-
agawa et al., 2010; Socher et al., 2011c). This
dataset consists of 10,000 positive and negative sin-
gle sentences describing movie sentiment. In this
and the next experiment we use binarized trees from
the Stanford Parser (Klein and Manning, 2003). We
use the exact same setup and parameters (regulariza-
tion, word vector size, etc.) as the published code of
Socher et al. (2011c).2
</bodyText>
<footnote confidence="0.99339">
2www.socher.org
</footnote>
<page confidence="0.969062">
1207
</page>
<note confidence="0.5103412">
Method Acc.
Tree-CRF (Nakagawa et al., 2010) 77.3
RAE (Socher et al., 2011c) 77.7
Linear MVR 77.1
MV-RNN 79.0
</note>
<tableCaption confidence="0.989468">
Table 1: Accuracy of classification on full length movie
review polarity (MR).
</tableCaption>
<figure confidence="0.403489">
S. C. Review sentence
1 The film is bright and flashy in all the right ways.
0 Not always too whimsical for its own good this
</figure>
<bodyText confidence="0.387499666666667">
strange hybrid of crime thriller, quirky character
study, third-rate romance and female empowerment
fantasy never really finds the tonal or thematic glue
it needs.
0 Doesn’t come close to justifying the hype that sur-
rounded its debut at the Sundance film festival two
years ago.
0 x Director Hoffman, his writer and Kline’s agent
should serve detention.
</bodyText>
<table confidence="0.794704">
1 x A bodice-ripper for intellectuals.
</table>
<tableCaption confidence="0.952083">
Table 2: Hard movie review examples of positive (1) and
negative (0) sentiment (S.) that of all methods only the
MV-RNN predicted correctly (C: 0 or could not classify
as correct either (C: x).
</tableCaption>
<bodyText confidence="0.945947818181818">
Table 1 shows comparisons to the system of (Nak-
agawa et al., 2010), a dependency tree based classifi-
cation method that uses CRFs with hidden variables.
The state of the art recursive autoencoder model of
Socher et al. (2011c) obtained 77.7% accuracy. Our
new MV-RNN gives the highest performance, out-
performing also the linear MVR (Sec. 2.2).
Table 2 shows several hard examples that only the
MV-RNN was able to classify correctly. None of the
methods correctly classified the last two examples
which require more world knowledge.
</bodyText>
<sectionHeader confidence="0.687212" genericHeader="method">
5 Classification of Semantic Relationships
</sectionHeader>
<bodyText confidence="0.9999472">
The previous task considered global classification of
an entire phrase or sentence. In our last experiment
we show that the MV-RNN can also learn how a syn-
tactic context composes an aggregate meaning of the
semantic relationships between words. In particular,
the task is finding semantic relationships between
pairs of nominals. For instance, in the sentence
“My [apartment]e1 has a pretty large [kitchen]e2.”,
we want to predict that the kitchen and apartment are
in a component-whole relationship. Predicting such
</bodyText>
<figureCaption confidence="0.990232">
Figure 5: The MV-RNN learns vectors in the path con-
necting two words (dotted lines) to determine their se-
mantic relationship. It takes into consideration a variable
length sequence of various word types in that path.
</figureCaption>
<bodyText confidence="0.99994625">
semantic relations is useful for information extrac-
tion and thesaurus construction applications. Many
approaches use features for all words on the path
between the two words of interest. We show that
by building a single compositional semantics for the
minimal constituent including both terms one can
achieve a higher performance.
This task requires the ability to deal with se-
quences of words of arbitrary type and length in be-
tween the two nouns in question.Fig. 5 explains our
method for classifying nominal relationships. We
first find the path in the parse tree between the two
words whose relation we want to classify. We then
select the highest node of the path and classify the
relationship using that node’s vector as features. We
apply the same type of MV-RNN model as in senti-
ment to the subtree spanned by the two words.
We use the dataset and evaluation framework
of SemEval-2010 Task 8 (Hendrickx et al., 2010).
There are 9 ordered relationships (with two direc-
tions) and an undirected other class, resulting in
19 classes. Among the relationships are: message-
topic, cause-effect, instrument-agency (etc. see Ta-
ble 3 for list). A pair is counted as correct if the
order of the words in the relationship is correct.
Table 4 lists results for several competing meth-
ods together with the resources and features used
by each method. We compare to the systems of
the competition which are described in Hendrickx
et al. (2010) as well as the RNN and linear MVR.
Most systems used a considerable amount of hand-
designed semantic resources. In contrast to these
methods, the MV-RNN only needs a parser for the
tree structure and learns all semantics from unla-
beled corpora and the training data. Only the Se-
mEval training dataset is specific to this task, the re-
</bodyText>
<figure confidence="0.916793583333333">
MV-RNN for Relationship Classification
... the [movie] showed [wars] ...
...
Classifier: Message-Topic
...
1208
Relationship Sentence with labeled nouns for which to predict relationships
Cause-Effect(e2,e1) Avian [influenza]e, is an infectious disease caused by type a strains of the influenza [virus]e2.
Entity-Origin(e1,e2) The [mother]e, left her native [land]e2 about the same time and they were married in that city.
Message-Topic(e2,e1) Roadside [attractions]e, are frequently advertised with [billboards]e2 to attract tourists.
Product-Producer(e1,e2) A child is told a [lie]e, for several years by their [parents]e2 before he/she realizes that ...
Entity-Destination(e1,e2) The accident has spread [oil]e, into the [ocean]e2.
</figure>
<tableCaption confidence="0.835130125">
Member-Collection(e2,e1) The siege started, with a [regiment]e, of lightly armored [swordsmen]e2 ramming down the gate.
Instrument-Agency(e2,e1) The core of the [analyzer]e, identifies the paths using the constraint propagation [method]e2.
Component-Whole(e2,e1) The size of a [tree]e, [crown]e2 is strongly correlated with the growth of the tree.
Content-Container(e1,e2) The hidden [camera]e,, found by a security guard, was hidden in a business card-sized [leaflet
box]e2 placed at an unmanned ATM in Tokyo’s Minato ward in early September.
Table 3: Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that
the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed
words. The paths vary in length and the words vary in type.
</tableCaption>
<table confidence="0.982130857142857">
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, WordNet, stemming, syntactic 74.8
patterns
SVM POS, WordNet, morphological fea- 77.6
tures, thesauri, Google n-grams
</table>
<bodyText confidence="0.9933528">
tity tags (NER) of the two words (+0.6). Features
were computed using the code of Ciaramita and Al-
tun (2006).3 With these features, the performance
improved over the state of the art system. Table 3
shows random correct classification examples.
</bodyText>
<table confidence="0.986121533333333">
MaxEnt POS, WordNet, morphological fea- 77.6
tures, noun compound system, the-
sauri, Google n-grams
SVM POS, WordNet, prefixes and other 82.2
morphological features, POS, depen-
dency parse features, Levin classes,
PropBank, FrameNet, NomLex-Plus,
Google n-grams, paraphrases, Tex-
tRunner
RNN - 74.8
Lin.MVR - 73.0
MV-RNN - 79.1
RNN POS,WordNet,NER 77.6
Lin.MVR POS,WordNet,NER 78.7
MV-RNN POS,WordNet,NER 82.4
</table>
<tableCaption confidence="0.9906916">
Table 4: Learning methods, their feature sets and F1
results for predicting semantic relations between nouns.
The MV-RNN outperforms all but one method without
any additional feature sets. By adding three such features,
it obtains state of the art performance.
</tableCaption>
<bodyText confidence="0.999831888888889">
maining inputs and the training setup are the same
as in previous sentiment experiments.
The best method on this dataset (Rink and
Harabagiu, 2010) obtains 82.2% F1. In order to
see whether our system can improve over this sys-
tem, we added three features to the MV-RNN vec-
tor and trained another softmax classifier. The fea-
tures and their performance increases were POS tags
(+0.9); WordNet hypernyms (+1.3) and named en-
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.9999842">
Distributional approaches have become omnipresent
for the recognition of semantic similarity between
words and the treatment of compositionality has
seen much progress in recent years. Hence, we can-
not do justice to the large amount of literature. Com-
monly, single words are represented as vectors of
distributional characteristics – e.g., their frequencies
in specific syntactic relations or their co-occurrences
with given context words (Pado and Lapata, 2007;
Baroni and Lenci, 2010; Turney and Pantel, 2010).
These representations have proven very effective in
sense discrimination and disambiguation (Sch¨utze,
1998), automatic thesaurus extraction (Lin, 1998;
Curran, 2004) and selectional preferences.
There are several sophisticated ideas for com-
positionality in vector spaces. Mitchell and Lap-
ata (2010) present an overview of the most impor-
tant compositional models, from simple vector ad-
dition and component-wise multiplication to tensor
products, and convolution (Metcalfe, 1990). They
measured the similarity between word pairs such
as compound nouns or verb-object pairs and com-
pared these with human similarity judgments. Sim-
ple vector averaging or multiplication performed
best, hence our focus on related baselines above.
</bodyText>
<footnote confidence="0.357247">
3sourceforge.net/projects/supersensetag/
</footnote>
<page confidence="0.991925">
1209
</page>
<bodyText confidence="0.9999067">
Other important models are tensor products (Clark
and Pulman, 2007), quantum logic (Widdows,
2008), holographic reduced representations (Plate,
1995) and the Compositional Matrix Space model
(Rudolph and Giesbrecht, 2010). RNNs are related
to autoencoder models such as the recursive autoas-
sociative memory (RAAM) (Pollack, 1990) or recur-
rent neural networks (Elman, 1991). Bottou (2011)
and Hinton (1990) discussed related models such as
recursive autoencoders for text understanding.
Our model builds upon and generalizes the mod-
els of (Mitchell and Lapata, 2010; Baroni and Zam-
parelli, 2010; Zanzotto et al., 2010; Socher et al.,
2011c) (see Sec. 2.2). We compare to them in
our experiments. Yessenalina and Cardie (2011) in-
troduce a sentiment analysis model that describes
words as matrices and composition as matrix mul-
tiplication. Since matrix multiplication is associa-
tive, this cannot capture different scopes of nega-
tion or syntactic differences. Their model, is a spe-
cial case of our encoding model (when you ignore
vectors, fix the tree to be strictly branching in one
direction and use as the matrix composition func-
tion P = AB). Since our classifiers are trained on
the vectors, we cannot compare to this approach di-
rectly. Grefenstette and Sadrzadeh (2011) learn ma-
trices for verbs in a categorical model. The trained
matrices improve correlation with human judgments
on the task of identifying relatedness of subject-
verb-object triplets.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986714285714">
We introduced a new model towards a complete
treatment of compositionality in word vector spaces.
Our model builds on a syntactically plausible parse
tree and can handle compositional phenomena. The
main novelty of our model is the combination of
matrix-vector representations with a recursive neu-
ral network. It can learn both the meaning vectors of
a word and how that word modifies its neighbors (via
its matrix). The MV-RNN combines attractive the-
oretical properties with good performance on large,
noisy datasets. It generalizes several models in the
literature, can learn propositional logic, accurately
predicts sentiment and can be used to classify se-
mantic relationships between nouns in a sentence.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999941923076923">
We thank for great discussions about the paper:
John Platt, Chris Potts, Josh Tenenbaum, Mihai Sur-
deanu, Quoc Le and Kevin Miller. The authors
gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181, and the DARPA Deep Learning program
under contract number FA8650-10-C-7020. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
DARPA, AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823588235294">
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
M. Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP.
L. Bottou. 2011. From machine learning to machine
reasoning. CoRR, abs/1102.1808.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In EMNLP.
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction,
pages 52–55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: deep neural networks
with multitask learning. In ICML.
J. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3).
G. Frege. 1892. ¨Uber Sinn und Bedeutung. In Zeitschrift
f¨ur Philosophie und philosophische Kritik, 100.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrat-
ing Logical Representations with Probabilistic Infor-
mation using Markov Logic. In Proceedings of the In-
ternational Conference on Computational Semantics.
C. Goller and A. K¨uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
</reference>
<page confidence="0.713457">
1210
</page>
<reference confidence="0.999932516483517">
E. Grefenstette and M. Sadrzadeh. 2011. Experimental
support for a categorical compositional distributional
model of meaning. In EMNLP.
T. L. Griffiths, J. B. Tenenbaum, and M. Steyvers. 2007.
Topics in semantic representation. Psychological Re-
view, 114.
I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov,
D. O´ S´eaghdha, S. Pad´o, M. Pennacchiotti, L. Ro-
mano, and S. Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
G. E. Hinton. 1990. Mapping part-whole hierarchies into
connectionist networks. Artificial Intelligence, 46(1-
2).
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL, pages
768–774.
E. J. Metcalfe. 1990. A compositive holographic asso-
ciative recall model. Psychological Review, 88:627–
661.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388–1429.
R. Montague. 1974. English as a formal language. Lin-
guaggi nella Societa e nella Tecnica, pages 189–224.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. In ACL.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161–199.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115–124.
T. A. Plate. 1995. Holographic reduced representations.
IEEE Transactions on Neural Networks, 6(3):623–
641.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
B. Rink and S. Harabagiu. 2010. UTD: Classifying se-
mantic relations by combining lexical and semantic re-
sources. In Proceedings of the 5th International Work-
shop on Semantic Evaluation.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In ACL.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24:97–124.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second AAAI
Symposium on Quantum Interaction.
A. Yessenalina and C. Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
EMNLP.
F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. 2010. Estimating linear models for composi-
tional distributional semantics. COLING.
</reference>
<page confidence="0.991213">
1211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979215">
<title confidence="0.999961">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
<author confidence="0.999507">Richard Socher Brody Huval Christopher D Manning Andrew Y</author>
<affiliation confidence="0.997527">Computer Science Department, Stanford University</affiliation>
<abstract confidence="0.99927716">Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="22631" citStr="Baroni and Lenci, 2010" startWordPosition="3869" endWordPosition="3872">ot awesome is more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. Predictions of the linear MVR model are almost identical to the standard RNN for these examples. defined as KL(g||p) = EZ gz log(gz/pz), where g is the gold distribution and p is the predicted one. We compare to several baselines and ablations of the MV-RNN model. An (adverb,adjective) pair is described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12</context>
<context position="36556" citStr="Baroni and Lenci, 2010" startWordPosition="6127" endWordPosition="6130">ftmax classifier. The features and their performance increases were POS tags (+0.9); WordNet hypernyms (+1.3) and named en6 Related work Distributional approaches have become omnipresent for the recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs </context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2767" citStr="Baroni and Zamparelli, 2010" startWordPosition="419" endWordPosition="422">function is repeated to combine the phrase very good with movie. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) </context>
<context position="7286" citStr="Baroni and Zamparelli, 2010" startWordPosition="1157" endWordPosition="1161">tiword sequences. Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all natural language operators. After all, inside the function g, we have the same linear transformation for all possible pairs of word vectors. Recent work has started to capture the behavior of natural language operators inside semantic vector spaces by modeling them as matrices, which would allow a matrix for “extremely” to appropriately modify vectors for “smelly” or “strong” (Baroni and Zamparelli, 2010; Zanzotto et al., 2010). These approaches are along the right lines but so far have been restricted to capture linear functions of pairs of words whereas we would like nonlinear functions to compute compositional meaning representations for multi-word phrases or full sentences. The MV-RNN combines the strengths of both of these ideas by (i) assigning a vector and a matrix to every word and (ii) learning an input-specific, nonlinear, compositional function for computing vector and matrix representations for multi-word sequences of any syntactic type. Assigning vector-matrix representations to </context>
<context position="12048" citStr="Baroni and Zamparelli (2010)" startWordPosition="1979" endWordPosition="1982"> one vector z, we get p = g(Wz) which is a single layer neural network. In this model, the word matrices can capture compositional effects specific to each word, whereas W captures a general composition function. This function builds upon and generalizes several recent models in the literature. The most related work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2011a) as described above are also special cases with both A and B set to the identity matrix. We will compare to these special cases in our experiments. 1203 Figure 2: Example of how the MV-RNN merges a phrase with another word at a nonterminal node of a pars</context>
<context position="37949" citStr="Baroni and Zamparelli, 2010" startWordPosition="6319" endWordPosition="6323">.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, w</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>From machine learning to machine reasoning.</title>
<date>2011</date>
<location>CoRR, abs/1102.1808.</location>
<contexts>
<context position="37743" citStr="Bottou (2011)" startWordPosition="6290" endWordPosition="6291">ouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case </context>
</contexts>
<marker>Bottou, 2011</marker>
<rawString>L. Bottou. 2011. From machine learning to machine reasoning. CoRR, abs/1102.1808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="34831" citStr="Ciaramita and Altun (2006)" startWordPosition="5865" endWordPosition="5869">amples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed words. The paths vary in length and the words vary in type. Classifier Feature Sets F1 SVM POS, stemming, syntactic patterns 60.1 SVM word pair, words in between 72.5 SVM POS, WordNet, stemming, syntactic 74.8 patterns SVM POS, WordNet, morphological fea- 77.6 tures, thesauri, Google n-grams tity tags (NER) of the two words (+0.6). Features were computed using the code of Ciaramita and Altun (2006).3 With these features, the performance improved over the state of the art system. Table 3 shows random correct classification examples. MaxEnt POS, WordNet, morphological fea- 77.6 tures, noun compound system, thesauri, Google n-grams SVM POS, WordNet, prefixes and other 82.2 morphological features, POS, dependency parse features, Levin classes, PropBank, FrameNet, NomLex-Plus, Google n-grams, paraphrases, TextRunner RNN - 74.8 Lin.MVR - 73.0 MV-RNN - 79.1 RNN POS,WordNet,NER 77.6 Lin.MVR POS,WordNet,NER 78.7 MV-RNN POS,WordNet,NER 82.4 Table 4: Learning methods, their feature sets and F1 res</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>S Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<contexts>
<context position="37423" citStr="Clark and Pulman, 2007" startWordPosition="6244" endWordPosition="6247">phisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare </context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>S. Clark and S. Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="8798" citStr="Collobert and Weston (2008)" startWordPosition="1401" endWordPosition="1405">r can become close to zero, while its matrix gains a clear operator meaning, here magnifying the meaning of the modified word in both positive and negative directions. In this section we describe the initial word representations, the details of combining two words as well as the multi-word extensions. This is followed by an explanation of our training procedure. 2.1 Matrix-Vector Neural Word Representation We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x E Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). Using Wikipedia text, their model learns word vectors by predicting how likely it is for each word to occur in its context. Similar to other local co-occurrence based vector space models, the resulting word vectors capture syntactic and semantic information. Every word is also associated with a matrix X. In all experiments, we initialize matrices as X = I + E, i.e., the identity plus a small amount of Gaussian noise. If the vectors have dimensionality n, then each word’s matrix has dimensionality X E Rnxn. While the initialization is random, the vectors and matrices will subsequently be modi</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="36750" citStr="Curran, 2004" startWordPosition="6153" endWordPosition="6154">ion of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="37728" citStr="Elman, 1991" startWordPosition="6288" endWordPosition="6289"> as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is </context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>J. L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7(2-3).</rawString>
</citation>
<citation valid="false">
<title>Uber Sinn und Bedeutung.</title>
<booktitle>In Zeitschrift f¨ur Philosophie und philosophische Kritik,</booktitle>
<pages>100</pages>
<marker></marker>
<rawString>G. Frege. 1892. ¨Uber Sinn und Bedeutung. In Zeitschrift f¨ur Philosophie und philosophische Kritik, 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Garrette</author>
<author>K Erk</author>
<author>R Mooney</author>
</authors>
<title>Integrating Logical Representations with Probabilistic Information using Markov Logic.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="26522" citStr="Garrette et al. (2011)" startWordPosition="4513" endWordPosition="4516">th no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules. Conversely, vector space models have the attractive property that they can automatically extract knowledge from large corpora without supervision. Unlike logic-based approaches, these models allow us to make fine-grained statements about the semantic similarity of words which correlate well with human judgments (Griffiths et al., 2007). Logic-based approaches are often seen as orthogonal to distributional vector-based approaches. However, Garrette et al. (2011) recently introduced a combination of a vector space model inside a Markov Logic Network. One open question is whether vector-based models can learn some of the simple logic encountered in language such as negation or conjunctives. To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). This is a necessary (though not sufficient) condition for the ability to pick up these phenomena in real datasets and tasks such as sentiment detection which we focus on</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>D. Garrette, K. Erk, and R. Mooney. 2011. Integrating Logical Representations with Probabilistic Information using Markov Logic. In Proceedings of the International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goller</author>
<author>A K¨uchler</author>
</authors>
<title>Learning taskdependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Neural Networks (ICNN-96).</booktitle>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>C. Goller and A. K¨uchler. 1996. Learning taskdependent distributed representations by backpropagation through structure. In Proceedings of the International Conference on Neural Networks (ICNN-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="38626" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="6432" endWordPosition="6435">c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. The trained matrices improve correlation with human judgments on the task of identifying relatedness of subjectverb-object triplets. 7 Conclusion We introduced a new model towards a complete treatment of compositionality in word vector spaces. Our model builds on a syntactically plausible parse tree and can handle compositional phenomena. The main novelty of our model is the combination of matrix-vector representations with a recursive neural network. It can learn both the meaning vectors of a word and how that word modifies its neighbors (via </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>E. Grefenstette and M. Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>J B Tenenbaum</author>
<author>M Steyvers</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<contexts>
<context position="1886" citStr="Griffiths et al., 2007" startWordPosition="275" endWordPosition="278">h between them. 1 Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very good with movie. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to</context>
<context position="26394" citStr="Griffiths et al., 2007" startWordPosition="4496" endWordPosition="4499">ves are often defined only in terms of the sets of entities they denote in the world. Most words are treated as atomic symbols with no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules. Conversely, vector space models have the attractive property that they can automatically extract knowledge from large corpora without supervision. Unlike logic-based approaches, these models allow us to make fine-grained statements about the semantic similarity of words which correlate well with human judgments (Griffiths et al., 2007). Logic-based approaches are often seen as orthogonal to distributional vector-based approaches. However, Garrette et al. (2011) recently introduced a combination of a vector space model inside a Markov Logic Network. One open question is whether vector-based models can learn some of the simple logic encountered in language such as negation or conjunctives. To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). This is a necessary (though not sufficien</context>
</contexts>
<marker>Griffiths, Tenenbaum, Steyvers, 2007</marker>
<rawString>T. L. Griffiths, J. B. Tenenbaum, and M. Steyvers. 2007. Topics in semantic representation. Psychological Review, 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Hendrickx</author>
<author>S N Kim</author>
<author>Z Kozareva</author>
<author>P Nakov</author>
<author>D O´ S´eaghdha</author>
<author>S Pad´o</author>
<author>M Pennacchiotti</author>
<author>L Romano</author>
<author>S Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov, D. O´ S´eaghdha, S. Pad´o, M. Pennacchiotti, L. Romano, and S. Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Mapping part-whole hierarchies into connectionist networks.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="37761" citStr="Hinton (1990)" startWordPosition="6293" endWordPosition="6294">t pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding mo</context>
</contexts>
<marker>Hinton, 1990</marker>
<rawString>G. E. Hinton. 1990. Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence, 46(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>B Rey</author>
<author>O Madani</author>
<author>W Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web.</booktitle>
<contexts>
<context position="1435" citStr="Jones et al., 2006" startWordPosition="202" endWordPosition="205">neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. 1 Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represent</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28781" citStr="Klein and Manning, 2003" startWordPosition="4907" endWordPosition="4910">truct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. Review sentence 1 The film is bright and flashy in all the right ways. 0 Not always too whimsical for its own good this strange hybrid of crime thriller, quirky character study, third-rate romance and female empowerment fantasy never really finds th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="36735" citStr="Lin, 1998" startWordPosition="6151" endWordPosition="6152">he recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Metcalfe</author>
</authors>
<title>A compositive holographic associative recall model.</title>
<date>1990</date>
<journal>Psychological Review,</journal>
<volume>88</volume>
<pages>661</pages>
<contexts>
<context position="37062" citStr="Metcalfe, 1990" startWordPosition="6197" endWordPosition="6198">tic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory</context>
</contexts>
<marker>Metcalfe, 1990</marker>
<rawString>E. J. Metcalfe. 1990. A compositive holographic associative recall model. Psychological Review, 88:627– 661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2738" citStr="Mitchell and Lapata, 2010" startWordPosition="415" endWordPosition="418">ghboring vectors. The same function is repeated to combine the phrase very good with movie. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... (</context>
<context position="9916" citStr="Mitchell and Lapata (2010)" startWordPosition="1598" endWordPosition="1601">ensionality X E Rnxn. While the initialization is random, the vectors and matrices will subsequently be modified to enable a sequence of words to compose a vector that can predict a distribution over semantic labels. Henceforth, we represent any phrase or sentence of length m as an ordered list of vectormatrix pairs ((a, A), ... , (m, M)), where each pair is retrieved based on the word at that position. 2.2 Composition Models for Two Words We first review composition functions for two words. In order to compute a parent vector p from two consecutive words and their respective vectors a and b, Mitchell and Lapata (2010) give as their most general function: p = f(a, b, R, K),where R is the a-priori known syntactic relation and K is background knowledge. There are many possible functions f. For our models, there is a constraint on p which is that it has the same dimensionality as each of the input vectors. This way, we can compare p easily with its children and p can be the input to a composition with another word. The latter is a requirement that will become clear in the next section. This excludes tensor products which were outperformed by simpler weighted addition and multiplication methods in (Mitchell and</context>
<context position="11775" citStr="Mitchell and Lapata, 2010" startWordPosition="1927" endWordPosition="1930"> such as the sigmoid or hyperbolic tangent tanh. Such a nonlinearity will allow us to approximate a wider range of functions beyond purely linear functions. We can also add a bias term before applying g but omit this for clarity. Rewriting the two transformed vectors as one vector z, we get p = g(Wz) which is a single layer neural network. In this model, the word matrices can capture compositional effects specific to each word, whereas W captures a general composition function. This function builds upon and generalizes several recent models in the literature. The most related work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; So</context>
<context position="22747" citStr="Mitchell and Lapata, 2010" startWordPosition="3895" endWordPosition="3898">dictions of the linear MVR model are almost identical to the standard RNN for these examples. defined as KL(g||p) = EZ gz log(gz/pz), where g is the gold distribution and p is the predicted one. We compare to several baselines and ablations of the MV-RNN model. An (adverb,adjective) pair is described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12,15, 20). All models performed best at vector sizes of below 12. Hence, it is the model’s power and not the number o</context>
<context position="36883" citStr="Mitchell and Lapata (2010)" startWordPosition="6169" endWordPosition="6173">ence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced represe</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a formal language.</title>
<date>1974</date>
<booktitle>Linguaggi nella Societa e nella Tecnica,</booktitle>
<pages>189--224</pages>
<contexts>
<context position="25528" citStr="Montague, 1974" startWordPosition="4363" endWordPosition="4364">e respectively at the top node. Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions. 3.2 Logic- and Vector-based Compositionality Another natural question is whether the MV-RNN can, in general, capture some of the simple boolean logic that is sometimes found in language. In other words, can it learn some of the propositional logic operators such as and, or, not in terms of vectors and matrices from a few examples. Answering this question can also be seen as a first step towards bridging the gap between logic-based, formal semantics (Montague, 1974) and vector space models. The logic-based view of language accounts nicely for compositionality by directly mapping syntactic constituents to lambda calculus expressions. At the word level, the focus is on function words, and nouns and adjectives are often defined only in terms of the sets of entities they denote in the world. Most words are treated as atomic symbols with no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules. Conversely, vector space models have the attractive property that they </context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. English as a formal language. Linguaggi nella Societa e nella Tecnica, pages 189–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>K Inui</author>
<author>S Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In NAACL, HLT.</booktitle>
<contexts>
<context position="28552" citStr="Nakagawa et al., 2010" startWordPosition="4870" endWordPosition="4874">s a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. Review sentence 1 Th</context>
<context position="29905" citStr="Nakagawa et al., 2010" startWordPosition="5098" endWordPosition="5102">r, quirky character study, third-rate romance and female empowerment fantasy never really finds the tonal or thematic glue it needs. 0 Doesn’t come close to justifying the hype that surrounded its debut at the Sundance film festival two years ago. 0 x Director Hoffman, his writer and Kline’s agent should serve detention. 1 x A bodice-ripper for intellectuals. Table 2: Hard movie review examples of positive (1) and negative (0) sentiment (S.) that of all methods only the MV-RNN predicted correctly (C: 0 or could not classify as correct either (C: x). Table 1 shows comparisons to the system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. The state of the art recursive autoencoder model of Socher et al. (2011c) obtained 77.7% accuracy. Our new MV-RNN gives the highest performance, outperforming also the linear MVR (Sec. 2.2). Table 2 shows several hard examples that only the MV-RNN was able to classify correctly. None of the methods correctly classified the last two examples which require more world knowledge. 5 Classification of Semantic Relationships The previous task considered global classification of an entire phrase or sentence. In our l</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In NAACL, HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Names and similarities on the web: fact extraction in the fast lane. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="36532" citStr="Pado and Lapata, 2007" startWordPosition="6123" endWordPosition="6126"> and trained another softmax classifier. The features and their performance increases were POS tags (+0.9); WordNet hypernyms (+1.3) and named en6 Related work Distributional approaches have become omnipresent for the recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nou</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="28529" citStr="Pang and Lee, 2005" startWordPosition="4866" endWordPosition="4869">imizer quickly yields a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>641</pages>
<contexts>
<context position="37505" citStr="Plate, 1995" startWordPosition="6255" endWordPosition="6256">n overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment an</context>
</contexts>
<marker>Plate, 1995</marker>
<rawString>T. A. Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623– 641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<contexts>
<context position="37685" citStr="Pollack, 1990" startWordPosition="6281" endWordPosition="6282">asured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Potts</author>
</authors>
<title>On the negativity of negation.</title>
<date>2010</date>
<booktitle>In David Lutz and Nan Li, editors, Proceedings of Semantics and Linguistic Theory 20.</booktitle>
<publisher>CLC Publications,</publisher>
<location>Ithaca, NY.</location>
<contexts>
<context position="19703" citStr="Potts (2010)" startWordPosition="3301" endWordPosition="3302">exts such as movie reviews. 3.1 Predicting Sentiment Distributions of Adverb-Adjective Pairs The first study considers the prediction of finegrained sentiment distributions of adverb-adjective pairs and analyzes different possibilities for computing the parent vector p. The results show that the MV-RNN operators are powerful enough to capture the operational meanings of various types of adverbs. For example, very is an intensifier, pretty is an attenuator, and not can negate or strongly attenuate the positivity of an adjective. For instance not great is still pretty good and not terrible; see Potts (2010) for details. We use a publicly available IMDB dataset of extracted adverb-adjective pairs from movie reviews.1 The dataset provides the distribution over star ratings: Each consecutive word pair appears a certain number of times in reviews that have also associated with them an overall rating of the movie. After normalizing by the total number of occurrences, one gets a multinomial distribution over ratings. Only word pairs that appear at least 50 times are kept. Of the remaining pairs, we use 4211 randomly sampled ones for training and a separate set of 1804 for testing. We never give the al</context>
</contexts>
<marker>Potts, 2010</marker>
<rawString>C. Potts. 2010. On the negativity of negation. In David Lutz and Nan Li, editors, Proceedings of Semantics and Linguistic Theory 20. CLC Publications, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1591" citStr="Ratinov et al., 2011" startWordPosition="225" endWordPosition="228">te of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. 1 Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very goo</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rink</author>
<author>S Harabagiu</author>
</authors>
<title>UTD: Classifying semantic relations by combining lexical and semantic resources.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="35783" citStr="Rink and Harabagiu, 2010" startWordPosition="6006" endWordPosition="6009">tures, Levin classes, PropBank, FrameNet, NomLex-Plus, Google n-grams, paraphrases, TextRunner RNN - 74.8 Lin.MVR - 73.0 MV-RNN - 79.1 RNN POS,WordNet,NER 77.6 Lin.MVR POS,WordNet,NER 78.7 MV-RNN POS,WordNet,NER 82.4 Table 4: Learning methods, their feature sets and F1 results for predicting semantic relations between nouns. The MV-RNN outperforms all but one method without any additional feature sets. By adding three such features, it obtains state of the art performance. maining inputs and the training setup are the same as in previous sentiment experiments. The best method on this dataset (Rink and Harabagiu, 2010) obtains 82.2% F1. In order to see whether our system can improve over this system, we added three features to the MV-RNN vector and trained another softmax classifier. The features and their performance increases were POS tags (+0.9); WordNet hypernyms (+1.3) and named en6 Related work Distributional approaches have become omnipresent for the recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributio</context>
</contexts>
<marker>Rink, Harabagiu, 2010</marker>
<rawString>B. Rink and S. Harabagiu. 2010. UTD: Classifying semantic relations by combining lexical and semantic resources. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rudolph</author>
<author>E Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language. In</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<pages>24--97</pages>
<contexts>
<context position="37577" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="6263" endWordPosition="6266">from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix </context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>S. Rudolph and E. Giesbrecht. 2010. Compositional matrix-space models of language. In ACL. H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24:97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="16676" citStr="Socher et al., 2010" startWordPosition="2815" endWordPosition="2818">pi, Pi) from the bottom-up and then take derivatives of the softmax classifiers at each node in the tree from the top down. Derivatives are computed efficiently via backpropagation through structure (Goller and K¨uchler, 1996). Even though the ... very good movie ... (a , A) (b , B) (c , C) Matrix-Vector Recursive Neural Network (p1 , P1) ( p2, P2 ) p2 = g(W ) LP1c Cp1 P2 = WM LC ] P1 1 N (x,t) ∂J ∂θ = 1204 objective is not convex, we found that L-BFGS run over the complete training data (batch mode) minimizes the objective well in practice and convergence is smooth. For more information see (Socher et al., 2010). 2.6 Low-Rank Matrix Approximations If every word is represented by an n-dimensional vector and additionally by an n x n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5) where U E Rnxr, V E Rrxn, a E Rn and we set the rank for all experiments to r = 3. 2.7 Discussion: Evaluation and Generality Evaluation of compositional vector spaces is a complex task. Most related work compares simil</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS.</title>
<date>2011</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2841" citStr="Socher et al., 2011" startWordPosition="431" endWordPosition="434">cess, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) f(Ba, Ab)= Ba= Ab= Recursive Matrix-Vector Model ... - vector - matrix 120</context>
<context position="6280" citStr="Socher et al. (2011" startWordPosition="981" endWordPosition="984">resentations of multi-word units from single word vector representations has been to form a linear combination of the single word representations, such as a sum or weighted average. This happens in information retrieval and in various text similarity functions based on lexical similarity. These approaches can work well when the meaning of a text is literally “the sum of its parts”, but fails when words function as operators that modify the meaning of another word: the meaning of “extremely strong” cannot be captured as the sum of word representations for “extremely” and “strong.” The model of Socher et al. (2011c) provided a new possibility for moving beyond a linear combination, through use of a matrix W that multiplied the word vectors (a, b), and a nonlinearity function g (such as a sigmoid or tanh). They compute the parent vector p that describes both words as [ a p = gCW b J/ and apply this function recursively inside a binarized parse tree so that it can compute vectors for multiword sequences. Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all n</context>
<context position="12348" citStr="Socher et al., 2011" startWordPosition="2037" endWordPosition="2040">ted work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2011a) as described above are also special cases with both A and B set to the identity matrix. We will compare to these special cases in our experiments. 1203 Figure 2: Example of how the MV-RNN merges a phrase with another word at a nonterminal node of a parse tree. 2.3 Recursive Compositions of Multiple Words and Phrases This section describes how we extend a word-pair matrix-vector-based compositional model to learn vectors and matrices for longer sequences of words. The main idea is to apply the same function f to pairs of constituents in a parse tre</context>
<context position="15212" citStr="Socher et al., 2011" startWordPosition="2547" endWordPosition="2550"> p) which can also be seen as features describing that phrase. We train these representations by adding on top of each parent node a simple softmax classifier to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d E RK is a K-dimensional multinomial distribution. For the applications below (excluding logic), the corresponding error function E(s, t, θ) that we minimize for a sentence s and its tree t is the sum of cross-entropy errors at all nodes. The only other methods that use this type of objective function are (Socher et al., 2011b; Socher et al., 2011c), who also combine it with either a score or reconstruction error. Hence, for comparisons to other related work, we need to merge variations of computing the parent vector p with this classifier. The main difference is that the MV-RNN has more flexibility since it has an input specific recursive function fA,B to compute each parent. In the following applications, we will use the softmax classifier to predict both sentiment distributions and noun-noun relationships. 2.5 Learning Let θ = (W, WM, Wlabel, L, LM) be our model parameters and λ a vector with regularization hyp</context>
<context position="23032" citStr="Socher et al., 2011" startWordPosition="3945" endWordPosition="3948"> described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12,15, 20). All models performed best at vector sizes of below 12. Hence, it is the model’s power and not the number of parameters that determines the performance. The table in Fig. 3 shows the average KL-divergence on the test set. It shows that the idea of matrix-vector representations for all words and having a nonlinearity are both important. The MV-RNN which combines these two ideas is best able</context>
<context position="28573" citStr="Socher et al., 2011" startWordPosition="4875" endWordPosition="4878">. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. Review sentence 1 The film is bright and </context>
<context position="30062" citStr="Socher et al. (2011" startWordPosition="5125" endWordPosition="5128">tifying the hype that surrounded its debut at the Sundance film festival two years ago. 0 x Director Hoffman, his writer and Kline’s agent should serve detention. 1 x A bodice-ripper for intellectuals. Table 2: Hard movie review examples of positive (1) and negative (0) sentiment (S.) that of all methods only the MV-RNN predicted correctly (C: 0 or could not classify as correct either (C: x). Table 1 shows comparisons to the system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. The state of the art recursive autoencoder model of Socher et al. (2011c) obtained 77.7% accuracy. Our new MV-RNN gives the highest performance, outperforming also the linear MVR (Sec. 2.2). Table 2 shows several hard examples that only the MV-RNN was able to classify correctly. None of the methods correctly classified the last two examples which require more world knowledge. 5 Classification of Semantic Relationships The previous task considered global classification of an entire phrase or sentence. In our last experiment we show that the MV-RNN can also learn how a syntactic context composes an aggregate meaning of the semantic relationships between words. In p</context>
<context position="37993" citStr="Socher et al., 2011" startWordPosition="6328" endWordPosition="6331">dels are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. Manning. 2011a. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2841" citStr="Socher et al., 2011" startWordPosition="431" endWordPosition="434">cess, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) f(Ba, Ab)= Ba= Ab= Recursive Matrix-Vector Model ... - vector - matrix 120</context>
<context position="6280" citStr="Socher et al. (2011" startWordPosition="981" endWordPosition="984">resentations of multi-word units from single word vector representations has been to form a linear combination of the single word representations, such as a sum or weighted average. This happens in information retrieval and in various text similarity functions based on lexical similarity. These approaches can work well when the meaning of a text is literally “the sum of its parts”, but fails when words function as operators that modify the meaning of another word: the meaning of “extremely strong” cannot be captured as the sum of word representations for “extremely” and “strong.” The model of Socher et al. (2011c) provided a new possibility for moving beyond a linear combination, through use of a matrix W that multiplied the word vectors (a, b), and a nonlinearity function g (such as a sigmoid or tanh). They compute the parent vector p that describes both words as [ a p = gCW b J/ and apply this function recursively inside a binarized parse tree so that it can compute vectors for multiword sequences. Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all n</context>
<context position="12348" citStr="Socher et al., 2011" startWordPosition="2037" endWordPosition="2040">ted work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2011a) as described above are also special cases with both A and B set to the identity matrix. We will compare to these special cases in our experiments. 1203 Figure 2: Example of how the MV-RNN merges a phrase with another word at a nonterminal node of a parse tree. 2.3 Recursive Compositions of Multiple Words and Phrases This section describes how we extend a word-pair matrix-vector-based compositional model to learn vectors and matrices for longer sequences of words. The main idea is to apply the same function f to pairs of constituents in a parse tre</context>
<context position="15212" citStr="Socher et al., 2011" startWordPosition="2547" endWordPosition="2550"> p) which can also be seen as features describing that phrase. We train these representations by adding on top of each parent node a simple softmax classifier to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d E RK is a K-dimensional multinomial distribution. For the applications below (excluding logic), the corresponding error function E(s, t, θ) that we minimize for a sentence s and its tree t is the sum of cross-entropy errors at all nodes. The only other methods that use this type of objective function are (Socher et al., 2011b; Socher et al., 2011c), who also combine it with either a score or reconstruction error. Hence, for comparisons to other related work, we need to merge variations of computing the parent vector p with this classifier. The main difference is that the MV-RNN has more flexibility since it has an input specific recursive function fA,B to compute each parent. In the following applications, we will use the softmax classifier to predict both sentiment distributions and noun-noun relationships. 2.5 Learning Let θ = (W, WM, Wlabel, L, LM) be our model parameters and λ a vector with regularization hyp</context>
<context position="23032" citStr="Socher et al., 2011" startWordPosition="3945" endWordPosition="3948"> described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12,15, 20). All models performed best at vector sizes of below 12. Hence, it is the model’s power and not the number of parameters that determines the performance. The table in Fig. 3 shows the average KL-divergence on the test set. It shows that the idea of matrix-vector representations for all words and having a nonlinearity are both important. The MV-RNN which combines these two ideas is best able</context>
<context position="28573" citStr="Socher et al., 2011" startWordPosition="4875" endWordPosition="4878">. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. Review sentence 1 The film is bright and </context>
<context position="30062" citStr="Socher et al. (2011" startWordPosition="5125" endWordPosition="5128">tifying the hype that surrounded its debut at the Sundance film festival two years ago. 0 x Director Hoffman, his writer and Kline’s agent should serve detention. 1 x A bodice-ripper for intellectuals. Table 2: Hard movie review examples of positive (1) and negative (0) sentiment (S.) that of all methods only the MV-RNN predicted correctly (C: 0 or could not classify as correct either (C: x). Table 1 shows comparisons to the system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. The state of the art recursive autoencoder model of Socher et al. (2011c) obtained 77.7% accuracy. Our new MV-RNN gives the highest performance, outperforming also the linear MVR (Sec. 2.2). Table 2 shows several hard examples that only the MV-RNN was able to classify correctly. None of the methods correctly classified the last two examples which require more world knowledge. 5 Classification of Semantic Relationships The previous task considered global classification of an entire phrase or sentence. In our last experiment we show that the MV-RNN can also learn how a syntactic context composes an aggregate meaning of the semantic relationships between words. In p</context>
<context position="37993" citStr="Socher et al., 2011" startWordPosition="6328" endWordPosition="6331">dels are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. </context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2841" citStr="Socher et al., 2011" startWordPosition="431" endWordPosition="434">cess, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) f(Ba, Ab)= Ba= Ab= Recursive Matrix-Vector Model ... - vector - matrix 120</context>
<context position="6280" citStr="Socher et al. (2011" startWordPosition="981" endWordPosition="984">resentations of multi-word units from single word vector representations has been to form a linear combination of the single word representations, such as a sum or weighted average. This happens in information retrieval and in various text similarity functions based on lexical similarity. These approaches can work well when the meaning of a text is literally “the sum of its parts”, but fails when words function as operators that modify the meaning of another word: the meaning of “extremely strong” cannot be captured as the sum of word representations for “extremely” and “strong.” The model of Socher et al. (2011c) provided a new possibility for moving beyond a linear combination, through use of a matrix W that multiplied the word vectors (a, b), and a nonlinearity function g (such as a sigmoid or tanh). They compute the parent vector p that describes both words as [ a p = gCW b J/ and apply this function recursively inside a binarized parse tree so that it can compute vectors for multiword sequences. Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all n</context>
<context position="12348" citStr="Socher et al., 2011" startWordPosition="2037" endWordPosition="2040">ted work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2011a) as described above are also special cases with both A and B set to the identity matrix. We will compare to these special cases in our experiments. 1203 Figure 2: Example of how the MV-RNN merges a phrase with another word at a nonterminal node of a parse tree. 2.3 Recursive Compositions of Multiple Words and Phrases This section describes how we extend a word-pair matrix-vector-based compositional model to learn vectors and matrices for longer sequences of words. The main idea is to apply the same function f to pairs of constituents in a parse tre</context>
<context position="15212" citStr="Socher et al., 2011" startWordPosition="2547" endWordPosition="2550"> p) which can also be seen as features describing that phrase. We train these representations by adding on top of each parent node a simple softmax classifier to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d E RK is a K-dimensional multinomial distribution. For the applications below (excluding logic), the corresponding error function E(s, t, θ) that we minimize for a sentence s and its tree t is the sum of cross-entropy errors at all nodes. The only other methods that use this type of objective function are (Socher et al., 2011b; Socher et al., 2011c), who also combine it with either a score or reconstruction error. Hence, for comparisons to other related work, we need to merge variations of computing the parent vector p with this classifier. The main difference is that the MV-RNN has more flexibility since it has an input specific recursive function fA,B to compute each parent. In the following applications, we will use the softmax classifier to predict both sentiment distributions and noun-noun relationships. 2.5 Learning Let θ = (W, WM, Wlabel, L, LM) be our model parameters and λ a vector with regularization hyp</context>
<context position="23032" citStr="Socher et al., 2011" startWordPosition="3945" endWordPosition="3948"> described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12,15, 20). All models performed best at vector sizes of below 12. Hence, it is the model’s power and not the number of parameters that determines the performance. The table in Fig. 3 shows the average KL-divergence on the test set. It shows that the idea of matrix-vector representations for all words and having a nonlinearity are both important. The MV-RNN which combines these two ideas is best able</context>
<context position="28573" citStr="Socher et al., 2011" startWordPosition="4875" endWordPosition="4878">. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2www.socher.org 1207 Method Acc. Tree-CRF (Nakagawa et al., 2010) 77.3 RAE (Socher et al., 2011c) 77.7 Linear MVR 77.1 MV-RNN 79.0 Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. Review sentence 1 The film is bright and </context>
<context position="30062" citStr="Socher et al. (2011" startWordPosition="5125" endWordPosition="5128">tifying the hype that surrounded its debut at the Sundance film festival two years ago. 0 x Director Hoffman, his writer and Kline’s agent should serve detention. 1 x A bodice-ripper for intellectuals. Table 2: Hard movie review examples of positive (1) and negative (0) sentiment (S.) that of all methods only the MV-RNN predicted correctly (C: 0 or could not classify as correct either (C: x). Table 1 shows comparisons to the system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. The state of the art recursive autoencoder model of Socher et al. (2011c) obtained 77.7% accuracy. Our new MV-RNN gives the highest performance, outperforming also the linear MVR (Sec. 2.2). Table 2 shows several hard examples that only the MV-RNN was able to classify correctly. None of the methods correctly classified the last two examples which require more world knowledge. 5 Classification of Semantic Relationships The previous task considered global classification of an entire phrase or sentence. In our last experiment we show that the MV-RNN can also learn how a syntactic context composes an aggregate meaning of the semantic relationships between words. In p</context>
<context position="37993" citStr="Socher et al., 2011" startWordPosition="6328" endWordPosition="6331">dels are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011c. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1636" citStr="Turney and Pantel, 2010" startWordPosition="232" endWordPosition="235">nt experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. 1 Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very good with movie. Despite their success, single w</context>
<context position="36582" citStr="Turney and Pantel, 2010" startWordPosition="6131" endWordPosition="6134">atures and their performance increases were POS tags (+0.9); WordNet hypernyms (+1.3) and named en6 Related work Distributional approaches have become omnipresent for the recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics – e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words (Pado and Lapata, 2007; Baroni and Lenci, 2010; Turney and Pantel, 2010). These representations have proven very effective in sense discrimination and disambiguation (Sch¨utze, 1998), automatic thesaurus extraction (Lin, 1998; Curran, 2004) and selectional preferences. There are several sophisticated ideas for compositionality in vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with hu</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second AAAI Symposium on Quantum Interaction.</booktitle>
<contexts>
<context position="37454" citStr="Widdows, 2008" startWordPosition="6250" endWordPosition="6251">n vector spaces. Mitchell and Lapata (2010) present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution (Metcalfe, 1990). They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above. 3sourceforge.net/projects/supersensetag/ 1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yes</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>D. Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second AAAI Symposium on Quantum Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>C Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2820" citStr="Yessenalina and Cardie, 2011" startWordPosition="427" endWordPosition="430"> with movie. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) f(Ba, Ab)= Ba= Ab= Recursive Matrix-Vector Model ... </context>
<context position="38080" citStr="Yessenalina and Cardie (2011)" startWordPosition="6342" endWordPosition="6345">08), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. The </context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>A. Yessenalina and C. Cardie. 2011. Compositional matrix-space models for sentiment analysis. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>I Korkontzelos</author>
<author>F Fallucchi</author>
<author>S Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<publisher>COLING.</publisher>
<contexts>
<context position="2790" citStr="Zanzotto et al., 2010" startWordPosition="423" endWordPosition="426">ne the phrase very good with movie. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) ... very good movie ... ( a , A ) ( b , B ) ( c , C ) f(Ba, Ab)= Ba= Ab= Recu</context>
<context position="7310" citStr="Zanzotto et al., 2010" startWordPosition="1162" endWordPosition="1165"> the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all natural language operators. After all, inside the function g, we have the same linear transformation for all possible pairs of word vectors. Recent work has started to capture the behavior of natural language operators inside semantic vector spaces by modeling them as matrices, which would allow a matrix for “extremely” to appropriately modify vectors for “smelly” or “strong” (Baroni and Zamparelli, 2010; Zanzotto et al., 2010). These approaches are along the right lines but so far have been restricted to capture linear functions of pairs of words whereas we would like nonlinear functions to compute compositional meaning representations for multi-word phrases or full sentences. The MV-RNN combines the strengths of both of these ideas by (i) assigning a vector and a matrix to every word and (ii) learning an input-specific, nonlinear, compositional function for computing vector and matrix representations for multi-word sequences of any syntactic type. Assigning vector-matrix representations to all words instead of onl</context>
<context position="11799" citStr="Zanzotto et al., 2010" startWordPosition="1931" endWordPosition="1935">erbolic tangent tanh. Such a nonlinearity will allow us to approximate a wider range of functions beyond purely linear functions. We can also add a bias term before applying g but omit this for clarity. Rewriting the two transformed vectors as one vector z, we get p = g(Wz) which is a single layer neural network. In this model, the word matrices can capture compositional effects specific to each word, whereas W captures a general composition function. This function builds upon and generalizes several recent models in the literature. The most related work is that of (Mitchell and Lapata, 2010; Zanzotto et al., 2010) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0nxn. Lastly, the models of (Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2011a) as d</context>
<context position="22771" citStr="Zanzotto et al., 2010" startWordPosition="3899" endWordPosition="3902">model are almost identical to the standard RNN for these examples. defined as KL(g||p) = EZ gz log(gz/pz), where g is the gold distribution and p is the predicted one. We compare to several baselines and ablations of the MV-RNN model. An (adverb,adjective) pair is described by its vectors (a, b) and matrices (A, B). 1 p = 0.5(a + b), vector average 2. p = a ® b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10−4,10−3) and word vector sizes (n = 6, 8,10,12,15, 20). All models performed best at vector sizes of below 12. Hence, it is the model’s power and not the number of parameters that determ</context>
<context position="37972" citStr="Zanzotto et al., 2010" startWordPosition="6324" endWordPosition="6327">1209 Other important models are tensor products (Clark and Pulman, 2007), quantum logic (Widdows, 2008), holographic reduced representations (Plate, 1995) and the Compositional Matrix Space model (Rudolph and Giesbrecht, 2010). RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) (Pollack, 1990) or recurrent neural networks (Elman, 1991). Bottou (2011) and Hinton (1990) discussed related models such as recursive autoencoders for text understanding. Our model builds upon and generalizes the models of (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011c) (see Sec. 2.2). We compare to them in our experiments. Yessenalina and Cardie (2011) introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to thi</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Manandhar. 2010. Estimating linear models for compositional distributional semantics. COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
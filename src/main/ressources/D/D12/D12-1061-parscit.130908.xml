<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.876962">
Learning Lexicon Models from Search Logs for Query Expansion
</title>
<author confidence="0.981878">
Jianfeng Gao Shasha Xie
</author>
<affiliation confidence="0.98247">
Microsoft Research, Redmond Educational Testing Service, Princeton
</affiliation>
<address confidence="0.702283">
Washington 98052, USA New Jersey 08540, USA
</address>
<email confidence="0.984012">
jfgao@microsoft.com sxie@ets.org
</email>
<author confidence="0.990394">
Xiaodong He Alnur Ali
</author>
<affiliation confidence="0.933463">
Microsoft Research, Redmond Microsoft Bing, Bellevue
</affiliation>
<address confidence="0.822599">
Washington 98052, USA Washington 98004, USA
</address>
<email confidence="0.997693">
xiaohe@microsoft.com alnurali@microsoft.com
</email>
<sectionHeader confidence="0.996646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915333333333">
This paper explores log-based query expan-
sion (QE) models for Web search. Three
lexicon models are proposed to bridge the
lexical gap between Web documents and
user queries. These models are trained on
pairs of user queries and titles of clicked
documents. Evaluations on a real world data
set show that the lexicon models, integrated
into a ranker-based QE system, not only
significantly improve the document retriev-
al performance but also outperform two
state-of-the-art log-based QE methods.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883781818182">
Term mismatch is a fundamental problem in Web
search, where queries and documents are com-
posed using different vocabularies and language
styles. Query expansion (QE) is an effective strate-
gy to address the problem. It expands a query is-
sued by a user with additional related terms, called
expansion terms, so that more relevant documents
can be retrieved.
In this paper we explore the use of clickthrough
data and translation models for QE. We select ex-
pansion terms for a query according to how likely
it is that the expansion terms occur in the title of a
document that is relevant to the query. Assuming
that a query is parallel to the titles of documents
clicked for that query (Gao et al. 2010a), three lex-
icon models are trained on query-title pairs ex-
tracted from clickthrough data. The first is a word
model that learns the translation probability be-
tween single words. The second model uses lexi-
calized triplets to incorporate word dependencies
for translation. The third is a bilingual topic model,
which represents a query as a distribution of hid-
den topics and learns the translation between a
query and a title term at the semantic level. We
will show that the word model provides a rich set
of expansion candidates while the triplet and topic
models can effectively select good expansion
terms, and that a ranker-based QE system which
incorporates all three of these models not only sig-
nificantly improves Web search result but outper-
forms other log-based QE methods that are state-
of-the-art.
There is growing interest in applying user logs
to improve QE. A recent survey is due to Baeze-
Yates and Ribeiro-Neto (2011). Below, we briefly
discuss two log-based QE methods that are closest
to ours and are re-implemented in this study for
comparison. Both systems use the same type of log
data that we used to train the lexicon models. The
term correlation model of Cui et al. (2002; 2003) is
to our knowledge the first to explore query-
document relations for direct extraction of expan-
sion terms for Web search. The method outper-
forms traditional QE methods that do not use log
data e.g. the local analysis model of Xu and Croft
(1996). In addition, as pointed out by Cui et al.
(2003) there are three important advantages that
make log-based QE a promising technology to im-
prove the performance of commercial search en-
gines. First, unlike traditional QE methods that are
based on relevance feedback, log-based QE derives
expansion terms from search logs, allowing term
correlations to be pre-computed offline. Compared
to methods that are based on thesauri either com-
piled manually (Prager et al. 2001) or derived au-
</bodyText>
<page confidence="0.997537">
666
</page>
<bodyText confidence="0.999677717948718">
tomatically from document collections (Jing and
Croft 1994), the log-based method is superior in
that it explicitly captures the correlation between
query terms and document terms, and thus can
bridge the lexical gap between them more effec-
tively. Second, since search logs retrain query-
document pairs clicked by millions of users, the
term correlations reflect the preference of the ma-
jority of users. Third, the term correlations evolve
along with the accumulation of user logs, thus can
reflect updated user interests at a specific time.
However, as pointed out by Riezler et al.
(2008), Cui et al.’s correlation-based method suf-
fers low precision of QE partly because the corre-
lation model does not explicitly capture context
information and is susceptible to noise. Riezler et
al. developed a QE system by retraining a standard
phrase-based statistical machine translation (SMT)
system using query-snippet pairs extracted from
clickthrough data (Riezler et al. 2008; Riezler and
Liu 2010). The SMT-based system can produce
cleaner, more relevant expansion terms because
rich context information useful for filtering noisy
expansions is captured by combining language
model and phrase translation model in its decoder.
Furthermore, in the SMT system all component
models are properly smoothed using sophisticated
techniques to avoid sparse data problems while the
correlation model relies on pure counts of term
frequencies. However, the SMT system is used as a
black box in their experiments. So the relative con-
tribution of different SMT components is not veri-
fied empirically. In this study we break this black
box in order to build a better, simpler QE system.
We will show that the proposed lexicon models
outperform significantly the term correlation mod-
el, and that a simpler QE system that incorporates
the lexicon models can beat the sophisticated,
black-box SMT system.
</bodyText>
<sectionHeader confidence="0.995278" genericHeader="method">
2 Lexicon Models
</sectionHeader>
<bodyText confidence="0.9999685">
We view search queries and Web documents as
two different languages, and cast QE as a means to
bridge the language gap by translating queries to
documents, represented by their titles. In this sec-
tion, we will describe three translation models that
are based on terms, triplets, and topics, respective-
ly, and the way these models are learned from que-
ry-title pairs extracted from clickthrough data.
</bodyText>
<subsectionHeader confidence="0.970746">
2.1 Word Model
</subsectionHeader>
<bodyText confidence="0.9991595">
The word model takes the form of IBM Model 1
(Brown et al. 1993; Berger and Lafferty 1999). Let
</bodyText>
<equation confidence="0.9519715">
Q q be a query, e be an expansion term
1, ... , q
</equation>
<bodyText confidence="0.888845">
candidate, the translation probability from Q to e is
defined as
</bodyText>
<equation confidence="0.9915545">
I Q)wm =I P(eI qj) P( qjl Q) (1)
j=1
</equation>
<bodyText confidence="0.999970833333333">
where P (q I Q) is the unsmoothed unigram proba-
bility of word q in query Q. The word translation
probabilities P (e l q) are estimated on the query-
title pairs derived from the clickthrough data by
assuming that the title terms are likely to be the
desired expansions of the paired query. Our train-
ing method follows the standard procedure of
training statistical word alignment models pro-
posed by Brown et al. (1993). Formally, we opti-
mize the model parameters B by maximizing the
probability of generating document titles from que-
ries over the entire training corpus:
</bodyText>
<equation confidence="0.999244666666667">
H
0* r x II P Di� Qi,0 (2)
i=1
</equation>
<bodyText confidence="0.9987645">
where both the titles D and the paired queries Q are
viewed as bag of words. The translation probability
</bodyText>
<equation confidence="0.9480138">
P(Di I Qi, 0) takes the form of IBM Model 1 as
I 1
U + 1)I
E III P(wiI qj, 0) (3)
i=1 j=1
</equation>
<bodyText confidence="0.999894">
where is a constant, I is the length of D, and j is
the length of Q. To find the optimal word transla-
tion probabilities of IBM Model 1, we used the EM
algorithm, where the number of iterations is deter-
mined empirically on held-out data.
</bodyText>
<subsectionHeader confidence="0.992583">
2.2 Triplet Model
</subsectionHeader>
<bodyText confidence="0.999359">
The word model is context independent. The triplet
model, which is originally proposed for SMT (Ha-
san et al. 2008), is intended to capture inter-term
dependencies for selecting expansion terms. The
model is based on lexicalized triplets ( e, q, q&apos;)
which can be understood as two query terms trig-
gering one expansion term. The translation proba-
bility of e given Q for the triplet model is parame-
terized as
</bodyText>
<equation confidence="0.894044333333333">
S Q,0 _
667
 |∑ ∑ (  |) (4)
where Z is a normalization factor based on the cor-
responding query length, i.e., , and
(  |) is the probability of translating into
</equation>
<bodyText confidence="0.96381">
given another query word . Since can be
any word in that is not necessary to be adjacent
to , the triple model is able to combine local (i.e.
word and phrase level) and global (i.e. query level)
contextual information useful for word translation.
Similar to the case of word model, we used the
EM algorithm to estimate the translation probabili-
ties  |on the query-title pairs. Since the
number of all possible triplets ( ) is large and
as a consequence the model training could suffer
the data sparseness problem, in our experiments
count-based cutoff is applied to prune the model to
a manageable size.
</bodyText>
<subsectionHeader confidence="0.999249">
2.3 Bilingual Topic Model (BLTM)
</subsectionHeader>
<bodyText confidence="0.998671954545455">
The BLTM was originally proposed for Web doc-
ument ranking by Gao et al. (2011). The idea un-
derlying the model is that a search query and its
relevant Web documents share a common distribu-
tion of (hidden) topics, but use different (probably
overlapping) vocabularies to express these topics.
Intuitively, BLTM-based QE works as follows.
First, a query is represented as a vector of topics.
Then, all the candidate expansion terms, which are
selected from document, are ranked by how likely
it is that these document terms are selected to best
describe those topics. In a sense, BLTM is similar
to the word model and the triplet model since they
all map a query to a document word. BLTM differs
in that the mapping is performed at the topic level
(via a language independent semantic representa-
tion) rather than at the word level. In our experi-
ments BLTM is found to often select a different set
of expansion terms and is complementary to the
word model and the triplet model.
Formally, BLTM-based QE assumes the follow-
ing story of generating from :
</bodyText>
<footnote confidence="0.8144645">
1. First, for each topic , a pair of different
word distributions are selected
from a Dirichlet prior with concentration pa-
rameter β, where is a topic-specific query
</footnote>
<table confidence="0.9984065">
Original query jaguar locator
Ranked expansion jaguar finder
candidates car locator
(altered words are in jaguar location
italic) jaguar directory
...
jaguar list
Expanded query OR(jaguar, car)
(selected expansion OR(locator, finder, location,
terms are in italic) directory)
</table>
<figureCaption confidence="0.999143666666667">
Figure 1. An example of an original query, its expan-
sion candidates and the expanded query generated by
the ranker-based QE system.
</figureCaption>
<bodyText confidence="0.5663156">
term distribution, and a topic-specific
document term distribution. Assuming there
are topics, we have two sets of distribu-
tions and
.
</bodyText>
<listItem confidence="0.899432428571429">
2. Given , a topic distribution is drawn
from a Dirichlet prior with concentration pa-
rameter .
3. Then a document term (i.e., expansion term
candidate) is generated by first selecting a
topic according to the topic distribution ,
and then drawing a word from .
</listItem>
<bodyText confidence="0.9954365">
By summing over all possible topics, we end up
with the following model form
</bodyText>
<equation confidence="0.951328">
 |∑   ||(5)
</equation>
<bodyText confidence="0.999952666666667">
The BLTM training follows the method described
in Gao et al. (2011). We used the EM algorithm to
estimate the parameters ( of BLTM by
maximizing the joint log-likelihood of the query-
title pairs and the parameters. In training, we also
constrain that the paired query and title have simi-
lar fractions of tokens assigned to each topic. The
constraint is enforced on expectation using posteri-
or regularization (Ganchev et al. 2010).
</bodyText>
<sectionHeader confidence="0.982849" genericHeader="method">
3 A Ranker-Based QE System
</sectionHeader>
<bodyText confidence="0.980538333333333">
This section describes a ranker-based QE system in
which the three lexicon models described above
are incorporated. The system expands an input
query in two distinct stages, candidate generation
and ranking, as illustrated by an example in Figure
1.
</bodyText>
<page confidence="0.992457">
668
</page>
<bodyText confidence="0.999988090909091">
In candidate generation, an input query Q is
first tokenized into a sequence of terms. For each
term q that is not a stop word, we consult a word
model described in Section 2.1 to identify the best
M altered words according to their word transla-
tion probabilities from q. Then, we form a list of
expansion candidates, each of which contains all
the original words in Q except for the word that is
substituted by one of its altered words. So, for a
query with J terms, there are at most M x J candi-
dates.
In the second stage, all the expansion candidates
are ranked using a ranker that is based on the Mar-
kov Random Field (MRF) model in which the
three lexicon models are incorporated as features.
Expansion terms of a query are taken from those
terms in the N-best ( N = 10 in our experiments)
expansion candidates of the query that have not
been seen in the original query string.
In the remainder of this section we will describe
in turn the MRF-based ranker, the ranking features,
and the way the ranker parameters are estimated.
</bodyText>
<subsectionHeader confidence="0.999512">
3.1 MRF-Based Ranker
</subsectionHeader>
<bodyText confidence="0.999879428571429">
The ranker is based on the MRF model that models
the joint distribution of PA(E, Q) over a set of ex-
pansion term random variables E = fel,..., el} and
a query random variable Q. It is constructed from a
graph G consisting of a query node and nodes for
each expansion term. Nodes in the graph represent
random variables and edges define the independ-
ence semantics between the variables. An MRF
satisfies the Markov property (Bishop 2006),
which states that a node is independent of all of its
non-neighboring nodes given observed values of
its neighbors, defined by the clique configurations
of G. The joint distribution over the random varia-
bles in G is defined as
</bodyText>
<equation confidence="0.969971">
1
PA(E, Q) = Z F1 T (c; A) (6)
A CEC(G)
</equation>
<bodyText confidence="0.999543444444444">
where C(G) is the set of cliques in G, and each
T (c; A) is a non-negative potential function de-
fined over a clique configuration c that measures
the compatibility of the configuration, A is a set of
parameters that are used within the potential func-
tion, and ZA normalizes the distribution. For rank-
ing expansion candidates, we can drop the expen-
sive computation of ZA since it is independent of
E, and simply rank each expansion candidate E by
</bodyText>
<figureCaption confidence="0.90579">
Figure 2: The structure of the Markov random field for
representing the term dependency among the query Q
and the expansion terms el, ... , el.
</figureCaption>
<bodyText confidence="0.949772142857143">
its unnormalized joint probability with Q under the
MRF. It is common to define MRF potential func-
tions of the exponential form as T (c; A) =
p( , where (c) is a real-valued feature
(c))
function over clique values and Ac is the weight of
the feature function. Then, we can compute the
</bodyText>
<equation confidence="0.897752166666667">
posterior PA (E I Q) as
PA(EI Q) = PA (E, Q) (7)
PA(Q)
rank I
� L log T (c; A) = I
CEC(G) CEC(
</equation>
<bodyText confidence="0.999667909090909">
which is essentially a weighted linear combination
of a set of features.
Therefore, to instantiate the MRF model, one
needs to define a graph structure and a set of po-
tential functions. In this paper, the graphical model
representation we propose for QE is a fully con-
nected graph shown in Figure 2, where all expan-
sion terms and the original query are assumed de-
pendent with each other. In what follows, we will
define six types of cliques that we are interested in
defining features (i.e., potential functions) over.
</bodyText>
<subsectionHeader confidence="0.994055">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9999943">
The cliques and features are inspired by the com-
ponent models used in SMT systems. The cliques
defined in G for MRF can be grouped into two cat-
egories. The first includes three types of cliques
involving both the query node and one or more
expansion terms. The potential functions defined
over these cliques attempt to abstract the idea be-
hind the query to title translation models. The other
three types, belonging to the second category, in-
volve only expansion terms. Their potential func-
</bodyText>
<page confidence="0.994334">
669
</page>
<bodyText confidence="0.9997663">
tions attempt to abstract the idea behind the target
language models.
The first type of cliques involves a single ex-
pansion term and the query node. The potentials
functions for these cliques are defined as
where the three feature functions of the form
are defined as the log probabilities of
translating to according to the word, triplet and
topic models defined in Equations (1), (4) and (5),
respectively.
</bodyText>
<equation confidence="0.694161">
|
|
|
</equation>
<bodyText confidence="0.997949428571428">
The second type of cliques contains the query
node and two expansion terms, and , which
appear in consecutive order in the expansion. The
potential functions over these cliques are defined
as
where the feature is defined as the log prob-
ability of generating an expansion bigram given
</bodyText>
<equation confidence="0.551712">
 ||
</equation>
<bodyText confidence="0.99985925">
Unlike the language models used for document
ranking (e.g., Zhai and Lafferty 2001), we cannot
compute the bigram probability by simply counting
the relative frequency of in because
the query is usually very short and the bigram is
unlikely to occur. Thus, we approximate the bi-
gram probability by assuming that the words in
are independent with each other. We thus have
</bodyText>
<equation confidence="0.914432333333333">
|
 |∏ |
∏
</equation>
<bodyText confidence="0.995731045454545">
where  |is the translation probability
computed using a variant of the triplet model de-
scribed in Section 2.2. The model variation differs
from the one of Equation (4) in two respects. First,
it models the translation in a different direction i.e.,
from expansion to query. Second, we add a con-
straint to the triplets such that ( ) must be an
ordered, contiguous bigram. The model variation is
also trained using EM on query-title pairs.
and  |are assigned respectively by the
unigram and bigram language models, estimated
from the collection of document titles of the click-
through data, and is the unigram probability
of the query term, estimated from the collection of
queries of the clickthrough data.
The third type of cliques contains the query
node and two expansion terms, and , which
occur unordered within the expansion. The poten-
tial functions over these cliques are defined as
where the feature is defined as the log prob-
ability of generating a pair of expansion terms
given
</bodyText>
<equation confidence="0.9242045">
 ||
Unlike  |defined in Equation (7), this
</equation>
<bodyText confidence="0.937014666666667">
class of features captures long-span term depend-
ency in the expansion candidate. Similar to the
computation of  |in Equation (7), we
</bodyText>
<equation confidence="0.967391">
approximate  |as
|
 |∏ |
∏
</equation>
<bodyText confidence="0.946874285714286">
where  |is the translation probability
computed using the triplet model described in Sec-
tion 2.2, but in the expansion-to-query direction.
is assigned by a unigram language model
estimated from the collection of document titles of
the clickthrough data.  |is assigned by a co-
occurrence model, estimated as
 |∑
where is the number of times that the two
terms occur in the same title in clickthrough data.
We now turn to the other three types of cliques
that do not contain the query node. The fourth type
of cliques contains only one expansion term. The
potential functions are defined as
</bodyText>
<page confidence="0.89785">
670
</page>
<equation confidence="0.425024">
(9)
</equation>
<bodyText confidence="0.9998555">
where is the unigram probability computed
using a unigram language model trained on the
collection of document titles.
The fifth type of cliques contains a pair of terms
appearing in consecutive order in the expansion.
The potential functions are defined as
</bodyText>
<equation confidence="0.805423">
∑ ∑
∑
∑
∑ ∑
</equation>
<bodyText confidence="0.999970428571429">
where there are in total 8 ’s to be estimated.
Although the MRF is by nature a generative
model, it is not always appropriate to train the pa-
rameters using conventional likelihood based ap-
proaches due to the metric divergence problem
(Morgan et al. 2004): i.e., the maximum likelihood
estimate is unlikely to be the one that optimizes the
evaluation metric. In this study the effectiveness of
a QE method is evaluated by first issuing a set of
queries which are expanded using the method to a
(11) search engine and then measuring the Web search
performance. Better QE methods are supposed to
lead to better Web search results using the corre-
spondingly expanded query set.
For this reason, the parameters of the MRF-
based ranker are optimized directly for Web
search. In our experiments, the objective in train-
ing is Normalized Discounted Cumulative Gain
(NDCG, Jarvelin and Kekalainen 2000), which is
widely used as quality measure for Web search.
Formally, we view parameter training as a multi-
dimensional optimization problem, with each fea-
ture class as one dimension. Since NDCG is not
differentiable, we tried in our experiments numeri-
cal algorithms that do not require the computation
of gradient. Among the best performers was the
Powell Search algorithm (Press et al., 1992). It first
constructs a set of virtual directions that are con-
jugate (i.e., independent with each other), then it
uses line search times ( in our case), each
on one virtual direction, to find the optimum. Line
search is a one-dimensional optimization algo-
rithm. Our implementation follows the one de-
scribed in Gao et al. (2005), which is used to opti-
mize averaged precision.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999570666666667">
We evaluate the performance of a QE method by
first issuing a set of queries which are expanded
using the method to a search engine and then
</bodyText>
<equation confidence="0.8251405">
(10)
|
</equation>
<bodyText confidence="0.957317222222222">
where  |is the bigram probability com-
puted using a bigram language model trained on
the collection of document titles.
The sixth type of cliques contains a pair of
terms appearing unordered within the expansion.
The potential functions are defined as
|
where  |is the assigned by a co-occurrence
model trained on the collection of document titles.
</bodyText>
<subsectionHeader confidence="0.919792">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999261333333333">
The MRF model uses 8 classes of features defined
on 6 types of cliques, as in Equations (6) to (11).
Following previous work (e.g., Metzler and Croft
2005; Bendersky et al. 2010), we assume that all
features within the same feature class are weighted
by the same tied parameter . Thus, the number of
free parameters of the MRF model is significantly
reduced. This not only makes the model training
easier but also improves the robustness of the
model. After tying the parameters and using the
exponential potential function form, the MRF-
based ranker can be parameterized as
</bodyText>
<figure confidence="0.4815232">
 |⇒ (12)
∑
∑
∑
∑
</figure>
<page confidence="0.99619">
671
</page>
<bodyText confidence="0.99971788">
measuring the Web search performance. Better QE
methods are supposed to lead to better Web search
results using the correspondingly expanded query
set.
Due to the characteristics of our QE methods,
we cannot conduct experiments on standard test
collections such as the TREC data because they do
not contain related user logs we need. Therefore,
following previous studies of log-based QE (e.g.,
Cui et al. 2003; Riezler et al. 2008), we use the
proprietary datasets that have been developed for
building a commercial search engine, and demon-
strate the effectiveness of our methods by compar-
ing them against previous state-of-the-art log-
based QE methods.
The relevance judgment set consists of 4,000
multi-term English queries. On average, each que-
ry is associated with 197 Web documents (URLs).
Each query-URL pair has a relevance label. The
label is human generated and is on a 5-level rele-
vance scale, 0 to 4, with 4 meaning document D is
the most relevant to query Q and 0 meaning D
is not relevant to Q.
The relevance judgment set is constructed as
follows. First, the queries are sampled from a year
of search engine logs. Adult, spam, and bot queries
are all removed. Queries are “de-duped” so that
only unique queries remain. To reflect a natural
query distribution, we do not try to control the
quality of these queries. For example, in our query
sets, there are roughly 20% misspelled queries, 20%
navigational queries, and 10% transactional que-
ries. Second, for each query, we collect Web doc-
uments to be judged by issuing the query to several
popular search engines (e.g., Google, Bing) and
fetching retrieval results from each. Finally, the
query-document pairs are judged by a group of
well-trained assessors. In this study all the queries
are preprocessed as follows. The text is white-
space tokenized and lowercased, numbers are re-
tained, and no stemming/inflection treatment is
performed. We split the judgment set into two non-
overlapping datasets, namely training and test sets,
respectively. Each dataset contains 2,000 queries.
The query-title pairs used for model training are
extracted from one year of query log files using a
procedure similar to Gao et al. (2009). In our ex-
periments we used a randomly sampled subset of
20,692,219 pairs that do not overlap the queries
and documents in the test set.
</bodyText>
<table confidence="0.99937525">
# QE methods IDCG@1 IDCG@3 IDCG@10
1 IoQE 34.70 36.50 41.54
2 TC 33.78 36.57 42.33 α
3 SMT 34.79 β 36.98 αβ 42.84 αβ
4 MRF 36.10 αrty 38.06 αrty 43.71 αrty
5 MRFum+bm+cm 33.31 36.12 42.26 α
6 MRFtc 34.50 β 36.59 42.33 α
7 MRFwm 34.73 β 36.62 42.73 αβ
8 MRFtm 35.13 αβ 37.46 αβγ 42.82 αβ
9 MRFbltm 34.34 β 36.19 41.98 α
10 MRFwm+tm 35.21 αβγ 37.46 αβγ 42.83 αβ
11 MRFwm+tm+bltm 35.84 αβγ 37.70 αβγ 43.14 αβγ
</table>
<tableCaption confidence="0.999594">
Table 1: Ranking results using BM25 with different
</tableCaption>
<bodyText confidence="0.979180578947368">
query expansion systems. The superscripts a, f3, and y
indicate statistically significant improvements
(p &lt; 0.05) over IoQE, TC, and SMT, respectively.
Rows 5 to 11 are different versions of MRF in Row 5,
They use the same candidate generator but use in the
ranker different feature classes, as specified by the
subscript. tc specifies the feature class defined as the
scoring function in Equation (13). Refer to Equation
(12) for the names of other feature classes.
Our Web document collection consists of ap-
proximately 2.5 billion Web pages. In the retrieval
experiments we use the index based on the content
fields (i.e., body and title text) of each Web page.
The Web search performance is evaluated by
mean NDCG. We report NDCG scores at trunca-
tion levels of 1, 3, and 10. We also perform a sig-
nificance test using the paired t-test. Differences
are considered statistically significant when p-
value is less than 0.05.
</bodyText>
<subsectionHeader confidence="0.999663">
4.1 Comparing Systems
</subsectionHeader>
<bodyText confidence="0.998030076923077">
Table 1 shows the main document ranking results
using different QE systems, developed and evalu-
ated using the datasets described above.
IoQE (Row 1) is the baseline retrieval system
that uses the raw input queries and the BM25 doc-
ument ranking model. Rows 2 to 4 are different QE
systems. Their results are obtained by first expand-
ing a query, then using BM25 to rank the docu-
ments with respect to the expanded query.
TC (Row 2) is our implementation of the corre-
lation-based QE system (Cui et al. 2002; 2003). It
takes the following steps to expand an input query
0:
</bodyText>
<page confidence="0.994733">
672
</page>
<listItem confidence="0.953273571428571">
1. Extract all query terms q (eliminating
stopwords) from Q.
2. Find all documents that have clicks on a
query that contains one or more of these
query terms.
3. For each title term w in these documents,
calculate its evidence of being selected as
an expansion term according to the whole
query via a scoring function Score (w |Q).
4. Select n title terms with the highest score
(where the value of n is optimized on train-
ing data) and formulate the expanded que-
ry by adding these terms into Q.
5. Use the expanded query to rank documents.
</listItem>
<bodyText confidence="0.9994435">
The scoring function is based on the term correla-
tion model, and is defined as
</bodyText>
<equation confidence="0.999532333333333">
w |Q) _ In FI P(w |q) + 1 (13)
qEQ
 |q) _ P(w |D)P(D |q)
</equation>
<bodyText confidence="0.999966089285714">
where Dq is the set of documents clicked for the
queries containing the term q and is collected from
search logs, P(w |D) is a normalized tf-idf weight
of the document term in D, and P (D  |q) is the rela-
tive occurrence of D among all the documents
clicked for the queries containing q. Table 1 shows
that TC leads to significant improvement over
IoQE in NDCG@10, but not in NDCG@1 and
NDCG@3 (Row 2 vs. Row 1). The result is not
entirely consistent with what reported in Cui et al.
(2003). A possible reason is that Cui et al. per-
formed the evaluation using documents and search
logs collected from the Encarta website, which is
much cleaner and more homogenous than the data
sets we used. The result suggests that although QE
improves the recall of relevant documents, it is
also likely to introduce noise that hurts the preci-
sion of document retrieval.
SMT (Row 3) is a SMT-based QE system. Fol-
lowing Riezler et al. (2008), the system is an im-
plementation of a phrase-based SMT system with a
standard set of features for translation model and
language model, combined under a log linear mod-
el framework (Koehn et al. 2003). Different from
Riezler et al.’s system where the translation model
is trained on query-snippet pairs and the language
model on queries, in our implementation the trans-
lation model is trained on query-title pairs and the
language model on titles. To apply the system to
QE, expansion terms of a query are taken from
those terms in the 10-best translations of the query
that have not been seen in the original query string.
We see that SMT significantly outperforms TC in
NDCG at all levels. The result confirms the con-
clusion of Riezler et al., demonstrating that context
information is crucial for improving retrieval pre-
cision by filtering noisy expansions.
Both TC and SMT, considered as state-of-the-
art QE methods, have been frequently used for
comparison in related studies. Thus, we also used
them as baselines in our experiments.
MRF (Row 4) is the ranker-based QE system
described in Section 3, which uses a MRF-based
ranker to incorporate all 8 classes of features de-
rived from a variety of lexicon translation models
and language models as in Equation (12). Results
show that the ranker-based QE system significantly
outperforms both IoQE and the two state-of-the-
art QE methods. The fact that MRF beats SMT
with a statistically significant margin although the
former is a much simpler system indicates that text
translation and QE are different tasks and some
SMT components, designed for the task of regular
text translation, are not as effective in selecting
expansion terms. We will explore this in more de-
tail in the next section.
</bodyText>
<subsectionHeader confidence="0.998336">
4.2 Comparing Models
</subsectionHeader>
<bodyText confidence="0.999977052631579">
The experiments presented in this section investi-
gate in detail the effectiveness of different models,
e.g., the lexicon models and the language models
described in Sections 2 and 3, in ranking expansion
candidates for QE. The results are summarized in
Rows 5 to 11 in Table 1, where a number of differ-
ent versions of the ranker-based QE system are
compared. These versions, labeled as MRFf, use
the same candidate generator, and differ in the fea-
ture classes (which are specified by the subscript f)
incorporated in the MRF-based ranker. In what
follows, we focus our discussion on the results of
the three lexicon models.
MRFwm (Row 7) uses the word translation
model described in Section 2.1. Both the word
model and term correlation model used in MRFtm
(Row 6) are context independent. They differ
mainly in the training methods. For the sake of
comparison, in our experiment the word model is
</bodyText>
<page confidence="0.998627">
673
</page>
<bodyText confidence="0.999934377358491">
EM-trained with the correlation model as initial
point. Rezler et al. (2008) hypothesize that statisti-
cal translation model is superior to correlation
model because the EM training captures the hidden
alignment information when mapping document
terms to query terms, leading to a better smoothed
probability distribution. Our result (Row 7 vs. Row
6) verifies the hypothesis. Notice that MRFr, out-
performs TC in NDCG@1 (Row 6 vs. Row 2)
mainly because in the former the expansion candi-
dates are generated by a word translation model
and are less noisy.
It is encouraging to observe that the rankers us-
ing the triplet model features achieve the QE per-
formance either in par with or better than that of
SMT (Rows 8, 10 and 11 vs. Row 3), although the
latter is a much more sophisticated system. The
result suggests that not all SMT components are
useful for QE. For example, language models are
indispensable for translation but are less effective
than word models for QE (Row 5 vs. Rows 6 and
7). We also observe that the triplet model not only
outperforms significantly the word model due to
the use of contextual information (Row 8 vs. Row
7), but also seems to subsume the latter in that
combining the features derived from both models
in the ranker leads to little improvement over the
ranker that uses only the triplet model features
(Row 10 vs. Row 8).
The bilingual topic model underperforms the
word model and the triplet model (Row 9 vs. Rows
7 and 8). However, we found that the bilingual top-
ic model often selects a different set of expansion
terms and is complementary to the other two lexi-
con models. As a result, unlike the case of combin-
ing the word model and triplet model features, in-
corporating the bilingual topic model features in
the ranker leads to some visible improvement in
NDCG at all positions (Row 11 vs. Row 10).
To better understand empirically how the MRF-
based QE system achieves the improvement, we
analyzed the expansions generated by our system
in detail and obtained several interesting findings.
First, as expected, in comparison with the word
model, the triplet translation model is more effec-
tive in benefitting long queries, e.g., notably que-
ries containing questions and queries containing
song lyrics. Second, unlike the two lexicon models,
the bilingual topic model tends to generate expan-
sions that are more likely to relate to an entire que-
ry rather than individual query terms. Third, the
features involving the order of the expansion terms
benefitted queries containing named entities.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999942409090909">
In comparison with log-based methods studied in
this paper, the QE methods based on automatic
relevance feedback have been studied much more
extensively in the information retrieval (IR) com-
munity, and have been proved useful for improving
IR performance on benchmark datasets such as
TREC (e.g., Rocchio 1971; Xu and Croft 1996;
Lavrenko 2001; Zhai and Lafferty 2001). Howev-
er, these methods cannot be applied directly to a
commercial Web search engine because the rele-
vant documents are not always available and gen-
erating pseudo-relevant documents requires multi-
phase retrieval, which is prohibitively expensive.
Although automatic relevance feedback is not the
focus of this study, our method shares a lot of simi-
larities with some of them. For example, similar to
the way the parameters of our QE ranker are esti-
mated, Cao et al. (2008) propose a method of se-
lecting expansion terms to directly optimize aver-
age precision. The MRF model has been previous-
ly used for QE, in the form of relevance feedback
and pseudo-relevance feedback (Metzler et al.
2007; Lang et al. 2010). While their MRF models
use the features derived from IR systems such as
Indri, we use the SMT-inspired features.
Using statistical translation models for IR is not
new (e.g., Berger and Lafferty 1999; Jin et al. 2002;
Xue et al. 2008). The effectiveness of the statistical
translation-based approach to Web search has been
demonstrated empirically in recent studies where
word-based and phrase-based translation models
are trained on large amounts of clickthrough data
(e.g., Gao et al. 2010a; 2011). Our work extends
these studies and constructs QE-oriented transla-
tion models that capture more flexible dependen-
cies.
In addition to QE, search logs have also been
used for other Web search tasks, such as document
ranking (Joachims 2002; Agichtein et al. 2006),
search query processing and spelling correction
(Huang et al. 2010; Gao et al. 2010b) image re-
trieval (Craswell and Szummer 2007), and user
query clustering (Baeza-Yates and Tiberi 2007;
Wen et al. 2002).
</bodyText>
<sectionHeader confidence="0.998271" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<page confidence="0.997505">
674
</page>
<bodyText confidence="0.999940466666667">
In this paper we extend the previous log-based QE
methods in two directions. First, we formulate QE
as the problem of translating a source language of
queries into a target language of documents, repre-
sented as titles. This allows us to adapt the estab-
lished techniques developed for SMT to QE. Spe-
cially, we propose three lexicon models based on
terms, lexicalized triplets, and topics, respectively.
These models are trained on pairs of user queries
and the titles of clicked documents using EM. Se-
cond, we present a ranker-based QE system, the
heart of which is a MRF-based ranker in which the
lexicon models are incorporated as features. We
perform experiments on the Web search task using
a real world data set. Results show that the pro-
posed system outperforms significantly other state-
of-the-art QE systems.
This study is part of a bigger, ongoing project,
aiming to develop a real-time QE system for Web
search, where simplicity is the key to the success.
Thus, what we learned from this study is particu-
larly encouraging. We demonstrate that with large
amounts of clickthrough data for model training,
simple lexicon models can achieve state-of-the-art
QE performance, and that the MRF-based ranker
provides a simple and flexible framework to incor-
porate a variety of features capturing different
types of term dependencies in such an effective
way that the Web search performance can be di-
rectly optimized.
</bodyText>
<sectionHeader confidence="0.977348" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.709917333333333">
Agichtein, E., Brill, E., and Dumais, S. 2006. Im-
proving web search ranking by incorporating us-
er behavior information. In SIGIR, pp. 19-26.
</bodyText>
<reference confidence="0.970155981818182">
Baeze-Yates, R., and Ribeiro-Neto, B. 2011. Mod-
ern Information Retrieval. Addison-Wesley.
Baeza-Yates, R. and Tiberi, A. 2007. Extracting
semantic relations from query logs. In SIGKDD,
pp. 76-85.
Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G.
2005. Query expansion using term relationships
in language models for information retrieval. In
CIKM, pp. 688-695.
Bendersky, M., Metzler, D., and Croft, B. 2010.
Learning concept importance using a weighted
dependence model. In WSDM, pp. 31-40.
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp.
222-229.
Bishop, C. M. 2006. Patten recognition and ma-
chine learning. Springer.
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3: 993-1022.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,
and Mercer, R. L. 1993. The mathematics of sta-
tistical machine translation: parameter estimation.
Computational Linguistics, 19(2): 263-311.
Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008.
Selecting good expansion terms for pseudo-
relevance feedback. In SIGIR, pp. 289-305.
Craswell, N. and Szummer, M. 2007. Random
walk on the click graph. In SIGIR. pp. 239-246.
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002.
Probabilistic query expansion using query logs.
In WWW, pp. 325-332.
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003.
Query expansion by mining user log. IEEE
Trans on Knowledge and Data Engineering. Vol.
15, No. 4. pp. 1-11.
Dempster, A., Laird, N., and Rubin, D. 1977. Max-
imum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, 39: 1-38.
Ganchev, K., Graca, J., Gillenwater, J., and Taskar,
B. 2010. Posterior regularization for structured
latent variable models. Journal of Machine
Learning Research, 11 (2010): 2001-2049.
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web
search. In SIGIR, pp. 675-684.
Gao, J., He, X., and Nie, J-Y. 2010a. Clickthrough-
based translation models for web search: from
word models to phrase models. In CIKM, pp.
1139-1148.
Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X.
2010b. A large scale ranker-based system for
query spelling correction. In COLING, pp. 358-
366.
</reference>
<page confidence="0.987248">
675
</page>
<reference confidence="0.982718506849315">
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y.
2009. Smoothing clickthrough data for web
search ranking. In SIGIR, pp. 355-362.
Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear
discriminant model for information retrieval. In
SIGIR, pp. 290-297.
Hasan, S., Ganitkevitch, J., Ney, H., and Andres-
Fnerre, J. 2008. Triplet lexicon models for statis-
tical machine translation. In EMNLP, pp. 372-
381.
Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and
Behr, F. 2010. Exploring web scale language
models for search query processing. In WWW, pp.
451-460.
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation
methods for retrieving highly relevant docu-
ments. In SIGIR, pp. 41-48
Jin, R., Hauptmann, A. G., and Zhai, C. 2002. Title
language model for information retrieval. In
SIGIR, pp. 42-48.
Jing, Y., and Croft., B. 1994. An association
thesaurus for information retrieval. In RIAO, pp.
146-160.
Joachims, T. 2002. Optimizing search engines us-
ing clickthrough data. In SIGKDD, pp. 133-142.
Koehn, P., Och, F., and Marcu, D. 2003. Statistical
phrase-based translation. In HLT/NAACL, pp.
127-133.
Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010.
Improving latent concept expansion using
markov random fields. In CIKM, pp. 249-258.
Lavrenko, V., and Croft, B. 2001. Relevance-based
language models. In SIGIR, pp. 120-128.
Lease, M. 2009. An improved markov random
field model for supporting verbose queries. In
SIGIR, pp. 476-483
Metzler, D., and Croft, B. 2005. A markov random
field model for term dependencies. In SIGIR, pp.
472-479.
Metzler, D., and Croft, B. 2007. Latent concept
expansion using markov random fields. In
SIGIR, pp. 311-318.
Morgan, W., Greiff, W., and Henderson, J. 2004.
Direct maximization of average precision by
hill-climbing with a comparison to a maximum
entropy approach. Technical report. MITRE.
Och, F. 2002. Statistical machine translation: from
single-word models to alignment templates. PhD
thesis, RWTH Aachen.
Prager, J., Chu-Carroll, J., and Czuba, K. 2001.
Use of Wordnet hypernyms for answering what
is questions. In TREC 10.
Press, W. H., Teukolsky, S. A., Vetterling, W. T.,
and Flannery, B. P. 1992. Numerical Recipes in
C. Cambridge Univ. Press.
Rocchio, J. 1971. Relevance feedback in infor-
mation retrieval. In The SMART retrieval system:
experiments in automatic document processing,
pp. 313-323, Prentice-Hall Inc.
Riezler, S., Liu, Y. and Vasserman, A. 2008.
Translating queries into snippets for improving
query expansion. In COLING 2008. 737-744.
Riezler, S., and Liu, Y. 2010. Query rewriting us-
ing monolingual statistical machine translation.
Computational Linguistics, 36(3): 569-582.
Wen, J., Nie, J-Y., and Zhang, H. 2002. Query
clustering using user logs. ACM TOIS, 20(1): 59-
81.
Xu, J., and Croft, B. 1996. Query expansion using
local and global document analysis. In SIGIR.
Xue, X., Jeon, J., Croft, W. B. 2008. Retrieval
models for Question and answer archives. In
SIGIR, pp. 475-482.
</reference>
<bodyText confidence="0.804365571428571">
Zhai, C., and Lafferty, J. 2001a. Model-based
feedback in the kl-divergence retrieval model. In
CIKM, pp. 403-410.
Zhai, C., and Lafferty, J. 2001b. A study of
smoothing methods for language models applied
to ad hoc information retrieval. In SIGIR, pp.
334-342.
</bodyText>
<page confidence="0.998579">
676
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477994">
<title confidence="0.999977">Learning Lexicon Models from Search Logs for Query Expansion</title>
<author confidence="0.976093">Jianfeng Gao Shasha Xie</author>
<affiliation confidence="0.566536">Research, Redmond Testing Service, Princeton</affiliation>
<address confidence="0.99983">98052, USA Jersey 08540, USA</address>
<email confidence="0.974971">jfgao@microsoft.comsxie@ets.org</email>
<author confidence="0.937061">Xiaodong He Alnur Ali</author>
<affiliation confidence="0.94625">Microsoft Research, Redmond Microsoft Bing, Bellevue</affiliation>
<address confidence="0.999432">Washington 98052, USA Washington 98004, USA</address>
<email confidence="0.998415">xiaohe@microsoft.comalnurali@microsoft.com</email>
<abstract confidence="0.998010230769231">This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeze-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>2011</date>
<publisher>Addison-Wesley.</publisher>
<marker>Baeze-Yates, Ribeiro-Neto, 2011</marker>
<rawString>Baeze-Yates, R., and Ribeiro-Neto, B. 2011. Modern Information Retrieval. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>A Tiberi</author>
</authors>
<title>Extracting semantic relations from query logs.</title>
<date>2007</date>
<booktitle>In SIGKDD,</booktitle>
<pages>76--85</pages>
<contexts>
<context position="33975" citStr="Baeza-Yates and Tiberi 2007" startWordPosition="5809" endWordPosition="5812">n demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as the problem of translating a source language of queries into a target language of documents, represented as titles. This allows us to adapt the established techniques developed for SMT to QE. Specially, we propose three lexicon models based on terms, lexicalized triplets, and topics, respectively. These models are trained on pairs of user queries and the titles of clicked documents using EM. Second, we present a ranker-based QE system, the heart of which </context>
</contexts>
<marker>Baeza-Yates, Tiberi, 2007</marker>
<rawString>Baeza-Yates, R. and Tiberi, A. 2007. Extracting semantic relations from query logs. In SIGKDD, pp. 76-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bai</author>
<author>D Song</author>
<author>P Bruza</author>
<author>J-Y Nie</author>
<author>G Cao</author>
</authors>
<title>Query expansion using term relationships in language models for information retrieval.</title>
<date>2005</date>
<booktitle>In CIKM,</booktitle>
<pages>688--695</pages>
<marker>Bai, Song, Bruza, Nie, Cao, 2005</marker>
<rawString>Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G. 2005. Query expansion using term relationships in language models for information retrieval. In CIKM, pp. 688-695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bendersky</author>
<author>D Metzler</author>
<author>B Croft</author>
</authors>
<title>Learning concept importance using a weighted dependence model.</title>
<date>2010</date>
<booktitle>In WSDM,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="20419" citStr="Bendersky et al. 2010" startWordPosition="3493" endWordPosition="3496">s which are expanded using the method to a search engine and then (10) | where |is the bigram probability computed using a bigram language model trained on the collection of document titles. The sixth type of cliques contains a pair of terms appearing unordered within the expansion. The potential functions are defined as | where |is the assigned by a co-occurrence model trained on the collection of document titles. 3.3 Parameter Estimation The MRF model uses 8 classes of features defined on 6 types of cliques, as in Equations (6) to (11). Following previous work (e.g., Metzler and Croft 2005; Bendersky et al. 2010), we assume that all features within the same feature class are weighted by the same tied parameter . Thus, the number of free parameters of the MRF model is significantly reduced. This not only makes the model training easier but also improves the robustness of the model. After tying the parameters and using the exponential potential function form, the MRFbased ranker can be parameterized as |⇒ (12) ∑ ∑ ∑ ∑ 671 measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. Due to the characteristics of ou</context>
</contexts>
<marker>Bendersky, Metzler, Croft, 2010</marker>
<rawString>Bendersky, M., Metzler, D., and Croft, B. 2010. Learning concept importance using a weighted dependence model. In WSDM, pp. 31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In SIGIR,</booktitle>
<pages>222--229</pages>
<contexts>
<context position="5947" citStr="Berger and Lafferty 1999" startWordPosition="954" endWordPosition="957">impler QE system that incorporates the lexicon models can beat the sophisticated, black-box SMT system. 2 Lexicon Models We view search queries and Web documents as two different languages, and cast QE as a means to bridge the language gap by translating queries to documents, represented by their titles. In this section, we will describe three translation models that are based on terms, triplets, and topics, respectively, and the way these models are learned from query-title pairs extracted from clickthrough data. 2.1 Word Model The word model takes the form of IBM Model 1 (Brown et al. 1993; Berger and Lafferty 1999). Let Q q be a query, e be an expansion term 1, ... , q candidate, the translation probability from Q to e is defined as I Q)wm =I P(eI qj) P( qjl Q) (1) j=1 where P (q I Q) is the unsmoothed unigram probability of word q in query Q. The word translation probabilities P (e l q) are estimated on the querytitle pairs derived from the clickthrough data by assuming that the title terms are likely to be the desired expansions of the paired query. Our training method follows the standard procedure of training statistical word alignment models proposed by Brown et al. (1993). Formally, we optimize th</context>
<context position="33226" citStr="Berger and Lafferty 1999" startWordPosition="5693" endWordPosition="5696">s not the focus of this study, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. The MRF model has been previously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Berger, A., and Lafferty, J. 1999. Information retrieval as statistical translation. In SIGIR, pp. 222-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<title>Patten recognition and machine learning.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="12659" citStr="Bishop 2006" startWordPosition="2148" endWordPosition="2149">ng. In the remainder of this section we will describe in turn the MRF-based ranker, the ranking features, and the way the ranker parameters are estimated. 3.1 MRF-Based Ranker The ranker is based on the MRF model that models the joint distribution of PA(E, Q) over a set of expansion term random variables E = fel,..., el} and a query random variable Q. It is constructed from a graph G consisting of a query node and nodes for each expansion term. Nodes in the graph represent random variables and edges define the independence semantics between the variables. An MRF satisfies the Markov property (Bishop 2006), which states that a node is independent of all of its non-neighboring nodes given observed values of its neighbors, defined by the clique configurations of G. The joint distribution over the random variables in G is defined as 1 PA(E, Q) = Z F1 T (c; A) (6) A CEC(G) where C(G) is the set of cliques in G, and each T (c; A) is a non-negative potential function defined over a clique configuration c that measures the compatibility of the configuration, A is a set of parameters that are used within the potential function, and ZA normalizes the distribution. For ranking expansion candidates, we ca</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Bishop, C. M. 2006. Patten recognition and machine learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M J Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="5920" citStr="Brown et al. 1993" startWordPosition="950" endWordPosition="953">model, and that a simpler QE system that incorporates the lexicon models can beat the sophisticated, black-box SMT system. 2 Lexicon Models We view search queries and Web documents as two different languages, and cast QE as a means to bridge the language gap by translating queries to documents, represented by their titles. In this section, we will describe three translation models that are based on terms, triplets, and topics, respectively, and the way these models are learned from query-title pairs extracted from clickthrough data. 2.1 Word Model The word model takes the form of IBM Model 1 (Brown et al. 1993; Berger and Lafferty 1999). Let Q q be a query, e be an expansion term 1, ... , q candidate, the translation probability from Q to e is defined as I Q)wm =I P(eI qj) P( qjl Q) (1) j=1 where P (q I Q) is the unsmoothed unigram probability of word q in query Q. The word translation probabilities P (e l q) are estimated on the querytitle pairs derived from the clickthrough data by assuming that the title terms are likely to be the desired expansions of the paired query. Our training method follows the standard procedure of training statistical word alignment models proposed by Brown et al. (1993</context>
</contexts>
<marker>Brown, Pietra, A, Pietra, J, Mercer, 1993</marker>
<rawString>Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer, R. L. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cao</author>
<author>J-Y Nie</author>
<author>J Gao</author>
<author>S Robertson</author>
</authors>
<title>Selecting good expansion terms for pseudorelevance feedback.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>289--305</pages>
<contexts>
<context position="32788" citStr="Cao et al. (2008)" startWordPosition="5619" endWordPosition="5622">l for improving IR performance on benchmark datasets such as TREC (e.g., Rocchio 1971; Xu and Croft 1996; Lavrenko 2001; Zhai and Lafferty 2001). However, these methods cannot be applied directly to a commercial Web search engine because the relevant documents are not always available and generating pseudo-relevant documents requires multiphase retrieval, which is prohibitively expensive. Although automatic relevance feedback is not the focus of this study, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. The MRF model has been previously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent stu</context>
</contexts>
<marker>Cao, Nie, Gao, Robertson, 2008</marker>
<rawString>Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008. Selecting good expansion terms for pseudorelevance feedback. In SIGIR, pp. 289-305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Craswell</author>
<author>M Szummer</author>
</authors>
<title>Random walk on the click graph.</title>
<date>2007</date>
<booktitle>In SIGIR.</booktitle>
<pages>239--246</pages>
<contexts>
<context position="33919" citStr="Craswell and Szummer 2007" startWordPosition="5801" endWordPosition="5804">stical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as the problem of translating a source language of queries into a target language of documents, represented as titles. This allows us to adapt the established techniques developed for SMT to QE. Specially, we propose three lexicon models based on terms, lexicalized triplets, and topics, respectively. These models are trained on pairs of user queries and the titles of clicked documents using EM. Second, </context>
</contexts>
<marker>Craswell, Szummer, 2007</marker>
<rawString>Craswell, N. and Szummer, M. 2007. Random walk on the click graph. In SIGIR. pp. 239-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>J-R Wen</author>
<author>J-Y Nie</author>
<author>W-Y Ma</author>
</authors>
<title>Probabilistic query expansion using query logs.</title>
<date>2002</date>
<booktitle>In WWW,</booktitle>
<pages>325--332</pages>
<contexts>
<context position="2803" citStr="Cui et al. (2002" startWordPosition="448" endWordPosition="451">ely select good expansion terms, and that a ranker-based QE system which incorporates all three of these models not only significantly improves Web search result but outperforms other log-based QE methods that are stateof-the-art. There is growing interest in applying user logs to improve QE. A recent survey is due to BaezeYates and Ribeiro-Neto (2011). Below, we briefly discuss two log-based QE methods that are closest to ours and are re-implemented in this study for comparison. Both systems use the same type of log data that we used to train the lexicon models. The term correlation model of Cui et al. (2002; 2003) is to our knowledge the first to explore querydocument relations for direct extraction of expansion terms for Web search. The method outperforms traditional QE methods that do not use log data e.g. the local analysis model of Xu and Croft (1996). In addition, as pointed out by Cui et al. (2003) there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines. First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations</context>
<context position="25063" citStr="Cui et al. 2002" startWordPosition="4292" endWordPosition="4295">-test. Differences are considered statistically significant when pvalue is less than 0.05. 4.1 Comparing Systems Table 1 shows the main document ranking results using different QE systems, developed and evaluated using the datasets described above. IoQE (Row 1) is the baseline retrieval system that uses the raw input queries and the BM25 document ranking model. Rows 2 to 4 are different QE systems. Their results are obtained by first expanding a query, then using BM25 to rank the documents with respect to the expanded query. TC (Row 2) is our implementation of the correlation-based QE system (Cui et al. 2002; 2003). It takes the following steps to expand an input query 0: 672 1. Extract all query terms q (eliminating stopwords) from Q. 2. Find all documents that have clicks on a query that contains one or more of these query terms. 3. For each title term w in these documents, calculate its evidence of being selected as an expansion term according to the whole query via a scoring function Score (w |Q). 4. Select n title terms with the highest score (where the value of n is optimized on training data) and formulate the expanded query by adding these terms into Q. 5. Use the expanded query to rank d</context>
</contexts>
<marker>Cui, Wen, Nie, Ma, 2002</marker>
<rawString>Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002. Probabilistic query expansion using query logs. In WWW, pp. 325-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>J-R Wen</author>
<author>J-Y Nie</author>
<author>W-Y Ma</author>
</authors>
<title>Query expansion by mining user log.</title>
<date>2003</date>
<journal>IEEE Trans on Knowledge and Data Engineering.</journal>
<volume>15</volume>
<pages>1--11</pages>
<contexts>
<context position="3106" citStr="Cui et al. (2003)" startWordPosition="503" endWordPosition="506"> survey is due to BaezeYates and Ribeiro-Neto (2011). Below, we briefly discuss two log-based QE methods that are closest to ours and are re-implemented in this study for comparison. Both systems use the same type of log data that we used to train the lexicon models. The term correlation model of Cui et al. (2002; 2003) is to our knowledge the first to explore querydocument relations for direct extraction of expansion terms for Web search. The method outperforms traditional QE methods that do not use log data e.g. the local analysis model of Xu and Croft (1996). In addition, as pointed out by Cui et al. (2003) there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines. First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations to be pre-computed offline. Compared to methods that are based on thesauri either compiled manually (Prager et al. 2001) or derived au666 tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and</context>
<context position="21245" citStr="Cui et al. 2003" startWordPosition="3632" endWordPosition="3635">training easier but also improves the robustness of the model. After tying the parameters and using the exponential potential function form, the MRFbased ranker can be parameterized as |⇒ (12) ∑ ∑ ∑ ∑ 671 measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. Due to the characteristics of our QE methods, we cannot conduct experiments on standard test collections such as the TREC data because they do not contain related user logs we need. Therefore, following previous studies of log-based QE (e.g., Cui et al. 2003; Riezler et al. 2008), we use the proprietary datasets that have been developed for building a commercial search engine, and demonstrate the effectiveness of our methods by comparing them against previous state-of-the-art logbased QE methods. The relevance judgment set consists of 4,000 multi-term English queries. On average, each query is associated with 197 Web documents (URLs). Each query-URL pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document D is the most relevant to query Q and 0 meaning D is not relevant to Q. Th</context>
<context position="26296" citStr="Cui et al. (2003)" startWordPosition="4529" endWordPosition="4532">oring function is based on the term correlation model, and is defined as w |Q) _ In FI P(w |q) + 1 (13) qEQ |q) _ P(w |D)P(D |q) where Dq is the set of documents clicked for the queries containing the term q and is collected from search logs, P(w |D) is a normalized tf-idf weight of the document term in D, and P (D |q) is the relative occurrence of D among all the documents clicked for the queries containing q. Table 1 shows that TC leads to significant improvement over IoQE in NDCG@10, but not in NDCG@1 and NDCG@3 (Row 2 vs. Row 1). The result is not entirely consistent with what reported in Cui et al. (2003). A possible reason is that Cui et al. performed the evaluation using documents and search logs collected from the Encarta website, which is much cleaner and more homogenous than the data sets we used. The result suggests that although QE improves the recall of relevant documents, it is also likely to introduce noise that hurts the precision of document retrieval. SMT (Row 3) is a SMT-based QE system. Following Riezler et al. (2008), the system is an implementation of a phrase-based SMT system with a standard set of features for translation model and language model, combined under a log linear</context>
</contexts>
<marker>Cui, Wen, Nie, Ma, 2003</marker>
<rawString>Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003. Query expansion by mining user log. IEEE Trans on Knowledge and Data Engineering. Vol. 15, No. 4. pp. 1-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., Laird, N., and Rubin, D. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39: 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Graca</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>2001--2049</pages>
<contexts>
<context position="10886" citStr="Ganchev et al. 2010" startWordPosition="1829" endWordPosition="1832">generated by first selecting a topic according to the topic distribution , and then drawing a word from . By summing over all possible topics, we end up with the following model form |∑ ||(5) The BLTM training follows the method described in Gao et al. (2011). We used the EM algorithm to estimate the parameters ( of BLTM by maximizing the joint log-likelihood of the querytitle pairs and the parameters. In training, we also constrain that the paired query and title have similar fractions of tokens assigned to each topic. The constraint is enforced on expectation using posterior regularization (Ganchev et al. 2010). 3 A Ranker-Based QE System This section describes a ranker-based QE system in which the three lexicon models described above are incorporated. The system expands an input query in two distinct stages, candidate generation and ranking, as illustrated by an example in Figure 1. 668 In candidate generation, an input query Q is first tokenized into a sequence of terms. For each term q that is not a stop word, we consult a word model described in Section 2.1 to identify the best M altered words according to their word translation probabilities from q. Then, we form a list of expansion candidates,</context>
</contexts>
<marker>Ganchev, Graca, Gillenwater, Taskar, 2010</marker>
<rawString>Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11 (2010): 2001-2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>K Toutanova</author>
<author>W-T Yih</author>
</authors>
<title>Clickthrough-based latent semantic models for web search.</title>
<date>2011</date>
<booktitle>In SIGIR,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="8432" citStr="Gao et al. (2011)" startWordPosition="1420" endWordPosition="1423"> triple model is able to combine local (i.e. word and phrase level) and global (i.e. query level) contextual information useful for word translation. Similar to the case of word model, we used the EM algorithm to estimate the translation probabilities |on the query-title pairs. Since the number of all possible triplets ( ) is large and as a consequence the model training could suffer the data sparseness problem, in our experiments count-based cutoff is applied to prune the model to a manageable size. 2.3 Bilingual Topic Model (BLTM) The BLTM was originally proposed for Web document ranking by Gao et al. (2011). The idea underlying the model is that a search query and its relevant Web documents share a common distribution of (hidden) topics, but use different (probably overlapping) vocabularies to express these topics. Intuitively, BLTM-based QE works as follows. First, a query is represented as a vector of topics. Then, all the candidate expansion terms, which are selected from document, are ranked by how likely it is that these document terms are selected to best describe those topics. In a sense, BLTM is similar to the word model and the triplet model since they all map a query to a document word</context>
<context position="10525" citStr="Gao et al. (2011)" startWordPosition="1769" endWordPosition="1772">tes and the expanded query generated by the ranker-based QE system. term distribution, and a topic-specific document term distribution. Assuming there are topics, we have two sets of distributions and . 2. Given , a topic distribution is drawn from a Dirichlet prior with concentration parameter . 3. Then a document term (i.e., expansion term candidate) is generated by first selecting a topic according to the topic distribution , and then drawing a word from . By summing over all possible topics, we end up with the following model form |∑ ||(5) The BLTM training follows the method described in Gao et al. (2011). We used the EM algorithm to estimate the parameters ( of BLTM by maximizing the joint log-likelihood of the querytitle pairs and the parameters. In training, we also constrain that the paired query and title have similar fractions of tokens assigned to each topic. The constraint is enforced on expectation using posterior regularization (Ganchev et al. 2010). 3 A Ranker-Based QE System This section describes a ranker-based QE system in which the three lexicon models described above are incorporated. The system expands an input query in two distinct stages, candidate generation and ranking, as</context>
</contexts>
<marker>Gao, Toutanova, Yih, 2011</marker>
<rawString>Gao, J., Toutanova, K., Yih., W-T. 2011. Clickthrough-based latent semantic models for web search. In SIGIR, pp. 675-684.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Gao</author>
<author>X He</author>
<author>J-Y 2010a Nie</author>
</authors>
<title>Clickthroughbased translation models for web search: from word models to phrase models.</title>
<booktitle>In CIKM,</booktitle>
<pages>1139--1148</pages>
<marker>Gao, He, Nie, </marker>
<rawString>Gao, J., He, X., and Nie, J-Y. 2010a. Clickthroughbased translation models for web search: from word models to phrase models. In CIKM, pp. 1139-1148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>X Li</author>
<author>D Micol</author>
<author>C Quirk</author>
<author>X Sun</author>
</authors>
<title>A large scale ranker-based system for query spelling correction.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>358--366</pages>
<contexts>
<context position="1609" citStr="Gao et al. 2010" startWordPosition="246" endWordPosition="249">re composed using different vocabularies and language styles. Query expansion (QE) is an effective strategy to address the problem. It expands a query issued by a user with additional related terms, called expansion terms, so that more relevant documents can be retrieved. In this paper we explore the use of clickthrough data and translation models for QE. We select expansion terms for a query according to how likely it is that the expansion terms occur in the title of a document that is relevant to the query. Assuming that a query is parallel to the titles of documents clicked for that query (Gao et al. 2010a), three lexicon models are trained on query-title pairs extracted from clickthrough data. The first is a word model that learns the translation probability between single words. The second model uses lexicalized triplets to incorporate word dependencies for translation. The third is a bilingual topic model, which represents a query as a distribution of hidden topics and learns the translation between a query and a title term at the semantic level. We will show that the word model provides a rich set of expansion candidates while the triplet and topic models can effectively select good expans</context>
<context position="33518" citStr="Gao et al. 2010" startWordPosition="5737" endWordPosition="5740">ously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as th</context>
</contexts>
<marker>Gao, Li, Micol, Quirk, Sun, 2010</marker>
<rawString>Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X. 2010b. A large scale ranker-based system for query spelling correction. In COLING, pp. 358-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>W Yuan</author>
<author>X Li</author>
<author>K Deng</author>
<author>J-Y Nie</author>
</authors>
<title>Smoothing clickthrough data for web search ranking.</title>
<date>2009</date>
<booktitle>In SIGIR,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="23020" citStr="Gao et al. (2009)" startWordPosition="3926" endWordPosition="3929"> engines (e.g., Google, Bing) and fetching retrieval results from each. Finally, the query-document pairs are judged by a group of well-trained assessors. In this study all the queries are preprocessed as follows. The text is whitespace tokenized and lowercased, numbers are retained, and no stemming/inflection treatment is performed. We split the judgment set into two nonoverlapping datasets, namely training and test sets, respectively. Each dataset contains 2,000 queries. The query-title pairs used for model training are extracted from one year of query log files using a procedure similar to Gao et al. (2009). In our experiments we used a randomly sampled subset of 20,692,219 pairs that do not overlap the queries and documents in the test set. # QE methods IDCG@1 IDCG@3 IDCG@10 1 IoQE 34.70 36.50 41.54 2 TC 33.78 36.57 42.33 α 3 SMT 34.79 β 36.98 αβ 42.84 αβ 4 MRF 36.10 αrty 38.06 αrty 43.71 αrty 5 MRFum+bm+cm 33.31 36.12 42.26 α 6 MRFtc 34.50 β 36.59 42.33 α 7 MRFwm 34.73 β 36.62 42.73 αβ 8 MRFtm 35.13 αβ 37.46 αβγ 42.82 αβ 9 MRFbltm 34.34 β 36.19 41.98 α 10 MRFwm+tm 35.21 αβγ 37.46 αβγ 42.83 αβ 11 MRFwm+tm+bltm 35.84 αβγ 37.70 αβγ 43.14 αβγ Table 1: Ranking results using BM25 with different quer</context>
</contexts>
<marker>Gao, Yuan, Li, Deng, Nie, 2009</marker>
<rawString>Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 2009. Smoothing clickthrough data for web search ranking. In SIGIR, pp. 355-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>H Qi</author>
<author>X Xia</author>
<author>J-Y Nie</author>
</authors>
<title>Linear discriminant model for information retrieval.</title>
<date>2005</date>
<booktitle>In SIGIR,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="19660" citStr="Gao et al. (2005)" startWordPosition="3363" endWordPosition="3366">ultidimensional optimization problem, with each feature class as one dimension. Since NDCG is not differentiable, we tried in our experiments numerical algorithms that do not require the computation of gradient. Among the best performers was the Powell Search algorithm (Press et al., 1992). It first constructs a set of virtual directions that are conjugate (i.e., independent with each other), then it uses line search times ( in our case), each on one virtual direction, to find the optimum. Line search is a one-dimensional optimization algorithm. Our implementation follows the one described in Gao et al. (2005), which is used to optimize averaged precision. 4 Experiments We evaluate the performance of a QE method by first issuing a set of queries which are expanded using the method to a search engine and then (10) | where |is the bigram probability computed using a bigram language model trained on the collection of document titles. The sixth type of cliques contains a pair of terms appearing unordered within the expansion. The potential functions are defined as | where |is the assigned by a co-occurrence model trained on the collection of document titles. 3.3 Parameter Estimation The MRF model uses </context>
</contexts>
<marker>Gao, Qi, Xia, Nie, 2005</marker>
<rawString>Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear discriminant model for information retrieval. In SIGIR, pp. 290-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>J Ganitkevitch</author>
<author>H Ney</author>
<author>J AndresFnerre</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>372--381</pages>
<contexts>
<context position="7269" citStr="Hasan et al. 2008" startWordPosition="1211" endWordPosition="1215">ntire training corpus: H 0* r x II P Di� Qi,0 (2) i=1 where both the titles D and the paired queries Q are viewed as bag of words. The translation probability P(Di I Qi, 0) takes the form of IBM Model 1 as I 1 U + 1)I E III P(wiI qj, 0) (3) i=1 j=1 where is a constant, I is the length of D, and j is the length of Q. To find the optimal word translation probabilities of IBM Model 1, we used the EM algorithm, where the number of iterations is determined empirically on held-out data. 2.2 Triplet Model The word model is context independent. The triplet model, which is originally proposed for SMT (Hasan et al. 2008), is intended to capture inter-term dependencies for selecting expansion terms. The model is based on lexicalized triplets ( e, q, q&apos;) which can be understood as two query terms triggering one expansion term. The translation probability of e given Q for the triplet model is parameterized as S Q,0 _ 667 |∑ ∑ ( |) (4) where Z is a normalization factor based on the corresponding query length, i.e., , and ( |) is the probability of translating into given another query word . Since can be any word in that is not necessary to be adjacent to , the triple model is able to combine local (i.e. word and </context>
</contexts>
<marker>Hasan, Ganitkevitch, Ney, AndresFnerre, 2008</marker>
<rawString>Hasan, S., Ganitkevitch, J., Ney, H., and AndresFnerre, J. 2008. Triplet lexicon models for statistical machine translation. In EMNLP, pp. 372-381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>J Gao</author>
<author>J Miao</author>
<author>X Li</author>
<author>K Wang</author>
<author>F Behr</author>
</authors>
<title>Exploring web scale language models for search query processing.</title>
<date>2010</date>
<booktitle>In WWW,</booktitle>
<pages>451--460</pages>
<contexts>
<context position="33856" citStr="Huang et al. 2010" startWordPosition="5790" endWordPosition="5793">2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as the problem of translating a source language of queries into a target language of documents, represented as titles. This allows us to adapt the established techniques developed for SMT to QE. Specially, we propose three lexicon models based on terms, lexicalized triplets, and topics, respectively. These models are trained on pairs of user</context>
</contexts>
<marker>Huang, Gao, Miao, Li, Wang, Behr, 2010</marker>
<rawString>Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and Behr, F. 2010. Exploring web scale language models for search query processing. In WWW, pp. 451-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jarvelin</author>
<author>J Kekalainen</author>
</authors>
<title>IR evaluation methods for retrieving highly relevant documents.</title>
<date>2000</date>
<booktitle>In SIGIR,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="18942" citStr="Jarvelin and Kekalainen 2000" startWordPosition="3244" endWordPosition="3247"> the maximum likelihood estimate is unlikely to be the one that optimizes the evaluation metric. In this study the effectiveness of a QE method is evaluated by first issuing a set of queries which are expanded using the method to a (11) search engine and then measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. For this reason, the parameters of the MRFbased ranker are optimized directly for Web search. In our experiments, the objective in training is Normalized Discounted Cumulative Gain (NDCG, Jarvelin and Kekalainen 2000), which is widely used as quality measure for Web search. Formally, we view parameter training as a multidimensional optimization problem, with each feature class as one dimension. Since NDCG is not differentiable, we tried in our experiments numerical algorithms that do not require the computation of gradient. Among the best performers was the Powell Search algorithm (Press et al., 1992). It first constructs a set of virtual directions that are conjugate (i.e., independent with each other), then it uses line search times ( in our case), each on one virtual direction, to find the optimum. Line</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2000</marker>
<rawString>Jarvelin, K. and Kekalainen, J. 2000. IR evaluation methods for retrieving highly relevant documents. In SIGIR, pp. 41-48</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jin</author>
<author>A G Hauptmann</author>
<author>C Zhai</author>
</authors>
<title>Title language model for information retrieval.</title>
<date>2002</date>
<booktitle>In SIGIR,</booktitle>
<pages>42--48</pages>
<contexts>
<context position="33243" citStr="Jin et al. 2002" startWordPosition="5697" endWordPosition="5700">udy, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. The MRF model has been previously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huan</context>
</contexts>
<marker>Jin, Hauptmann, Zhai, 2002</marker>
<rawString>Jin, R., Hauptmann, A. G., and Zhai, C. 2002. Title language model for information retrieval. In SIGIR, pp. 42-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jing</author>
<author>B Croft</author>
</authors>
<title>An association thesaurus for information retrieval.</title>
<date>1994</date>
<booktitle>In RIAO,</booktitle>
<pages>146--160</pages>
<contexts>
<context position="3601" citStr="Jing and Croft 1994" startWordPosition="579" endWordPosition="582">at do not use log data e.g. the local analysis model of Xu and Croft (1996). In addition, as pointed out by Cui et al. (2003) there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines. First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations to be pre-computed offline. Compared to methods that are based on thesauri either compiled manually (Prager et al. 2001) or derived au666 tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively. Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE pa</context>
</contexts>
<marker>Jing, Croft, 1994</marker>
<rawString>Jing, Y., and Croft., B. 1994. An association thesaurus for information retrieval. In RIAO, pp. 146-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>SIGKDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="33764" citStr="Joachims 2002" startWordPosition="5778" endWordPosition="5779">stical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as the problem of translating a source language of queries into a target language of documents, represented as titles. This allows us to adapt the established techniques developed for SMT to QE. Specially, we propose three lexicon models based on term</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, T. 2002. Optimizing search engines using clickthrough data. In SIGKDD, pp. 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT/NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="26932" citStr="Koehn et al. 2003" startWordPosition="4640" endWordPosition="4643">n is that Cui et al. performed the evaluation using documents and search logs collected from the Encarta website, which is much cleaner and more homogenous than the data sets we used. The result suggests that although QE improves the recall of relevant documents, it is also likely to introduce noise that hurts the precision of document retrieval. SMT (Row 3) is a SMT-based QE system. Following Riezler et al. (2008), the system is an implementation of a phrase-based SMT system with a standard set of features for translation model and language model, combined under a log linear model framework (Koehn et al. 2003). Different from Riezler et al.’s system where the translation model is trained on query-snippet pairs and the language model on queries, in our implementation the translation model is trained on query-title pairs and the language model on titles. To apply the system to QE, expansion terms of a query are taken from those terms in the 10-best translations of the query that have not been seen in the original query string. We see that SMT significantly outperforms TC in NDCG at all levels. The result confirms the conclusion of Riezler et al., demonstrating that context information is crucial for </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT/NAACL, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lang</author>
<author>D Metzler</author>
<author>B Wang</author>
<author>J-T Li</author>
</authors>
<title>Improving latent concept expansion using markov random fields.</title>
<date>2010</date>
<booktitle>In CIKM,</booktitle>
<pages>249--258</pages>
<contexts>
<context position="33025" citStr="Lang et al. 2010" startWordPosition="5660" endWordPosition="5663"> the relevant documents are not always available and generating pseudo-relevant documents requires multiphase retrieval, which is prohibitively expensive. Although automatic relevance feedback is not the focus of this study, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. The MRF model has been previously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more fl</context>
</contexts>
<marker>Lang, Metzler, Wang, Li, 2010</marker>
<rawString>Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010. Improving latent concept expansion using markov random fields. In CIKM, pp. 249-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>B Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In SIGIR,</booktitle>
<pages>120--128</pages>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Lavrenko, V., and Croft, B. 2001. Relevance-based language models. In SIGIR, pp. 120-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lease</author>
</authors>
<title>An improved markov random field model for supporting verbose queries.</title>
<date>2009</date>
<booktitle>In SIGIR,</booktitle>
<pages>476--483</pages>
<marker>Lease, 2009</marker>
<rawString>Lease, M. 2009. An improved markov random field model for supporting verbose queries. In SIGIR, pp. 476-483</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>B Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In SIGIR,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="20395" citStr="Metzler and Croft 2005" startWordPosition="3489" endWordPosition="3492"> issuing a set of queries which are expanded using the method to a search engine and then (10) | where |is the bigram probability computed using a bigram language model trained on the collection of document titles. The sixth type of cliques contains a pair of terms appearing unordered within the expansion. The potential functions are defined as | where |is the assigned by a co-occurrence model trained on the collection of document titles. 3.3 Parameter Estimation The MRF model uses 8 classes of features defined on 6 types of cliques, as in Equations (6) to (11). Following previous work (e.g., Metzler and Croft 2005; Bendersky et al. 2010), we assume that all features within the same feature class are weighted by the same tied parameter . Thus, the number of free parameters of the MRF model is significantly reduced. This not only makes the model training easier but also improves the robustness of the model. After tying the parameters and using the exponential potential function form, the MRFbased ranker can be parameterized as |⇒ (12) ∑ ∑ ∑ ∑ 671 measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. Due to t</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Metzler, D., and Croft, B. 2005. A markov random field model for term dependencies. In SIGIR, pp. 472-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>B Croft</author>
</authors>
<title>Latent concept expansion using markov random fields.</title>
<date>2007</date>
<booktitle>In SIGIR,</booktitle>
<pages>311--318</pages>
<marker>Metzler, Croft, 2007</marker>
<rawString>Metzler, D., and Croft, B. 2007. Latent concept expansion using markov random fields. In SIGIR, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Morgan</author>
<author>W Greiff</author>
<author>J Henderson</author>
</authors>
<title>Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach.</title>
<date>2004</date>
<tech>Technical report. MITRE.</tech>
<contexts>
<context position="18306" citStr="Morgan et al. 2004" startWordPosition="3139" endWordPosition="3142">pe of cliques contains only one expansion term. The potential functions are defined as 670 (9) where is the unigram probability computed using a unigram language model trained on the collection of document titles. The fifth type of cliques contains a pair of terms appearing in consecutive order in the expansion. The potential functions are defined as ∑ ∑ ∑ ∑ ∑ ∑ where there are in total 8 ’s to be estimated. Although the MRF is by nature a generative model, it is not always appropriate to train the parameters using conventional likelihood based approaches due to the metric divergence problem (Morgan et al. 2004): i.e., the maximum likelihood estimate is unlikely to be the one that optimizes the evaluation metric. In this study the effectiveness of a QE method is evaluated by first issuing a set of queries which are expanded using the method to a (11) search engine and then measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. For this reason, the parameters of the MRFbased ranker are optimized directly for Web search. In our experiments, the objective in training is Normalized Discounted Cumulative Gain </context>
</contexts>
<marker>Morgan, Greiff, Henderson, 2004</marker>
<rawString>Morgan, W., Greiff, W., and Henderson, J. 2004. Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach. Technical report. MITRE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical machine translation: from single-word models to alignment templates.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>RWTH Aachen.</institution>
<marker>Och, 2002</marker>
<rawString>Och, F. 2002. Statistical machine translation: from single-word models to alignment templates. PhD thesis, RWTH Aachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Use of Wordnet hypernyms for answering what is questions.</title>
<date>2001</date>
<journal>In TREC</journal>
<volume>10</volume>
<contexts>
<context position="3524" citStr="Prager et al. 2001" startWordPosition="567" endWordPosition="570">nsion terms for Web search. The method outperforms traditional QE methods that do not use log data e.g. the local analysis model of Xu and Croft (1996). In addition, as pointed out by Cui et al. (2003) there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines. First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations to be pre-computed offline. Compared to methods that are based on thesauri either compiled manually (Prager et al. 2001) or derived au666 tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively. Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al.</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2001</marker>
<rawString>Prager, J., Chu-Carroll, J., and Czuba, K. 2001. Use of Wordnet hypernyms for answering what is questions. In TREC 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical Recipes in C.</title>
<date>1992</date>
<publisher>Cambridge Univ. Press.</publisher>
<contexts>
<context position="19333" citStr="Press et al., 1992" startWordPosition="3307" endWordPosition="3310">query set. For this reason, the parameters of the MRFbased ranker are optimized directly for Web search. In our experiments, the objective in training is Normalized Discounted Cumulative Gain (NDCG, Jarvelin and Kekalainen 2000), which is widely used as quality measure for Web search. Formally, we view parameter training as a multidimensional optimization problem, with each feature class as one dimension. Since NDCG is not differentiable, we tried in our experiments numerical algorithms that do not require the computation of gradient. Among the best performers was the Powell Search algorithm (Press et al., 1992). It first constructs a set of virtual directions that are conjugate (i.e., independent with each other), then it uses line search times ( in our case), each on one virtual direction, to find the optimum. Line search is a one-dimensional optimization algorithm. Our implementation follows the one described in Gao et al. (2005), which is used to optimize averaged precision. 4 Experiments We evaluate the performance of a QE method by first issuing a set of queries which are expanded using the method to a search engine and then (10) | where |is the bigram probability computed using a bigram langua</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. 1992. Numerical Recipes in C. Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<booktitle>In The SMART retrieval system: experiments in automatic document processing,</booktitle>
<pages>313--323</pages>
<publisher>Prentice-Hall Inc.</publisher>
<contexts>
<context position="32256" citStr="Rocchio 1971" startWordPosition="5533" endWordPosition="5534">econd, unlike the two lexicon models, the bilingual topic model tends to generate expansions that are more likely to relate to an entire query rather than individual query terms. Third, the features involving the order of the expansion terms benefitted queries containing named entities. 5 Related Work In comparison with log-based methods studied in this paper, the QE methods based on automatic relevance feedback have been studied much more extensively in the information retrieval (IR) community, and have been proved useful for improving IR performance on benchmark datasets such as TREC (e.g., Rocchio 1971; Xu and Croft 1996; Lavrenko 2001; Zhai and Lafferty 2001). However, these methods cannot be applied directly to a commercial Web search engine because the relevant documents are not always available and generating pseudo-relevant documents requires multiphase retrieval, which is prohibitively expensive. Although automatic relevance feedback is not the focus of this study, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize </context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>Rocchio, J. 1971. Relevance feedback in information retrieval. In The SMART retrieval system: experiments in automatic document processing, pp. 313-323, Prentice-Hall Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>Y Liu</author>
<author>A Vasserman</author>
</authors>
<title>Translating queries into snippets for improving query expansion.</title>
<date>2008</date>
<booktitle>In COLING</booktitle>
<pages>737--744</pages>
<contexts>
<context position="4131" citStr="Riezler et al. (2008)" startWordPosition="665" endWordPosition="668">r et al. 2001) or derived au666 tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively. Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase </context>
<context position="21267" citStr="Riezler et al. 2008" startWordPosition="3636" endWordPosition="3639">ut also improves the robustness of the model. After tying the parameters and using the exponential potential function form, the MRFbased ranker can be parameterized as |⇒ (12) ∑ ∑ ∑ ∑ 671 measuring the Web search performance. Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set. Due to the characteristics of our QE methods, we cannot conduct experiments on standard test collections such as the TREC data because they do not contain related user logs we need. Therefore, following previous studies of log-based QE (e.g., Cui et al. 2003; Riezler et al. 2008), we use the proprietary datasets that have been developed for building a commercial search engine, and demonstrate the effectiveness of our methods by comparing them against previous state-of-the-art logbased QE methods. The relevance judgment set consists of 4,000 multi-term English queries. On average, each query is associated with 197 Web documents (URLs). Each query-URL pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document D is the most relevant to query Q and 0 meaning D is not relevant to Q. The relevance judgment s</context>
<context position="26732" citStr="Riezler et al. (2008)" startWordPosition="4605" endWordPosition="4608">t TC leads to significant improvement over IoQE in NDCG@10, but not in NDCG@1 and NDCG@3 (Row 2 vs. Row 1). The result is not entirely consistent with what reported in Cui et al. (2003). A possible reason is that Cui et al. performed the evaluation using documents and search logs collected from the Encarta website, which is much cleaner and more homogenous than the data sets we used. The result suggests that although QE improves the recall of relevant documents, it is also likely to introduce noise that hurts the precision of document retrieval. SMT (Row 3) is a SMT-based QE system. Following Riezler et al. (2008), the system is an implementation of a phrase-based SMT system with a standard set of features for translation model and language model, combined under a log linear model framework (Koehn et al. 2003). Different from Riezler et al.’s system where the translation model is trained on query-snippet pairs and the language model on queries, in our implementation the translation model is trained on query-title pairs and the language model on titles. To apply the system to QE, expansion terms of a query are taken from those terms in the 10-best translations of the query that have not been seen in the</context>
</contexts>
<marker>Riezler, Liu, Vasserman, 2008</marker>
<rawString>Riezler, S., Liu, Y. and Vasserman, A. 2008. Translating queries into snippets for improving query expansion. In COLING 2008. 737-744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>Y Liu</author>
</authors>
<title>Query rewriting using monolingual statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<pages>569--582</pages>
<contexts>
<context position="4535" citStr="Riezler and Liu 2010" startWordPosition="725" endWordPosition="728">ence of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase translation model in its decoder. Furthermore, in the SMT system all component models are properly smoothed using sophisticated techniques to avoid sparse data problems while the correlation model relies on pure counts of term frequencies. However, the SMT system is used as a black box in their experiments. So the relative contribution of different SMT components is not verified empirically. In this s</context>
</contexts>
<marker>Riezler, Liu, 2010</marker>
<rawString>Riezler, S., and Liu, Y. 2010. Query rewriting using monolingual statistical machine translation. Computational Linguistics, 36(3): 569-582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wen</author>
<author>J-Y Nie</author>
<author>H Zhang</author>
</authors>
<title>Query clustering using user logs.</title>
<date>2002</date>
<journal>ACM TOIS,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>59--81</pages>
<contexts>
<context position="33993" citStr="Wen et al. 2002" startWordPosition="5813" endWordPosition="5816"> recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao et al. 2010b) image retrieval (Craswell and Szummer 2007), and user query clustering (Baeza-Yates and Tiberi 2007; Wen et al. 2002). 6 Conclusions 674 In this paper we extend the previous log-based QE methods in two directions. First, we formulate QE as the problem of translating a source language of queries into a target language of documents, represented as titles. This allows us to adapt the established techniques developed for SMT to QE. Specially, we propose three lexicon models based on terms, lexicalized triplets, and topics, respectively. These models are trained on pairs of user queries and the titles of clicked documents using EM. Second, we present a ranker-based QE system, the heart of which is a MRF-based ran</context>
</contexts>
<marker>Wen, Nie, Zhang, 2002</marker>
<rawString>Wen, J., Nie, J-Y., and Zhang, H. 2002. Query clustering using user logs. ACM TOIS, 20(1): 59-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="3056" citStr="Xu and Croft (1996)" startWordPosition="493" endWordPosition="496">terest in applying user logs to improve QE. A recent survey is due to BaezeYates and Ribeiro-Neto (2011). Below, we briefly discuss two log-based QE methods that are closest to ours and are re-implemented in this study for comparison. Both systems use the same type of log data that we used to train the lexicon models. The term correlation model of Cui et al. (2002; 2003) is to our knowledge the first to explore querydocument relations for direct extraction of expansion terms for Web search. The method outperforms traditional QE methods that do not use log data e.g. the local analysis model of Xu and Croft (1996). In addition, as pointed out by Cui et al. (2003) there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines. First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations to be pre-computed offline. Compared to methods that are based on thesauri either compiled manually (Prager et al. 2001) or derived au666 tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitl</context>
<context position="32275" citStr="Xu and Croft 1996" startWordPosition="5535" endWordPosition="5538">the two lexicon models, the bilingual topic model tends to generate expansions that are more likely to relate to an entire query rather than individual query terms. Third, the features involving the order of the expansion terms benefitted queries containing named entities. 5 Related Work In comparison with log-based methods studied in this paper, the QE methods based on automatic relevance feedback have been studied much more extensively in the information retrieval (IR) community, and have been proved useful for improving IR performance on benchmark datasets such as TREC (e.g., Rocchio 1971; Xu and Croft 1996; Lavrenko 2001; Zhai and Lafferty 2001). However, these methods cannot be applied directly to a commercial Web search engine because the relevant documents are not always available and generating pseudo-relevant documents requires multiphase retrieval, which is prohibitively expensive. Although automatic relevance feedback is not the focus of this study, our method shares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. </context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Xu, J., and Croft, B. 1996. Query expansion using local and global document analysis. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval models for Question and answer archives.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="33261" citStr="Xue et al. 2008" startWordPosition="5701" endWordPosition="5704">hares a lot of similarities with some of them. For example, similar to the way the parameters of our QE ranker are estimated, Cao et al. (2008) propose a method of selecting expansion terms to directly optimize average precision. The MRF model has been previously used for QE, in the form of relevance feedback and pseudo-relevance feedback (Metzler et al. 2007; Lang et al. 2010). While their MRF models use the features derived from IR systems such as Indri, we use the SMT-inspired features. Using statistical translation models for IR is not new (e.g., Berger and Lafferty 1999; Jin et al. 2002; Xue et al. 2008). The effectiveness of the statistical translation-based approach to Web search has been demonstrated empirically in recent studies where word-based and phrase-based translation models are trained on large amounts of clickthrough data (e.g., Gao et al. 2010a; 2011). Our work extends these studies and constructs QE-oriented translation models that capture more flexible dependencies. In addition to QE, search logs have also been used for other Web search tasks, such as document ranking (Joachims 2002; Agichtein et al. 2006), search query processing and spelling correction (Huang et al. 2010; Gao</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xue, X., Jeon, J., Croft, W. B. 2008. Retrieval models for Question and answer archives. In SIGIR, pp. 475-482.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
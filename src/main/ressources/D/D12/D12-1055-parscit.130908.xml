<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020792">
<title confidence="0.97443">
Assessment of ESL Learners’ Syntactic Competence Based on Similarity
Measures
</title>
<author confidence="0.896922">
Su-Youn Yoon Suma Bhat
</author>
<affiliation confidence="0.754633">
Educational Testing Service Beckman Institute,
</affiliation>
<address confidence="0.832962">
Princeton, NJ 08541 Urbana, IL 61801
</address>
<email confidence="0.997941">
syoon@ets.org spbhat2@illinois.edu
</email>
<sectionHeader confidence="0.999634" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999555875">
This study provides a novel method that measures
ESL (English as a second language) learners’ com-
petence in grammar usage (syntactic competence).
Being interdisciplinary in nature, it shows how to
combine the core findings in the ESL literature with
various empirical NLP techniques for the purpose of
automated scoring.
Grammar usage is one of the dimensions of lan-
guage ability that is assessed during non-native pro-
ficiency level testing in a foreign language. Overall
proficiency in the target language can be assessed
by testing the abilities in various areas including flu-
ency, pronunciation, and intonation; grammar and
vocabulary; and discourse structure. Testing rubrics
for human raters contain descriptors used for the
subjective assessment of several of these features.
With the recent move towards the objective assess-
ment of language ability (spoken and written), it is
imperative that we develop methods for quantifying
these abilities and measuring them automatically.
Ortega (2003) indicated that “the range of forms
that surface in language production and the degree
of sophistication of such forms” were two impor-
tant areas in grammar usage and called the combina-
tion of these two areas “syntactic complexity.” Fea-
tures that measure syntactic complexity have been
frequently studied in ESL literature and have been
found to be highly correlated with students’ profi-
ciency levels in writing.
Studies in automated speech scoring have focused
on fluency (Cucchiarini et al., 2000; Cucchiarini et
al., 2002), pronunciation (Witt and Young, 1997;
</bodyText>
<sectionHeader confidence="0.669573" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999931294117647">
This study presents a novel method that
measures English language learners’ syntac-
tic competence towards improving automated
speech scoring systems. In contrast to most
previous studies which focus on the length of
production units such as the mean length of
clauses, we focused on capturing the differ-
ences in the distribution of morpho-syntactic
features or grammatical expressions across
proficiency. We estimated the syntactic com-
petence through the use of corpus-based NLP
techniques. Assuming that the range and so-
phistication of grammatical expressions can
be captured by the distribution of Part-of-
Speech (POS) tags, vector space models of
POS tags were constructed. We use a large
corpus of English learners’ responses that are
classified into four proficiency levels by hu-
man raters. Our proposed feature measures
the similarity of a given response with the
most proficient group and is then estimates the
learner’s syntactic competence level.
Widely outperforming the state-of-the-art
measures of syntactic complexity, our method
attained a significant correlation with human-
rated scores. The correlation between human-
rated scores and features based on manual
transcription was 0.43 and the same based on
ASR-hypothesis was slightly lower, 0.42. An
important advantage of our method is its ro-
bustness against speech recognition errors not
to mention the simplicity of feature genera-
tion that captures a reasonable set of learner-
specific syntactic errors.
</bodyText>
<page confidence="0.957327">
600
</page>
<note confidence="0.7754215">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997665072463768">
Witt, 1999; Franco et al., 1997; Neumeyer et al.,
2000), and intonation (Zechner et al., 2009), and rel-
atively fewer studies have been conducted on gram-
mar usage. More recently, Lu (2010), Chen and
Yoon (2011) and Chen and Zechner (2011) have
measured syntactic competence in speech scoring.
Chen and Yoon (2011) estimated the complexity of
sentences based on the average length of the clauses
or sentences. In addition to these length measures,
Lu (2010) and Chen and Zechner (2011) measured
the parse-tree based features such as the mean depth
of parsing tree levels. However, these studies found
that these measures did not show satisfactory empir-
ical performance in automatic speech scoring (Chen
and Yoon, 2011; Chen and Zechner, 2011) when the
features were calculated from the output of a speech
recognition engine.
This study considers new features that measure
syntactic complexity and is novel in two important
ways. First, in contrast to most features that in-
fer syntactic complexity based upon the length of
the unit, we directly measure students’ sophistica-
tion and range in grammar usage. Second, instead
of rating a student’s response using a scale based on
native speech production, our experiments compare
it with a similar body of learners’ speech. Elicit-
ing native speakers’ data and rating it for grammar
usage (supervised approach) can be arbitrary, since
there can be a very wide range of possible grammat-
ical structures that native speakers utilize. Instead,
we proceed in a semi-supervised fashion. A large
amount of learners’ spoken responses were collected
and classified into four groups according to their
proficiency level. We then sought to find how dis-
tinct the proficiency classes were based on the distri-
bution of POS tags. Given a student’s response, we
calculated the similarity with a sample of responses
for each score level based on the proportion and dis-
tribution of Part-of-Speech using NLP techniques.
POS tag distribution has been used in various
tasks such as text genre classification (Feldman et
al., 2009); in a language testing context, it has been
used in grammatical error detection (Chodorow and
Leacock, 2000; Tetreault and Chodorow, 2008) and
essay scoring. Recently, Roark et al. (2011) ex-
plored POS tag distribution to capture the differ-
ences in syntactic complexity between healthy sub-
jects and subjects with mild cognitive impairment,
but no other research has used POS tag distribution
in measuring syntactic complexity, to the best of au-
thors’ knowledge.
An assessment of ESL learners’ syntactic compe-
tence should consider the structure of sentences as a
whole - a task which may not be captured by the sim-
plistic POS tag distribution. However, studies of Lu
(2010) and Chen and Zechner (2011) showed that
more complex syntactic features are unreliable in
ASR-based scoring system. Furthermore, we show
that POS unigrams or bigrams indeed capture a rea-
sonable portion of learners’ range and sophistication
of grammar usage in our discussion in Section 7.
This paper will proceed as follows: we will re-
view related work in Section 2 and present the
method to calculate syntactic complexity in Section
3. Data and experiment setup will be explained in
Section 4 and Section 5. The results will be pre-
sented in Section 6. Finally, in Section 7, we discuss
the levels of syntactic competence that are captured
using our proposed measure.
</bodyText>
<sectionHeader confidence="0.999689" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999842875">
Second Language Acquisition (SLA) researchers
have developed many quantitative measures to es-
timate the level of acquisition of syntactic compe-
tence. Bardovi-Harlig and Bofman (1989) classi-
fied these measures into two groups. The first group
is related to the acquisition of specific morphosyn-
tactic features or grammatical expressions. Tests of
negations or relative clauses - whether these expres-
sions occurred in the test responses without errors -
fell into this group (hereafter, the expression-based
group). The second group is related to the length of
the clause or the relationship between clauses and
hence not tied to particular structures (hereafter, the
length-based group). Examples of the second group
measures include the average length of clause unit
and dependent clauses per sentence unit.
These syntactic measures have been extensively
studied in ESL writing. Ortega (2003) synthesized
25 research studies which employed syntactic mea-
sures on ESL writing and reported a significant re-
lationship between the proposed features and writ-
ing proficiency. He reported that a subset of features
such as the mean length of the clause unit increased
with students’ proficiency. More recently, Lu (2010)
</bodyText>
<page confidence="0.997657">
601
</page>
<bodyText confidence="0.999980784615385">
has conducted a more systematic study using an au-
tomated system. He applied 14 syntactic measures
to a large database of Chinese learners’ writing sam-
ples and found that syntactic measures were strong
predictors of students’ writing proficiency.
Studies in the area of automated speech scor-
ing have only recently begun to actively investi-
gate the usefulness of syntactic measures for scoring
spontaneous speech (Chen et al., 2010; Bernstein
et al., 2010). These have identified clause bound-
aries (identified from manual annotations and au-
tomatically) and obtained length-based features. In
addition to these conventional syntactic complexity
features, Lu (2009) implemented an automated sys-
tem that calculates the revised Developmental Level
(D-Level) Scale (Covington et al., 2006) using nat-
ural language processing (NLP) techniques. The
original D-Level Scale was proposed by Rosenberg
and Abbeduto (1987) based primarily on observa-
tions of child language acquisition. They classified
children’s grammatical acquisition into 7 different
groups according to the presence of certain types of
complex sentences. The revised D-Level Scale clas-
sified sentences into the eight levels according to the
presence of particular grammatical expressions. For
instance, level 0 is comprised of simple sentences,
while level 5 is comprised of sentences joined by
subordinating conjunction or nonfinite clauses in an
adjunct position. The D-Level Scale has been less
studied in the speech scoring. To our knowledge,
Chen and Zechner (2011) is the only study that ap-
plied the D-Level analyzer to ESL learners’ spoken
responses.
In contrast to ESL writing, applying syntactic
complexity features, both conventional length-based
features and D-Level features, presents serious ob-
stacles for speaking. First, the length of the spo-
ken responses are typically shorter than written re-
sponses. Most measures are based on sentence or
sentence-like units, and in speaking tests that elicit
only a few sentences the measures are less reli-
able. Chen and Yoon (2011) observed a marked
decrease in correlation between syntactic measures
and proficiency as response length decreased. In
addition, speech recognition errors only worsen the
situation. Chen and Zechner (2011) showed that
the significant correlation between syntactic mea-
sures and speech proficiency (correlation coefficient
= 0.49) became insignificant when they were applied
to the speech recognition word hypotheses. Errors
in speech recognition seriously influenced the mea-
sures and decreased the performance. Due to these
problems, the existing syntactic measures do not
seem reliable enough for being used in automated
speech proficiency scoring.
In this study, we propose novel syntactic measures
which are relatively robust against speech recogni-
tion errors and are reliable in short responses. In
contrast to recent studies focusing on length-based
features, we focus on capturing differences in the
distribution of morphosyntactic features or gram-
matical expressions across proficiency levels. We in-
vestigate the distribution of a broader class of gram-
matical forms through the use of corpus-based NLP
techniques.
</bodyText>
<sectionHeader confidence="0.991751" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99998375">
Many previous studies, that assess syntactic com-
plexity based on the distribution of morpho-
syntactic features and grammatical expressions, lim-
ited their experiments to a few grammatical expres-
sions. Covington et al. (2006) and Lu (2009) cov-
ered all sentence types, but their approaches were
based on expert observation (supervised rubrics),
and descriptions of each level were brief and ab-
stract. It is important to develop a more detailed and
refined scale, but developing scales in a supervised
way is difficult due to the subjectivity and the com-
plexity of structures involved.
In order to overcome this problem, we employed
NLP technology and a corpus-based approach. We
hypothesize that the level of acquired grammatical
forms is signaled by the distribution of the POS tags,
and the differences in grammatical proficiency re-
sult in differences in POS tag distribution. Based on
this assumption, we collected large amount of ESL
learners’ spoken responses and classified them into
four groups according to their proficiency levels.
The syntactic competence was estimated based on
the similarity between the test responses and learn-
ers’ corpus.
A POS-based vector space model (VSM), in
which the response belonging to separate profi-
ciency levels were converted to vectors and the sim-
ilarity between vectors were calculated using cosine
</bodyText>
<page confidence="0.993356">
602
</page>
<bodyText confidence="0.999962969696969">
similarity measure and tf-idf weighting, was em-
ployed. Such a score-category-based VSM has been
used in automated essay scoring. Attali and Burstein
(2006) to assess the lexical content of an essay by
comparing the words in the test essay with the words
in a sample essays from each score category. We
extend this to assessment of grammar usage using
vectors of POS tags.
Proficient speakers use complicated grammati-
cal expressions, while beginners use simple expres-
sions and sentences with frequent grammatical er-
rors. POS tags (or sequences) capturing these ex-
pressions may be seen in corresponding proportions
in each score group. These distributional differences
are captured by inverse-document frequency.
In addition, we identify frequent POS tag se-
quences as those having high mutual information
and include them in our experiments. Temple (2000)
pointed out that the proficient learners are charac-
terized by increased automaticity in speech produc-
tion. These speakers tend to memorize frequently
used multi-word sequences as a chunk and retrieve
the whole chunk as a single unit. The degree of auto-
maticity can be captured by the frequent occurrence
of POS sequences with high mutual information.
We quantify the usefulness of the generated fea-
tures for the purpose of automatic scoring by first
considering its correlation with the human scores.
We then compare the performance of our features
with those in Lu (2011), where the features are a
collection of measures of syntactic complexity that
have shown promising directions in previous stud-
ies.
</bodyText>
<sectionHeader confidence="0.998644" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.9999855">
Two different sets of data were used in this study:
the AEST 48K dataset and AEST balanced dataset.
Both were collections of responses from the AEST,
a high-stakes test of English proficiency and had
no overlaps. The AEST assessment consists of 6
items in which speakers are prompted to provide re-
sponses lasting between 45 and 60 seconds per item.
In summary, approximately 3 minutes of speech is
collected per speaker.
Among the 6 items, two items are tasks that ask
examinees to provide information or opinions on fa-
miliar topics based on their personal experience or
background knowledge. The four remaining items
are integrated tasks that include other language skills
such as listening and reading. All items extract
spontaneous, unconstrained natural speech. The
size, purpose, and speakers’ native language infor-
mation for each dataset is summarized in Table 1.
Each response was rated by trained human raters
using a 4-point scoring scale, where 1 indicates
a low speaking proficiency and 4 indicates a high
speaking proficiency. In order to evaluate the relia-
bility of the human ratings, the data should be scored
by two raters. Since none of the AEST balanced
data was double scored the inter-rater agreement ra-
tio was estimated using a large (41K) double-scored
dataset using the same scoring guidelines and scor-
ing process. The Pearson correlation coefficient was
0.63 suggesting a reasonable inter-rater agreement.
The distribution of the scores for this data can be
found in Table 2.
We used the AEST 48K dataset as the training
data and the AEST balanced dataset as the evalua-
tion data.
</bodyText>
<sectionHeader confidence="0.999489" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.810351">
5.1 Overview
</subsectionHeader>
<bodyText confidence="0.9999073125">
Our experimental procedure is as follows. All tran-
scriptions were tagged using the POS tagger de-
scribed in Section 5.3 and POS tag sequences were
extracted. Next, the POS-based VSMs (one for
each score class) were created using the AEST 48K
dataset. Finally, for a given test response in the
AEST balanced dataset, similarity features were
generated.
A score-class-specific POS-based VSM was cre-
ated using POS tags generated from the manual tran-
scriptions. For evaluation, two different types of
transcriptions (manual transcription and word hy-
potheses from the speech recognizer described in
Section 5.2) were used in order to investigate the in-
fluence of speech recognition errors in the feature
performance.
</bodyText>
<subsectionHeader confidence="0.997636">
5.2 Speech recognition
</subsectionHeader>
<bodyText confidence="0.9999605">
An HMM recognizer was trained on AEST 48K
dataset - approximately 733 hours of non-native
speech collected from 7872 speakers. A gender in-
dependent triphone acoustic model and combination
</bodyText>
<page confidence="0.994022">
603
</page>
<table confidence="0.9996055">
Corpus name Purpose Number of Number of Native languages Size
speakers responses (Hrs)
AEST 48K ASR training and 7872 47227 China (20%), Korea (19%), 733
data POS model train- Japanese (7%), India (7%), oth-
ing ers (46%)
AEST bal- Feature develop- 480 2880 Korean (15%), Chinese (14%), 44
anced data ment and evalua- Japanese (7%), Spanish (9%),
tion Others (55%)
</table>
<tableCaption confidence="0.991227">
Table 1: Data size and speakers native languages
</tableCaption>
<table confidence="0.9999456">
Corpus name Size Score1 Score2 Score3 Score4
AEST 48K data Number of files 1953 16834 23106 5334
(%) 4 36 49 11
AEST balanced data Number of files 141 1133 1266 340
(%) 5 40 45 12
</table>
<tableCaption confidence="0.999157">
Table 2: Proficiency scores and data sizes
</tableCaption>
<bodyText confidence="0.982653">
of bigram, trigram, and four-gram language models
were used. The word error rate (WER) on the held-
out test dataset was 27%.
</bodyText>
<subsectionHeader confidence="0.995153">
5.3 POS tagger
</subsectionHeader>
<bodyText confidence="0.999958142857143">
POS tags were generated using the POS tagger im-
plemented in the OpenNLP toolkit. It was trained
on the Switchboard (SWBD) corpus. This POS tag-
ger was trained on about 528K word/tag pairs and
achieved a tagging accuracy of 96.3% on a test set
of 379K words. The Penn POS tag set was used in
the tagger.
</bodyText>
<subsectionHeader confidence="0.958051">
5.4 Unit generation using mutual information
</subsectionHeader>
<bodyText confidence="0.9998019375">
POS bigrams with high mutual information were se-
lected and used as a single unit. First, all POS bi-
grams which occurred less than 50 times were fil-
tered out. Next, the remaining POS tag bigrams
were sorted by their mutual information scores, and
two different sets (top50 and top110) were selected.
The selected POS pairs were transformed into com-
pound tags. As a result, we generated three sets
of POS units by this process: the original POS set
without the compound unit (Base), the original set
and an additional 50 compound units (Base+mi50),
and the original set and an additional 110 units
(Base+mi110).
Finally, unigram, bigram and trigram were gener-
ated for each set separately. The size of total terms
in each condition was presented in table 3.
</bodyText>
<table confidence="0.9981485">
Base Base+mi50 Base+mi110
Unigram 42 93 151
Bigram 1366 4284 9691
Trigram 21918 54856 135430
</table>
<tableCaption confidence="0.99969">
Table 3: Number of terms used in VSMs
</tableCaption>
<subsectionHeader confidence="0.995507">
5.5 Building VSMs
</subsectionHeader>
<bodyText confidence="0.99887575">
For each ngram, three sets of VSMs were built us-
ing three sets of tags as terms, yielding a total of
nine VSMs. The results were based on the individ-
ual model and we did not combine any models.
</bodyText>
<subsectionHeader confidence="0.987543">
5.6 Cosine similarity-based features
</subsectionHeader>
<bodyText confidence="0.999965466666667">
The cosine similarity has been frequently used in
the information retrieval field to identify the relevant
documents for the given query. This measures the
similarity between a given query and a document by
measuring the cosine of the angle between vectors in
a high-dimensional space, whereby each term in the
query and documents corresponding to a unique di-
mension. If a document is relevant to the query, then
it shares many terms resulting in a small angle. In
this study, the term was a single or compound POS
tag (unigram,bigram or trigram) weighted by its tf-
idf, and the document was the response.
First, the inverse document frequency was calcu-
lated from the training data, and each response was
treated as a document. Next, responses in the same
</bodyText>
<page confidence="0.997878">
604
</page>
<table confidence="0.999446833333333">
Unigram Bigram Trigram
Base Base Base Base Base Base Base Base Base
+mi50 +mi110 +mi50 +mi110 +mi50 +mi110
Trans- 0.301** 0.297** 0.329** 0.427** 0.361** 0.366** 0.402** 0.322** 0.295**
cription
ASR 0.246** 0.272** 0.304** 0.415** 0.348** 0.347** 0.373** 0.311** 0.282**
</table>
<tableCaption confidence="0.995677">
Table 4: Pearson correlation coefficients between ngram-based features and expert proficiency scores
** Correlation is significant at the 0.01 level
</tableCaption>
<bodyText confidence="0.9998226">
score group were concatenated, and a single vector
was generated for each score group. A total of 4
vectors were generated using training data. For each
test response, a similarity score was calculated as
follows:
</bodyText>
<equation confidence="0.9555116">
cos(q,
� N �
qi = tf(ti, �q) x log df(ti)
�dji = t f (ti, dj) x log df N
( i)
</equation>
<bodyText confidence="0.894043277777778">
where q� is a vector of the test response,
dj is a vector of the scoreGroupj,
n is the total number of POS tags,
tf(ti, qj is the term frequency of POS tag ti in the
test response,
tf(ti, dj) is the term frequency of POS tag ti in the
scoreGroupj,
N is the total number of training responses,
df(ti) is the document frequency of POS tag ti in
the total training responses
Finally, a total of 4 cos scores (one per score
group) were generated. Among these four values,
the cos4, the similarity score to the responses in the
score group 4, was selected as a feature with the fol-
lowing intuition. cos4 measures the similarity of a
given test response to the representative vector of
score class 4; the larger the value, the closer it would
be to score class 4.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.998869">
6.1 Correlation
</subsectionHeader>
<bodyText confidence="0.997668678571429">
Table 4 shows correlations between cosine similarity
features and proficiency scores rated by experts.
The bigram-based features outperformed both
unigram-based and trigram-based features. In par-
ticular, the similarities using the base tag set with
bigrams achieved the best performance. By adding
the mutual information-based compound units to the
original POS tag sets, the performance of features
improved in the unigram models. However, there
was no performance gain in either bigram or tri-
gram models; on the contrary, there was a large
drop in performance. Unigrams have good coverage
but limited power in distinguishing different score
levels. On the other hand, trigrams have opposite
characteristics. Bigrams seem to strike a balance
in both coverage and complexity (from among the
three considered here) and may thus have resulted in
the best performace.
The performance of ASR-based features were
comparable to that of transcription-based features.
The best performing feature among ASR-based-
features were from the bigram and base set, with
correlations nearly the same as the best performing
one among the transcription-based-features. See-
ing how close the correlations were in the case of
transcription-based and ASR-hypothesis based fea-
ture extraction, we conclude that the proposed mea-
sure is robust to ASR errors.
</bodyText>
<subsectionHeader confidence="0.9998405">
6.2 Comparison with other Measures of
Syntactic Complexity
</subsectionHeader>
<bodyText confidence="0.9999344">
We compared the performance of our features with
the features of syntactic complexity proposed in (Lu,
2011). Towards this, the clause boundaries of the
ASR hypotheses, were automatically detected using
the automated clause boundary detection method1.
</bodyText>
<footnote confidence="0.966836">
1The automated clause boundary detection method in this
study was a Maximum Entropy Model based on word bigrams,
POS tag bigrams, and pause features. The method achieved an
</footnote>
<equation confidence="0.895345">
��
i=1
dj) _
qi2
qidji
di2
��
i=1
��
i=1
</equation>
<page confidence="0.994392">
605
</page>
<bodyText confidence="0.998736333333333">
The utterances were then parsed using the Stanford
Parser, and a total of 22 features including both
length-related features and parse-tree based features
were generated using (Lu, 2011). Finally, we calcu-
lated Pearson correlation coefficients between these
features and human proficiency scores.
</bodyText>
<table confidence="0.996767666666667">
Study Feature Correlation
Current study bigram based cos4 0.41**
(Lu, 2011) DCC 0.14**
</table>
<tableCaption confidence="0.995231">
Table 5: Comparison between (Lu, 2011) and this study
** Correlation is significant at the 0.01 level
</tableCaption>
<bodyText confidence="0.999971947368421">
As indicated in Table 5, the best performing fea-
ture was mean number of dependent clauses per
clause (DCC) and the correlation r was 0.14. No
features other than DCC achieved statistically sig-
nificant correlation. Our best performing feature (bi-
gram based cos4) widely outperformed the best of
Lu (2011)’s features (correlations approximately 0.3
apart).
A logical explanation for the poor performance of
Lu (2011)’s features is that the features are gener-
ated using multi-stage automated process, and the
errors in each process contributes the low feature
performance. For instance, the errors in the auto-
mated clause boundary detection may result in a se-
rious drop in the performance. With the spoken re-
sponses being particularly short (a typical response
in the data set had 10 clauses on average), even one
error in clause boundary detection can seriously af-
fect the reliability of features.
</bodyText>
<sectionHeader confidence="0.997163" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999785161290322">
While the measure of syntactic competence that we
study here is an abstraction of the overall syntactic
competence, without consideration of specific con-
structions, we analyzed the results further with the
intention of casting light on the level of details of
syntactic competence that can be explained using
our measure. Furthermore, this section will show
that bigram POS sequences can yield significant in-
formation on the range and sophistication of gram-
mar usage in the specific assessment context (spon-
F-score of 0.60 on the non-native speakers’ ASR hypotheses.
A detailed description of the method is presented in (Chen and
Zechner, 2011)
taneous speech comprised of only declarative sen-
tences).
ESL speakers with high proficiency scores are ex-
pected to use more complicated grammatical expres-
sions that result in a high proportion of POS tags
related to these expressions in that score group. The
distribution of POS tags was analyzed in detail in or-
der to investigate whether there were systematic dis-
tributional changes according to proficiency levels.
Owing to space constraints, we restrict our discus-
sion to the analysis using unigrams (base and com-
pund). For each score group, the POS tags were
sorted based on the frequencies in training data, and
the rank orders were calculated. The more frequent
the POS tag, the higher its rank.
A total of 150 POS tags, including the original
POS tag set and top 110 compound tags, were clas-
sified into 5 classes:
</bodyText>
<listItem confidence="0.991057928571428">
• Absence-of-low-proficiency (ABS): Group of
POS tags that appear in all score groups except
the lowest proficiency group;
• Increase (INC): Group of POS tags whose
ranks increase consistently as proficiency in-
creases;
• Decrease (DEC): Group of POS tags whose
ranks decrease consistently as proficiency in-
creases;
• Constant (CON): Group of POS tags whose
ranks remain same despite change in profi-
ciency;
• Mix: Group of POS tags of with no consistent
pattern in the ranks.
</listItem>
<bodyText confidence="0.733803">
Table 6 presents the number of POS tags in each
class.
</bodyText>
<table confidence="0.770121">
ABS INC DEC CON Mix
14 37 33 18 48
</table>
<tableCaption confidence="0.998613">
Table 6: Tag distribution and proficiency scores
</tableCaption>
<bodyText confidence="0.99901925">
The ‘ABS’ class mostly consists of ‘WP’ and
‘WDT’; more than 50% of tags in this class are re-
lated to these two tags. ‘WP’ is a Wh-pronoun while
‘WDT’ is a Wh-determiner. Since most sentences in
</bodyText>
<page confidence="0.996954">
606
</page>
<bodyText confidence="0.999991653846154">
our data are declarative sentences, ‘Wh’ phrase sig-
nals the use of relative clause. Therefore, the lack
of these tags strongly support the hypothesis that the
speakers in score group 1 showed incompetence in
the use of relative clauses or their use in limited sit-
uations.
The ‘INC’ class can be sub-classified into three
groups: verb, comparative, and relative clause. Verb
group is includes the infinitive (TO VB), passive
(VB VBN, VBD VBN, VBN, VBN IN, VBN RP),
and gerund forms (VBG, VBG RP, VBG TO). Next,
the comparative group encompasses comparative
constructions. Finally, the relative clause group sig-
nals the presence of relative clauses. The increased
proportion of these tags reflects the use of more
complicated tense forms and modal forms as well
as more frequent use of relative clauses. It supports
the hypothesis that speakers with higher proficiency
scores tend to use more complicated grammatical
expressions.
The ‘DEC’ class can be sub-classified into five
groups: noun, simple tense verb, GW and UH,
non-compound, and comparative. The noun group
is comprised of many noun or proper noun-related
expressions, and their high proportions are consis-
tent with the tendency that less proficient speakers
use nouns more frequently. Secondly, the simple
tense verb group is comprised of the base form (VB)
and simple present and past forms(PRP VBD, VB,
VBD TO, VBP TO, VBZ). The expressions in these
groups are simpler than those in ‘Increase’ group.
The ‘UH’ tag is for interjection and filler words
such as ‘uh’ and ‘um’, while the ‘GW’ tag is for
word-fragments. These two spontaneous speech
phenomena are strongly related to fluency, and it
signals problems in speech production. Frequent
occurrences of these two tags are evidence of fre-
quent planning problems and their inclusion in the
‘DEC’ class suggests that instances of speech plan-
ning problems decrease with increased proficiency.
Tags in the non-compound group, such as ‘DT’,
‘MD’, ‘RBS’, and ‘TO’, have related compound
tags. The non-compound tags are associated with
the expressions that do not co-occur with strongly
related words, and they tend to be related to errors.
For instance, the non-compound ‘MD’ tag signals
that there is an expression that a modal verb is not
followed by ‘VB’ (base form) and as seen in the ex-
amples, ‘the project may can change’ and ‘the others
must can not be good’, they are related to grammat-
ical errors.
Finally, the comparative group includes
‘RBR JJR’. The decrease of ‘RBR JJR’ is re-
lated to the correct acquisition of the comparative
form. ‘RBR’ is for comparative adverbs and ‘JJR’ is
for comparative adjectives, and the combination of
two tags is strongly related to double-marked errors
such as ‘more easier’. In the intermediate stage in
the acquisition of comparative form, learners tend
to use the double-marked form. The compound tags
correctly capture this erroneous stage.
The ‘Decrease’ class also includes three Wh-
related tags (WDT NN, WDT VBP, WRB), but the
proportion is much smaller than the ‘Increase’ class.
The above analysis shows that the combination of
original and compound POS tags correctly capture
systematic changes in the grammatical expressions
according to changes in proficiency levels.
The robust performance of our proposed mea-
sure to speech recognition errors may be better ap-
preciated in the context of similar studies. Com-
pared with the state-of-the art measures of syntac-
tic complexity proposed in Lu (2011) our features
achieve significantly better performance especially
when generated from ASR hypotheses. It is to
be noted that the performance drop between the
transcription-based feature and the ASR hypothesis-
based feature was marginal.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999967384615385">
In this paper, we presented features that measure
syntactic competence for the automated speech scor-
ing. The features measured the range and sophisti-
cation of grammatical expressions based on POS tag
distributions. A corpus with a large number of learn-
ers’ responses was collected and classified into four
groups according to proficiency levels. The syntac-
tic competence of the test response was estimated by
identifying the most similar group from the learners’
corpus. Furthermore, speech recognition errors only
resulted in a minor performance drop. The robust-
ness against speech recognition errors is an impor-
tant advantage of our method.
</bodyText>
<page confidence="0.997227">
607
</page>
<sectionHeader confidence="0.998327" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999581333333333">
The authors would like to thank Shasha Xie, Klaus
Zechner, and Keelan Evanini for their valuable com-
ments, help with data preparation and experiments.
</bodyText>
<sectionHeader confidence="0.990259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846918367347">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e–rater R v.2. The Journal of Technology,
Learning, and Assessment, 4(3).
Kathleen Bardovi-Harlig and Theodora Bofman. 1989.
Attainment of syntactic and morphological accuracy
by advanced language learners. Studies in Second
Language Acquisition, 11:17–34.
Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010.
Fluency and structural complexity as predictors of L2
oral proficiency. In Proceedings of InterSpeech 2010,
Tokyo, Japan, September.
Lei Chen and Su-Youn Yoon. 2011. Detecting structural
events for assessing non-native speech. In Proceed-
ings of the 6th Workshop on Innovative Use of NLP for
Building Educational Applications, pages 38–45.
Miao Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics 2011, pages 722–731.
Lei Chen, Joel Tetreault, and Xiaoming Xi. 2010.
Towards using structural events to assess non-native
speech. In Proceedings of the NAACL HLT 2010 Fifth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 74–79.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
In Proceedings of NAACL00, pages 140–147.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown. 2006. How complex
is that sentence? A proposed revision of the Rosen-
berg and Abbeduto D-Level Scale. Technical report,
CASPR Research Report 2006-01, Athens, GA: The
University of Georgia, Artificial Intelligence Center.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners’
fluency: Comparisons between read and spontaneous
speech. The Journal of theAcoustical Society ofAmer-
ica, 107(2):989–999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners’
fluency: Comparisons between read and spontaneous
speech. The Journal of theAcoustical Society ofAmer-
ica, 111(6):2862–2873.
Sergey Feldman, M.A. Marin, Maria Ostendorf, and
Maya R. Gupta. 2009. Part-of-speech histograms for
genre classification of text. In Acoustics, Speech and
Signal Processing, 2009. ICASSP 2009. IEEE Interna-
tional Conference on, pages 4781 –4784, april.
Horacio Franco, Leonardo Neumeyer, Yoon Kim, and
Orith Ronen. 1997. Automatic pronunciation scoring
for language instruction. In Proceedings of ICASSP
97, pages 1471–1474.
Xiaofei Lu. 2009. Automatic measurement of syntac-
tic complexity in child language acquisition. Interna-
tional Journal of Corpus Linguistics, 14(1):3–28.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474–496.
Xiaofei Lu. 2011. L2 syntactic complex-
ity analyze. Retrieved March 17, 2012 from
http://www.personal.psu.edu/xxl13/
downloads/l2sca.html/.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, pages 88–93.
Lourdes Ortega. 2003. Syntactic complexity measures
and their relationship to L2 proficiency: A research
synthesis of college–level L2 writing. Applied Lin-
guistics, 24(4):492–518.
Brian. Roark, Margaret Mitchell, John-Paul. Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild cog-
nitive impairment. Audio, Speech, and Language
Processing, IEEE Transactions on, 19(7):2081 –2090,
sept.
Sheldon Rosenberg and Leonard Abbeduto. 1987. Indi-
cators of linguistic competence in the peer group con-
versational behavior of mildly retarded adults. Applied
Psycholinguistics, 8:19–32.
Liz Temple. 2000. Second language learner speech pro-
duction. Studia Linguistica, pages 288–297.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in esl writing.
In In Proceedings of COLING.
Silke Witt and Steve Young. 1997. Performance mea-
sures for phone-level pronunciation teaching in CALL.
In Proceedings of the Workshop on Speech Technology
in Language Learning, pages 99–102.
Silke Witt. 1999. Use of the speech recognition in
computer-assisted language learning. Unpublished
dissertation, Cambridge University Engineering de-
partment, Cambridge, U.K.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51:883–895, October.
</reference>
<page confidence="0.997239">
608
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.831810">
<title confidence="0.9978435">Assessment of ESL Learners’ Syntactic Competence Based on Similarity Measures</title>
<author confidence="0.998387">Su-Youn Yoon Suma Bhat</author>
<affiliation confidence="0.998883">Educational Testing Service Beckman Institute,</affiliation>
<address confidence="0.999972">Princeton, NJ 08541 Urbana, IL 61801</address>
<email confidence="0.837584">syoon@ets.orgspbhat2@illinois.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e–rater R v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="12806" citStr="Attali and Burstein (2006)" startWordPosition="1955" endWordPosition="1958">. Based on this assumption, we collected large amount of ESL learners’ spoken responses and classified them into four groups according to their proficiency levels. The syntactic competence was estimated based on the similarity between the test responses and learners’ corpus. A POS-based vector space model (VSM), in which the response belonging to separate proficiency levels were converted to vectors and the similarity between vectors were calculated using cosine 602 similarity measure and tf-idf weighting, was employed. Such a score-category-based VSM has been used in automated essay scoring. Attali and Burstein (2006) to assess the lexical content of an essay by comparing the words in the test essay with the words in a sample essays from each score category. We extend this to assessment of grammar usage using vectors of POS tags. Proficient speakers use complicated grammatical expressions, while beginners use simple expressions and sentences with frequent grammatical errors. POS tags (or sequences) capturing these expressions may be seen in corresponding proportions in each score group. These distributional differences are captured by inverse-document frequency. In addition, we identify frequent POS tag se</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e–rater R v.2. The Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Bardovi-Harlig</author>
<author>Theodora Bofman</author>
</authors>
<title>Attainment of syntactic and morphological accuracy by advanced language learners.</title>
<date>1989</date>
<booktitle>Studies in Second Language Acquisition,</booktitle>
<pages>11--17</pages>
<contexts>
<context position="7103" citStr="Bardovi-Harlig and Bofman (1989)" startWordPosition="1094" endWordPosition="1097">cation of grammar usage in our discussion in Section 7. This paper will proceed as follows: we will review related work in Section 2 and present the method to calculate syntactic complexity in Section 3. Data and experiment setup will be explained in Section 4 and Section 5. The results will be presented in Section 6. Finally, in Section 7, we discuss the levels of syntactic competence that are captured using our proposed measure. 2 Related Work Second Language Acquisition (SLA) researchers have developed many quantitative measures to estimate the level of acquisition of syntactic competence. Bardovi-Harlig and Bofman (1989) classified these measures into two groups. The first group is related to the acquisition of specific morphosyntactic features or grammatical expressions. Tests of negations or relative clauses - whether these expressions occurred in the test responses without errors - fell into this group (hereafter, the expression-based group). The second group is related to the length of the clause or the relationship between clauses and hence not tied to particular structures (hereafter, the length-based group). Examples of the second group measures include the average length of clause unit and dependent c</context>
</contexts>
<marker>Bardovi-Harlig, Bofman, 1989</marker>
<rawString>Kathleen Bardovi-Harlig and Theodora Bofman. 1989. Attainment of syntactic and morphological accuracy by advanced language learners. Studies in Second Language Acquisition, 11:17–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Bernstein</author>
<author>Jian Cheng</author>
<author>Masanori Suzuki</author>
</authors>
<title>Fluency and structural complexity as predictors of L2 oral proficiency.</title>
<date>2010</date>
<booktitle>In Proceedings of InterSpeech 2010,</booktitle>
<location>Tokyo, Japan,</location>
<contexts>
<context position="8592" citStr="Bernstein et al., 2010" startWordPosition="1325" endWordPosition="1328">writing proficiency. He reported that a subset of features such as the mean length of the clause unit increased with students’ proficiency. More recently, Lu (2010) 601 has conducted a more systematic study using an automated system. He applied 14 syntactic measures to a large database of Chinese learners’ writing samples and found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automated speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addition to these conventional syntactic complexity features, Lu (2009) implemented an automated system that calculates the revised Developmental Level (D-Level) Scale (Covington et al., 2006) using natural language processing (NLP) techniques. The original D-Level Scale was proposed by Rosenberg and Abbeduto (1987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 different groups accordin</context>
</contexts>
<marker>Bernstein, Cheng, Suzuki, 2010</marker>
<rawString>Jared Bernstein, Jian Cheng, and Masanori Suzuki. 2010. Fluency and structural complexity as predictors of L2 oral proficiency. In Proceedings of InterSpeech 2010, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Su-Youn Yoon</author>
</authors>
<title>Detecting structural events for assessing non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>38--45</pages>
<contexts>
<context position="3723" citStr="Chen and Yoon (2011)" startWordPosition="552" endWordPosition="555">ts robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show satisfactory empirical performance in automatic speech scoring (Chen and Yoon, 2011; Chen and Zechner, 2011) when the features were calculated from the output of a speech recogn</context>
<context position="10178" citStr="Chen and Yoon (2011)" startWordPosition="1560" endWordPosition="1563">. The D-Level Scale has been less studied in the speech scoring. To our knowledge, Chen and Zechner (2011) is the only study that applied the D-Level analyzer to ESL learners’ spoken responses. In contrast to ESL writing, applying syntactic complexity features, both conventional length-based features and D-Level features, presents serious obstacles for speaking. First, the length of the spoken responses are typically shorter than written responses. Most measures are based on sentence or sentence-like units, and in speaking tests that elicit only a few sentences the measures are less reliable. Chen and Yoon (2011) observed a marked decrease in correlation between syntactic measures and proficiency as response length decreased. In addition, speech recognition errors only worsen the situation. Chen and Zechner (2011) showed that the significant correlation between syntactic measures and speech proficiency (correlation coefficient = 0.49) became insignificant when they were applied to the speech recognition word hypotheses. Errors in speech recognition seriously influenced the measures and decreased the performance. Due to these problems, the existing syntactic measures do not seem reliable enough for bei</context>
</contexts>
<marker>Chen, Yoon, 2011</marker>
<rawString>Lei Chen and Su-Youn Yoon. 2011. Detecting structural events for assessing non-native speech. In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, pages 38–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>722--731</pages>
<contexts>
<context position="3751" citStr="Chen and Zechner (2011)" startWordPosition="557" endWordPosition="560">ech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show satisfactory empirical performance in automatic speech scoring (Chen and Yoon, 2011; Chen and Zechner, 2011) when the features were calculated from the output of a speech recognition engine. This study con</context>
<context position="6265" citStr="Chen and Zechner (2011)" startWordPosition="960" endWordPosition="963">ion (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captured by the simplistic POS tag distribution. However, studies of Lu (2010) and Chen and Zechner (2011) showed that more complex syntactic features are unreliable in ASR-based scoring system. Furthermore, we show that POS unigrams or bigrams indeed capture a reasonable portion of learners’ range and sophistication of grammar usage in our discussion in Section 7. This paper will proceed as follows: we will review related work in Section 2 and present the method to calculate syntactic complexity in Section 3. Data and experiment setup will be explained in Section 4 and Section 5. The results will be presented in Section 6. Finally, in Section 7, we discuss the levels of syntactic competence that </context>
<context position="9664" citStr="Chen and Zechner (2011)" startWordPosition="1479" endWordPosition="1482">987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 different groups according to the presence of certain types of complex sentences. The revised D-Level Scale classified sentences into the eight levels according to the presence of particular grammatical expressions. For instance, level 0 is comprised of simple sentences, while level 5 is comprised of sentences joined by subordinating conjunction or nonfinite clauses in an adjunct position. The D-Level Scale has been less studied in the speech scoring. To our knowledge, Chen and Zechner (2011) is the only study that applied the D-Level analyzer to ESL learners’ spoken responses. In contrast to ESL writing, applying syntactic complexity features, both conventional length-based features and D-Level features, presents serious obstacles for speaking. First, the length of the spoken responses are typically shorter than written responses. Most measures are based on sentence or sentence-like units, and in speaking tests that elicit only a few sentences the measures are less reliable. Chen and Yoon (2011) observed a marked decrease in correlation between syntactic measures and proficiency </context>
<context position="25181" citStr="Chen and Zechner, 2011" startWordPosition="3983" endWordPosition="3986">syntactic competence that we study here is an abstraction of the overall syntactic competence, without consideration of specific constructions, we analyzed the results further with the intention of casting light on the level of details of syntactic competence that can be explained using our measure. Furthermore, this section will show that bigram POS sequences can yield significant information on the range and sophistication of grammar usage in the specific assessment context (sponF-score of 0.60 on the non-native speakers’ ASR hypotheses. A detailed description of the method is presented in (Chen and Zechner, 2011) taneous speech comprised of only declarative sentences). ESL speakers with high proficiency scores are expected to use more complicated grammatical expressions that result in a high proportion of POS tags related to these expressions in that score group. The distribution of POS tags was analyzed in detail in order to investigate whether there were systematic distributional changes according to proficiency levels. Owing to space constraints, we restrict our discussion to the analysis using unigrams (base and compund). For each score group, the POS tags were sorted based on the frequencies in t</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Miao Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics 2011, pages 722–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Chen</author>
<author>Joel Tetreault</author>
<author>Xiaoming Xi</author>
</authors>
<title>Towards using structural events to assess non-native speech.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>74--79</pages>
<contexts>
<context position="8567" citStr="Chen et al., 2010" startWordPosition="1321" endWordPosition="1324">posed features and writing proficiency. He reported that a subset of features such as the mean length of the clause unit increased with students’ proficiency. More recently, Lu (2010) 601 has conducted a more systematic study using an automated system. He applied 14 syntactic measures to a large database of Chinese learners’ writing samples and found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automated speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addition to these conventional syntactic complexity features, Lu (2009) implemented an automated system that calculates the revised Developmental Level (D-Level) Scale (Covington et al., 2006) using natural language processing (NLP) techniques. The original D-Level Scale was proposed by Rosenberg and Abbeduto (1987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 </context>
</contexts>
<marker>Chen, Tetreault, Xi, 2010</marker>
<rawString>Lei Chen, Joel Tetreault, and Xiaoming Xi. 2010. Towards using structural events to assess non-native speech. In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors. In</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL00,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="5673" citStr="Chodorow and Leacock, 2000" startWordPosition="863" endWordPosition="866">ount of learners’ spoken responses were collected and classified into four groups according to their proficiency level. We then sought to find how distinct the proficiency classes were based on the distribution of POS tags. Given a student’s response, we calculated the similarity with a sample of responses for each score level based on the proportion and distribution of Part-of-Speech using NLP techniques. POS tag distribution has been used in various tasks such as text genre classification (Feldman et al., 2009); in a language testing context, it has been used in grammatical error detection (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captured by the simplistic POS tag distribution. However, studies of Lu (2010) and Chen and Zechner (2011) showed </context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An unsupervised method for detecting grammatical errors. In In Proceedings of NAACL00, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
<author>Congzhou He</author>
<author>Cati Brown</author>
<author>Lorina Naci</author>
<author>John Brown</author>
</authors>
<title>How complex is that sentence? A proposed revision of the Rosenberg and Abbeduto D-Level Scale.</title>
<date>2006</date>
<tech>Technical report, CASPR Research Report 2006-01,</tech>
<institution>The University of Georgia, Artificial Intelligence Center.</institution>
<location>Athens, GA:</location>
<contexts>
<context position="8920" citStr="Covington et al., 2006" startWordPosition="1369" endWordPosition="1372">found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automated speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addition to these conventional syntactic complexity features, Lu (2009) implemented an automated system that calculates the revised Developmental Level (D-Level) Scale (Covington et al., 2006) using natural language processing (NLP) techniques. The original D-Level Scale was proposed by Rosenberg and Abbeduto (1987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 different groups according to the presence of certain types of complex sentences. The revised D-Level Scale classified sentences into the eight levels according to the presence of particular grammatical expressions. For instance, level 0 is comprised of simple sentences, while level 5 is comprised of sentences joined by subordinating conjunction or no</context>
<context position="11529" citStr="Covington et al. (2006)" startWordPosition="1755" endWordPosition="1758">against speech recognition errors and are reliable in short responses. In contrast to recent studies focusing on length-based features, we focus on capturing differences in the distribution of morphosyntactic features or grammatical expressions across proficiency levels. We investigate the distribution of a broader class of grammatical forms through the use of corpus-based NLP techniques. 3 Method Many previous studies, that assess syntactic complexity based on the distribution of morphosyntactic features and grammatical expressions, limited their experiments to a few grammatical expressions. Covington et al. (2006) and Lu (2009) covered all sentence types, but their approaches were based on expert observation (supervised rubrics), and descriptions of each level were brief and abstract. It is important to develop a more detailed and refined scale, but developing scales in a supervised way is difficult due to the subjectivity and the complexity of structures involved. In order to overcome this problem, we employed NLP technology and a corpus-based approach. We hypothesize that the level of acquired grammatical forms is signaled by the distribution of the POS tags, and the differences in grammatical profic</context>
</contexts>
<marker>Covington, He, Brown, Naci, Brown, 2006</marker>
<rawString>Michael A. Covington, Congzhou He, Cati Brown, Lorina Naci, and John Brown. 2006. How complex is that sentence? A proposed revision of the Rosenberg and Abbeduto D-Level Scale. Technical report, CASPR Research Report 2006-01, Athens, GA: The University of Georgia, Artificial Intelligence Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech.</title>
<date>2000</date>
<journal>The Journal of theAcoustical Society ofAmerica,</journal>
<volume>107</volume>
<issue>2</issue>
<contexts>
<context position="1730" citStr="Cucchiarini et al., 2000" startWordPosition="254" endWordPosition="257">tten), it is imperative that we develop methods for quantifying these abilities and measuring them automatically. Ortega (2003) indicated that “the range of forms that surface in language production and the degree of sophistication of such forms” were two important areas in grammar usage and called the combination of these two areas “syntactic complexity.” Features that measure syntactic complexity have been frequently studied in ESL literature and have been found to be highly correlated with students’ proficiency levels in writing. Studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Abstract This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and sophist</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2000</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000. Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech. The Journal of theAcoustical Society ofAmerica, 107(2):989–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catia Cucchiarini</author>
<author>Helmer Strik</author>
<author>Lou Boves</author>
</authors>
<title>Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech.</title>
<date>2002</date>
<journal>The Journal of theAcoustical Society ofAmerica,</journal>
<volume>111</volume>
<issue>6</issue>
<contexts>
<context position="1757" citStr="Cucchiarini et al., 2002" startWordPosition="258" endWordPosition="261">at we develop methods for quantifying these abilities and measuring them automatically. Ortega (2003) indicated that “the range of forms that surface in language production and the degree of sophistication of such forms” were two important areas in grammar usage and called the combination of these two areas “syntactic complexity.” Features that measure syntactic complexity have been frequently studied in ESL literature and have been found to be highly correlated with students’ proficiency levels in writing. Studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Abstract This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and sophistication of grammatical expr</context>
</contexts>
<marker>Cucchiarini, Strik, Boves, 2002</marker>
<rawString>Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002. Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech. The Journal of theAcoustical Society ofAmerica, 111(6):2862–2873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Feldman</author>
<author>M A Marin</author>
<author>Maria Ostendorf</author>
<author>Maya R Gupta</author>
</authors>
<title>Part-of-speech histograms for genre classification of text.</title>
<date>2009</date>
<booktitle>In Acoustics, Speech and Signal Processing,</booktitle>
<pages>4781--4784</pages>
<contexts>
<context position="5565" citStr="Feldman et al., 2009" startWordPosition="846" endWordPosition="849">l structures that native speakers utilize. Instead, we proceed in a semi-supervised fashion. A large amount of learners’ spoken responses were collected and classified into four groups according to their proficiency level. We then sought to find how distinct the proficiency classes were based on the distribution of POS tags. Given a student’s response, we calculated the similarity with a sample of responses for each score level based on the proportion and distribution of Part-of-Speech using NLP techniques. POS tag distribution has been used in various tasks such as text genre classification (Feldman et al., 2009); in a language testing context, it has been used in grammatical error detection (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captur</context>
</contexts>
<marker>Feldman, Marin, Ostendorf, Gupta, 2009</marker>
<rawString>Sergey Feldman, M.A. Marin, Maria Ostendorf, and Maya R. Gupta. 2009. Part-of-speech histograms for genre classification of text. In Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, pages 4781 –4784, april.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Franco</author>
<author>Leonardo Neumeyer</author>
<author>Yoon Kim</author>
<author>Orith Ronen</author>
</authors>
<title>Automatic pronunciation scoring for language instruction.</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP 97,</booktitle>
<pages>1471--1474</pages>
<contexts>
<context position="3545" citStr="Franco et al., 1997" startWordPosition="522" endWordPosition="525">en humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show</context>
</contexts>
<marker>Franco, Neumeyer, Kim, Ronen, 1997</marker>
<rawString>Horacio Franco, Leonardo Neumeyer, Yoon Kim, and Orith Ronen. 1997. Automatic pronunciation scoring for language instruction. In Proceedings of ICASSP 97, pages 1471–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic measurement of syntactic complexity in child language acquisition.</title>
<date>2009</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="8799" citStr="Lu (2009)" startWordPosition="1354" endWordPosition="1355">ated system. He applied 14 syntactic measures to a large database of Chinese learners’ writing samples and found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automated speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addition to these conventional syntactic complexity features, Lu (2009) implemented an automated system that calculates the revised Developmental Level (D-Level) Scale (Covington et al., 2006) using natural language processing (NLP) techniques. The original D-Level Scale was proposed by Rosenberg and Abbeduto (1987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 different groups according to the presence of certain types of complex sentences. The revised D-Level Scale classified sentences into the eight levels according to the presence of particular grammatical expressions. For instance, le</context>
<context position="11543" citStr="Lu (2009)" startWordPosition="1760" endWordPosition="1761">rrors and are reliable in short responses. In contrast to recent studies focusing on length-based features, we focus on capturing differences in the distribution of morphosyntactic features or grammatical expressions across proficiency levels. We investigate the distribution of a broader class of grammatical forms through the use of corpus-based NLP techniques. 3 Method Many previous studies, that assess syntactic complexity based on the distribution of morphosyntactic features and grammatical expressions, limited their experiments to a few grammatical expressions. Covington et al. (2006) and Lu (2009) covered all sentence types, but their approaches were based on expert observation (supervised rubrics), and descriptions of each level were brief and abstract. It is important to develop a more detailed and refined scale, but developing scales in a supervised way is difficult due to the subjectivity and the complexity of structures involved. In order to overcome this problem, we employed NLP technology and a corpus-based approach. We hypothesize that the level of acquired grammatical forms is signaled by the distribution of the POS tags, and the differences in grammatical proficiency result i</context>
</contexts>
<marker>Lu, 2009</marker>
<rawString>Xiaofei Lu. 2009. Automatic measurement of syntactic complexity in child language acquisition. International Journal of Corpus Linguistics, 14(1):3–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="3701" citStr="Lu (2010)" startWordPosition="550" endWordPosition="551">method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show satisfactory empirical performance in automatic speech scoring (Chen and Yoon, 2011; Chen and Zechner, 2011) when the features were calculated from the out</context>
<context position="6237" citStr="Lu (2010)" startWordPosition="957" endWordPosition="958">l error detection (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captured by the simplistic POS tag distribution. However, studies of Lu (2010) and Chen and Zechner (2011) showed that more complex syntactic features are unreliable in ASR-based scoring system. Furthermore, we show that POS unigrams or bigrams indeed capture a reasonable portion of learners’ range and sophistication of grammar usage in our discussion in Section 7. This paper will proceed as follows: we will review related work in Section 2 and present the method to calculate syntactic complexity in Section 3. Data and experiment setup will be explained in Section 4 and Section 5. The results will be presented in Section 6. Finally, in Section 7, we discuss the levels o</context>
<context position="8133" citStr="Lu (2010)" startWordPosition="1254" endWordPosition="1255">es and hence not tied to particular structures (hereafter, the length-based group). Examples of the second group measures include the average length of clause unit and dependent clauses per sentence unit. These syntactic measures have been extensively studied in ESL writing. Ortega (2003) synthesized 25 research studies which employed syntactic measures on ESL writing and reported a significant relationship between the proposed features and writing proficiency. He reported that a subset of features such as the mean length of the clause unit increased with students’ proficiency. More recently, Lu (2010) 601 has conducted a more systematic study using an automated system. He applied 14 syntactic measures to a large database of Chinese learners’ writing samples and found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automated speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addit</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>L2 syntactic complexity analyze. Retrieved</title>
<date>2011</date>
<note>from http://www.personal.psu.edu/xxl13/ downloads/l2sca.html/.</note>
<contexts>
<context position="14077" citStr="Lu (2011)" startWordPosition="2159" endWordPosition="2160">em in our experiments. Temple (2000) pointed out that the proficient learners are characterized by increased automaticity in speech production. These speakers tend to memorize frequently used multi-word sequences as a chunk and retrieve the whole chunk as a single unit. The degree of automaticity can be captured by the frequent occurrence of POS sequences with high mutual information. We quantify the usefulness of the generated features for the purpose of automatic scoring by first considering its correlation with the human scores. We then compare the performance of our features with those in Lu (2011), where the features are a collection of measures of syntactic complexity that have shown promising directions in previous studies. 4 Data Two different sets of data were used in this study: the AEST 48K dataset and AEST balanced dataset. Both were collections of responses from the AEST, a high-stakes test of English proficiency and had no overlaps. The AEST assessment consists of 6 items in which speakers are prompted to provide responses lasting between 45 and 60 seconds per item. In summary, approximately 3 minutes of speech is collected per speaker. Among the 6 items, two items are tasks t</context>
<context position="22780" citStr="Lu, 2011" startWordPosition="3607" endWordPosition="3608">based features were comparable to that of transcription-based features. The best performing feature among ASR-basedfeatures were from the bigram and base set, with correlations nearly the same as the best performing one among the transcription-based-features. Seeing how close the correlations were in the case of transcription-based and ASR-hypothesis based feature extraction, we conclude that the proposed measure is robust to ASR errors. 6.2 Comparison with other Measures of Syntactic Complexity We compared the performance of our features with the features of syntactic complexity proposed in (Lu, 2011). Towards this, the clause boundaries of the ASR hypotheses, were automatically detected using the automated clause boundary detection method1. 1The automated clause boundary detection method in this study was a Maximum Entropy Model based on word bigrams, POS tag bigrams, and pause features. The method achieved an �� i=1 dj) _ qi2 qidji di2 �� i=1 �� i=1 605 The utterances were then parsed using the Stanford Parser, and a total of 22 features including both length-related features and parse-tree based features were generated using (Lu, 2011). Finally, we calculated Pearson correlation coeffic</context>
<context position="24042" citStr="Lu (2011)" startWordPosition="3803" endWordPosition="3804">ores. Study Feature Correlation Current study bigram based cos4 0.41** (Lu, 2011) DCC 0.14** Table 5: Comparison between (Lu, 2011) and this study ** Correlation is significant at the 0.01 level As indicated in Table 5, the best performing feature was mean number of dependent clauses per clause (DCC) and the correlation r was 0.14. No features other than DCC achieved statistically significant correlation. Our best performing feature (bigram based cos4) widely outperformed the best of Lu (2011)’s features (correlations approximately 0.3 apart). A logical explanation for the poor performance of Lu (2011)’s features is that the features are generated using multi-stage automated process, and the errors in each process contributes the low feature performance. For instance, the errors in the automated clause boundary detection may result in a serious drop in the performance. With the spoken responses being particularly short (a typical response in the data set had 10 clauses on average), even one error in clause boundary detection can seriously affect the reliability of features. 7 Discussion While the measure of syntactic competence that we study here is an abstraction of the overall syntactic c</context>
<context position="30275" citStr="Lu (2011)" startWordPosition="4827" endWordPosition="4828">und tags correctly capture this erroneous stage. The ‘Decrease’ class also includes three Whrelated tags (WDT NN, WDT VBP, WRB), but the proportion is much smaller than the ‘Increase’ class. The above analysis shows that the combination of original and compound POS tags correctly capture systematic changes in the grammatical expressions according to changes in proficiency levels. The robust performance of our proposed measure to speech recognition errors may be better appreciated in the context of similar studies. Compared with the state-of-the art measures of syntactic complexity proposed in Lu (2011) our features achieve significantly better performance especially when generated from ASR hypotheses. It is to be noted that the performance drop between the transcription-based feature and the ASR hypothesisbased feature was marginal. 8 Conclusions In this paper, we presented features that measure syntactic competence for the automated speech scoring. The features measured the range and sophistication of grammatical expressions based on POS tag distributions. A corpus with a large number of learners’ responses was collected and classified into four groups according to proficiency levels. The </context>
</contexts>
<marker>Lu, 2011</marker>
<rawString>Xiaofei Lu. 2011. L2 syntactic complexity analyze. Retrieved March 17, 2012 from http://www.personal.psu.edu/xxl13/ downloads/l2sca.html/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Neumeyer</author>
<author>Horacio Franco</author>
<author>Vassilios Digalakis</author>
<author>Mitchel Weintraub</author>
</authors>
<title>Automatic scoring of pronunciation quality. Speech Communication,</title>
<date>2000</date>
<pages>88--93</pages>
<contexts>
<context position="3569" citStr="Neumeyer et al., 2000" startWordPosition="526" endWordPosition="529">and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show satisfactory empirical </context>
</contexts>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 2000</marker>
<rawString>Leonardo Neumeyer, Horacio Franco, Vassilios Digalakis, and Mitchel Weintraub. 2000. Automatic scoring of pronunciation quality. Speech Communication, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lourdes Ortega</author>
</authors>
<title>Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college–level L2 writing.</title>
<date>2003</date>
<journal>Applied Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1233" citStr="Ortega (2003)" startWordPosition="177" endWordPosition="178">is assessed during non-native proficiency level testing in a foreign language. Overall proficiency in the target language can be assessed by testing the abilities in various areas including fluency, pronunciation, and intonation; grammar and vocabulary; and discourse structure. Testing rubrics for human raters contain descriptors used for the subjective assessment of several of these features. With the recent move towards the objective assessment of language ability (spoken and written), it is imperative that we develop methods for quantifying these abilities and measuring them automatically. Ortega (2003) indicated that “the range of forms that surface in language production and the degree of sophistication of such forms” were two important areas in grammar usage and called the combination of these two areas “syntactic complexity.” Features that measure syntactic complexity have been frequently studied in ESL literature and have been found to be highly correlated with students’ proficiency levels in writing. Studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Abstract This study presents a novel </context>
<context position="7813" citStr="Ortega (2003)" startWordPosition="1204" endWordPosition="1205">ific morphosyntactic features or grammatical expressions. Tests of negations or relative clauses - whether these expressions occurred in the test responses without errors - fell into this group (hereafter, the expression-based group). The second group is related to the length of the clause or the relationship between clauses and hence not tied to particular structures (hereafter, the length-based group). Examples of the second group measures include the average length of clause unit and dependent clauses per sentence unit. These syntactic measures have been extensively studied in ESL writing. Ortega (2003) synthesized 25 research studies which employed syntactic measures on ESL writing and reported a significant relationship between the proposed features and writing proficiency. He reported that a subset of features such as the mean length of the clause unit increased with students’ proficiency. More recently, Lu (2010) 601 has conducted a more systematic study using an automated system. He applied 14 syntactic measures to a large database of Chinese learners’ writing samples and found that syntactic measures were strong predictors of students’ writing proficiency. Studies in the area of automa</context>
</contexts>
<marker>Ortega, 2003</marker>
<rawString>Lourdes Ortega. 2003. Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college–level L2 writing. Applied Linguistics, 24(4):492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell Roark</author>
<author>John-Paul Hosom</author>
<author>Kristy Hollingshead</author>
<author>Jeffrey Kaye</author>
</authors>
<title>Spoken language derived measures for detecting mild cognitive impairment. Audio, Speech, and Language Processing,</title>
<date>2011</date>
<journal>IEEE Transactions on,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="5753" citStr="Roark et al. (2011)" startWordPosition="875" endWordPosition="878">ding to their proficiency level. We then sought to find how distinct the proficiency classes were based on the distribution of POS tags. Given a student’s response, we calculated the similarity with a sample of responses for each score level based on the proportion and distribution of Part-of-Speech using NLP techniques. POS tag distribution has been used in various tasks such as text genre classification (Feldman et al., 2009); in a language testing context, it has been used in grammatical error detection (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captured by the simplistic POS tag distribution. However, studies of Lu (2010) and Chen and Zechner (2011) showed that more complex syntactic features are unreliable in ASR-based scoring system.</context>
</contexts>
<marker>Roark, Hosom, Hollingshead, Kaye, 2011</marker>
<rawString>Brian. Roark, Margaret Mitchell, John-Paul. Hosom, Kristy Hollingshead, and Jeffrey Kaye. 2011. Spoken language derived measures for detecting mild cognitive impairment. Audio, Speech, and Language Processing, IEEE Transactions on, 19(7):2081 –2090, sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheldon Rosenberg</author>
<author>Leonard Abbeduto</author>
</authors>
<title>Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults. Applied Psycholinguistics,</title>
<date>1987</date>
<pages>8--19</pages>
<contexts>
<context position="9045" citStr="Rosenberg and Abbeduto (1987)" startWordPosition="1387" endWordPosition="1390"> speech scoring have only recently begun to actively investigate the usefulness of syntactic measures for scoring spontaneous speech (Chen et al., 2010; Bernstein et al., 2010). These have identified clause boundaries (identified from manual annotations and automatically) and obtained length-based features. In addition to these conventional syntactic complexity features, Lu (2009) implemented an automated system that calculates the revised Developmental Level (D-Level) Scale (Covington et al., 2006) using natural language processing (NLP) techniques. The original D-Level Scale was proposed by Rosenberg and Abbeduto (1987) based primarily on observations of child language acquisition. They classified children’s grammatical acquisition into 7 different groups according to the presence of certain types of complex sentences. The revised D-Level Scale classified sentences into the eight levels according to the presence of particular grammatical expressions. For instance, level 0 is comprised of simple sentences, while level 5 is comprised of sentences joined by subordinating conjunction or nonfinite clauses in an adjunct position. The D-Level Scale has been less studied in the speech scoring. To our knowledge, Chen</context>
</contexts>
<marker>Rosenberg, Abbeduto, 1987</marker>
<rawString>Sheldon Rosenberg and Leonard Abbeduto. 1987. Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults. Applied Psycholinguistics, 8:19–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liz Temple</author>
</authors>
<title>Second language learner speech production. Studia Linguistica,</title>
<date>2000</date>
<pages>288--297</pages>
<contexts>
<context position="13504" citStr="Temple (2000)" startWordPosition="2066" endWordPosition="2067">ith the words in a sample essays from each score category. We extend this to assessment of grammar usage using vectors of POS tags. Proficient speakers use complicated grammatical expressions, while beginners use simple expressions and sentences with frequent grammatical errors. POS tags (or sequences) capturing these expressions may be seen in corresponding proportions in each score group. These distributional differences are captured by inverse-document frequency. In addition, we identify frequent POS tag sequences as those having high mutual information and include them in our experiments. Temple (2000) pointed out that the proficient learners are characterized by increased automaticity in speech production. These speakers tend to memorize frequently used multi-word sequences as a chunk and retrieve the whole chunk as a single unit. The degree of automaticity can be captured by the frequent occurrence of POS sequences with high mutual information. We quantify the usefulness of the generated features for the purpose of automatic scoring by first considering its correlation with the human scores. We then compare the performance of our features with those in Lu (2011), where the features are a </context>
</contexts>
<marker>Temple, 2000</marker>
<rawString>Liz Temple. 2000. Second language learner speech production. Studia Linguistica, pages 288–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in esl writing. In</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5704" citStr="Tetreault and Chodorow, 2008" startWordPosition="867" endWordPosition="870">ponses were collected and classified into four groups according to their proficiency level. We then sought to find how distinct the proficiency classes were based on the distribution of POS tags. Given a student’s response, we calculated the similarity with a sample of responses for each score level based on the proportion and distribution of Part-of-Speech using NLP techniques. POS tag distribution has been used in various tasks such as text genre classification (Feldman et al., 2009); in a language testing context, it has been used in grammatical error detection (Chodorow and Leacock, 2000; Tetreault and Chodorow, 2008) and essay scoring. Recently, Roark et al. (2011) explored POS tag distribution to capture the differences in syntactic complexity between healthy subjects and subjects with mild cognitive impairment, but no other research has used POS tag distribution in measuring syntactic complexity, to the best of authors’ knowledge. An assessment of ESL learners’ syntactic competence should consider the structure of sentences as a whole - a task which may not be captured by the simplistic POS tag distribution. However, studies of Lu (2010) and Chen and Zechner (2011) showed that more complex syntactic fea</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in esl writing. In In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Witt</author>
<author>Steve Young</author>
</authors>
<title>Performance measures for phone-level pronunciation teaching in CALL.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Speech Technology in Language Learning,</booktitle>
<pages>99--102</pages>
<contexts>
<context position="1794" citStr="Witt and Young, 1997" startWordPosition="263" endWordPosition="266">e abilities and measuring them automatically. Ortega (2003) indicated that “the range of forms that surface in language production and the degree of sophistication of such forms” were two important areas in grammar usage and called the combination of these two areas “syntactic complexity.” Features that measure syntactic complexity have been frequently studied in ESL literature and have been found to be highly correlated with students’ proficiency levels in writing. Studies in automated speech scoring have focused on fluency (Cucchiarini et al., 2000; Cucchiarini et al., 2002), pronunciation (Witt and Young, 1997; Abstract This study presents a novel method that measures English language learners’ syntactic competence towards improving automated speech scoring systems. In contrast to most previous studies which focus on the length of production units such as the mean length of clauses, we focused on capturing the differences in the distribution of morpho-syntactic features or grammatical expressions across proficiency. We estimated the syntactic competence through the use of corpus-based NLP techniques. Assuming that the range and sophistication of grammatical expressions can be captured by the distri</context>
</contexts>
<marker>Witt, Young, 1997</marker>
<rawString>Silke Witt and Steve Young. 1997. Performance measures for phone-level pronunciation teaching in CALL. In Proceedings of the Workshop on Speech Technology in Language Learning, pages 99–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Witt</author>
</authors>
<title>Use of the speech recognition in computer-assisted language learning. Unpublished dissertation,</title>
<date>1999</date>
<institution>Cambridge University Engineering department,</institution>
<location>Cambridge, U.K.</location>
<contexts>
<context position="3524" citStr="Witt, 1999" startWordPosition="520" endWordPosition="521">lation between humanrated scores and features based on manual transcription was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these </context>
</contexts>
<marker>Witt, 1999</marker>
<rawString>Silke Witt. 1999. Use of the speech recognition in computer-assisted language learning. Unpublished dissertation, Cambridge University Engineering department, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Derrick Higgins</author>
<author>Xiaoming Xi</author>
<author>David M Williamson</author>
</authors>
<title>Automatic scoring of non-native spontaneous speech in tests of spoken english.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<pages>51--883</pages>
<contexts>
<context position="3608" citStr="Zechner et al., 2009" startWordPosition="532" endWordPosition="535">on was 0.43 and the same based on ASR-hypothesis was slightly lower, 0.42. An important advantage of our method is its robustness against speech recognition errors not to mention the simplicity of feature generation that captures a reasonable set of learnerspecific syntactic errors. 600 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 600–608, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Witt, 1999; Franco et al., 1997; Neumeyer et al., 2000), and intonation (Zechner et al., 2009), and relatively fewer studies have been conducted on grammar usage. More recently, Lu (2010), Chen and Yoon (2011) and Chen and Zechner (2011) have measured syntactic competence in speech scoring. Chen and Yoon (2011) estimated the complexity of sentences based on the average length of the clauses or sentences. In addition to these length measures, Lu (2010) and Chen and Zechner (2011) measured the parse-tree based features such as the mean depth of parsing tree levels. However, these studies found that these measures did not show satisfactory empirical performance in automatic speech scoring</context>
</contexts>
<marker>Zechner, Higgins, Xi, Williamson, 2009</marker>
<rawString>Klaus Zechner, Derrick Higgins, Xiaoming Xi, and David M. Williamson. 2009. Automatic scoring of non-native spontaneous speech in tests of spoken english. Speech Communication, 51:883–895, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.998015">
A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes
</title>
<author confidence="0.999316">
Robert V. Lindsey
</author>
<affiliation confidence="0.999942">
University of Colorado, Boulder
</affiliation>
<email confidence="0.982345">
robert.lindsey@colorado.edu
</email>
<author confidence="0.411456">
William P. Headden III
</author>
<affiliation confidence="0.359423">
Two Cassowaries Inc.
</affiliation>
<email confidence="0.747133">
headdenw@twocassowaries.com
</email>
<author confidence="0.811923">
Michael J. Stipicevic
</author>
<affiliation confidence="0.777578">
Google Inc.
</affiliation>
<email confidence="0.995226">
stip@google.com
</email>
<sectionHeader confidence="0.996604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999638052631579">
Topic models traditionally rely on the bag-
of-words assumption. In data mining appli-
cations, this often results in end-users being
presented with inscrutable lists of topical un-
igrams, single words inferred as representa-
tive of their topics. In this article, we present
a hierarchical generative probabilistic model
of topical phrases. The model simultane-
ously infers the location, length, and topic of
phrases within a corpus and relaxes the bag-
of-words assumption within phrases by using
a hierarchy of Pitman-Yor processes. We use
Markov chain Monte Carlo techniques for ap-
proximate inference in the model and perform
slice sampling to learn its hyperparameters.
We show via an experiment on human subjects
that our model finds substantially better, more
interpretable topical phrases than do compet-
ing models.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840869565218">
Probabilistic topic models have been the focus of
intense study in recent years. The archetypal topic
model, Latent Dirichlet Allocation (LDA), posits
that words within a document are conditionally
independent given their topic (Blei et al., 2003).
This “bag-of-words” assumption is a common sim-
plification in which word order is ignored, but
one which introduces undesirable properties into
a model meant to serve as an unsupervised ex-
ploratory tool for data analysis.
When an end-user runs a topic model, the output
he or she is often interested in is a list of topical
unigrams, words probable in a topic (hence, repre-
sentative of it). In many situations, such as during
the use of the topic model for the analysis of a new
or ill-understood corpus, these lists can be insuffi-
ciently informative. For instance, if a layperson ran
LDA on the NIPS corpus, he would likely get a topic
whose most prominent words include policy, value,
and reward. Seeing these words isolated from their
context in a list would not be particularly insightful
to the layperson unfamiliar with computer science
research. An alternative to LDA which produced
richer output like policy iteration algorithm, value
function, and model-based reinforcement learning
alongside the unigrams would be much more en-
lightening. Most situations where a topic model is
actually useful for data exploration require a model
whose output is rich enough to dispel the need for
the user’s extensive prior knowledge of the data.
Furthermore, lists of topical unigrams are often
made only marginally interpretable by virtue of their
non-compositionality, the principle that a colloca-
tion’s meaning typically is not derivable from its
constituent words (Schone and Jurafsky, 2001). For
example, the meaning of compact disc as a mu-
sic medium comes from neither the unigram com-
pact nor the unigram disc, but emerges from the bi-
gram as a whole. Moreover, non-compositionality
is topic dependent; compact disc should be inter-
preted as a music medium in a music topic, and as
a small region bounded by a circle in a mathemati-
cal topic. LDA is prone to decompose collocations
into different topics and violate the principle of non-
compositionality, and its unigram lists are harder to
interpret as a result.
</bodyText>
<page confidence="0.982259">
214
</page>
<note confidence="0.879622">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 214–222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999949911764706">
We present an extension of LDA called Phrase-
Discovering LDA (PDLDA) that satisfies two
desiderata: providing rich, interpretable output and
honoring the non-compositionality of collocations.
PDLDA is built in the tradition of the “Topical N-
Gram” (TNG) model of Wang et al. (2007). TNG is
a topic model which satisfies the first desideratum by
producing lists of representative, topically cohesive
n-grams of the form shown in Figure 1. We diverge
from TNG by our addressing the second desidera-
tum, and we do so through a more straightforward
and intuitive definition of what constitutes a phrase
and its topic. In the furtherance of our goals, we
employ a hierarchical method of modeling phrases
that uses dependent Pitman-Yor processes to ame-
liorate overfitting. Pitman-Yor processes have been
successfully used in the past in n-gram (Teh, 2006)
and LDA-based models (Wallach, 2006) for creat-
ing Bayesian language models which exploit word
order, and they prove equally useful in this scenario
of exploiting both word order and topics.
This article is organized as follows: after describ-
ing TNG, we discuss PDLDA and how PDLDA ad-
dresses the limitations of TNG. We then provide de-
tails of our inference procedures and evaluate our
model against competing models on a subset of the
TREC AP corpus (Harman, 1992) in an experi-
ment on human subjects which assesses the inter-
pretability of topical n-gram lists. The experiment
is premised on the notion that topic models should
be evaluated through a real-world task instead of
through information-theoretic measures which often
negatively correlate with topic quality (Chang et al.,
2009).
</bodyText>
<sectionHeader confidence="0.898927" genericHeader="method">
2 Background: LDA and TNG
</sectionHeader>
<bodyText confidence="0.9953056">
LDA represents documents as probabilistic mixtures
of latent topics. Each word w in a corpus w is drawn
from a distribution φ indexed by a topic z, where z is
drawn from a distribution θ indexed by its document
d. The formal definition of LDA is
</bodyText>
<equation confidence="0.987348">
θd — Dirichlet (α) zi  |d, θ — Discrete (θd)
φz — Dirichlet (β) wi  |zi, φ — Discrete (φzi)
</equation>
<bodyText confidence="0.999945294117647">
where θd is document d’s topic distribution, φz is
topic z’s distribution over words, zi is the topic as-
signment of the ith token, and wi is the ith word.
α and β are hyperparameters to the Dirichlet priors.
Here and throughout the article, we use a bold font
for vector notation: for example, z is the vector of all
topic assignments, and its ith entry, zi, corresponds
to the topic assignment of the ith token in the corpus.
TNG extends LDA to model n-grams of arbitrary
length in order to create the kind of rich output for
text mining discussed in the introduction. It does
this by representing a joint distribution P(z, c|w)
where each ci is a Boolean variable that signals the
start of a new n-gram beginning at the ith token. c
partitions a corpus into consecutive non-overlapping
n-grams of various lengths. Formally, TNG differs
from LDA by the distributional assumptions
</bodyText>
<equation confidence="0.976039">
wi  |wi−1, zi, ci = 1, φ — Discrete(φzi)
wi  |wi−1, zi, ci = 0, σ — Discrete(σziwi−1)
ci  |wi−1, zi−1, π — Bernoulli(πzi−1wi−1)
</equation>
<bodyText confidence="0.9999819">
where the new distributions πzw and σzw are en-
dowed with conjugate prior distributions: πzw —
Beta(λ) and σzw — Dirichlet(δ). When ci = 0,
word wi is joined into a topic-specific bigram with
wi−1. When ci = 1, wi is drawn from a topic-
specific unigram distribution and is the start of a new
n-gram.
An unusual feature of TNG is that words within
a topical n-gram, a sequence of words delineated
by c, do not share the same topic. To compen-
sate for this after running a Gibbs sampler, Wang
et al. (2007) analyze each topical n-gram post hoc
as if the topic of the final word in the n-gram was
the topic assignment of the entire n-gram. Though
this design simplifies inference, we perceive it as a
shortcoming since the aforementioned principle of
non-compositionality supports the intuitive idea that
each collocation ought to be drawn from a single
topic. Another potential drawback of TNG is that
the topic-specific bigram distributions σzw share no
probability mass between each other or with the un-
igram distributions φz. Hence, observing a bigram
under one topic does not make it more likely under
another topic or make its constituent unigrams more
probable. To be more concrete, in TNG, observing
space shuttle under a topic z (or under two topics,
one for each word) regrettably does not make space
shuttle more likely under a topic z&apos; =� z, nor does it
make observing shuttle more likely under any topic.
Smoothing, the sharing of probability mass between
</bodyText>
<page confidence="0.995405">
215
</page>
<figure confidence="0.960315566666667">
water
air
temperature
heat
liquid
gas
gases
hot
pressure
atmosphere
warm
cold
surface
oxygen
clouds
water vapor
air pollution
air pressure
warm air
cold water
earth&apos;s surface
room temperature
boiling point
drinking water
atmospheric pressure
cold war
high temperatures
liquid water
cold air
warm water
</figure>
<figureCaption confidence="0.923041857142857">
water vapor condenses
warm air rises
cold air mass
called water vapor
water vapor changes
process takes place
warm air mass
</figureCaption>
<bodyText confidence="0.961194375">
clean air act
gas called water vapor
dry spell holds
air pressure inside
sewage treatment plant
air pollution laws
high melting points
high melting point
</bodyText>
<figure confidence="0.966298428571429">
(e) Topic 5
matter
atoms
elements
electrons
atom
molecules
form
oxygen
hydrogen
particles
element
solution
substance
reaction
nucleus
chemical reactions
atomic number
hydrogen atoms
hydrogen atom
periodic table
chemical change
physical properties
chemical reaction
water molecules
sodium chloride
small amounts
positive charge
carbon atoms
physical change
chemical properties
like charges repel
positively charged nucleus
unlike charges attract
outer energy level
</figure>
<figureCaption confidence="0.941768090909091">
reaction takes place
negatively charged electrons
chemical change takes place
form new substances
physical change takes place
form sodium chloride
modern atomic theory
electrically charged particles
increasing atomic number
second ionization energies
higher energy levels
</figureCaption>
<figure confidence="0.952550096774193">
(a) Topic 1
words
word
sentence
write
writing
paragraph
sentences
meaning
use
subject
language
read
example
verb
topic
main idea
topic sentence
english language
following paragraph
words like
quotation marks
direct object
word processing
sentence tells
figurative language
writing process
following sentences
subject matter
standard english
use words
</figure>
<bodyText confidence="0.872514266666667">
word processing center
word processing systems
word processing equipment
speak different languages
use quotation marks
single main idea
use words like
topic sentence states
present perfect tense
express complete thoughts
word processing software
use formal english
standard american english
collective noun refers
formal standard english
</bodyText>
<listItem confidence="0.505913">
(c) Topic 3
</listItem>
<bodyText confidence="0.910046222222222">
china
africa
india
europe
people
chinese
asia
egypt
world
rome
land
east
trade
countries
empire
middle east
western europe
north africa
mediterranean sea
years ago
roman empire
far east
southeast asia
west africa
saudi arabia
capital letter
asia minor
united states
capital city
centuries ago
2000 years ago
east india company
eastern united states
4000 years ago
southwestern united states
middle atlantic states
northeastern united states
western united states
southeastern united states
200 years ago
middle atlantic region
indus river valley
western roman empire
british north america act
coast guard station
</bodyText>
<figure confidence="0.99950034375">
(f) Topic 6
president
congress
vote
party
constitution
state
members
office
government
states
elected
representatives
senate
house
washington
supreme court
new york
democratic party
vice president
political parties
national government
executive branch
civil rights
new government
political party
andrewjackson
chief justice
federal government
state legislatures
public opinion
civil rights act
</figure>
<figureCaption confidence="0.816225285714286">
civil rights movement
supreme court ruled
president theodore roosevel
second continental congress
equal rights amendment
strong central government
sherman antitrust act
civil rights legislation
public opinion polls
major political parties
congress shall make
federal district court
supreme court decisions
american foreign policy
</figureCaption>
<figure confidence="0.999663322580645">
(b) Topic 2
energy
used
oil
heat
coal
use
fuel
produce
power
source
light
electricity
burn
gas
gasoline
natural resources
natural gas
heat energy
iron ore
carbon dioxide
potential energy
solar energy
light energy
fossil fuels
hot water
steam engine
large amounts
sun&apos;s energy
radiant energy
nuclear energy
</figure>
<figureCaption confidence="0.903482333333333">
nuclear power plants
nuclear power plant
important natural resources
electric power plants
called fossil fuels
important natural resource
produce large amounts
called solar energy
electric light bulb
use electrical energy
use solar energy
carbon dioxide gas
called potential energy
gas called carbon dioxide
called crude oil
</figureCaption>
<figure confidence="0.984971">
(d) Topic 4
</figure>
<figureCaption confidence="0.99901">
Figure 1: Six out of one hundred topics found by our model, PDLDA, on the Touchstone Applied Science
</figureCaption>
<bodyText confidence="0.833535333333333">
Associates (TASA) corpus (Landauer and Dumais, 1997). Each column within a box shows the top fifteen
phrases for a topic and is restricted to phrases of a minimum length of one, two, or three words, respectively.
The rows are ordered by likelihood.
</bodyText>
<page confidence="0.997616">
216
</page>
<figureCaption confidence="0.99948">
Figure 2: PDLDA drawn in plate notation.
</figureCaption>
<bodyText confidence="0.999858375">
contexts, is desirable so that a model like this does
not need to independently infer the probability of
every bigram under every topic. The advantages of
smoothing are especially pronounced for small cor-
pora or for a large number of topics. In these sit-
uations, the observed number of bigrams in a given
topic will necessarily be very small and thus not sup-
port strong inferences.
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="method">
3 PDLDA
</sectionHeader>
<bodyText confidence="0.9997051875">
A more natural definition of a topical phrase, one
which meets our second desideratum, is to have each
phrase possess a single topic. We adopt this in-
tuitive idea in PDLDA. It can also be understood
through the lens of Bayesian changepoint detection.
Changepoint detection is used in time series mod-
els in which the generative parameters periodically
change abruptly (Adams and MacKay, 2007). View-
ing a sentence as a time series of words, we posit that
the generative parameter, the topic, changes period-
ically in accordance with the changepoint indicators
c. Because there is no restriction on the number of
words between changepoints, topical phrases can be
arbitrarily long but will always have a single topic
drawn from Bd.
The full definition of PDLDA is given by
</bodyText>
<equation confidence="0.9924137">
wi  |u — Discrete(Gu)
Gu — PYP(a|u|, b|u|, G,r(u))
G∅ — PYP(a0, b0, H)
�
Szi−1 if ci = 0
Discrete (Bd) if ci = 1
ci  |wi−1, zi−1, 7r — Bernoulli (7rwi−1zi−1)
with the prior distriutions over the parameters as
Bd — Dirichlet (α) 7rzw — Beta (A)
a|u |— Beta (p) b|u |— Gamma (E)
</equation>
<bodyText confidence="0.999508333333333">
Like TNG, PDLDA assumes that the probability
of a changepoint ci+1 after the ith token depends on
the current topic zi and word wi. This causes the
length of a phrase to depend on its topic and con-
stituent words. The changepoints explicitly model
which words tend to start and end phrases in each
document. Depending on ci, zi is either set deter-
ministically to the preceding topic (when ci = 0)
or is drawn anew from Bd (when ci = 1). In this
way, each topical phrase has a single topic drawn
from its document’s topic distribution. As in TNG,
the parameters 7rzw and Bd are given conjugate priors
parameterized by A and α.
Let u be a context vector consisting of the
phrase topic and the past m words: u °_ &lt;
zi, wi−1, wi−2, ... , wi−m &gt;. The operator 7r(u) de-
notes the prefix of u, the vector with the rightmost
element of u removed. |u |denotes the length of u,
and 0 represents an empty context. For practical rea-
sons, we pad u with a special start symbol when the
context overlaps a phrase boundary. For example,
the first word wi of a phrase beginning at a position
i necessarily has ci = 1; consequently, all the pre-
ceding words wi−j in the context vector are treated
as start symbols so that wi is effectively drawn from
a topic-specific unigram distribution.
In PDLDA, each token is drawn from a distribu-
tion conditioned on its context u. When m = 1,
this conditioning is analogous to TNG’s word dis-
tribution. However, in contrast with TNG, the word
</bodyText>
<figure confidence="0.9994397">
...
... z
i-1
...
X
wwi w
i-1 i+1
c
i
π
V
z
i
α
θ
T
c
i+1
G
P E
a b
z
i+1
u
...
...
...
|u|
D
zi  |d, zi−1, Bd, ci —
</figure>
<page confidence="0.857267">
217
</page>
<figureCaption confidence="0.993673">
Figure 3: Illustration of the hierarchical Pitman-Yor
</figureCaption>
<bodyText confidence="0.9937964">
process for a toy two-word vocabulary V = {honda,
civic} and two-topic (T = 2) model with m = 1.
Each node G in the tree is a Pitman-Yor process
whose base distribution is its parent node, and H is a
uniform distribution over V . When, for example, the
context is u = z1 : honda, the darkened path is fol-
lowed and the probability of the next word is calcu-
lated from the shaded node using Equation 1, which
combines predictions from all the nodes along the
darkened path.
distributions used are Pitman-Yor processes (PYPs)
linked together into a tree structure. This hierar-
chical construction creates the desired smoothing
among different contexts. The next section explains
this hierarchical distribution in more detail.
</bodyText>
<subsectionHeader confidence="0.99976">
3.1 Hierarchical Pitman-Yor process
</subsectionHeader>
<bodyText confidence="0.999954854166667">
Words in PDLDA are emitted from Gu, which has
a PYP prior (Pitman and Yor, 1997). PYPs are a
generalization of the Dirichlet Process, with the ad-
dition of a discount parameter 0 &lt; a &lt; 1. When
considering the distribution of a sequence of words
w drawn iid from a PYP-distributed G, one can an-
alytically marginalize G and consider the resulting
conditional distribution of w given its parameters a,
b, and base distribution φ. This marginal can best
be understood by considering the distribution of any
wi|w1, ... , wi−1, a, b, φ, which is characterized by
a generative process known as the generalized Chi-
nese Restaurant Process (CRP) (Pitman, 2002). In
the CRP metaphor, one imagines a restaurant with
an unbounded number of tables, where each table
has one shared dish (a draw from φ) and can seat an
unlimited number of customers. The CRP specifies a
process by which customers entering the restaurant
choose a table to sit at and, consequently, the dish
they eat. The first customer to arrive always sits at
the first table. Subsequent customers sit at an occu-
pied table k with probability proportional to ck − a
and choose a new unoccupied table with probabil-
ity proportional to b + ta, where ck is the number
of customers seated at table k and t is the number
of occupied tables in G. For our language modeling
purposes, “customers” are word tokens and “dishes”
are word types.
The hierarchical PYP (HPYP) is an intuitive re-
cursive formulation of the PYP in which the base
distribution φ is itself PYP-distributed. Figure 3
demonstrates this principle as applied to PDLDA.
The hierarchy forms a tree structure, where leaves
are restaurants corresponding to full contexts and in-
ternal nodes correspond to partial contexts. An edge
between a parent and child node represents a depen-
dency of the child on the parent, where the base dis-
tribution of the child node is its parent. This smooths
each context’s distribution like the Bayesian n-gram
model of Teh (2006), which is a Bayesian version
of interpolated Kneser-Ney smoothing (Chen and
Goodman, 1998). One ramification of this setup
is that if a word occurs in a context u, the shar-
ing makes it more likely in other contexts that have
something in common with u, such as a shared topic
or word.
The HPYP gives the following probability for a
word following the context u being w:
</bodyText>
<equation confidence="0.988317">
cuw· − a|u|tuw
Pu(w  |τ, a, b) = +
b|u |+ cu··
b|u |+ a|u|tu· Pπ(u)(w  |τ, a, b) (1)
b|u |+ cu··
</equation>
<bodyText confidence="0.999836909090909">
where Pπ(∅)(w|τ, a, b) = G∅(w), cuw· is the num-
ber of customers eating dish w in restaurant u, and
tuw is the number of tables serving w in restau-
rant u, and τ represents the current seating arrange-
ment. Here and throughout the rest of the paper, we
use a dot to indicate marginal counts: e.g., cuw· =
Ek cuwk where cuwk is the number of customers
eating w in u at table k. The base distribution of
G∅ was chosen to be uniform: H(w) = 1/V with V
being the vocabulary size. The above equation an in-
terpolation between distributions of context lengths
</bodyText>
<figure confidence="0.9984236">
GØ
z1
z2
civic
civic
honda
honda
H
Gz1
G
z2
Gz1:civic
Gz1:honda
Gz2:civic
Gz2:honda
</figure>
<page confidence="0.99696">
218
</page>
<bodyText confidence="0.996519">
|u|, |u |− 1, ... 0 and realizes the sharing of statisti-
cal strength between different contexts.
</bodyText>
<subsectionHeader confidence="0.983954">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.999950916666667">
In this section, we describe Markov chain Monte
Carlo procedures to sample from P (z, c, τ|w, U),
the posterior distribution over topic assignments z,
phrase boundaries c, and seating arrangements τ
given an observed corpus w. Let U be short-
hand for α, λ, a, b. In order to draw samples
from P (z, c, τ|w, U), we employ a Metropolis-
Hastings sampler for approximate inference. The
sampler we use is a collapsed sampler (Griffiths and
Steyvers, 2004), wherein θ, φ, and G are analyti-
cally marginalized. Because we marginalize each G,
we use the Chinese Restaurant Franchise representa-
tion of the hierarchical PYPs (Teh, 2006). However,
rather than onerously storing the table assignment
of every token in w, we store only the counts of how
many tables there are in a restaurant and how many
customers are sitting at each table in that restaurant.
We refer the inquisitive reader to the appendix of
Teh (2006) for further details of this procedure.
Our sampling strategy for a given token i in doc-
ument d is to jointly propose changes to the change-
point ci and topic assignment zi, and then to the
seating arrangement τ. Recall that according to the
model, if ci = 0, zi = zi−1; otherwise zi is gen-
erated from the topic distribution for document d.
Since the topic assignment remains the same until a
new changepoint at a position i0 is reached, each to-
ken wj for j from position i until i0 − 1 will depend
on zi because for these j, zj = zi. We call this set of
tokens the phrase suffix of the ith token and denote
it s(i). More formally, let s(i) be the maximal set
of continuous indices j &gt; i including i such that, if
j =� i, cj = 0. That is, s(i) are the indices compris-
ing the remainder of the phrase beginning at position
i. In addition, let x(i) indicate the extended suffix
version of s(i) which includes one additional index:
x(i)°_ {s(i) U {max (s(i)) + 1}}. In addition to
the words in the suffix s(i), the changepoint indica-
tor variables cj for j in x(i) are also conditioned on
zi. To make these dependencies more explicit, we
refer to zs(i) °= zj bj E s(i), which are constrained
by the model to share a topic.
The variables that depend directly on zi, ci are
zs(i), ws(i), cx(i). The proposal distribution first
draws from a multinomial over T + 1 options: one
option for ci = 0, zi = zi − 1; and one for ci = 1
paired with each possible zi = z E 1... T. This is
given by
</bodyText>
<equation confidence="0.998268416666667">
P(zs(i), ci  |z¬s(i), c¬i, τ¬s(i), w, U) a
n¬x(j)
zj−1wj−1cj + λcj
n¬x(j)
zj−1wj−1· + λ0 + λ1
Y P(zj  |c, z¬s(j), U) Puj(wj  |τ¬s(i), U)
j∈s(i)
with
n¬s(j) + α
dzj
P(zj  |c, z¬s(j), U) =
δzj,zj−1 if cj = 0
</equation>
<bodyText confidence="0.998807529411765">
where Puj(wj  |τ¬s(i), U) is given by Equation 1,
T is the number of topics, n¬s(j)
dz is the number of
phrases in document d that have topic z when s(j)’s
assignment is excluded, and n¬s(j)
zwc is the number of
times a changepoint c has followed a word w with
topic z when s(j)’s assignments are excluded.
After drawing a proposal for ci, zs(i) for token i,
the sampler adds a customer eating wi to a table
serving wi in restaurant ui. An old table k is se-
lected with probability a max(0, cuwk − a|u|) and
a new table is selected with probability a (b|ui |+
a|ui|tui·)Pπ(u)(wi).
Let z0s(i), c0i, τ0s(i) denote the proposed change to
zs(i), ci, τs(i). We accept the proposal with probabil-
ity min(A,1) where
</bodyText>
<equation confidence="0.7997335">
A = P�(z0s(i), c0i, τ0s(i)) Q(zs(i), ci, τs(i))
P�(zs(i), ci, τs(i)) Q(z0s(i),c0i,τ0s(i))
</equation>
<bodyText confidence="0.999980416666667">
where Q is the proposal distribution and P� is the
true unnormalized distribution. P� differs from Q in
that the probability of each word wj and the seating
arrangement depends only on -,s(j), as opposed to
the simplification of using -,s(i). Almost all propos-
als are accepted; hence, this theoretically motivated
Metropolis Hastings correction step makes little dif-
ference in practice.
Because the parameters a and b have no intuitive
interpretation and we lack any strong belief about
what they should be, we give them vague priors
where ρ1 = ρ2 = 1 and E1 = 10, E2 = .1. We then
</bodyText>
<equation confidence="0.83576275">
Y
j∈x(i)
⎧
⎨⎪⎪
⎪⎪⎩
n¬s(j)
d· + Tα
ifcj=1
</equation>
<page confidence="0.997791">
219
</page>
<bodyText confidence="0.9998758">
interleave a slice sampling algorithm (Neal, 2000)
between sweeps of the Metropolis-Hastings sampler
to learn these parameters. We chose not to do infer-
ence on α in order to make the tests of our model
against TNG more equitable.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999968">
An integral part of modeling topical phrases is the
relaxation of the bag-of-words assumption in LDA.
There are many models that make this relaxation.
Among them, Griffiths and Steyvers (2005) present
a model in which words are generated either con-
ditioned on a topic or conditioned on the previous
word in a bigram, but not both. They use this to
model human performance on a word-association
task. Wallach (2006) experiments with incorpo-
rating LDA into a bigram language model. Her
model uses a hierarchical Dirichlet to share param-
eters across bigrams in a topic in a manner similar
to our use of PYPs, but it lacks a notion of the topic
being shared between the words in an n-gram. The
Hidden Topic Markov Model (HTMM) (Gruber et
al., 2007) assumes that all words in a sentence have
the same topic, and consecutive sentences are likely
to have the same topic. By dropping the indepen-
dence assumption among topics, HTMM is able to
achieve lower perplexity scores than LDA at mini-
mal additional computational costs. These models
are unconcerned with topical n-grams and thus do
not model phrases.
Johnson (2010) presents an Adaptor Grammar
model of topical phrases. Adaptor Grammars are
a framework for specifying nonparametric Bayesian
models over context-free grammars in which certain
subtrees are “memoized” or remembered for reuse.
In Johnson’s model, subtrees corresponding to com-
mon phrases for a topic are memoized, resulting in a
model in which each topic is associated with a distri-
bution over whole phrases. While it is a theoretically
elegant method for finding topical phrases, for large
corpora we found inference to be impractically slow.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="method">
5 Phrase Intrusion Experiment
</sectionHeader>
<bodyText confidence="0.804932071428571">
Perplexity is the typical information theoretic mea-
sure of language model quality used in lieu of ex-
trinsic measures, which are more difficult and costly
to run. However, it is well known that perplexity
Trial 1 of 80 Trial 3 of 80
countries fda
britain book
france smoking
museum cigarettes
Trial 2 of 80 Trial 4 of 80
air force roman catholic church
beverly hills air traffic controllers
defense minister roman catholic priest
u.s. troops roman catholic bishop
</bodyText>
<figureCaption confidence="0.764296333333333">
Figure 4: Experimental setup of the phrase intrusion
experiment in which subjects must click on the n-
gram that does not belong.
</figureCaption>
<bodyText confidence="0.999497">
scores may negatively correlate with actual quality
as assessed by humans (Chang et al., 2009). With
that fact in mind, we expanded the methodology of
Chang et al. (2009) to create a “phrase intrusion”
task that quantitatively compares the quality of the
topical n-gram lists produced by our model against
those of other models.
Each of 48 subjects underwent 80 trials of a web-
based experiment on Amazon Mechanical Turk, a
reliable (Paolacci et al., 2010) and increasingly com-
mon venue for conducting online experiments. In
each trial, a subject is presented with a randomly or-
dered list of four n-grams (cf. Figure 4). Each sub-
ject’s task is to select the intruder phrase, a spurious
n-gram not belonging with the others in the list. If,
other than the intruder, the items in the list are all
on the same topic, then subjects can easily identify
the intruder because the list is semantically cohesive
and makes sense. If the list is incohesive and has no
discernible topic, subjects must guess arbitrarily and
performance is at random.
To construct each trial’s list, we chose two top-
ics z and z&apos; (z 7� z&apos;), then selected the three most
probable n-grams from z and the intruder phrase, an
n-gram probable in z&apos; and improbable in z. This
design ensures that the intruder is not identifiable
due solely to its being rare. Interspersed among the
phrase intrusion trials were several simple screen-
ing trials intended to affirm that subjects possessed
a minimal level of attentiveness and reading com-
prehension. For example, one such screening trial
presented subjects with the list banana, apple, tele-
vision, orange. Subjects who got any of these trials
</bodyText>
<page confidence="0.97493">
220
</page>
<figure confidence="0.999776954545455">
Model Precision
0.8
0.6
0.4
0.2
0
1
Unigrams Bigrams Trigrams
PDLDA
TNG
LDA
Model Precision
0.8
0.6
0.4
0.2
0
1
Bigrams Trigrams
PDLDA
TNG
(a) Word repetition allowed within a list. (b) Word repetition not allowed.
</figure>
<figureCaption confidence="0.986351">
Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model.
Excluding trials with repeated words does not qualitatively affect the results.
</figureCaption>
<bodyText confidence="0.972965272727273">
wrong were excluded from our analyses.
Each subject was presented with trials constructed
from the output of PDLDA and TNG for unigrams,
bigrams, and trigrams. For unigrams, we also tested
the output of the original smoothed LDA (Blei et
al., 2003). The experiment was conducted twice for
a 2,246-document subset of the TREC AP corpus
(Blei et al., 2003; Harman, 1992): the first time pro-
ceeded as described above, but the second time did
not allow word repetition within a topic’s list. The
topical phrases found by TNG and PDLDA often
revolve around a central n-gram, with other words
pre- or post- appended to it. In this intrusion exper-
iment, any n-gram not containing the central word
or phrase may be trivially identifiable, regardless of
its relevance to the topic. For example, the intruder
in Trial 4 of Figure 4 is easily identifiable even if
a subject does not understand English. This second
experiment was designed to test whether our conclu-
sions hinge on word repetition.
We used the MALLET toolbox (McCallum,
2002) for the implementations of LDA and TNG.
Each model was run with 100 topics for 5,000 it-
erations. We set m = 2, a = .01, Q = .01, A = 1,
Tr1 = 7r2 = 1, p1 = 10, and p2 = .1. For all mod-
els, we treated certain punctuation as the start of a
phrase by setting cj = 1 for all tokens j immediately
following periods, commas, semicolons, and excla-
mation and question marks. To reduce runtime, we
removed stopwords occuring in the MALLET tool-
box’s stopword list. Because TNG and LDA had
trouble with single character words not in the sto-
plist, we manually removed them before the experi-
ment. Any token immediately following a removed
word was treated as if it were the start of a phrase.
As in Chang et al. (2009), performance is mea-
sured via model precision, the fraction of subjects
agreeing with the model. It is defined as MPm,n
k =
✶(im,n
k,s = �m,n
k,s )/5 where �m,n
k,s is the index of
the intruding n-gram for subject s among the words
generated from the kth topic of model m, im,n
k,s is the
intruder selected by s, and 5 is the number of sub-
jects. The model precisions are shown in Figure 5.
PDLDA achieves the highest precision in all condi-
tions. Model precision is low in all models, which is
a reflection of how challenging the task is on a small
corpus laden with proper nouns and low-frequency
words. Figure 5b demonstrates that the outcome of
the experiment does not depend strongly on whether
the topical n-gram lists have repeated words.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9993535">
We presented a topic model which simultaneously
segments a corpus into phrases of varying lengths
and assigns topics to them. The topical phrases
found by PDLDA are much richer sources of in-
formation than the topical unigrams typically pro-
duced in topic modeling. As evidenced by the
phrase-intrusion experiment, the topical n-gram lists
that PDLDA finds are much more interpretable than
</bodyText>
<figure confidence="0.8671215">
�
s
</figure>
<page confidence="0.993248">
221
</page>
<bodyText confidence="0.999826">
those found by TNG.
The formalism of Bayesian changepoint detection
arose naturally from the intuitive assumption that the
topic of a sequence of tokens changes periodically,
and that the tokens in between changepoints com-
prise a phrase. This formalism provides a principled
way to discover phrases within the LDA framework.
We presented a model embodying these principles
and showed how to incorporate dependent Pitman-
Yor processes into it.
</bodyText>
<sectionHeader confidence="0.996376" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999394">
The first author is supported by an NSF Graduate
Research Fellowship. The first and second authors
began this project while working at J.D. Power &amp;
Associates. We are indebted to Michael Mozer, Matt
Wilder, and Nicolas Nicolov for their advice.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754532467532">
Ryan Prescott Adams and David J.C. MacKay. 2007.
Bayesian online changepoint detection. Technical re-
port, University of Cambridge, Cambridge, UK.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation. Jour-
nal of Machine Learning Research, 3:993–1022.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems (NIPS).
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537–544. MIT Press.
Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark
Steyvers. 2007. Topics in semantic representation.
Psychological Review, 114:211–244.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic Markov models. Journal of Machine
Learning Research - Proceedings Track, 2:163–170.
Donna Harman. 1992. Overview of the first text re-
trieval conference (trec–1). In Proceedings of the
first Text REtrieval Conference (TREC–1), Washing-
ton DC, USA.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148–1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211 – 240.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705–767.
Gabriele Paolacci, Jesse Chandler, and Panagiotis G.
Ipeirotis. 2010. Running experiments on Amazon
Mechanical Turk. Judgment and Decision Making,
5(5):411–419.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855–900.
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Lillian Lee and
Donna Harman, editors, Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Language
Processing, pages 100–108.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 985–992, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, pages 977–984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing.
</reference>
<page confidence="0.997967">
222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.600760">
<title confidence="0.999926">A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</title>
<author confidence="0.99998">V Robert</author>
<affiliation confidence="0.999962">University of Colorado,</affiliation>
<email confidence="0.999866">robert.lindsey@colorado.edu</email>
<author confidence="0.999808">William P Headden</author>
<affiliation confidence="0.626745">Two Cassowaries</affiliation>
<email confidence="0.998005">headdenw@twocassowaries.com</email>
<author confidence="0.999977">J Michael</author>
<affiliation confidence="0.999668">Google Inc.</affiliation>
<email confidence="0.999481">stip@google.com</email>
<abstract confidence="0.9980465">Topic models traditionally rely on the bagof-words assumption. In data mining applications, this often results in end-users being with inscrutable lists of unsingle words inferred as representative of their topics. In this article, we present a hierarchical generative probabilistic model The model simultaneously infers the location, length, and topic of phrases within a corpus and relaxes the bagof-words assumption within phrases by using a hierarchy of Pitman-Yor processes. We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters. We show via an experiment on human subjects that our model finds substantially better, more interpretable topical phrases than do competing models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ryan Prescott Adams</author>
<author>David J C MacKay</author>
</authors>
<title>Bayesian online changepoint detection.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>University of Cambridge,</institution>
<location>Cambridge, UK.</location>
<contexts>
<context position="13256" citStr="Adams and MacKay, 2007" startWordPosition="2081" endWordPosition="2084">othing are especially pronounced for small corpora or for a large number of topics. In these situations, the observed number of bigrams in a given topic will necessarily be very small and thus not support strong inferences. 3 PDLDA A more natural definition of a topical phrase, one which meets our second desideratum, is to have each phrase possess a single topic. We adopt this intuitive idea in PDLDA. It can also be understood through the lens of Bayesian changepoint detection. Changepoint detection is used in time series models in which the generative parameters periodically change abruptly (Adams and MacKay, 2007). Viewing a sentence as a time series of words, we posit that the generative parameter, the topic, changes periodically in accordance with the changepoint indicators c. Because there is no restriction on the number of words between changepoints, topical phrases can be arbitrarily long but will always have a single topic drawn from Bd. The full definition of PDLDA is given by wi |u — Discrete(Gu) Gu — PYP(a|u|, b|u|, G,r(u)) G∅ — PYP(a0, b0, H) � Szi−1 if ci = 0 Discrete (Bd) if ci = 1 ci |wi−1, zi−1, 7r — Bernoulli (7rwi−1zi−1) with the prior distriutions over the parameters as Bd — Dirichlet </context>
</contexts>
<marker>Adams, MacKay, 2007</marker>
<rawString>Ryan Prescott Adams and David J.C. MacKay. 2007. Bayesian online changepoint detection. Technical report, University of Cambridge, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1357" citStr="Blei et al., 2003" startWordPosition="191" endWordPosition="194">umption within phrases by using a hierarchy of Pitman-Yor processes. We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters. We show via an experiment on human subjects that our model finds substantially better, more interpretable topical phrases than do competing models. 1 Introduction Probabilistic topic models have been the focus of intense study in recent years. The archetypal topic model, Latent Dirichlet Allocation (LDA), posits that words within a document are conditionally independent given their topic (Blei et al., 2003). This “bag-of-words” assumption is a common simplification in which word order is ignored, but one which introduces undesirable properties into a model meant to serve as an unsupervised exploratory tool for data analysis. When an end-user runs a topic model, the output he or she is often interested in is a list of topical unigrams, words probable in a topic (hence, representative of it). In many situations, such as during the use of the topic model for the analysis of a new or ill-understood corpus, these lists can be insufficiently informative. For instance, if a layperson ran LDA on the NIP</context>
<context position="28322" citStr="Blei et al., 2003" startWordPosition="4773" endWordPosition="4776">Unigrams Bigrams Trigrams PDLDA TNG LDA Model Precision 0.8 0.6 0.4 0.2 0 1 Bigrams Trigrams PDLDA TNG (a) Word repetition allowed within a list. (b) Word repetition not allowed. Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model. Excluding trials with repeated words does not qualitatively affect the results. wrong were excluded from our analyses. Each subject was presented with trials constructed from the output of PDLDA and TNG for unigrams, bigrams, and trigrams. For unigrams, we also tested the output of the original smoothed LDA (Blei et al., 2003). The experiment was conducted twice for a 2,246-document subset of the TREC AP corpus (Blei et al., 2003; Harman, 1992): the first time proceeded as described above, but the second time did not allow word repetition within a topic’s list. The topical phrases found by TNG and PDLDA often revolve around a central n-gram, with other words pre- or post- appended to it. In this intrusion experiment, any n-gram not containing the central word or phrase may be trivially identifiable, regardless of its relevance to the topic. For example, the intruder in Trial 4 of Figure 4 is easily identifiable eve</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="5246" citStr="Chang et al., 2009" startWordPosition="815" endWordPosition="818">ord order and topics. This article is organized as follows: after describing TNG, we discuss PDLDA and how PDLDA addresses the limitations of TNG. We then provide details of our inference procedures and evaluate our model against competing models on a subset of the TREC AP corpus (Harman, 1992) in an experiment on human subjects which assesses the interpretability of topical n-gram lists. The experiment is premised on the notion that topic models should be evaluated through a real-world task instead of through information-theoretic measures which often negatively correlate with topic quality (Chang et al., 2009). 2 Background: LDA and TNG LDA represents documents as probabilistic mixtures of latent topics. Each word w in a corpus w is drawn from a distribution φ indexed by a topic z, where z is drawn from a distribution θ indexed by its document d. The formal definition of LDA is θd — Dirichlet (α) zi |d, θ — Discrete (θd) φz — Dirichlet (β) wi |zi, φ — Discrete (φzi) where θd is document d’s topic distribution, φz is topic z’s distribution over words, zi is the topic assignment of the ith token, and wi is the ith word. α and β are hyperparameters to the Dirichlet priors. Here and throughout the arti</context>
<context position="26107" citStr="Chang et al., 2009" startWordPosition="4399" endWordPosition="4402">e model quality used in lieu of extrinsic measures, which are more difficult and costly to run. However, it is well known that perplexity Trial 1 of 80 Trial 3 of 80 countries fda britain book france smoking museum cigarettes Trial 2 of 80 Trial 4 of 80 air force roman catholic church beverly hills air traffic controllers defense minister roman catholic priest u.s. troops roman catholic bishop Figure 4: Experimental setup of the phrase intrusion experiment in which subjects must click on the ngram that does not belong. scores may negatively correlate with actual quality as assessed by humans (Chang et al., 2009). With that fact in mind, we expanded the methodology of Chang et al. (2009) to create a “phrase intrusion” task that quantitatively compares the quality of the topical n-gram lists produced by our model against those of other models. Each of 48 subjects underwent 80 trials of a webbased experiment on Amazon Mechanical Turk, a reliable (Paolacci et al., 2010) and increasingly common venue for conducting online experiments. In each trial, a subject is presented with a randomly ordered list of four n-grams (cf. Figure 4). Each subject’s task is to select the intruder phrase, a spurious n-gram no</context>
<context position="29809" citStr="Chang et al. (2009)" startWordPosition="5044" endWordPosition="5047">terations. We set m = 2, a = .01, Q = .01, A = 1, Tr1 = 7r2 = 1, p1 = 10, and p2 = .1. For all models, we treated certain punctuation as the start of a phrase by setting cj = 1 for all tokens j immediately following periods, commas, semicolons, and exclamation and question marks. To reduce runtime, we removed stopwords occuring in the MALLET toolbox’s stopword list. Because TNG and LDA had trouble with single character words not in the stoplist, we manually removed them before the experiment. Any token immediately following a removed word was treated as if it were the start of a phrase. As in Chang et al. (2009), performance is measured via model precision, the fraction of subjects agreeing with the model. It is defined as MPm,n k = ✶(im,n k,s = �m,n k,s )/5 where �m,n k,s is the index of the intruding n-gram for subject s among the words generated from the kth topic of model m, im,n k,s is the intruder selected by s, and 5 is the number of subjects. The model precisions are shown in Figure 5. PDLDA achieves the highest precision in all conditions. Model precision is low in all models, which is a reflection of how challenging the task is on a small corpus laden with proper nouns and low-frequency wor</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context position="18357" citStr="Chen and Goodman, 1998" startWordPosition="3003" endWordPosition="3006">ive formulation of the PYP in which the base distribution φ is itself PYP-distributed. Figure 3 demonstrates this principle as applied to PDLDA. The hierarchy forms a tree structure, where leaves are restaurants corresponding to full contexts and internal nodes correspond to partial contexts. An edge between a parent and child node represents a dependency of the child on the parent, where the base distribution of the child node is its parent. This smooths each context’s distribution like the Bayesian n-gram model of Teh (2006), which is a Bayesian version of interpolated Kneser-Ney smoothing (Chen and Goodman, 1998). One ramification of this setup is that if a word occurs in a context u, the sharing makes it more likely in other contexts that have something in common with u, such as a shared topic or word. The HPYP gives the following probability for a word following the context u being w: cuw· − a|u|tuw Pu(w |τ, a, b) = + b|u |+ cu·· b|u |+ a|u|tu· Pπ(u)(w |τ, a, b) (1) b|u |+ cu·· where Pπ(∅)(w|τ, a, b) = G∅(w), cuw· is the number of customers eating dish w in restaurant u, and tuw is the number of tables serving w in restaurant u, and τ represents the current seating arrangement. Here and throughout t</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="19929" citStr="Griffiths and Steyvers, 2004" startWordPosition="3299" endWordPosition="3302">ic honda honda H Gz1 G z2 Gz1:civic Gz1:honda Gz2:civic Gz2:honda 218 |u|, |u |− 1, ... 0 and realizes the sharing of statistical strength between different contexts. 3.2 Inference In this section, we describe Markov chain Monte Carlo procedures to sample from P (z, c, τ|w, U), the posterior distribution over topic assignments z, phrase boundaries c, and seating arrangements τ given an observed corpus w. Let U be shorthand for α, λ, a, b. In order to draw samples from P (z, c, τ|w, U), we employ a MetropolisHastings sampler for approximate inference. The sampler we use is a collapsed sampler (Griffiths and Steyvers, 2004), wherein θ, φ, and G are analytically marginalized. Because we marginalize each G, we use the Chinese Restaurant Franchise representation of the hierarchical PYPs (Teh, 2006). However, rather than onerously storing the table assignment of every token in w, we store only the counts of how many tables there are in a restaurant and how many customers are sitting at each table in that restaurant. We refer the inquisitive reader to the appendix of Teh (2006) for further details of this procedure. Our sampling strategy for a given token i in document d is to jointly propose changes to the changepoi</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Joshua B Tenenbaum</author>
<author>Mark Steyvers</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<pages>114--211</pages>
<marker>Griffiths, Tenenbaum, Steyvers, 2007</marker>
<rawString>Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark Steyvers. 2007. Topics in semantic representation. Psychological Review, 114:211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Yair Weiss</author>
<author>Michal Rosen-Zvi</author>
</authors>
<title>Hidden topic Markov models.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<pages>2--163</pages>
<contexts>
<context position="24480" citStr="Gruber et al., 2007" startWordPosition="4137" endWordPosition="4140">that make this relaxation. Among them, Griffiths and Steyvers (2005) present a model in which words are generated either conditioned on a topic or conditioned on the previous word in a bigram, but not both. They use this to model human performance on a word-association task. Wallach (2006) experiments with incorporating LDA into a bigram language model. Her model uses a hierarchical Dirichlet to share parameters across bigrams in a topic in a manner similar to our use of PYPs, but it lacks a notion of the topic being shared between the words in an n-gram. The Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) assumes that all words in a sentence have the same topic, and consecutive sentences are likely to have the same topic. By dropping the independence assumption among topics, HTMM is able to achieve lower perplexity scores than LDA at minimal additional computational costs. These models are unconcerned with topical n-grams and thus do not model phrases. Johnson (2010) presents an Adaptor Grammar model of topical phrases. Adaptor Grammars are a framework for specifying nonparametric Bayesian models over context-free grammars in which certain subtrees are “memoized” or remembered for reuse. In Jo</context>
</contexts>
<marker>Gruber, Weiss, Rosen-Zvi, 2007</marker>
<rawString>Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007. Hidden topic Markov models. Journal of Machine Learning Research - Proceedings Track, 2:163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>Overview of the first text retrieval conference (trec–1).</title>
<date>1992</date>
<booktitle>In Proceedings of the first Text REtrieval Conference (TREC–1),</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="4922" citStr="Harman, 1992" startWordPosition="768" endWordPosition="769">hat uses dependent Pitman-Yor processes to ameliorate overfitting. Pitman-Yor processes have been successfully used in the past in n-gram (Teh, 2006) and LDA-based models (Wallach, 2006) for creating Bayesian language models which exploit word order, and they prove equally useful in this scenario of exploiting both word order and topics. This article is organized as follows: after describing TNG, we discuss PDLDA and how PDLDA addresses the limitations of TNG. We then provide details of our inference procedures and evaluate our model against competing models on a subset of the TREC AP corpus (Harman, 1992) in an experiment on human subjects which assesses the interpretability of topical n-gram lists. The experiment is premised on the notion that topic models should be evaluated through a real-world task instead of through information-theoretic measures which often negatively correlate with topic quality (Chang et al., 2009). 2 Background: LDA and TNG LDA represents documents as probabilistic mixtures of latent topics. Each word w in a corpus w is drawn from a distribution φ indexed by a topic z, where z is drawn from a distribution θ indexed by its document d. The formal definition of LDA is θd</context>
<context position="28442" citStr="Harman, 1992" startWordPosition="4795" endWordPosition="4796">lowed within a list. (b) Word repetition not allowed. Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model. Excluding trials with repeated words does not qualitatively affect the results. wrong were excluded from our analyses. Each subject was presented with trials constructed from the output of PDLDA and TNG for unigrams, bigrams, and trigrams. For unigrams, we also tested the output of the original smoothed LDA (Blei et al., 2003). The experiment was conducted twice for a 2,246-document subset of the TREC AP corpus (Blei et al., 2003; Harman, 1992): the first time proceeded as described above, but the second time did not allow word repetition within a topic’s list. The topical phrases found by TNG and PDLDA often revolve around a central n-gram, with other words pre- or post- appended to it. In this intrusion experiment, any n-gram not containing the central word or phrase may be trivially identifiable, regardless of its relevance to the topic. For example, the intruder in Trial 4 of Figure 4 is easily identifiable even if a subject does not understand English. This second experiment was designed to test whether our conclusions hinge on</context>
</contexts>
<marker>Harman, 1992</marker>
<rawString>Donna Harman. 1992. Overview of the first text retrieval conference (trec–1). In Proceedings of the first Text REtrieval Conference (TREC–1), Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1148--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="24849" citStr="Johnson (2010)" startWordPosition="4199" endWordPosition="4200">erarchical Dirichlet to share parameters across bigrams in a topic in a manner similar to our use of PYPs, but it lacks a notion of the topic being shared between the words in an n-gram. The Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) assumes that all words in a sentence have the same topic, and consecutive sentences are likely to have the same topic. By dropping the independence assumption among topics, HTMM is able to achieve lower perplexity scores than LDA at minimal additional computational costs. These models are unconcerned with topical n-grams and thus do not model phrases. Johnson (2010) presents an Adaptor Grammar model of topical phrases. Adaptor Grammars are a framework for specifying nonparametric Bayesian models over context-free grammars in which certain subtrees are “memoized” or remembered for reuse. In Johnson’s model, subtrees corresponding to common phrases for a topic are memoized, resulting in a model in which each topic is associated with a distribution over whole phrases. While it is a theoretically elegant method for finding topical phrases, for large corpora we found inference to be impractically slow. 5 Phrase Intrusion Experiment Perplexity is the typical i</context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1148–1157, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="12233" citStr="Landauer and Dumais, 1997" startWordPosition="1906" endWordPosition="1909">potential energy solar energy light energy fossil fuels hot water steam engine large amounts sun&apos;s energy radiant energy nuclear energy nuclear power plants nuclear power plant important natural resources electric power plants called fossil fuels important natural resource produce large amounts called solar energy electric light bulb use electrical energy use solar energy carbon dioxide gas called potential energy gas called carbon dioxide called crude oil (d) Topic 4 Figure 1: Six out of one hundred topics found by our model, PDLDA, on the Touchstone Applied Science Associates (TASA) corpus (Landauer and Dumais, 1997). Each column within a box shows the top fifteen phrases for a topic and is restricted to phrases of a minimum length of one, two, or three words, respectively. The rows are ordered by likelihood. 216 Figure 2: PDLDA drawn in plate notation. contexts, is desirable so that a model like this does not need to independently infer the probability of every bigram under every topic. The advantages of smoothing are especially pronounced for small corpora or for a large number of topics. In these situations, the observed number of bigrams in a given topic will necessarily be very small and thus not sup</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211 – 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="29103" citStr="McCallum, 2002" startWordPosition="4907" endWordPosition="4908"> but the second time did not allow word repetition within a topic’s list. The topical phrases found by TNG and PDLDA often revolve around a central n-gram, with other words pre- or post- appended to it. In this intrusion experiment, any n-gram not containing the central word or phrase may be trivially identifiable, regardless of its relevance to the topic. For example, the intruder in Trial 4 of Figure 4 is easily identifiable even if a subject does not understand English. This second experiment was designed to test whether our conclusions hinge on word repetition. We used the MALLET toolbox (McCallum, 2002) for the implementations of LDA and TNG. Each model was run with 100 topics for 5,000 iterations. We set m = 2, a = .01, Q = .01, A = 1, Tr1 = 7r2 = 1, p1 = 10, and p2 = .1. For all models, we treated certain punctuation as the start of a phrase by setting cj = 1 for all tokens j immediately following periods, commas, semicolons, and exclamation and question marks. To reduce runtime, we removed stopwords occuring in the MALLET toolbox’s stopword list. Because TNG and LDA had trouble with single character words not in the stoplist, we manually removed them before the experiment. Any token immed</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="23541" citStr="Neal, 2000" startWordPosition="3975" endWordPosition="3976">ed distribution. P� differs from Q in that the probability of each word wj and the seating arrangement depends only on -,s(j), as opposed to the simplification of using -,s(i). Almost all proposals are accepted; hence, this theoretically motivated Metropolis Hastings correction step makes little difference in practice. Because the parameters a and b have no intuitive interpretation and we lack any strong belief about what they should be, we give them vague priors where ρ1 = ρ2 = 1 and E1 = 10, E2 = .1. We then Y j∈x(i) ⎧ ⎨⎪⎪ ⎪⎪⎩ n¬s(j) d· + Tα ifcj=1 219 interleave a slice sampling algorithm (Neal, 2000) between sweeps of the Metropolis-Hastings sampler to learn these parameters. We chose not to do inference on α in order to make the tests of our model against TNG more equitable. 4 Related Work An integral part of modeling topical phrases is the relaxation of the bag-of-words assumption in LDA. There are many models that make this relaxation. Among them, Griffiths and Steyvers (2005) present a model in which words are generated either conditioned on a topic or conditioned on the previous word in a bigram, but not both. They use this to model human performance on a word-association task. Walla</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford Neal. 2000. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Paolacci</author>
<author>Jesse Chandler</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<date>2010</date>
<booktitle>Running experiments on Amazon Mechanical Turk. Judgment and Decision Making,</booktitle>
<pages>5--5</pages>
<contexts>
<context position="26468" citStr="Paolacci et al., 2010" startWordPosition="4460" endWordPosition="4463">iest u.s. troops roman catholic bishop Figure 4: Experimental setup of the phrase intrusion experiment in which subjects must click on the ngram that does not belong. scores may negatively correlate with actual quality as assessed by humans (Chang et al., 2009). With that fact in mind, we expanded the methodology of Chang et al. (2009) to create a “phrase intrusion” task that quantitatively compares the quality of the topical n-gram lists produced by our model against those of other models. Each of 48 subjects underwent 80 trials of a webbased experiment on Amazon Mechanical Turk, a reliable (Paolacci et al., 2010) and increasingly common venue for conducting online experiments. In each trial, a subject is presented with a randomly ordered list of four n-grams (cf. Figure 4). Each subject’s task is to select the intruder phrase, a spurious n-gram not belonging with the others in the list. If, other than the intruder, the items in the list are all on the same topic, then subjects can easily identify the intruder because the list is semantically cohesive and makes sense. If the list is incohesive and has no discernible topic, subjects must guess arbitrarily and performance is at random. To construct each </context>
</contexts>
<marker>Paolacci, Chandler, Ipeirotis, 2010</marker>
<rawString>Gabriele Paolacci, Jesse Chandler, and Panagiotis G. Ipeirotis. 2010. Running experiments on Amazon Mechanical Turk. Judgment and Decision Making, 5(5):411–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability,</title>
<date>1997</date>
<pages>25--855</pages>
<contexts>
<context position="16380" citStr="Pitman and Yor, 1997" startWordPosition="2665" endWordPosition="2668">tion over V . When, for example, the context is u = z1 : honda, the darkened path is followed and the probability of the next word is calculated from the shaded node using Equation 1, which combines predictions from all the nodes along the darkened path. distributions used are Pitman-Yor processes (PYPs) linked together into a tree structure. This hierarchical construction creates the desired smoothing among different contexts. The next section explains this hierarchical distribution in more detail. 3.1 Hierarchical Pitman-Yor process Words in PDLDA are emitted from Gu, which has a PYP prior (Pitman and Yor, 1997). PYPs are a generalization of the Dirichlet Process, with the addition of a discount parameter 0 &lt; a &lt; 1. When considering the distribution of a sequence of words w drawn iid from a PYP-distributed G, one can analytically marginalize G and consider the resulting conditional distribution of w given its parameters a, b, and base distribution φ. This marginal can best be understood by considering the distribution of any wi|w1, ... , wi−1, a, b, φ, which is characterized by a generative process known as the generalized Chinese Restaurant Process (CRP) (Pitman, 2002). In the CRP metaphor, one imag</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. Annals of Probability, 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pitman</author>
</authors>
<title>Combinatorial stochastic processes.</title>
<date>2002</date>
<tech>Technical Report 621,</tech>
<institution>Department of Statistics, University of California at Berkeley.</institution>
<contexts>
<context position="16949" citStr="Pitman, 2002" startWordPosition="2764" endWordPosition="2765">which has a PYP prior (Pitman and Yor, 1997). PYPs are a generalization of the Dirichlet Process, with the addition of a discount parameter 0 &lt; a &lt; 1. When considering the distribution of a sequence of words w drawn iid from a PYP-distributed G, one can analytically marginalize G and consider the resulting conditional distribution of w given its parameters a, b, and base distribution φ. This marginal can best be understood by considering the distribution of any wi|w1, ... , wi−1, a, b, φ, which is characterized by a generative process known as the generalized Chinese Restaurant Process (CRP) (Pitman, 2002). In the CRP metaphor, one imagines a restaurant with an unbounded number of tables, where each table has one shared dish (a draw from φ) and can seat an unlimited number of customers. The CRP specifies a process by which customers entering the restaurant choose a table to sit at and, consequently, the dish they eat. The first customer to arrive always sits at the first table. Subsequent customers sit at an occupied table k with probability proportional to ck − a and choose a new unoccupied table with probability proportional to b + ta, where ck is the number of customers seated at table k and</context>
</contexts>
<marker>Pitman, 2002</marker>
<rawString>J. Pitman. 2002. Combinatorial stochastic processes. Technical Report 621, Department of Statistics, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>100--108</pages>
<editor>In Lillian Lee and Donna Harman, editors,</editor>
<contexts>
<context position="2848" citStr="Schone and Jurafsky, 2001" startWordPosition="430" endWordPosition="433">ive to LDA which produced richer output like policy iteration algorithm, value function, and model-based reinforcement learning alongside the unigrams would be much more enlightening. Most situations where a topic model is actually useful for data exploration require a model whose output is rich enough to dispel the need for the user’s extensive prior knowledge of the data. Furthermore, lists of topical unigrams are often made only marginally interpretable by virtue of their non-compositionality, the principle that a collocation’s meaning typically is not derivable from its constituent words (Schone and Jurafsky, 2001). For example, the meaning of compact disc as a music medium comes from neither the unigram compact nor the unigram disc, but emerges from the bigram as a whole. Moreover, non-compositionality is topic dependent; compact disc should be interpreted as a music medium in a music topic, and as a small region bounded by a circle in a mathematical topic. LDA is prone to decompose collocations into different topics and violate the principle of noncompositionality, and its unigram lists are harder to interpret as a result. 214 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural La</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Lillian Lee and Donna Harman, editors, Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL44,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4458" citStr="Teh, 2006" startWordPosition="690" endWordPosition="691">al NGram” (TNG) model of Wang et al. (2007). TNG is a topic model which satisfies the first desideratum by producing lists of representative, topically cohesive n-grams of the form shown in Figure 1. We diverge from TNG by our addressing the second desideratum, and we do so through a more straightforward and intuitive definition of what constitutes a phrase and its topic. In the furtherance of our goals, we employ a hierarchical method of modeling phrases that uses dependent Pitman-Yor processes to ameliorate overfitting. Pitman-Yor processes have been successfully used in the past in n-gram (Teh, 2006) and LDA-based models (Wallach, 2006) for creating Bayesian language models which exploit word order, and they prove equally useful in this scenario of exploiting both word order and topics. This article is organized as follows: after describing TNG, we discuss PDLDA and how PDLDA addresses the limitations of TNG. We then provide details of our inference procedures and evaluate our model against competing models on a subset of the TREC AP corpus (Harman, 1992) in an experiment on human subjects which assesses the interpretability of topical n-gram lists. The experiment is premised on the notio</context>
<context position="18266" citStr="Teh (2006)" startWordPosition="2992" endWordPosition="2993">nd “dishes” are word types. The hierarchical PYP (HPYP) is an intuitive recursive formulation of the PYP in which the base distribution φ is itself PYP-distributed. Figure 3 demonstrates this principle as applied to PDLDA. The hierarchy forms a tree structure, where leaves are restaurants corresponding to full contexts and internal nodes correspond to partial contexts. An edge between a parent and child node represents a dependency of the child on the parent, where the base distribution of the child node is its parent. This smooths each context’s distribution like the Bayesian n-gram model of Teh (2006), which is a Bayesian version of interpolated Kneser-Ney smoothing (Chen and Goodman, 1998). One ramification of this setup is that if a word occurs in a context u, the sharing makes it more likely in other contexts that have something in common with u, such as a shared topic or word. The HPYP gives the following probability for a word following the context u being w: cuw· − a|u|tuw Pu(w |τ, a, b) = + b|u |+ cu·· b|u |+ a|u|tu· Pπ(u)(w |τ, a, b) (1) b|u |+ cu·· where Pπ(∅)(w|τ, a, b) = G∅(w), cuw· is the number of customers eating dish w in restaurant u, and tuw is the number of tables serving</context>
<context position="20104" citStr="Teh, 2006" startWordPosition="3329" endWordPosition="3330">on, we describe Markov chain Monte Carlo procedures to sample from P (z, c, τ|w, U), the posterior distribution over topic assignments z, phrase boundaries c, and seating arrangements τ given an observed corpus w. Let U be shorthand for α, λ, a, b. In order to draw samples from P (z, c, τ|w, U), we employ a MetropolisHastings sampler for approximate inference. The sampler we use is a collapsed sampler (Griffiths and Steyvers, 2004), wherein θ, φ, and G are analytically marginalized. Because we marginalize each G, we use the Chinese Restaurant Franchise representation of the hierarchical PYPs (Teh, 2006). However, rather than onerously storing the table assignment of every token in w, we store only the counts of how many tables there are in a restaurant and how many customers are sitting at each table in that restaurant. We refer the inquisitive reader to the appendix of Teh (2006) for further details of this procedure. Our sampling strategy for a given token i in document d is to jointly propose changes to the changepoint ci and topic assignment zi, and then to the seating arrangement τ. Recall that according to the model, if ci = 0, zi = zi−1; otherwise zi is generated from the topic distri</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL44, pages 985–992, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>977--984</pages>
<contexts>
<context position="4495" citStr="Wallach, 2006" startWordPosition="695" endWordPosition="696">al. (2007). TNG is a topic model which satisfies the first desideratum by producing lists of representative, topically cohesive n-grams of the form shown in Figure 1. We diverge from TNG by our addressing the second desideratum, and we do so through a more straightforward and intuitive definition of what constitutes a phrase and its topic. In the furtherance of our goals, we employ a hierarchical method of modeling phrases that uses dependent Pitman-Yor processes to ameliorate overfitting. Pitman-Yor processes have been successfully used in the past in n-gram (Teh, 2006) and LDA-based models (Wallach, 2006) for creating Bayesian language models which exploit word order, and they prove equally useful in this scenario of exploiting both word order and topics. This article is organized as follows: after describing TNG, we discuss PDLDA and how PDLDA addresses the limitations of TNG. We then provide details of our inference procedures and evaluate our model against competing models on a subset of the TREC AP corpus (Harman, 1992) in an experiment on human subjects which assesses the interpretability of topical n-gram lists. The experiment is premised on the notion that topic models should be evaluat</context>
<context position="24150" citStr="Wallach (2006)" startWordPosition="4079" endWordPosition="4080">2000) between sweeps of the Metropolis-Hastings sampler to learn these parameters. We chose not to do inference on α in order to make the tests of our model against TNG more equitable. 4 Related Work An integral part of modeling topical phrases is the relaxation of the bag-of-words assumption in LDA. There are many models that make this relaxation. Among them, Griffiths and Steyvers (2005) present a model in which words are generated either conditioned on a topic or conditioned on the previous word in a bigram, but not both. They use this to model human performance on a word-association task. Wallach (2006) experiments with incorporating LDA into a bigram language model. Her model uses a hierarchical Dirichlet to share parameters across bigrams in a topic in a manner similar to our use of PYPs, but it lacks a notion of the topic being shared between the words in an n-gram. The Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) assumes that all words in a sentence have the same topic, and consecutive sentences are likely to have the same topic. By dropping the independence assumption among topics, HTMM is able to achieve lower perplexity scores than LDA at minimal additional computational cos</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: beyond bagof-words. In Proceedings of the 23rd International Conference on Machine Learning, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="3891" citStr="Wang et al. (2007)" startWordPosition="597" endWordPosition="600">he principle of noncompositionality, and its unigram lists are harder to interpret as a result. 214 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 214–222, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics We present an extension of LDA called PhraseDiscovering LDA (PDLDA) that satisfies two desiderata: providing rich, interpretable output and honoring the non-compositionality of collocations. PDLDA is built in the tradition of the “Topical NGram” (TNG) model of Wang et al. (2007). TNG is a topic model which satisfies the first desideratum by producing lists of representative, topically cohesive n-grams of the form shown in Figure 1. We diverge from TNG by our addressing the second desideratum, and we do so through a more straightforward and intuitive definition of what constitutes a phrase and its topic. In the furtherance of our goals, we employ a hierarchical method of modeling phrases that uses dependent Pitman-Yor processes to ameliorate overfitting. Pitman-Yor processes have been successfully used in the past in n-gram (Teh, 2006) and LDA-based models (Wallach, 2</context>
<context position="7117" citStr="Wang et al. (2007)" startWordPosition="1159" endWordPosition="1162">, zi, ci = 1, φ — Discrete(φzi) wi |wi−1, zi, ci = 0, σ — Discrete(σziwi−1) ci |wi−1, zi−1, π — Bernoulli(πzi−1wi−1) where the new distributions πzw and σzw are endowed with conjugate prior distributions: πzw — Beta(λ) and σzw — Dirichlet(δ). When ci = 0, word wi is joined into a topic-specific bigram with wi−1. When ci = 1, wi is drawn from a topicspecific unigram distribution and is the start of a new n-gram. An unusual feature of TNG is that words within a topical n-gram, a sequence of words delineated by c, do not share the same topic. To compensate for this after running a Gibbs sampler, Wang et al. (2007) analyze each topical n-gram post hoc as if the topic of the final word in the n-gram was the topic assignment of the entire n-gram. Though this design simplifies inference, we perceive it as a shortcoming since the aforementioned principle of non-compositionality supports the intuitive idea that each collocation ought to be drawn from a single topic. Another potential drawback of TNG is that the topic-specific bigram distributions σzw share no probability mass between each other or with the unigram distributions φz. Hence, observing a bigram under one topic does not make it more likely under </context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 7th IEEE International Conference on Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040537">
<title confidence="0.903353">
No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities
</title>
<author confidence="0.998137">
Thomas Lin, Mausam, Oren Etzioni
</author>
<affiliation confidence="0.997305">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.582937">
Seattle, WA 98195, USA
</address>
<email confidence="0.994243">
{tlin, mausam, etzioni}@cs.washington.edu
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99926805">
Entity linking systems link noun-phrase men-
tions in text to their corresponding Wikipedia
articles. However, NLP applications would
gain from the ability to detect and type all
entities mentioned in text, including the long
tail of entities not prominent enough to have
their own Wikipedia articles. In this paper we
show that once the Wikipedia entities men-
tioned in a corpus of textual assertions are
linked, this can further enable the detection
and fine-grained typing of the unlinkable en-
tities. Our proposed method for detecting un-
linkable entities achieves 24% greater accu-
racy than a Named Entity Recognition base-
line, and our method for fine-grained typing is
able to propagate over 1,000 types from linked
Wikipedia entities to unlinkable entities. De-
tection and typing of unlinkable entities can
increase yield for NLP applications such as
typed question answering.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792416666667">
A key challenge in machine reading (Etzioni et al.,
2006) is to identify the entities mentioned in text,
and associate them with appropriate background in-
formation such as their type. Consider the sentence
“Some people think that pineapple juice is good for
vitamin C.” To analyze this sentence, a machine
should know that “pineapple juice” refers to a bev-
erage, while “vitamin C” refers to a nutrient.
Entity linking (Bunescu and Pas¸ca, 2006;
Cucerzan, 2007) addresses this problem by link-
ing noun phrases within the sentence to entries
in a large, fixed entity catalog (almost always
</bodyText>
<table confidence="0.916301714285714">
example noun phrases status
“apple juice” “orange juice”
“prune juice” “wheatgrass juice”
“radiation exposure” “workplace stress”
“asbestos exposure” “financial stress”
“IJCAI” “OOPSLA”
“EMNLP” “ICAPS”
</table>
<tableCaption confidence="0.9068845">
Table 1: Wikipedia has entries for prominent entities,
while missing tail and new entities of the same types.
</tableCaption>
<bodyText confidence="0.999802666666667">
Wikipedia). Thus, entity linking has a limited and
somewhat arbitrary range. In our example, systems
by (Ferragina and Scaiella, 2010) and (Ratinov et
al., 2011) both link “vitamin C” correctly, but link
“pineapple juice” to “pineapple.” “Pineapple juice”
is not entity linked as a beverage because it is not
prominent enough to have its own Wikipedia entry.
As Table 1 shows, Wikipedia often has prominent
entities, while missing tail and new entities of the
same types.1 (Wang et al., 2012) notes that there
are more than 900 different active shoe brands, but
only 82 exist in Wikipedia. In scenarios such as in-
telligence analysis and local search, non-Wikipedia
entities are often the most important.
Hence, we introduce the unlinkable noun phrase
problem: Given a noun phrase that does not link
into Wikipedia, return whether it is an entity, as well
its fine-grained semantic types. Deciding if a non-
Wikipedia noun phrase is an entity is challenging
because many of them are not entities (e.g., “Some
people,” “an addition” and “nearly half”). Predict-
</bodyText>
<footnote confidence="0.9923005">
1The same problem occurs with Freebase, which is also
missing the same Table 1 entities.
</footnote>
<figure confidence="0.996062">
present
absent
present
absent
present
absent
</figure>
<page confidence="0.991124">
893
</page>
<note confidence="0.80568">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 893–903, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99853625">
ing semantic types is a challenge because of the di-
versity of entity types in general text. In our experi-
ments, we utilized the Freebase type system, which
contains over 1,000 semantic types.
The first part of this paper proposes a novel
method for detecting entities by observing that enti-
ties often have different usage-over-time character-
istics than non-entities. Evaluation shows that our
method achieves 24% relative accuracy gain over
a NER baseline. The second part of this paper
shows how instance-to-instance class propagation
(Kozareva et al., 2011) can be adapted and scaled to
semantically type general noun-phrase entities using
types from linked entities, by leveraging over one
million different possible textual relations.
Contributions of our research include:
</bodyText>
<listItem confidence="0.989537">
• We motivate and introduce the unlinkable noun
phrase problem, which extends previous work
in entity linking.
• We propose a novel method for discriminating
entities from arbitrary noun phrases, utilizing
features derived from Google Books ngrams.
• We adapt and scale instance-to-instance class
propagation in order to associate types with
non-Wikipedia entities.
• We implement and evaluate our methods, em-
pirically verifying improvement over appropri-
ate baselines.
</listItem>
<sectionHeader confidence="0.99061" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99991175">
In this section we provide an overview of entity link-
ing, how we entity link our data set, and describe
how our problem and approach differ from related
areas such as NER and Web extraction.
</bodyText>
<subsectionHeader confidence="0.993498">
2.1 Entity Linking
</subsectionHeader>
<bodyText confidence="0.999969134615385">
Given text, the task of entity linking (Bunescu
and Pas¸ca, 2006; Cucerzan, 2007; Milne and Wit-
ten, 2008; Kulkarni et al., 2009) is to identify the
Wikipedia entities within the text, and mark them
with which Wikipedia entity they correspond to. En-
tity linking elevates us from plain text into mean-
ingful entities that have properties, semantic types,
and relationships with each other. Other entity cata-
logs can be used in place of Wikipedia, especially in
domain-specific contexts, but general purpose link-
ing systems all use Wikipedia because of its broad
general coverage, and to leverage its article texts and
link structure during the linking process.
A problem we observed when using entity link-
ing systems is that despite containing over 3 million
entities, Wikipedia does not cover a significant num-
ber of entities. This occurs with entities that are not
prominent enough to have their own dedicated arti-
cle and with entities that are very new. For exam-
ple, Facebook has over 600 million users, and each
of them could be considered an entity. The REVERB
extractor (Fader et al., 2011) on the ClueWeb09 Web
corpus found over 1.4 billion noun phrases partic-
ipating in textual relationships, and a sizable por-
tion of these noun phrases are entities. While re-
cent research has used NIL features to determine
whether they are being asked to link an entity not in
Wikipedia (Dredze et al., 2010; Ploch, 2011), there
has been no research on whether given noun phrases
that are unlinkable (for not being in Wikipedia) are
entities, and how to make them usable if they are.
Our goal is to address this problem of learning
whether non-Wikipedia noun phrases are entities,
and assigning semantic types to them to make them
useful. We begin with a corpus of 15 million “(noun
phrase subject, textual relation, noun phrase object)”
assertions from the Web that were extracted by RE-
VERB (Fader et al., 2011).2 REVERB already filters
out relative pronouns, WHO-adverbs, and existential
“there” noun phrases that do not make meaningful
arguments. We then employ standard entity linking
techniques including string matching, prominence
priors (Fader et al., 2009), and context matching
(Bunescu and Pas¸ca, 2006) to link the noun phrase
subjects into Wikipedia.
In this manner, we were able to entity link the
noun phrase subject of 9,699,967 extractions, while
the remaining 5,028,301 extractions had no matches
(mostly due to no close string matches). There were
1,401,713 distinct noun phrase subjects in the 5 mil-
lion extractions that had no matches. These are the
unlinkable noun phrases we will study here.
</bodyText>
<subsectionHeader confidence="0.997873">
2.2 Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.979010333333333">
Named Entity Recognition (NER) is the task of
identifying named entities in text. A key difference
between our final goals and NER is that in the con-
</bodyText>
<footnote confidence="0.999366">
2available at http://reverb.cs.washington.edu
</footnote>
<page confidence="0.998551">
894
</page>
<bodyText confidence="0.999951777777778">
text of entity linking and Wikipedia, there are many
more entities than just the named entities. For ex-
ample, “apple juice” and “television” are Wikipedia
entities (with Wikipedia articles), but are not tradi-
tional named entities. Still, as named entities do
comprise a sizable portion of our unlinkable noun
phrases, we compare against a NER baseline in our
entity detection step.
Fine-grained NER (Sekine and Nobata, 2004; Lee
et al., 2007) has studied scaling NER to up to 200 se-
mantic types. This differs from our semantic typing
of unlinked entities because our approach assumes
access to corpora-level relationships between a large
set of linked entities (with semantic types) and the
unlinked entities. As a result we are able to propa-
gate 1,339 Freebase semantic types from the linked
entities to the unlinked entities, which is substan-
tially more types than fine-grained NER.
</bodyText>
<subsectionHeader confidence="0.999868">
2.3 Extracting Entity Sets
</subsectionHeader>
<bodyText confidence="0.999925222222222">
There is a line of research in using Web extraction
(Etzioni et al., 2005) and entity set expansion (Pantel
et al., 2009) to extract lists of typed entities from the
Web (e.g., a list of every city). Our problem instead
focuses on determining whether any individual noun
phrase is an entity, and what semantic types it holds.
Given a noun phrase representing a person name, we
return that this is a person entity even if it is not in a
list of people names harvested from the Web.
</bodyText>
<sectionHeader confidence="0.989968" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.993830666666667">
Our goal is: given (1) a large set of linked assertions
L and (2) a large set of unlinked assertions U, for
each unlinkable noun phrase subject n E U, predict:
(1) whether n is an entity, and if so, then (2) the set
of Freebase semantic types for n. For L we use the
9.7 million assertions whose subject argument we
were able to link in Section 2.1, and for U we use
the 5 million assertions that we could not link.
We divide the system into two components. The
first component (described in Section 4) takes any
unlinkable noun phrase and outputs whether it is an
entity. All n E U classified as entities are placed in
a set E. The second component (described in Sec-
tion 5) uses L and U to predict the semantic types
for each entity e E E.
</bodyText>
<figureCaption confidence="0.984756833333333">
Figure 1: Usage over time in Google books for the noun
phrase “Prices quoted” (e.g., from “Prices quoted are for
2 adults”) which is not an entity.
Figure 2: Usage over time for the unlinkable noun phrase
“Soluble fibre,” which is an entity. The best fit line has
steeper slope compared to Figure 1.
</figureCaption>
<sectionHeader confidence="0.965431" genericHeader="method">
4 Detecting Unlinkable Entities
</sectionHeader>
<bodyText confidence="0.999955952380953">
This first task takes in any unlinkable noun phrase
and outputs whether it is an entity. There is a long
history of discussion in analytic philosophy litera-
ture on the question of what exists (e.g., (Quine,
1948)). We adopt a more pragmatic view, defin-
ing an “entity” as a noun phrase that could have a
Wikipedia-style article if there were no notability or
newness considerations, and which would have se-
mantic types. We are interested in entities that could
help populate an entity store. “EMNLP 2012” is an
example of an entity, while “The method” and “12
cats” are examples of noun phrases that are not en-
tities. This is challenging because at a surface level,
many entities and non-entities look similar: “Sex
and the City” is an entity, while “John and David”
is not. “Eminem” is an entity, while “Youd” (a typo
from “You’d”) is not.
We address this task by training a classifier with
features primarily derived from a timestamped cor-
pus. An intuition here is that when considering
only unlinkable noun phrases, usage patterns across
</bodyText>
<page confidence="0.998081">
895
</page>
<figureCaption confidence="0.992220333333333">
Figure 3: Plot of R2 vs Slope for the usage over time of a collection of noun phrases selected for illustrative purposes.
Many of the non-entities occur at lower Slope and higher R2, while the entities often have higher slope and/or lower
R2. “Bluetooth technology” actually has even higher slope, but was adjusted left to fit in this figure.
</figureCaption>
<bodyText confidence="0.999848222222222">
time often differ for entities and non-entities. Noun
phrase entities that are observed in text going back
hundreds of years (e.g., “Europe”) almost all have
their own Wikipedia entries, so in unlinkable noun
phrase space, the remaining noun phrases that are
observed in text going back hundreds of years tend
to be all the textual references and expressions that
are not entities. We plan to use this signal to help
separate the entities from the non-entities.
</bodyText>
<subsectionHeader confidence="0.994842">
4.1 Classifier Features
</subsectionHeader>
<bodyText confidence="0.9999728">
We use the Google Books ngram corpus (Michel
et al., 2010), which contains timestamped usage
of 1-grams through 5-grams in millions of digi-
tized books for each year from 1500 to 2007.3 We
use ngram match count values from case-insensitive
matching. To avoid sparsity anomalies we observed
in years before 1740, we use the data from 1740 on-
ward. While it has not been used for our task before,
this corpus is a rich resource that enables reason-
ing about knowledge (Evans and Foster, 2011) and
</bodyText>
<footnote confidence="0.976049">
3available at http://books.google.com/ngrams/datasets
</footnote>
<bodyText confidence="0.999950045454545">
understanding semantic changes of words over time
(Wijaya and Yeniterzi, 2011). Talukdar et al. (2012)
recently used it to effectively temporally scope rela-
tional facts.
Our first feature is the slope of the least-squares
best fit line for usage over time. For example, if a
term appears 25 times in books in 1950, 30 times in
1951, ..., 100 times in 2007, then we compute the
straight line that best fits {(1950, 25), (1951, 31), ...,
(2007, 100)}, and examine the slope. We have ob-
served cases of non-entity noun phrases (e.g., Fig-
ure 1) having lower slopes than entity noun phrases
(e.g., Figure 2). Note that we do not normalize
match counts by yearly total frequency, but we do
normalize counts for each term to range from 0 to 1
(setting the max count for each term to 1) to avoid
bias from entity prominence. To capture the current
usage, in cases where there exists a &gt; 5 year gap in
usage of a term we only use the data after the gap.
Another feature is the R2 fit of the best fit line.
Higher R2 indicates that the data is closer to a
straight line. Figure 3 plots R2 vs Slope values
</bodyText>
<page confidence="0.996478">
896
</page>
<table confidence="0.997996833333333">
system correctly classified
Majority class baseline 50.4%
Named Entity Recognition 63.3%
Slope feature only 61.1%
PUF feature combination 69.1%
ALL features 78.4%
</table>
<tableCaption confidence="0.9423095">
Table 2: Our classifier using all features (ALL) outper-
forms majority class and NER baselines.
</tableCaption>
<figureCaption confidence="0.999038">
Figure 4: UsageSinceYear of example unlinked terms.
</figureCaption>
<bodyText confidence="0.999962">
for some sample noun phrases. We observed that
along with their lower Slope, the non-entities often
also had higher R2, indicating that their usage does
not vary wildly from year to year. This contrasts
with certain entities (e.g., “FY 99” for “Fiscal Year
1999”) whose usage sometimes varied sharply from
year to year based on their prominence in those spe-
cific years.
A third feature is UsageSinceYear, which finds the
year from when a term last started continually being
used. For example, a UsageSinceYear value of 1920
would indicate that the term was used in books ev-
ery year from 1920 through 2007. Figure 4 shows
examples of where various terms fall along this di-
mension.
From the books ngram corpus, we also calcu-
late features for: PercentProperCaps - the percent-
age of case-insensitive matches for the term where
all words began with a capital letter, PercentExact-
Match - the percentage of case-insensitive matches
for the term that matched the capitalization in the
assertion exactly, and Frequency - the total number
of case-insensitive occurrences of the term in the
book ngrams data, summed across all years, which
reflects prominence. Last, we also include a sim-
ple numeric feature to detect the presence of leading
numeric words (e.g., “5” in “5 days” or “Three” in
“Three choices”).
</bodyText>
<subsectionHeader confidence="0.980003">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999977953488372">
From the corpus of 15 million REVERB assertions,
there were 1.4 million unlinked noun phrases includ-
ing 17% unigrams, 51% bigrams, 21% trigrams, and
11% 4-grams or longer. Bigrams comprise over half
the noun phrases and the books bigram data is a self-
contained download that is easier to obtain and store
than the full books ngram corpus, so we focus on
bigrams in our evaluation. In a random sample of
unlinked bigrams, we found that 73% were present
in the books ngram data (65% exact match, 8% case-
insensitive match only), while 27% were not (these
were mostly entities or errors with non-alphabetic
characters). Coverage is a greater issue with longer
ngrams (e.g., there are many more possible 5-grams
than bigrams, so any individual 5-gram is less likely
to reach the minimum threshold to be included in the
books data), but as mentioned earlier, only 11% of
unlinkable noun phrases were 4-grams or longer.
We randomly sampled 250 unlinked bigrams that
had books ngram data, and asked 2 annotators to la-
bel each as “entity,” “non-entity,” or “unclear.” Our
goal is to separate noun phrases that are clearly en-
tities (e.g., “prune juice”) from those that are clearly
not entities (e.g., “prices quoted”), rather than to de-
bate phrases that may be in some entity store defi-
nitions but not others, so we asked the annotators to
choose “unclear” when there was any doubt. There
were 151 bigrams that both annotators believed to
be very clear labels, including 69 that both annota-
tors labeled as entities, 70 that both annotators la-
beled as non-entities, and 12 with label disagree-
ment. Cohen’s kappa was 0.84, indicating excellent
agreement. Our experiment is now to separate the
69 clear entities from the 70 clear non-entities.
For experiment baselines we use the majority
class baseline MAJ, as well as a Named Entity
Recognition baseline NER. For NER we used the
Illinois Named Entity Tagger (Ratinov and Roth,
2009) on the highest setting (that achieved 90.5 F1
score on the CoNLL03 shared task). NER expects a
sentence, so we use the longest assertion in the cor-
pus that the noun phrase was observed in. We eval-
uate several combinations of our features to test dif-
</bodyText>
<page confidence="0.990007">
897
</page>
<bodyText confidence="0.998574625">
ferent aspects of our system: Slope uses only Slope,
PUF uses PercentProperCaps + UsageSinceYear +
Frequency, and ALL uses all features. We evaluate
using the WEKA J48 Decision Tree on default set-
tings, with leave-one-out cross validation.
Table 2 shows the results. MAJ correctly classifies
50.4% of instances, NER correctly classifies 63.3%
and ALL correctly classifies 78.4%.
</bodyText>
<subsectionHeader confidence="0.998469">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999990692307692">
Overall, 78.4% correctly classified instances is fairly
strong performance on this task. By using the de-
scribed features, our classifier was able to detect and
filter many of the non-entity noun phrases in this
scenario. Compared to the 63.3% of NER, it is an
absolute gain of 15.1%, a relative gain of 24%, and a
reduction in error of 41.1% (from 36.7% to 21.6%).
Student’s t-test at 95% confidence verified that the
difference was significant.
We found that while low Slope (especially with
higher R2) often indicated non-entity, there were nu-
merous cases where higher Slope did not necessarily
indicate entity. For example, the noun phrase “sev-
eral websites” has fairly sharp slope, but still does
not denote a clear entity. In these cases, the addi-
tion of other features can serve as additional useful
signal. One error from ALL is the term “Analyst esti-
mates,” which the annotators labeled as a non-entity,
but which occasionally appears in text (especially ti-
tles) as “Analyst Estimates,” and is a relatively re-
cent phrase. NER misses entities such as “synthetic
cubism” and “hunter orange” that occur in our data
but are not traditional named entities. We observed
that while none of our features achieves over 70%
accuracy by themselves, they perform well in con-
junction with each other.
</bodyText>
<sectionHeader confidence="0.944963" genericHeader="method">
5 Propagating Semantic Types
</sectionHeader>
<bodyText confidence="0.999971901960784">
This second task uses a set of linked assertions L and
set of unlinked assertions U to predict the semantic
types for each entity e E E. If the previous step
output that “Sun Microsystems” is likely to be an
entity, then the goal of this step is to further predict
that it has the Freebase types such as organization
and software developer.
From L we use the set of linked entities and the
textual relations they occur with. For example, L
might contain that the entity Microsoft links to a par-
ticular Wikipedia article, and also that it occurs with
textual relations such as “has already announced”
and “has released updates for.” For each Wikipedia-
linked entity in L, we further look up its exact set
of Freebase types.4 From U we obtain the set of
textual relations that each e E E is in the domain
of. We now have a large set of class-labeled in-
stances (all entities in L), a large set of unlabeled
instances (E), and a method to connect the unla-
beled instances with the class-labeled instances (via
any shared textual relations), so we cast this task
as an instance-to-instance class propagation problem
(Kozareva et al., 2011) for propagating class labels
from labeled to unlabeled instances.
We build on the recent work of Kozareva et al.
(2011), and adapt their approach to leverage the
scale and resources of our scenario. While they use
only one type of edge between instances, namely
shared presence in the high precision DAP pattern
(Hovy et al., 2009), our final system uses 1.3 mil-
lion textual relations from |L U U|, corresponding
to 1.3 million potential edge types. Their evaluation
involved only 20 semantic classes, while we use all
1,339 Freebase types covered by our entities in L.
There is a rich history of other approaches for
predicting semantic types. (Talukdar et al., 2008)
and (Talukdar and Pereira, 2010) model relation-
ships between instances and classes, but our un-
linked entities do not come with any class informa-
tion. Pattern-based approaches (Pas¸ca, 2004; Pantel
and Ravichandran, 2004) are popular, but (Kozareva
et al., 2011) notes that they “are constraint to the in-
formation matched by the pattern and often suffer
from recall,” meaning that they do not cover many
instances. Classifiers have also been trained for fine-
grained semantic typing, but for noticeably fewer
types than we work with. (Rahman and Ng, 2010)
studied hierarchical and collective classification us-
ing 92 types, and FIGER (Ling and Weld, 2012) re-
cently used an adapted perceptron for multi-class
multi-label classification into 112 types.
</bodyText>
<subsectionHeader confidence="0.958102">
5.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.992097">
Given an entity e, our algorithm involves: (1) find-
ing the textual relations that e is in the domain of, (2)
</bodyText>
<footnote confidence="0.98041">
4data available at http://download.freebase.com/wex
</footnote>
<page confidence="0.994946">
898
</page>
<figureCaption confidence="0.986693">
Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase “Sun Microsystems.” We
predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linked
entities that are also in the domain of those textual relations, and (3) observing their semantic types.
</figureCaption>
<bodyText confidence="0.99847128125">
finding linked entities that are also in the domain of
those textual relations, and then (3) predicting types
by observing the types of those linked entities. Fig-
ure 5 illustrates how we would predict the semantic
types of the noun phrase “Sun Microsystems.”
Find Relations: Obtain the set R of all textual re-
lations in U that a is in the domain of. For example,
if U contains the assertion “(Sun Microsystems, has
released a minor update to, Java 1.4.2),” then the tex-
tual relation “has released a minor update to” should
be added to R when typing “Sun Microsystems.”
Find Similar Entities: Find the linked entities in
L that are in the domain of the most relations in
R. In our example, entities such as “Microsoft” and
“Apple Inc.” have the greatest overlap in textual re-
lations because they are most often in the domain
of the same textual relations, e.g., (“Microsoft, has
released a minor update to, Windows Live Essen-
tials”). Create a set S of the entities that share the
most textual relations. We found keeping 10 similar
entities (|S |= 10) is generally enough to predict the
original entity’s types in the final step.
Predict Types: Return the most frequent Freebase
types of the entities in S as the prediction. To
avoid penalizing very small types, if there are n in-
stances of semantic class C in S, then we rank C us-
ing a type score T(n, C, S) = max(n/|S|, n/|C|),
which we found to perform better than T (n, C, S) =
avg(n/|S|, n/|C|). For “Sun Microsystems,” busi-
ness operation was the top predicted type because
all entities in S were observed to include business
operation type.
</bodyText>
<subsectionHeader confidence="0.996862">
5.2 Edge Validity
</subsectionHeader>
<bodyText confidence="0.999989380952381">
This algorithm will only be effective if entities that
share textual relation strings are more likely to be
of the same semantic types. To verify this, we sam-
pled 30,000 linked entities from L that had at least
30 textual relations each, and associated each with
their 30 most frequent relations. From the 900 mil-
lion possible entity pairs, we then randomly sample
500 entity pairs that shared exactly k out of 30 rela-
tions, for each k from 0 to 15. At each k we then use
our sampled pairs to estimate the probability that any
two entities sharing exactly k relations (out of their
30 possible) will share at least one type.
The results are shown in Figure 6. We found that
entities sharing more textual relations were in fact
more likely to have semantic types in common. Two
entities that shared exactly 0 of 30 textual relations
were only 11% likely to share a semantic type, while
two entities that shared exactly 10 of 30 relations
were 80% likely to share a semantic type. This vali-
dates our use of textual relations as a signal-bearing
edge in instance-to-instance class propagation.
</bodyText>
<subsectionHeader confidence="0.999454">
5.3 Weighting Textual Relations
</subsectionHeader>
<bodyText confidence="0.99994">
The algorithm as currently described treats all tex-
tual relations equally, when in reality some are
stronger signal to entity type than others. For exam-
ple, two entities in the domain of the “came with”
relation often will not share semantic types, but two
entities in the domain of the “autographed” relation
will almost always share a type. To capture this intu-
ition, we define relation weight w(r) as the observed
probability (among the linked entities) that two en-
</bodyText>
<page confidence="0.998809">
899
</page>
<figureCaption confidence="0.9966105">
Figure 6: Entities that share more textual relations are
more likely to have semantic types in common.
</figureCaption>
<bodyText confidence="0.999956">
tities will share a Freebase type if they both occur
in the domain of r. If D(r) = all entities observed
in the domain of relation r and T(e) = all Freebase
types listed for entity e, then weight w(r) of a tex-
tual relation string r is:
</bodyText>
<equation confidence="0.994283">
I(e1,e2)
|D(r) |- (|D(r) |-1)
I(e1,e2) _ r1, if |T(e1) n T(e2) |&gt; 0
0, otherwise.
</equation>
<bodyText confidence="0.999532636363636">
Table 3 shows examples of high weight relations,
and Table 4 shows low weight relations. We now
modify the Find Similar Entities step such that if
a linked entity shares a set of relations Q with the
entity being typed, then it receives a score which
considers all shared relations q E Q but uses the
high weight relations more. On a development set
we found that a score of EqCQ 104&apos;w(q) was effec-
tive, as higher weight signifies much stronger signal.
This score then determines which entities to place in
S.
</bodyText>
<subsectionHeader confidence="0.971112">
5.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9711495">
The goal of the evaluation is to judge how well our
method can predict the Freebase semantic types of
entities in our scenario. Our linked entities cov-
ered 1,339 Freebase types, including many interest-
ing types such as computer operating system, reli-
gious text, airline and baseball team. Human judges
would have trouble manually annotating new enti-
ties with all these types because there are too many
to keep in mind and understand the characteristics
“is a highway in”
“is a university located in”
“became the president of”
“turned down the role of”
“has an embassy in”
</bodyText>
<tableCaption confidence="0.995262857142857">
Table 3: Example relations found to have high weight.
“comes with”
“is a generic term for”
“works best on”
“can be made from”
“is almost identical to”
Table 4: Example relations found to have low weight.
</tableCaption>
<bodyText confidence="0.999964628571429">
of. Instead, we automatically generate testing data
by sampling entities from L, and then test on abil-
ity to recover the actual Freebase types (which we
know).
We sample a HEAD set of distinct 500 Freebase
entities (drawn randomly from our set of linked ex-
tractions), and a TAIL set of 500 entities (drawn ran-
domly from our set of linked entities). An entity
that occurs in many extractions is more likely to be
in HEAD than TAIL. Our sampling also picks only
entities that occur with at least 10 relations, which
is appropriate for the Web scenario where more in-
stances can always be queried for.
For baselines we use random baseline BRandom
and a frequency baseline BFrequeney which always
returns types in order of their frequency among
all linked entities (e.g., always person, then loca-
tion, etc). We evaluate our system without rela-
tion weighting (SNoWeight) and also with relation
weighting (SWeighted). For SWeighted we leave all
the test set entities out when calculating global re-
lation weights. Our metrics are Precision at 1 and
F1 score. Precision at 1 measures how often the top
returned type is a correct type, and is useful for ap-
plications that want one type per entity. F1 mea-
sures how well the method recovers the full set of
Freebase types (for each test case we graph preci-
sion/recall and take the max F1), and is useful for
applications such as typed question answering.
Table 5 shows the results. BRandom performs
poorly because there are so many semantic types,
and very few of them are correct for each test
case. BFrequeney performs slightly better on TAIL
than HEAD because TAIL contains more entities of
the most frequent types. SNoWeight p erformance
</bodyText>
<equation confidence="0.9908195">
�w(r) _
e1,e2ED(r), e1Oe2
</equation>
<page confidence="0.980272">
900
</page>
<table confidence="0.990381333333333">
HEAD
Prec@1 F1
BRandom 0.008 0.028
BMrequency 0.244 0.302
SNoWeight 0.5421 0.4651
SWeighted 0.610$ 0.521$
TAIL
Prec@1 F1
0.004 0.023
0.298 0.322
0.5101 0.4561
0.598$ 0.522$
</table>
<tableCaption confidence="0.993476">
Table 5: Evaluation on HEAD and TAIL, 500 elements each. t indicates statistical significance over BFrequency, and
t over both BFrequency and SNoWeight. Significance is measured using the Student’s t-test at 95% confidence. The
top type predicted by our SWeighted method is correct about 60% of the time, while the top type predicted by the
BFrequency baseline is correct under 30% of the time.
</tableCaption>
<bodyText confidence="0.962198666666667">
is statistically significant above all baselines, and
SWeighted is statistically significant over SNoWeight
on both test sets and metrics.
</bodyText>
<subsectionHeader confidence="0.990233">
5.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999976236842105">
SWeighted was successful at recovering the correct
Freebase types of many entities. For example, it
finds that “Atherosclerosis” is a medical risk fac-
tor by connecting it to “obesity” and “diabetes,” that
“Supernatural” is a TV program and a Netflix title
by connecting it to “House” and “30 Rock,” and that
“America West” is an aircraft owner and an airline
by connecting it to “Etihad Airways” and “China
Eastern Airlines.” While precision at 1 around 60%
may not be high enough yet for certain applications,
it is significantly better than competing approaches,
which are under 30%, and we hope that our values
can serve as a non-trivial baseline on this task for
future systems.
One example where SWeighted made some mis-
takes is fictional characters. Many fictional charac-
ters participate in a textual relations that make them
look like people (e.g., “was born on”), but predicting
that they belong to people class is incorrect. Some
performance hit was also due to entity linking errors.
From an assertion like “The Four Seasons is located
in Hamamatsu,” our entity linker (and other entity
linkers we tried) prefer linking “The Four Seasons”
to Vivaldi’s music composition rather than the hotel
chain. We are then unable to recover music compo-
sition type from relations like “is located in.” Our
algorithm relies on accurate entity linking in L, but
there is a precision/recall tradeoff to consider here
because it also benefits from higher coverage of en-
tities and relations in L.
As a general reference for performance of
state-of-the-art fine-grained entity classification, the
FIGER system (Ling and Weld, 2012) for classify-
ing into 112 types reported F1 scores ranging from
0.471 to 0.693 in their experiments. It is important to
note that these numbers are not directly comparable
to us because they used different data, different (and
fewer) types, and different evaluation methodology.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999968722222222">
This paper presented an approach for working with
non-Wikipedia entities in text. Consider the follow-
ing possibilities for a noun phrase in a text corpus:
Wikipedia Entity: (e.g., “Computer Science,”
“South America,” “apple juice”) - Entity linking
techniques can identify and type these.
Non-Wikipedia, Non-Entity: (e.g., “strange
things,” “Early studies,” “A link”) - Our classifier
from Section 4 is able to filter these.
Non-Wikipedia, Entity: (e.g., “Safflower oil,”
“prune juice,” “Amazon UK”) - We identify these
as entities, then propagate semantic types to them.
Our technique finds that “Safflower oil” occurs with
high weight relations such as “is sometimes used to
treat” and “can be substituted for,” making it similar
to linked entities such as “Phentermine” and “Dan-
delion,” and then correctly predicts semantic types
including food ingredient and medical treatment.
</bodyText>
<subsectionHeader confidence="0.981237">
6.1 Typed Question Answering
</subsectionHeader>
<bodyText confidence="0.999871285714286">
From our set of 15 million assertions, we found and
typed many non-Wikipedia entities. In food while
Wikipedia has “crab meat,” we find it is missing oth-
ers such as “rabbit meat” and “goat milk.” In job ti-
tles it has “scientist” and “lawyer,” but we find it is
missing “PhD student,” “fashion designer,” and oth-
ers. We find many of the people and employers not
</bodyText>
<page confidence="0.993277">
901
</page>
<bodyText confidence="0.999151277777778">
prominent enough for Wikipedia.
One application of this research is to increase the
yield of applications such as Typed Question An-
swering (Buscaldi and Rosso, 2006). For example,
consider the query “What computer game is a lot of
fun?” A search for assertions matching “* is a lot
of fun” in the data yields around 1,000 results such
as “camping,” “David Sedaris” and “Hawaii.” En-
tity linking allows us to identify just the computer
games in Wikipedia that match the query, such as
“Civilization.” However, around 400 query matches
could not be entity linked. Our noun-phrase clas-
sifier filters out non-entities such as “actual play,”
“Just this” and “Two kids.” After predicting types
for the matches that did not get filtered, we find ad-
ditional non-Wikipedia computer games that match
the query, including “Cooking Dash,” “Delicious
Deluxe” and “Slingo Supreme.”
</bodyText>
<sectionHeader confidence="0.999828" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999995193548387">
An area we are continuing to improve the system
on is textual ambiguity. For example, an unlinkable
noun phrase might simultaneously be the name of a
film, a car, and a person. Instead of outputting that
the noun phrase holds all of those types, a stronger
output would be to realize that the noun phrase is
ambiguous, determine how many senses it has, and
determine which sense is being referred to in each
instance. We have ideas for how to detect ambiguous
entities using mutual exclusion (Carlson, 2010) and
functional relations. For example, if we predict that
a noun phrase has film and car types but we also
observe in our linked instances that these types are
mutually exclusive, then this is good evidence that
the noun phrase refers to multiple terms.
We also plan to continue improving our tech-
niques, as there is still plenty of room for improve-
ment on both subtasks. For detecting new entities,
we are interested in seeing if timestamped Twitter
data could be analyzed to increase both recall and
precision. For predicting semantic types, (Kozareva
et al., 2011) proposed additional techniques which
we have not fully explored. Also, we can incor-
porate additional signals such as shared term heads
when they are available, in order to help find terms
that are likely to share types. Last, we would like
to feed back our system output to improve system
performance. For example, non-entity noun phrases
that make it to the typing step might lead to particu-
lar predicted type distributions that indicate an error
occurred earlier in the process.
</bodyText>
<sectionHeader confidence="0.997826" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999183375">
In this paper we showed that while entity linking
cannot link to entities outside of Wikipedia, once a
large text corpus has been entity linked, the presence
and content of the existing links can be leveraged to
help detect and semantically type the non-Wikipedia
entities as well. We designed techniques for de-
tecting whether unlinkable noun phrases are entities,
and if they are, then propagating semantic types to
them from the linked entities. In our evaluations, we
showed that our techniques achieve statistically sig-
nificant improvement over baseline methods.
Our research here takes initial steps toward a fu-
ture where the vast universe of entities that are not
prominent enough to include in manually-authored
knowledge bases is analyzed automatically instead
of being left behind.
</bodyText>
<sectionHeader confidence="0.996923" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998761">
We thank Stephen Soderland, Xiao Ling, and the
three anonymous reviewers for their helpful feed-
back on earlier drafts. This research was sup-
ported in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, and DARPA contract FA8750-
09-C-0179, and carried out at the University of
Washington’s Turing Center.
</bodyText>
<sectionHeader confidence="0.987397" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7435345">
Razvan Bunescu and Marius Pas¸ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association of Computational Linguis-
tics (EACL).
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).
Andrew Carlson. 2010. Coupled Semi-Supervised
Learning. Ph.D. thesis, Carnegie Mellon University.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP.
</reference>
<page confidence="0.981163">
902
</page>
<reference confidence="0.999741443396227">
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Proceedings of COL-
ING.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An
experimental study. In Artificial Intelligence.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine Reading. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI).
James A. Evans and Jacob G. Foster. 2011. Metaknowl-
edge. In Science.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity disam-
biguation to arbitrary Web text. In IJCAI-09 Workshop
on User-contributed Knowledge and Artificial Intelli-
gence (WikiAI09).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of CIKM.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009.
Toward completeness in concept extraction and classi-
fication. In Proceedings of EMNLP.
Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via related
instances. In Proceedings of EMNLP.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in text. In Proceedings of KDD.
Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang.
2007. Fine-grained named entity recognition and rela-
tion extraction for question answering. In Proceedings
of SIGIR.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference on
Artificial Intelligence (AAAI).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, The Google Books
Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,
Peter Norvig, Jon Orwant, and Steven Pinker. 2010.
Quantitative analysis of culture using millions of digi-
tized books. In Science.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17h ACM Inter-
national Conference on Information and Knowledge
Management (CIKM).
Marius Pas¸ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the ACM
International Conference on Information and Knowl-
edge Management (CIKM).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings of
HLT-NAACL.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Danuta Ploch. 2011. Exploring entity relations for
named entity disambiguation. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL).
Willard Van Orman Quine. 1948. On what there is. In
Review of Metaphysics.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL).
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
Satoshi Sekine and Chikashi Nobata. 2004. Definition,
dictionaries and tagger for extended named entity hier-
archy. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC).
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas¸ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of EMNLP.
Partha Pratim Talukdar, Derry Tanti Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of rela-
tional facts. In Proceedings of WSDM.
Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit
Chaudhuri. 2012. Targeted disambiguation of ad-hoc,
homogeneous sets of named entities. In Proceedings
of the 21st International World Wide Web Conference
(WWW).
Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic changes of words over centuries.
In Workshop on Detecting and Exploiting Cultural Di-
versity on the Social Web.
</reference>
<page confidence="0.999065">
903
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965493">
<title confidence="0.999864">No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities</title>
<author confidence="0.999907">Thomas Lin</author>
<author confidence="0.999907">Oren Mausam</author>
<affiliation confidence="0.999834">Computer Science &amp; University of</affiliation>
<address confidence="0.9991">Seattle, WA 98195,</address>
<email confidence="0.975921">mausam,</email>
<abstract confidence="0.999549714285714">Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association of Computational Linguistics (EACL).</booktitle>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association of Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>Mining knowledge from wikipedia for the question answering task.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="33063" citStr="Buscaldi and Rosso, 2006" startWordPosition="5515" endWordPosition="5518">s semantic types including food ingredient and medical treatment. 6.1 Typed Question Answering From our set of 15 million assertions, we found and typed many non-Wikipedia entities. In food while Wikipedia has “crab meat,” we find it is missing others such as “rabbit meat” and “goat milk.” In job titles it has “scientist” and “lawyer,” but we find it is missing “PhD student,” “fashion designer,” and others. We find many of the people and employers not 901 prominent enough for Wikipedia. One application of this research is to increase the yield of applications such as Typed Question Answering (Buscaldi and Rosso, 2006). For example, consider the query “What computer game is a lot of fun?” A search for assertions matching “* is a lot of fun” in the data yields around 1,000 results such as “camping,” “David Sedaris” and “Hawaii.” Entity linking allows us to identify just the computer games in Wikipedia that match the query, such as “Civilization.” However, around 400 query matches could not be entity linked. Our noun-phrase classifier filters out non-entities such as “actual play,” “Just this” and “Two kids.” After predicting types for the matches that did not get filtered, we find additional non-Wikipedia co</context>
</contexts>
<marker>Buscaldi, Rosso, 2006</marker>
<rawString>Davide Buscaldi and Paolo Rosso. 2006. Mining knowledge from wikipedia for the question answering task. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
</authors>
<title>Coupled Semi-Supervised Learning.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="34286" citStr="Carlson, 2010" startWordPosition="5722" endWordPosition="5723">mes that match the query, including “Cooking Dash,” “Delicious Deluxe” and “Slingo Supreme.” 7 Future Work An area we are continuing to improve the system on is textual ambiguity. For example, an unlinkable noun phrase might simultaneously be the name of a film, a car, and a person. Instead of outputting that the noun phrase holds all of those types, a stronger output would be to realize that the noun phrase is ambiguous, determine how many senses it has, and determine which sense is being referred to in each instance. We have ideas for how to detect ambiguous entities using mutual exclusion (Carlson, 2010) and functional relations. For example, if we predict that a noun phrase has film and car types but we also observe in our linked instances that these types are mutually exclusive, then this is good evidence that the noun phrase refers to multiple terms. We also plan to continue improving our techniques, as there is still plenty of room for improvement on both subtasks. For detecting new entities, we are interested in seeing if timestamped Twitter data could be analyzed to increase both recall and precision. For predicting semantic types, (Kozareva et al., 2011) proposed additional techniques </context>
</contexts>
<marker>Carlson, 2010</marker>
<rawString>Andrew Carlson. 2010. Coupled Semi-Supervised Learning. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1584" citStr="Cucerzan, 2007" startWordPosition="243" endWordPosition="244"> unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering. 1 Introduction A key challenge in machine reading (Etzioni et al., 2006) is to identify the entities mentioned in text, and associate them with appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always example noun phrases status “apple juice” “orange juice” “prune juice” “wheatgrass juice” “radiation exposure” “workplace stress” “asbestos exposure” “financial stress” “IJCAI” “OOPSLA” “EMNLP” “ICAPS” Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011)</context>
<context position="5006" citStr="Cucerzan, 2007" startWordPosition="770" endWordPosition="771">entities from arbitrary noun phrases, utilizing features derived from Google Books ngrams. • We adapt and scale instance-to-instance class propagation in order to associate types with non-Wikipedia entities. • We implement and evaluate our methods, empirically verifying improvement over appropriate baselines. 2 Background In this section we provide an overview of entity linking, how we entity link our data set, and describe how our problem and approach differ from related areas such as NER and Web extraction. 2.1 Entity Linking Given text, the task of entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009) is to identify the Wikipedia entities within the text, and mark them with which Wikipedia entity they correspond to. Entity linking elevates us from plain text into meaningful entities that have properties, semantic types, and relationships with each other. Other entity catalogs can be used in place of Wikipedia, especially in domain-specific contexts, but general purpose linking systems all use Wikipedia because of its broad general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed </context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="6323" citStr="Dredze et al., 2010" startWordPosition="990" endWordPosition="993">es not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The REVERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by REVERB (Fader et al., 2011).2 REVERB already filters out relative pronouns, WHO-adverbs, and existential “there” noun phra</context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An experimental study.</title>
<date>2005</date>
<journal>In Artificial Intelligence.</journal>
<contexts>
<context position="8745" citStr="Etzioni et al., 2005" startWordPosition="1379" endWordPosition="1382"> detection step. Fine-grained NER (Sekine and Nobata, 2004; Lee et al., 2007) has studied scaling NER to up to 200 semantic types. This differs from our semantic typing of unlinked entities because our approach assumes access to corpora-level relationships between a large set of linked entities (with semantic types) and the unlinked entities. As a result we are able to propagate 1,339 Freebase semantic types from the linked entities to the unlinked entities, which is substantially more types than fine-grained NER. 2.3 Extracting Entity Sets There is a line of research in using Web extraction (Etzioni et al., 2005) and entity set expansion (Pantel et al., 2009) to extract lists of typed entities from the Web (e.g., a list of every city). Our problem instead focuses on determining whether any individual noun phrase is an entity, and what semantic types it holds. Given a noun phrase representing a person name, we return that this is a person entity even if it is not in a list of people names harvested from the Web. 3 Architecture Our goal is: given (1) a large set of linked assertions L and (2) a large set of unlinked assertions U, for each unlinkable noun phrase subject n E U, predict: (1) whether n is a</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the Web: An experimental study. In Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
</authors>
<title>Machine Reading.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="1181" citStr="Etzioni et al., 2006" startWordPosition="176" endWordPosition="179">nce the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering. 1 Introduction A key challenge in machine reading (Etzioni et al., 2006) is to identify the entities mentioned in text, and associate them with appropriate background information such as their type. Consider the sentence “Some people think that pineapple juice is good for vitamin C.” To analyze this sentence, a machine should know that “pineapple juice” refers to a beverage, while “vitamin C” refers to a nutrient. Entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always example noun phrases status “apple juice” “orange juice” “prune juice”</context>
</contexts>
<marker>Etzioni, Banko, Cafarella, 2006</marker>
<rawString>Oren Etzioni, Michele Banko, and Michael J. Cafarella. 2006. Machine Reading. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Evans</author>
<author>Jacob G Foster</author>
</authors>
<date>2011</date>
<publisher>Metaknowledge. In Science.</publisher>
<contexts>
<context position="12600" citStr="Evans and Foster, 2011" startWordPosition="2060" endWordPosition="2063">ressions that are not entities. We plan to use this signal to help separate the entities from the non-entities. 4.1 Classifier Features We use the Google Books ngram corpus (Michel et al., 2010), which contains timestamped usage of 1-grams through 5-grams in millions of digitized books for each year from 1500 to 2007.3 We use ngram match count values from case-insensitive matching. To avoid sparsity anomalies we observed in years before 1740, we use the data from 1740 onward. While it has not been used for our task before, this corpus is a rich resource that enables reasoning about knowledge (Evans and Foster, 2011) and 3available at http://books.google.com/ngrams/datasets understanding semantic changes of words over time (Wijaya and Yeniterzi, 2011). Talukdar et al. (2012) recently used it to effectively temporally scope relational facts. Our first feature is the slope of the least-squares best fit line for usage over time. For example, if a term appears 25 times in books in 1950, 30 times in 1951, ..., 100 times in 2007, then we compute the straight line that best fits {(1950, 25), (1951, 31), ..., (2007, 100)}, and examine the slope. We have observed cases of non-entity noun phrases (e.g., Figure 1) h</context>
</contexts>
<marker>Evans, Foster, 2011</marker>
<rawString>James A. Evans and Jacob G. Foster. 2011. Metaknowledge. In Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Scaling Wikipedia-based named entity disambiguation to arbitrary Web text.</title>
<date>2009</date>
<booktitle>In IJCAI-09 Workshop on User-contributed Knowledge and Artificial Intelligence (WikiAI09).</booktitle>
<contexts>
<context position="7081" citStr="Fader et al., 2009" startWordPosition="1110" endWordPosition="1113"> how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by REVERB (Fader et al., 2011).2 REVERB already filters out relative pronouns, WHO-adverbs, and existential “there” noun phrases that do not make meaningful arguments. We then employ standard entity linking techniques including string matching, prominence priors (Fader et al., 2009), and context matching (Bunescu and Pas¸ca, 2006) to link the noun phrase subjects into Wikipedia. In this manner, we were able to entity link the noun phrase subject of 9,699,967 extractions, while the remaining 5,028,301 extractions had no matches (mostly due to no close string matches). There were 1,401,713 distinct noun phrase subjects in the 5 million extractions that had no matches. These are the unlinkable noun phrases we will study here. 2.2 Named Entity Recognition Named Entity Recognition (NER) is the task of identifying named entities in text. A key difference between our final goal</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2009</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2009. Scaling Wikipedia-based named entity disambiguation to arbitrary Web text. In IJCAI-09 Workshop on User-contributed Knowledge and Artificial Intelligence (WikiAI09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6019" citStr="Fader et al., 2011" startWordPosition="937" endWordPosition="940">contexts, but general purpose linking systems all use Wikipedia because of its broad general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed when using entity linking systems is that despite containing over 3 million entities, Wikipedia does not cover a significant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The REVERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>Tagme: On-the-fly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="2157" citStr="Ferragina and Scaiella, 2010" startWordPosition="324" endWordPosition="327">ntity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always example noun phrases status “apple juice” “orange juice” “prune juice” “wheatgrass juice” “radiation exposure” “workplace stress” “asbestos exposure” “financial stress” “IJCAI” “OOPSLA” “EMNLP” “ICAPS” Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1 (Wang et al., 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlink</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. 2010. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
</authors>
<title>Toward completeness in concept extraction and classification.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20739" citStr="Hovy et al., 2009" startWordPosition="3441" endWordPosition="3444"> instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L U U|, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) </context>
</contexts>
<marker>Hovy, Kozareva, Riloff, 2009</marker>
<rawString>Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009. Toward completeness in concept extraction and classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Konstantin Voevodski</author>
<author>ShangHua Teng</author>
</authors>
<title>Class label enhancement via related instances.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4015" citStr="Kozareva et al., 2011" startWordPosition="617" endWordPosition="620">14 July 2012. c�2012 Association for Computational Linguistics ing semantic types is a challenge because of the diversity of entity types in general text. In our experiments, we utilized the Freebase type system, which contains over 1,000 semantic types. The first part of this paper proposes a novel method for detecting entities by observing that entities often have different usage-over-time characteristics than non-entities. Evaluation shows that our method achieves 24% relative accuracy gain over a NER baseline. The second part of this paper shows how instance-to-instance class propagation (Kozareva et al., 2011) can be adapted and scaled to semantically type general noun-phrase entities using types from linked entities, by leveraging over one million different possible textual relations. Contributions of our research include: • We motivate and introduce the unlinkable noun phrase problem, which extends previous work in entity linking. • We propose a novel method for discriminating entities from arbitrary noun phrases, utilizing features derived from Google Books ngrams. • We adapt and scale instance-to-instance class propagation in order to associate types with non-Wikipedia entities. • We implement </context>
<context position="20407" citStr="Kozareva et al., 2011" startWordPosition="3385" endWordPosition="3388">a article, and also that it occurs with textual relations such as “has already announced” and “has released updates for.” For each Wikipedialinked entity in L, we further look up its exact set of Freebase types.4 From U we obtain the set of textual relations that each e E E is in the domain of. We now have a large set of class-labeled instances (all entities in L), a large set of unlabeled instances (E), and a method to connect the unlabeled instances with the class-labeled instances (via any shared textual relations), so we cast this task as an instance-to-instance class propagation problem (Kozareva et al., 2011) for propagating class labels from labeled to unlabeled instances. We build on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L U U|, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of ot</context>
<context position="34854" citStr="Kozareva et al., 2011" startWordPosition="5816" endWordPosition="5819">iguous entities using mutual exclusion (Carlson, 2010) and functional relations. For example, if we predict that a noun phrase has film and car types but we also observe in our linked instances that these types are mutually exclusive, then this is good evidence that the noun phrase refers to multiple terms. We also plan to continue improving our techniques, as there is still plenty of room for improvement on both subtasks. For detecting new entities, we are interested in seeing if timestamped Twitter data could be analyzed to increase both recall and precision. For predicting semantic types, (Kozareva et al., 2011) proposed additional techniques which we have not fully explored. Also, we can incorporate additional signals such as shared term heads when they are available, in order to help find terms that are likely to share types. Last, we would like to feed back our system output to improve system performance. For example, non-entity noun phrases that make it to the typing step might lead to particular predicted type distributions that indicate an error occurred earlier in the process. 8 Conclusion In this paper we showed that while entity linking cannot link to entities outside of Wikipedia, once a la</context>
</contexts>
<marker>Kozareva, Voevodski, Teng, 2011</marker>
<rawString>Zornitsa Kozareva, Konstantin Voevodski, and ShangHua Teng. 2011. Class label enhancement via related instances. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in text.</title>
<date>2009</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="5054" citStr="Kulkarni et al., 2009" startWordPosition="777" endWordPosition="780">ilizing features derived from Google Books ngrams. • We adapt and scale instance-to-instance class propagation in order to associate types with non-Wikipedia entities. • We implement and evaluate our methods, empirically verifying improvement over appropriate baselines. 2 Background In this section we provide an overview of entity linking, how we entity link our data set, and describe how our problem and approach differ from related areas such as NER and Web extraction. 2.1 Entity Linking Given text, the task of entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009) is to identify the Wikipedia entities within the text, and mark them with which Wikipedia entity they correspond to. Entity linking elevates us from plain text into meaningful entities that have properties, semantic types, and relationships with each other. Other entity catalogs can be used in place of Wikipedia, especially in domain-specific contexts, but general purpose linking systems all use Wikipedia because of its broad general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed when using entity linking systems is that despit</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in text. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changki Lee</author>
<author>Yi-Gyu Hwang</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Fine-grained named entity recognition and relation extraction for question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="8201" citStr="Lee et al., 2007" startWordPosition="1289" endWordPosition="1292"> (NER) is the task of identifying named entities in text. A key difference between our final goals and NER is that in the con2available at http://reverb.cs.washington.edu 894 text of entity linking and Wikipedia, there are many more entities than just the named entities. For example, “apple juice” and “television” are Wikipedia entities (with Wikipedia articles), but are not traditional named entities. Still, as named entities do comprise a sizable portion of our unlinkable noun phrases, we compare against a NER baseline in our entity detection step. Fine-grained NER (Sekine and Nobata, 2004; Lee et al., 2007) has studied scaling NER to up to 200 semantic types. This differs from our semantic typing of unlinked entities because our approach assumes access to corpora-level relationships between a large set of linked entities (with semantic types) and the unlinked entities. As a result we are able to propagate 1,339 Freebase semantic types from the linked entities to the unlinked entities, which is substantially more types than fine-grained NER. 2.3 Extracting Entity Sets There is a line of research in using Web extraction (Etzioni et al., 2005) and entity set expansion (Pantel et al., 2009) to extra</context>
</contexts>
<marker>Lee, Hwang, Jang, 2007</marker>
<rawString>Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang. 2007. Fine-grained named entity recognition and relation extraction for question answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Daniel S Weld</author>
</authors>
<title>Fine-grained entity recognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="21729" citStr="Ling and Weld, 2012" startWordPosition="3601" endWordPosition="3604"> model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and FIGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding the textual relations that e is in the domain of, (2) 4data available at http://download.freebase.com/wex 898 Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase “Sun Microsystems.” We predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linked entities that are also in the domain of those textual relations, and (3) observing the</context>
<context position="31330" citStr="Ling and Weld, 2012" startWordPosition="5241" endWordPosition="5244">assertion like “The Four Seasons is located in Hamamatsu,” our entity linker (and other entity linkers we tried) prefer linking “The Four Seasons” to Vivaldi’s music composition rather than the hotel chain. We are then unable to recover music composition type from relations like “is located in.” Our algorithm relies on accurate entity linking in L, but there is a precision/recall tradeoff to consider here because it also benefits from higher coverage of entities and relations in L. As a general reference for performance of state-of-the-art fine-grained entity classification, the FIGER system (Ling and Weld, 2012) for classifying into 112 types reported F1 scores ranging from 0.471 to 0.693 in their experiments. It is important to note that these numbers are not directly comparable to us because they used different data, different (and fewer) types, and different evaluation methodology. 6 Discussion This paper presented an approach for working with non-Wikipedia entities in text. Consider the following possibilities for a noun phrase in a text corpus: Wikipedia Entity: (e.g., “Computer Science,” “South America,” “apple juice”) - Entity linking techniques can identify and type these. Non-Wikipedia, Non-</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva P Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
</authors>
<title>The Google Books Team,</title>
<date>2010</date>
<journal>Joseph</journal>
<booktitle>In Science.</booktitle>
<contexts>
<context position="12171" citStr="Michel et al., 2010" startWordPosition="1986" endWordPosition="1989">ctually has even higher slope, but was adjusted left to fit in this figure. time often differ for entities and non-entities. Noun phrase entities that are observed in text going back hundreds of years (e.g., “Europe”) almost all have their own Wikipedia entries, so in unlinkable noun phrase space, the remaining noun phrases that are observed in text going back hundreds of years tend to be all the textual references and expressions that are not entities. We plan to use this signal to help separate the entities from the non-entities. 4.1 Classifier Features We use the Google Books ngram corpus (Michel et al., 2010), which contains timestamped usage of 1-grams through 5-grams in millions of digitized books for each year from 1500 to 2007.3 We use ngram match count values from case-insensitive matching. To avoid sparsity anomalies we observed in years before 1740, we use the data from 1740 onward. While it has not been used for our task before, this corpus is a rich resource that enables reasoning about knowledge (Evans and Foster, 2011) and 3available at http://books.google.com/ngrams/datasets understanding semantic changes of words over time (Wijaya and Yeniterzi, 2011). Talukdar et al. (2012) recently </context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, 2010</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, and Steven Pinker. 2010. Quantitative analysis of culture using millions of digitized books. In Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17h ACM International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="5030" citStr="Milne and Witten, 2008" startWordPosition="772" endWordPosition="776">bitrary noun phrases, utilizing features derived from Google Books ngrams. • We adapt and scale instance-to-instance class propagation in order to associate types with non-Wikipedia entities. • We implement and evaluate our methods, empirically verifying improvement over appropriate baselines. 2 Background In this section we provide an overview of entity linking, how we entity link our data set, and describe how our problem and approach differ from related areas such as NER and Web extraction. 2.1 Entity Linking Given text, the task of entity linking (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009) is to identify the Wikipedia entities within the text, and mark them with which Wikipedia entity they correspond to. Entity linking elevates us from plain text into meaningful entities that have properties, semantic types, and relationships with each other. Other entity catalogs can be used in place of Wikipedia, especially in domain-specific contexts, but general purpose linking systems all use Wikipedia because of its broad general coverage, and to leverage its article texts and link structure during the linking process. A problem we observed when using entity linkin</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. Learning to link with wikipedia. In Proceedings of the 17h ACM International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</booktitle>
<marker>Pas¸ca, 2004</marker>
<rawString>Marius Pas¸ca. 2004. Acquisition of categorized named entities for web search. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="21297" citStr="Pantel and Ravichandran, 2004" startWordPosition="3530" endWordPosition="3533">ly shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L U U|, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and FIGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding the</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danuta Ploch</author>
</authors>
<title>Exploring entity relations for named entity disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6337" citStr="Ploch, 2011" startWordPosition="994" endWordPosition="995">icant number of entities. This occurs with entities that are not prominent enough to have their own dedicated article and with entities that are very new. For example, Facebook has over 600 million users, and each of them could be considered an entity. The REVERB extractor (Fader et al., 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. While recent research has used NIL features to determine whether they are being asked to link an entity not in Wikipedia (Dredze et al., 2010; Ploch, 2011), there has been no research on whether given noun phrases that are unlinkable (for not being in Wikipedia) are entities, and how to make them usable if they are. Our goal is to address this problem of learning whether non-Wikipedia noun phrases are entities, and assigning semantic types to them to make them useful. We begin with a corpus of 15 million “(noun phrase subject, textual relation, noun phrase object)” assertions from the Web that were extracted by REVERB (Fader et al., 2011).2 REVERB already filters out relative pronouns, WHO-adverbs, and existential “there” noun phrases that do no</context>
</contexts>
<marker>Ploch, 2011</marker>
<rawString>Danuta Ploch. 2011. Exploring entity relations for named entity disambiguation. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willard Van Orman Quine</author>
</authors>
<title>On what there is. In Review of Metaphysics.</title>
<date>1948</date>
<contexts>
<context position="10452" citStr="Quine, 1948" startWordPosition="1697" endWordPosition="1698">) uses L and U to predict the semantic types for each entity e E E. Figure 1: Usage over time in Google books for the noun phrase “Prices quoted” (e.g., from “Prices quoted are for 2 adults”) which is not an entity. Figure 2: Usage over time for the unlinkable noun phrase “Soluble fibre,” which is an entity. The best fit line has steeper slope compared to Figure 1. 4 Detecting Unlinkable Entities This first task takes in any unlinkable noun phrase and outputs whether it is an entity. There is a long history of discussion in analytic philosophy literature on the question of what exists (e.g., (Quine, 1948)). We adopt a more pragmatic view, defining an “entity” as a noun phrase that could have a Wikipedia-style article if there were no notability or newness considerations, and which would have semantic types. We are interested in entities that could help populate an entity store. “EMNLP 2012” is an example of an entity, while “The method” and “12 cats” are examples of noun phrases that are not entities. This is challenging because at a surface level, many entities and non-entities look similar: “Sex and the City” is an entity, while “John and David” is not. “Eminem” is an entity, while “Youd” (a</context>
</contexts>
<marker>Quine, 1948</marker>
<rawString>Willard Van Orman Quine. 1948. On what there is. In Review of Metaphysics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Inducing finegrained semantic classes via hierarchical and collective classification.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="21630" citStr="Rahman and Ng, 2010" startWordPosition="3586" endWordPosition="3589"> approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and FIGER (Ling and Weld, 2012) recently used an adapted perceptron for multi-class multi-label classification into 112 types. 5.1 Algorithm Given an entity e, our algorithm involves: (1) finding the textual relations that e is in the domain of, (2) 4data available at http://download.freebase.com/wex 898 Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase “Sun Microsystems.” We predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) fi</context>
</contexts>
<marker>Rahman, Ng, 2010</marker>
<rawString>Altaf Rahman and Vincent Ng. 2010. Inducing finegrained semantic classes via hierarchical and collective classification. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="17304" citStr="Ratinov and Roth, 2009" startWordPosition="2856" endWordPosition="2859">, so we asked the annotators to choose “unclear” when there was any doubt. There were 151 bigrams that both annotators believed to be very clear labels, including 69 that both annotators labeled as entities, 70 that both annotators labeled as non-entities, and 12 with label disagreement. Cohen’s kappa was 0.84, indicating excellent agreement. Our experiment is now to separate the 69 clear entities from the 70 clear non-entities. For experiment baselines we use the majority class baseline MAJ, as well as a Named Entity Recognition baseline NER. For NER we used the Illinois Named Entity Tagger (Ratinov and Roth, 2009) on the highest setting (that achieved 90.5 F1 score on the CoNLL03 shared task). NER expects a sentence, so we use the longest assertion in the corpus that the noun phrase was observed in. We evaluate several combinations of our features to test dif897 ferent aspects of our system: Slope uses only Slope, PUF uses PercentProperCaps + UsageSinceYear + Frequency, and ALL uses all features. We evaluate using the WEKA J48 Decision Tree on default settings, with leave-one-out cross validation. Table 2 shows the results. MAJ correctly classifies 50.4% of instances, NER correctly classifies 63.3% and</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2184" citStr="Ratinov et al., 2011" startWordPosition="329" endWordPosition="332">2006; Cucerzan, 2007) addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always example noun phrases status “apple juice” “orange juice” “prune juice” “wheatgrass juice” “radiation exposure” “workplace stress” “asbestos exposure” “financial stress” “IJCAI” “OOPSLA” “EMNLP” “ICAPS” Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1 (Wang et al., 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: G</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Chikashi Nobata</author>
</authors>
<title>Definition, dictionaries and tagger for extended named entity hierarchy.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="8182" citStr="Sekine and Nobata, 2004" startWordPosition="1285" endWordPosition="1288"> Named Entity Recognition (NER) is the task of identifying named entities in text. A key difference between our final goals and NER is that in the con2available at http://reverb.cs.washington.edu 894 text of entity linking and Wikipedia, there are many more entities than just the named entities. For example, “apple juice” and “television” are Wikipedia entities (with Wikipedia articles), but are not traditional named entities. Still, as named entities do comprise a sizable portion of our unlinkable noun phrases, we compare against a NER baseline in our entity detection step. Fine-grained NER (Sekine and Nobata, 2004; Lee et al., 2007) has studied scaling NER to up to 200 semantic types. This differs from our semantic typing of unlinked entities because our approach assumes access to corpora-level relationships between a large set of linked entities (with semantic types) and the unlinked entities. As a result we are able to propagate 1,339 Freebase semantic types from the linked entities to the unlinked entities, which is substantially more types than fine-grained NER. 2.3 Extracting Entity Sets There is a line of research in using Web extraction (Etzioni et al., 2005) and entity set expansion (Pantel et </context>
</contexts>
<marker>Sekine, Nobata, 2004</marker>
<rawString>Satoshi Sekine and Chikashi Nobata. 2004. Definition, dictionaries and tagger for extended named entity hierarchy. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Fernando Pereira</author>
</authors>
<title>Experiments in graph-based semi-supervised learning methods for class-instance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="21109" citStr="Talukdar and Pereira, 2010" startWordPosition="3502" endWordPosition="3505"> on the recent work of Kozareva et al. (2011), and adapt their approach to leverage the scale and resources of our scenario. While they use only one type of edge between instances, namely shared presence in the high precision DAP pattern (Hovy et al., 2009), our final system uses 1.3 million textual relations from |L U U|, corresponding to 1.3 million potential edge types. Their evaluation involved only 20 semantic classes, while we use all 1,339 Freebase types covered by our entities in L. There is a rich history of other approaches for predicting semantic types. (Talukdar et al., 2008) and (Talukdar and Pereira, 2010) model relationships between instances and classes, but our unlinked entities do not come with any class information. Pattern-based approaches (Pas¸ca, 2004; Pantel and Ravichandran, 2004) are popular, but (Kozareva et al., 2011) notes that they “are constraint to the information matched by the pattern and often suffer from recall,” meaning that they do not cover many instances. Classifiers have also been trained for finegrained semantic typing, but for noticeably fewer types than we work with. (Rahman and Ng, 2010) studied hierarchical and collective classification using 92 types, and FIGER (</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>Partha Pratim Talukdar and Fernando Pereira. 2010. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly supervised acquisition of labeled class instances using graph random walks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Derry Tanti Wijaya</author>
<author>Tom Mitchell</author>
</authors>
<title>Coupled temporal scoping of relational facts.</title>
<date>2012</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="12761" citStr="Talukdar et al. (2012)" startWordPosition="2079" endWordPosition="2082">ram corpus (Michel et al., 2010), which contains timestamped usage of 1-grams through 5-grams in millions of digitized books for each year from 1500 to 2007.3 We use ngram match count values from case-insensitive matching. To avoid sparsity anomalies we observed in years before 1740, we use the data from 1740 onward. While it has not been used for our task before, this corpus is a rich resource that enables reasoning about knowledge (Evans and Foster, 2011) and 3available at http://books.google.com/ngrams/datasets understanding semantic changes of words over time (Wijaya and Yeniterzi, 2011). Talukdar et al. (2012) recently used it to effectively temporally scope relational facts. Our first feature is the slope of the least-squares best fit line for usage over time. For example, if a term appears 25 times in books in 1950, 30 times in 1951, ..., 100 times in 2007, then we compute the straight line that best fits {(1950, 25), (1951, 31), ..., (2007, 100)}, and examine the slope. We have observed cases of non-entity noun phrases (e.g., Figure 1) having lower slopes than entity noun phrases (e.g., Figure 2). Note that we do not normalize match counts by yearly total frequency, but we do normalize counts fo</context>
</contexts>
<marker>Talukdar, Wijaya, Mitchell, 2012</marker>
<rawString>Partha Pratim Talukdar, Derry Tanti Wijaya, and Tom Mitchell. 2012. Coupled temporal scoping of relational facts. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi Wang</author>
<author>Kaushik Chakrabarti</author>
<author>Tao Cheng</author>
<author>Surajit Chaudhuri</author>
</authors>
<title>Targeted disambiguation of ad-hoc, homogeneous sets of named entities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International World Wide Web Conference (WWW).</booktitle>
<contexts>
<context position="2515" citStr="Wang et al., 2012" startWordPosition="384" endWordPosition="387">“EMNLP” “ICAPS” Table 1: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al., 2011) both link “vitamin C” correctly, but link “pineapple juice” to “pineapple.” “Pineapple juice” is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.1 (Wang et al., 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: Given a noun phrase that does not link into Wikipedia, return whether it is an entity, as well its fine-grained semantic types. Deciding if a nonWikipedia noun phrase is an entity is challenging because many of them are not entities (e.g., “Some people,” “an addition” and “nearly half”). Predict1The same problem occurs with Freeba</context>
</contexts>
<marker>Wang, Chakrabarti, Cheng, Chaudhuri, 2012</marker>
<rawString>Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit Chaudhuri. 2012. Targeted disambiguation of ad-hoc, homogeneous sets of named entities. In Proceedings of the 21st International World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derry Tanti Wijaya</author>
<author>Reyyan Yeniterzi</author>
</authors>
<title>Understanding semantic changes of words over centuries.</title>
<date>2011</date>
<booktitle>In Workshop on Detecting and Exploiting Cultural Diversity on the Social Web.</booktitle>
<contexts>
<context position="12737" citStr="Wijaya and Yeniterzi, 2011" startWordPosition="2075" endWordPosition="2078">es We use the Google Books ngram corpus (Michel et al., 2010), which contains timestamped usage of 1-grams through 5-grams in millions of digitized books for each year from 1500 to 2007.3 We use ngram match count values from case-insensitive matching. To avoid sparsity anomalies we observed in years before 1740, we use the data from 1740 onward. While it has not been used for our task before, this corpus is a rich resource that enables reasoning about knowledge (Evans and Foster, 2011) and 3available at http://books.google.com/ngrams/datasets understanding semantic changes of words over time (Wijaya and Yeniterzi, 2011). Talukdar et al. (2012) recently used it to effectively temporally scope relational facts. Our first feature is the slope of the least-squares best fit line for usage over time. For example, if a term appears 25 times in books in 1950, 30 times in 1951, ..., 100 times in 2007, then we compute the straight line that best fits {(1950, 25), (1951, 31), ..., (2007, 100)}, and examine the slope. We have observed cases of non-entity noun phrases (e.g., Figure 1) having lower slopes than entity noun phrases (e.g., Figure 2). Note that we do not normalize match counts by yearly total frequency, but w</context>
</contexts>
<marker>Wijaya, Yeniterzi, 2011</marker>
<rawString>Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Understanding semantic changes of words over centuries. In Workshop on Detecting and Exploiting Cultural Diversity on the Social Web.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000191">
<title confidence="0.973521">
A Novel Discriminative Framework for Sentence-Level Discourse Analysis
</title>
<author confidence="0.571254">
Shafiq Joty and Giuseppe Carenini and Raymond T. Ng
</author>
<email confidence="0.659637">
{rjoty, carenini, rng}@cs.ubc.ca
</email>
<affiliation confidence="0.9981245">
Department of Computer Science
University of British Columbia
</affiliation>
<address confidence="0.552767">
Vancouver, BC, V6T 1Z4, Canada
</address>
<sectionHeader confidence="0.975495" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998942">
We propose a complete probabilistic discrim-
inative framework for performing sentence-
level discourse analysis. Our framework com-
prises a discourse segmenter, based on a bi-
nary classifier, and a discourse parser, which
applies an optimal CKY-like parsing algo-
rithm to probabilities inferred from a Dynamic
Conditional Random Field. We show on two
corpora that our approach outperforms the
state-of-the-art, often by a wide margin.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999495791666667">
Automatic discourse analysis has been shown to
be critical in several fundamental Natural Lan-
guage Processing (NLP) tasks including text gener-
ation (Prasad et al., 2005), summarization (Marcu,
2000b), sentence compression (Sporleder and Lap-
ata, 2005) and question answering (Verberne et al.,
2007). Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988), one of the most influential
theories of discourse, posits a tree representation of
a discourse, known as a Discourse Tree (DT), as
exemplified by the sample DT shown in Figure 1.
The leaves of a DT correspond to contiguous atomic
text spans, also called Elementary Discourse Units
(EDUs) (three in the example). The adjacent EDUs
are connected by a rhetorical relation (e.g., ELAB-
ORATION), and the resulting larger text spans are
recursively also subject to this relation linking. A
span linked by a rhetorical relation can be either
a NUCLEUS or a SATELLITE depending on how
central the message is to the author. Discourse anal-
ysis in RST involves two subtasks: (i) breaking the
text into EDUs (known as discourse segmentation)
and (ii) linking the EDUs into a labeled hierarchical
tree structure (known as discourse parsing).
</bodyText>
<figureCaption confidence="0.999474">
Figure 1: Discourse structure of a sentence in RST-DT.
</figureCaption>
<bodyText confidence="0.999970416666667">
Previous studies on discourse analysis have been
quite successful in identifying what machine learn-
ing approaches and what features are more useful for
automatic discourse segmentation and parsing (Sori-
cut and Marcu, 2003; Subba and Eugenio, 2009; du-
Verle and Prendinger, 2009). However, all the pro-
posed solutions suffer from at least one of the fol-
lowing two key limitations: first, they make strong
independence assumptions on the structure and the
labels of the resulting DT, and typically model the
construction of the DT and the labeling of the rela-
tions separately; second, they apply a greedy, sub-
optimal algorithm to build the structure of the DT.
In this paper, we propose a new sentence-level
discourse parser that addresses both limitations. The
crucial component is a probabilistic discriminative
parsing model, expressed as a Dynamic Conditional
Random Field (DCRF) (Sutton et al., 2007). By
representing the structure and the relation of each
discourse tree constituent jointly and by explicitly
capturing the sequential and hierarchical dependen-
cies between constituents of a discourse tree, our
DCRF model does not make any independence as-
sumption among these properties. Furthermore, our
</bodyText>
<page confidence="0.968957">
904
</page>
<note confidence="0.7681715">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 904–915, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999934217391304">
parsing model supports a bottom-up parsing algo-
rithm which is non-greedy and provably optimal.
The discourse parser assumes that the input text
has been already segmented into EDUs. As an addi-
tional contribution of this paper, we propose a novel
discriminative approach to discourse segmentation
that not only achieves state-of-the-art performance,
but also reduces the time and space complexities by
using fewer features. Notice that the combination
of our segmenter with our parser forms a complete
probabilistic discriminative framework for perform-
ing sentence-level discourse analysis.
Our framework was tested in a series of experi-
ments. The empirical evaluation indicates that our
approach to discourse parsing outperforms the state-
of-the-art by a wide margin. Moreover, we show this
to be the case on two very different genres: news ar-
ticles and instructional how-to-do manuals.
In the rest of the paper, after discussing related
work, we present our discourse parser. Then, we
describe our segmenter. The experiments and the
corpora we used are described next, followed by a
discussion of the key results and some error analysis.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999821714285714">
Automatic discourse analysis has a long history;
see (Stede, 2011) for a detailed overview. Sori-
cut and Marcu (2003) present the publicly available
SPADE1 system that comes with probabilistic mod-
els for sentence-level discourse segmentation and
parsing based on lexical and syntactic features de-
rived from the lexicalized syntactic tree of a sen-
tence. Their parsing algorithm finds the most proba-
ble DT for a sentence, where the probabilities of the
constituents are estimated by their parsing model.
A constituent (e.g., ATTRIBUTION-NS[(1,2),3] in
Figure 1) in a DT has two components, first, the la-
bel denoting the relation and second, the structure
indicating which spans are being linked by the rela-
tion. The nuclearity statuses of the spans are built
into the relation labels (e.g., NS[(1,2),3] means that
span (1,2) is the NUCLEUS and it comes before
span 3 which is the SATELLITE). SPADE is limited
in several ways. It makes an independence assump-
tion between the label and the structure while mod-
eling a constituent, and it ignores the sequential and
</bodyText>
<footnote confidence="0.930361">
1http://www.isi.edu/licensed-sw/spade/
</footnote>
<bodyText confidence="0.99997975">
hierarchical dependencies between the constituents
in the parsing model. Furthermore, SPADE relies
only on lexico-syntactic features, and it follows a
generative approach to estimate the model param-
eters for the segmentation and the parsing models.
SPADE was trained and tested on the RST-DT cor-
pus (Carlson et al., 2002), which contains human-
annotated discourse trees for news articles.
Subsequent research addresses the question of
how much syntax one really needs in discourse
analysis. Sporleder and Lapata (2005) focus on
discourse chunking, comprising the two subtasks
of segmentation and non-hierarchical nuclearity as-
signment. More specifically, they examine whether
features derived via part of speech (POS) and chunk
taggers would be sufficient for these purposes. Their
results on RST-DT turn out to be comparable to
SPADE without using any features from the syntac-
tic tree. Later, Fisher and Roark (2007) demonstrate
over 4% absolute “performance gain” in segmenta-
tion, by combining the features extracted from the
syntactic tree with the ones derived via taggers. Us-
ing quite a large number of features in a binary log-
linear model they achieve the state-of-the-art seg-
mentation performance on the RST-DT test set.
On the different genre of instructional manuals,
Subba and Eugenio (2009) propose a shift-reduce
parser that relies on a classifier to find the appro-
priate relation between two text segments. Their
classifier is based on Inductive Logic Programming
(ILP), which learns first-order logic rules from a
large set of features including the linguistically rich
compositional semantics coming from a semantic
parser. They show that the compositional seman-
tics improves the classification performance. How-
ever, their discourse parser implements a greedy ap-
proach (hence not optimal) and their classifier disre-
gards the sequence and hierarchical dependencies.
Using RST-DT, Hernault et al. (2010) present
the HILDA system that comes with a segmenter
and a parser based on Support Vector Machines
(SVMs). The segmenter is a binary SVM classi-
fier which relies on the same lexico-syntactic fea-
tures used in SPADE, but with more context. The
discourse parser builds a DT iteratively utilizing two
SVM classifiers in each iteration: (i) a binary classi-
fier decides which of the two adjacent spans to link,
and (ii) a multi-class classifier then connects the se-
</bodyText>
<page confidence="0.997731">
905
</page>
<bodyText confidence="0.999919230769231">
lected spans with the appropriate relation. They use
a very large set of features in their parser. How-
ever, taking a radically-greedy approach, they model
structure and relations separately, and ignore the se-
quence dependencies in their models.
Recently, there has been an explosion of interest
in Conditional Random Fields (CRFs) (Lafferty et
al., 2001) for solving structured output classification
problems, with many successful applications in NLP
including syntactic parsing (Finkel et al., 2008), syn-
tactic chunking (Sha and Pereira, 2003) and dis-
course chunking (Ghosh et al., 2011) in Penn Dis-
course Treebank (Prasad et al., 2008). CRFs being a
discriminative approach to sequence modeling (i.e.,
directly models the conditional p(y|x, 0)), have sev-
eral advantages over its generative counterparts such
as Hidden Markov Models (HMMs) and Markov
Random Fields (MRFs), which first model the joint
p(y, x|0), then infer the conditional p(y|x, 0)). Key
advantages include the ability to incorporate arbi-
trary overlapping local and global features, and the
ability to relax strong independence assumptions. It
has been advocated that CRFs are generally more
accurate since they do not “waste effort” modeling
complex distributions (i.e., p(x)) that are not rele-
vant for the target task (Murphy, 2012).
</bodyText>
<sectionHeader confidence="0.988278" genericHeader="method">
3 The Discourse Parser
</sectionHeader>
<bodyText confidence="0.9995276">
Assuming that a sentence is already segmented into
a sequence of EDUs e1, e2,... en manually or by an
automatic segmenter (see Section 4), the discourse
parsing problem is to decide which spans to con-
nect (i.e., structure of the DT) and which relations
(i.e., labels of the internal nodes) to use in the pro-
cess of building the hierarchical DT. To build the
DTs effectively, a common assumption is that they
are binary trees (Soricut and Marcu, 2003; duVerle
and Prendinger, 2009). That is, multi-nuclear re-
lations (e.g., LIST, JOINT, SEQUENCE) involving
more than two EDUs are mapped to a hierarchi-
cal right-branching binary tree. For example, a flat
LIST (e1, e2, e3, e4) is mapped to a right-branching
binary tree LIST(e1, LIST(e2, LIST(e3, e4))).
Our discourse parser has two components. The
first component, the parsing model, assigns a proba-
bility to every possible DT. The second component,
the parsing algorithm, finds the most probable DT
among the candidate discourse trees.
</bodyText>
<subsectionHeader confidence="0.994583">
3.1 Parsing Model
</subsectionHeader>
<bodyText confidence="0.9969033">
A DT can be represented as a set of constituents
of the form R[i, m, j], which denotes a rhetorical
relation R that holds between the span containing
EDUs i through m, and the span containing EDUs
m+1 through j. For example, the DT in Figure 1
can be written as {ELABORATION-NS[1,1,2],
ATTRIBUTION-NS[1,2,3]}. Notice that a rela-
tion R also indicates the nuclearity assignments
of the spans being connected, which can be one
of NUCLEUS-SATELLITE (NS), SATELLITE-
NUCLEUS (SN) and NUCLEUS-NUCLEUS (NN).
Given the model parameters O and a candi-
date DT T, for all the constituents c in T, our
parsing model estimates the conditional probabil-
ity P(c|C, 0), which specifies the joint probabil-
ity of the relation R and the structure [i, m, j]
associated with the constituent c, given that c
has a set of sub-constituents C. For instance,
for the DT shown in Figure 1, our model
would estimate P(R′[1,1, 2]|0), P(R′[2, 2, 3]|0),
</bodyText>
<equation confidence="0.888251">
P(R′[1, 2,3]|R″[1,1, 2], 0) etc. for all R′ and R″ranging on the set of relations. In what follows we
</equation>
<bodyText confidence="0.999952608695652">
describe our probabilistic parsing model to compute
all these conditional probabilities P(c|C, 0). We
will demonstrate how our approach not only models
the structure and the relation jointly, but it also cap-
tures linear sequence dependencies and hierarchical
dependencies between constituents of a DT.
Our novel parsing model is the Dynamic Condi-
tional Random Field (DCRF) (Sutton et al., 2007)
shown in Figure 2. A DCRF is a generalization
of linear-chain CRFs to represent complex interac-
tion between labels, such as when performing mul-
tiple labeling tasks on the same sequence. The ob-
served nodes Wj in the figure are the text spans.
A text span can be either an EDU or a concatena-
tion of a sequence of EDUs. The structure nodes
Sj∈{0,1} in the figure represent whether text spans
Wj_1 and Wj should be connected or not. The re-
lation nodes Rj∈{1... M} denote the discourse re-
lation between spans Wj_1 and Wj, given that M is
the total number of relations in our relation set. No-
tice that we now model the structure and the relation
jointly and also take the sequential dependencies be-
tween adjacent constituents into consideration.
</bodyText>
<page confidence="0.996838">
906
</page>
<figureCaption confidence="0.999829">
Figure 2: A Dynamic CRF as a discourse parsing model.
</figureCaption>
<bodyText confidence="0.99841625">
We can obtain the conditional probabilities of
the constituents (i.e., P(c|C, O)) of all candidate
DTs for a sentence by applying the DCRF pars-
ing model recursively at different levels, and by
computing the posterior marginals of the relation-
structure pairs. To illustrate, consider the example
sentence in Figure 1 where we have three EDUs
e1, e2 and e3. The DCRF model for the first
level is shown in Figure 3(a), where the (observed)
EDUs are the spans in the span sequence. Given
this model, we obtain the probabilities of the con-
stituents R[1,1, 2] and R[2, 2, 3] by computing the
posterior marginals P(R2, 52=1|e1, e2, e3, O) and
P(R3, 53=1|e1, e2, e3, O), respectively. At the sec-
ond level (see Figure 3(b)), there are two possi-
ble span sequences (e1:2, e3) and (e1, e2:3). In the
first sequence, EDUs e1 and e2 are linked into
a larger span, and in the second one, EDUs e2
and e3 are connected into a larger span. We ap-
ply our DCRF model to the two possible span se-
quences and obtain the probabilities of the con-
stituents R[1, 2,3] and R[1,1, 3] by computing
the posterior marginals P(R3, 53=1|e1:2, e3, O) and
P(R2:3, 52:3=1|e1, e2:3, O), respectively.
</bodyText>
<figureCaption confidence="0.801098">
Figure 3: DCRF model applied to the sequences at differ-
ent levels in the example in Fig. 1. (a) A sequence at the
first level (b) Two possible sequences at the second level.
</figureCaption>
<bodyText confidence="0.999855566666666">
To further clarify the process, let us as-
sume that the sentence contains four EDUs
e1, e2, e3 and e4. At the first level (Fig-
ure 4(a)), there is only one possible span se-
quence to which we apply our DCRF model.
We obtain the probabilities of the constituents
R[1,1, 2], R[2, 2, 3] and R[3, 3, 4] by computing the
posterior marginals P(R2, 52=1|e1, e2, e3, e4, O),
P(R3, 53=1|e1, e2, e3, e4, O) and P(R4, 54=1|e1,
e2, e3, e4, O), respectively. At the second level
(Figure 4(b)), there are three possible sequences
(e1:2, e3, e4), (e1, e2:3, e4) and (e1, e2, e3:4). When
the DCRF model is applied to the sequence
(e1:2, e3, e4), we obtain the probabilities of the
constituent R[1, 2,3] by computing the posterior
marginal P(R3, 53=1|e1:2, e3, e4, O). Likewise, the
posterior marginals P(R2:3, 52:3=1|e1, e2:3, e4, O)
and P(R4, 54=1|e1, e2:3, e4, O) in the DCRF model
applied to the sequence (e1, e2:3, e4) represents
the probabilities of the constituents R[1,1, 3]
and R[2, 3, 4], respectively. Similarly, we at-
tain the probabilities of the constituent R[2, 2, 4]
from the DCRF model applied to the sequence
(e1, e2, e3:4) by computing the posterior marginal
P(R3:4, 53:4=1|e1, e2, e3:4, O). At the third level
(Figure 4(c)), there are three possible sequences
(e1:3, e4), (e1, e2:4) and (e1:2, e3:4), to which we ap-
ply our model and acquire the probabilities of the
constituents R[1, 3, 4], R[1,1, 4] and R[1, 2,4] by
computing their respective posterior marginals.
</bodyText>
<figureCaption confidence="0.697942">
Figure 4: DCRF model applied to the sequences at differ-
ent levels of a discourse tree. (a) A sequence at the first
level, (b) Three possible sequences at the second level,
(c) Two possible sequences at the third level.
</figureCaption>
<bodyText confidence="0.999922">
Our DCRF model is designed using MALLET
(McCallum, 2002). In order to avoid overfitting we
regularize the DCRF model with 12 regularization
and learn the model parameters using the limited-
memory BFGS (L-BFGS) fitting algorithm. Since
exact inference can be intractable in DCRF models,
</bodyText>
<page confidence="0.98627">
907
</page>
<bodyText confidence="0.988186">
we perform approximate inference (to compute the
posterior marginals) using tree-based reparameteri-
zation (Wainwright et al., 2002).
</bodyText>
<subsectionHeader confidence="0.542145">
3.1.1 Features Used in the Parsing Model
</subsectionHeader>
<bodyText confidence="0.99995565">
Crucial to parsing performance is the set of fea-
tures used, as summarized in Table 1. Note that
these features are defined on two consecutive spans
Wj−1 and Wj of a span sequence. Most of the fea-
tures have been explored in previous studies. How-
ever, we improve some of these as explained below.
Organizational features encode useful informa-
tion about the surface structure of a sentence as
shown by (duVerle and Prendinger, 2009). We mea-
sure the length of the spans in terms of the number of
EDUs and tokens in it. However, in order to better
adjust to the length variations, rather than comput-
ing their absolute numbers in a span, we choose to
measure their relative numbers with respect to their
total numbers in the sentence. For example, in a sen-
tence containing three EDUs, a span containing two
of these EDUs will have a relative EDU number of
0.67. We also measure the distances of the spans
from the beginning and to the end of the sentence in
terms of the number of EDUs.
</bodyText>
<sectionHeader confidence="0.815296" genericHeader="method">
8 organizational features
</sectionHeader>
<bodyText confidence="0.87207825">
Relative number of EDUs in span 1 and span 2.
Relative number of tokens in span 1 and span 2.
Distances of span 1 in EDUs to the beginning and to the end.
Distances of span 2 in EDUs to the beginning and to the end.
</bodyText>
<sectionHeader confidence="0.962141" genericHeader="method">
8 N-gram features
</sectionHeader>
<bodyText confidence="0.943281125">
Beginning and end lexical N-grams in span 1.
Beginning and end lexical N-grams in span 2.
Beginning and end POS N-grams in span 1.
Beginning and end POS N-grams in span 2.
5 dominance set features
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two text spans.
</bodyText>
<sectionHeader confidence="0.897305" genericHeader="method">
2 contextual features
</sectionHeader>
<bodyText confidence="0.62842">
Previous and next feature vectors.
</bodyText>
<sectionHeader confidence="0.534455" genericHeader="method">
2 substructure features
</sectionHeader>
<bodyText confidence="0.722665">
Root nodes of the left and right rhetorical subtrees.
</bodyText>
<tableCaption confidence="0.996485">
Table 1: Features used in the DCRF parsing model.
</tableCaption>
<bodyText confidence="0.999835368421053">
Discourse connectives (e.g., because, but), when
present, signal rhetorical relations between two text
segments (Knott and Dale, 1994; Marcu, 2000a).
However, previous studies (e.g., Hernault et al.
(2010), Biran and Rambow (2011)) suggest that an
empirically acquired lexical N-gram dictionary is
more effective than a fixed list of connectives, since
this approach is domain independent and capable
of capturing non-lexical cues such as punctuations.
To build the lexical N-gram dictionary empirically
from the training corpus we consider the first and
last N tokens (NE{1, 2}) of each span and rank
them according to their mutual information2 with
the two labels, Structure and Relation. Intuitively,
the most informative cues are not only the most fre-
quent, but also the ones that are indicative of the la-
bels in the training data (Blitzer, 2008). In addition
to the lexical N-grams we also encode POS tags of
the first and last N tokens (NE{1, 2}) as features.
</bodyText>
<figureCaption confidence="0.998402">
Figure 5: A discourse segmented lexicalized syntactic
tree. Boxed nodes form the dominance set D.
</figureCaption>
<bodyText confidence="0.998781">
Dominance set extracted from the Discourse Seg-
mented Lexicalized Syntactic Tree (DS-LST) (Sori-
cut and Marcu, 2003) has been shown to be a very
effective feature in SPADE. Figure 5 shows the DS-
LST for our running example (see Figure 1 and 3).
In a DS-LST, each EDU except the one with the root
node must have a head node NH that is attached to
an attachment node NA residing in a separate EDU.
A dominance set D (shown at the bottom of Figure 5
for our example) contains these attachment points of
the EDUs in a DS-LST. In addition to the syntactic
and lexical information of the head and attachment
nodes, each element in D also represents a domi-
nance relationship between the EDUs involved. The
EDU with NA dominates the EDU with NH. In or-
</bodyText>
<footnote confidence="0.99155">
2In contrast, HILDA ranks the N-grams by frequencies.
</footnote>
<page confidence="0.992107">
908
</page>
<bodyText confidence="0.999977875">
der to extract dominance set features for two consec-
utive spans eZ:j and ej+1:k, we first compute D from
the DS-LST of the sentence. We then extract the
element from D that holds across the EDUs j and
j + 1. In our running example, for the spans e1 and
e2 (Figure 3(a)), the relevant dominance set element
is (1, efforts/NP)&gt;(2, to/S). We encode the syntac-
tic labels and lexical heads of NH and NA and the
dominance relationship (i.e., which of the two spans
is dominating) as features in our model.
We also incorporate more contextual information
by including the above features computed for the
neighboring span pairs in the current feature vector.
We incorporate hierarchical dependencies be-
tween constituents in a DT by means of the sub-
structure features. For the two adjacent spans eZ:j
and ej+1:k, we extract the roots of the rhetorical
subtrees spanning over eZ:j (left) and ej+1:k (right).
In our example (see Figure 1 and Figure 3 (b)),
the root of the rhetorical subtree spanning over e1:2
is ELABORATION-NS. However, this assumes the
presence of a labeled DT which is not the case when
we apply the parser to a new sentence. This problem
can be easily solved by looping twice through build-
ing the model and the parsing algorithm (described
below). We first build the model without considering
the substructure features. Then we find the optimal
DT employing our parsing algorithm. This interme-
diate DT will now provide labels for the substruc-
tures. Next we can build a new, more accurate model
by including the substructure features, and run again
the parsing algorithm to find the final optimal DT.
</bodyText>
<subsectionHeader confidence="0.999633">
3.2 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.99998825">
Our parsing model above assigns a conditional prob-
ability to every possible DT constituent for a sen-
tence, the job of the parsing algorithm is to find the
most probable DT. Formally, this can be written as,
</bodyText>
<equation confidence="0.916446">
DT∗ = argmax DTP(DT|O)
</equation>
<bodyText confidence="0.971009772727273">
Our discourse parser implements a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse of a sentence using dynamic pro-
gramming; see (Jurafsky and Martin, 2008) for a
description. Specifically, with n number of EDUs
in a sentence, we use the upper-triangular por-
tion of the n x n Dynamic Programming Table
(DPT). The cell [i, j] in the DPT represents the
span containing EDUs i through j and stores the
probability of a constituent R[i, m, j], where m =
argmax Z≤k≤jP(R[i, k, j]).
In contrast to HILDA which implements a greedy
algorithm, our approach finds a DT that is glob-
ally optimal. Our approach is also different from
SPADE’s implementation. SPADE first finds the
tree structure that is globally optimal, then it assigns
the most probable relations to the internal nodes.
More specifically, the cell [i, j] in SPADE’s DPT
stores the probability of a constituent R[i, m, j],
where m = argmax Z≤k≤jP([i, k, j]). Disregard-
ing the relation label R while building the DPT, this
approach may find a tree that is not globally optimal.
</bodyText>
<sectionHeader confidence="0.9959" genericHeader="method">
4 The Discourse Segmenter
</sectionHeader>
<bodyText confidence="0.99998948">
Our discourse parser above assumes that the input
sentences have been already segmented into EDUs.
Since it has been shown that discourse segmentation
is a primary source of inaccuracy for discourse pars-
ing (Soricut and Marcu, 2003), we have developed
our own segmenter, that not only achieves state-of-
the-art performance as shown later, but also reduces
the time complexity by using fewer features.
Our segmenter implements a binary classifier to
decide for each word (except the last word) in a sen-
tence, whether to put an EDU boundary after that
word. We use a Logistic Regression (LR) (i.e., dis-
criminative) model with l2 regularization and learn
the model parameters using the L-BFGS algorithm,
which gives quadratic convergence rate. To avoid
overfitting, we use 5-fold cross validation to learn
the regularization strength parameter from the train-
ing data. We also use a simple bagging technique
(Breiman, 1996) to deal with the sparsity of bound-
ary tags. Note that, our first attempt at this task im-
plemented a linear-chain CRF model to capture the
sequence dependencies between the tags in a dis-
criminative way. However, the binary LR classifier,
using the same features, not only outperforms the
CRF model, but also reduces the space complexity.
</bodyText>
<subsectionHeader confidence="0.989396">
4.1 Features Used in the Segmentation Model
</subsectionHeader>
<bodyText confidence="0.9998914">
Our set of features for discourse segmentation are
mostly inspired from previous studies but used in a
novel way as we describe below.
Our first subset of features which we call SPADE
features, includes the lexico-syntactic patterns ex-
</bodyText>
<page confidence="0.993922">
909
</page>
<bodyText confidence="0.999990652173913">
tracted from the lexicalized syntactic tree for the
given sentence. These features replicates the fea-
tures used in SPADE, but used in a discriminative
way. To decide on an EDU boundary after a token
wk, we find the lowest constituent in the lexicalized
syntactic tree that spans over tokens wi ... wj such
that i&lt;_k&lt;j. The production that expands this con-
stituent in the tree and its different variations, form
the feature set. For example in Figure 5, the produc-
tion NP(efforts)—*PRP$(its)NNS(efforts)TS(to) and
its different variations depending on whether they
include the lexical heads and how many non-
terminals (up to two) to consider before and after
the potential EDU boundary (T), are used to de-
termine the existence of a boundary after the word
efforts (see (Fisher and Roark, 2007) for details).
SPADE uses these features in a generative way,
meaning that, it inserts an EDU boundary if the rela-
tive frequency (i.e., Maximum Likelihood Estimate
(MLE)) of a potential boundary given the production
in the training corpus is greater than 0.5. If the pro-
duction has not been observed frequently enough, it
uses its other variations to perform further smooth-
ing. In contrast, we compute the MLE estimates for
a production and its other variations, and use those
as features with/without binarizing the values.
Shallow syntactic parse (or Chunk) and POS tags
have been shown to possess valuable cues for dis-
course segmentation (Fisher and Roark, 2007). For
example, it is less likely that an EDU boundary oc-
curs within a chunk. We, therefore, annotate the to-
kens of a sentence with chunk and POS tags by a
state-of-the-art tagger3 and encode these as features.
EDUs are normally multi-word strings. Thus, a
token near the beginning or end of a sentence is un-
likely to be the end of a segment. Therefore, for each
token we include its relative position in the sentence
and distances to the beginning and end as features.
It is unlikely that two consecutive tokens are
tagged with EDU boundaries. We incorporate con-
textual information for a token by including the
above features computed for its neighboring tokens.
We also experimented with different N-gram
(NE11, 2,31) features extracted from the token se-
quence, POS sequence and chunk sequence. How-
ever, since such features did not improve the seg-
</bodyText>
<footnote confidence="0.750609">
3http://cogcomp.cs.illinois.edu/page/software
</footnote>
<bodyText confidence="0.9827985">
mentation accuracy on the development set, they
were excluded from our final set of features.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.92883">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999982933333333">
To demonstrate the generality of our model, we ex-
periment with two different genres. First, we use the
standard RST-DT corpus (Carlson et al., 2002) that
contains discourse annotations for 385 Wall Street
Journal news articles from the Penn Treebank (Mar-
cus et al., 1994). Second, we use the Instructional
corpus developed by Subba and Eugenio (2009) that
contains discourse annotations for 176 instructional
how-to-do manuals on home-repair.
The RST-DT corpus is partitioned into a training
set of 347 documents (7673 sentences) and a test set
of 38 documents (991 sentences), and 53 documents
(1208 sentences) have been (doubly) annotated by
two human annotators, based on which we compute
the human agreement. We use the human-annotated
syntactic trees from Penn Treebank to train SPADE
in our experiments using RST-DT as done in (Sori-
cut and Marcu, 2003). We extracted a sentence-level
DT from a document-level DT by finding the subtree
that exactly spans over the sentence. By our count,
7321 sentences in the training set, 951 sentences
in the test set and 1114 sentences in the doubly-
annotated set have a well-formed DT in RST-DT.
The Instructional corpus contains 3430 sentences in
total, out of which 3032 have a well-formed DT.
This forms our sentence-level corpora for discourse
parsing. However, the existence of a well-formed
DT in not a necessity for discourse segmentation,
therefore, we do not exclude any sentence in our dis-
course segmentation experiments.
</bodyText>
<subsectionHeader confidence="0.993637">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999419777777778">
We perform our experiments on discourse pars-
ing in RST-DT with the 18 coarser relations (see
Figure 6) defined in (Carlson and Marcu, 2001)
and also used in SPADE and HILDA. By attach-
ing the nuclearity statuses (i.e., NS, SN, NN) to
these relations we get 39 distinct relations4. Our
experiments on the Instructional corpus consider
the same 26 primary relations (e.g., GOAL:ACT,
CAUSE:EFFECT, GENERAL-SPECIFIC) used in
</bodyText>
<footnote confidence="0.964946">
4Not all relations take all the possible nuclearity statuses.
</footnote>
<page confidence="0.996568">
910
</page>
<bodyText confidence="0.99979692">
(Subba and Eugenio, 2009) and also treat the re-
versals of non-commutative relations as separate re-
lations. That is, PREPARATION-ACT and ACT-
PREPARATION are two different relations. Attach-
ing the nuclearity statuses to these relations gives 70
distinct relations in the Instructional corpus.
We use SPADE as our baseline model and apply
the same modifications to its default setting as de-
scribed in (Fisher and Roark, 2007), which delivers
improved performance. Specifically, in testing, we
replace the Charniak parser (Charniak, 2000) with a
more accurate reranking parser (Charniak and John-
son, 2005). We use the reranking parser in all our
models to generate the syntactic trees. This parser
was trained on the sections of the Penn Treebank not
included in the test set. For a fair comparison, we ap-
ply the same canonical lexical head projection rules
(Magerman, 1995; Collins, 2003) to lexicalize the
syntactic trees as done in SPADE and HILDA. Note
that, all the previous works described in Section 2,
report their models’ performance on a particular test
set of a specific corpus. To compare our results with
the previous studies, we test our models on those
specific test sets. In addition, we show more general
performance based on 10-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.998757">
5.3 Parsing based on Manual Segmentation
</subsectionHeader>
<bodyText confidence="0.999048210526316">
First, we present the results of our discourse parser
based on manual segmentation. The parsing perfor-
mance is assessed using the unlabeled (i.e., span)
and labeled (i.e., nuclearity, relation) precision, re-
call and F-score as described in (Marcu, 2000b, page
143). For brevity, we report only the F-scores in Ta-
ble 2. Notice that, our parser (DCRF) consistently
outperforms SPADE (SP) on the RST-DT test set5.
Especially, on relation labeling, which is the hardest
among the three tasks, we get an absolute F-score
improvement of 9.5%, which represents a relative
error rate reduction of 29.3%. Our F-score of 77.1
in relation labeling is also close to the human agree-
ment (i.e., F-score of 83.0) on the doubly-annotated
data. Our results on the RST-DT test set are con-
sistent with the mean scores over 10-folds, when we
perform 10-fold cross validation on RST-DT.
The improvement is even larger on the Instruc-
tional corpus, where we compare our mean results
</bodyText>
<footnote confidence="0.910296">
5The improvements are statistically significant (p &lt; 0.01).
</footnote>
<bodyText confidence="0.889028333333333">
over 10-folds with the results reported in Subba and
Eugenio (S&amp;E) (2009) on a test set6, giving ab-
solute F-score improvements of 4.8%, 15.5% and
10.6% in span, nuclearity and relations, respectively.
Our parser reduces the errors by 67.6%, 54.6% and
28.6% in span, nuclearity and relations, respectively.
</bodyText>
<table confidence="0.989487833333333">
RST-DT Instructional
Test set 10-fold Doubly S&amp;E 10-fold
Scores SP DCRF DCRF Human ILP DCRF
Span 93.5 94.6 93.7 95.7 92.9 97.7
Nuc. 85.8 86.9 85.2 90.4 71.8 87.2
Rel. 67.6 77.1 75.4 83.0 63.0 73.6
</table>
<tableCaption confidence="0.999494">
Table 2: Parsing results using manual segmentation.
</tableCaption>
<bodyText confidence="0.994806058823529">
If we compare the performance of our model on
the two corpora, we see that our model is more accu-
rate in finding the right tree structure (see Span) on
the Instructional corpus. This may be due to the fact
that sentences in the Instructional domain are rela-
tively short and contain fewer EDUs than sentences
in the News domain, thus making it easier to find
the right tree structure. However, when we compare
the performance on the relation labeling task, we ob-
serve a decrease on the Instructional corpus. This
may be due to the small amount of data available for
training and the imbalanced distribution of a large
number of discourse relations in this corpus.
To analyze the features, Table 3 presents the pars-
ing results on the RST-DT test set using different
subsets of features. Every new subset of features
appears to improve the accuracy. More specifically,
when we add the organizational features with the
dominance set features (see 52), we get about 2%
absolute improvement in nuclearity and relations.
With N-gram features (53), the gain is even higher;
6% in relations and 3.5% in nuclearity, demonstrat-
ing the utility of the N-gram features. This is con-
sistent with the findings of (duVerle and Prendinger,
2009; Schilder, 2002). Including the Contextual fea-
tures (54), we get further 3% and 2.2% improve-
ments in nuclearity and relations, respectively. No-
tice that, adding the substructure features (55) does
not help much in sentence-level parsing, giving only
6Subba and Eugenio (2009) report their results based on an
arbitrary split between a training set and a test set. We asked the
authors for their particular split. However, since we could not
obtain that information, we compare our model’s performance
based on 10-fold cross validation with their reported results.
</bodyText>
<page confidence="0.99536">
911
</page>
<bodyText confidence="0.9995528">
an improvement of 0.8% in relations. Therefore, one
may choose to avoid using this computationally ex-
pensive feature in time-constrained scenarios. How-
ever, in the future, it will be interesting to see its im-
portance in document-level parsing with large trees.
</bodyText>
<table confidence="0.9995655">
Scores S1 S2 S3 S4 S5
Span 91.3 92.1 93.3 94.6 94.6
Nuclearity 78.2 80.3 83.8 86.8 86.9
Relation 66.2 68.1 74.1 76.3 77.1
</table>
<tableCaption confidence="0.945281857142857">
Table 3: Parsing results based on manual segmentation
using different subsets of features on RST-DT test set.
Feature subsets S1 = {Dominance set}, S2 = {Dominance
set, Organizational}, S3 = {Dominance set, Organiza-
tional, N-gram}, S4 = {Dominance set, Organizational,
N-gram, Contextual}, S5 (all) = {Dominance set, Orga-
nizational, N-gram, Contextual, Substructure}.
</tableCaption>
<bodyText confidence="0.9979709">
by the reranking parser. Our model delivers abso-
lute F-score improvements of 3.8% and 8.1% on the
RST-DT and the Instructional corpora, respectively,
which is statistically significant in both cases (p &lt;
3.0e-06). However, when we compare our results on
the two corpora, we observe a substantial decrease in
performance on the Instructional corpus. This could
be due to a smaller amount of data in this corpus and
the inaccuracies in the syntactic parser and taggers,
which are trained on news articles.
</bodyText>
<table confidence="0.9880675">
RST-DT Instructional
Test Set 10-fold 10-fold 10-fold
HIL SP F&amp;R LR SP LR SP LR
P 77.9 83.8 91.3 88.0 83.7 87.5 65.1 73.9
R 70.6 86.8 89.7 92.3 86.2 89.9 82.8 89.7
F 74.1 85.2 90.5 90.1 84.9 88.7 72.8 80.9
</table>
<tableCaption confidence="0.999671">
Table 4: Segmentation results of different models.
</tableCaption>
<subsectionHeader confidence="0.986328">
5.4 Evaluation of the Discourse Segmenter
</subsectionHeader>
<bodyText confidence="0.99991168">
We evaluate the segmentation accuracy with respect
to the intra-sentential segment boundaries following
(Fisher and Roark, 2007). Specifically, if a sen-
tence contains n EDUs, which corresponds to n − 1
intra-sentence segment boundaries, we measure the
model’s ability to correctly identify these n − 1
boundaries. Human agreement for this task is quite
high (F-score of 98.3) on RST-DT.
Table 4 shows the results of different models in
(P)recision, (R)ecall, and (F)-score on the two cor-
pora. We compare our model’s (LR) results with
HILDA (HIL), SPADE (SP) and the results reported
in Fisher and Roark (F&amp;R) (2007) on the RST-DT
test set. HILDA gives the weakest performance7.
Our results are also much better than SPADE8, with
an absolute F-score improvement of 4.9%, and com-
parable to the results of F&amp;R, even though we use
fewer features. Furthermore, we perform 10-fold
cross validation on both corpora and compare with
SPADE. However, SPADE does not come with a
training module for its segmenter. We reimple-
mented this module and verified it on the RST-DT
test set. Due to the lack of human-annotated syntac-
tic trees in the Instructional corpus, we train SPADE
in this corpus using the syntactic trees produced
</bodyText>
<footnote confidence="0.999923333333333">
7Note that, the high segmentation accuracy reported in (Her-
nault et al., 2010) is due to a less stringent evaluation metric.
8The improvements are statistically significant (p&lt;2.4e-06)
</footnote>
<subsectionHeader confidence="0.983">
5.5 Parsing based on Automatic Segmentation
</subsectionHeader>
<bodyText confidence="0.999911454545454">
In order to evaluate our full system, we feed our
discourse parser the output of our discourse seg-
menter. Table 5 shows the F-score results. We com-
pare our results with SPADE on the RST-DT test set.
We achieve absolute F-score improvements of 3.6%,
3.4% and 7.4% in span, nuclearity and relation, re-
spectively. These improvements are statistically sig-
nificant (p&lt;0.001). Our system, therefore, reduces
the errors by 15.5%, 11.4%, and 17.6% in span, nu-
clearity and relations, respectively. These results are
also consistent with the mean results over 10-folds.
</bodyText>
<table confidence="0.980634166666667">
RST-DT Instructional
Test set 10-fold 10-fold
Scores SPADE DCRF DCRF DCRF
Span 76.7 80.3 78.7 71.9
Nuclearity 70.2 73.6 72.2 64.3
Relation 58.0 65.4 64.2 54.8
</table>
<tableCaption confidence="0.999642">
Table 5: Parsing results using automatic segmentation.
</tableCaption>
<bodyText confidence="0.999787444444444">
For the Instructional corpus, the last column of
Table 5 shows the mean 10-fold cross validation re-
sults. We cannot compare with S&amp;E because no re-
sults were reported using an automatic segmenter.
However, it is interesting to observe how much our
full system is affected by an automatic segmenter
on both RST-DT and the Instructional corpus (see
Table 2 and Table 5). Nevertheless, taking into ac-
count the segmentation results in Table 4, this is
</bodyText>
<page confidence="0.991153">
912
</page>
<bodyText confidence="0.9999648">
not surprising because previous studies (Soricut and
Marcu, 2003) have already shown that automatic
segmentation is the primary impediment to high ac-
curacy discourse parsing. This demonstrates the
need for a more accurate segmentation model in the
Instructional genre. A promising future direction
would be to apply effective domain adaptation meth-
ods (e.g., easyadapt (Daume, 2007)) to improve
the segmentation performance in the Instructional
domain by leveraging the rich data in RST-DT.
</bodyText>
<subsectionHeader confidence="0.975451">
5.6 Error Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.999725727272727">
The results in Table 2 suggest that given a manually
segmented discourse, our sentence-level discourse
parser finds the unlabeled (i.e., span) discourse tree
and assigns the nuclearity statuses to the spans at a
performance level close to human annotators. We,
therefore, look more closely into the performance of
our parser on the hardest task of relation labeling.
Figure 6 shows the confusion matrix for the rela-
tion labeling task using manual segmentation on the
RST-DT test set. The relation labels are ordered ac-
cording to their frequency in the RST-DT training
set and represented by their initial letters. For exam-
ple, EL represents ELABORATION and CA repre-
sents CAUSE. In general, errors can be explained by
two different phenomena acting together: (i) the fre-
quency of the relations in the training data, and (ii)
the semantic (or pragmatic) similarity between the
relations. The most frequent relations (e.g., ELAB-
ORATION) tend to confuse the less frequent ones
(e.g., SUMMARY), and the relations which are se-
mantically similar (e.g., CAUSE, EXPLANATION)
confuse each other, making it hard to distinguish for
the computational models. Notice that, the confu-
sions caused by JOINT appears to be high consid-
ering its frequency. The confusion between JOINT
and TEMPORAL may be due to the fact that both of
these coarser relations9 contain finer relations (i.e.,
list in JOINT and sequence in TEMPORAL), which
are semantically similar, as pointed out by Carlson
and Marcu (2001). The confusion between JOINT
and BACKGROUND may be explained by their dif-
ferent (semantic vs. pragmatic) interpretation in the
RST theory (Stede, 2011, page 85).
</bodyText>
<footnote confidence="0.987046">
9JOINT is actually not a relation, but is characterized by
juxtaposition of two EDUs without a relation.
</footnote>
<figureCaption confidence="0.984446666666667">
Figure 6: Confusion matrix for the relation labels on
the RST-DT test set. Y-axis represents true and X-axis
represents predicted labels. The relation labels are TOPIC-
</figureCaption>
<reference confidence="0.7104855">
COMMENT, EVALUATION, SUMMARY, MANNER-MEANS,
COMPARISON, EXPLANATION, CONDITION, TEMPORAL,
CAUSE, ENABLEMENT, BACKGROUND, CONTRAST, JOINT,
SAME-UNIT, ATTRIBUTION, ELABORATION.
</reference>
<bodyText confidence="0.99961">
Based on these observations we will pursue two
ways to improve our discourse parser. We need a
more robust (e.g., bagging) method to deal with the
imbalanced distribution of relations, along with a
better representation of semantic knowledge. For
example, compositional semantics (Subba and Eu-
genio, 2009) and subjectivity (Somasundaran, 2010)
can be quite relevant for identifying relations.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999348">
In this paper, we have described a complete prob-
abilistic discriminative framework for performing
sentence-level discourse analysis. Experiments indi-
cate that our approach outperforms the state-of-the-
art on two corpora, often by a wide margin.
In ongoing work, we plan to generalize our
DCRF-based parser to multi-sentential text and also
verify to what extent parsing and segmentation can
be jointly performed. A longer term goal is to extend
our framework to also work with graph structures
of discourse, as recommended by several recent dis-
course theories (Wolf and Gibson, 2005). Once we
achieve similar performance on graph structures, we
will perform extrinsic evaluation to determine their
relative utility for various NLP tasks.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999797">
We are grateful to G. Murray, J. CK Cheung, the 3
reviewers and the NSERC CGS-D award.
</bodyText>
<page confidence="0.998199">
913
</page>
<sectionHeader confidence="0.989225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999536942307692">
Or Biran and Owen Rambow. 2011. Identifying Justifi-
cations in Written Dialogs by Classifying Text as Ar-
gumentative. Int. J. Semantic Computing, 5(4):363–
381.
J. Blitzer, 2008. Domain Adaptation of Natural Lan-
guage Processing Systems. PhD thesis, University of
Pennsylvania.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123–140, August.
L. Carlson and D. Marcu. 2001. Discourse Tagging Ref-
erence Manual. Technical Report ISI-TR-545, Univer-
sity of Southern California Information Sciences Insti-
tute.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 173–
180, NJ, USA. ACL.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics Conference, pages 132–139, Seattle, Washington.
ACL.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4):589–637, December.
H. Daume. 2007. Frustratingly Easy Domain Adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
256–263, Prague, Czech Republic. ACL.
D. duVerle and H. Prendinger. 2009. A Novel Discourse
Parser based on Support Vector Machine Classifica-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 665–673, Suntec, Singapore.
ACL.
J. Finkel, A. Kleeman, and C. Manning. 2008. Efficient,
Feature-based, Conditional Random Field Parsing. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 959–967,
Columbus, Ohio, USA. ACL.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmenta-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
488–495, Prague, Czech Republic. ACL.
S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.
2011. Shallow Discourse Parsing with Conditional
Random Fields. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1071–1079, Chiang Mai, Thailand. AFNLP.
H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.
2010. HILDA: A Discourse Parser Using Support
Vector Machine Classification. Dialogue and Dis-
course, 1(3):1–33.
D. Jurafsky and J. Martin, 2008. Speech and Language
Processing, chapter 14. Prentice Hall.
A. Knott and R. Dale. 1994. Using Linguistic Phenom-
ena to Motivate a Set of Coherence Relations. Dis-
course Processes, 18(1):35–62.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Magerman. 1995. Statistical Decision-tree Mod-
els for Parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276–283, Cambridge, Massachusetts. ACL.
W. Mann and S. Thompson. 1988. Rhetorical Structure
Theory: Toward a Functional Theory of Text Organi-
zation. Text, 8(3):243–281.
D. Marcu. 2000a. The Rhetorical Parsing of Unrestricted
Texts: A Surface-based Approach. Computational
Linguistics, 26:395–448.
D. Marcu. 2000b. The Theory and Practice ofDiscourse
Parsing and Summarization. MIT Press, Cambridge,
MA, USA.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313–330.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
K. Murphy. 2012. Machine Learning A Probabilistic
Perspective (Forthcoming, August 2012). MIT Press,
Cambridge, MA, USA.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and
B. Webber. 2005. The Penn Discourse TreeBank as a
Resource for Natural Language Generation. In Pro-
ceedings of the Corpus Linguistics Workshop on Us-
ing Corpora for Natural Language Generation, pages
25–32, Birmingham, U.K.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn Discourse
TreeBank 2.0. In Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 2961–2968, Marrakech, Mo-
rocco. ELRA.
</reference>
<page confidence="0.984304">
914
</page>
<reference confidence="0.999938076923077">
F. Schilder. 2002. Robust Discourse Parsing via Dis-
course Markers, Topicality and Position. Natural Lan-
guage Engineering, 8(3):235–255, June.
F. Sha and F. Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of the
2003 Conference ofthe North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, pages 134–141, Ed-
monton, Canada. ACL.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing Using Syntactic and Lexical Informa-
tion. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 149–156, Edmonton, Canada.
ACL.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 257–264, Vancouver, British
Columbia, Canada. ACL.
M. Stede. 2011. Discourse Processing. Synthesis Lec-
tures on Human Language Technologies. Morgan And
Claypool Publishers, November.
R. Subba and B. Di Eugenio. 2009. An Effective
Discourse Parser that Uses Rich Linguistic Informa-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 566–574, Boulder, Colorado. ACL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting Se-
quence Data. Journal of Machine Learning Research
(JMLR), 8:693–723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings of
the 30th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 735–736, Amsterdam, The Netherlands. ACM.
M. Wainwright, T. Jaakkola, and A. Willsky. 2002. Tree-
based Reparameterization for Approximate Inference
on Loopy Graphs. In Advances in Neural Information
Processing Systems 14, pages 1001–1008. MIT Press.
F. Wolf and E. Gibson. 2005. Representing Discourse
Coherence: A Corpus-Based Study. Computational
Linguistics, 31:249–288, June.
</reference>
<page confidence="0.998695">
915
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949606">
<title confidence="0.998724">A Novel Discriminative Framework for Sentence-Level Discourse Analysis</title>
<author confidence="0.980381">Shafiq Joty</author>
<author confidence="0.980381">Giuseppe Carenini</author>
<author confidence="0.980381">T Raymond</author>
<email confidence="0.984626">carenini,</email>
<affiliation confidence="0.999929">Department of Computer University of British</affiliation>
<address confidence="0.998377">Vancouver, BC, V6T 1Z4, Canada</address>
<abstract confidence="0.998609727272727">We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>EVALUATION COMMENT</author>
<author>MANNER-MEANS SUMMARY</author>
<author>EXPLANATION COMPARISON</author>
<author>TEMPORAL CONDITION</author>
<author>ENABLEMENT CAUSE</author>
<author>CONTRAST BACKGROUND</author>
<author>SAME-UNIT JOINT</author>
<author>ELABORATION ATTRIBUTION</author>
</authors>
<marker>COMMENT, SUMMARY, COMPARISON, CONDITION, CAUSE, BACKGROUND, JOINT, ATTRIBUTION, </marker>
<rawString>COMMENT, EVALUATION, SUMMARY, MANNER-MEANS, COMPARISON, EXPLANATION, CONDITION, TEMPORAL, CAUSE, ENABLEMENT, BACKGROUND, CONTRAST, JOINT, SAME-UNIT, ATTRIBUTION, ELABORATION.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying Justifications in Written Dialogs by Classifying Text as Argumentative.</title>
<date>2011</date>
<journal>Int. J. Semantic Computing,</journal>
<volume>5</volume>
<issue>4</issue>
<pages>381</pages>
<contexts>
<context position="18069" citStr="Biran and Rambow (2011)" startWordPosition="2931" endWordPosition="2934">grams in span 2. 5 dominance set features Syntactic labels of the head node and the attachment node. Lexical heads of the head node and the attachment node. Dominance relationship between the two text spans. 2 contextual features Previous and next feature vectors. 2 substructure features Root nodes of the left and right rhetorical subtrees. Table 1: Features used in the DCRF parsing model. Discourse connectives (e.g., because, but), when present, signal rhetorical relations between two text segments (Knott and Dale, 1994; Marcu, 2000a). However, previous studies (e.g., Hernault et al. (2010), Biran and Rambow (2011)) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the most frequent, but also the ones that are indicative of the labels in the training </context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Or Biran and Owen Rambow. 2011. Identifying Justifications in Written Dialogs by Classifying Text as Argumentative. Int. J. Semantic Computing, 5(4):363– 381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
</authors>
<title>Domain Adaptation of Natural Language Processing Systems.</title>
<date>2008</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18689" citStr="Blitzer, 2008" startWordPosition="3033" endWordPosition="3034">est that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the most frequent, but also the ones that are indicative of the labels in the training data (Blitzer, 2008). In addition to the lexical N-grams we also encode POS tags of the first and last N tokens (NE{1, 2}) as features. Figure 5: A discourse segmented lexicalized syntactic tree. Boxed nodes form the dominance set D. Dominance set extracted from the Discourse Segmented Lexicalized Syntactic Tree (DS-LST) (Soricut and Marcu, 2003) has been shown to be a very effective feature in SPADE. Figure 5 shows the DSLST for our running example (see Figure 1 and 3). In a DS-LST, each EDU except the one with the root node must have a head node NH that is attached to an attachment node NA residing in a separat</context>
</contexts>
<marker>Blitzer, 2008</marker>
<rawString>J. Blitzer, 2008. Domain Adaptation of Natural Language Processing Systems. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="23568" citStr="Breiman, 1996" startWordPosition="3866" endWordPosition="3867">e-art performance as shown later, but also reduces the time complexity by using fewer features. Our segmenter implements a binary classifier to decide for each word (except the last word) in a sentence, whether to put an EDU boundary after that word. We use a Logistic Regression (LR) (i.e., discriminative) model with l2 regularization and learn the model parameters using the L-BFGS algorithm, which gives quadratic convergence rate. To avoid overfitting, we use 5-fold cross validation to learn the regularization strength parameter from the training data. We also use a simple bagging technique (Breiman, 1996) to deal with the sparsity of boundary tags. Note that, our first attempt at this task implemented a linear-chain CRF model to capture the sequence dependencies between the tags in a discriminative way. However, the binary LR classifier, using the same features, not only outperforms the CRF model, but also reduces the space complexity. 4.1 Features Used in the Segmentation Model Our set of features for discourse segmentation are mostly inspired from previous studies but used in a novel way as we describe below. Our first subset of features which we call SPADE features, includes the lexico-synt</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
</authors>
<title>Discourse Tagging Reference Manual.</title>
<date>2001</date>
<tech>Technical Report ISI-TR-545,</tech>
<institution>University of Southern California Information Sciences Institute.</institution>
<contexts>
<context position="28295" citStr="Carlson and Marcu, 2001" startWordPosition="4634" endWordPosition="4637">he training set, 951 sentences in the test set and 1114 sentences in the doublyannotated set have a well-formed DT in RST-DT. The Instructional corpus contains 3430 sentences in total, out of which 3032 have a well-formed DT. This forms our sentence-level corpora for discourse parsing. However, the existence of a well-formed DT in not a necessity for discourse segmentation, therefore, we do not exclude any sentence in our discourse segmentation experiments. 5.2 Experimental Setup We perform our experiments on discourse parsing in RST-DT with the 18 coarser relations (see Figure 6) defined in (Carlson and Marcu, 2001) and also used in SPADE and HILDA. By attaching the nuclearity statuses (i.e., NS, SN, NN) to these relations we get 39 distinct relations4. Our experiments on the Instructional corpus consider the same 26 primary relations (e.g., GOAL:ACT, CAUSE:EFFECT, GENERAL-SPECIFIC) used in 4Not all relations take all the possible nuclearity statuses. 910 (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct re</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>L. Carlson and D. Marcu. 2001. Discourse Tagging Reference Manual. Technical Report ISI-TR-545, University of Southern California Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank (RST-DT) LDC2002T07. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5966" citStr="Carlson et al., 2002" startWordPosition="910" endWordPosition="913">] means that span (1,2) is the NUCLEUS and it comes before span 3 which is the SATELLITE). SPADE is limited in several ways. It makes an independence assumption between the label and the structure while modeling a constituent, and it ignores the sequential and 1http://www.isi.edu/licensed-sw/spade/ hierarchical dependencies between the constituents in the parsing model. Furthermore, SPADE relies only on lexico-syntactic features, and it follows a generative approach to estimate the model parameters for the segmentation and the parsing models. SPADE was trained and tested on the RST-DT corpus (Carlson et al., 2002), which contains humanannotated discourse trees for news articles. Subsequent research addresses the question of how much syntax one really needs in discourse analysis. Sporleder and Lapata (2005) focus on discourse chunking, comprising the two subtasks of segmentation and non-hierarchical nuclearity assignment. More specifically, they examine whether features derived via part of speech (POS) and chunk taggers would be sufficient for these purposes. Their results on RST-DT turn out to be comparable to SPADE without using any features from the syntactic tree. Later, Fisher and Roark (2007) demo</context>
<context position="26807" citStr="Carlson et al., 2002" startWordPosition="4396" endWordPosition="4399">s. We incorporate contextual information for a token by including the above features computed for its neighboring tokens. We also experimented with different N-gram (NE11, 2,31) features extracted from the token sequence, POS sequence and chunk sequence. However, since such features did not improve the seg3http://cogcomp.cs.illinois.edu/page/software mentation accuracy on the development set, they were excluded from our final set of features. 5 Experiments 5.1 Corpora To demonstrate the generality of our model, we experiment with two different genres. First, we use the standard RST-DT corpus (Carlson et al., 2002) that contains discourse annotations for 385 Wall Street Journal news articles from the Penn Treebank (Marcus et al., 1994). Second, we use the Instructional corpus developed by Subba and Eugenio (2009) that contains discourse annotations for 176 instructional how-to-do manuals on home-repair. The RST-DT corpus is partitioned into a training set of 347 documents (7673 sentences) and a test set of 38 documents (991 sentences), and 53 documents (1208 sentences) have been (doubly) annotated by two human annotators, based on which we compute the human agreement. We use the human-annotated syntacti</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>L. Carlson, D. Marcu, and M. Okurowski. 2002. RST Discourse Treebank (RST-DT) LDC2002T07. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>NJ, USA. ACL.</publisher>
<contexts>
<context position="29242" citStr="Charniak and Johnson, 2005" startWordPosition="4778" endWordPosition="4782">e nuclearity statuses. 910 (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we s</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173– 180, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference,</booktitle>
<pages>132--139</pages>
<publisher>ACL.</publisher>
<location>Seattle, Washington.</location>
<contexts>
<context position="29175" citStr="Charniak, 2000" startWordPosition="4770" endWordPosition="4771">ECIFIC) used in 4Not all relations take all the possible nuclearity statuses. 910 (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, pages 132–139, Seattle, Washington. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="29526" citStr="Collins, 2003" startWordPosition="4830" endWordPosition="4831">structional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labeled (i.e., nuclear</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics, 29(4):589–637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="37923" citStr="Daume, 2007" startWordPosition="6197" endWordPosition="6198">g to observe how much our full system is affected by an automatic segmenter on both RST-DT and the Instructional corpus (see Table 2 and Table 5). Nevertheless, taking into account the segmentation results in Table 4, this is 912 not surprising because previous studies (Soricut and Marcu, 2003) have already shown that automatic segmentation is the primary impediment to high accuracy discourse parsing. This demonstrates the need for a more accurate segmentation model in the Instructional genre. A promising future direction would be to apply effective domain adaptation methods (e.g., easyadapt (Daume, 2007)) to improve the segmentation performance in the Instructional domain by leveraging the rich data in RST-DT. 5.6 Error Analysis and Discussion The results in Table 2 suggest that given a manually segmented discourse, our sentence-level discourse parser finds the unlabeled (i.e., span) discourse tree and assigns the nuclearity statuses to the spans at a performance level close to human annotators. We, therefore, look more closely into the performance of our parser on the hardest task of relation labeling. Figure 6 shows the confusion matrix for the relation labeling task using manual segmentati</context>
</contexts>
<marker>Daume, 2007</marker>
<rawString>H. Daume. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 256–263, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D duVerle</author>
<author>H Prendinger</author>
</authors>
<title>A Novel Discourse Parser based on Support Vector Machine Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>665--673</pages>
<publisher>ACL.</publisher>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2222" citStr="duVerle and Prendinger, 2009" startWordPosition="333" endWordPosition="337">er a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the text into EDUs (known as discourse segmentation) and (ii) linking the EDUs into a labeled hierarchical tree structure (known as discourse parsing). Figure 1: Discourse structure of a sentence in RST-DT. Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing model, expressed as a Dynamic Conditional Random Field (DCR</context>
<context position="9830" citStr="duVerle and Prendinger, 2009" startWordPosition="1515" endWordPosition="1518">y do not “waste effort” modeling complex distributions (i.e., p(x)) that are not relevant for the target task (Murphy, 2012). 3 The Discourse Parser Assuming that a sentence is already segmented into a sequence of EDUs e1, e2,... en manually or by an automatic segmenter (see Section 4), the discourse parsing problem is to decide which spans to connect (i.e., structure of the DT) and which relations (i.e., labels of the internal nodes) to use in the process of building the hierarchical DT. To build the DTs effectively, a common assumption is that they are binary trees (Soricut and Marcu, 2003; duVerle and Prendinger, 2009). That is, multi-nuclear relations (e.g., LIST, JOINT, SEQUENCE) involving more than two EDUs are mapped to a hierarchical right-branching binary tree. For example, a flat LIST (e1, e2, e3, e4) is mapped to a right-branching binary tree LIST(e1, LIST(e2, LIST(e3, e4))). Our discourse parser has two components. The first component, the parsing model, assigns a probability to every possible DT. The second component, the parsing algorithm, finds the most probable DT among the candidate discourse trees. 3.1 Parsing Model A DT can be represented as a set of constituents of the form R[i, m, j], whic</context>
<context position="16479" citStr="duVerle and Prendinger, 2009" startWordPosition="2651" endWordPosition="2654">table in DCRF models, 907 we perform approximate inference (to compute the posterior marginals) using tree-based reparameterization (Wainwright et al., 2002). 3.1.1 Features Used in the Parsing Model Crucial to parsing performance is the set of features used, as summarized in Table 1. Note that these features are defined on two consecutive spans Wj−1 and Wj of a span sequence. Most of the features have been explored in previous studies. However, we improve some of these as explained below. Organizational features encode useful information about the surface structure of a sentence as shown by (duVerle and Prendinger, 2009). We measure the length of the spans in terms of the number of EDUs and tokens in it. However, in order to better adjust to the length variations, rather than computing their absolute numbers in a span, we choose to measure their relative numbers with respect to their total numbers in the sentence. For example, in a sentence containing three EDUs, a span containing two of these EDUs will have a relative EDU number of 0.67. We also measure the distances of the spans from the beginning and to the end of the sentence in terms of the number of EDUs. 8 organizational features Relative number of EDU</context>
<context position="32747" citStr="duVerle and Prendinger, 2009" startWordPosition="5363" endWordPosition="5366">ribution of a large number of discourse relations in this corpus. To analyze the features, Table 3 presents the parsing results on the RST-DT test set using different subsets of features. Every new subset of features appears to improve the accuracy. More specifically, when we add the organizational features with the dominance set features (see 52), we get about 2% absolute improvement in nuclearity and relations. With N-gram features (53), the gain is even higher; 6% in relations and 3.5% in nuclearity, demonstrating the utility of the N-gram features. This is consistent with the findings of (duVerle and Prendinger, 2009; Schilder, 2002). Including the Contextual features (54), we get further 3% and 2.2% improvements in nuclearity and relations, respectively. Notice that, adding the substructure features (55) does not help much in sentence-level parsing, giving only 6Subba and Eugenio (2009) report their results based on an arbitrary split between a training set and a test set. We asked the authors for their particular split. However, since we could not obtain that information, we compare our model’s performance based on 10-fold cross validation with their reported results. 911 an improvement of 0.8% in relat</context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>D. duVerle and H. Prendinger. 2009. A Novel Discourse Parser based on Support Vector Machine Classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 665–673, Suntec, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>A Kleeman</author>
<author>C Manning</author>
</authors>
<title>Efficient, Feature-based, Conditional Random Field Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>959--967</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="8523" citStr="Finkel et al., 2008" startWordPosition="1303" endWordPosition="1306">y classifier decides which of the two adjacent spans to link, and (ii) a multi-class classifier then connects the se905 lected spans with the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy approach, they model structure and relations separately, and ignore the sequence dependencies in their models. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al., 2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al., 2011) in Penn Discourse Treebank (Prasad et al., 2008). CRFs being a discriminative approach to sequence modeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assu</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J. Finkel, A. Kleeman, and C. Manning. 2008. Efficient, Feature-based, Conditional Random Field Parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 959–967, Columbus, Ohio, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fisher</author>
<author>B Roark</author>
</authors>
<title>The Utility of Parsederived Features for Automatic Discourse Segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>488--495</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6561" citStr="Fisher and Roark (2007)" startWordPosition="999" endWordPosition="1002">orpus (Carlson et al., 2002), which contains humanannotated discourse trees for news articles. Subsequent research addresses the question of how much syntax one really needs in discourse analysis. Sporleder and Lapata (2005) focus on discourse chunking, comprising the two subtasks of segmentation and non-hierarchical nuclearity assignment. More specifically, they examine whether features derived via part of speech (POS) and chunk taggers would be sufficient for these purposes. Their results on RST-DT turn out to be comparable to SPADE without using any features from the syntactic tree. Later, Fisher and Roark (2007) demonstrate over 4% absolute “performance gain” in segmentation, by combining the features extracted from the syntactic tree with the ones derived via taggers. Using quite a large number of features in a binary loglinear model they achieve the state-of-the-art segmentation performance on the RST-DT test set. On the different genre of instructional manuals, Subba and Eugenio (2009) propose a shift-reduce parser that relies on a classifier to find the appropriate relation between two text segments. Their classifier is based on Inductive Logic Programming (ILP), which learns first-order logic ru</context>
<context position="24980" citStr="Fisher and Roark, 2007" startWordPosition="4098" endWordPosition="4101"> an EDU boundary after a token wk, we find the lowest constituent in the lexicalized syntactic tree that spans over tokens wi ... wj such that i&lt;_k&lt;j. The production that expands this constituent in the tree and its different variations, form the feature set. For example in Figure 5, the production NP(efforts)—*PRP$(its)NNS(efforts)TS(to) and its different variations depending on whether they include the lexical heads and how many nonterminals (up to two) to consider before and after the potential EDU boundary (T), are used to determine the existence of a boundary after the word efforts (see (Fisher and Roark, 2007) for details). SPADE uses these features in a generative way, meaning that, it inserts an EDU boundary if the relative frequency (i.e., Maximum Likelihood Estimate (MLE)) of a potential boundary given the production in the training corpus is greater than 0.5. If the production has not been observed frequently enough, it uses its other variations to perform further smoothing. In contrast, we compute the MLE estimates for a production and its other variations, and use those as features with/without binarizing the values. Shallow syntactic parse (or Chunk) and POS tags have been shown to possess </context>
<context position="29063" citStr="Fisher and Roark, 2007" startWordPosition="4754" endWordPosition="4757">experiments on the Instructional corpus consider the same 26 primary relations (e.g., GOAL:ACT, CAUSE:EFFECT, GENERAL-SPECIFIC) used in 4Not all relations take all the possible nuclearity statuses. 910 (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their mod</context>
<context position="34998" citStr="Fisher and Roark, 2007" startWordPosition="5723" endWordPosition="5726">performance on the Instructional corpus. This could be due to a smaller amount of data in this corpus and the inaccuracies in the syntactic parser and taggers, which are trained on news articles. RST-DT Instructional Test Set 10-fold 10-fold 10-fold HIL SP F&amp;R LR SP LR SP LR P 77.9 83.8 91.3 88.0 83.7 87.5 65.1 73.9 R 70.6 86.8 89.7 92.3 86.2 89.9 82.8 89.7 F 74.1 85.2 90.5 90.1 84.9 88.7 72.8 80.9 Table 4: Segmentation results of different models. 5.4 Evaluation of the Discourse Segmenter We evaluate the segmentation accuracy with respect to the intra-sentential segment boundaries following (Fisher and Roark, 2007). Specifically, if a sentence contains n EDUs, which corresponds to n − 1 intra-sentence segment boundaries, we measure the model’s ability to correctly identify these n − 1 boundaries. Human agreement for this task is quite high (F-score of 98.3) on RST-DT. Table 4 shows the results of different models in (P)recision, (R)ecall, and (F)-score on the two corpora. We compare our model’s (LR) results with HILDA (HIL), SPADE (SP) and the results reported in Fisher and Roark (F&amp;R) (2007) on the RST-DT test set. HILDA gives the weakest performance7. Our results are also much better than SPADE8, with</context>
</contexts>
<marker>Fisher, Roark, 2007</marker>
<rawString>S. Fisher and B. Roark. 2007. The Utility of Parsederived Features for Automatic Discourse Segmentation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 488–495, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ghosh</author>
<author>R Johansson</author>
<author>G Riccardi</author>
<author>S Tonelli</author>
</authors>
<title>Shallow Discourse Parsing with Conditional Random Fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1071--1079</pages>
<publisher>AFNLP.</publisher>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="8611" citStr="Ghosh et al., 2011" startWordPosition="1318" endWordPosition="1321">sifier then connects the se905 lected spans with the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy approach, they model structure and relations separately, and ignore the sequence dependencies in their models. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al., 2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al., 2011) in Penn Discourse Treebank (Prasad et al., 2008). CRFs being a discriminative approach to sequence modeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assumptions. It has been advocated that CRFs are generally more accurate since they do not “</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli. 2011. Shallow Discourse Parsing with Conditional Random Fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, Chiang Mai, Thailand. AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hernault</author>
<author>H Prendinger</author>
<author>D duVerle</author>
<author>M Ishizuka</author>
</authors>
<title>HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse,</title>
<date>2010</date>
<contexts>
<context position="7560" citStr="Hernault et al. (2010)" startWordPosition="1150" endWordPosition="1153"> (2009) propose a shift-reduce parser that relies on a classifier to find the appropriate relation between two text segments. Their classifier is based on Inductive Logic Programming (ILP), which learns first-order logic rules from a large set of features including the linguistically rich compositional semantics coming from a semantic parser. They show that the compositional semantics improves the classification performance. However, their discourse parser implements a greedy approach (hence not optimal) and their classifier disregards the sequence and hierarchical dependencies. Using RST-DT, Hernault et al. (2010) present the HILDA system that comes with a segmenter and a parser based on Support Vector Machines (SVMs). The segmenter is a binary SVM classifier which relies on the same lexico-syntactic features used in SPADE, but with more context. The discourse parser builds a DT iteratively utilizing two SVM classifiers in each iteration: (i) a binary classifier decides which of the two adjacent spans to link, and (ii) a multi-class classifier then connects the se905 lected spans with the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy app</context>
<context position="18044" citStr="Hernault et al. (2010)" startWordPosition="2927" endWordPosition="2930">Beginning and end POS N-grams in span 2. 5 dominance set features Syntactic labels of the head node and the attachment node. Lexical heads of the head node and the attachment node. Dominance relationship between the two text spans. 2 contextual features Previous and next feature vectors. 2 substructure features Root nodes of the left and right rhetorical subtrees. Table 1: Features used in the DCRF parsing model. Discourse connectives (e.g., because, but), when present, signal rhetorical relations between two text segments (Knott and Dale, 1994; Marcu, 2000a). However, previous studies (e.g., Hernault et al. (2010), Biran and Rambow (2011)) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the most frequent, but also the ones that are indicative of th</context>
<context position="36165" citStr="Hernault et al., 2010" startWordPosition="5918" endWordPosition="5922">ce7. Our results are also much better than SPADE8, with an absolute F-score improvement of 4.9%, and comparable to the results of F&amp;R, even though we use fewer features. Furthermore, we perform 10-fold cross validation on both corpora and compare with SPADE. However, SPADE does not come with a training module for its segmenter. We reimplemented this module and verified it on the RST-DT test set. Due to the lack of human-annotated syntactic trees in the Instructional corpus, we train SPADE in this corpus using the syntactic trees produced 7Note that, the high segmentation accuracy reported in (Hernault et al., 2010) is due to a less stringent evaluation metric. 8The improvements are statistically significant (p&lt;2.4e-06) 5.5 Parsing based on Automatic Segmentation In order to evaluate our full system, we feed our discourse parser the output of our discourse segmenter. Table 5 shows the F-score results. We compare our results with SPADE on the RST-DT test set. We achieve absolute F-score improvements of 3.6%, 3.4% and 7.4% in span, nuclearity and relation, respectively. These improvements are statistically significant (p&lt;0.001). Our system, therefore, reduces the errors by 15.5%, 11.4%, and 17.6% in span, </context>
</contexts>
<marker>Hernault, Prendinger, duVerle, Ishizuka, 2010</marker>
<rawString>H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J Martin</author>
</authors>
<title>Speech and Language Processing, chapter 14.</title>
<date>2008</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="21749" citStr="Jurafsky and Martin, 2008" startWordPosition="3561" endWordPosition="3564">provide labels for the substructures. Next we can build a new, more accurate model by including the substructure features, and run again the parsing algorithm to find the final optimal DT. 3.2 Parsing Algorithm Our parsing model above assigns a conditional probability to every possible DT constituent for a sentence, the job of the parsing algorithm is to find the most probable DT. Formally, this can be written as, DT∗ = argmax DTP(DT|O) Our discourse parser implements a probabilistic CKY-like bottom-up algorithm for computing the most likely parse of a sentence using dynamic programming; see (Jurafsky and Martin, 2008) for a description. Specifically, with n number of EDUs in a sentence, we use the upper-triangular portion of the n x n Dynamic Programming Table (DPT). The cell [i, j] in the DPT represents the span containing EDUs i through j and stores the probability of a constituent R[i, m, j], where m = argmax Z≤k≤jP(R[i, k, j]). In contrast to HILDA which implements a greedy algorithm, our approach finds a DT that is globally optimal. Our approach is also different from SPADE’s implementation. SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to t</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>D. Jurafsky and J. Martin, 2008. Speech and Language Processing, chapter 14. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>R Dale</author>
</authors>
<title>Using Linguistic Phenomena to Motivate a Set of Coherence Relations.</title>
<date>1994</date>
<booktitle>Discourse Processes,</booktitle>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="17972" citStr="Knott and Dale, 1994" startWordPosition="2917" endWordPosition="2920">d lexical N-grams in span 2. Beginning and end POS N-grams in span 1. Beginning and end POS N-grams in span 2. 5 dominance set features Syntactic labels of the head node and the attachment node. Lexical heads of the head node and the attachment node. Dominance relationship between the two text spans. 2 contextual features Previous and next feature vectors. 2 substructure features Root nodes of the left and right rhetorical subtrees. Table 1: Features used in the DCRF parsing model. Discourse connectives (e.g., because, but), when present, signal rhetorical relations between two text segments (Knott and Dale, 1994; Marcu, 2000a). However, previous studies (e.g., Hernault et al. (2010), Biran and Rambow (2011)) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are</context>
</contexts>
<marker>Knott, Dale, 1994</marker>
<rawString>A. Knott and R. Dale. 1994. Using Linguistic Phenomena to Motivate a Set of Coherence Relations. Discourse Processes, 18(1):35–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8377" citStr="Lafferty et al., 2001" startWordPosition="1284" endWordPosition="1287">ures used in SPADE, but with more context. The discourse parser builds a DT iteratively utilizing two SVM classifiers in each iteration: (i) a binary classifier decides which of the two adjacent spans to link, and (ii) a multi-class classifier then connects the se905 lected spans with the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy approach, they model structure and relations separately, and ignore the sequence dependencies in their models. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al., 2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al., 2011) in Penn Discourse Treebank (Prasad et al., 2008). CRFs being a discriminative approach to sequence modeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Ke</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical Decision-tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<publisher>ACL.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="29510" citStr="Magerman, 1995" startWordPosition="4828" endWordPosition="4829">ations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labele</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical Decision-tree Models for Parsing. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 276–283, Cambridge, Massachusetts. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1060" citStr="Mann and Thompson, 1988" startWordPosition="147" endWordPosition="150">ssifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann and S. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--395</pages>
<contexts>
<context position="899" citStr="Marcu, 2000" startWordPosition="127" endWordPosition="128">istic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursi</context>
<context position="17985" citStr="Marcu, 2000" startWordPosition="2921" endWordPosition="2922">pan 2. Beginning and end POS N-grams in span 1. Beginning and end POS N-grams in span 2. 5 dominance set features Syntactic labels of the head node and the attachment node. Lexical heads of the head node and the attachment node. Dominance relationship between the two text spans. 2 contextual features Previous and next feature vectors. 2 substructure features Root nodes of the left and right rhetorical subtrees. Table 1: Features used in the DCRF parsing model. Discourse connectives (e.g., because, but), when present, signal rhetorical relations between two text segments (Knott and Dale, 1994; Marcu, 2000a). However, previous studies (e.g., Hernault et al. (2010), Biran and Rambow (2011)) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the</context>
<context position="30199" citStr="Marcu, 2000" startWordPosition="4938" endWordPosition="4939"> Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labeled (i.e., nuclearity, relation) precision, recall and F-score as described in (Marcu, 2000b, page 143). For brevity, we report only the F-scores in Table 2. Notice that, our parser (DCRF) consistently outperforms SPADE (SP) on the RST-DT test set5. Especially, on relation labeling, which is the hardest among the three tasks, we get an absolute F-score improvement of 9.5%, which represents a relative error rate reduction of 29.3%. Our F-score of 77.1 in relation labeling is also close to the human agreement (i.e., F-score of 83.0) on the doubly-annotated data. Our results on the RST-DT test set are consistent with the mean scores over 10-folds, when we perform 10-fold cross validati</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000a. The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach. Computational Linguistics, 26:395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice ofDiscourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="899" citStr="Marcu, 2000" startWordPosition="127" endWordPosition="128">istic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursi</context>
<context position="17985" citStr="Marcu, 2000" startWordPosition="2921" endWordPosition="2922">pan 2. Beginning and end POS N-grams in span 1. Beginning and end POS N-grams in span 2. 5 dominance set features Syntactic labels of the head node and the attachment node. Lexical heads of the head node and the attachment node. Dominance relationship between the two text spans. 2 contextual features Previous and next feature vectors. 2 substructure features Root nodes of the left and right rhetorical subtrees. Table 1: Features used in the DCRF parsing model. Discourse connectives (e.g., because, but), when present, signal rhetorical relations between two text segments (Knott and Dale, 1994; Marcu, 2000a). However, previous studies (e.g., Hernault et al. (2010), Biran and Rambow (2011)) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of connectives, since this approach is domain independent and capable of capturing non-lexical cues such as punctuations. To build the lexical N-gram dictionary empirically from the training corpus we consider the first and last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the</context>
<context position="30199" citStr="Marcu, 2000" startWordPosition="4938" endWordPosition="4939"> Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labeled (i.e., nuclearity, relation) precision, recall and F-score as described in (Marcu, 2000b, page 143). For brevity, we report only the F-scores in Table 2. Notice that, our parser (DCRF) consistently outperforms SPADE (SP) on the RST-DT test set5. Especially, on relation labeling, which is the hardest among the three tasks, we get an absolute F-score improvement of 9.5%, which represents a relative error rate reduction of 29.3%. Our F-score of 77.1 in relation labeling is also close to the human agreement (i.e., F-score of 83.0) on the doubly-annotated data. Our results on the RST-DT test set are consistent with the mean scores over 10-folds, when we perform 10-fold cross validati</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000b. The Theory and Practice ofDiscourse Parsing and Summarization. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="26930" citStr="Marcus et al., 1994" startWordPosition="4415" endWordPosition="4419"> also experimented with different N-gram (NE11, 2,31) features extracted from the token sequence, POS sequence and chunk sequence. However, since such features did not improve the seg3http://cogcomp.cs.illinois.edu/page/software mentation accuracy on the development set, they were excluded from our final set of features. 5 Experiments 5.1 Corpora To demonstrate the generality of our model, we experiment with two different genres. First, we use the standard RST-DT corpus (Carlson et al., 2002) that contains discourse annotations for 385 Wall Street Journal news articles from the Penn Treebank (Marcus et al., 1994). Second, we use the Instructional corpus developed by Subba and Eugenio (2009) that contains discourse annotations for 176 instructional how-to-do manuals on home-repair. The RST-DT corpus is partitioned into a training set of 347 documents (7673 sentences) and a test set of 38 documents (991 sentences), and 53 documents (1208 sentences) have been (doubly) annotated by two human annotators, based on which we compute the human agreement. We use the human-annotated syntactic trees from Penn Treebank to train SPADE in our experiments using RST-DT as done in (Soricut and Marcu, 2003). We extracte</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="15643" citStr="McCallum, 2002" startWordPosition="2519" endWordPosition="2520">omputing the posterior marginal P(R3:4, 53:4=1|e1, e2, e3:4, O). At the third level (Figure 4(c)), there are three possible sequences (e1:3, e4), (e1, e2:4) and (e1:2, e3:4), to which we apply our model and acquire the probabilities of the constituents R[1, 3, 4], R[1,1, 4] and R[1, 2,4] by computing their respective posterior marginals. Figure 4: DCRF model applied to the sequences at different levels of a discourse tree. (a) A sequence at the first level, (b) Three possible sequences at the second level, (c) Two possible sequences at the third level. Our DCRF model is designed using MALLET (McCallum, 2002). In order to avoid overfitting we regularize the DCRF model with 12 regularization and learn the model parameters using the limitedmemory BFGS (L-BFGS) fitting algorithm. Since exact inference can be intractable in DCRF models, 907 we perform approximate inference (to compute the posterior marginals) using tree-based reparameterization (Wainwright et al., 2002). 3.1.1 Features Used in the Parsing Model Crucial to parsing performance is the set of features used, as summarized in Table 1. Note that these features are defined on two consecutive spans Wj−1 and Wj of a span sequence. Most of the f</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Murphy</author>
</authors>
<title>Machine Learning A Probabilistic Perspective (Forthcoming,</title>
<date>2012</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9325" citStr="Murphy, 2012" startWordPosition="1430" endWordPosition="1431">odeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assumptions. It has been advocated that CRFs are generally more accurate since they do not “waste effort” modeling complex distributions (i.e., p(x)) that are not relevant for the target task (Murphy, 2012). 3 The Discourse Parser Assuming that a sentence is already segmented into a sequence of EDUs e1, e2,... en manually or by an automatic segmenter (see Section 4), the discourse parsing problem is to decide which spans to connect (i.e., structure of the DT) and which relations (i.e., labels of the internal nodes) to use in the process of building the hierarchical DT. To build the DTs effectively, a common assumption is that they are binary trees (Soricut and Marcu, 2003; duVerle and Prendinger, 2009). That is, multi-nuclear relations (e.g., LIST, JOINT, SEQUENCE) involving more than two EDUs a</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>K. Murphy. 2012. Machine Learning A Probabilistic Perspective (Forthcoming, August 2012). MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>A Joshi</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank as a Resource for Natural Language Generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation,</booktitle>
<pages>25--32</pages>
<location>Birmingham, U.K.</location>
<contexts>
<context position="871" citStr="Prasad et al., 2005" startWordPosition="122" endWordPosition="125">stract We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting l</context>
</contexts>
<marker>Prasad, Joshi, Dinesh, Lee, Miltsakaki, Webber, 2005</marker>
<rawString>R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, and B. Webber. 2005. The Penn Discourse TreeBank as a Resource for Natural Language Generation. In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation, pages 25–32, Birmingham, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2961--2968</pages>
<location>Marrakech, Morocco. ELRA.</location>
<contexts>
<context position="8660" citStr="Prasad et al., 2008" startWordPosition="1327" endWordPosition="1330"> the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy approach, they model structure and relations separately, and ignore the sequence dependencies in their models. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al., 2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al., 2011) in Penn Discourse Treebank (Prasad et al., 2008). CRFs being a discriminative approach to sequence modeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assumptions. It has been advocated that CRFs are generally more accurate since they do not “waste effort” modeling complex distributions (i.e</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC), pages 2961–2968, Marrakech, Morocco. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Schilder</author>
</authors>
<title>Robust Discourse Parsing via Discourse Markers, Topicality and Position.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="32764" citStr="Schilder, 2002" startWordPosition="5367" endWordPosition="5368">discourse relations in this corpus. To analyze the features, Table 3 presents the parsing results on the RST-DT test set using different subsets of features. Every new subset of features appears to improve the accuracy. More specifically, when we add the organizational features with the dominance set features (see 52), we get about 2% absolute improvement in nuclearity and relations. With N-gram features (53), the gain is even higher; 6% in relations and 3.5% in nuclearity, demonstrating the utility of the N-gram features. This is consistent with the findings of (duVerle and Prendinger, 2009; Schilder, 2002). Including the Contextual features (54), we get further 3% and 2.2% improvements in nuclearity and relations, respectively. Notice that, adding the substructure features (55) does not help much in sentence-level parsing, giving only 6Subba and Eugenio (2009) report their results based on an arbitrary split between a training set and a test set. We asked the authors for their particular split. However, since we could not obtain that information, we compare our model’s performance based on 10-fold cross validation with their reported results. 911 an improvement of 0.8% in relations. Therefore, </context>
</contexts>
<marker>Schilder, 2002</marker>
<rawString>F. Schilder. 2002. Robust Discourse Parsing via Discourse Markers, Topicality and Position. Natural Language Engineering, 8(3):235–255, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference ofthe North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>134--141</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8567" citStr="Sha and Pereira, 2003" startWordPosition="1310" endWordPosition="1313">cent spans to link, and (ii) a multi-class classifier then connects the se905 lected spans with the appropriate relation. They use a very large set of features in their parser. However, taking a radically-greedy approach, they model structure and relations separately, and ignore the sequence dependencies in their models. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al., 2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al., 2011) in Penn Discourse Treebank (Prasad et al., 2008). CRFs being a discriminative approach to sequence modeling (i.e., directly models the conditional p(y|x, 0)), have several advantages over its generative counterparts such as Hidden Markov Models (HMMs) and Markov Random Fields (MRFs), which first model the joint p(y, x|0), then infer the conditional p(y|x, 0)). Key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assumptions. It has been advocated that CRFs are</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of the 2003 Conference ofthe North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 134–141, Edmonton, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
</authors>
<title>Discourse-Level Relations for Opinion Analysis.</title>
<date>2010</date>
<tech>PhD thesis,</tech>
<institution>University of Pittsburgh.</institution>
<marker>Somasundaran, 2010</marker>
<rawString>S. Somasundaran, 2010. Discourse-Level Relations for Opinion Analysis. PhD thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing Using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>149--156</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2166" citStr="Soricut and Marcu, 2003" startWordPosition="324" endWordPosition="328">A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the text into EDUs (known as discourse segmentation) and (ii) linking the EDUs into a labeled hierarchical tree structure (known as discourse parsing). Figure 1: Discourse structure of a sentence in RST-DT. Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing mod</context>
<context position="4663" citStr="Soricut and Marcu (2003)" startWordPosition="706" endWordPosition="710">e empirical evaluation indicates that our approach to discourse parsing outperforms the stateof-the-art by a wide margin. Moreover, we show this to be the case on two very different genres: news articles and instructional how-to-do manuals. In the rest of the paper, after discussing related work, we present our discourse parser. Then, we describe our segmenter. The experiments and the corpora we used are described next, followed by a discussion of the key results and some error analysis. 2 Related work Automatic discourse analysis has a long history; see (Stede, 2011) for a detailed overview. Soricut and Marcu (2003) present the publicly available SPADE1 system that comes with probabilistic models for sentence-level discourse segmentation and parsing based on lexical and syntactic features derived from the lexicalized syntactic tree of a sentence. Their parsing algorithm finds the most probable DT for a sentence, where the probabilities of the constituents are estimated by their parsing model. A constituent (e.g., ATTRIBUTION-NS[(1,2),3] in Figure 1) in a DT has two components, first, the label denoting the relation and second, the structure indicating which spans are being linked by the relation. The nuc</context>
<context position="9799" citStr="Soricut and Marcu, 2003" startWordPosition="1511" endWordPosition="1514">y more accurate since they do not “waste effort” modeling complex distributions (i.e., p(x)) that are not relevant for the target task (Murphy, 2012). 3 The Discourse Parser Assuming that a sentence is already segmented into a sequence of EDUs e1, e2,... en manually or by an automatic segmenter (see Section 4), the discourse parsing problem is to decide which spans to connect (i.e., structure of the DT) and which relations (i.e., labels of the internal nodes) to use in the process of building the hierarchical DT. To build the DTs effectively, a common assumption is that they are binary trees (Soricut and Marcu, 2003; duVerle and Prendinger, 2009). That is, multi-nuclear relations (e.g., LIST, JOINT, SEQUENCE) involving more than two EDUs are mapped to a hierarchical right-branching binary tree. For example, a flat LIST (e1, e2, e3, e4) is mapped to a right-branching binary tree LIST(e1, LIST(e2, LIST(e3, e4))). Our discourse parser has two components. The first component, the parsing model, assigns a probability to every possible DT. The second component, the parsing algorithm, finds the most probable DT among the candidate discourse trees. 3.1 Parsing Model A DT can be represented as a set of constituen</context>
<context position="19017" citStr="Soricut and Marcu, 2003" startWordPosition="3084" endWordPosition="3088"> last N tokens (NE{1, 2}) of each span and rank them according to their mutual information2 with the two labels, Structure and Relation. Intuitively, the most informative cues are not only the most frequent, but also the ones that are indicative of the labels in the training data (Blitzer, 2008). In addition to the lexical N-grams we also encode POS tags of the first and last N tokens (NE{1, 2}) as features. Figure 5: A discourse segmented lexicalized syntactic tree. Boxed nodes form the dominance set D. Dominance set extracted from the Discourse Segmented Lexicalized Syntactic Tree (DS-LST) (Soricut and Marcu, 2003) has been shown to be a very effective feature in SPADE. Figure 5 shows the DSLST for our running example (see Figure 1 and 3). In a DS-LST, each EDU except the one with the root node must have a head node NH that is attached to an attachment node NA residing in a separate EDU. A dominance set D (shown at the bottom of Figure 5 for our example) contains these attachment points of the EDUs in a DS-LST. In addition to the syntactic and lexical information of the head and attachment nodes, each element in D also represents a dominance relationship between the EDUs involved. The EDU with NA domina</context>
<context position="22882" citStr="Soricut and Marcu, 2003" startWordPosition="3755" endWordPosition="3758">e tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell [i, j] in SPADE’s DPT stores the probability of a constituent R[i, m, j], where m = argmax Z≤k≤jP([i, k, j]). Disregarding the relation label R while building the DPT, this approach may find a tree that is not globally optimal. 4 The Discourse Segmenter Our discourse parser above assumes that the input sentences have been already segmented into EDUs. Since it has been shown that discourse segmentation is a primary source of inaccuracy for discourse parsing (Soricut and Marcu, 2003), we have developed our own segmenter, that not only achieves state-ofthe-art performance as shown later, but also reduces the time complexity by using fewer features. Our segmenter implements a binary classifier to decide for each word (except the last word) in a sentence, whether to put an EDU boundary after that word. We use a Logistic Regression (LR) (i.e., discriminative) model with l2 regularization and learn the model parameters using the L-BFGS algorithm, which gives quadratic convergence rate. To avoid overfitting, we use 5-fold cross validation to learn the regularization strength pa</context>
<context position="27517" citStr="Soricut and Marcu, 2003" startWordPosition="4507" endWordPosition="4511">e Penn Treebank (Marcus et al., 1994). Second, we use the Instructional corpus developed by Subba and Eugenio (2009) that contains discourse annotations for 176 instructional how-to-do manuals on home-repair. The RST-DT corpus is partitioned into a training set of 347 documents (7673 sentences) and a test set of 38 documents (991 sentences), and 53 documents (1208 sentences) have been (doubly) annotated by two human annotators, based on which we compute the human agreement. We use the human-annotated syntactic trees from Penn Treebank to train SPADE in our experiments using RST-DT as done in (Soricut and Marcu, 2003). We extracted a sentence-level DT from a document-level DT by finding the subtree that exactly spans over the sentence. By our count, 7321 sentences in the training set, 951 sentences in the test set and 1114 sentences in the doublyannotated set have a well-formed DT in RST-DT. The Instructional corpus contains 3430 sentences in total, out of which 3032 have a well-formed DT. This forms our sentence-level corpora for discourse parsing. However, the existence of a well-formed DT in not a necessity for discourse segmentation, therefore, we do not exclude any sentence in our discourse segmentati</context>
<context position="37606" citStr="Soricut and Marcu, 2003" startWordPosition="6148" endWordPosition="6151">ity 70.2 73.6 72.2 64.3 Relation 58.0 65.4 64.2 54.8 Table 5: Parsing results using automatic segmentation. For the Instructional corpus, the last column of Table 5 shows the mean 10-fold cross validation results. We cannot compare with S&amp;E because no results were reported using an automatic segmenter. However, it is interesting to observe how much our full system is affected by an automatic segmenter on both RST-DT and the Instructional corpus (see Table 2 and Table 5). Nevertheless, taking into account the segmentation results in Table 4, this is 912 not surprising because previous studies (Soricut and Marcu, 2003) have already shown that automatic segmentation is the primary impediment to high accuracy discourse parsing. This demonstrates the need for a more accurate segmentation model in the Instructional genre. A promising future direction would be to apply effective domain adaptation methods (e.g., easyadapt (Daume, 2007)) to improve the segmentation performance in the Instructional domain by leveraging the rich data in RST-DT. 5.6 Error Analysis and Discussion The results in Table 2 suggest that given a manually segmented discourse, our sentence-level discourse parser finds the unlabeled (i.e., spa</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 149–156, Edmonton, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>M Lapata</author>
</authors>
<title>Discourse Chunking and its Application to Sentence Compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>257--264</pages>
<publisher>ACL.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="952" citStr="Sporleder and Lapata, 2005" startWordPosition="131" endWordPosition="135">rforming sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span li</context>
<context position="6162" citStr="Sporleder and Lapata (2005)" startWordPosition="938" endWordPosition="941">ucture while modeling a constituent, and it ignores the sequential and 1http://www.isi.edu/licensed-sw/spade/ hierarchical dependencies between the constituents in the parsing model. Furthermore, SPADE relies only on lexico-syntactic features, and it follows a generative approach to estimate the model parameters for the segmentation and the parsing models. SPADE was trained and tested on the RST-DT corpus (Carlson et al., 2002), which contains humanannotated discourse trees for news articles. Subsequent research addresses the question of how much syntax one really needs in discourse analysis. Sporleder and Lapata (2005) focus on discourse chunking, comprising the two subtasks of segmentation and non-hierarchical nuclearity assignment. More specifically, they examine whether features derived via part of speech (POS) and chunk taggers would be sufficient for these purposes. Their results on RST-DT turn out to be comparable to SPADE without using any features from the syntactic tree. Later, Fisher and Roark (2007) demonstrate over 4% absolute “performance gain” in segmentation, by combining the features extracted from the syntactic tree with the ones derived via taggers. Using quite a large number of features i</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>C. Sporleder and M. Lapata. 2005. Discourse Chunking and its Application to Sentence Compression. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 257–264, Vancouver, British Columbia, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stede</author>
</authors>
<title>Discourse Processing. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan And Claypool Publishers,</publisher>
<contexts>
<context position="4613" citStr="Stede, 2011" startWordPosition="700" endWordPosition="701"> tested in a series of experiments. The empirical evaluation indicates that our approach to discourse parsing outperforms the stateof-the-art by a wide margin. Moreover, we show this to be the case on two very different genres: news articles and instructional how-to-do manuals. In the rest of the paper, after discussing related work, we present our discourse parser. Then, we describe our segmenter. The experiments and the corpora we used are described next, followed by a discussion of the key results and some error analysis. 2 Related work Automatic discourse analysis has a long history; see (Stede, 2011) for a detailed overview. Soricut and Marcu (2003) present the publicly available SPADE1 system that comes with probabilistic models for sentence-level discourse segmentation and parsing based on lexical and syntactic features derived from the lexicalized syntactic tree of a sentence. Their parsing algorithm finds the most probable DT for a sentence, where the probabilities of the constituents are estimated by their parsing model. A constituent (e.g., ATTRIBUTION-NS[(1,2),3] in Figure 1) in a DT has two components, first, the label denoting the relation and second, the structure indicating whi</context>
</contexts>
<marker>Stede, 2011</marker>
<rawString>M. Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan And Claypool Publishers, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
<author>B Di Eugenio</author>
</authors>
<title>An Effective Discourse Parser that Uses Rich Linguistic Information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>566--574</pages>
<publisher>ACL.</publisher>
<location>Boulder, Colorado.</location>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>R. Subba and B. Di Eugenio. 2009. An Effective Discourse Parser that Uses Rich Linguistic Information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 566–574, Boulder, Colorado. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>8--693</pages>
<contexts>
<context position="2846" citStr="Sutton et al., 2007" startWordPosition="434" endWordPosition="437">wever, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing model, expressed as a Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007). By representing the structure and the relation of each discourse tree constituent jointly and by explicitly capturing the sequential and hierarchical dependencies between constituents of a discourse tree, our DCRF model does not make any independence assumption among these properties. Furthermore, our 904 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 904–915, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics parsing model supports a bottom-up parsing algorithm</context>
<context position="11768" citStr="Sutton et al., 2007" startWordPosition="1834" endWordPosition="1837">sub-constituents C. For instance, for the DT shown in Figure 1, our model would estimate P(R′[1,1, 2]|0), P(R′[2, 2, 3]|0), P(R′[1, 2,3]|R″[1,1, 2], 0) etc. for all R′ and R″ranging on the set of relations. In what follows we describe our probabilistic parsing model to compute all these conditional probabilities P(c|C, 0). We will demonstrate how our approach not only models the structure and the relation jointly, but it also captures linear sequence dependencies and hierarchical dependencies between constituents of a DT. Our novel parsing model is the Dynamic Conditional Random Field (DCRF) (Sutton et al., 2007) shown in Figure 2. A DCRF is a generalization of linear-chain CRFs to represent complex interaction between labels, such as when performing multiple labeling tasks on the same sequence. The observed nodes Wj in the figure are the text spans. A text span can be either an EDU or a concatenation of a sequence of EDUs. The structure nodes Sj∈{0,1} in the figure represent whether text spans Wj_1 and Wj should be connected or not. The relation nodes Rj∈{1... M} denote the discourse relation between spans Wj_1 and Wj, given that M is the total number of relations in our relation set. Notice that we </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>C. Sutton, A. McCallum, and K. Rohanimanesh. 2007. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. Journal of Machine Learning Research (JMLR), 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Verberne</author>
<author>L Boves</author>
<author>N Oostdijk</author>
<author>P Coppen</author>
</authors>
<title>Evaluating Discourse-based Answer Extraction for Why-question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>735--736</pages>
<publisher>ACM.</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="999" citStr="Verberne et al., 2007" startWordPosition="139" endWordPosition="142">work comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a N</context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2007</marker>
<rawString>S. Verberne, L. Boves, N. Oostdijk, and P. Coppen. 2007. Evaluating Discourse-based Answer Extraction for Why-question Answering. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 735–736, Amsterdam, The Netherlands. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>Treebased Reparameterization for Approximate Inference on Loopy Graphs.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 14,</booktitle>
<pages>1001--1008</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16007" citStr="Wainwright et al., 2002" startWordPosition="2570" endWordPosition="2573">model applied to the sequences at different levels of a discourse tree. (a) A sequence at the first level, (b) Three possible sequences at the second level, (c) Two possible sequences at the third level. Our DCRF model is designed using MALLET (McCallum, 2002). In order to avoid overfitting we regularize the DCRF model with 12 regularization and learn the model parameters using the limitedmemory BFGS (L-BFGS) fitting algorithm. Since exact inference can be intractable in DCRF models, 907 we perform approximate inference (to compute the posterior marginals) using tree-based reparameterization (Wainwright et al., 2002). 3.1.1 Features Used in the Parsing Model Crucial to parsing performance is the set of features used, as summarized in Table 1. Note that these features are defined on two consecutive spans Wj−1 and Wj of a span sequence. Most of the features have been explored in previous studies. However, we improve some of these as explained below. Organizational features encode useful information about the surface structure of a sentence as shown by (duVerle and Prendinger, 2009). We measure the length of the spans in terms of the number of EDUs and tokens in it. However, in order to better adjust to the </context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2002</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2002. Treebased Reparameterization for Approximate Inference on Loopy Graphs. In Advances in Neural Information Processing Systems 14, pages 1001–1008. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing Discourse Coherence: A Corpus-Based Study.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--249</pages>
<marker>Wolf, Gibson, 2005</marker>
<rawString>F. Wolf and E. Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Study. Computational Linguistics, 31:249–288, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
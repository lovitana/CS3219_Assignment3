<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.997781">
Biased Representation Learning for Domain Adaptation
</title>
<author confidence="0.997307">
Fei Huang, Alexander Yates
</author>
<affiliation confidence="0.959742">
Temple University
Computer and Information Sciences
</affiliation>
<address confidence="0.8654645">
324 Wachman Hall
Philadelphia, PA 19122
</address>
<email confidence="0.999491">
ffhuang,yatesl@temple.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748571428571">
Representation learning is a promising tech-
nique for discovering features that allow su-
pervised classifiers to generalize from a source
domain dataset to arbitrary new domains. We
present a novel, formal statement of the rep-
resentation learning task. We argue that be-
cause the task is computationally intractable
in general, it is important for a representa-
tion learner to be able to incorporate expert
knowledge during its search for helpful fea-
tures. Leveraging the Posterior Regularization
framework, we develop an architecture for in-
corporating biases into representation learn-
ing. We investigate three types of biases, and
experiments on two domain adaptation tasks
show that our biased learners identify signif-
icantly better sets of features than unbiased
learners, resulting in a relative reduction in er-
ror of more than 16% for both tasks, with re-
spect to existing state-of-the-art representation
learning techniques.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891895833333">
Supervised natural language processing (NLP) sys-
tems have been widely used and have achieved im-
pressive performance on many NLP tasks. Howev-
er, they exhibit a significant drop-off in performance
when tested on domains that differ from their train-
ing domains. (Gildea, 2001; Sekine, 1997; Pradhan
et al., 2007) One major cause for poor performance
on out of-domain texts is the traditional representa-
tion used by supervised NLP systems (Ben-David et
al., 2007). Most systems depend on lexical features,
which can differ greatly between domains, so that
important words in the test data may never be seen
in the training data. The connection between word-
s and labels may also change across domains. For
instance, “signaling” appears only as a present par-
ticiple (VBG) in WSJ text (as in, “signaling that...”),
but predominantly as a noun (as in “signaling path-
way”) in biomedical text.
Recently, several authors have found that learning
new features based on distributional similarity can
significantly improve domain adaptation (Blitzer et
al., 2006; Huang and Yates, 2009; Turian et al.,
2010; Dhillon et al., 2011). This framework is at-
tractive for several reasons: experimentally, learned
features can yield significant improvements over s-
tandard supervised models on out-of-domain test-
s. Moreover, since the representation-learning tech-
niques are unsupervised, they can easily be applied
to arbitrary new domains. There is no need to supply
additional labeled examples for each new domain.
Traditional representations still hold one signif-
icant advantage over representation-learning, how-
ever: because features are hand-crafted, these rep-
resentations can readily incorporate the linguistic
or domain expert knowledge that leads to state-of-
the-art in-domain performance. In contrast, the on-
ly guide for existing representation-learning tech-
niques is a corpus of unlabeled text.
To address this shortcoming, we introduce
representation-learning techniques that incorporate
a domain expert’s preferences over the learned fea-
tures. For example, out of the set of all possi-
ble distributional-similarity features, we might pre-
fer those that help predict the labels in a labeled
training data set. To capture this preference, we
might bias a representation-learning algorithm to-
wards features with low joint entropy with the labels
in the training data. This particular biased form of
</bodyText>
<page confidence="0.862982">
1313
</page>
<note confidence="0.7751585">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1313–1323, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999962170731707">
representation learning is a type of semi-supervised
learning that allows our system to learn task-specific
representations from a source domain’s training da-
ta, rather than the single representation for all tasks
produced by current, unsupervised representation-
learning techniques.
We present a novel formal statement of represen-
tation learning, and demonstrate that it is computa-
tionally intractable in general. It is therefore criti-
cal for representation learning to be flexible enough
to incorporate the intuitions and knowledge of hu-
man experts, to guide the search for representations
efficiently and effectively. Leveraging the Posteri-
or Regularization framework (Ganchev et al., 2010),
we present an architecture for learning representa-
tions for sequence-labeling tasks that allows for bi-
ases. In addition to a bias towards task-specific rep-
resentations, we investigate a bias towards repre-
sentations that have similar features across domain-
s, to improve domain-independence; and a bias to-
wards multi-dimensional representations, where d-
ifferent dimensions are independent of one another.
In this paper, we focus on incorporating the bias-
es with HMM-type representations (Hidden Markov
Model). However, this technique can also be ap-
plied to other graphical model-based representations
with little modification. Our experiments show that
on two different domain-adaptation tasks, our biased
representations improve significantly over unbiased
ones. In a part-of-speech tagging experiment, our
best model provides a 25% relative reduction in er-
ror over a state-of-the-art Chinese POS tagger, and
a 19% relative reduction in error over an unbiased
representation from previous work.
The next section describes background and previ-
ous work. Section 3 introduces our framework for
learning biased representations. Section 4 describes
how we estimate parameters for the biased objective
functions efficiently. Section 5 details our experi-
ments and results, and section 6 concludes and out-
lines directions for future work.
</bodyText>
<sectionHeader confidence="0.970156" genericHeader="introduction">
2 Background and Previous Work
</sectionHeader>
<subsectionHeader confidence="0.818869">
2.1 Terminology and Notation
</subsectionHeader>
<bodyText confidence="0.999495315789474">
A representation is a set of features that describe da-
ta points. Formally, given an instance set X, it is a
function R : X —* Y for some suitable space Y (of-
ten Rd), which is then used as the input space for a
classifier. For instance, a traditional representation
for POS tagging over vocabulary V would include
(in part) |V  |dimensions, and would map a word to a
binary vector with a 1 in only one of the dimensions.
By a structured representation, we mean a function
R that incorporates some form of joint inference. In
this paper, we use Viterbi decoding of variants of
Hidden Markov Models (HMMs) for our structured
representations, although our techniques are appli-
cable to arbitrary (Dynamic) Bayes Nets. A domain
is a probability distribution D over the instance set
X; R(D) denotes the induced distribution over Y.
In domain adaptation tasks, a learner is given sam-
ples from a source domain DS, and is evaluated on
samples from a target domain DT.
</bodyText>
<subsectionHeader confidence="0.998801">
2.2 Theoretical Background
</subsectionHeader>
<bodyText confidence="0.998825">
Ben-David et al. (2010) give a theoretical analysis
of domain adaptation which shows that the choice
of representation is crucial. A good choice is one
that minimizes error on the training data, but equally
important is that the representation must make data
from the two domains look similar. Ben-David et al.
show that for every hypothesis h, we can provably
bound the error of h on the target domain by its error
on the source domain plus a measure of the distance
between DS and DT:
</bodyText>
<equation confidence="0.5546565">
Ex—D, L(x, R, f, h) c Ex—DSL(x, R, f, h)
+ d1(R(DS), R(DT))
</equation>
<bodyText confidence="0.9983375">
where L is a loss function, f is the target function,
and the variation divergence di is given by
</bodyText>
<equation confidence="0.985908">
d1(D, D&apos;) = 2 sup |PrD[B] − PrD&apos;[B] |(1)
BEB
</equation>
<bodyText confidence="0.974479">
where B is the set of measurable sets under D, D�.
</bodyText>
<subsectionHeader confidence="0.998091">
2.3 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999848">
Ben-David et al.’s theory provides learning bound-
s for domain adaptation under a fixed R. We now
reformulate this theory to define the task of repre-
sentation learning for domain adaptation as the fol-
lowing optimization problem: given a set of unla-
beled instances US drawn from the source domain
and unlabeled instances UT from the target domain,
as well as a set of labeled instances LS drawn from
</bodyText>
<page confidence="0.990607">
1314
</page>
<bodyText confidence="0.9987885">
the source domain, identify a function R* from the
space of possible representations R:
</bodyText>
<equation confidence="0.99915475">
R* = argmin
REIZhE
{ minl (E,;—DSL(x, R, f, h)) (2)
+ d1(R(DS), R(DT))I
</equation>
<bodyText confidence="0.999953066666667">
Unlike most learning problems, where the repre-
sentation R is fixed, this problem formulation in-
volves a search over the space of representation-
s and hypotheses. The equation also highlights an
important underlying tension: the best representa-
tion for the source domain would naturally include
domain-specific features, and allow a hypothesis to
learn domain-specific patterns. We are aiming, how-
ever, for the best general classifier, that happens to
be trained on training data from one or a few do-
mains. Domain-specific features would contribute
to distance between domains, and to classifier errors
on data taken from unseen domains. By optimizing
for this combined objective function, we allow the
optimization method to trade off between features
that are best for classifying source-domain data and
features that allow generalization to new domains.
Naturally, the objective function in Equation 2 is
completely intractable. Just finding the optimal hy-
pothesis for a fixed representation of the training da-
ta is intractable for many hypothesis classes. And
the d1 metric is intractable to compute from samples
of a distribution, although Ben-David et al. propose
some tractable bounds (2007; 2010). We view Equa-
tion 2 as a high-level goal rather than a computable
objective. We leverage prior knowledge to bias the
representation learner towards attractive regions of
the representations space R, and we develop effi-
cient, greedy optimization techniques for learning
effective representations.
</bodyText>
<subsectionHeader confidence="0.980637">
2.4 Previous Work
</subsectionHeader>
<bodyText confidence="0.9999566875">
There is a long tradition of research on representa-
tions for NLP, mostly falling into one of three cat-
egories: 1) vector space models and dimensionality
reduction techniques (Salton and McGill, 1983; Tur-
ney and Pantel, 2010; Sahlgren, 2005; Deerwester et
al., 1990; Honkela, 1997) 2) using structured repre-
sentations to identify clusters based on distributional
similarity, and using those clusters as features (Lin
and Wu, 2009; Candito and Crabb´e, 2009; Huang
and Yates, 2009; Ahuja and Downey, 2010; Turi-
an et al., 2010; Huang et al., 2011); 3) and struc-
tured representations that induce multi-dimensional
real-valued features (Dhillon et al., 2011; Emami et
al., 2003; Morin and Bengio, 2005). Our work fall-
s into the second category, but builds on the pre-
vious work by demonstrating how to improve the
distributional-similarity clusters with prior knowl-
edge. To our knowledge, we are the first to apply
semi-supervised representation learning techniques
for structured NLP tasks.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daum´e
III, 2007; Jiang and Zhai, 2007; Daum´e III et al.,
2010). Learning bounds are known (Blitzer et al.,
2007; Mansour et al., 2009). A few authors have
considered domain adaptation with no labeled data
from the target domain (Blitzer et al., 2006; Huang
et al., 2011) by using features based on distributional
similarity. We demonstrate empirically that incorpo-
rating biases into this type of representation-learning
process can significantly improve results.
</bodyText>
<sectionHeader confidence="0.973964" genericHeader="method">
3 Biased Representation Learning
</sectionHeader>
<bodyText confidence="0.979929941176471">
As before, let US and UT be unlabeled data, and LS
be labeled data from the source domain only. Pre-
vious work on representation learning with Hidden
Markov Models (HMMs) (Huang and Yates, 2009)
has estimated parameters θ for the HMM from un-
labeled data alone, and then determined the Viterbi-
optimal latent states for training and test data to pro-
duce new features for a supervised classifier. The
objective function for HMM learning in this case is
marginal log-likelihood, optimized using the Baum-
Welch algorithm:
p(x, Y = y|θ) (3)
where x is a sentence, Y is the sequence of latent
random variables for the sentence, and y is an in-
stance of the latent sequence. The joint distribution
in an HMM factors into observation and transition
distributions, typically mixtures of multinomials:
</bodyText>
<equation confidence="0.9559826">
p(x, y|θ) = P(y1)P(x1|y1) 11 P(yi|yi−1)P(xi|yi)
i&gt;2
� �
L(θ) = log
xEUSUUT y
</equation>
<page confidence="0.966106">
1315
</page>
<figureCaption confidence="0.5802125">
Figure 1: Illustration of how the entropy bias is incor-
porated into HMM learning. The dotted oval shows the
space of desired distributions in the hidden space, which
have small or zero entropy with the real labels. The learn-
ing algorithm aims to maximize the log-likelihood of the
unlabeled data, and to minimize the KL divergence be-
tween the real distribution, p,,,,, and the closest desired
distribution, p,
</figureCaption>
<bodyText confidence="0.999992464285714">
Intuitively, this form of representation learning i-
dentifies clusters of distributionally-similar words:
those words with the same Viterbi-optimal latent s-
tate. The Viterbi-optimal latent states are then used
as features for the supervised classifier. Our previ-
ous work (2009) has shown that the features from
the learned HMM significantly improve the accura-
cy of POS taggers and chunkers on benchmark do-
main adaptation datasets.
We use the HMM model from our previous work
(2009) as our baseline. Our techniques follow the
same general setup, as it provides an efficient and
empirically-proven starting point for exploring (one
part of) the space of possible representations. Note,
however, that the HMM on its own does not provide
even an approximate solution to the objective func-
tion in our problem formulation (Eqn. 2), since it
makes no attempt to find the representation that min-
imizes loss on labeled data. To address this and other
concerns, we modify the objective function for HM-
M training. Specifically, we encode biases for rep-
resentation learning by defining a set of properties φ
that we believe a good representation function would
minimize. One possible bias is that the HMM states
should be predictive of the labels in labeled training
data. We can encode this as a property that computes
the entropy between the HMM states and the label-
s. For example, in Figure 1, we want to learn the
best HMM distribution for the sentence “Innocen-
t bystanders are often the victims” for POS tagging
task. The hidden sequence y1, y2, y3, y4, y5, y6 can
have any distribution p1, p2, p3, ..., pm, ..., per, from
the latent space Y. Since we are doing POS tagging,
we want the distribution to learn the information en-
coded in the original POS labels “JJ NNS RB VBP
DT NNS”. Therefore, by calculating the entropy be-
tween the hidden sequence and real labels, we can
identify a subset of desired distributions that have
low entropy, shown in the dotted oval. By minimiz-
ing the KL divergence between the learned distribu-
tion and the set of desired distributions, we can find
the best distribution which is the closest to our de-
sire.
The following subsections describe the specific
properties we investigate; here we show how to in-
corporate them into the objective function. Let z
be the sequence of labels in LS, and let φ(x, y, z)
be a property of the completed data that we wish
the learned representation to minimize, based on our
prior beliefs. Let Q be the subspace of the possible
distributions over Y that have a small expected val-
ue for φ: Q = {q(Y)|EY—q[φ(x, Y, z)] ≤ ξ}, for
some constant ξ. We then add penalty terms to the
objective function (3) for the divergence between the
HMM distribution p and the “good” distributions q,
as well as for ξ:
</bodyText>
<equation confidence="0.995369666666667">
L(θ) − min [KL(q(Y)||p(Y|x, θ)) + σ|ξ|] (4)
q,�
s.t. EY—q[φ(x, Y, z)] ≤ ξ (5)
</equation>
<bodyText confidence="0.981192142857143">
where KL is the Kullback-Leibler divergence, and
σ is a free parameter indicating how important the
bias is compared with the marginal log likelihood.
To incorporate multiple biases, we define a vec-
tor of properties φ, and we constrain each property
φi ≤ ξi. Everything else remains the same, except
that in the penalty term σ|ξ|, the absolute value is
replaced with a suitable norm: σ kξk. To allow our-
selves to place weights on the relative importance
of the different biases, we use a norm of the form
�
kxkA = (x�Ax), where A is a diagonal matrix
whose diagonal entries Aii are free parameters that
provide weights on the different properties. For our
</bodyText>
<figure confidence="0.996504285714286">
victims
Innocent bystanders are often the
y6
y1 y2 y3 y4 y5
pm
KL(pm  ||pn)
...
P2
...
P(Y)
Eφ,.,py(Y,z)
Innocent bystanders are often the victims
JJ NNS RB VBP DT
NNS
</figure>
<page confidence="0.980467">
1316
</page>
<bodyText confidence="0.999891">
experiments, we set the free parameters σ and Aii
using a grid search over development data, as de-
scribed in Section 5.1
</bodyText>
<subsectionHeader confidence="0.998658">
3.1 A Bias for Task-specific Representations
</subsectionHeader>
<bodyText confidence="0.999558058823529">
Current representation learning techniques are unsu-
pervised, so they will generate the exact same repre-
sentation for different tasks. Yet it is exceedingly
rare that two state-of-the-art NLP systems for differ-
ent tasks share the same feature set, even if they do
tend to share some core set of lexical features.
Traditional non-learned (i.e., manually-
engineered) representations essentially always
include task-specific features. In response, we
propose to bias our representation learning such
that the learned representations are optimized for a
specific task. In particular, we propose a property
that measures how difficult it is to predict the labels
in training data, given the learned latent states.
Our entropy property uses conditional entropy of
the labels given the latent state as the measure of
unpredictability:
</bodyText>
<equation confidence="0.9859805">
φentropy(y, z) = � � P(yi, zi) log P�(zi|yi) (6)
i
</equation>
<bodyText confidence="0.999418333333333">
where P is the empirical probability and i indicates
the ith position in the data. We can plug this feature
into Equation 5 to obtain a new version of Equation
4 as an objective function for task-specific represen-
tations. We refer to this model as HMM+E. Un-
like previous formulations for supervised and semi-
supervised dimensionality reduction (Zhang et al.,
2007; Yang et al., 2006), our framework works effi-
ciently for structured representations.
</bodyText>
<subsectionHeader confidence="0.998095">
3.2 A Bias for Domain-Independent Features
</subsectionHeader>
<bodyText confidence="0.999949">
Following the theory in Section 2.2, we devise a bi-
ased objective to provide an explicit mechanism for
minimizing the distance between the source and tar-
get domain. As before, we construct a property of
the completed data:
</bodyText>
<equation confidence="0.953538">
φdistance(y) = d1(�PS, PT)
</equation>
<bodyText confidence="0.955940272727273">
where �PS(Y ) is the empirical distribution over la-
tent state values estimated from source-domain la-
tent states, and similarly for PT(Y). Essentially,
1Note that t , unlike A and or, is not a free parameter. It is
explicitly minimized in the modified objective function.
minimizing this property will bias the the represen-
tation towards features that appear approximately as
often in the source domain as the target domain. We
refer to the model trained with a bias of minimiz-
ing φdistance as HMM+D, and the model with both
φdistance and φentropy biases as HMM+D+E.
</bodyText>
<subsectionHeader confidence="0.8524665">
3.3 A Bias for Multi-Dimensional
Representations
</subsectionHeader>
<bodyText confidence="0.999965285714286">
Words are multidimensional objects. In English,
words can be nouns or verbs, singular or plural,
count or mass, just to name a few dimensions along
which they may vary. Factorial HMMs (FHMM-
s) (Ghahramani and Jordan, 1997) can learn multi-
dimensional models, but inference and learning are
complex and computationally expensive even in su-
pervised settings. Our previous work (2010) creat-
ed a multi-dimensional representation called an “I-
HMM” by training several HMM layers indepen-
dently; we showed that by finding several latent cat-
egories for each word, this representation can pro-
vide useful and domain-independent features for su-
pervised learners. In this work, we also learn a sim-
ilar multi-dimensional model (I-HMM+D+E), but
within each layer we add in the two biases described
above. While more efficient than FHMMs, the draw-
back of these I-HMM-based models is that there
is no mechanism to encourage the different HMM
models to learn different things. As a result, the lay-
ers may produce similar or equivalent features de-
scribing the dominant aspect of distributional sim-
ilarity in the data, but miss features that are less
strong, but still important, in the data.
To encourage learning a truly multi-dimensional
representation, we add a bias towards I-HMM mod-
els in which each layer is different from all previ-
ous layers. We define an entropy-based predictabili-
ty property that measures how predictable each pre-
vious layer is, given the current one. Formally, let
yli denote the hidden state at the ith position in lay-
er l of the model. For a given layer l, this proper-
ty measures the conditional entropy of ym given yl,
summed over layers m &lt; l, and subtracts this from
the maximum possible entropy:
</bodyText>
<equation confidence="0.838673333333333">
φpredict
l (y) = MAX+ � P�(yli, ymi ) log P� (ym i |yl i)
i;m&lt;l
</equation>
<bodyText confidence="0.970838">
The entropy between layer l and the previous layer-
</bodyText>
<page confidence="0.971425">
1317
</page>
<bodyText confidence="0.994587833333333">
s m measures how unpredictable the previous lay-
ers are, given layer l. By biasing the model such
that MAX minus the entropy approaches zero, we
encourage layer l towards completely different fea-
tures from previous layers. We call the model with
this bias P-HMM+D+E.
</bodyText>
<sectionHeader confidence="0.991904" genericHeader="method">
4 Efficient Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999811888888889">
Several machine learning paradigms have been de-
veloped recently for incorporating biases and con-
straints into parameter estimation (Liang et al.,
2009; Chang et al., 2007; Mann and McCallum,
2007). We leverage the Posterior Regularization
(PR) framework for our problem because of its flex-
ibility in handling different kinds of biases; we pro-
vide a brief overview of the technique here, but see
(Ganchev et al., 2010) for full details.
</bodyText>
<subsectionHeader confidence="0.996744">
4.1 Overview of PR
</subsectionHeader>
<bodyText confidence="0.999974888888889">
PR introduces a modified EM algorithm to handle
constrained objectives, like Equation 4. The modi-
fied E-step estimates a distribution q(Y) that is close
to the current estimate of p(Y|x, θ), but also close
to the ideal set of distributions that (in expectation)
have φ = 0 for each property φ. The M step re-
mains the same, except that it re-estimates parame-
ters with respect to expected latent states computed
with q rather than p.
</bodyText>
<equation confidence="0.990515666666667">
M step:
θt+1 = argmax Eqt+1[log p(x, Y|θt))]
�
</equation>
<bodyText confidence="0.999723666666667">
To make the optimization task in the E-step more
tractable, PR transforms it to a dual problem:
where k·k. is the dual norm of k·k. The gradient of
this dual objective is −Eq[φ(x, Y, z)]. A projected
subgradient descent algorithm is used to perform the
optimization.
</bodyText>
<subsectionHeader confidence="0.99769">
4.2 Modifying φ for Tractability
</subsectionHeader>
<bodyText confidence="0.999990944444444">
In unstructured settings, this optimization problem
is relatively straightforward. However, for struc-
tured representations, we need to ensure that the
dynamic programming algorithms needed for infer-
ence remain tractable for the biased objectives. For
efficient PR over structured models, the properties φ
need to be decomposed as a sum over the cliques in
the structured model. Unfortunately, the properties
we mention above do not decompose so nicely, so
we must resort to approximations.
In order to efficiently compute the expected val-
ue of the entropy property with respect to Y ∼ q,
we need to be able to compute each componen-
t EYi—q[φentropy(Yi,zi)] separately. Yet P depends
on the setting of other latent states Yj in the corpus.
To avoid this problem, we pre-compute the expected
empirical distributions over the completed data. For
each specific value y and z:
</bodyText>
<equation confidence="0.9984425">
|x|
L Pq(y,z) = 11[zi = z]q(Yi = y)
|LS |x i=1
|x|
Pq (y) = LS |LL q(Yi = y)
x i=1
</equation>
<bodyText confidence="0.99998095">
These expected empirical distributions �Pq can be
computed efficiently using standard inference algo-
rithms, such as the forward algorithm for HMMs.
Note that �Pq depends on q, but unlike the original
P from Equation 6, they do not depend on the data
completions y. Thus we can compute �Pq once for
each qt, and then substitute it for P for all values
of Y in the computation of EY—qφentropy(Y, z),
making this computation tractable. For the entropy-
based predictability properties, the calculation is
similar, but instead of using the label z, we use the
decoded states yli from previous layers.
For the distance property, Ben-David et al.’s anal-
ysis depends on a particular notion of distance (E-
qn. 1) that is computationally intractable. They also
propose more tractable lower bounds, but these are
again incompatible with the PR framework. Since
no computationally feasible exact algorithm exists
for this distance feature, we resort to a crude but ef-
ficient approximation of this measure: for each pos-
</bodyText>
<figure confidence="0.915777">
E step: min KL(q(Y)||p(Y|x, θt)) + σ kξk
qt+1 = arg min �
q s.t. Eq[φ(x, Y, z)] ≤ ξ
max L− log p(Y|x, θ) exp{−λ·φ(x, Y, z)}
a?0111a11*&lt;_0r Y
</figure>
<page confidence="0.953939">
1318
</page>
<bodyText confidence="0.951504">
sible value y of the latent states, we define:
</bodyText>
<equation confidence="0.9581015">
�
φdist
y (Y) =
ijxiEUg
1[yi = y]q(Yi = y)
|UT|
</equation>
<bodyText confidence="0.9998628">
Each of these individual properties is tractable for
structured models. Combining these properties us-
ing the · A norm results in a Euclidean distance
(weighted by A) between the frequencies of features
in each domain, rather than di distance.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999968384615385">
We tested the structured representations with biases
on two NLP tasks: Chinese POS tagging and En-
glish NER. In both cases, we use a domain adapta-
tion setting where no labeled data is available for the
target domain — a particularly difficult setting, but
one that provides a strong test for an NLP system’s
ability to generalize. In our work (Huang and Yates,
2009), we used a plain HMM for domain adaptation
tasks in which there is labeled source data and un-
labeled source and target data, but no labeled target
data for training. Therefore, here, we use the HMM
technique as a baseline, and build on it by including
biases.
</bodyText>
<subsectionHeader confidence="0.952934">
5.1 Chinese POS tagging
</subsectionHeader>
<bodyText confidence="0.999989789473684">
We use the UCLA Corpus of Written Chinese,
which is a part of The Lancaster Corpus of Man-
darin Chinese (LCMC). The UCLA Corpus consists
of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres. We use gold-standard
word segmentation labels during training and test-
ing. The LCMC tagset consists of 50 Chinese POS
tags. Each genre averages 5284 word tokens, for a
total of 68,695 tokens among all genres. We use the
‘news’ genre as our source domain and randomly se-
lect 20% of every other genre as labeled test data. To
train our representation models, we use the ‘news’
text, plus the remaining 80% of the texts from the
other genres. We use 90% of the labeled news text
for training, and 10% for development. We replace
hapax legomena in the unlabeled data with the spe-
cial symbol *UNKNOWN*, and also do the same
for word types in the labeled test sets that never ap-
pear in our unlabeled training texts.
</bodyText>
<figureCaption confidence="0.997855">
Figure 2: Grid search for parameters on news text
</figureCaption>
<bodyText confidence="0.949393189189189">
Following our previous HMM setup in (Huang
and Yates, 2009) for consistency, we use an HMM
with 80 latent states. For our multi-layer models,
we use 7 layers of HMMs. We tuned the free pa-
rameters σ and A on development data. We varied
σ from 0.1 to 1000. To tune A, we start by setting
the diagonal entry for φentropy to 1, without loss of
generality. We then tie all the entries in A for φdist
y
to a single parameter α, and tie all of the entries for
φpredict
y to a parameter β. We vary α and β over the
set {0.01,0.1,1,10,100}. Figure 2 shows our results
for σ and α on news development data. A setting
of α = 0.01 and σ = 100 performs best, with all
σ = 100 doing reasonably well. Results for each
of these models on the general fiction test text con-
firm the general trends seen on development data —
a comforting sign, since this indicates we can opti-
mize the free parameters on in-domain development
data, rather than requiring labeled data from the tar-
get domain. Our models tended to perform better
with increasing β on development data, though with
diminishing returns. We pick the largest setting test-
ed, β = 100, for our final models.
We use a linear-chain Conditional Random Field
(CRF) for our supervised classifier. To incorporate
the learned representations, we use the Viterbi Algo-
rithm to find the optimal latent state sequence from
each HMM-based model and then use the optimal
states as features in the CRF. Table 1 presents the
full list of features in the CRF. To handle Chinese,
we add in two features introduced in previous work
(Wang et al., 2009): radical features and repeated
characters. A radical is a portion of a Chinese char-
acter that consists of a small number of pen or brush
strokes in a regular pattern.
</bodyText>
<figure confidence="0.997184555555556">
Accuracy
0.928
0.923
0.918
0.913
0.908
0.903
0.898
0.893
0.888
News Domain (development data)
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
0.1 1 10 100 1000
σ (log scale)
1[yi = y]q(Yi = y)
|US|
��
ijxiEUT
</figure>
<page confidence="0.855447">
1319
</page>
<figureCaption confidence="0.996001">
Figure 3: Validating parameter settings on fiction text
</figureCaption>
<figure confidence="0.942617857142857">
CRF Feature Set
Transition
bz1[zj = z]
bz,z/1[zj = z and zj_1 = z&apos;]
Word
b,,,,z1[xj = w and zj = z]
Radical
bz,r1[�cEx,radical(c) = r and zj = z]
Repeated Words
bA,B,z1[xj = AABB and zj = z]
bA,z1[(xj = AAMor xj = AAJ ) and zj = z]
bA,B,z1[xj = ABAB and zj = z]
Features from Representation Learning
by,l,z1[ylj = y and zj = z]
</figure>
<tableCaption confidence="0.813905">
Table 1: Features used in our Chinese POS tagging
CRF systems. c represents a character within a word.
</tableCaption>
<bodyText confidence="0.999547881355933">
Table 2 shows our results. We compare against
the Baseline CRF without any additional representa-
tions and the unbiased HMM, a state-of-the-art do-
main adaptation technique from previous work, over
all 13 domains (source and target). We also com-
pare against a state-of-the-art Chinese POS tagger
for in-domain text, the CRF-based Stanford tagger
(Tseng et al., 2005), retrained for this corpus. H-
MM+D+E outperforms the Stanford tagger on 10
out of 12 target domains and the unbiased HMM on
all domains, while the P-HMM+D+E outperform-
s the Stanford tagger (2.6% average improvement)
and HMM (1.7%) on all 12 target domains. The I-
HMM+D+E is slightly better than the HMM+D+E
(.3%), but incorporating the multi-dimensional bias
(P-HMM+D+E) adds an additional 0.6% improve-
ment.
Our interpretation for the success of I-
HMM+D+E and P-HMM+D+E is that the increase
in the state space of the models yields improved
performance. Because P-HMM+D+E biases against
redundant states found in I-HMM+D+E, it effective-
ly increases the state space beyond I-HMM+D+E.
Ahuja and Downey (2010) and our own work with
HMMs as representations (2010) have previously
shown that increasing the state space of the HMM
can significantly improve the representation, but
memory constraints eventually prevent further
progress this way. The I-HMM+D+E and P-
HMM+D+E models can provide similar benefits,
but because they split parameters across multiple
HMMs, they can accommodate much greater state
spaces in the same amount of memory.
We also tested the entropy and distance biases
separately. Figure 4 shows the result of the distance-
biased HMM+D on the general-fiction test text, as
we vary Q over the set {0.1,1,10,100,1000} (we ob-
served similar results for other domains). For all val-
ues of Q, the biased representation outperforms the
unbiased HMM. There is also a strong negative cor-
relation between the expected value of I IOdiatanceI I
and the resulting accuracy, as expected from Ben-
David et al.’s theoretical analysis. The HMM+E
model outperforms the HMM on the (source) news
domain by 0.3%, but actually performs worse for
most target domains. We suspect that the entropy
feature, which is learned only from labeled source-
domain data, makes the representation biased to-
wards features that are important in the source do-
main only. However, after we add in the distance
bias and a parameter to balance the weights from
both biases, the representation is able to capture the
label information as well as the target domain fea-
tures. Thus, the representation won’t solely depend
on source data. HMM+D+E, which combines both
biases, outperforms HMM+D, suggesting that task-
specific features for domain adaptation can be help-
ful, but only if there is some control for the domain-
independence of the features.
</bodyText>
<subsectionHeader confidence="0.997598">
5.2 English Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.979734">
To evaluate on a second task, we turn to Named En-
tity Recognition. We use the training data from the
</bodyText>
<figure confidence="0.9859279375">
General Fiction Domain (test data)
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
0.1 1 10 100 1000
Accuracy
0.92
0.91
0.89
0.88
0.87
0.86
0.85
0.84
0.83
0.82
0.9
σ (log scale)
</figure>
<page confidence="0.94498">
1320
</page>
<table confidence="0.9997687">
news (source) lore reli humour gen-fic essay mystery romance sci-fi skill science adv-fic report avg
words 9774 5428 3248 3326 4913 5214 5774 5489 3070 5464 5262 5071 6662 5284
CRF w/o HMM 93.8 85.0 80.0 85.4 85.0 83.8 84.7 86.0 82.8 78.2 82.2 77.1 85.3 84.5
HMM+E 97.1 88.2 83.1 87.5 87.4 89.2 89.5 87.1 86.7 82.1 87.2 79.4 91.7 88.3
Stanford 98.8 88.4 83.5 89.0 87.5 88.4 87.4 87.5 88.6 82.7 86.0 82.1 91.7 88.7
HMM 96.9 89.7 85.2 89.6 89.4 89.0 90.1 89.0 87.0 84.9 87.8 80.0 91.4 89.2
HMM+D 97.4 89.9 85.4 89.4 89.6 89.9 90.1 88.6 87.9 85.3 87.9 80.0 92.0 89.5
HMM+D+E 97.7 90.1 86.1 89.8 90.9 89.7 90.3 89.8 88.4 85.6 87.9 81.2 92.0 89.9
I-HMM+D+E 97.8 90.5 87.0 89.1 91.1 90.2 90.0 90.5 89.8 86.0 87.1 82.2 92.1 90.2
P-HMM+D+E 98.2 91.5 87.7 89.0 91.8 91.0 89.9 91.4 90.4 87.0 87.7 83.4 92.4 90.8
</table>
<tableCaption confidence="0.97109">
Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the
Stanford tagger on all target domains. The ‘avg’ column includes source-domain development data results. Differ-
ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p &lt; 0.01 on average and on 11
out of 12 target domain. We used the two-tailed Chi-square test with Yates’ correction.
</tableCaption>
<figure confidence="0.998559607142857">
System F1
CRF without HMM 66.15
HMM+E 74.25
HMM 75.06
HMM+D 75.75
HMM+D+E 76.03
I-HMM+D+E 77.04
P-HMM+D+E 78.62
HMM+D on General Fiction Test
4.55E-05 4.60E-05 4.65E-05 4.70E-05 4.75E-05 4.80E-05
ǁEq(φdistance)ǁ
Accuracy
0.904
0.903
0.902
0.901
0.899
0.898
0.897
0.896
0.895
0.894
0.9
σ=10
σ=1 σ=0.1
σ=1000
σ=100
Unconstrained HMM
</figure>
<tableCaption confidence="0.639956">
Table 3: English Named Entity recognition results
</tableCaption>
<figureCaption confidence="0.9900465">
Figure 4: Greater distance between domains correlates
with worse target-domain tagging accuracy.
</figureCaption>
<bodyText confidence="0.999570518518519">
CoNLL 2003 shared task for our labeled training set,
consisting of 204k tokens from the newswire do-
main. We tested the system on the MUC7 formal
run test data, consisting of 59k tokens of stories on
the telecommunications and aerospace industries.
To train our representations, we use the CoNL-
L training data and the MUC7 training data without
labels. We again use a CRF, with features introduced
by Zhang and Johnson (2003) for our baseline. We
use the same setting of free parameters from our
POS tagging experiments.
Results are shown in Table 3. Our best biased
representation P-HMM+D+E outperformed the un-
biased HMM representation by 3.6%, and beats the
I-HMM+D+E by 1.6%. The domain-distance and
multi-dimensional biases help most, while the task-
specific bias helps somewhat, but only when the
domain-distance bias is included. The best sys-
tem tested on this dataset achieved a slightly bet-
ter F1 score (78.84) (Turian et al., 2010), but used
a much larger training corpus (they use RCV1 cor-
pus which contains approximately 63 million token-
s). Other studies (Turian et al., 2010; Huang et
al., 2011) have performed a detailed comparison be-
tween these types of systems, so we concentrate on
comparisons between biased and unbiased represen-
tations here.
</bodyText>
<subsectionHeader confidence="0.984947">
5.3 Does the task-specific bias actually help?
</subsectionHeader>
<bodyText confidence="0.999968444444444">
In this section, we test whether the task-specific
bias (entropy bias) actually learns something task-
specific. We learn the entropy-biased representa-
tions for two tasks on the same set of sentences,
labeled differently for the two tasks: English POS
tagging and Named Entity Recognition. Then we
switch the representations to see whether they will
help or hurt the performance on the other task. We
randomly picked 500 sentences from WSJ section
</bodyText>
<page confidence="0.953274">
1321
</page>
<table confidence="0.99743325">
Representation/Task POS Accuracy NER F1
HMM 88.5 66.3
HMM+E(POS labels) 89.7 64.5
HMM+E(NER labels) 86.5 68.0
</table>
<tableCaption confidence="0.6183112">
Table 4: Results of POS tagging and Named Entity
recognition tasks with different representations. With the
entropy-biased representation, the system has better per-
formance on the task which the bias is trained for, but
worse performance on the other task.
</tableCaption>
<bodyText confidence="0.9999755">
0-18 as our labeled training data and 500 sentences
from WSJ section 20-23 as testing data. Because
WSJ data does not have gold standard NER tags,
we manually labeled these sentences with NER tags.
For simplicity, we only use three types of NER tags:
person, organization and location. The result is
shown in Table 4. When the entropy bias uses la-
bels from the same task as the classifier, the perfor-
mance is improved: about 1.2% in accuracy on POS
tagging and 1.7% in F1 score on NER. Switching
the representations for the tasks actually hurts the
performance compared with the unbiased represen-
tation. The results suggest that the entropy bias does
indeed yield a task-specific representation.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989375">
We introduce three types of biases into represen-
tation learning for sequence labeling using the PR
framework. Our experiments on POS tagging and
NER indicate domain-independent biases and multi-
dimensional biases significantly improve the repre-
sentations, while the task-specific bias improves per-
formance on out-of-domain data if it is combined
with the domain-independent bias. Our results indi-
cate the power of representation learning in building
domain-agnostic classifiers, but also the complexi-
ty of the task and the limitations of current tech-
niques, as even the best models still fall significantly
short of in-domain performance. Important consid-
erations for future work include identifying further
effective and tractable biases, and extending beyond
sequence-labeling to other types of NLP tasks.
</bodyText>
<sectionHeader confidence="0.998826" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9142875">
This research was supported in part by NSF grant
IIS-1065397.
</bodyText>
<sectionHeader confidence="0.991696" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998538346153846">
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems 20, Cambridge, MA. MIT
Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151–175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
M. Candito and B. Crabb´e. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138–141.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Pro-
ceedings of the ACL.
Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Neural Information Processing Systems (NIP-
S).
A. Emami, P. Xu, and F. Jelinek. 2003. Using a con-
nectionist model in a syntactical based language mod-
el. In Proceedings of the International Conference on
Spoken Language Processing, pages 372–375.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:10–49.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245–273.
</reference>
<page confidence="0.843973">
1322
</page>
<reference confidence="0.999561727272727">
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In In Pro-
ceedings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language models as representation-
s for weakly supervised nlp tasks. In Conference on
Natural Language Learning (CoNLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030–1038.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In In Proc. ICML.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
International Workshop on Artificial Intelligence and
Statistics, pages 246–252.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In Pro-
ceedings of NAACL-HLT, pages 556–563.
M. Sahlgren. 2005. An introduction to random indexing.
In In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering (TKE).
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96–102.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384–394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
Lijie Wang, Wanxiang Che, and Ting Liu. 2009. An
svmtool-based chinese pos tagger. In Journal of Chi-
nese Information Processing.
X. Yang, H. Fu, H. Zha, and J. Barlow. 2006. Semi-
supervised nonlinear dimensionality reduction. In
Proceedings of the 23rd International Conference on
Machine Learning.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
D. Zhang, Z.H. Zhou, and S. Chen. 2007. Semi-
supervised dimensionality reduction. In Proceedings
of the 7th SIAM International Conference on Data
Mining.
</reference>
<page confidence="0.956775">
1323
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686492">
<title confidence="0.999738">Biased Representation Learning for Domain Adaptation</title>
<author confidence="0.8734445">Fei Huang</author>
<author confidence="0.8734445">Alexander Temple</author>
<affiliation confidence="0.971211">Computer and Information</affiliation>
<address confidence="0.9645995">324 Wachman Philadelphia, PA</address>
<abstract confidence="0.999640818181818">Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arun Ahuja</author>
<author>Doug Downey</author>
</authors>
<title>Improved extraction assessment through better language models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT).</booktitle>
<contexts>
<context position="10190" citStr="Ahuja and Downey, 2010" startWordPosition="1578" endWordPosition="1581">e R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the sou</context>
<context position="29594" citStr="Ahuja and Downey (2010)" startWordPosition="4889" endWordPosition="4892">f 12 target domains and the unbiased HMM on all domains, while the P-HMM+D+E outperforms the Stanford tagger (2.6% average improvement) and HMM (1.7%) on all 12 target domains. The IHMM+D+E is slightly better than the HMM+D+E (.3%), but incorporating the multi-dimensional bias (P-HMM+D+E) adds an additional 0.6% improvement. Our interpretation for the success of IHMM+D+E and P-HMM+D+E is that the increase in the state space of the models yields improved performance. Because P-HMM+D+E biases against redundant states found in I-HMM+D+E, it effectively increases the state space beyond I-HMM+D+E. Ahuja and Downey (2010) and our own work with HMMs as representations (2010) have previously shown that increasing the state space of the HMM can significantly improve the representation, but memory constraints eventually prevent further progress this way. The I-HMM+D+E and PHMM+D+E models can provide similar benefits, but because they split parameters across multiple HMMs, they can accommodate much greater state spaces in the same amount of memory. We also tested the entropy and distance biases separately. Figure 4 shows the result of the distancebiased HMM+D on the general-fiction test text, as we vary Q over the </context>
</contexts>
<marker>Ahuja, Downey, 2010</marker>
<rawString>Arun Ahuja and Doug Downey. 2010. Improved extraction assessment through better language models. In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1607" citStr="Ben-David et al., 2007" startWordPosition="236" endWordPosition="239">ting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques. 1 Introduction Supervised natural language processing (NLP) systems have been widely used and have achieved impressive performance on many NLP tasks. However, they exhibit a significant drop-off in performance when tested on domains that differ from their training domains. (Gildea, 2001; Sekine, 1997; Pradhan et al., 2007) One major cause for poor performance on out of-domain texts is the traditional representation used by supervised NLP systems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2007. Analysis of representations for domain adaptation. In Advances in Neural Information Processing Systems 20, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--151</pages>
<contexts>
<context position="6866" citStr="Ben-David et al. (2010)" startWordPosition="1033" endWordPosition="1036">a 1 in only one of the dimensions. By a structured representation, we mean a function R that incorporates some form of joint inference. In this paper, we use Viterbi decoding of variants of Hidden Markov Models (HMMs) for our structured representations, although our techniques are applicable to arbitrary (Dynamic) Bayes Nets. A domain is a probability distribution D over the instance set X; R(D) denotes the induced distribution over Y. In domain adaptation tasks, a learner is given samples from a source domain DS, and is evaluated on samples from a target domain DT. 2.2 Theoretical Background Ben-David et al. (2010) give a theoretical analysis of domain adaptation which shows that the choice of representation is crucial. A good choice is one that minimizes error on the training data, but equally important is that the representation must make data from the two domains look similar. Ben-David et al. show that for every hypothesis h, we can provably bound the error of h on the target domain by its error on the source domain plus a measure of the distance between DS and DT: Ex—D, L(x, R, f, h) c Ex—DSL(x, R, f, h) + d1(R(DS), R(DT)) where L is a loss function, f is the target function, and the variation dive</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2196" citStr="Blitzer et al., 2006" startWordPosition="329" endWordPosition="332">ystems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representa</context>
<context position="11065" citStr="Blitzer et al., 2006" startWordPosition="1722" endWordPosition="1725"> previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning with Hidden Markov Models (HMMs) (Huang and Yates, 2009) has estimated parameters θ for the HMM from unlabeled data alone, and then determined the Viterbioptimal latent states for training and test data to produce ne</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jenn Wortman</author>
</authors>
<title>Learning bounds for domain adaptation.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="10927" citStr="Blitzer et al., 2007" startWordPosition="1699" endWordPosition="1702">ed features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning with Hidden Markov Models (HMMs) (Huang and Yates, 2009) has estimated paramet</context>
</contexts>
<marker>Blitzer, Crammer, Kulesza, Pereira, Wortman, 2007</marker>
<rawString>John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jenn Wortman. 2007. Learning bounds for domain adaptation. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
</authors>
<title>Improving generative statistical parsing with semi-supervised word clustering.</title>
<date>2009</date>
<booktitle>In IWPT,</booktitle>
<pages>138--141</pages>
<marker>Candito, Crabb´e, 2009</marker>
<rawString>M. Candito and B. Crabb´e. 2009. Improving generative statistical parsing with semi-supervised word clustering. In IWPT, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Guiding semisupervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="20949" citStr="Chang et al., 2007" startWordPosition="3379" endWordPosition="3382">the maximum possible entropy: φpredict l (y) = MAX+ � P�(yli, ymi ) log P� (ym i |yl i) i;m&lt;l The entropy between layer l and the previous layer1317 s m measures how unpredictable the previous layers are, given layer l. By biasing the model such that MAX minus the entropy approaches zero, we encourage layer l towards completely different features from previous layers. We call the model with this bias P-HMM+D+E. 4 Efficient Parameter Estimation Several machine learning paradigms have been developed recently for incorporating biases and constraints into parameter estimation (Liang et al., 2009; Chang et al., 2007; Mann and McCallum, 2007). We leverage the Posterior Regularization (PR) framework for our problem because of its flexibility in handling different kinds of biases; we provide a brief overview of the technique here, but see (Ganchev et al., 2010) for full details. 4.1 Overview of PR PR introduces a modified EM algorithm to handle constrained objectives, like Equation 4. The modified E-step estimates a distribution q(Y) that is close to the current estimate of p(Y|x, θ), but also close to the ideal set of distributions that (in expectation) have φ = 0 for each property φ. The M step remains th</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semisupervision with constraint-driven learning. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL Workshop on Domain Adaptation (DANLP).</booktitle>
<marker>Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the ACL Workshop on Domain Adaptation (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="9952" citStr="Deerwester et al., 1990" startWordPosition="1542" endWordPosition="1545">. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge.</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2263" citStr="Dhillon et al., 2011" startWordPosition="341" endWordPosition="344">atures, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incorporate the linguistic or domain expert knowl</context>
<context position="10340" citStr="Dhillon et al., 2011" startWordPosition="1601" endWordPosition="1604">earch on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et </context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. 2011. Multi-view learning of word embeddings via cca. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Emami</author>
<author>P Xu</author>
<author>F Jelinek</author>
</authors>
<title>Using a connectionist model in a syntactical based language model.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>372--375</pages>
<contexts>
<context position="10360" citStr="Emami et al., 2003" startWordPosition="1605" endWordPosition="1608">ns for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few au</context>
</contexts>
<marker>Emami, Xu, Jelinek, 2003</marker>
<rawString>A. Emami, P. Xu, and F. Jelinek. 2003. Using a connectionist model in a syntactical based language model. In Proceedings of the International Conference on Spoken Language Processing, pages 372–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--10</pages>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:10–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Factorial hidden markov models.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="18874" citStr="Ghahramani and Jordan, 1997" startWordPosition="3031" endWordPosition="3034">It is explicitly minimized in the modified objective function. minimizing this property will bias the the representation towards features that appear approximately as often in the source domain as the target domain. We refer to the model trained with a bias of minimizing φdistance as HMM+D, and the model with both φdistance and φentropy biases as HMM+D+E. 3.3 A Bias for Multi-Dimensional Representations Words are multidimensional objects. In English, words can be nouns or verbs, singular or plural, count or mass, just to name a few dimensions along which they may vary. Factorial HMMs (FHMMs) (Ghahramani and Jordan, 1997) can learn multidimensional models, but inference and learning are complex and computationally expensive even in supervised settings. Our previous work (2010) created a multi-dimensional representation called an “IHMM” by training several HMM layers independently; we showed that by finding several latent categories for each word, this representation can provide useful and domain-independent features for supervised learners. In this work, we also learn a similar multi-dimensional model (I-HMM+D+E), but within each layer we add in the two biases described above. While more efficient than FHMMs, </context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden markov models. Machine Learning, 29(2-3):245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus Variation and Parser Performance.</title>
<date>2001</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1420" citStr="Gildea, 2001" startWordPosition="208" endWordPosition="209">three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques. 1 Introduction Supervised natural language processing (NLP) systems have been widely used and have achieved impressive performance on many NLP tasks. However, they exhibit a significant drop-off in performance when tested on domains that differ from their training domains. (Gildea, 2001; Sekine, 1997; Pradhan et al., 2007) One major cause for poor performance on out of-domain texts is the traditional representation used by supervised NLP systems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biome</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus Variation and Parser Performance. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Honkela</author>
</authors>
<title>Self-organizing maps of words for natural language processing applications. In</title>
<date>1997</date>
<booktitle>In Proceedings of the International ICSC Symposium on Soft Computing.</booktitle>
<contexts>
<context position="9968" citStr="Honkela, 1997" startWordPosition="1546" endWordPosition="1547">bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledg</context>
</contexts>
<marker>Honkela, 1997</marker>
<rawString>T. Honkela. 1997. Self-organizing maps of words for natural language processing applications. In In Proceedings of the International ICSC Symposium on Soft Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2219" citStr="Huang and Yates, 2009" startWordPosition="333" endWordPosition="336">l., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incor</context>
<context position="10166" citStr="Huang and Yates, 2009" startWordPosition="1574" endWordPosition="1577">he representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is a</context>
<context position="11505" citStr="Huang and Yates, 2009" startWordPosition="1788" endWordPosition="1791">rning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning with Hidden Markov Models (HMMs) (Huang and Yates, 2009) has estimated parameters θ for the HMM from unlabeled data alone, and then determined the Viterbioptimal latent states for training and test data to produce new features for a supervised classifier. The objective function for HMM learning in this case is marginal log-likelihood, optimized using the BaumWelch algorithm: p(x, Y = y|θ) (3) where x is a sentence, Y is the sequence of latent random variables for the sentence, and y is an instance of the latent sequence. The joint distribution in an HMM factors into observation and transition distributions, typically mixtures of multinomials: p(x, </context>
<context position="24834" citStr="Huang and Yates, 2009" startWordPosition="4047" endWordPosition="4050">Each of these individual properties is tractable for structured models. Combining these properties using the · A norm results in a Euclidean distance (weighted by A) between the frequencies of features in each domain, rather than di distance. 5 Experiments We tested the structured representations with biases on two NLP tasks: Chinese POS tagging and English NER. In both cases, we use a domain adaptation setting where no labeled data is available for the target domain — a particularly difficult setting, but one that provides a strong test for an NLP system’s ability to generalize. In our work (Huang and Yates, 2009), we used a plain HMM for domain adaptation tasks in which there is labeled source data and unlabeled source and target data, but no labeled target data for training. Therefore, here, we use the HMM technique as a baseline, and build on it by including biases. 5.1 Chinese POS tagging We use the UCLA Corpus of Written Chinese, which is a part of The Lancaster Corpus of Mandarin Chinese (LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POStagged text in 13 genres. We use gold-standard word segmentation labels during training and testing. The LCMC tagset consists of 50 Ch</context>
<context position="26145" citStr="Huang and Yates, 2009" startWordPosition="4282" endWordPosition="4285">all genres. We use the ‘news’ genre as our source domain and randomly select 20% of every other genre as labeled test data. To train our representation models, we use the ‘news’ text, plus the remaining 80% of the texts from the other genres. We use 90% of the labeled news text for training, and 10% for development. We replace hapax legomena in the unlabeled data with the special symbol *UNKNOWN*, and also do the same for word types in the labeled test sets that never appear in our unlabeled training texts. Figure 2: Grid search for parameters on news text Following our previous HMM setup in (Huang and Yates, 2009) for consistency, we use an HMM with 80 latent states. For our multi-layer models, we use 7 layers of HMMs. We tuned the free parameters σ and A on development data. We varied σ from 0.1 to 1000. To tune A, we start by setting the diagonal entry for φentropy to 1, without loss of generality. We then tie all the entries in A for φdist y to a single parameter α, and tie all of the entries for φpredict y to a parameter β. We vary α and β over the set {0.01,0.1,1,10,100}. Figure 2 shows our results for σ and α on news development data. A setting of α = 0.01 and σ = 100 performs best, with all σ = </context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</booktitle>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring representation-learning approaches to domain adaptation. In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
<author>Arun Ahuja</author>
<author>Doug Downey</author>
</authors>
<title>Language models as representations for weakly supervised nlp tasks.</title>
<date>2011</date>
<booktitle>In Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="10232" citStr="Huang et al., 2011" startWordPosition="1587" endWordPosition="1590">ation techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; </context>
<context position="34417" citStr="Huang et al., 2011" startWordPosition="5689" endWordPosition="5692">from our POS tagging experiments. Results are shown in Table 3. Our best biased representation P-HMM+D+E outperformed the unbiased HMM representation by 3.6%, and beats the I-HMM+D+E by 1.6%. The domain-distance and multi-dimensional biases help most, while the taskspecific bias helps somewhat, but only when the domain-distance bias is included. The best system tested on this dataset achieved a slightly better F1 score (78.84) (Turian et al., 2010), but used a much larger training corpus (they use RCV1 corpus which contains approximately 63 million tokens). Other studies (Turian et al., 2010; Huang et al., 2011) have performed a detailed comparison between these types of systems, so we concentrate on comparisons between biased and unbiased representations here. 5.3 Does the task-specific bias actually help? In this section, we test whether the task-specific bias (entropy bias) actually learns something taskspecific. We learn the entropy-biased representations for two tasks on the same set of sentences, labeled differently for the two tasks: English POS tagging and Named Entity Recognition. Then we switch the representations to see whether they will help or hurt the performance on the other task. We r</context>
</contexts>
<marker>Huang, Yates, Ahuja, Downey, 2011</marker>
<rawString>Fei Huang, Alexander Yates, Arun Ahuja, and Doug Downey. 2011. Language models as representations for weakly supervised nlp tasks. In Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10852" citStr="Jiang and Zhai, 2007" startWordPosition="1686" endWordPosition="1689">; 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning wit</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning from measurements in exponential families.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="20929" citStr="Liang et al., 2009" startWordPosition="3375" endWordPosition="3378">subtracts this from the maximum possible entropy: φpredict l (y) = MAX+ � P�(yli, ymi ) log P� (ym i |yl i) i;m&lt;l The entropy between layer l and the previous layer1317 s m measures how unpredictable the previous layers are, given layer l. By biasing the model such that MAX minus the entropy approaches zero, we encourage layer l towards completely different features from previous layers. We call the model with this bias P-HMM+D+E. 4 Efficient Parameter Estimation Several machine learning paradigms have been developed recently for incorporating biases and constraints into parameter estimation (Liang et al., 2009; Chang et al., 2007; Mann and McCallum, 2007). We leverage the Posterior Regularization (PR) framework for our problem because of its flexibility in handling different kinds of biases; we provide a brief overview of the technique here, but see (Ganchev et al., 2010) for full details. 4.1 Overview of PR PR introduces a modified EM algorithm to handle constrained objectives, like Equation 4. The modified E-step estimates a distribution q(Y) that is close to the current estimate of p(Y|x, θ), but also close to the ideal set of distributions that (in expectation) have φ = 0 for each property φ. T</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning from measurements in exponential families. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>X Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>1030--1038</pages>
<contexts>
<context position="10116" citStr="Lin and Wu, 2009" startWordPosition="1566" endWordPosition="1569">ation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation ha</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>D. Lin and X Wu. 2009. Phrase clustering for discriminative learning. In ACL-IJCNLP, pages 1030–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>A McCallum</author>
</authors>
<title>Simple, robust, scalable semi-supervised learning via expectation regularization. In</title>
<date>2007</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="20975" citStr="Mann and McCallum, 2007" startWordPosition="3383" endWordPosition="3386"> entropy: φpredict l (y) = MAX+ � P�(yli, ymi ) log P� (ym i |yl i) i;m&lt;l The entropy between layer l and the previous layer1317 s m measures how unpredictable the previous layers are, given layer l. By biasing the model such that MAX minus the entropy approaches zero, we encourage layer l towards completely different features from previous layers. We call the model with this bias P-HMM+D+E. 4 Efficient Parameter Estimation Several machine learning paradigms have been developed recently for incorporating biases and constraints into parameter estimation (Liang et al., 2009; Chang et al., 2007; Mann and McCallum, 2007). We leverage the Posterior Regularization (PR) framework for our problem because of its flexibility in handling different kinds of biases; we provide a brief overview of the technique here, but see (Ganchev et al., 2010) for full details. 4.1 Overview of PR PR introduces a modified EM algorithm to handle constrained objectives, like Equation 4. The modified E-step estimates a distribution q(Y) that is close to the current estimate of p(Y|x, θ), but also close to the ideal set of distributions that (in expectation) have φ = 0 for each property φ. The M step remains the same, except that it re-</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G. S. Mann and A. McCallum. 2007. Simple, robust, scalable semi-supervised learning via expectation regularization. In In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mansour</author>
<author>M Mohri</author>
<author>A Rostamizadeh</author>
</authors>
<title>Domain adaptation with multiple sources.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="10950" citStr="Mansour et al., 2009" startWordPosition="1703" endWordPosition="1706">t al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning with Hidden Markov Models (HMMs) (Huang and Yates, 2009) has estimated parameters θ for the HMM from </context>
</contexts>
<marker>Mansour, Mohri, Rostamizadeh, 2009</marker>
<rawString>Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="10385" citStr="Morin and Bengio, 2005" startWordPosition="1609" endWordPosition="1612">alling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered dom</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the International Workshop on Artificial Intelligence and Statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>556--563</pages>
<contexts>
<context position="1457" citStr="Pradhan et al., 2007" startWordPosition="212" endWordPosition="215">xperiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques. 1 Introduction Supervised natural language processing (NLP) systems have been widely used and have achieved impressive performance on many NLP tasks. However, they exhibit a significant drop-off in performance when tested on domains that differ from their training domains. (Gildea, 2001; Sekine, 1997; Pradhan et al., 2007) One major cause for poor performance on out of-domain texts is the traditional representation used by supervised NLP systems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2007</marker>
<rawString>Sameer Pradhan, Wayne Ward, and James H. Martin. 2007. Towards robust semantic role labeling. In Proceedings of NAACL-HLT, pages 556–563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE).</booktitle>
<contexts>
<context position="9927" citStr="Sahlgren, 2005" startWordPosition="1540" endWordPosition="1541"> Ben-David et al. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clust</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>M. Sahlgren. 2005. An introduction to random indexing. In In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="9886" citStr="Salton and McGill, 1983" startWordPosition="1531" endWordPosition="1534">o compute from samples of a distribution, although Ben-David et al. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to im</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>The domain dependence of parsing.</title>
<date>1997</date>
<booktitle>In Proc. Applied Natural Language Processing (ANLP),</booktitle>
<pages>96--102</pages>
<contexts>
<context position="1434" citStr="Sekine, 1997" startWordPosition="210" endWordPosition="211"> biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques. 1 Introduction Supervised natural language processing (NLP) systems have been widely used and have achieved impressive performance on many NLP tasks. However, they exhibit a significant drop-off in performance when tested on domains that differ from their training domains. (Gildea, 2001; Sekine, 1997; Pradhan et al., 2007) One major cause for poor performance on out of-domain texts is the traditional representation used by supervised NLP systems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Re</context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. The domain dependence of parsing. In Proc. Applied Natural Language Processing (ANLP), pages 96–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>Morphological features help pos tagging of unknown words across language varieties.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="28891" citStr="Tseng et al., 2005" startWordPosition="4778" endWordPosition="4781">BB and zj = z] bA,z1[(xj = AAMor xj = AAJ ) and zj = z] bA,B,z1[xj = ABAB and zj = z] Features from Representation Learning by,l,z1[ylj = y and zj = z] Table 1: Features used in our Chinese POS tagging CRF systems. c represents a character within a word. Table 2 shows our results. We compare against the Baseline CRF without any additional representations and the unbiased HMM, a state-of-the-art domain adaptation technique from previous work, over all 13 domains (source and target). We also compare against a state-of-the-art Chinese POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng et al., 2005), retrained for this corpus. HMM+D+E outperforms the Stanford tagger on 10 out of 12 target domains and the unbiased HMM on all domains, while the P-HMM+D+E outperforms the Stanford tagger (2.6% average improvement) and HMM (1.7%) on all 12 target domains. The IHMM+D+E is slightly better than the HMM+D+E (.3%), but incorporating the multi-dimensional bias (P-HMM+D+E) adds an additional 0.6% improvement. Our interpretation for the success of IHMM+D+E and P-HMM+D+E is that the increase in the state space of the models yields improved performance. Because P-HMM+D+E biases against redundant states</context>
</contexts>
<marker>Tseng, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Daniel Jurafsky, and Christopher Manning. 2005. Morphological features help pos tagging of unknown words across language varieties. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>384--394</pages>
<contexts>
<context position="2240" citStr="Turian et al., 2010" startWordPosition="337" endWordPosition="340"> depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incorporate the linguistic</context>
<context position="10211" citStr="Turian et al., 2010" startWordPosition="1582" endWordPosition="1586">cient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domain</context>
<context position="34250" citStr="Turian et al., 2010" startWordPosition="5660" endWordPosition="5663">UC7 training data without labels. We again use a CRF, with features introduced by Zhang and Johnson (2003) for our baseline. We use the same setting of free parameters from our POS tagging experiments. Results are shown in Table 3. Our best biased representation P-HMM+D+E outperformed the unbiased HMM representation by 3.6%, and beats the I-HMM+D+E by 1.6%. The domain-distance and multi-dimensional biases help most, while the taskspecific bias helps somewhat, but only when the domain-distance bias is included. The best system tested on this dataset achieved a slightly better F1 score (78.84) (Turian et al., 2010), but used a much larger training corpus (they use RCV1 corpus which contains approximately 63 million tokens). Other studies (Turian et al., 2010; Huang et al., 2011) have performed a detailed comparison between these types of systems, so we concentrate on comparisons between biased and unbiased representations here. 5.3 Does the task-specific bias actually help? In this section, we test whether the task-specific bias (entropy bias) actually learns something taskspecific. We learn the entropy-biased representations for two tasks on the same set of sentences, labeled differently for the two ta</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="9911" citStr="Turney and Pantel, 2010" startWordPosition="1535" endWordPosition="1539"> a distribution, although Ben-David et al. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijie Wang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>An svmtool-based chinese pos tagger.</title>
<date>2009</date>
<journal>In Journal of Chinese Information Processing.</journal>
<contexts>
<context position="27658" citStr="Wang et al., 2009" startWordPosition="4561" endWordPosition="4564">he target domain. Our models tended to perform better with increasing β on development data, though with diminishing returns. We pick the largest setting tested, β = 100, for our final models. We use a linear-chain Conditional Random Field (CRF) for our supervised classifier. To incorporate the learned representations, we use the Viterbi Algorithm to find the optimal latent state sequence from each HMM-based model and then use the optimal states as features in the CRF. Table 1 presents the full list of features in the CRF. To handle Chinese, we add in two features introduced in previous work (Wang et al., 2009): radical features and repeated characters. A radical is a portion of a Chinese character that consists of a small number of pen or brush strokes in a regular pattern. Accuracy 0.928 0.923 0.918 0.913 0.908 0.903 0.898 0.893 0.888 News Domain (development data) alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100 0.1 1 10 100 1000 σ (log scale) 1[yi = y]q(Yi = y) |US| �� ijxiEUT 1319 Figure 3: Validating parameter settings on fiction text CRF Feature Set Transition bz1[zj = z] bz,z/1[zj = z and zj_1 = z&apos;] Word b,,,,z1[xj = w and zj = z] Radical bz,r1[�cEx,radical(c) = r and zj = z] Repeated Words b</context>
</contexts>
<marker>Wang, Che, Liu, 2009</marker>
<rawString>Lijie Wang, Wanxiang Che, and Ting Liu. 2009. An svmtool-based chinese pos tagger. In Journal of Chinese Information Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>H Fu</author>
<author>H Zha</author>
<author>J Barlow</author>
</authors>
<title>Semisupervised nonlinear dimensionality reduction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="17679" citStr="Yang et al., 2006" startWordPosition="2837" endWordPosition="2840">ict the labels in training data, given the learned latent states. Our entropy property uses conditional entropy of the labels given the latent state as the measure of unpredictability: φentropy(y, z) = � � P(yi, zi) log P�(zi|yi) (6) i where P is the empirical probability and i indicates the ith position in the data. We can plug this feature into Equation 5 to obtain a new version of Equation 4 as an objective function for task-specific representations. We refer to this model as HMM+E. Unlike previous formulations for supervised and semisupervised dimensionality reduction (Zhang et al., 2007; Yang et al., 2006), our framework works efficiently for structured representations. 3.2 A Bias for Domain-Independent Features Following the theory in Section 2.2, we devise a biased objective to provide an explicit mechanism for minimizing the distance between the source and target domain. As before, we construct a property of the completed data: φdistance(y) = d1(�PS, PT) where �PS(Y ) is the empirical distribution over latent state values estimated from source-domain latent states, and similarly for PT(Y). Essentially, 1Note that t , unlike A and or, is not a free parameter. It is explicitly minimized in the</context>
</contexts>
<marker>Yang, Fu, Zha, Barlow, 2006</marker>
<rawString>X. Yang, H. Fu, H. Zha, and J. Barlow. 2006. Semisupervised nonlinear dimensionality reduction. In Proceedings of the 23rd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>D Johnson</author>
</authors>
<title>A robust risk minimization based named entity recognition system.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="33736" citStr="Zhang and Johnson (2003)" startWordPosition="5577" endWordPosition="5580">94 0.9 σ=10 σ=1 σ=0.1 σ=1000 σ=100 Unconstrained HMM Table 3: English Named Entity recognition results Figure 4: Greater distance between domains correlates with worse target-domain tagging accuracy. CoNLL 2003 shared task for our labeled training set, consisting of 204k tokens from the newswire domain. We tested the system on the MUC7 formal run test data, consisting of 59k tokens of stories on the telecommunications and aerospace industries. To train our representations, we use the CoNLL training data and the MUC7 training data without labels. We again use a CRF, with features introduced by Zhang and Johnson (2003) for our baseline. We use the same setting of free parameters from our POS tagging experiments. Results are shown in Table 3. Our best biased representation P-HMM+D+E outperformed the unbiased HMM representation by 3.6%, and beats the I-HMM+D+E by 1.6%. The domain-distance and multi-dimensional biases help most, while the taskspecific bias helps somewhat, but only when the domain-distance bias is included. The best system tested on this dataset achieved a slightly better F1 score (78.84) (Turian et al., 2010), but used a much larger training corpus (they use RCV1 corpus which contains approxim</context>
</contexts>
<marker>Zhang, Johnson, 2003</marker>
<rawString>T. Zhang and D. Johnson. 2003. A robust risk minimization based named entity recognition system. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>Z H Zhou</author>
<author>S Chen</author>
</authors>
<title>Semisupervised dimensionality reduction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th SIAM International Conference on Data Mining.</booktitle>
<contexts>
<context position="17659" citStr="Zhang et al., 2007" startWordPosition="2833" endWordPosition="2836">ficult it is to predict the labels in training data, given the learned latent states. Our entropy property uses conditional entropy of the labels given the latent state as the measure of unpredictability: φentropy(y, z) = � � P(yi, zi) log P�(zi|yi) (6) i where P is the empirical probability and i indicates the ith position in the data. We can plug this feature into Equation 5 to obtain a new version of Equation 4 as an objective function for task-specific representations. We refer to this model as HMM+E. Unlike previous formulations for supervised and semisupervised dimensionality reduction (Zhang et al., 2007; Yang et al., 2006), our framework works efficiently for structured representations. 3.2 A Bias for Domain-Independent Features Following the theory in Section 2.2, we devise a biased objective to provide an explicit mechanism for minimizing the distance between the source and target domain. As before, we construct a property of the completed data: φdistance(y) = d1(�PS, PT) where �PS(Y ) is the empirical distribution over latent state values estimated from source-domain latent states, and similarly for PT(Y). Essentially, 1Note that t , unlike A and or, is not a free parameter. It is explici</context>
</contexts>
<marker>Zhang, Zhou, Chen, 2007</marker>
<rawString>D. Zhang, Z.H. Zhou, and S. Chen. 2007. Semisupervised dimensionality reduction. In Proceedings of the 7th SIAM International Conference on Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>